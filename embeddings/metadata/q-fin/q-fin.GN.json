[
    {
        "title": "Taxation and Valuation",
        "authors": [
            "Leonid A. Levin"
        ],
        "category": "q-fin.GN",
        "published_year": "2000",
        "summary": "  I explain the root of persistent failure of efforts to remove tax-induced\ndistortions of economic incentives. It lies in FUNDAMENTAL IMPOSSIBILITY of\nobjectively evaluating tax base. Distortions can be entirely avoided in the\nsector of publicly traded corporations. Evaluation can be bypassed by taxing it\nin shares (to be auctioned) rather than cash.\n  Stock capital includes cost basis (B) and unrealized gains (G). Gains are\npresently tax-deferred until realized in divestment. The deferral is remedied\nby corporate income tax (rate t). i is the variable interest rate on special\nconstant value \"cv-bonds\".\n  The proposed system replaces (1) corporate income tax - with interest on the\ndeferred G*t, and (2) divestment taxes - with interest on B*t. To collect both,\nIRS will periodically take to auction a fraction i*t of privately held publicly\ntraded shares. Note: (2) is a neutral simplification: Investments can be split\ninto B(1-t) stock and B*t in bond portfolios. Bond interest buys back the\nauctioned shares, and tax-free divestment matches the original yield.\n  The i*t stock tax matches the income tax on cv-bond portfolios of equal\nvalue. The Treasury, too, could match its income (in bond sales) to the rate t\ntax on the full stock market return (without tempting price manipulation). It\ncan vary i to keep the bond volume at a fraction t of market capitalization;\nthen share auctions supply bond interest. Taxpayers, too, could unilaterally\nmatch their burden to such tax by keeping a fraction t of capital in bonds.\n  The main feature is: nothing companies and investors do can change their tax\n(fraction i*t of shares), so business decisions would be exactly the same as\nwithout taxes. No longer would taxes on dividends and capital gains impede\ncapital flow, companies would forget bewildering mazes of tax laws,\nregulations, precedents; Congress would still collect the same revenue it now\ndoes.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0012013v22"
    },
    {
        "title": "Fluctuations of company yearly profits versus scaled revenue: Fat tail\n  distribution of Levy type",
        "authors": [
            "H. E. Roman",
            "R. A. Siliprandi",
            "C. Dose",
            "C. Riccardi",
            "M. Porto"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We analyze annual revenues and earnings data for the 500 largest-revenue U.S.\ncompanies during the period 1954-2007. We find that mean year profits are\nproportional to mean year revenues, exception made for few anomalous years,\nfrom which we postulate a linear relation between company expected mean profit\nand revenue. Mean annual revenues are used to scale both company profits and\nrevenues. Annual profit fluctuations are obtained as difference between actual\nannual profit and its expected mean value, scaled by a power of the revenue to\nget a stationary behavior as a function of revenue. We find that profit\nfluctuations are broadly distributed having approximate power-law tails with a\nLevy-type exponent $\\alpha \\simeq 1.7$, from which we derive the associated\nbreak-even probability distribution. The predictions are compared with\nempirical data.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.3885v1"
    },
    {
        "title": "Steady coexistence of the subjects of the market representing the\n  private and state capital",
        "authors": [
            "Viktor I. Shapovalov"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  The sustainability conditions for the market participants with a different\nownership model were also determined. It was revealed, that the nonlinear form\nof the equations describing the market behavior with the prevailing private\ncapital, predetermines the development of such a market according to the\nsubharmonic cascade scenario. The latter is presumably the reason of the\nperiodically arising economic crises.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.4028v1"
    },
    {
        "title": "Economic law of increase of Kolmogorov complexity. Transition from\n  financial crisis 2008 to the zero-order phase transition (social explosion)",
        "authors": [
            "V. P. Maslov"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  In Maslov (2003), a two level model of the occurrence of financial pyramid\n(bubbles) has been considered. We also considered the mathematical analogy of\nthis model to Bose condensation. In the present paper, we explain why Ponzi\nschemes and bubbles result in a crisis in real economics. In Maslov (2005), the\nlaw of increase of entropy in financial systems, and consequently increase of\nKolmogorov complexity, is formulated. If this law is broken, the financial\nsystem makes a phase transition to a different state. In Maslov (2005) the\nauthor considered a two level model of the zeroth-order phase transition which\nwas interpreted in Maslov (2006) as an analog of social catastrophe. In the\npresent paper we also examine this model.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.4737v1"
    },
    {
        "title": "Optimal dividend distribution under Markov-regime switching",
        "authors": [
            "Zhengjun Jiang",
            "Martijn Pistorius"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We investigate the problem of optimal dividend distribution for a company in\nthe presence of regime shifts. We consider a company whose cumulative net\nrevenues evolve as a Brownian motion with positive drift that is modulated by a\nfinite state Markov chain, and model the discount rate as a deterministic\nfunction of the current state of the chain. In this setting the objective of\nthe company is to maximize the expected cumulative discounted dividend payments\nuntil the moment of bankruptcy, which is taken to be the first time that the\ncash reserves (the cumulative net revenues minus cumulative dividend payments)\nare zero. We show that, if the drift is positive in each state, it is optimal\nto adopt a barrier strategy at certain positive regime-dependent levels, and\nprovide an explicit characterization of the value function as the fixed point\nof a contraction. In the case that the drift is small and negative in one\nstate, the optimal strategy takes a different form, which we explicitly\nidentify if there are two regimes. We also provide a numerical illustration of\nthe sensitivities of the optimal barriers and the influence of\nregime-switching.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.4978v2"
    },
    {
        "title": "Mathematical analysis of Soros's theory of reflexivity",
        "authors": [
            "C. P. Kwong"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  The mathematical model proposed by George Soros for his theory of reflexivity\nis analyzed under the framework of discrete dynamical systems. We show the\nimportance of the notion of fixed points for explaining the behavior of a\nreflexive system governed by its cognitive and manipulative functions. The\ninterrelationship between these two functions induces fixed points with\ndifferent characteristics, which in turn generate various system behaviors\nincluding the so-called \"boom then bust\" phenomenon in Soros's theory.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4447v1"
    },
    {
        "title": "The Reality Game",
        "authors": [
            "Dmitriy Cherkashin",
            "J. Doyne Farmer",
            "Seth Lloyd"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We introduce an evolutionary game with feedback between perception and\nreality, which we call the reality game. It is a game of chance in which the\nprobabilities for different objective outcomes (e.g., heads or tails in a coin\ntoss) depend on the amount wagered on those outcomes. By varying the `reality\nmap', which relates the amount wagered to the probability of the outcome, it is\npossible to move continuously from a purely objective game in which\nprobabilities have no dependence on wagers to a purely subjective game in which\nprobabilities equal the amount wagered. We study self-reinforcing games, in\nwhich betting more on an outcome increases its odds, and self-defeating games,\nin which the opposite is true. This is investigated in and out of equilibrium,\nwith and without rational players, and both numerically and analytically. We\nintroduce a method of measuring the inefficiency of the game, similar to\nmeasuring the magnitude of the arbitrage opportunities in a financial market.\nWe prove that convergence to equilibrium is is a power law with an extremely\nslow rate of convergence: The more subjective the game, the slower the\nconvergence.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.0100v3"
    },
    {
        "title": "A Conceptual Model for Bidirectional Service, Information and Product\n  Quality in an IS Outsourcing Collaboration Environment",
        "authors": [
            "Subrata Chakrabarty"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This paper advances theory on the process of collaboration between entities\nand its implications on the quality of services, information, and/or products\n(SIPs) that the collaborating entities provide to each other. It investigates\nthe scenario of outsourced IS projects (such as custom software development)\nwhere the extent of collaboration between a client and vendor is high. Using\nthe social exchange theory, the proposed conceptual model tries to establish\nthe \"bidirectional\" nature of SIP quality in a collaborative environment, where\nthe SIPs exchanged are possibly \"dependent\" on each other, and if any entity\nwishes to receive high SIP quality then it should make efforts to provide high\nSIP quality in return too. Furthermore, it advocates increasing efforts to link\nfinancial stakes (tangible or intangible monetary benefits or risks) to the\nquality of SIP being continuously exchanged throughout the project lifecycle.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.0188v1"
    },
    {
        "title": "Time and symmetry in models of economic markets",
        "authors": [
            "Lee Smolin"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  These notes discuss several topics in neoclassical economics and\nalternatives, with an aim of reviewing fundamental issues in modeling economic\nmarkets. I start with a brief, non-rigorous summary of the basic Arrow-Debreu\nmodel of general equilibrium, as well as its extensions to include time and\ncontingency. I then argue that symmetries due to similarly endowed individuals\nand similar products are generically broken by the constraints of scarcity,\nleading to the existence of multiple equilibria.\n  This is followed by an evaluation of the strengths and weaknesses of the\nmodel generally. Several of the weaknesses are concerned with the treatments of\ntime and contingency. To address these we discuss a class of agent based\nmodels.\n  Another set of issues has to do with the fundamental meaning of prices and\nthe related question of what the observables of a non-equilibrium, dynamic\nmodel of an economic market should be. We argue that these issues are addressed\nby formulating economics in the language of a gauge theory, as proposed\noriginally by Malaney and Weinstein. We review some of their work and provide a\nsketch of how gauge invariance can be incorporated into the formulation of\nagent based models.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.4274v1"
    },
    {
        "title": "Mapping markets to the statistical mechanics: the derivatives act\n  against the self-regulation of stock market",
        "authors": [
            "David B. Saakian"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Mapping the economy to the some statistical physics models we get strong\nindications that, in contrary to the pure stock market, the stock market with\nderivatives could not self-regulate.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.3254v1"
    },
    {
        "title": "The Transfer Pricing Problem with Non-Linearities",
        "authors": [
            "S. Zverovich"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  A number of approaches to solving the well-known transfer pricing problem are\nknown. However, few models satisfactorily resolve the core problem of allowing\nboth the source and receiving divisions to earn a profit on transfers during a\nperiod in such a way that sub-optimal output levels are avoided. In 1969,\nSamuel proposed to use a transfer price schedule instead of just a single\ntransfer price. An essential improvement of Samuels' model was given by Tomkins\n(1990) in his pragmatic-analytical transfer pricing approach, which is a\ncombination of a single cost-plus transfer price and the pragmatic process of\nnegotiation. This fundamental approach was developed under the assumption that\nthe net average revenue curve for the final product is linear.\n  In this paper, Tomkins' pragmatic-analytical model is further developed for\nnon-linear net average revenue curves. In particular, typical quadratic\nfunctions are considered and corresponding transfer price schedules are\ndetermined. A similar technique can be used for the transfer pricing problem\nwith any net average revenue curve.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.3346v1"
    },
    {
        "title": "Does economics need a scientific revolution?",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Economics does not need a scientific revolution. Economics needs accurate\nmeasurements according to high standards of natural sciences and meticulous\nwork on revealing empirical relationships between measured variables.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.0729v1"
    },
    {
        "title": "The Problem of Modeling of Economic Dynamics",
        "authors": [
            "S. I. Chernyshov",
            "A. V. Voronin",
            "S. A. Razumovsky"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  The correctness of Harrods model in the differential form is studied. The\ninadequacy of exponential growth of economy is shown; an alternative result is\nobtained. By example of Phillips model, an approach to correction of\nmacroeconomic models (in terms of initial prerequisites) is generalized. A\nmethodology based on balance relations for modelling of economic dynamics,\nincluding obtaining forecast estimates, is developed. The problems thus\nconsidered are reduced to the solution of Volterra and Fredholm integral\nequations of the second kind.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.0756v3"
    },
    {
        "title": "An operatorial approach to stock markets",
        "authors": [
            "F. Bagarello"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We propose and discuss some toy models of stock markets using the same\noperatorial approach adopted in quantum mechanics. Our models are suggested by\nthe discrete nature of the number of shares and of the cash which are exchanged\nin a real market, and by the existence of conserved quantities, like the total\nnumber of shares or some linear combination of cash and shares. The same\nframework as the one used in the description of a gas of interacting bosons is\nadopted.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.0896v1"
    },
    {
        "title": "What are the limits on Commercial Bank Lending?",
        "authors": [
            "Jacky Mallett"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Analysis of the 2007-8 credit crisis has concentrated on issues of relaxed\nlending standards, and the perception of irrational behaviour by speculative\ninvestors in real estate and other assets. Asset backed securities have been\nextensively criticised for creating a moral hazard in loan issuance and an\nassociated increase in default risk, by removing the immediate lender's\nincentive to ensure that the underlying loans could be repaid. However\nsignificant monetary issues can accompany any form of increased commercial bank\nlending, and these appear to have been overlooked by this analysis. In this\npaper we propose a general explanation for credit crises based on an\nexamination of the mechanics of the banking system, and in particular its\ninternal controls on the supply of credit. We suggest that the current credit\ncrisis is the result of multiple failures in the Basel regulatory framework,\nincluding the removal of central bank reserve requirements from some classes of\ndeposit accounts within the banking system, allowing financial instruments\nrepresenting debt to be used as regulatory capital, and in particular the\nintroduction of securitized lending which effectively removed a previously\nimplicit control over the total quantity of lending originating from the\nbanking system. We further argue that the interaction of these problems has led\nto a destabilising imbalance between total money and loan supply growth, in\nthat total lending sourced from the commercial bank sector increased at a\nfaster rate than accompanying growth in the money supply. This not only created\na multi-decade macro-economic debt spiral, but by increasing the ratio of debt\nto money within the monetary system acted to increase the risk of loan\ndefaults, and consequentially reduce the overall stability of the banking\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1426v4"
    },
    {
        "title": "La Loi organique relative aux lois de finances (LOLF) dans les\n  institutions culturelles publiques du spectacle vivant en France",
        "authors": [
            "Ammar Kessab"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  In a crisis of public finances, France bases all its hopes on the \"evaluation\nof performance\" to moderate the effects of a complex crisis. Under the banner\nof \"modernization of the State\", a new \"financial constitution\" called the\nOrganic Law on finance laws (LOLF) became the main lever of reform of public\nmanagement. Fully applied to the Cultural Affairs since 2006, the LOLF is based\non a set of performance indicators and sets the public performance arts\ninstitutions specific targets. This article defines and analyzes the pattern of\nthe design and the course of these indicators and targets. It also examines the\ncontroversy generated by this new mode of governance.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.3929v2"
    },
    {
        "title": "A Markovian Model Market - Akerlof's Lemmons and the Asymmetry of\n  Information",
        "authors": [
            "Paulo F. C. Tilles",
            "Fernando F. Ferreira",
            "Gerson Francisco",
            "Carlos de B. Pereira",
            "Flavia Mori Sarti"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  In this work we study an economic agent based model under different\nasymmetric information degrees. This model is quite simple and can be treated\nanalytically since the buyers evaluate the quality of a certain good taking\ninto account only the quality of the last good purchased plus her perceptive\ncapacity \\beta . As a consequence the system evolves according to a stationary\nMarkovian stochastic process. The value of a product offered by the seller\nincreases with quality according to the exponent \\alpha, which is a measure of\ntechnology. It incorporates all the technological capacity of production\nsystems such as education, scientific development and techniques that change\nthe productivity growth. The technological level plays an important role to\nexplain how the asymmetry of information may affect the market evolution in\nthis model. We observe that, for high technological levels, the market can\ncontrol adverse selection. The model allows us to compute the maximum\nasymmetric information degree before market collapse. Below this critical point\nthe market evolves during a very limited time and then dies out completely.\nWhen \\beta is closer to 1(symmetric information), the market becomes more\nprofitable for high quality goods, although high and low quality markets\ncoexist. All the results we obtained from the model are analytical and the\nmaximum asymmetric information level is a consequence of an ergodicity\nbreakdown in the process of quality evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.0468v1"
    },
    {
        "title": "Presentation Du Nouvel Accord De Bale Sur Les Fonds Propres",
        "authors": [
            "Hamza Fekir"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  In order to adapt to the liberalization of the financial sphere started in\nthe Eighties, marked in particular by the end of the framing of credit, the\ndisappearance of the various forms of protection of the State whose profited\nthe banks, and the privatization of the near total of the establishments in\nEurope, the banking regulation evolved to a prudential approach, perceived like\nthe only mode of regulation not entering in contradiction with the rules of the\nmarket. The current banking regulation is pressed on the supervision, the\ndiscipline of the market and the ratios prudential; in particular the ratios of\nthe minimal own capital stocks. The object of this article is the presentation\nof the architecture of the new agreement of Basle (1999) which is based on\nthree pillars consolidating it self mutually.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.2546v1"
    },
    {
        "title": "A la Recherche des Facteurs Déterminants de l'Intégration\n  Internationale des Marchés Boursiers : une Analyse sur Données de Panel",
        "authors": [
            "Mohamed El Hedi Arouri"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  The aim of this paper is to identify the determinants of international stock\nmarkets integration. Intuitively we selected a great number of factors linked\nto financial integration. Then, we developed an international asset-pricing\nmodel with time-varying degree of integration. This model is estimated for 30\ncountries (10 developed countries and 20 emerging countries) using panel data\neconometrics. In order to investigate whether the financial integration in\nemerging markets and that in developed markets react differently to the\neconomic and financial innovations, we estimated the model as well jointly for\nall markets as separately for developed markets and emerging markets. Our\nresults show that trade openness exerts a positive effect on financial\nintegration across all markets, the global factors drive integration in\ndeveloped markets whereas the factors related to economic and political\nstability affect financial integration in emerging markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3871v1"
    },
    {
        "title": "Structural Breaks in the Mexico's Integration into the World Stock\n  Market",
        "authors": [
            "Mohamed El Hedi Arouri",
            "Jamel Jouini"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This article investigates the evolution of the Mexican stock market\nintegration into the world market. First, we estimate the time-varying Mexican\ndegree of market integration using an international conditional version of the\nCAPM with segmentation effects. Second, we study the structural breaks in this\nseries. Finally, we relate the obtained results to important facts and economic\nevents\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3873v1"
    },
    {
        "title": "A Prediction Market for Toxic Assets Prices",
        "authors": [
            "Alan Holland"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We propose the development of a prediction market for forecasting prices for\n\"toxic assets\" to be transferred from Irish banks to the National Asset\nManagement Agency (NAMA). Such a market allows market participants to assume a\nstake in a security whose value is tied to a future event. We propose that\nsecurities are created whose value hinges on the transfer amount paid for loans\nfrom NAMA to a bank. In essence, bets are accepted on whether the price is\nhigher or lower than a certain quoted figure. The prices of the securities\nrepresent transfer prices for toxic assets increases or decreases in line with\nmarket opinion. Prediction markets offer a proven means of aggregating\ndistributed knowledge pertaining to fair market values in a scalable and\ntransparent manner. They are incentive compatible (i.e. induce truthful\nreporting) and robust to strategic manipulation. We propose that a prediction\nmarket is run in parallel with the pricing procedure recommended by the\nEuropean Commission. This procedure need not necessarily take heed of the\nprediction markets view in all cases but it may offer guidance and a means of\nanomaly detection. An online prediction market would offer everybody an\nopportunity to \"have their say\" in an open and transparent manner.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4171v1"
    },
    {
        "title": "An example of a stochastic equilibrium with incomplete markets",
        "authors": [
            "Gordan Zitkovic"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We prove existence and uniqueness of stochastic equilibria in a class of\nincomplete continuous-time financial environments where the market participants\nare exponential utility maximizers with heterogeneous risk-aversion\ncoefficients and general Markovian random endowments. The incompleteness\nfeatured in our setting - the source of which can be thought of as a credit\nevent or a catastrophe - is genuine in the sense that not only the prices, but\nalso the family of replicable claims itself is determined as a part of the\nequilibrium. Consequently, equilibrium allocations are not necessarily Pareto\noptimal and the related representative-agent techniques cannot be used.\nInstead, we follow a novel route based on new stability results for a class of\nsemilinear partial differential equations related to the\nHamilton-Jacobi-Bellman equation for the agents' utility-maximization problems.\nThis approach leads to a reformulation of the problem where the Banach fixed\npoint theorem can be used not only to show existence and uniqueness, but also\nto provide a simple and efficient numerical procedure for its computation.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.0208v2"
    },
    {
        "title": "Improved and Developed Upper Bound of Price of Anarchy in Two Echelon\n  Case",
        "authors": [
            "T. Shinzato",
            "I. Kaku"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Price of anarchy, the performance ratio, which could characterize the loss of\nefficiency of the distributed supply chain management compared with the\nintegrated supply chain management is discussed by utilizing newsvendor problem\nin single period which is well-known. In particular, some of remarkable\ndistributed policies are handled, the performance ratios in each case which\nhave been investigated in the previous works are analyzed theoretically and the\ntighter upper bound of price of anarchy and the lower bound are presented.\nFurthermore our approach is developed based on a generalized framework and a\ngeometric interpretation of price of anarchy is appeared via the literature of\nconvex optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.5489v1"
    },
    {
        "title": "A quantum statistical approach to simplified stock markets",
        "authors": [
            "Fabio Bagarello"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We use standard perturbation techniques originally formulated in quantum\n(statistical) mechanics in the analysis of a toy model of a stock market which\nis given in terms of bosonic operators. In particular we discuss the\nprobability of transition from a given value of the {\\em portfolio} of a\ncertain trader to a different one. This computation can also be carried out\nusing some kind of {\\em Feynman graphs} adapted to the present context.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.2531v1"
    },
    {
        "title": "Heterogeneous Beliefs with Partial Observations",
        "authors": [
            "A. A. Brown"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This paper examines a heterogeneous beliefs model in which there is a process\nthat is only partially observed by the agents. The economy contains a risky\nasset producing dividends continuously in time. The dividends are observed by\nthe agents. The dividends are assumed to be a known function of some other\nunobserved process. The agents use filtering to estimate the value of this\nunobserved process. The agents have different beliefs about the dynamics of the\nunobserved process and therefore form different estimates. We analyse this\nmodel and derive the state price density. We use this to derive the riskless\nrate. We also characterise the price of the risky asset in terms of the\nsolution of a series of differential equations.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.4950v1"
    },
    {
        "title": "Heterogeneous Beliefs with Finite-Lived Agents",
        "authors": [
            "A. A. Brown",
            "L. C. G. Rogers"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This paper will examine a model with many agents, each of whom has a\ndifferent belief about the dynamics of a risky asset. The agents are Bayesian\nand so learn about the asset over time. All agents are assumed to have a finite\n(but random) lifetime. When an agent dies, he passes his wealth (but not his\nknowledge) onto his heir. As a result, the agents never become sure of the\ndynamics of the risky asset. We derive expressions for the stock price and\nriskless rate. We then use numerical examples to exhibit their behaviour.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.4953v1"
    },
    {
        "title": "A note on heterogeneous beliefs with CRRA utilities",
        "authors": [
            "A. A. Brown"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This note will extend the research presented in Brown & Rogers (2009) to the\ncase of CRRA agents. We consider the model outlined in that paper in which\nagents had diverse beliefs about the dividends produced by a risky asset. We\nnow assume that the agents all have CRRA utility, with some integer coefficient\nof relative risk aversion. This is a generalisation of Brown & Rogers which\nconsidered logarithmic agents. We derive expressions for the state price\ndensity, riskless rate, stock price and wealths of the agents. This sheds light\non the effects of risk aversion in an equilibrium with diverse beliefs.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.4964v1"
    },
    {
        "title": "Optimal intervention in the foreign exchange market when interventions\n  affect market dynamics",
        "authors": [
            "Alec N. Kercheval",
            "Juan F. Moreno"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We address the problem of optimal Central Bank intervention in the exchange\nrate market when interventions create feedback in the rate dynamics. In\nparticular, we extend the work done on optimal impulse control by Cadenillas\nand Zapatero to incorporate temporary market reactions, of random duration and\nlevel, to Bank interventions, and to establish results for more general rate\nprocesses. We obtain new explicit optimal impulse control strategies that\naccount for these market reactions, and show that they cannot be obtained\nsimply by adjusting the intervention cost in a model without market reactions.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.1142v1"
    },
    {
        "title": "Weighted Trade Network in a Model of Preferential Bipartite Transactions",
        "authors": [
            "Abhijit Chakraborty",
            "S. S. Manna"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Using a model of wealth distribution where traders are characterized by\nquenched random saving propensities and trade among themselves by bipartite\ntransactions, we mimic the enhanced rates of trading of the rich by introducing\nthe preferential selection rule using a pair of continuously tunable\nparameters. The bipartite trading defines a growing trade network of traders\nlinked by their mutual trade relationships. With the preferential selection\nrule this network appears to be highly heterogeneous characterized by the\nscale-free nodal degree and the link weight distributions and presents\nsignatures of non-trivial strength-degree correlations. With detailed numerical\nsimulations and using finite-size scaling analysis we present evidence that the\nassociated critical exponents are continuous functions of the tuning\nparameters. However the wealth distribution has been observed to follow the\nwell-known Pareto law robustly for all positive values of the tuning\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.3984v1"
    },
    {
        "title": "Diagnostics of Rational Expectation Financial Bubbles with Stochastic\n  Mean-Reverting Termination Times",
        "authors": [
            "Li Lin",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We propose two rational expectation models of transient financial bubbles\nwith heterogeneous arbitrageurs and positive feedbacks leading to\nself-reinforcing transient stochastic faster-than-exponential price dynamics.\nAs a result of the nonlinear feedbacks, the termination of a bubble is found to\nbe characterized by a finite-time singularity in the bubble price formation\nprocess ending at some potential critical time $\\tilde{t}_c$, which follows a\nmean-reversing stationary dynamics. Because of the heterogeneity of the\nrational agents' expectations, there is a synchronization problem for the\noptimal exit times determined by these arbitrageurs, which leads to the\nsurvival of the bubble almost all the way to its theoretical end time. The\nexplicit exact analytical solutions of the two models provide nonlinear\ntransformations which allow us to develop novel tests for the presence of\nbubbles in financial time series. Avoiding the difficult problem of parameter\nestimation of the stochastic differential equation describing the price\ndynamics, the derived operational procedures allow us to diagnose bubbles that\nare in the making and to forecast their termination time. The tests performed\non three financial markets, the US S&P500 index from 1 February 1980 to 31\nOctober 2008, the US NASDAQ composite index from 1 January 1980 to 31 July 2008\nand the Hong Kong Hang Seng index from 1 December 1986 to 30 November 2008,\nsuggest the feasibility of advance bubble warning.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.1921v1"
    },
    {
        "title": "Credit derivatives: instruments of hedging and factors of instability.\n  The example of ?Credit Default Swaps? on French reference entities",
        "authors": [
            "Nathalie Rey"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Through a long-period analysis of the inter-temporal relations between the\nFrench markets for credit default swaps (CDS), shares and bonds between 2001\nand 2008, this article shows how a financial innovation like CDS could heighten\nfinancial instability. After describing the operating principles of credit\nderivatives in general and CDS in particular, we construct two difference VAR\nmodels on the series: the share return rates, the variation in bond spreads and\nthe variation in CDS spreads for thirteen French companies, with the aim of\nbringing to light the relations between these three markets. According to these\nmodels, there is indeed an interdependence between the French share, CDS and\nbond markets, with a strong influence of the share market on the other two.\nThis interdependence increases during periods of tension on the markets\n(2001-2002, and since the summer of 2007).\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4039v1"
    },
    {
        "title": "About Some Applications of Kolmogorov Equations to the Simulation of\n  Financial Institutions Activity",
        "authors": [
            "Mikhail I. Rumyantsev"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  The goal of this article is to describe the concepts of system dynamics and\nits applications to the simulation modeling of financial institutions daily\nactivity. The hybrid method of the re-engineering of banking business processes\nbased upon combination of system dynamics, queuing theory and tools of ordinary\ndifferential equations (Kolmogorov equations) is offered.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.1037v1"
    },
    {
        "title": "Modelling and predicting labor force productivity",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Labor productivity in Turkey, Spain, Belgium, Austria, Switzerland, and New\nZealand has been analyzed and modeled. These counties extend the previously\nanalyzed set of the US, UK, Japan, France, Italy, and Canada. Modelling is\nbased on the link between the rate of labor participation and real GDP per\ncapita. New results validate the link and allow predicting a drop in\nproductivity by 2010 in almost all studied countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4889v1"
    },
    {
        "title": "Inflation and unemployment in Japan: from 1980 to 2050",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The evolution of inflation, p(t), and unemployment, UE(t), in Japan has been\nmodeled. Both variables were represented as linear functions of the change rate\nof labor force, dLF/LF. These models provide an accurate description of\ndisinflation in the 1990s and a deflationary period in the 2000s. In Japan,\nthere exists a statistically reliable (R2=0.68) Phillips curve, which is\ncharacterized by a negative relation between inflation and unemployment and\ntheir synchronous evolution: UE(t) = -0.94p(t) + 0.045. Effectively, growing\nunemployment has resulted in decreasing inflation since 1982. A linear and\nlagged generalized relationship between inflation, unemployment and labor force\nhas been also obtained for Japan: p(t) = 2.8*dLF(t)/LF(t) + 0.9*UE(t) - 0.0392.\nLabor force projections allow a prediction of inflation and unemployment in\nJapan: CPI inflation will be negative (between -0.5% and -1% per year) during\nthe next 40 years. Unemployment will increase from ~4.0% in 2010 to 5.3% in\n2050.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.0277v1"
    },
    {
        "title": "What is Fair Pay for Executives? An Information Theoretic Analysis of\n  Wage Distributions",
        "authors": [
            "Venkat Venkatasubramanian"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The high pay packages of U.S. CEOs have raised serious concerns about what\nwould constitute a fair pay.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2269v1"
    },
    {
        "title": "Regulation Simulation",
        "authors": [
            "Philip Maymin"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  A deterministic trading strategy by a representative investor on a single\nmarket asset, which generates complex and realistic returns with its first four\nmoments similar to the empirical values of European stock indices, is used to\nsimulate the effects of financial regulation that either pricks bubbles, props\nup crashes, or both. The results suggest that regulation makes the market\nprocess appear more Gaussian and less complex, with the difference more\npronounced for more frequent intervention, though particular periods can be\nworse than the non-regulated version, and that pricking bubbles and propping up\ncrashes are not symmetrical.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2281v1"
    },
    {
        "title": "Arbitrage strategy",
        "authors": [
            "Constantinos Kardaras"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  An arbitrage strategy allows a financial agent to make certain profit out of\nnothing, i.e., out of zero initial investment. This has to be disallowed on\neconomic basis if the market is in equilibrium state, as opportunities for\nriskless profit would result in an instantaneous movement of prices of certain\nfinancial instruments. The principle of not allowing for arbitrage\nopportunities in financial markets has far-reaching consequences, most notably\nthe option-pricing and hedging formulas in complete markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2740v1"
    },
    {
        "title": "Free Lunch",
        "authors": [
            "Constantinos Kardaras"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The concept of absence of opportunities for free lunches is one of the\npillars in the economic theory of financial markets. This natural assumption\nhas proved very fruitful and has lead to great mathematical, as well as\neconomical, insights in Quantitative Finance. Formulating rigorously the exact\ndefinition of absence of opportunities for riskless profit turned out to be a\nhighly non-trivial fact that troubled mathematicians and economists for at\nleast two decades. The purpose of this note is to give a quick (and,\nnecessarily, incomplete) account of the recent work aimed at providing a simple\nand intuitive no-free-lunch assumption that would suffice in formulating a\nversion of the celebrated Fundamental Theorem of Asset Pricing.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2741v1"
    },
    {
        "title": "The Problem of Modeling of Economic Dynamics (new version)",
        "authors": [
            "S. I. Chernyshov",
            "A. V. Voronin",
            "S. A. Razumovsky"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The correctness of Harrods model in the differential form is studied. The\ninadequacy of exponential growth of economy is shown; an alternative result is\nobtained. By example of Phillips model, an approach to correction of\nmacroeconomic models (in terms of initial prerequisites) is generalized. A\nmethodology based on balance relations for modelling of economic dynamics,\nincluding obtaining forecast estimates, is developed. The problems thus\nconsidered are reduced to the solution of Volterra and Fredholm integral\nequations of the second kind.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4382v1"
    },
    {
        "title": "Diversity and Arbitrage in a Regulatory Breakup Model",
        "authors": [
            "Winslow Strong",
            "Jean-Pierre Fouque"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  In 1999 Robert Fernholz observed an inconsistency between the normative\nassumption of existence of an equivalent martingale measure (EMM) and the\nempirical reality of diversity in equity markets. We explore a method of\nimposing diversity on market models by a type of antitrust regulation that is\ncompatible with EMMs. The regulatory procedure breaks up companies that become\ntoo large, while holding the total number of companies constant by imposing a\nsimultaneous merge of other companies. The regulatory events are assumed to\nhave no impact on portfolio values. As an example, regulation is imposed on a\nmarket model in which diversity is maintained via a log-pole in the drift of\nthe largest company. The result is the removal of arbitrage opportunities from\nthis market while maintaining the market's diversity.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.5650v2"
    },
    {
        "title": "Diagnosis and Prediction of Market Rebounds in Financial Markets",
        "authors": [
            "Wanfeng Yan",
            "Ryan Woodard",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We introduce the concept of \"negative bubbles\" as the mirror image of\nstandard financial bubbles, in which positive feedback mechanisms may lead to\ntransient accelerating price falls. To model these negative bubbles, we adapt\nthe Johansen-Ledoit-Sornette (JLS) model of rational expectation bubbles with a\nhazard rate describing the collective buying pressure of noise traders. The\nprice fall occurring during a transient negative bubble can be interpreted as\nan effective random downpayment that rational agents accept to pay in the hope\nof profiting from the expected occurrence of a possible rally. We validate the\nmodel by showing that it has significant predictive power in identifying the\ntimes of major market rebounds. This result is obtained by using a general\npattern recognition method which combines the information obtained at multiple\ntimes from a dynamical calibration of the JLS model. Error diagrams, Bayesian\ninference and trading strategies suggest that one can extract genuine\ninformation and obtain real skill from the calibration of negative bubbles with\nthe JLS model. We conclude that negative bubbles are in general predictably\nassociated with large rebounds or rallies, which are the mirror images of the\ncrashes terminating standard bubbles.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.5926v2"
    },
    {
        "title": "L'effet de levier de trésorerie",
        "authors": [
            "Jean-Claude Juhel"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The effect of leverage on liquidity is a tool for analysing the level of\nliquidity for a given production process. It measures the sensitivity of the\nlevel of liquidity that results from changes in the volume of production and\nunit operating margin. A commercial activity is liquid at the moment when all\ncosts are covered by revenues. However, not all of the cash flows from\nproduction influence liquidity levels. The estimated costs do not directly\ninfluence the level of liquidity. Therefore, two indicators are to be taken\ninto consideration: the elasticity of ongoing liquidity - fixed costs include\nestimated costs, and, the elasticity of immediate liquidity - fixed costs only\ninclude costs that are payable. The coefficients of leverage of ongoing\nliquidity and of leverage of immediate liquidity in relation to the operating\nmargin have a behaviour that is identical to that calculated in relation to\nproduction. If the productive capacity remains unchanged, the regulation of the\nchange in elasticity of the costs and of its influence on the unitary operating\nmargin is the sole parameter available to the entrepreneur to maintain the\nliquidity of the company at the desired level. But, if the productive capacity\nis variable, the entrepreneur can use the volume of sales to control liquidity\nbut then the transformation of the production process must be analysed so as to\nadjust the relevant elements to retain in the operating structure the degree of\nliquidity wished for.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.0682v1"
    },
    {
        "title": "Quantum Portfolios of Observables and the Risk Neutral Valuation Model",
        "authors": [
            "Fredrick Michael"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Quantum Portfolios of quantum algorithms encoded on qbits have recently been\nreported. In this paper a discussion of the continuous variables version of\nquantum portfolios is presented. A risk neutral valuation model for options\ndependent on the measured values of the observables, analogous to the\ntraditional Black-Scholes valuation model, is obtained from the underlying\nstochastic equations. The quantum algorithms are here encoded on simple\nharmonic oscillator (SHO) states, and a Fokker-Planck equation for the Glauber\nP-representation is obtained as a starting point for the analysis. A discussion\nof the observation of the polarization of a portfolio of qbits is also obtained\nand the resultant Fokker-Planck equation is used to obtain the risk neutral\nvaluation of the qbit polarization portfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.0844v1"
    },
    {
        "title": "Fundamental defect of the macroeconomic thinking as one of the main\n  causes of the crisis endured",
        "authors": [
            "Eugen Perchik"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The main points of the first section of the article written by S.I.\nChernyshov, A.V. Voronin and S.A. Razumovsky arXiv:1003.4382), which deals with\nthe fundamental bases of the macroeconomic theory, have been analyzed. An\nincorrectness of the Harrod's model of the economical growth in its generally\naccepted interpretation was specifically considered. The inevitability of the\neconomic crisis has been shown to follow directly from the premises of this\nmodel. At the same time there is an opportunity to realize the damping\nstrategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3067v1"
    },
    {
        "title": "A discussion of stock market speculation by Pierre-Joseph Proudhon",
        "authors": [
            "Jean-Claude Juhel",
            "Dominique Dufour"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The object of this contribution is to present the ideas behind the thinking\nof the French economist Pierre-Joseph Proudhon (1809-1865) in relation to the\ncauses and effects of Stock market speculation. It is based upon the works of\nthis author but particularly on his \"Manuel du sp\\'eculateur \\`a la Bourse\"\n(Stock Market Speculator Manual) edited in 1857 in Paris. Compared to the\nmarkets of today, however, the stock market described by Proudhon appears\nembryonic. Nevertheless it represents the location for transactions in\nfinancial assets, commodities, precious metals and even some transactions\ninvolving options. This contribution is organised in the following manner - the\nfirst section is devoted to the development of Proudhon's thought in relation\nto speculation. It is divided into two parts. The first part is dedicated to\nPierre-Joseph Proudhon's definitions of stock market speculation or gambling\nwith shares that for him served no purpose either from a human or economic\nperspective and was therefore condemnable and to be contrasted with\nentrepreneurial speculation that, even though it is a highly-risky activity,\ninvolves the spirit of enterprise and provides the lifeblood of economic\ngrowth. The second part allows us to present Pierre-Joseph Proudhon's\npropositions in relation to restricting the speculation that he considers\nobnoxious. The second section has two objectives: one part places in\nperspective the views of Proudhon and the characteristics of stock market\nactivity under the Second Empire whilst the other part examines current-day\naspects of the characteristics evoked by Proudhon. We are interested especially\nin the question of the regulation and that of the relevance today of certain\naccounting practices.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0221v1"
    },
    {
        "title": "Stable-1/2 Bridges and Insurance",
        "authors": [
            "Edward Hoyle",
            "Lane P. Hughston",
            "Andrea Macrina"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We develop a class of non-life reserving models using a stable-1/2 random\nbridge to simulate the accumulation of paid claims, allowing for an essentially\narbitrary choice of a priori distribution for the ultimate loss. Taking an\ninformation-based approach to the reserving problem, we derive the process of\nthe conditional distribution of the ultimate loss. The \"best-estimate ultimate\nloss process\" is given by the conditional expectation of the ultimate loss. We\nderive explicit expressions for the best-estimate ultimate loss process, and\nfor expected recoveries arising from aggregate excess-of-loss reinsurance\ntreaties. Use of a deterministic time change allows for the matching of any\ninitial (increasing) development pattern for the paid claims. We show that\nthese methods are well-suited to the modelling of claims where there is a\nnon-trivial probability of catastrophic loss. The generalized inverse-Gaussian\n(GIG) distribution is shown to be a natural choice for the a priori ultimate\nloss distribution. For particular GIG parameter choices, the best-estimate\nultimate loss process can be written as a rational function of the paid-claims\nprocess. We extend the model to include a second paid-claims process, and allow\nthe two processes to be dependent. The results obtained can be applied to the\nmodelling of multiple lines of business or multiple origin years. The\nmulti-dimensional model has the property that the dimensionality of\ncalculations remains low, regardless of the number of paid-claims processes. An\nalgorithm is provided for the simulation of the paid-claims processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0496v5"
    },
    {
        "title": "GDP Trend Deviations and the Yield Spread: the Case of Five E.U.\n  Countries",
        "authors": [
            "Periklis Gogas",
            "Ioannis Pragidis"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Several studies have established the predictive power of the yield curve in\nterms of real economic activity. In this paper we use data for a variety of\nE.U. countries: both EMU (Germany, France, Italy) and non-EMU members (Sweden\nand the U.K.). The data used range from 1991:Q1 to 2009:Q1. For each country,\nwe extract the long run trend and the cyclical component of real economic\nactivity, while the corresponding interbank interest rates of long and short\nterm maturities are used for the calculation of the country specific yield\nspreads. We also augment the models tested with non monetary policy variables:\nthe countries' unemployment rates and stock indices. The methodology employed\nin the effort to forecast real output, is a probit model of the inverse\ncumulative distribution function of the standard distribution, using several\nformal forecasting and goodness of fit evaluation tests. The results show that\nthe yield curve augmented with the non-monetary variables has significant\nforecasting power in terms of real economic activity but the results differ\nqualitatively between the individual economies examined raising non-trivial\npolicy implications.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1326v1"
    },
    {
        "title": "Two-sided estimates for stock price distribution densities in\n  jump-diffusion models",
        "authors": [
            "Archil Gulisashvili",
            "Josep Vives"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We consider uncorrelated Stein-Stein, Heston, and Hull-White models and their\nperturbations by compound Poisson processes with jump amplitudes distributed\naccording to a double exponential law. Similar perturbations of the\nBlack-Scholes model were studied by S. Kou. For perturbed stochastic volatility\nmodels, we obtain two-sided estimates for the stock price distribution density\nand compare the tail behavior of this density before and after perturbation. It\nis shown that if the value of the parameter, characterizing the right tail of\nthe double exponential law, is small, then the stock price density in the\nperturbed model decays slower than the density in the original model. On the\nother hand, if the value of this parameter is large, then there are no\nsignificant changes in the behavior of the stock price distribution density.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1917v1"
    },
    {
        "title": "Inequality reversal: effects of the savings propensity and correlated\n  returns",
        "authors": [
            "Anindya S. Chakrabarti",
            "Bikas K. Chakrabarti"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  In the last decade, a large body of literature has been developed to explain\nthe universal features of inequality in terms of income and wealth. By now, it\nis established that the distributions of income and wealth in various economies\nshow a number of statistical regularities. There are several models to explain\nsuch static features of inequality in an unifying framework and the kinetic\nexchange models, in particular, provide one such framework. Here we focus on\nthe dynamic features of inequality. In the process of development and growth,\ninequality in an economy in terms of income and wealth follows a particular\npattern of rising in the initial stage followed by an eventual fall. This\ninverted U-shaped curve is known as the Kuznets Curve. We examine the\npossibilities of such behavior of an economy in the context of a generalized\nkinetic exchange model. It is shown that under some specific conditions, our\nmodel economy indeed shows inequality reversal.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.3518v1"
    },
    {
        "title": "Business fluctuations in a credit-network economy",
        "authors": [
            "Domenico Delli Gatti",
            "Mauro Gallegati",
            "Bruce Greenwald",
            "Alberto Russo",
            "Joseph E. Stiglitz"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We model a network economy with three sectors: downstream firms, upstream\nfirms, and banks. Agents are linked by productive and credit relationships so\nthat the behavior of one agent influences the behavior of the others through\nnetwork connections. Credit interlinkages among agents are a source of\nbankruptcy diffusion: in fact, failure of fulfilling debt commitments would\nlead to bankruptcy chains. All in all, the bankruptcy in one sector can diffuse\nto other sectors through linkages creating a vicious cycle and bankruptcy\navalanches in the network economy. Our analysis show how the choices of credit\nsupply by both banks and firms are interrelated. While the initial impact of\nmonetary policy is on bank behaviour, we show the interactive play between the\nchoices made by banks, the choices made by firms in their role as providers of\ncredit, and the choices made by firms in their role as producers.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.3521v1"
    },
    {
        "title": "Econophysics: A new discipline",
        "authors": [
            "Sonia R. Bentes"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This paper debates the contribution of Econophysics to the economic or\nfinancial domains. Since the traditional approach performed by Economics or\nFinance has revealed to be insufficient in fully characterizing and explaining\nthe correspondingly phenomena, we discuss whether Econophysics can provide a\nnew insight onto these matters. Thus, an assessment is presented in order to\nweight its potential opportunities and limitations. This is particularly\nrelevant as it is widely recognized that during its yet short existence\nEconophysics has experienced a growing interest not only by physicists but also\nby economists in searching for new approaches that could help explaining\nexisting questions. In fact, many papers have been submitted, some books have\nbeen released, new journals have been published, several conferences have been\nheld, a site is maintained -- http://www.unifr.ch/econophysics where news,\nevents, book reviews, papers and a blog are exhibited; a 3-year licentiate\nstudies (University of Silesia [1]) and a B.Sc. course (University of Wroclaw\n[2]) have been created and also some Ph.D. thesis have been written. Therefore,\na fundamental question arises: Is this just a fad or is it something much more\nconsistent that will prevail? This is what this paper addresses.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.3956v1"
    },
    {
        "title": "Is an historical economic crisis upcoming?",
        "authors": [
            "Caglar Tuncay"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  In this work, the time chart of Dow Jones Industrial Average (DJIA) index is\nanalyzed and approach of recession time term is predicted, which may be\nhallmark of a worldwide economic crisis. However, the methods used for the\nprediction will be disclosed a few years from now. On the other hand, this work\nwill be updated by the author frequently once in every few months where no\nrevisions will be made on the previous uploads and a timestamp will designate\neach part. Thus, the time evolution of the crisis can be followed and the\nprediction may be verified by the readers in time.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1960v1"
    },
    {
        "title": "Incomplete Continuous-time Securities Markets with Stochastic Income\n  Volatility",
        "authors": [
            "Peter Ove Christensen",
            "Kasper Larsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  In an incomplete continuous-time securities market with uncertainty generated\nby Brownian motions, we derive closed-form solutions for the equilibrium\ninterest rate and market price of risk processes. The economy has a finite\nnumber of heterogeneous exponential utility investors, who receive partially\nunspanned income and can trade continuously on a finite time-interval in a\nmoney market account and a single risky security. Besides establishing the\nexistence of an equilibrium, our main result shows that if the investors'\nunspanned income has stochastic countercyclical volatility, the resulting\nequilibrium can display both lower interest rates and higher risk premia\ncompared to the Pareto efficient equilibrium in an otherwise identical complete\nmarket.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.3479v2"
    },
    {
        "title": "Fifteen Years of Econophysics Research",
        "authors": [
            "Bikas K. Chakrabarti",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Econophysics is a new research field, which makes an attempt to bring\neconomics in the fold of natural sciences or specifically attempts for a\n\"physics of economics\". The term Econophysics was formally born in Kolkata in\n1995. The entry on Econophysics in The New Palgrave Dictionary of Economics,\n2nd Ed., Vol 2, Macmillan, NY (2008), pp 729-732, begins with \"... the term\n'econophysics' was neologized in 1995 at the second Statphys- Kolkata\nconference in Kolkata (formerly Calcutta), India ...\". The Econophysics\nresearch therefore formally completes fifteen years of research by the end of\nthis year! The importance and proliferation of the interdisciplinary research\nof Econophysics is highlighted in the special issue of Science & Culture, which\npresents a collection of twenty nine papers (giving country wise perspectives,\nreviews of the recent developments and original research communications),\nwritten by more than forty renowned experts in physics, mathematics or\neconomics, from all over the world. We present here the list of contents and\nthe editorial. The manuscript files are available at\nhttp://fiquant.mas.ecp.fr/chakraboa for preview. This special issue will be\npublished online at http://www.scienceandculture-isna.org/journal.htm, at the\nend of October 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.3401v1"
    },
    {
        "title": "How to predict and avert economic crisis",
        "authors": [
            "Yong Tao"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Our study shows that many firms would accumulate at zero output level\n(namely, Bankruptcy status) if a perfectly competitive market reaches full\nemployment (namely, those people who should obtain employment have obtained\nemployment). As a result, appearance of economic crisis is determined by two\npoints; that is, (a). Stock market approaches perfect competition; (b). Society\nreaches full employment. The empirical research of these two points would lead\nto early warning of economic crisis. Moreover, it is a surprise that the state\nof economic crisis would be a feasible equilibrium within the framework of the\nArrow-Debreu model. That means that we can not understand the origin of\neconomic crisis within the framework of modern economics, for example, the\ngeneral equilibrium theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.5154v2"
    },
    {
        "title": "Entering New Markets-a Challenge in Times of Crisis",
        "authors": [
            "Anca Gheorghiu",
            "Anda Gheorghiu"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  After September 2008, the advanced economies severe decline caused demand for\nemerging economies' exports to drop and the crisis became truly global, much\ndeeper and broader than expected. In these times of global depression, most\ncountries and companies are affected, some more than others. The financial\ncrisis has turned out to be much deeper and broader than expected. Entering new\nmarkets has always been a hazardous entrepreneurial attempt, but also a\nrewarding one, in the case of success. The paper aims to asses the market entry\nrisk of a company trying to make a good acquisition, to buy shares of another\ncompany, activating in a foreign country. For this purpose, the case of\nElectroputere S.A., the old Romanian producer of railway equipment, has been\nchosen. The data were collected from the records of Bucharest Stock Exchange.\nAfter two years from the acquisition, one can draw a conclusion whether the\nstrategy of the investor was a good one or a waste of money.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.6050v1"
    },
    {
        "title": "A note comprising a negative resolution of the Efficient Market\n  Hypothesis",
        "authors": [
            "Robert Viragh"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This note comprises a negative resolution of the Efficient Market Hypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.0423v2"
    },
    {
        "title": "Leverage Bubble",
        "authors": [
            "Wanfeng Yan",
            "Ryan Woodard",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Leverage is strongly related to liquidity in a market and lack of liquidity\nis considered a cause and/or consequence of the recent financial crisis. A\nrepurchase agreement is a financial instrument where a security is sold\nsimultaneously with an agreement to buy it back at a later date. Repurchase\nagreements (repos) market size is a very important element in calculating the\noverall leverage in a financial market. Therefore, studying the behavior of\nrepos market size can help to understand a process that can contribute to the\nbirth of a financial crisis. We hypothesize that herding behavior among large\ninvestors led to massive over-leveraging through the use of repos, resulting in\na bubble (built up over the previous years) and subsequent crash in this market\nin early 2008. We use the Johansen-Ledoit-Sornette (JLS) model of rational\nexpectation bubbles and behavioral finance to study the dynamics of the repo\nmarket that led to the crash. The JLS model qualifies a bubble by the presence\nof characteristic patterns in the price dynamics, called log-periodic power law\n(LPPL) behavior. We show that there was significant LPPL behavior in the market\nbefore that crash and that the predicted range of times predicted by the model\nfor the end of the bubble is consistent with the observations.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.0458v2"
    },
    {
        "title": "Inferring Fundamental Value and Crash Nonlinearity from Bubble\n  Calibration",
        "authors": [
            "Wanfeng Yan",
            "Ryan Woodard",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Identifying unambiguously the presence of a bubble in an asset price remains\nan unsolved problem in standard econometric and financial economic approaches.\nA large part of the problem is that the fundamental value of an asset is, in\ngeneral, not directly observable and it is poorly constrained to calculate.\nFurther, it is not possible to distinguish between an exponentially growing\nfundamental price and an exponentially growing bubble price. We present a\nseries of new models based on the Johansen-Ledoit-Sornette (JLS) model, which\nis a flexible tool to detect bubbles and predict changes of regime in financial\nmarkets. Our new models identify the fundamental value of an asset price and\ncrash nonlinearity from a bubble calibration. In addition to forecasting the\ntime of the end of a bubble, the new models can also estimate the fundamental\nvalue and the crash nonlinearity. Besides, the crash nonlinearity obtained in\nthe new models presents a new approach to possibly identify the dynamics of a\ncrash after a bubble. We test the models using data from three historical\nbubbles ending in crashes from different markets. They are: the Hong Kong Hang\nSeng index 1997 crash, the S&P 500 index 1987 crash and the Shanghai Composite\nindex 2009 crash. All results suggest that the new models perform very well in\ndescribing bubbles, forecasting their ending times and estimating fundamental\nvalue and the crash nonlinearity. The performance of the new models is tested\nunder both the Gaussian and non-Gaussian residual assumption. Under the\nGaussian residual assumption, nested hypotheses with the Wilks statistics are\nused and the p-values suggest that models with more parameters are necessary.\nUnder non-Gaussian residual assumption, we use a bootstrap method to get type I\nand II errors of the hypotheses. All tests confirm that the generalized JLS\nmodels provide useful improvements over the standard JLS model.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.5343v1"
    },
    {
        "title": "Comprehending environmental and economic sustainability: Comparative\n  analysis of stability principles in the biosphere and free market economy",
        "authors": [
            "Victor G. Gorshkov",
            "Anastassia M. Makarieva",
            "Bai-Lian Li"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Using the formalism of Lyapunov potential function it is shown that the\nstability principles for biomass in the ecosystem and for employment in\neconomics are mathematically similar. The ecosystem is found to have a stable\nand an unstable stationary state with high (forest) and low (grasslands)\nbiomass, respectively. In economics, there is a stable stationary state with\nhigh employment, which corresponds to mass production of conventional goods\nsold at low cost price, and an unstable stationary state with lower employment,\nwhich corresponds to production of novel goods appearing in the course of\ntechnological progress. An additional stable stationary state is described for\neconomics, the one corresponding to very low employment in production of life\nessentials such as energy and raw materials. In this state the civilization\ncurrently pays 10% of global GDP for energy produced by a negligible minority\nof the working population (currently ~0.2%) and sold at prices greatly\nexceeding the cost price by 40 times. It is shown that economic ownership over\nenergy sources is equivalent to equating measurable variables of different\ndimensions (stores and fluxes), which leads to effective violation of the laws\nof energy and matter conservation.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.5978v1"
    },
    {
        "title": "Equilibrium notions and framing effects",
        "authors": [
            "Christian Hilbe"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Experimental economics has repeatedly demonstrated that the Nash equilibrium\nmakes inaccurate predictions for a vast set of games. Instead, several\nalternative theoretical concepts predict behavior that is much more in tune\nwith observed data, with the quantal response equilibrium as the most prominent\nexample. However, here we show that this equilibrium notion itself, like any\nother concept that varies smoothly with the payoffs, is necessarily subject to\nframing effects: If the same economic problem is represented in a different but\nequivalent way, the predicted results will differ. As a consequence, we argue\nthat tools and methods that are successful in explaining human behavior in\nlaboratory experiments may be unsuitable for doing theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.1188v2"
    },
    {
        "title": "Preliminaries to an investigation of reduced product set finance",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Principles of financial product synthesis from a few basic financial products\nconstitute an interesting research topic inspired by Islamic finance. We make\nan effort to answer general questions that should be answered before starting\nto investigate the main issues concerning this topic with the formalization of\nfinancial products and principles of financial product synthesis. We also\noutline the outcome of some preparatory explorations, which have been conducted\nwith the purpose to form a reasonable preliminary picture of the details of\nfinancial products that are relevant to the study of the principles of\nfinancial product synthesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.4291v1"
    },
    {
        "title": "An almost linear stochastic map related to the particle system models of\n  social sciences",
        "authors": [
            "Anindya S. Chakrabarti"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We propose a stochastic map model of economic dynamics. In the last decade,\nan array of observations in economics has been investigated in the econophysics\nliterature, a major example being the universal features of inequality in terms\nof income and wealth. Another area of inquiry is the formation of opinion in a\nsociety. The proposed model attempts to produce positively skewed distributions\nand the power law distributions as has been observed in the real data of income\nand wealth. Also, it shows a non-trivial phase transition in the opinion of a\nsociety (opinion formation). A number of physical models also generates similar\nresults. In particular, the kinetic exchange models have been especially\nsuccessful in this regard. Therefore, we compare the results obtained from\nthese two approaches and discuss a number of new features and drawbacks of this\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.3617v2"
    },
    {
        "title": "Leverage efficiency",
        "authors": [
            "Ole Peters",
            "Alexander Adamou"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Peters (2011a) defined an optimal leverage which maximizes the time-average\ngrowth rate of an investment held at constant leverage. It was hypothesized\nthat this optimal leverage is attracted to 1, such that, e.g., leveraging an\ninvestment in the market portfolio cannot yield long-term outperformance. This\nplaces a strong constraint on the stochastic properties of prices of traded\nassets, which we call \"leverage efficiency.\" Market conditions that deviate\nfrom leverage efficiency are unstable and may create leverage-driven bubbles.\nHere we expand on the hypothesis and its implications. These include a theory\nof noise that explains how systemic stability rules out smooth price changes at\nany pricing frequency; a resolution of the so-called equity premium puzzle; a\nprotocol for central bank interest rate setting to avoid leverage-driven price\ninstabilities; and a method for detecting fraudulent investment schemes by\nexploiting differences between the stochastic properties of their prices and\nthose of legitimately-traded assets. To submit the hypothesis to a rigorous\ntest we choose price data from different assets: the S&P500 index, Bitcoin,\nBerkshire Hathaway Inc., and Bernard L. Madoff Investment Securities LLC.\nAnalysis of these data supports the hypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.4548v3"
    },
    {
        "title": "Dynamics of a Service Economy Driven by Random Transactions",
        "authors": [
            "Robert W. Easton"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Agents buy and sell services. All services are of equal quality. Buyers\nchoose sellers at random. Monetary and fiscal policies are imposed by a central\nbank and a central government. Credit is supplied by a commercial banking\nsystem. Propensities to buy, sell, and lend depend on account balances,\ninterest rates, tax rates and loan default rates. Computer simulations track\nweekly sales, loans, account balances, commercial bank profits, solvency and\ncompliance with reserve requirements, and government debt.\n  The model of this economy is fully specified by a computer program. The\nprogram allows the user to explore the effects of parameter changes. Monetary\nand fiscal policies are implemented by choices of parameters such as interest\nrates and reserve requirements, and government tax and spending rates. Credit\nsupply, consumer confidence, and loan default rates strongly affect the\nbehavior of the economy in terms of sales.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.0312v1"
    },
    {
        "title": "Gaussian Noise Effects on the Evolution of Wealth in a Closed System of\n  n-Economies",
        "authors": [
            "J. M. Pellon-Diaz",
            "A. Aragones-Munoz",
            "A. Sandoval-Villalbazo",
            "A. Diaz-Reynoso"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Based on the stochastic model proposed by Patriarca-Kaski-Chakraborti that\ndescribes the exchange of wealth between $n$ economic agents, we analyze the\nevolution of the corresponding economies under the assumption of a Gaussian\nbackground, modeling the exchange parameter $\\epsilon$. We demonstrate, that\nwithin Gaussian noise, the variance of the resulting wealth distribution will\nsignificantly decrease, and the equilibrium state is reached faster than in the\ncase of a uniform distributed $\\epsilon$ parameter. Also, we show that the\nsystem with Gaussian noise strongly resembles a deterministic system which is\nsolved by means of a Z-Transform based technique.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.1713v2"
    },
    {
        "title": "The Australian Phillips curve and more",
        "authors": [
            "Ivan Kitov",
            "Oleg Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  A quantitative model is presented linking the rate of inflation and\nunemployment to the change in the level of labor force. The link between the\ninvolved variables is a linear one with all coefficients of individual and\ngeneralized models obtained empirically. To achieve the best fit between\nmeasured and predicted time series cumulative curves are used as a simplified\nversion of the 1-D boundary elements method. All models for Australia are\nsimilar to those obtained for the US, France, Japan and other developed\ncountries and thus validate the concept and related quantitative model.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.1851v1"
    },
    {
        "title": "Non - Randomness Stock Market Price Model",
        "authors": [
            "Aleksey Kharevsky"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  A new model for the stock market price analysis is proposed. It is suggested\nto look at price as an everywhere discontinuous function of time of bounded\nvariation.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3009v1"
    },
    {
        "title": "Inflation and unemployment in Switzerland: from 1970 to 2050",
        "authors": [
            "Oleg Kitov",
            "Ivan Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  An empirical model is presented linking inflation and unemployment rate to\nthe change in the level of labour force in Switzerland. The involved variables\nare found to be cointegrated and we estimate lagged linear deterministic\nrelationships using the method of cumulative curves, a simplified version of\nthe 1D Boundary Elements Method. The model yields very accurate predictions of\nthe inflation rate on a three year horizon. The results are coherent with the\nmodels estimated previously for the US, Japan, France and other developed\ncountries and provide additional validation of our quantitative framework based\nsolely on labour force. Finally, given the importance of inflation forecasts\nfor the Swiss monetary policy, we present a prediction extended into 2050 based\non official projections of the labour force level.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5405v1"
    },
    {
        "title": "Shocks in financial markets, price expectation, and damped harmonic\n  oscillators",
        "authors": [
            "Leonidas Sandoval Junior",
            "Italo De Paula Franca"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Using a modified damped harmonic oscillator model equivalent to a model of\nmarket dynamics with price expectations, we analyze the reaction of financial\nmarkets to shocks. In order to do this, we gather data from indices of a\nvariety of financial markets for the 1987 Black Monday, the Russian crisis of\n1998, the crash after September 11th (2001), and the recent downturn of markets\ndue to the subprime mortgage crisis in the USA (2008). Analyzing those data we\nwere able to establish the amount by which each market felt the shocks, a\ndampening factor which expresses the capacity of a market of absorving a shock,\nand also a frequency related with volatility after the shock. The results gauge\nthe efficiency of different markets in recovering from such shocks, and measure\nsome level of dependence between them. We also show, using the correlation\nmatrices between the indices used, that financial markets are now much more\nconnected than they were two decades ago.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.1992v2"
    },
    {
        "title": "Noise, risk premium, and bubble",
        "authors": [
            "Grzegorz Andruszkiewicz",
            "Dorje C. Brody"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The existence of the pricing kernel is shown to imply the existence of an\nambient information process that generates market filtration. This information\nprocess consists of a signal component concerning the value of the random\nvariable X that can be interpreted as the timing of future cash demand, and an\nindependent noise component. The conditional expectation of the signal, in\nparticular, determines the market risk premium vector. An addition to the\nsignal of any term that is independent of X, which generates a drift in the\nnoise, is shown to change the drifts of price processes in the physical\nmeasure, without affecting the current asset price levels. Such a drift in the\nnoise term can induce anomalous price dynamics, and can be seen to explain the\nmechanism of observed phenomena of equity premium and financial bubbles.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.3206v1"
    },
    {
        "title": "A win-win monetary policy in Canada",
        "authors": [
            "Oleg Kitov",
            "Ivan Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The Lucas critique has exposed the problem of the trade-off between changes\nin monetary policy and structural breaks in economic time series. The search\nfor and characterisation of such breaks has been a major econometric task ever\nsince. We have developed an integral technique similar to CUSUM using an\nempirical model quantitatively linking the rate of inflation and unemployment\nto the change in the level of labour force in Canada. Inherently, our model\nbelongs to the class of Phillips curve models, and the link between the\ninvolved variables is a linear one with all coefficients of individual and\ngeneralized models obtained by empirical calibration. To achieve the best LSQ\nfit between measured and predicted time series cumulative curves are used as a\nsimplified version of the 1-D boundary elements (integral) method. The distance\nbetween the cumulative curves (in L2 metrics) is very sensitive to structural\nbreaks since it accumulates true differences and suppresses uncorrelated noise\nand systematic errors. Our previous model of inflation and unemployment in\nCanada is enhanced by the introduction of structural breaks and is validated by\nnew data in the past and future. The most exiting finding is that the\nintroduction of inflation targeting as a new monetary policy in 1991 resulted\nin a structural break manifested in a lowered rate of price inflation\naccompanied by a substantial fall in the rate of unemployment. Therefore, the\nnew monetary policy in Canada is a win-win one.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5994v1"
    },
    {
        "title": "An Application Specific Informal Logic for Interest Prohibition Theory",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Interest prohibition theory concerns theoretical aspects of interest\nprohibition. We attempt to lay down some aspects of interest prohibition theory\nwrapped in a larger framework of informal logic. The reason for this is that\ninterest prohibition theory has to deal with a variety of arguments which is so\nwide that a limitation to so-called correct arguments in advance is\ncounterproductive. We suggest that an application specific informal logic must\nbe developed for dealing with the principles of interest prohibition theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.0308v1"
    },
    {
        "title": "If Entry Strategy and Money go Together, What is the Right Side of the\n  Coin?",
        "authors": [
            "Jean-Philippe Timsit",
            "Annick Castiaux"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The goal of this study is to determine which strategic model, either IO or\nRBV, allows firms to generate the highest performance on a competitive market.\nContrasting with classical studies that mobilize analyses as VARCOMP, we deploy\na multi-agent system simulating the behavior of firms adopting RBV or IO\nstrategic models. For an equivalent proportion of both strategic orientations,\nwe study the instant and total performances of the firms on hypercompetitive\nmarkets. We show that the performance of best-performing IO firms, measured by\nthe ROA, is higher in the short term, but that RBV firms obtain an average\nhigher sustained performance, in the long term. Moreover, when they are in\ncompetition with IO firms on a highly profitable and competitive market, RBV\nfirms which dare to enter such markets obtained generally the highest\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.0777v1"
    },
    {
        "title": "Non - Randomness Stock Market Price Model (Amended)",
        "authors": [
            "Aleksey Kharevsky"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  A new model for the stock market price analysis is proposed. It is suggested\nto look at price as an everywhere discontinuous function of time of bounded\nvariation.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2308v1"
    },
    {
        "title": "Interest Rates and Inflation",
        "authors": [
            "Michael Coopersmith"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  A relation between interest rates and inflation is presented using a two\ncomponent economic model and a simple general principle. Preliminary results\nindicate a remarkable similarity to classical economic theories, in particular\nthat of Wicksell.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2344v1"
    },
    {
        "title": "Interest prohibition and financial product innovation",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We give a rough sketch of the Judaic, Greek, Islamic and Christian positions\nin the matter of interest prohibition during the last few millennia and discuss\nthe way in which interest prohibition is dealt with in Islamic finance, the\nproblems with authority-based arguments for interest prohibition, and the\nprospects of interest prohibition with the advent of electronic money.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2471v1"
    },
    {
        "title": "Why Money Trickles Up - Wealth & Income Distributions",
        "authors": [
            "Geoff Willis"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This paper combines ideas from classical economics and modern finance with\nthe general Lotka-Volterra models of Levy & Solomon to provide straightforward\nexplanations of wealth and income distributions. Using a simple and realistic\neconomic formulation, the distributions of both wealth and income are fully\nexplained. Both the power tail and the log-normal like body are fully captured.\nIt is of note that the full distribution, including the power law tail, is\ncreated via the use of absolutely identical agents. It is further demonstrated\nthat a simple scheme of compulsory saving could eliminate poverty at little\ncost to the taxpayer.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.2122v2"
    },
    {
        "title": "The Bowley Ratio",
        "authors": [
            "Geoff Willis"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The paper gives a simple algebraic description, and background justification,\nfor the Bowley Ratio, the relative returns to labour and capital, in a simple\neconomy.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.2123v1"
    },
    {
        "title": "Dialectical Roots for Interest Prohibition Theory",
        "authors": [
            "Jan Aldert Bergstra"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  It is argued that arguments for strict prohibition of interests must be based\non the use of arguments from authority. This is carried out by first making a\nsurvey of so-called dialectical roots for interest prohibition and then\ndemonstrating that for at least one important positive interest bearing\nfinancial product, the savings account with interest, its prohibition cannot be\ninferred from a match with any of these root cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.2900v1"
    },
    {
        "title": "Pricing, liquidity and the control of dynamic systems in finance and\n  economics",
        "authors": [
            "Geoff Willis"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The paper discusses various practical consequences of treating economics and\nfinance as an inherently dynamic and chaotic system. On the theoretical side\nthis looks at the general applicability of the market-making pricing approach\nto economics in general. The paper also discuses the consequences of the\nendogenous creation of liquidity and the role of liquidity as a state variable.\nOn the practical side, proposals are made for reducing chaotic behaviour in\nboth housing markets and stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.5503v1"
    },
    {
        "title": "Chaos structures. Multicurrency adviser on the basis of NSW model and\n  social-financial nets",
        "authors": [
            "A. M. Avdeenko"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Algorithm of multicurrency trading at the market of Forex is realized on the\nbasis of nonlinear stochastic wavelets. The distinctive feature of the\nalgorithm is the possibility of weakly- and strongly connected horizontal\nself-assemblies, as well as use of nested structures. On-line trading with\neight currency couples has shown high effectiveness and stability of the\nalgorithm. It is discussed the problem of possibility of excess profit earning\nin electronic markets via development of social-financial nets based on\nsynchronization of work of individual traders by means of proposed algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.4502v1"
    },
    {
        "title": "Revenue diversification in emerging market banks: implications for\n  financial performance",
        "authors": [
            "Saoussen Ben Gamra",
            "Dominique Plihon"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Shaped by structural forces of change, banking in emerging markets has\nrecently experienced a decline in its traditional activities, leading banks to\ndiversify into new business strategies. This paper examines whether the\nobserved shift into non-interest based activities improves financial\nperformance. Using a sample of 714 banks across 14 East-Asian and\nLatin-American countries over the post 1997-crisis changing structure, we find\nthat diversification gains are more than offset by the cost of increased\nexposure to the non-interest income, specifically by the trading income\nvolatility. But this diversification performance's effect is found to be no\nlinear with risk, and significantly not uniform among banks and across business\nlines. An implication of these findings is that banking institutions can reap\ndiversification benefits as long as they well-studied it depending on their\nspecific characteristics, competences and risk levels, and as they choose the\nright niche.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0170v1"
    },
    {
        "title": "Role of Diversification Risk in Financial Bubbles",
        "authors": [
            "Wanfeng Yan",
            "Ryan Woodard",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We present an extension of the Johansen-Ledoit-Sornette (JLS) model to\ninclude an additional pricing factor called the \"Zipf factor\", which describes\nthe diversification risk of the stock market portfolio. Keeping all the\ndynamical characteristics of a bubble described in the JLS model, the new model\nprovides additional information about the concentration of stock gains over\ntime. This allows us to understand better the risk diversification and to\nexplain the investors' behavior during the bubble generation. We apply this new\nmodel to two famous Chinese stock bubbles, from August 2006 to October 2007\n(bubble 1) and from October 2008 to August 2009 (bubble 2). The Zipf factor is\nfound highly significant for bubble 1, corresponding to the fact that valuation\ngains were more concentrated on the large firms of the Shanghai index. It is\nlikely that the widespread acknowledgement of the 80-20 rule in the Chinese\nmedia and discussion forums led many investors to discount the risk of a lack\nof diversification, therefore enhancing the role of the Zipf factor. For bubble\n2, the Zipf factor is found marginally relevant, suggesting a larger weight of\nmarket gains on small firms. We interpret this result as the consequence of the\nresponse of the Chinese economy to the very large stimulus provided by the\nChinese government in the aftermath of the 2008 financial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0838v1"
    },
    {
        "title": "Finance Without Probabilistic Prior Assumptions",
        "authors": [
            "Frank Riedel"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We develop the fundamental theorem of asset pricing in a probability-free\ninfinite-dimensional setup. We replace the usual assumption of a prior\nprobability by a certain continuity property in the state variable.\nProbabilities enter then endogenously as full support martingale measures\n(instead of equivalent martingale measures). A variant of the\nHarrison-Kreps-Theorem on viability and no arbitrage is shown. Finally, we show\nhow to embed the superhedging problem in a classical infinite-dimensional\nlinear programming problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.1078v1"
    },
    {
        "title": "Keynesian Economics After All",
        "authors": [
            "A. Johansen",
            "I. Simonsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  It is demonstrated that the US economy has on the long-term in reality been\ngoverned by the Keynesian approach to economics independent of the current\nofficial economical policy. This is done by calculating the two-point\ncorrelation function between the fluctuations of the DJIA and the US public\ndebt. We find that the origin of this condition is mainly related to the wars\nthat the USA has fought during the time period investigated. Wars mean a large\ninflux of public money into the economy, thus as a consequence creating a\nsignificant economical upturn in the DJIA. A reason for this straight-cut\nresult of our analysis, is that very few wars have been fought on US-territory\nand those that have, were in the 18th century, when the partial destruction of\ncities, factories, railways and so on, was more limited and with less effect on\nthe over-all economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3095v1"
    },
    {
        "title": "Clarifications to Questions and Criticisms on the\n  Johansen-Ledoit-Sornette Bubble Model",
        "authors": [
            "Didier Sornette",
            "Ryan Woodard",
            "Wanfeng Yan",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The Johansen-Ledoit-Sornette (JLS) model of rational expectation bubbles with\nfinite-time singular crash hazard rates has been developed to describe the\ndynamics of financial bubbles and crashes. It has been applied successfully to\na large variety of financial bubbles in many different markets. Having been\ndeveloped for more than one decade, the JLS model has been studied, analyzed,\nused and criticized by several researchers. Much of this discussion is helpful\nfor advancing the research. However, several serious misconceptions seem to be\npresent within this collective conversation both on theoretical and empirical\naspects. Several of these problems appear to stem from the fast evolution of\nthe literature on the JLS model and related works. In the hope of removing\npossible misunderstanding and of catalyzing useful future developments, we\nsummarize these common questions and criticisms concerning the JLS model and\noffer a synthesis of the existing state-of-the-art and best-practice advices.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3171v3"
    },
    {
        "title": "Detection of Crashes and Rebounds in Major Equity Markets",
        "authors": [
            "Wanfeng Yan",
            "Reda Rebib",
            "Ryan Woodard",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Financial markets are well known for their dramatic dynamics and consequences\nthat affect much of the world's population. Consequently, much research has\naimed at understanding, identifying and forecasting crashes and rebounds in\nfinancial markets. The Johansen-Ledoit-Sornette (JLS) model provides an\noperational framework to understand and diagnose financial bubbles from\nrational expectations and was recently extended to negative bubbles and\nrebounds. Using the JLS model, we develop an alarm index based on an advanced\npattern recognition method with the aim of detecting bubbles and performing\nforecasts of market crashes and rebounds. Testing our methodology on 10 major\nglobal equity markets, we show quantitatively that our developed alarm performs\nmuch better than chance in forecasting market crashes and rebounds. We use the\nderived signal to develop elementary trading strategies that produce\nstatistically better performances than a simple buy and hold strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.0077v1"
    },
    {
        "title": "A Stable and Robust Calibration Scheme of the Log-Periodic Power Law\n  Model",
        "authors": [
            "Vladimir Filimonov",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We present a simple transformation of the formulation of the log-periodic\npower law formula of the Johansen-Ledoit-Sornette model of financial bubbles\nthat reduces it to a function of only three nonlinear parameters. The\ntransformation significantly decreases the complexity of the fitting procedure\nand improves its stability tremendously because the modified cost function is\nnow characterized by good smooth properties with in general a single minimum in\nthe case where the model is appropriate to the empirical data. We complement\nthe approach with an additional subordination procedure that slaves two of the\nnonlinear parameters to what can be considered to be the most crucial nonlinear\nparameter, the critical time $t_c$ defined as the end of the bubble and the\nmost probably time for a crash to occur. This further decreases the complexity\nof the search and provides an intuitive representation of the results of the\ncalibration. With our proposed methodology, metaheuristic searches are not\nlonger necessary and one can resort solely to rigorous controlled local search\nalgorithms, leading to dramatic increase in efficiency. Empirical tests on the\nShanghai Composite index (SSE) from January 2007 to March 2008 illustrate our\nfindings.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.0099v3"
    },
    {
        "title": "Quis pendit ipsa pretia: facebook valuation and diagnostic of a bubble\n  based on nonlinear demographic dynamics",
        "authors": [
            "Peter Cauwels",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We present a novel methodology to determine the fundamental value of firms in\nthe social-networking sector based on two ingredients: (i) revenues and profits\nare inherently linked to its user basis through a direct channel that has no\nequivalent in other sectors; (ii) the growth of the number of users can be\ncalibrated with standard logistic growth models and allows for reliable\nextrapolations of the size of the business at long time horizons. We illustrate\nthe methodology with a detailed analysis of facebook, one of the biggest of the\nsocial-media giants. There is a clear signature of a change of regime that\noccurred in 2010 on the growth of the number of users, from a pure exponential\nbehavior (a paradigm for unlimited growth) to a logistic function with\nasymptotic plateau (a paradigm for growth in competition). We consider three\ndifferent scenarios, a base case, a high growth and an extreme growth scenario.\nUsing a discount factor of 5%, a profit margin of 29% and 3.5 USD of revenues\nper user per year yields a value of facebook of 15.3 billion USD in the base\ncase scenario, 20.2 billion USD in the high growth scenario and 32.9 billion\nUSD in the extreme growth scenario. According to our methodology, this would\nimply that facebook would need to increase its profit per user before the IPO\nby a factor of 3 to 6 in the base case scenario, 2.5 to 5 in the high growth\nscenario and 1.5 to 3 in the extreme growth scenario in order to meet the\ncurrent, widespread, high expectations. To prove the wider applicability of our\nmethodology, the analysis is repeated on Groupon, the well-known\ndeal-of-the-day website which is expected to go public in November 2011. The\nresults are in line with the facebook analysis. Customer growth will plateau.\nBy not taking this fundamental property of the growth process into\nconsideration, estimates of its IPO are wildly overpriced.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.1319v2"
    },
    {
        "title": "A Modified GHG Intensity Indicator: Toward a Sustainable Global Economy\n  based on a Carbon Border Tax and Emissions Trading",
        "authors": [
            "Reza Farrahi Moghaddam",
            "Fereydoun Farrahi Moghaddam",
            "Mohamed Cheriet"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  It will be difficult to gain the agreement of all the actors on any proposal\nfor climate change management, if universality and fairness are not considered.\nIn this work, a universal measure of emissions to be applied at the\ninternational level is proposed, based on a modification of the Greenhouse Gas\nIntensity (GHG-INT) measure. It is hoped that the generality and low\nadministrative cost of this measure, which we call the Modified Greenhouse Gas\nIntensity measure (MGHG-INT), will eliminate any need to classify nations. The\ncore of the MGHG-INT is what we call the IHDI-adjusted Gross Domestic Product\n(IDHIGDP), based on the Inequality-adjusted Human Development Index (IHDI). The\nIDHIGDP makes it possible to propose universal measures, such as MGHG-INT. We\nalso propose a carbon border tax applicable at national borders, based on\nMGHG-INT and IDHIGDP. This carbon tax is supported by a proposed global\nEmissions Trading System (ETS). The proposed carbon tax is analyzed in a\nshort-term scenario, where it is shown that it can result in significant\nreduction in global emissions while keeping the economy growing at a positive\nrate. In addition to annual GHG emissions, cumulative GHG emissions over two\ndecades are considered with almost the same results.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.1567v3"
    },
    {
        "title": "Conservative self-organized extremal model for wealth distribution",
        "authors": [
            "Abhijit Chakraborty",
            "G. Mukherjee",
            "S. S. Manna"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We present a detailed numerical analysis of the modified version of a\nconservative self-organized extremal model introduced by Pianegonda et. al. for\nthe distribution of wealth of the people in a society. Here the trading process\nhas been modified by the stochastic bipartite trading rule. More specifically\nin a trade one of the agents is necessarily the one with the globally minimal\nvalue of wealth, the other one being selected randomly from the neighbors of\nthe first agent. The pair of agents then randomly re-shuffle their entire\namount of wealth without saving. This model has most of the characteristics\nsimilar to the self-organized critical Bak-Sneppen model of evolutionary\ndynamics. Numerical estimates of a number of critical exponents indicate this\nmodel is likely to belong to a new universality class different from the well\nknown models in the literature. In addition the persistence time, which is the\ntime interval between two successive updates of wealth of an agent has been\nobserved to have a non-trivial power law distribution. An opposite version of\nthe model has also been studied where the agent with maximal wealth is selected\ninstead of the one with minimal wealth, which however, exhibits similar\nbehavior as the Minimal Wealth model.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.2075v1"
    },
    {
        "title": "Integration and Contagion in US Housing Markets",
        "authors": [
            "John Cotter",
            "Stuart Gabriel",
            "Richard Roll"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This paper explores integration and contagion among US metropolitan housing\nmarkets. The analysis applies Federal Housing Finance Agency (FHFA) house price\nrepeat sales indexes from 384 metropolitan areas to estimate a multi-factor\nmodel of U.S. housing market integration. It then identifies statistical jumps\nin metropolitan house price returns as well as MSA contemporaneous and lagged\njump correlations. Finally, the paper evaluates contagion in housing markets\nvia parametric assessment of MSA house price spatial dynamics.\n  A R-squared measure reveals an upward trend in MSA housing market integration\nover the 2000s to approximately .83 in 2010. Among California MSAs, the trend\nwas especially pronounced, as average integration increased from about .55 in\n1997 to close to .95 in 2008! The 2000s bubble period similarly was\ncharacterized by elevated incidence of statistical jumps in housing returns.\nAgain, jump incidence and MSA jump correlations were especially high in\nCalifornia. Analysis of contagion among California markets indicates that house\nprice returns in San Francisco often led those of surrounding communities; in\ncontrast, southern California MSA house price returns appeared to move largely\nin lock step.\n  The high levels of housing market integration evidenced in the analysis\nsuggest limited investor opportunity to diversify away MSA-specific housing\nrisk. Further, results suggest that macro and policy shocks propagate through a\nlarge number of MSA housing markets. Research findings are relevant to all\nmarket participants, including institutional investors in MBS as well as those\nwho regulate housing, the housing GSEs, mortgage lenders, and related financial\ninstitutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.4119v1"
    },
    {
        "title": "Entrepreneurship: what's happening?",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Much has been said lately about entrepreneurship, so it seems important to\nleave here some personal analysis on this topic. The issues outlined here\nresult from a work in about a year in which because a personal and professional\nobligations it was doing some research on these issues. This is an interesting\ntopic that has not yet expired and on which there is much to research, do it is\nan area where there are many challenges.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5516v1"
    },
    {
        "title": "Entrepreneurship: some considerations",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  In this work it is presented some considerations about entrepreneurship. Most\nof these questions are linked with Portuguese context. Portugal has some\nparticularities, namely because the asymmetries between the littoral and the\ninterior. This situation carried out some problems that complicate and prevent\nthe appearance of new innovated business. In a situation of crisis like that we\nhave today this context can become a really problem to solve some questions.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5518v1"
    },
    {
        "title": "Agglomeration and Interregional Mobility of Labor in Portugal",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The aim of this paper is to analyze the relationship between inter-industry,\nintra-industry and inter-regional clustering and demand for labor by companies\nin Portugal. Is expected at the outset that there is more demand for work where\nthe agglomeration is greater. It should be noted, as a summary conclusion, the\nresults are consistent with the theoretical developments of the New Economic\nGeography, namely the demand for labor is greater where firms are better able\nto cluster that is where transport costs are lower and where there is a strong\nlinks \"backward and forward\" and strong economies of agglomeration.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5534v1"
    },
    {
        "title": "The Importance of Increasing Returns to Scale in the Process of\n  Agglomeration in Portugal: A Non-linear Empirical Analysis",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  With this work we try to analyse the agglomeration process in the Portuguese\nregions, using the New Economic Geography models. In these models the base idea\nis that where has increasing returns to scale in the manufactured industry and\nlow transport costs, there is agglomeration. Of referring, as summary\nconclusion, that with this work the existence of increasing returns to scale\nand low transport cost, in the Portuguese regions, was proven and as such the\nexistence of agglomeration in Portugal.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5538v1"
    },
    {
        "title": "The Verdoorn Law in the Portuguese Regions: A Panel Data Analysis",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This work aims to test the Verdoorn Law, with the alternative specifications\nof (1)Kaldor (1966), for five regions (NUTS II) Portuguese from 1986 to 1994\nand for the 28 NUTS III Portuguese in the period 1995 to 1999. Will, therefore,\nto analyze the existence of increasing returns to scale that characterize the\nphenomena of polarization with circular and cumulative causes and can explain\nthe processes of regional divergence. It is intended to test, even in this\nwork, the alternative interpretation of (2)Rowthorn (1975) Verdoorn's Law for\nthe same regions and periods. The results of this work will be complemented\nwith estimates of these relationships to other sectors of the economy than the\nindustry (primary and services sector), for each of the manufacturing\nindustries operating in the Portuguese regions and for the total economy of\neach region (3)(Martinho, 2011).\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5544v1"
    },
    {
        "title": "An Alternative Use of the Verdoorn Law at the Portuguese NUTs II Level",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  With this study we want to test the validity of the well known \"Verdoorn's\nLaw\" which considers the relationship between the growth of productivity and\noutput in the case of the Portuguese economy at a regional and sectoral levels\n(NUTs II) for the period 1995-1999. The importance of some additional variables\nin the original specification of Verdoorn's Law is also tested, such as, trade\nflows, capital accumulation and labour concentration. The main objective of the\nstudy is to confirm the presence of economies to scale that characterise the\npolarisation process with cumulative causation properties, explaining regional\ndivergence. By introducing new variables to the original specification of\nVerdoorn's Law we intend to examine how the economies to scale are influenced\nby the consideration of factors related to the Polarisation (Keynensian\ntradition) and Agglomeration (spatial economics tradition) phenomena. The\nresults obtained from the regression analysis based on panel estimation show\nthat the original specification of Verdoorn's Law is more robust and confirm\nthe presence of increasing economies to scale at both, regional and sectoral\nlevels. However, the additional variables related to trade flows, capital\naccumulation and labour concentration have few influence on the performance of\neconomies to scale (1)(Martinho, 2011).\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5548v1"
    },
    {
        "title": "Sectoral Convergence in Output Per Worker Between Portuguese Regions",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The aim of this paper is to present a further contribution to the analysis of\nabsolute convergence (and), associated with the neoclassical theory, and\nconditional, associated with endogenous growth theory, of the sectoral\nproductivity at regional level. Presenting some empirical evidence of absolute\nconvergence of productivity for each of the economic sectors and industries in\neach of the regions of mainland Portugal (NUTS II and NUTS III) in the period\n1986 to 1994 and from 1995 to 1999. The finest spatial unit NUTS III is only\nconsidered for each of the economic sectors in the period 1995 to 1999. They\nare also presented empirical evidence of conditional convergence of\nproductivity, but only for each of the economic sectors of the NUTS II of\nPortugal, from 1995 to 1999. The structural variables used in the analysis of\nconditional convergence is the ratio of capital/output, the flow of\ngoods/output and location ratio. The main conclusions should be noted that the\nsigns of convergence are stronger in the first period than in the second and\nthat convergence is conditional, especially in industry and in all sectors\n(1)(Martinho, 2011).\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5552v1"
    },
    {
        "title": "Spatial Effects in Convergence of Portuguese Product",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This study analyses, through cross-section estimation methods, the influence\nof spatial effects in the conditional product convergence in the parishes'\neconomies of mainland Portugal between 1991 and 2001 (the last year with data\navailable for this spatial disaggregation level). To analyse the data, Moran's\nI statistics is considered, and it is stated that product is subject to\npositive spatial autocorrelation (product develops in a similar manner to\nproduct in neighbouring regions). Taking into account the estimation results,\nit is stated that there are not indications of convergence (the population is\nin the littoral of Portugal) and it can be seen that spatial spillover effects,\nspatial lag (capturing spatial autocorrelation through a spatially lagged\ndependent variable) and spatial error (capturing spatial autocorrelation\nthrough a spatially lagged error term) condition the convergence of product of\nPortuguese parishes in the period under consideration (1)(Martinho, 2011).\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5556v1"
    },
    {
        "title": "Polarization Versus Agglomeration",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The aim of this paper is to analyze the processes of polarization and\nagglomeration, to explain the mechanisms and causes of these phenomena in order\nto identify similarities and differences. As the main implication of this study\nshould be noted that both process pretend to explain the concentration of\neconomic activity and population in certain places, through cumulative\nphenomena, but with different perspectives, in other words, the polarization\nwith a view of economic development and agglomeration with a perspective of\nspace.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5557v1"
    },
    {
        "title": "Geographic Concentration in Portugal and Regional Specific Factors",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This paper pretends to analyze the importance which the natural advantages\nand local resources are in the manufacturing industry location, in relation\nwith the \"spillovers\" effects and industrial policies. To this, we estimate the\nRybczynski equation matrix for the various manufacturing industries in\nPortugal, at regional level (NUTS II) and for the period 1980 to 1999.\nEstimations are displayed with the model mentioned and for four different\nperiods, namely 1980 to 1985, from 1986 to 1994, from 1980 to 1994 and from\n1995 to 1999. The consideration of the various periods until 1994, aims to\ncapture the effects of our entrance at the, in that time, EEC (European\nEconomic Community) and the consideration of a period from 1995 is because the\nchange in methodology for compiling statistical data taken from this time in\nPortugal. As a summary conclusion, noted that the location of manufacturing in\nPortugal is still mostly explained by specific factors, with a tendency to\nincrease in some cases the explanation by these factors, having the effect\n\"spillovers\" and industrial policies little importance in this context.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5558v1"
    },
    {
        "title": "Regional Agglomeration in Portugal: A Linear Analysis",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This work aims to study the Portuguese regional agglomeration process, using\nthe linear form the New Economic Geography models that emphasize the importance\nof spatial factors (distance, costs of transport and communication) in\nexplaining of the concentration of economic activity in certain locations. In a\ntheoretical context, it is intended to explain the complementarily of\nclustering models, associated with the New Economic Geography, and polarization\nassociated with the Keynesian tradition, describing the mechanisms by which\nthese processes are based. As a summary conclusion, we can say which the\nagglomeration process shows some signs of concentration in Lisboa e Vale do\nTejo (which is evidence of regional divergence in Portugal) and the\nproductivity factor significantly improves the results that explain the\nregional clustering in Portugal (despite being ignored in the models of New\nEconomic Geography).\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5559v1"
    },
    {
        "title": "Analysis of Net Migration Between the Portuguese Regions",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This work aims mainly to present a project of research about the\nidentification of the determinants that affect the mobility of labor. The\nempirical part of the work will be performed for the NUTS II and NUTS III of\nPortugal, from 1996 to 2002 and for 1991 and 2001, respectively (given the\navailability of statistical data). As main conclusion it can be said, for the\nNUTS II (1996-2002), which is confirmed the existence of some labor mobility in\nPortugal and that regional mobility is mainly influenced positively by the\noutput growth and negatively by the unemployment rates and by the weight of the\nagricultural sector. NUTS III level (1991 and 2001) is something similar, but\nwith this level of spatial disaggregation (and in this period) the basic\nequipment (amenities), particularly in terms of availability of housing, are\nthe main determinants of migration.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5564v1"
    },
    {
        "title": "Spatial Effects and Convergence Theory in the Portuguese Situation",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This study analyses, through cross-section estimation methods, the influence\nof spatial effects and human capital in the conditional productivity\nconvergence (product per worker) in the economic sectors of NUTs III of\nmainland Portugal between 1995 and 2002. To analyse the data, Moran's I\nstatistics is considered, and it is stated that productivity is subject to\npositive spatial autocorrelation (productivity develops in a similar manner to\nproductivity in neighbouring regions), above all, in agriculture and services.\nIndustry and the total of all sectors present indications that they are subject\nto positive spatial autocorrelation in productivity. On the other hand, it is\nstated that the indications of convergence, specifically bearing in mind the\nconcept of absolute convergence, are greater in industry. Taking into account\nthe estimation results, it is stated once again that the indications of\nconvergence are greater in industry, and it can be seen that spatial spillover\neffects, spatial lag (capturing spatial autocorrelation through a spatially\nredundant dependent variable) and spatial error (capturing spatial\nautocorrelation through a spatially redundant error term), as well as human\ncapital, condition the convergence of productivity in the various economic\nsectors of Portuguese region in the period under consideration (Martinho,\n2011).\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5571v1"
    },
    {
        "title": "Spatial Effects and Verdoorn Law in the Portuguese Context",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The consideration of spatial effects at a regional level is becoming\nincreasingly frequent and the work of Anselin (1988), among others, has\ncontributed to this. This study analyses, through cross-section estimation\nmethods, the influence of spatial effects in productivity (product per worker)\nin the NUTs III economic sectors of mainland Portugal from 1995 to 1999 and\nfrom 2000 to 2005 (taking in count the availability of data), considering the\nVerdoorn relationship. To analyse the data, by using Moran I statistics, it is\nstated that productivity is subject to a positive spatial autocorrelation\n(productivity of each of the regions develops in a similar manner to each of\nthe neighbouring regions), above all in services. The total of all sectors\npresent, also, indicators of being subject to positive autocorrelation in\nproductivity. Bearing in mind the results of estimations, it can been that the\neffects of spatial spillovers, spatial lags (measuring spatial autocorrelation\nthrough the spatially lagged dependent variable) and spatial error (measuring\nspatial autocorrelation through the spatially lagged error terms), influence\nthe Verdoorn relationship when it is applied to the economic sectors of\nPortuguese regions (Martinho, 2011).\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5573v1"
    },
    {
        "title": "Spatial Autocorrelation and Verdoorn Law in the Portuguese NUTs III",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This study analyses, through cross-section estimation methods, the influence\nof spatial effects in productivity (product per worker), at economic sectors\nlevel of the NUTs III of mainland Portugal, from 1995 to 1999 and from 2000 to\n2005 (taking in count the data availability and the Portuguese and European\ncontext), considering the Verdoorn relationship. From the analyses of the data,\nby using Moran I statistics, it is stated that productivity is subject to a\npositive spatial autocorrelation (productivity of each of the regions develops\nin a similar manner to each of the neighbouring regions), above all in\nservices. The total sectors of all regional economy present, also, indicators\nof being subject to positive autocorrelation in productivity. Bearing in mind\nthe results of estimations, it can been that the effects of spatial spillovers,\nspatial lags (measuring spatial autocorrelation through the spatially lagged\ndependent variable) and spatial error (measuring spatial autocorrelation\nthrough the spatially lagged error terms), influence the Verdoorn relationship\nwhen it is applied to the economic sectors of Portuguese regions. The results\nobtained for the two periods are different, as expected, and are better in\nsecond period, because, essentially, the European and national public supports\n(Martinho, 2011).\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5578v1"
    },
    {
        "title": "Confronting the Kaya Identity with Investment and Capital Stocks",
        "authors": [
            "Eric Kemp-Benedict"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Scaling relations, such as the IPAT equation and the Kaya identity, are\nuseful for quickly gauging the scale of economic, technological, and\ndemographic changes required to reduce environmental impacts and pressures; in\nthe case of the Kaya identity, the environmental pressure is greenhouse gas\nemissions. However, when considering large-scale economic transformation, as\nwith a shift to a low-carbon economy, the IPAT and Kaya identities and their\ncousins fail to capture the legacy of existing capital, on the one hand, and\nthe need for new investment, on the other. While detailed models can capture\nthese factors, they do not allow for rapid exploration of widely different\nalternatives, which is the appeal of the IPAT and Kaya identities. In this\npaper we present an extended Kaya identity that includes investment and capital\nstocks. The identity we propose is a sum of terms, rather than a simple scaling\nrelation. Nevertheless, it allows for quick analysis and rapid exploration of a\nvariety of different possible paths toward a low-carbon economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.0758v2"
    },
    {
        "title": "Firm dynamics in a closed, conserved economy: A model of size\n  distribution of employment and related statistics",
        "authors": [
            "Anindya S. Chakrabarti"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We address the issue of the distribution of firm size. To this end we propose\na model of firms in a closed, conserved economy populated with\nzero-intelligence agents who continuously move from one firm to another. We\nthen analyze the size distribution and related statistics obtained from the\nmodel. Our ultimate goal is to reproduce the well known statistical features\nobtained from the panel study of the firms i.e., the power law in size (in\nterms of income and/or employment), the Laplace distribution in the growth\nrates and the slowly declining standard deviation of the growth rates\nconditional on the firm size. First, we show that the model generalizes the\nusual kinetic exchange models with binary interaction to interactions between\nan arbitrary number of agents. When the number of interacting agents is in the\norder of the system itself, it is possible to decouple the model. We provide\nsome exact results on the distributions. Our model easily reproduces the power\nlaw. The fluctuations in the growth rate falls with increasing size following a\npower law (with an exponent 1 whereas the data suggests that the exponent is\naround 1/6). However, the distribution of the difference of the firm-size in\nthis model has Laplace distribution whereas the real data suggests that the\ndifference of the log sizes has the same distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2168v1"
    },
    {
        "title": "Pseudo Hermitian formulation of Black-Scholes equation",
        "authors": [
            "T. K. Jana",
            "P. Roy"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We show that the non Hermitian Black-Scholes Hamiltonian and its various\ngeneralizations are eta-pseudo Hermitian. The metric operator eta is explicitly\nconstructed for this class of Hamitonians. It is also shown that the effective\nBlack-Scholes Hamiltonian and its partner form a pseudo supersymmetric system.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.3217v1"
    },
    {
        "title": "Valuation of Zynga",
        "authors": [
            "Zalán Forró",
            "Peter Cauwels",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  On December 16, Zynga, the well-known social game developing company went\npublic. This event is following other recent IPOs in the world of social\nnetworking companies, such as Groupon, Linkedin or Pandora to cite a few. With\na valuation close to 7 billion USD at the time when it went public, Zynga has\nbecome the biggest web IPO since Google. This recent enthusiasm for social\nnetworking companies, and in particular Zynga, brings up the question whether\nor not they are overvalued. The common denominator of all these IPOs is that a\nlot of estimates about their valuation have been circulating, without any\nspecifics given about the methodology or assumptions used to obtain those\nnumbers. To bring more substance to the debate, we propose a two-tiered\napproach. First, we introduce a new model to forecast the global user base of\nZynga, based on the analysis of the individual dynamics of its major games.\nNext, we model the revenues per user using a logistic growth function, a\nstandard model for growth in competition. This leads to bracket the valuation\nof Zynga using three different scenarios (base one, optimistic and very\noptimistic): 4.17 billion USD in the base case, 5.16 billion in the high growth\nand 7.02 billion in the extreme growth scenario respectively. Thus, only the\nunlikely extreme growth scenario could potentially justify today's 6.6 billion\nUSD valuation of Zynga. This suggests that Zynga at its IPO has been\noverpriced.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.6024v1"
    },
    {
        "title": "Choix stratégiques de la firme et contrôle financier",
        "authors": [
            "Jean-Claude Juhel"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Observation of the workings of productive organizations shows that the\ncharacteristics of a trade, backed by nature given to a technological\nenvironment, determine the productive combination implemented by the decision\nmaker, and the structure of the operating cycle which is related. The choice of\nthe production function and the choice of the ring structure strain the\noperating conditions under which the firm's cash flow will evolve. New tools\nfor financial control - leverage cash and operating cash surplus - provide the\nentrepreneur the information relevant to the efficiency of the strategic\nchoices of the firm.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.1949v1"
    },
    {
        "title": "Second-order Price Dynamics: Approach to Equilibrium with Perpetual\n  Arbitrage",
        "authors": [
            "Eric Kemp-Benedict"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The notion that economies should normally be in equilibrium is by now\nwell-established; equally well-established is that economies are almost never\nprecisely in equilibrium. Using a very general formulation, we show that under\ndynamics that are second-order in time a price system can remain away from\nequilibrium with permanent and repeating opportunities for arbitrage, even when\na damping term drives the system towards equilibrium. We also argue that\nsecond-order dynamic equations emerge naturally when there are heterogeneous\neconomic actors, some behaving as active and knowledgeable arbitrageurs, and\nothers using heuristics. The essential mechanism is that active arbitrageurs\nare able to repeatedly benefit from the suboptimal heuristics that govern most\neconomic behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.5926v1"
    },
    {
        "title": "Coherent Price Systems and Uncertainty-Neutral Valuation",
        "authors": [
            "Patrick Beißner"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We consider fundamental questions of arbitrage pricing arising when the\nuncertainty model is given by a set of possible mutually singular probability\nmeasures. With a single probability model, essential equivalence between the\nabsence of arbitrage and the existence of an equivalent martingale measure is a\nfolk theorem, see Harrison and Kreps (1979). We establish a microeconomic\nfoundation of sublinear price systems and present an extension result. In this\ncontext we introduce a prior dependent notion of marketed spaces and viable\nprice systems. We associate this extension with a canonically altered concept\nof equivalent symmetric martingale measure sets, in a dynamic trading framework\nunder absence of prior depending arbitrage. We prove the existence of such sets\nwhen volatility uncertainty is modeled by a stochastic differential equation,\ndriven by Peng's G-Brownian motions.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.6632v1"
    },
    {
        "title": "Discovering East Africa's Industrial Opportunities",
        "authors": [
            "Cesar A. Hidalgo"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  What are East Africa's industrial opportunities? In this article we explore\nthis question by using the Product Space to study the productive structure of\nfive south-east African countries: Kenya, Mozambique, Rwanda, Tanzania and\nZambia. The Product Space is a network connecting products that tend to be\nexported by the same sets of countries. Since countries are more likely to\ndevelop products that are close by in the Product Space to the ones that they\nalready produce, the Product Space can be used to help anticipate a country's\nindustrial opportunities.\n  Our results suggest that the most natural avenue for future product\ndiversification for these five south-east African nations resides in the\nagricultural sector, since all of these nations appear to have productive\nstructures that are pre-adapted to the production of many agricultural products\nthat none of them are currently exporting.\n  We conclude this paper by exploring the potential benefits of further\nregional economic integration by doing an exercise in which we pull together\nthe productive structures of these five countries. This exercise shows that the\nproducts that become more accessible in the combined economy are once again\npredominantly agricultural. These results suggest that while diversification\ninto all sectors should remain an important long-term goal of the region, the\npath towards increased diversification in the near future may well lie in a\nmore empowered and diverse agricultural sector.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.0163v1"
    },
    {
        "title": "When games meet reality: is Zynga overvalued?",
        "authors": [
            "Zalán Forró",
            "Peter Cauwels",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  On December 16th, 2011, Zynga, the well-known social game developing company\nwent public. This event followed other recent IPOs in the world of social\nnetworking companies, such as Groupon or Linkedin among others. With a\nvaluation close to 7 billion USD at the time when it went public, Zynga became\none of the biggest web IPOs since Google. This recent enthusiasm for social\nnetworking companies raises the question whether they are overvalued. Indeed,\nduring the few months since its IPO, Zynga showed significant variability, its\nmarket capitalization going from 5.6 to 10.2 billion USD, hinting at a possible\nirrational behavior from the market. To bring substance to the debate, we\npropose a two-tiered approach to compute the intrinsic value of Zynga. First,\nwe introduce a new model to forecast its user base, based on the individual\ndynamics of its major games. Next, we model the revenues per user using a\nlogistic function, a standard model for growth in competition. This allows us\nto bracket the valuation of Zynga using three different scenarios: 3.4, 4.0 and\n4.8 billion USD in the base case, high growth and extreme growth scenario\nrespectively. This suggests that Zynga has been overpriced ever since its IPO.\nFinally, we propose an investment strategy (dated April 19th, 2012 on the\narXive), which is based on our diagnostic of a bubble for Zynga and how this\nherding / bubbly sentiment can be expected to play together with two important\ncoming events (the quarterly financial result announcement around April 26th,\n2012 followed by the end of a first lock-up period around April 30th, 2012). On\nthe long term, our analysis indicates that Zynga's price should decrease\nsignificantly. The paper ends with a post-mortem analysis added on May 24th,\n2012, just before going to press, showing that we have successfully predicted\nthe downward trend of Zynga. Since April 27th, 2012, Zynga dropped 25%.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.0350v3"
    },
    {
        "title": "Price and Quantity Trajectories: Second-order Dynamics",
        "authors": [
            "Eric Kemp-Benedict"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In two previous papers the author developed a second-order price adjustment\n(t\\^atonnement) process. This paper extends the approach to include both\nquantity and price adjustments. We demonstrate three results: a analogue to\nphysical energy, called \"activity\" arises naturally in the model, and is not\nconserved in general; price and quantity trajectories must either end at a\nlocal minimum of a scalar potential or circulate endlessly; and disturbances\ninto a subspace of substitutable commodities decay over time. From this we\nargue, although we do not prove, that the model features global stability,\ncombined with local instability, a characteristic of many real markets.\nFollowing these observations and a brief survey of empirical results for\nprice-setting and consumption behavior in markets for \"real\" goods (as opposed\nto financial markets), we conjecture that Stigler and Becker's well-known\ntheory of consumer preference opens the possibility of substantial degeneracy\nin commodity space, and therefore that price and quantity trajectories could\nlie on a relatively low-dimensional subspace within the full commodity space.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.3156v1"
    },
    {
        "title": "Value matters: Predictability of Stock Index Returns",
        "authors": [
            "Natascia Angelini",
            "Giacomo Bormetti",
            "Stefano Marmi",
            "Franco Nardini"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We present a simple dynamical model of stock index returns which is grounded\non the ability of the Cyclically Adjusted Price Earning (CAPE) valuation ratio\ndevised by Robert Shiller to predict long-horizon performances of the market.\nMore precisely, we discuss a discrete time dynamics in which the return growth\ndepends on three components: i) a momentum component, naturally justified in\nterms of agents' belief that expected returns are higher in bullish markets\nthan in bearish ones, ii) a fundamental component proportional to the\nlogarithmic CAPE at time zero. The initial value of the ratio determines the\nreference growth level, from which the actual stock price may deviate as an\neffect of random external disturbances, and iii) a driving component which\nensures the diffusive behaviour of stock prices. Under these assumptions, we\nprove that for a sufficiently large horizon the expected rate of return and the\nexpected gross return are linear in the initial logarithmic CAPE, and their\nvariance goes to zero with a rate of convergence consistent with the diffusive\nbehaviour. Eventually this means that the momentum component may generate\nbubbles and crashes in the short and medium run, nevertheless the valuation\nratio remains a good reference point of future long-run returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.5055v2"
    },
    {
        "title": "ConocoPhillips' share price model revisited",
        "authors": [
            "Ivan Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Three years ago we found a statistically reliable link between\nConocoPhillips' (NYSE: COP) stock price and the difference between the core and\nheadline CPI in the United States. In this article, the original relationship\nis revisited with new data available since 2009. The agreement between the\nobserved monthly closing price (adjusted for dividends and splits) and that\npredicted from the CPI difference is confirmed. The original quantitative link\nis validated. In order to improve the accuracy of the COP price prediction a\nseries of advanced models is developed. The original set of two major CPIs is\nextended by smaller components of the headline CPIs (e.g. the CPIs of motor\nfuel and housing energy) and several PPIs (e.g. the PPIs of crude oil and coal)\nwhich may be inherently related to ConocoPhillips and other energy companies.\nThese advanced models have demonstrated much lower modeling errors with better\nstatistical properties. The earlier reported quasi-linear trend in the CPI\ndifference is also revisited. This trend allows for an accurate prediction of\nthe COP prices at a five to ten year horizon.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.5171v1"
    },
    {
        "title": "The monetary growth order",
        "authors": [
            "Günter von Kiedrowski",
            "Eörs Szathmáry"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Growth of monetary assets and debts is commonly described by the formula of\ncompound interest which for the case of continuous compounding is the\nexponential growth law. Its differential form is dc/dt = i c where dc/dt\ndescribes the rate of monetary growth, i the compounded interest rate and c the\nactual principal. Exponential growth of this type is fixed to be neither\nresource-limited nor self-limiting which is in contrast to real economic growth\n(such as the GDP) which may have exponential, but also subexponential, linear,\nsaturation, and even decline phases. As a result assets and debts commonly\noutgrow their economic fundament giving rise to the financial equivalent of\nMalthusian catastrophes after a certain interval of time. We here introduce an\nalternative for exponential compounding and propose to replace dc/dt = i c by\ndc/dt = i c^p where the exponent p (called reaction order in chemistry) is a\nquantity which will be termed monetary growth order. The monetary growth order\np is seen as a tuning handle which enables to adjust gross monetary growth to\nreal economic growth. It is suggested that the central banks take a serious\nlook to this control instrument which allows tuning in crisis situations and\nimmediate return to the exponential norm if needed.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.6590v1"
    },
    {
        "title": "Impact of the economic crisis on the Italian public healthcare\n  expenditure",
        "authors": [
            "Carlo Castellana"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The global financial crisis, beginning in 2008, took an historic toll on\nnational economies around the world. Following equity market crashes,\nunemployment rates rose significantly in many countries: Italy was among those.\nWhat will be the impact of such large shocks on Italian healthcare finances? An\nempirical model for estimating the impact of the crisis on Italian public\nhealthcare expenditure is presented. Based on data from epidemiological studies\nrelated to past economic crisis, the financial impact is estimated to be\ncomparable to the healthcare deficit of Italian Regions (EUR 3-5 bn). According\nto current agreements between the Italian State and its Regions, public funding\nof regional National Health Services (NHSs) is limited to the amount of\nregional deficit and is subject to previous assessment of strict adherence to\nconstraint on regional healthcare balance-sheet. Those Regions that will fail\nto comply to balance-sheet constraints will suffer cuts on their public NHS\nfinancing with foreseeable bad consequences for the health of their regional\npopulation. The current crisis could be a good timing for a large-scale\nre-engineering of the Italian NHS, probably the only way for\nself-sustainability of the public system.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2863v1"
    },
    {
        "title": "Global Green Economy and Environmental Sustainability: a Coopetitive\n  Model",
        "authors": [
            "David Carfì",
            "Daniele Schilirò"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  This paper provides a coopetitive model for a global green economy, taking\ninto account the environmental sustainability. In particular, we propose a\ndifferentiable coopetitive game G (in the sense recently introduced by D.\nCarf`{\\i}) to represent a global green economy interaction, among a country c\nand the rest of the world w. Our game G is a linear parametric (Euclidean)\nperturbation of the classic Cournot duopoly. In the paper we offer the complete\nstudy of the proposed model and in particular a deep examination of its\npossible coopetitive solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2872v1"
    },
    {
        "title": "Asymmetric R&D Alliances and Coopetitive Games",
        "authors": [
            "Daniela Baglieri",
            "David Carfì",
            "Giovanni Battista Dagnino"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In this paper we show how the study of asymmetric R&D alliances, that are\nthose between young and small firms and large and MNEs firms for knowledge\nexploration and/or exploitation, requires the adoption of a coopetitive\nframework which consider both collaboration and competition. We draw upon the\nliterature on asymmetric R&D collaboration and coopetition to propose a\nmathematical model for the coopetitive games which is particularly suitable for\nexploring asymmetric R&D alliances.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2878v1"
    },
    {
        "title": "Restructuring the Italian NHS: a case study of the regional hospital\n  network",
        "authors": [
            "Carlo Castellana"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  One of the main issues affecting the Italian NHS is the healthcare deficit:\naccording to current agreements between the Italian State and its Regions,\npublic funding of regional NHS is now limited to the amount of regional deficit\nand is subject to previous assessment of strict adherence to constraint on\nregional healthcare balance sheet. Many Regions with previously uncontrolled\nhealthcare deficit have now to plan their \"Piano di Rientro\" (PdR) and submit\nit for the approval of the Italian Ministry of Economy and Finances. Those\nRegions that will fail to comply to deficit constraints will suffer cuts on\ntheir public NHS financing. A smart Health Planning can make sure health\nspending is managed appropriately. Indeed a restructuring of the Italian\nhealthcare system has recently been enforced in order to cope for the clumsy\nregional healthcare balance sheets. Half of total Italian healthcare\nexpenditure is accounted by hospital services which therefore configure as one\nof the main restructuring targets. This paper provides a general framework for\nplanning a re-engineering of a hospital network. This framework is made of\neconomic, legal and healthcare constraints. We apply the general framework to\nthe particular case of Puglia region and explore a set of re-engineered\nsolutions which to different extent could help solve the difficult dilemma:\ncutting costs without worsening the delivery of public healthcare services.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.3519v1"
    },
    {
        "title": "Behavioural breaks in the heterogeneous agent model: the impact of\n  herding, overconfidence, and market sentiment",
        "authors": [
            "Jiri Kukacka",
            "Jozef Barunik"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The main aim of this work is to incorporate selected findings from\nbehavioural finance into a Heterogeneous Agent Model using the Brock and Hommes\n(1998) framework. Behavioural patterns are injected into an asset pricing\nframework through the so-called `Break Point Date', which allows us to examine\ntheir direct impact. In particular, we analyse the dynamics of the model around\nthe behavioural break. Price behaviour of 30 Dow Jones Industrial Average\nconstituents covering five particularly turbulent U.S. stock market periods\nreveals interesting pattern in this aspect. To replicate it, we apply numerical\nanalysis using the Heterogeneous Agent Model extended with the selected\nfindings from behavioural finance: herding, overconfidence, and market\nsentiment. We show that these behavioural breaks can be well modelled via the\nHeterogeneous Agent Model framework and they extend the original model\nconsiderably. Various modifications lead to significantly different results and\nmodel with behavioural breaks is also able to partially replicate price\nbehaviour found in the data during turbulent stock market periods.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.3763v2"
    },
    {
        "title": "Real GDP per capita since 1870",
        "authors": [
            "Ivan Kitov",
            "Oleg Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The growth rate of real GDP per capita in the biggest OECD countries is\nrepresented as a sum of two components - a steadily decreasing trend and\nfluctuations related to the change in some specific age population. The long\nterm trend in the growth rate is modelled by an inverse function of real GDP\nper capita with a constant numerator. This numerator is equivalent to a\nconstant annual increment of real GDP per capita. For the most advanced\neconomies, the GDP estimates between 1950 and 2007 have shown very weak and\nstatistically insignificant linear trends (both positive and negative) in the\nannual increment. The fluctuations around relevant mean increments are\ncharacterized by practically normal distribution. For many countries, there\nexist historical estimates of real GDP since 1870. These estimates extend the\ntime span of our analysis together with a few new estimates from 2008 to 2011.\nThere are severe structural breaks in the corresponding time series between\n1940 and 1950, with the slope of linear regression increasing by a factor of\n4.0 (Switzerland) to 22.1 (Spain). Therefore, the GDP estimates before 1940 and\nafter 1950 have been analysed separately. All findings of the original study\nare validated by the newly available data. The most important is that all\nslopes (except that for Australia after 1950) of the regression lines obtained\nfor the annual increments of real GDP per capita are small and statistically\ninsignificant, i.e. one cannot reject the null hypothesis of a zero slope and\nthus constant increment. Hence the growth in real GDP per capita is a linear\none since 1870 with a break in slope between 1940 and 1950.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.5671v1"
    },
    {
        "title": "The Effect of Market Power on Risk-Sharing",
        "authors": [
            "Michail Anthropelos"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The paper studies an oligopolistic equilibrium model of financial agents who\naim to share their random endowments. The risk-sharing securities and their\nprices are endogenously determined as the outcome of a strategic game played\namong all the participating agents. In the complete-market setting, each\nagent's set of strategic choices consists of the security payoffs and the\npricing kernel that are consistent with the optimal-sharing rules; while in the\nincomplete setting, agents respond via demand functions on a vector of given\ntradeable securities. It is shown that at the (Nash) risk-sharing equilibrium,\nthe sharing securities are suboptimal, since agents submit for sharing\ndifferent risk exposures than their true endowments. On the other hand, the\nNash equilibrium prices stay unaffected by the game only in the special case of\nagents with the same risk aversion. In addition, agents with sufficiently lower\nrisk aversion act as predatory traders, since they absorb utility surplus from\nthe high risk averse agents and reduce the efficiency of sharing. The main\nresults of the paper also hold under the generalized models that allow the\npresence of noise traders and heterogeneity in agents' beliefs.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.0384v3"
    },
    {
        "title": "Why price inflation in developed countries is systematically\n  underestimated",
        "authors": [
            "Ivan Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  There is an extensive historical dataset on real GDP per capita prepared by\nAngus Maddison. This dataset covers the period since 1870 with continuous\nannual estimates in developed countries. All time series for individual\neconomies have a clear structural break between 1940 and 1950. The behavior\nbefore 1940 and after 1950 can be accurately (R2 from 0.7 to 0.99) approximated\nby linear time trends. The corresponding slopes of regressions lines before and\nafter the break differ by a factor of 4 (Switzerland) to 19 (Spain). We have\nextrapolated the early trends into the second interval and obtained much lower\nestimates of real GDP per capita in 2011: from 2.4 (Switzerland) to 5.0 (Japan)\ntimes smaller than the current levels. When the current linear trends are\nextrapolated into the past, they intercept the zero line between 1908\n(Switzerland) and 1944 (Japan). There is likely an internal conflict between\nthe estimating procedures before 1940 and after 1950. A reasonable explanation\nof the discrepancy is that the GDP deflator in developed countries has been\nhighly underestimated since 1950. In the USA, the GDP deflator is\nunderestimated by a factor of 1.4. This is exactly the ratio of the interest\nrate controlled by the Federal Reserve and the rate of inflation. Hence, the\nFederal Reserve actually retains its interest rate at the level of true price\ninflation when corrected for the bias in the GDP deflator.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.0450v1"
    },
    {
        "title": "Shaping the international financial system in century of globalization",
        "authors": [
            "Viktor O. Ledenyov",
            "Dimitri O. Ledenyov"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We educe a perspective on how best to regulate the bank of tomorrow in frames\nof debate launched by the International Centre for Financial Regulation and\nFinancial Times. Our goal is to create a conceptual framework for policymakers\nand regulators to shape the international financial system in century of\nglobalization using the 1888 FT's motto: \"Without fear and without favour.\" Our\nprospect employs an analytical approach, which focuses on the origins and\nevolution of banking system, its transformation over the recent decades,\nsubsequent encountering the limits to growth and redefinition of new strategic\nboundaries of emerging financial industry. We identify the main reasons and\nlimitations, which led to the global financial crisis. We propose the new\nresearch agendas with the aim to understand the situation in finances, evaluate\nthe created systemic damages, and find the possible ways to resolve the\nexisting problems through introduction of new banking regulation. We think that\nthe global economic and financial systems are highly nonlinear systems. In our\nopinion, the frequency, phase and amplitude modulation during the mixing of\nwaves, which characterize the Kitchin, Juglar, Kuznets, Kondratiev economic\ncycles, may result in origination of strong nonlinear dynamics in financial\nsystem, accompanied by chaos- induced phenomena. These nonlinear effects have\nto be taken to the account, when adding the liquidity to the financial system\nin small quantas in series over time period during the Quantitative Easing\npolicy execution by central banks. We propose the Random Tax to be selectively\nimposed on the profits, obtained by market agents during high-risk high-profit\nspeculative transactions. We expect that the Random Tax will stabilize the\nfinancial system in conditions of free market capitalism.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2022v1"
    },
    {
        "title": "Designing the new architecture of international financial system in era\n  of great changes by globalization",
        "authors": [
            "Viktor O. Ledenyov",
            "Dimitri O. Ledenyov"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We present a broad agenda for meaningful banking regulation reform aiming the\ncreation of evolutive competitive environment to maximize the effectiveness of\ninternational financial system through the introduction of fair competition\nprocess among the banks in free market capitalism. We assume that the\ninternational financial system may evolve or decline within the evolutive\ncompetitive environment depending on both the environmental regulation policies\nas well as the competition between the banks. We present the commonly known\ndefinition of competition and apply the conceptual collateral thinking to\nidentify the source of competitive strengths of financial institutions in free\nmarket capitalism. We explore how the banks conduct a search for competitive\nstrategies. We show that the root cause of crisis in finances is hidden in the\nvery wrong regulation policies and ideas behind these policies, which failed to\ncreate the evolutive competitive environment for effective, profitable,\nresponsible and sustainable bank operation within existing international\nfinancial system. We review a number of initiatives on meaningful banking\nregulation reform proposed by central bankers from the G20 nations. We propose\nto introduce the Random Tax and the Quantum Tax. We argue that the introduction\nof the Random Tax and the Quantum Tax may compensate for the negative effects\ncommonly associated with the existing banking regulation limitations imposed on\nthe international financial system. We believe that the Random Tax and the\nQuantum Tax will improve the evolutive competitive environment and make it\npossible for the management teams at financial institutions to search for and\nto execute the winning virtuous business strategies toward the effective,\nprofitable, responsible and sustainable banks operation.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2778v1"
    },
    {
        "title": "International trade of flowers. Tendencies and policies",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  There are few papers about the international trade of flowers, so it is\nbelieved that this paper, with this topic, could be an important contribution\nto the international scientific community. It is intended to analyze if the\ninternational trade flowers tendencies and policies are adapted to the actual\nworld global context. For that it was used data about the import and export of\nflowers, in different forms, between Portugal and the world. This is an\napproach to understand the international trade flowers tendencies and policies.\nTo better understand the data analyzed it is made several estimations based in\nthe absolute convergence theory and an analyze of the data volatility. As main\nconclusions, there is a tendency to the countries trade the flowers between the\nneighbors and is needed a more coherent policy for the international trade of\nflowers.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.3384v1"
    },
    {
        "title": "International trade of fruits between Portugal and the world",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  For Portugal there are few or none works about the international trade of\nfruits between Portugal and the other countries. In this work it aims to\nanalyze the more recent data for the Portuguese international trade of fruits.\nThey were used data for the years from 2006 to 2010, available by the INE\n(Statistics Portugal), gently given by the AICEP (Trade & Investment Agency).\nTo complement this data analysis they were made some estimations with several\neconometrics method and based in the neoclassical theory, with the absolute\nconvergence model. It was concluded that the biggest relationship, in the\ninternational trade of fruits, is with the European countries and there are not\nstatistical regularity in the estimations and the data are not stationary.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.3385v1"
    },
    {
        "title": "Import and export of horticultural products in Portugal",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  With this work it is analyzed the import and export of horticultural products\nbetween Portugal and the other world countries. It is used data about\nPortuguese international trade of vegetables from 2006 to 2010. The data were\nobtained from the INE (Statistics Portugal), gently given by the AICEP (Trade &\nInvestment Agency). It is did some estimations taking into account the models\nfrom the convergence theory, with panel data and using methods by fixed\neffects, random effects and dynamic effects, for the Portuguese import and\nexport of vegetables, separately. It is found convergence in all estimations.\nThe volatility was also tested. All the tests show no stationary of the data.\nSo, in statically means the data show weak regularity. In this way all the\nconclusion, must be did very carefully. This lack of regularity is a result of\nlack of a national coherent policy for the sector.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.3387v1"
    },
    {
        "title": "The Keynesian theory and the manufactured industry in Portugal",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  About the economic growth the Keynesian theorists defend circular and\ncumulative processes, benefiting the rich localities and harming the poorest,\nwithout external interventions. In these processes the Verdoorn law has an\nimportant role. For Verdoorn (1949) the productivity growth rate is endogenous\nand depends of the output growth rate, capturing dynamic contexts, endogeneity\nof the factors and increasing economies of scale, namely in the industry. This\nrelationship later becomes the second law of Kaldor (1966 and 1967). For\nPortugal there are few works or none, than those of the author, with the\nVerdoorn law. In this way, seem important analyze this relationship for the\nmanufactured industry of the Portuguese regions and conclude about these\ncontexts in Portugal. It was used data from two periods, 1986-1994 and\n1995-1999, and panel data econometric methods. The two periods is to capture\nthe effect of the Portuguese entrance in the European Economic Community and of\nthe first Community Support Framework (1989-1993) for Portugal. As main\nconclusion, for the two periods, it is verified strong increasing returns in\nthe manufactured industry and as consequence regional divergence of this\nsector.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.1771v1"
    },
    {
        "title": "Existence of Financial Equilibria in Continuous Time with Potentially\n  Complete Markets",
        "authors": [
            "Frederik Herzberg",
            "Frank Riedel"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We prove that in smooth Markovian continuous-time economies with potentially\ncomplete asset markets, Radner equilibria with endogenously complete markets\nexist.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.2010v1"
    },
    {
        "title": "Exploiting the flexibility of a family of models for taxation and\n  redistribution",
        "authors": [
            "Maria Letizia Bertotti",
            "Giovanni Modanese"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We discuss a family of models expressed by nonlinear differential equation\nsystems describing closed market societies in the presence of taxation and\nredistribution. We focus in particular on three example models obtained in\ncorrespondence to different parameter choices. We analyse the influence of the\nvarious choices on the long time shape of the income distribution. Several\nsimulations suggest that behavioral heterogeneity among the individuals plays a\ndefinite role in the formation of fat tails of the asymptotic stationary\ndistributions. This is in agreement with results found with different\napproaches and techniques. We also show that an excellent fit for the\ncomputational outputs of our models is provided by the k-generalized\ndistribution introduced by G. Kaniadakis (Physica A 296 (2001) 405-425).\n",
        "pdf_link": "http://arxiv.org/pdf/1207.6081v2"
    },
    {
        "title": "Entangled Economy: an ecosystems approach to modeling systemic level\n  dynamics",
        "authors": [
            "Juan David Robalino",
            "Henrik Jeldtoft Jensen"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We present a model of an economy inspired by individual based model\napproaches in evolutionary ecology. We demonstrate that evolutionary dynamics\nin a space of companies interconnected through a correlated interaction matrix\nproduces time dependencies of the total size of the economy total number of\ncompanies, companies age and capital distribution that compares well with\nstatistics for USA. We discuss the relevance of our modeling framework to\npolicy making.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.6091v1"
    },
    {
        "title": "The financial framework of the sustainability of health universal\n  coverage in Italy. A quantitative financial model for the assessment of the\n  italian stability and reform program of public health financing",
        "authors": [
            "Stefano Olgiati",
            "Alessandro Danovi"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Italy and the Eurozone are heading in the year 2012 into a financial\ndepression of unprecedented magnitude, with a forthcoming multitude of often\ncontradictory public economic and financial stability emergency interventions\nwhose ultimate endogenous and exogenous effects on public and private health\nspending and on the sustainability of universal coverage are difficult to\npredict ex ante. The research question is to assess whether it is possible to\nsynthesise into a single and simple quantitative index such multitude of public\neconomic and financial stability interventions and assess their magnitude and\ndirection towards increasing or decreasing sustainability of publicly funded\nhealth care and universal coverage. We have analyzed the Italian Economic and\nStability Reform Program 2011-2014 and we have proposed a quantitative\nsynthetic sustainability index {\\sigma} based on simple partial and absolute\ndifferential equations. The sustainability index {\\sigma} highlights that in\ncase the growth of the GDP in the period 2011-2014 be insufficient - as is\nalready the case in the first semester of 2012 - all the assumptions on which\nthe Italian Economic and Stability Reform Program 2011-2014 rests will fall,\nand Universal Coverage will become unsustainable. Health and Public Health\nprofessionals should intervene immediately with Italian and Eurozone national\nbudgets planners and financial health regulators before unselective exogenously\ninduced health financing and provision shortages produce irreparable\nepidemiological effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.6278v1"
    },
    {
        "title": "Does GDP measure growth in the economy or simply growth in the money\n  supply?",
        "authors": [
            "Jacky Mallett",
            "Charles Keen"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Gross Domestic Product(GDP) is a widely used measurement of economic growth\nrepresenting the market value of all final goods and services produced by a\ncountry within a given time. In this paper we question the assumption that GDP\nmeasures production, and suggest that in reality it merely captures changes in\nthe rate of expansion of the money supply used to measure the price data it is\nderived from. We first review the Quantity Theory of Money $MV=PT$, and show\nthat the Velocity of Circulation of Money(V) does not affect the price level as\nclaimed, as it is also a factor of the quantity of transactions(T). It then\nfollows directly that attempts to measure total production from any form of\nprice data as the GDP measurement does, will necessarily be confounded by the\ninverse relationship between prices and the quantity of production, which\nrequires that as the total quantity of production increases, prices will drop.\nFinally, in support of this claim we present an empirical analysis of the GDP\nof nine countries and one currency union, showing that when normalized for\nmoney supply growth GDP measures have been uniformly shrinking over the last 20\nyears, and discuss the possible reasons for this behaviour.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.0642v1"
    },
    {
        "title": "General Balance Functions in the Theory of Interest",
        "authors": [
            "David Spring"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We develop an axiomatic theory of balance functions (future value functions)\nin the theory of interest that is derived from financial considerations and\nwhich applies to general regulated payment streams, including continuous\npayment streams. Balance functions exist and are unique up to an initial choice\nof deposit and investment accumulation functions. In terms of these balance\nfunctions we also construct a unique internal rate of return for each regulated\npayment stream that is an investment project. This theory subsumes and\nclarifies previous theories of internal rate of return functions for more\nspecialized classes of investment projects.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.1479v1"
    },
    {
        "title": "Spontaneous Economic Order",
        "authors": [
            "Yong Tao"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  This paper provides an attempt to formalize Hayek's notion of spontaneous\norder within the framework of the Arrow-Debreu economy. Our study shows that if\na competitive economy is enough fair and free, then a spontaneous economic\norder shall emerge in long-run competitive equilibria so that social members\ntogether occupy an optimal distribution of income. Despite this, the\nspontaneous order might degenerate in the form of economic crises whenever an\nequilibrium economy approaches the extreme competition. Remarkably, such a\ntheoretical framework of spontaneous order provides a bridge linking Austrian\neconomics and Neoclassical economics, where we shall comprehend a truth:\n\"Freedom promotes technological progress\".\n",
        "pdf_link": "http://arxiv.org/pdf/1210.0898v2"
    },
    {
        "title": "Modèles de coûts en fonderie sable : les limites d'une approche\n  générique",
        "authors": [
            "Nicolas Perry",
            "Magali Mauchand",
            "Alain Bernard"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The control of the costs, as soon as possible of the product life cycle,\nbecame a major asset in the competitiveness of the companies confronted with\nthe universalization of competition. After having proposed the problems related\nthis control difficulties, we will present an approach defining a concept of\ncost entity related to the activities of the product to be designed and\nrealized. We will then try to apply this approach to the fields of the sand\ncasting. This work will highlight the hierarchisation difficulties with the\nentities composing the models created as well as the limits of a generic\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.2088v1"
    },
    {
        "title": "Physical assets replacement: an analytical approach",
        "authors": [
            "Igor Gimenes Cesca",
            "Douglas Duarte Novaes"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The economic life of an asset is the optimum length of its usefulness, which\nis the moment that the asset's expenses are minimum. In this paper, the\neconomic life of physical assets, such as industry machine and equipment, can\nbe interpreted as the moment that the minimum is reached by its equivalent\nproperty cost function, defined as the sum of all equivalent capital and\nmaintenance costs during its life.\n  Many authors in classical papers have used principles of engineering economic\nto solve the assets replacement problem. However, in the literature, the main\nattributes found were proved with intuitive ideas instead mathematical\nanalysis. Therefore, in this paper the main goal is to study these principles\nof engineering economic with mathematical techniques.\n  Here, is used non-smooth analysis to classify all the possibilities for the\nminimum of a class of equivalent property cost functions of assets. The minimum\nof these function gives the optimum moment for the asset to be replaced, i.e.,\nits economic life.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.3678v1"
    },
    {
        "title": "A case for FDI in Multi-brand retail in India",
        "authors": [
            "Jatin Prasad",
            "Dr Jyoti Singh"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  India is ranked as the third most attractive nation for retail investment\namong emerging markets and many MNCs have been looking for the potential\nbenefits to be taken from it. The development of organized retail has the\npotential of generating employment, improvement in technology, development of\nreal estate etc. On the other hand critics of the FDI feel that allowing FDI\nwould jeopardize the unorganized retail sector and would not only adversely\naffect the small retailers and consumers but will give rise to monopolies of\nlarge corporate houses also, which can adversely affect the pricing and\navailability of goods. A case for the prospects for the same is discussed in\nthis paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.6201v1"
    },
    {
        "title": "Will Central Counterparties become the New Rating Agencies?",
        "authors": [
            "Chris Kenyon",
            "Andrew Green"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Central Counterparties (CCPs) are widely promoted as a requirement for safe\nbanking with little dissent except on technical grounds (such as proliferation\nof CCPs). Whilst CCPs can have major operational positives, we argue that CCPs\nhave many of the business characteristics of Rating Agencies, and face similar\nbusiness pressures. Thus we see a risk that prices from CCPs may develop the\ncharacteristics attributed to ratings from Rating Agency pre-crisis. Business\nover-reliance on ratings of questionable accuracy is seen as a cause of the\nfinancial crisis. We see the potential for same situation to be repeated with\nprices from CCPs. Thus the regulatory emphasis on CCPs, rather than on\ncollateralization, may create the preconditions for an avoidable repeat of the\nfinancial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.6349v2"
    },
    {
        "title": "Momentum universe shrinkage effect in price momentum",
        "authors": [
            "Jaehyung Choi",
            "Sungsoo Choi",
            "Wonseok Kang"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We test the price momentum effect in the Korean stock markets under the\nmomentum universe shrinkage to subuniverses of the KOSPI 200. Performance of\nthe momentum strategy is not homogeneous with respect to change of the momentum\nuniverse. It is found that some submarkets generate the higher momentum returns\nthan other universes do but large-size companies such as the KOSPI 50\ncomponents hinder the performance of the momentum strategy. The observation is\nalso cross-checked with size portfolios and liquidity portfolios. Transactions\nby investor groups, in particular, the trading patterns by foreign investors\ncan be a source of the momentum universe shrinkage effect in the momentum\nreturns.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.6517v1"
    },
    {
        "title": "A note on estimating stochastic volatility and its volatility: a new\n  simple method",
        "authors": [
            "Moawia Alghalith"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We present a new simple method of estimating stochastic volatility and its\nvolatility. This method is applicable to both cross-sectional and time-series\ndata. Moreover, this method does not require volatility data series.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.0380v1"
    },
    {
        "title": "Cross comparison and modelling of Goldman Sachs, Morgan Stanley,\n  JPMorgan Chase, Bank of America, and Franklin Resources",
        "authors": [
            "Ivan Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We have studied statistical characteristics of five share price time series.\nFor each stock price, we estimated a best fit quantitative model for the\nmonthly closing price as based on the decomposition into two defining consumer\nprice indices selected from a large set of CPIs. It was found that there are\ntwo pairs of similar models (Bank of America/Morgan Stanley and Goldman\nSachs/JPMorgan Chase) with a standalone model for Franklin Resources. From each\npair, one can choose the company with the highest return depending on the\nfuture evolution of defining CPIs\n",
        "pdf_link": "http://arxiv.org/pdf/1212.1661v1"
    },
    {
        "title": "La structure du capital et la profitabilité: Le cas des entreprises\n  industrielles françaises",
        "authors": [
            "Mazen Kebewar"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The objective of this article is to analyze the impact of capital structure\non profitability. This impact can be explained by three essential theories:\nsignaling theory, tax theory and the agency costs theory. A sample of 1846\nFrench industrial firms are taken over the period 1999-2006, as a dynamic panel\nstudy by using the generalized method of moments (GMM). We show that capital\nstructure has no influence on the profitability of French firms, regardless the\nsize of the company.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6795v1"
    },
    {
        "title": "The effect of debt on corporate profitability : Evidence from French\n  service sector",
        "authors": [
            "Mazen Kebewar"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Current study aims to provide new empirical evidence on the impact of debt on\ncorporate profitability. This impact can be explained by three essential\ntheories: signaling theory, tax theory and the agency cost theory. Using panel\ndata sample of 2240 French non listed companies of service sector during\n1999-2006. By utilizing generalized method of moments (GMM) econometric\ntechnique on three measures of profitability ratio (PROF1, PROF2 and ROA), we\nshow that debt ratio has no effect on corporate profitability, regardless of\nthe size of company (VSEs, SMEs or LEs)\n",
        "pdf_link": "http://arxiv.org/pdf/1301.0072v2"
    },
    {
        "title": "Consumers behavior of Portuguese wine",
        "authors": [
            "Vitor Joao Pereira Domingues Martinho"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  There are few papers about the consumption pattern of the Portuguese wine,\nusing econometrics techniques. This work, pretend to analyze the consumers\nbehavior of the wine produced in Portugal, determining the demand equation with\npanel data methods. There were used statistical data available in the Alentejo\nRegional Winegrowing Commission (CVRA) website. These data were obtained from a\nstudy about the market analysis, made, in 2009, by the A.C. Nielsen. The data\nare disaggregated by region and type of wine (Doc Verde+Regional Minho, Doc\nRegiao do Douro+Regional Terras Durienses, Doc Regiao da Bairrada+Regional\nBeiras, Doc Regiao do Dao+Regional Beiras, Doc da Regiao de Lisboa+Regional de\nLisboa, Doc da Regiao do Tejo+Regional do Tejo, Doc da Regiao de\nSetubal+Regional Terras do Sado, Doc Alentejo+Regional Alentejano e Doc da\nRegiao do Algarve+Regional do Algarve), year (2008 and 2009) and by form of\nconsumption (take home, direct consumption and discount). This work found some\nlinear regularity in the consumers behavior of the Portuguese wine.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2169v1"
    },
    {
        "title": "Cash Flow Entropy",
        "authors": [
            "Ulrich Kirchner",
            "Simon Moolman"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In this paper we aim to find a measure for the diversity of cash flows\nbetween agents in an economy. We argue that cash flows can be linked to\nprobabilities of finding a currency unit in a given cash flow. We then use the\ninformation entropy as a natural measure of diversity. This leads to a\nhirarchical inequality measure, which includes the well-known Theil index as a\nspecial case, and a constraint on agent entropy differentials, i.e., the\ndifference between cash inflow and outflow entropies. The last result is\nparticularly intriguing as it formalises the fact that an economy must contain\nboth, cash flow aggregating and cash flow diversifying agents.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.5504v1"
    },
    {
        "title": "A primer on reflexivity and price dynamics under systemic risk",
        "authors": [
            "Tom Fischer"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  A simple quantitative example of a reflexive feedback process and the\nresulting price dynamics after an exogenous price shock to a financial network\nis presented. Furthermore, an outline of a theory that connects financial\nreflexivity, which stems from cross-ownership and delayed or incomplete\ninformation, and no-arbitrage pricing theory under systemic risk is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.6415v1"
    },
    {
        "title": "Basis of financial arithmetic from the viewpoint of the utility theory",
        "authors": [
            "Krzysztof Piasecki"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The main goal of this paper is presentation a modern axiomatic approach to\nfinancial arithmetic. At the first, the axiomatic financial arithmetic theory\nwas proposed by Peccati who has introduced the axiomatic definition of the\nfuture value. This theory has been extensively developed in past years.\nProposed approach to financial arithmetic is based on the financial flow\nutility concept. This utility function is defined as linear extension of\nmulticriteria comparison determined by the time preference and the capital\npreference. Then the present value is equal to financial flow utility.\nTherefore, the law of diminishing marginal wealth utility has been considered\nas additional feature of the present value. The future value is defined as the\ninverse of utility function. This definition is a generalization of the Peccati\none. The net present value is given as the unique additive extension of\nfinancial flow utility. Moreover, the synergy effect and the diversification\neffect will be discussed. At the end, the axiomatic present value definition\nwill be specified in three ways.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.0537v1"
    },
    {
        "title": "The Heston Riemannian distance function",
        "authors": [
            "Archil Gulisashvili",
            "Peter Laurence"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The Heston model is a popular stock price model with stochastic volatility\nthat has found numerous applications in practice. In the present paper, we\nstudy the Riemannian distance function associated with the Heston model and\nobtain explicit formulas for this function using geometrical and analytical\nmethods. Geometrical approach is based on the study of the Heston geodesics,\nwhile the analytical approach exploits the links between the Heston distance\nfunction and the sub-Riemannian distance function in the Grushin plane. For the\nGrushin plane, we establish an explicit formula for the Legendre-Fenchel\ntransform of the limiting cumulant generating function and prove a partial\nlarge deviation principle that is true only inside a special set.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.2337v1"
    },
    {
        "title": "Quality Control and Due Diligence in Project Management: Getting\n  Decisions Right by Taking the Outside View",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper explores how theories of the planning fallacy and the outside view\nmay be used to conduct quality control and due diligence in project management.\nFirst, a much-neglected issue in project management is identified, namely that\nthe front-end estimates of costs and benefits--used in the business cases,\ncost-benefit analyses, and social and environmental impact assessments that\ntypically support decisions on projects--are typically significantly different\nfrom actual ex post costs and benefits, and are therefore poor predictors of\nthe actual value and viability of projects. Second, it is discussed how\nKahneman and Tversky's theories of the planning fallacy and the outside view\nmay help explain and remedy this situation through quality control of\ndecisions. Third, it is described what quality control and due diligence are in\nthe context of project management, and an eight-step procedure is outlined for\ndue diligence based on the outside view. Fourth, the procedure is tested on a\nreal-life, multibillion-dollar project, organized as a public-private\npartnership. Finally, Akerlof and Shiller's recent discussion in economics of\n\"firing the forecaster\" is discussed together with its relevance to project\nmanagement. In sum, the paper demonstrates the need, the theoretical basis, a\npractical methodology, and a real-life example for how to de-bias project\nmanagement using quality control and due diligence based on the outside view.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.2544v1"
    },
    {
        "title": "A Modern Approach to the Efficient-Market Hypothesis",
        "authors": [
            "Gabriel Frahm"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Market efficiency at least requires the absence of weak arbitrage\nopportunities, but this is not sufficient to establish a situation where the\nmarket is sensitive, i.e., where it \"fully reflects\" or \"rapidly adjusts to\"\nsome information flow including the evolution of asset prices. By contrast, No\nWeak Arbitrage together with market sensitivity is sufficient and necessary for\na market to be informationally efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.3001v5"
    },
    {
        "title": "Bimodality in the firm size distributions: a kinetic exchange model\n  approach",
        "authors": [
            "Anindya S. Chakrabarti"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Firm growth process in the developing economies is known to produce\ndivergence in their growth path giving rise to bimodality in the size\ndistribution. Similar bimodality has been observed in wealth distribution as\nwell. Here, we introduce a modified kinetic exchange model which can reproduce\nsuch features. In particular, we will show numerically that a nonlinear\nretention rate (or savings propensity) causes this bimodality. This model can\naccommodate binary trading as well as the whole system-side trading thus making\nit more suitable to explain the non-standard features of wealth distribution as\nwell as firm size distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.3818v2"
    },
    {
        "title": "An examination of the effect on the Icelandic Banking System of\n  Verðtryggð Lán (Indexed-Linked Loans)",
        "authors": [
            "Jacky Mallett"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In 1979 following a decade of hyperinflation, Iceland introduced\nVer{\\dh}trygg{\\dh} l\\'an, negatively amortised, index-linked loans whose\noutstanding principal is increased by the rate of the consumer price inflation\nindex(CPI). The loans were part of a general government policy which used\nindexation to the CPI to address the economic consequences of the\nhyperinflation. Although most other forms of indexation were subsequently\nremoved, loan indexation has remained, and these loans now comprise the\nmajority of mortgages in Iceland. Although it is still often argued that\nindex-linked loans helped to stop the hyperinflation, these arguments are\ntypically based on high level macro-economic interpretations of the Icelandic\neconomy, they fail to identify specific mechanisms to support their claims. In\nthis paper we take the opposite approach, and present a detailed analysis of\nthe monetary mechanics used for the loans at the double entry bookkeeping level\nof the banking system.\n  Based on this analysis there appears to be no evidence or mechanism that\nwould support the claim that index-linked loans reduce or stop inflation. On\nthe contrary: our research shows that the bookkeeping treatment of these loans\nwithin the banking system directly contributes to the banking system's monetary\nexpansion rate, and hence index-linked loans act to increase the inflation rate\nto which they are linked, rather than reducing it. They consequently create a\npositive feedback loop within the banking system's monetary regulation\noperating directly on the money supply. Since the feedback into monetary\nexpansion only occurs at annual CPI rates above approximately 2%, we suggest\none solution would be to stabilise the money supply to 0% growth, and we\nexplore some ways this could be achieved by modifying the Basel Regulatory\nFramework within the Icelandic Banking System.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.4112v2"
    },
    {
        "title": "On The EU and Euro-zone Stability",
        "authors": [
            "Dimitris Sardelis"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The aim of the present article is to offer a strictly mathematical,\nstatistical treatment of the current account balances in EU and in the\nEurozone. Based on Eurostat data, an overview of the total and annual balances\nis first made for different collections among the EU countries. Then, using the\nMathematica technical computing software, curve fitting is employed to\ndetermine the functions which best reflect how surpluses and deficits\naccumulate with time. It is shown that both EU and the Eurozone economies\nultimately have to pass through a critical, turning point beyond which the\naccumulation of deficits exceeds the accumulation of surpluses thus marking a\nperiod of instability. An interval estimate at a ninety eight percent degree of\nconfidence, yields that EU is found in a phase of instability since 2011 while\nthe instability turning point for the Eurozone is bound to occur any year from\n2015 to 2018.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.6212v1"
    },
    {
        "title": "On the theory of firm in nonlinear dynamic financial and economic\n  systems",
        "authors": [
            "Dimitri O. Ledenyov",
            "Viktor O. Ledenyov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The new business paradigms originate a strong necessity to re-think the\ntheory of the firm with the aim to get a better understanding on the\norganizational and functional principles of the firm, operating in the\ninvestment economies in the prosperous societies. In this connection, we make\nthe innovative research to advance our scientific knowledge on the theory of\nfirm in the conditions of the nonlinear dynamic financial and economic systems.\nWe propose that the nonlinearities have to be taken to the consideration and\nthe nonlinear differential equation have to be used to model the firm in the\nmodern theories of the firm in the nonlinear dynamic financial and economic\nsystems. We apply the econophysical approach with the dynamic regimes modeling\non the bifurcation diagram as in the dynamic chaos theory with the purpose to\naccurately characterize the nonlinearities in the economic theory of the firm.\nWe introduce the Ledenyov firm stability theorem, based on the Lyapunov\nstability criteria, to precisely characterize the stability of the firm in the\nnonlinear dynamic financial and economic systems in the time of globalization.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.6721v2"
    },
    {
        "title": "Is There A Real Estate Bubble in Switzerland?",
        "authors": [
            "Diego Ardila",
            "Peter Cauwels",
            "Dorsa Sanadgol",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We have analyzed the risks of possible development of bubbles in the Swiss\nresidential real estate market. The data employed in this work has been\ncollected by comparis.ch, and carefully cleaned from duplicate records through\na procedure based on supervised machine learning methods. The study uses the\nlog periodic power law (LPPL) bubble model to analyze the development of asking\nprices of residential properties in all Swiss districts between 2005 and 2013.\nThe results suggest that there are 11 critical districts that exhibit\nsignatures of bubbles, and seven districts where bubbles have already burst.\nDespite these strong signatures, it is argued that, based on the current\neconomic environment, a soft landing rather than a severe crash is expected.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.4514v1"
    },
    {
        "title": "Quantitative easing is an incomplete strategy that must be accompanied\n  by the nullification of debt",
        "authors": [
            "Karl Svozil"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Compound interest as well as inflation grows exponentially with time, whereas\nother means to repay debt grow polynomially. For this and other, mostly\npolitical, reasons, debt without inflation is unsustainable. We suggest a\ndiscontinuous way to eliminate debt by nullifying it. This scenario is\npreferable to current central bank strategies of quantitative easing because it\nallows the disposal of debt without hyperinflation or bloated balance sheets.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.6183v3"
    },
    {
        "title": "Inflation Uncertainty, Output Growth Uncertainty and Macroeconomic\n  Performance: Comparing Alternative Exchange Rate Regimes in Eastern Europe",
        "authors": [
            "Muhammad Khan",
            "Mazen Kebewar",
            "Nikolay Nenovsky"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In the late 90's, after severe financial and economic crisis, accompanied by\ninflation and exchange rate instability, Eastern Europe emerged into two groups\nof countries with radically contrasting monetary regimes (Currency Boards and\nInflation targeting). The task of our study is to compare econometrically the\nperformance of these two regimes in terms of the relationship between\ninflation, output growth, nominal and real uncertainties from 2000 till now. In\nother words, we test the hypothesis of non-neutrality of monetary and exchange\nrate regimes with respect to these connections. In a whole, the empirical\nresults do not allow us to judge which monetary regime is more appropriate and\nreasonable to assume. EU enlargement is one of the possible explanations for\nthe numbing of the differences and the lack of coherence between the two\nregimes in terms of inflation, growth and their uncertainties\n",
        "pdf_link": "http://arxiv.org/pdf/1303.6192v1"
    },
    {
        "title": "Comparison of Capital Costs per Route-Kilometre in Urban Rail",
        "authors": [
            "Bent Flyvbjerg",
            "Nils Bruzelius",
            "Bert van Wee"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Because of the prominent position of urban rail in reducing urban\ntransport-related problems, such as congestion and air pollution, insights into\nthe costs of possible new urban rail projects is very relevant for those\ninvolved with cost estimations, policy makers, cost-benefit analysts, and other\ntarget groups. Knowledge of the differences in costs per kilometre, including\nexplanations of differences and their breakdowns is currently lacking in the\nliterature. This paper aims to provide a first stage insight into how cost per\nkilometre varies across urban rail projects. The methodology applied is a\nsimple cost comparison across projects where the data collected are comparable.\nWe conclude that capital costs per route-kilometre of urban rail vary highly\nbetween projects. Looking at European projects and excluding outliers, the\ntotal capital costs per route-kilometre (including stations and rolling stock)\nlie mainly between US$50-100 million (2002 prices). Including US projects, the\nrange is US$50-150 million. The main reasons for the high variation in the\nroute-kilometre costs are differences between projects as regards the ratio of\nunderground to above-ground construction, ground conditions, station spacing,\ntype of rolling stock, environmental and safety constraints and labour costs.\nWe warn, however, that the observations used to reach the conclusions are too\nfew to obtain results with statistical significance. Our results must therefore\nbe seen as a first step towards collecting more data so that a more succinct\nstatistical analysis can be conducted. Another conclusion is therefore that\nthis area has future research potential.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.6569v1"
    },
    {
        "title": "Survival of the Unfittest: Why the Worst Infrastructure Gets Built, And\n  What We Can Do about It",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The article first describes characteristics of major infrastructure projects.\nSecond, it documents a much neglected topic in economics: that ex ante\nestimates of costs and benefits are often very different from actual ex post\ncosts and benefits. For large infrastructure projects the consequence is cost\noverruns, benefit shortfalls, and the systematic underestimation of risks.\nThird, implications for cost-benefit analysis are described, including that\nsuch analysis is not to be trusted for major infrastructure projects. Fourth,\nthe article uncovers the causes of this state of affairs in terms of perverse\nincentives that encourage promoters to underestimate costs and overestimate\nbenefits in the business cases for their projects. But the projects that are\nmade to look best on paper are the projects that amass the highest cost\noverruns and benefit shortfalls in reality. The article depicts this situation\nas \"survival of the un-fittest.\" Fifth, the article sets out to explain how the\nproblem may be solved, with a view to arriving at more efficient and more\ndemocratic projects, and avoiding the scandals that often accompany major\ninfrastructure investments. Finally, the article identifies current trends in\nmajor infrastructure development. It is argued that a rapid increase in\nstimulus spending combined with more investments in emerging economies combined\nwith more spending on information technology is catapulting infrastructure\ninvestment from the frying pan into the fire.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.6571v1"
    },
    {
        "title": "Underestimating Costs in Public Works Projects: Error or Lie?",
        "authors": [
            "Bent Flyvbjerg",
            "Mette K. Skamris Holm",
            "Søren L. Buhl"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This article presents results from the first statistically significant study\nof cost escalation in transportation infrastructure projects. Based on a sample\nof 258 transportation infrastructure projects worth US$90 billion and\nrepresenting different project types, geographical regions, and historical\nperiods, it is found with overwhelming statistical significance that the cost\nestimates used to decide whether such projects should be built are highly and\nsystematically misleading. Underestimation cannot be explained by error and is\nbest explained by strategic misrepresentation, that is, lying. The policy\nimplications are clear: legislators, administrators, investors, media\nrepresentatives, and members of the public who value honest numbers should not\ntrust cost estimates and cost-benefit analyses produced by project promoters\nand their analysts.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.6604v1"
    },
    {
        "title": "How (In)accurate Are Demand Forecasts in Public Works Projects? The Case\n  of Transportation",
        "authors": [
            "Bent Flyvbjerg",
            "Mette Skamris Holm",
            "Søren L. Buhl"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This article presents results from the first statistically significant study\nof traffic forecasts in transportation infrastructure projects. The sample used\nis the largest of its kind, covering 210 projects in 14 nations worth US$59\nbillion. The study shows with very high statistical significance that\nforecasters generally do a poor job of estimating the demand for transportation\ninfrastructure projects. The result is substantial downside financial and\neconomic risks. Such risks are typically ignored or downplayed by planners and\ndecision makers, to the detriment of social and economic welfare. For nine out\nof ten rail projects passenger forecasts are overestimated; average\noverestimation is 106 percent. This results in large benefit shortfalls for\nrail projects. For half of all road projects the difference between actual and\nforecasted traffic is more than plus/minus 20 percent. Forecasts have not\nbecome more accurate over the 30-year period studied. If techniques and skills\nfor arriving at accurate demand forecasts have improved over time, as often\nclaimed by forecasters, this does not show in the data. The causes of\ninaccuracy in forecasts are different for rail and road projects, with\npolitical causes playing a larger role for rail than for road. The cure is\ntransparency, accountability, and new forecasting methods. The challenge is to\nchange the governance structures for forecasting and project development. The\narticle shows how planners may help achieve this.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.6654v1"
    },
    {
        "title": "Policy and Planning for Large Infrastructure Projects: Problems, Causes,\n  Cures",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper argues, first, that a major problem in the planning of large\ninfrastructure projects is the high level of misinformation about costs and\nbenefits that decision makers face in deciding whether to build, and the high\nrisks such misinformation generates. Second, it explores the causes of\nmisinformation and risk, mainly in the guise of optimism bias and strategic\nmisrepresentation. Finally, the paper presents a number of measures aimed at\nimproving planning and decision making for large infrastructure projects,\nincluding changed incentive structures and better planning methods. Thus the\npaper is organized as a simple triptych consisting in problems, causes, and\ncures.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.7400v1"
    },
    {
        "title": "Measuring Inaccuracy in Travel Demand Forecasting: Methodological\n  Considerations Regarding Ramp Up and Sampling",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Project promoters, forecasters, and managers sometimes object to two things\nin measuring inaccuracy in travel demand forecasting: (1) using the forecast\nmade at the time of making the decision to build as the basis for measuring\ninaccuracy and (2) using traffic during the first year of operations as the\nbasis for measurement. This paper presents the case against both objections.\nFirst, if one is interested in learning whether decisions about building\ntransport infrastructure are based on reliable information, then it is exactly\nthe traffic forecasted at the time of making the decision to build that is of\ninterest. Second, although ideally studies should take into account so-called\ndemand \"ramp up\" over a period of years, the empirical evidence and practical\nconsiderations do not support this ideal requirement, at least not for large-N\nstudies. Finally, the paper argues that large samples of inaccuracy in travel\ndemand forecasts are likely to be conservatively biased, i.e., accuracy in\ntravel demand forecasts estimated from such samples would likely be higher than\naccuracy in travel demand forecasts in the project population. This bias must\nbe taken into account when interpreting the results from statistical analyses\nof inaccuracy in travel demand forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.7401v1"
    },
    {
        "title": "Cost Overruns and Demand Shortfalls in Urban Rail and Other\n  Infrastructure",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Risk, including economic risk, is increasingly a concern for public policy\nand management. The possibility of dealing effectively with risk is hampered,\nhowever, by lack of a sound empirical basis for risk assessment and management.\nThe paper demonstrates the general point for cost and demand risks in urban\nrail projects. The paper presents empirical evidence that allow valid economic\nrisk assessment and management of urban rail projects, including benchmarking\nof individual or groups of projects. Benchmarking of the Copenhagen Metro is\npresented as a case in point. The approach developed is proposed as a model for\nother types of policies and projects in order to improve economic and financial\nrisk assessment and management in policy and planning.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.7402v1"
    },
    {
        "title": "Delusion and Deception in Large Infrastructure Projects: Two Models for\n  Explaining and Preventing Executive Disaster",
        "authors": [
            "Bent Flyvbjerg",
            "Massimo Garbuio",
            "Dan Lovallo"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The Economist recently reported that infrastructure spending is the largest\nit is ever been as a share of world GDP. With $22 trillion in projected\ninvestments over the next ten years in emerging economies alone, the magazine\ncalls it the \"biggest investment boom in history.\" The efficiency of\ninfrastructure planning and execution is therefore particularly important at\npresent. Unfortunately, the private sector, the public sector and\nprivate/public sector partnerships have a dismal record of delivering on large\ninfrastructure cost and performance promises. This paper explains why and how\nto solve the problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.7403v1"
    },
    {
        "title": "Megaprojects and Risk: An Anatomy of Ambition",
        "authors": [
            "Bent Flyvbjerg",
            "Nils Bruzelius",
            "Werner Rothengatter"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Back cover text: Megaprojects and Risk provides the first detailed\nexamination of the phenomenon of megaprojects. It is a fascinating account of\nhow the promoters of multibillion-dollar megaprojects systematically and\nself-servingly misinform parliaments, the public and the media in order to get\nprojects approved and built. It shows, in unusual depth, how the formula for\napproval is an unhealthy cocktail of underestimated costs, overestimated\nrevenues, undervalued environmental impacts and overvalued economic development\neffects. This results in projects that are extremely risky, but where the risk\nis concealed from MPs, taxpayers and investors. The authors not only explore\nthe problems but also suggest practical solutions drawing on theory and hard,\nscientific evidence from the several hundred projects in twenty nations that\nillustrate the book. Accessibly written, it will be essential reading in its\nfield for students, scholars, planners, economists, auditors, politicians,\njournalists and interested citizens.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.7404v1"
    },
    {
        "title": "How Planners Deal with Uncomfortable Knowledge: The Dubious Ethics of\n  the American Planning Association",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  With a point of departure in the concept \"uncomfortable knowledge,\" this\narticle presents a case study of how the American Planning Association (APA)\ndeals with such knowledge. APA was found to actively suppress publicity of\nmalpractice concerns and bad planning in order to sustain a boosterish image of\nplanning. In the process, APA appeared to disregard and violate APA's own Code\nof Ethics. APA justified its actions with a need to protect APA members'\ninterests, seen as preventing planning and planners from being presented in\npublic in a bad light. The current article argues that it is in members'\ninterest to have malpractice critiqued and reduced, and that this best happens\nby exposing malpractice, not by denying or diverting attention from it as APA\ndid in this case. Professions, organizations, and societies that stifle\ncritique tend to degenerate and become socially and politically irrelevant\n\"zombie institutions.\" The article asks whether such degeneration has set in\nfor APA and planning. Finally, it is concluded that more debate about APA's\nethics and actions is needed for improving planning practice. Nine key\nquestions are presented to constructively stimulate such debate.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.7405v1"
    },
    {
        "title": "Why Your IT Project Might Be Riskier Than You Think",
        "authors": [
            "Bent Flyvbjerg",
            "Alexander Budzier"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Out-of-control information technology (IT) projects have ended the careers of\ntop managers, such as EADS CEO Noel Forgeard and Levi Strauss' CIO David\nBergen. Moreover, IT projects have brought down whole companies, like Kmart in\nthe US and Auto Windscreen in the UK. Software and other IT is now such an\nintegral part of most business processes and products that CEOs must know their\nIT risks, which are typically substantial and overlooked. The analysis of a\nsample of 1,471 IT projects showed that the average cost overrun was 27% - but\nthat figure masks a far more alarming 'fat tail' risk. Fully one in six of the\nprojects in the sample was a Black Swan, with a cost overrun of 200%, on\naverage, and a schedule overrun of almost 70%. This highlights the true pitfall\nof IT change initiatives: It's not that they're particularly prone to high cost\noverruns on average - it is that there are a disproportionate number of Black\nSwans. By focusing on averages instead of the more damaging outliers, most\nmanagers and consultants have been missing the real risk in doing IT. In\nconclusion, the article outlines ideas as to what can be done to avoid Black\nSwans.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.0265v2"
    },
    {
        "title": "Five Misunderstandings About Case-Study Research",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This article examines five common misunderstandings about case-study\nresearch: (1) Theoretical knowledge is more valuable than practical knowledge;\n(2) One cannot generalize from a single case, therefore the single case study\ncannot contribute to scientific development; (3) The case study is most useful\nfor generating hypotheses, while other methods are more suitable for hypotheses\ntesting and theory building; (4) The case study contains a bias toward\nverification; and (5) It is often difficult to summarize specific case studies.\nThe article explains and corrects these misunderstandings one by one and\nconcludes with the Kuhnian insight that a scientific discipline without a large\nnumber of thoroughly executed case studies is a discipline without systematic\nproduction of exemplars, and that a discipline without exemplars is an\nineffective one. Social science may be strengthened by the execution of more\ngood case studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.1186v1"
    },
    {
        "title": "Why Mass Media Matter to Planning Research: The Case of Megaprojects",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This article asks how planning scholarship may effectively gain impact in\nplanning practice through media exposure. In liberal democracies the public\nsphere is dominated by mass media. Therefore, working with such media is a\nprerequisite for effective public impact of planning research. Using the\nexample of megaproject planning, it is illustrated how so-called \"phronetic\nplanning research,\" which explicitly incorporates in its methodology active and\nstrategic collaboration with media, may be helpful in generating change in\nplanning practice via the public sphere. Main lessons learned are: (1) Working\nwith mass media is an extremely cost-effective way to increase the impact of\nplanning scholarship on practice; (2) Recent developments in information\ntechnology and social media have made impact via mass media even more\neffective; (3) Research on \"tension points,\" i.e., points of potential\nconflict, are particularly interesting to media and the public, and are\nespecially likely to generate change in practice; and (4) Tension points bite\nback; planning researchers should be prepared for, but not afraid of, this.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.1665v1"
    },
    {
        "title": "Return on net sales from three companies in the manufacturing of\n  fabricated metal products (except machinery and equipment)",
        "authors": [
            "Marta Tomczak",
            "Anna Ziolkowska",
            "Martyna Rosik"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Keywords: corporate finance, Wroc{\\l}aw University of Economics, net profit\nmargin lub net sales profitability\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3350v1"
    },
    {
        "title": "Hierarchy of Frustrations as Supplementary Indices in Complex System\n  Dynamics, Applied to the U.S. Intermarket",
        "authors": [
            "Krzysztof Sokalski"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Definition of frustration is expressed by transitivity of binary entanglement\nrelation in considered complex system. Extending this definition into n-ary\nrelation a hierarchy of frustrations is derived. As a complex system the U.S.\nIntermarket is chosen where the correlation coefficient of intermarket sectors\nplays the role of entanglement measure. In each hierarchy level the frustration\nand the transitivity are interpreted as values of an order of measure for\ncorresponding subsystem. The derived theory is applied to 1983-2012 data of the\nU.S. Intermarket.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3722v1"
    },
    {
        "title": "Pricing and Valuation under the Real-World Measure",
        "authors": [
            "Gabriel Frahm"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In general it is not clear which kind of information is supposed to be used\nfor calculating the fair value of a contingent claim. Even if the information\nis specified, it is not guaranteed that the fair value is uniquely determined\nby the given information. A further problem is that asset prices are typically\nexpressed in terms of a risk-neutral measure. This makes it difficult to\ntransfer the fundamental results of financial mathematics to econometrics. I\nshow that the aforementioned problems evaporate if the financial market is\ncomplete and sensitive. In this case, after an appropriate choice of the\nnumeraire, the discounted price processes turn out to be uniformly integrable\nmartingales under the real-world measure. This leads to a Law of One Price and\na simple real-world valuation formula in a model-independent framework where\nthe number of assets as well as the lifetime of the market can be finite or\ninfinite.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3824v13"
    },
    {
        "title": "What Causes Cost Overrun in Transport Infrastructure Projects?\"",
        "authors": [
            "Bent Flyvbjerg",
            "Mette K. Skamris Holm",
            "Søren L. Buhl"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This article presents results from the first statistically significant study\nof causes of cost escalation in transport infrastructure projects. The study is\nbased on a sample of 258 rail, bridge, tunnel and road projects worth US$90\nbillion. The focus is on the dependence of cost escalation on (1) length of\nproject implementation phase, (2) size of project and (3) type of project\nownership. First, it is found with very high statistical significance that cost\nescalation is strongly dependent on length of implementation phase. The policy\nimplications are clear: Decision makers and planners should be highly concerned\nabout delays and long implementation phases because they translate into risks\nof substantial cost escalations. Second, it is found that projects have grown\nlarger over time and that for bridges and tunnels larger projects have larger\npercentage cost escalations. Finally, by comparing cost escalation for three\ntypes of project ownership--private, state-owned enterprise and other public\nownership--it is shown that the oft-seen claim that public ownership is\nproblematic and private ownership effective in curbing cost escalation is an\noversimplification. Type of accountability appears to matter more to cost\nescalation than type of ownership.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.4476v1"
    },
    {
        "title": "Overspend? Late? Failure? What the Data Say About IT Project Risk in the\n  Public Sector",
        "authors": [
            "Alexander Budzier",
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Implementing large-scale information and communication technology (IT)\nprojects carries large risks and easily might disrupt operations, waste\ntaxpayers' money, and create negative publicity. Because of the high risks it\nis important that government leaders manage the attendant risks. We analysed a\nsample of 1,355 public sector IT projects. The sample included large-scale\nprojects, on average the actual expenditure was $130 million and the average\nduration was 35 months. Our findings showed that the typical project had no\ncost overruns and took on average 24% longer than initially expected. However,\ncomparing the risk distribution with the normative model of a thin-tailed\ndistribution, projects' actual costs should fall within -30% and +25% of the\nbudget in nearly 99 out of 100 projects. The data showed, however, that a\nstaggering 18% of all projects are outliers with cost overruns >25%. Tests\nshowed that the risk of outliers is even higher for standard software (24%) as\nwell as in certain project types, e.g., data management (41%), office\nmanagement (23%), eGovernment (21%) and management information systems (20%).\nAnalysis showed also that projects duration adds risk: every additional year of\nproject duration increases the average cost risk by 4.2 percentage points.\nLastly, we suggest four solutions that public sector organization can take: (1)\nbenchmark your organization to know where you are, (2) de-bias your IT project\ndecision-making, (3) reduce the complexities of your IT projects, and (4)\ndevelop Masterbuilders to learn from the best in the field.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.4525v3"
    },
    {
        "title": "On the accurate characterization of business cycles in nonlinear dynamic\n  financial and economic systems",
        "authors": [
            "Dimitri O. Ledenyov",
            "Viktor O. Ledenyov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The accurate characterization of the business cycles in the nonlinear dynamic\nfinancial and economic systems in the time of globalization represents a\nformidable research problem. The central banks and other financial institutions\nmake their decisions on the minimum capital requirements, countercyclical\ncapital buffer allocation and capital investments, going from the precise data\non the business cycles. We consider the two possible interaction scenarios,\nwhen there are: the linear interactions, and the non-linear interactions. In\nour opinion, the main parameters of the business cycle may deviate during the\nbusiness cycle nonlinear interaction with the nonlinear dynamic financial and\neconomic systems, because of the origination of the nonlinear effects such as\nthe Four Waves Mixing (FWM), Stimulated Brillouin Scattering (SBS), Stimulated\nRaman Scattering (SRS), Carrier Induced Phase Modulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.4807v1"
    },
    {
        "title": "Firm's Information Environment and Stock Liquidity : Evidence from\n  Tunisian Context,",
        "authors": [
            "Nadia Loukil",
            "Ouidad Yousfi"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper analyzes the relationship between public disclosure, private\ninformation and stock liquidity in Tunisian context using a sample of 41 listed\nfirms in the Tunis Stock Exchange in 2007. First, we find no evidence that\nthere is a relation between public and private information. Second, Tunisian\ninvestors do not trust the information disclosed in both annual reports and web\nsites, consequently it has no effects on stock liquidity, in contrast with\nprivate information.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.4852v1"
    },
    {
        "title": "Government Solvency, Austerity and Fiscal Consolidation in the OECD: A\n  Keynesian Appraisal of Transversality and No Ponzi Game Conditions",
        "authors": [
            "Karim Azizi",
            "Nicolas Canry",
            "Jean-Bernard Chatelain",
            "Bruno Tinel"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper investigates the relevance of the No-Ponzi game condition for\npublic debt (i.e. the public debt growth rate has to be lower than the real\ninterest rate, a necessary assumption for Ricardian equivalence) and of the\ntransversality condition for the GDP growth rate (i.e. the GDP growth rate has\nto be lower than the real interest rate). First, on the unbalanced panel of 21\ncountries from 1961 to 2010 available in OECD database, those two conditions\nwere simultaneously validated only for 29% of the cases under examination.\nSecond, those two conditions were more frequent in the 1980s and the 1990s when\nmonetary policies were more restrictive. Third, in tune with the Keynesian\nview, when the real interest rate is higher than the GDP growth, it corresponds\nto 75% of the cases of the increases of the debt/GDP ratio but to only 43% of\nthe cases of the decreases of the debt/GDP ratio (fiscal consolidations).\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7330v1"
    },
    {
        "title": "Deriving Derivatives",
        "authors": [
            "Andrei N. Soklakov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Quantitative structuring is a rigorous framework for the design of financial\nproducts. We show how it incorporates traditional investment ideas while\nsupporting a more accurate expression of clients' views. We touch upon adjacent\ntopics regarding the safety of financial derivatives and the role of pricing\nmodels in product design.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7533v7"
    },
    {
        "title": "Elasticity theory of structuring",
        "authors": [
            "Andrei N. Soklakov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Financial derivatives have often been criticized as casino-style betting\ninstruments. It turns out that many naive ways of making them are indeed\nequivalent to gambling. Fortunately, this inadvertent effect can be understood\nand prevented. We present a theory of product design which achieves that.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7535v6"
    },
    {
        "title": "Some Possible Solution of Problem of Sovereign Debts: a short plan",
        "authors": [
            "T. S. Kholupenko",
            "E. E. Kholupenko",
            "P. A. Guseva"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Possible solution of problem of sovereign debts is suggested. At the current\nmoment this solution still can be provided only by methods of the world\nmonetary policy.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.2251v1"
    },
    {
        "title": "Modeling and Solving Alternative Financial Solutions Seeking",
        "authors": [
            "Emmanuel Frenod",
            "Jean-Philippe Gouigoux",
            "Landry Touré"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In this paper we build a method to optimize Multi-Year Prospective Budgets.\nFirst we present a systemic model of Local Community Finances. Then, from two\nacceptable Multi-Year Prospective Budgets the method implements a Genetic\nAlgorithm to generate a collection of admissible Multi-Year Prospective Budgets\namong which Decision-Makers can choose. The method is tested on simplified\ncases and on in operational situation and gives satisfactory results.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.2820v2"
    },
    {
        "title": "How interbank lending amplifies overlapping portfolio contagion: A case\n  study of the Austrian banking network",
        "authors": [
            "Fabio Caccioli",
            "J. Doyne Farmer",
            "Nick Foti",
            "Daniel Rockmore"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In spite of the growing theoretical literature on cascades of failures in\ninterbank lending networks, empirical results seem to suggest that networks of\ndirect exposures are not the major channel of financial contagion. In this\npaper we show that networks of interbank exposures can however significantly\namplify contagion due to overlapping portfolios. To illustrate this point, we\nconsider the case of the Austrian interbank network and perform stress tests on\nit according to different protocols. We consider in particular contagion due to\n(i) counterparty loss; (ii) roll-over risk; and (iii) overlapping portfolios.\nWe find that the average number of bankruptcies caused by counterparty loss and\nroll-over risk is fairly small if these contagion mechanisms are considered in\nisolation. Once portfolio overlaps are also accounted for, however, we observe\nthat the network of direct interbank exposures significantly contributes to\nsystemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3704v1"
    },
    {
        "title": "Kinetic properties in inhomogeneous self-aware media",
        "authors": [
            "A. Morozovskiy",
            "A. A. Snarskii",
            "I. V. Bezsudnov",
            "V. A. Sevryukov",
            "J. Malinsky"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The new framework for finance is proposed. This framework based on three\nknown approaches in econophysics. Assumptions of the framework are the\nfollowing: 1. For the majority of situations market follows non-arbitrage\ncondition. 2. For the small number of situations market influenced by the\nactions of big firms. 3. If actions of big players lead to the arbitrage\nopportunity, small players could self-organize to take advantage of this\nopportunity. The framework is an attempt to combine approaches of Bouchaud,\nGabaix, Sornette, Stanley and coauthors. Suggested framework is applied for the\nanalysis of market impact models, behavior of big players, self-organization of\nmarket firm and volatility description.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.3668v2"
    },
    {
        "title": "Inflation, unemployment, and labor force. Phillips curves and long-term\n  projections for Japan",
        "authors": [
            "Ivan Kitov",
            "Oleg Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The evolution of the rate of price inflation and unemployment in Japan has\nbeen modeled within the Phillips curve framework. As an extension to the\nPhillips curve, we represent both variables as linear functions of the change\nrate of labor force. All models were first estimated in 2005 for the period\nbetween 1980 and 2003. Here we update these original models with data through\n2012. The revisited models accurately describe disinflation during the 1980s\nand 1990s as well as the whole deflationary period started in the late 1990s.\nThe Phillips curve for Japan confirms the original concept that growing\nunemployment results in decreasing inflation. A linear and lagged generalized\nPhillips curve expressed as a link between inflation, unemployment, and labor\nforce has been also re-estimated and validated by new data. Labor force\nprojections allow a long-term inflation and unemployment forecast: the GDP\ndeflator will be negative (between -0.5% and -2% per year) during the next 40\nyears. The rate of unemployment will increase from 4.3% in 2012 to 5.5% in\n2050.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1757v1"
    },
    {
        "title": "Econophysics: Comments on a few Applications, Successes, Methods, &\n  Models",
        "authors": [
            "Marcel Ausloos"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  For this special issue, the article aims at discussing a few econophysics\nproblems studied so far rather successfully. The following \"applications\" in\nmicro-econo-physics are considered : (i) financial crashes; it is emphasized\nthat one can distinguish between endogenous and exogenous causes; (ii)\nportofolio control, selection and inherent risk measure; (iii) foreign currency\nexchanges, also distinguishing endogenous and exogenous money control; (iv)\nprice and asset evolution values. It is shown that some macro-econo-physics\nproblem have been also tackled, like geographic/political constraints, the\nglobalization of the economy and country clustering. Moreover, it is daring to\nsuggest prospect for studies and researches, whence presenting some selection\nof a few interesting perspectives.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1953v1"
    },
    {
        "title": "Learning curve for collective behavior of zero-intelligence agents in\n  successive job-hunting processes with a diversity of Jaynes-Shannon's MaxEnt\n  principle",
        "authors": [
            "He Chen",
            "Jun-ichi Inoue"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Inspired by the unsupervised learning or self-organization in the machine\nlearning context, here we attempt to draw `learning curve' for the collective\nbehavior of job-seeking `zero-intelligence' labors in successive job-hunting\nprocesses. Our labor market is supposed to be opened especially for university\ngraduates in Japan, where the students have several successive chances\nn=0,1,2,... to obtain their positions within an academic (business) year. In\nthis sense, the `cumulative unemployment rate' in our model system is regarded\nas an error-measurement in the collective intelligence of students, and the\njob-hunting stage n-dependence of the error constructs a learning curve. In our\nsimple toy-model of probabilistic labor market, the diversity of students'\nbehavior is built-in by means of the Jaynes-Shannon's MaxEnt (Maximum Entropy)\nprinciple. Then, we discuss the speed of convergence for the error-measurement,\nwhere we consider a scenario in which the students do not use any information\nabout the result of job-hunting processes in the previous stage. Our approach\nenables us to examine the existence of the condition on which macroscopic\nquantity, say, `stage-wise unemployment rate' becomes `scale-invariant' in the\nsense that it does not depend on the job-hunting stage n. From the macroscopic\nview point, the problem could be regarded as a human resource allocation.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.5053v1"
    },
    {
        "title": "Statistical Mechanics of Labor Markets",
        "authors": [
            "He Chen",
            "Jun-ichi Inoue"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We introduce a probabilistic model of labor markets for university graduates,\nin particular, in Japan. To make a model of the market efficiently, we take\ninto account several hypotheses. Namely, each company fixes the (business year\nindependent) number of opening positions for newcomers. The ability of\ngathering newcomers depends on the result of job matching process in past\nbusiness years. This fact means that the ability of the company is weaken if\nthe company did not make their quota or the company gathered applicants too\nmuch over the quota. All university graduates who are looking for their jobs\ncan access the public information about the ranking of companies. Assuming the\nabove essential key points, we construct the local energy function of each\ncompany and describe the probability that an arbitrary company gets students at\neach business year by a Boltzmann-Gibbs distribution. We evaluate the relevant\nphysical quantities such as the employment rate. We find that the system\nundergoes a sort of `phase transition' from the `good employment phase' to\n`poor employment phase' when one controls the degree of importance for the\nranking.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.5156v1"
    },
    {
        "title": "Dynamics of probabilistic labor markets: statistical physics perspective",
        "authors": [
            "He Chen",
            "Jun-ichi Inoue"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We introduce a toy probabilistic model to analyze job-matching processes in\nrecent Japanese labor markets for university graduates by means of statistical\nphysics. We show that the aggregation probability of each company is rewritten\nby means of non-linear map under several conditions. Mathematical treatment of\nthe map enables us to discuss the condition on which the rankings of arbitrary\ntwo companies are reversed during the dynamics. The so-called `mismatch'\nbetween students and companies is discussed from both empirical and theoretical\nviewpoints.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.5158v1"
    },
    {
        "title": "A new financial metric for the art market",
        "authors": [
            "Ventura Charlin",
            "Arturo Cifuentes"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper introduces a new financial metric for the art market. The metric\nis based on the price per unit of area and is applicable to two-dimensional art\nobjects such as paintings.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.6929v3"
    },
    {
        "title": "Estimating the FDI Impact on Economic Growth and Export Performances of\n  the European Economies in Transition",
        "authors": [
            "Olivera Kostoska",
            "Pece Mitrevski"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Within the last two decades, Foreign Direct Investment (FDI) has been\nobserved as one of the prime instruments in the process of restructuring the\nEuropean economies in transition. Many scholars argue that FDI is expected to\nbe a source of valuable technology transfer thus might certainly have positive\neffects on host country development efforts. Nonetheless, there are no\nclear-cut findings about the FDI genuine performances in supporting the\neconomic growth, productivity and export improvements within the European\ntransition countries. Using a large and comprehensive data set, we will\ntherefore analyze the linkage between FDI and above mentioned variables, so as\nto recommend national policy appropriate measures aimed at averting negative\nand strengthening the positive FDI spillovers.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1342v1"
    },
    {
        "title": "Inflation, unemployment, and labour force. Phillips curves and long-term\n  projections for Austria",
        "authors": [
            "Ivan Kitov",
            "Oleg Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We model the rate of inflation and unemployment in Austria since the early\n1960s within the Phillips/Fisher framework. The change in labour force is the\ndriving force representing economic activity in the Phillips curve. For\nAustria, this macroeconomic variable was first tested as a predictor of\ninflation and unemployment in 2005 with the involved time series ended in 2003.\nHere we extend all series by nine new readings available since 2003 and\nre-estimate the previously estimated relationships between inflation,\nunemployment, and labour force. As before, a structural break is allowed in\nthese relationships, which is related to numerous changes in definitions in the\n1980s. The break year is estimated together with other model parameters by the\nBoundary Element Method with the LSQ fitting between observed and predicted\nintegral curves. The precision of inflation prediction, as described by the\nroot-mean-square (forecasting) error is by 20% to 70% better than that\nestimated by AR(1) model. The estimates of model forecasting error are\navailable for those time series where the change in labour force leads by one\n(the GDP deflator) or two (CPI) years. For the whole period between 1965 and\n2012 as well as for the intervals before and after the structural break (1986\nfor all inflation models) separately, our model is superior to the na\\\"ive\nforecasting, which in turn, is not worse than any other forecasting model. The\nlevel of statistical reliability and the predictive power of the link between\ninflation and labour force imply that the National Bank of Austria does not\ncontrol inflation and unemployment beyond revisions to definitions. The labour\nforce projection provided by Statistic Austria allows foreseeing inflation at a\nforty-year horizon: the rate of CPI inflation will hover around 1.3% and the\nGDP deflator will likely sink below zero between 2018 and 2034.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1786v1"
    },
    {
        "title": "Geometrization of Econophysics : An Alternative Approach for Measuring\n  Elements of Risk Management of an Economic System",
        "authors": [
            "M. E. Kahil"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The relationship between micro-structure and macro-structure of complex\nsystems using information geometry has been dealt by several authors. From this\nperspective, we are going to apply it as a geometrical structure connecting\nboth microeconomics and macroeconomics . The results lead us to introduce new\nmodified quantities into both micro-macro economics that enable us to describe\nthe link between them. The importance of such a scheme is to find out -with\nsome accuracy- a new method can be introduced for examining the stability of an\neconomic system. This type of requirement is expressed by examining the\nstability of the equations of path deviations for some economic systems as\ndescribed in a statistical manifold. Such a geometization scheme of economic\nsystems is an important step toward identifying risk management factors and so\ncontributes to the growing literature of econophysics.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.2220v2"
    },
    {
        "title": "A note on the policy implications of the fiscal multiplier",
        "authors": [
            "Evangelos F. Magirou"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We present an elementary analysis of the dynamical aspects of the GDP /\ngovernment surplus multiplier with relevance to the assessment of a country's\ndebt repayment policy. We show the (at first) counter intuitive result that in\norder to reduce the Debt/GDP ratio, countries with high Debt to GDP should go\ninto further debt, as long as the Debt to GDP ratio is roughly greater than the\ninverse of the multiplier. Thus small values of the multiplier make further\ndebt undesirable, and conversely.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.3083v1"
    },
    {
        "title": "The Origin of Fat Tails",
        "authors": [
            "Martin Gremm"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We propose a random walk model of asset returns where the parameters depend\non market stress. Stress is measured by, e.g., the value of an implied\nvolatility index. We show that model parameters including standard deviations\nand correlations can be estimated robustly and that all distributions are\napproximately normal. Fat tails in observed distributions occur because time\nseries sample different stress levels and therefore different normal\ndistributions. This provides a quantitative description of the observed\ndistribution including the fat tails. We discuss simple applications in risk\nmanagement and portfolio construction.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.4538v2"
    },
    {
        "title": "Structure and causality relations in a global network of financial\n  companies",
        "authors": [
            "Leonidas Sandoval Junior"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This work uses the stocks of the 197 largest companies in the world, in terms\nof market capitalization, in the financial area in the study of causal\nrelationships between them using Transfer Entropy, which is calculated using\nthe stocks of those companies and their counterparts lagged by one day. With\nthis, we can assess which companies influence others according to sub-areas of\nthe financial sector, which are banks, diversified financial services, savings\nand loans, insurance, private equity funds, real estate investment companies,\nand real estate trust funds. We also analyzed the causality relations between\nthose stocks and the network formed by them based on this measure, verifying\nthat they cluster mainly according to countries of origin, and then by industry\nand sub-industry. Then we collected data on the stocks of companies in the\nfinancial sector of some countries that are suffering the most with the current\ncredit crisis: Greece, Cyprus, Ireland, Spain, Portugal, and Italy, and assess,\nalso using transfer entropy, which companies from the largest 197 are most\naffected by the stocks of these countries in crisis. The intention is to map a\nnetwork of influences that may be used in the study of possible contagions\noriginating in those countries in financial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.5388v1"
    },
    {
        "title": "Does Banque de France control inflation and unemployment?",
        "authors": [
            "Ivan Kitov",
            "Oleg Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We re-estimate statistical properties and predictive power of a set of\nPhillips curves, which are expressed as linear and lagged relationships between\nthe rates of inflation, unemployment, and change in labour force. For France,\nseveral relationships were estimated eight years ago. The change rate of labour\nforce was used as a driving force of inflation and unemployment within the\nPhillips curve framework. The set of nested models starts with a simplistic\nversion without autoregressive terms and one lagged term of explanatory\nvariable. The lag is determined empirically together with all coefficients. The\nmodel is estimated using the Boundary Element Method (BEM) with the least\nsquares method applied to the integral solutions of the differential equations.\nAll models include one structural break might be associated with revisions to\ndefinitions and measurement procedures in the 1980s and 1990s as well as with\nthe change in monetary policy in 1994-1995. For the GDP deflator, our original\nmodel provided a root mean squared forecast error (RMSFE) of 1.0% per year at a\nfour-year horizon for the period between 1971 and 2004. The rate of CPI\ninflation is predicted with RMSFE=1.5% per year. For the naive (no change)\nforecast, RMSFE at the same time horizon is 2.95% and 3.3% per year,\nrespectively. Our model outperforms the naive one by a factor of 2 to 3. The\nrelationships for inflation were successfully tested for cointegration. We have\nformally estimated several vector error correction (VEC) models for two\nmeasures of inflation. At a four year horizon, the estimated VECMs provide\nsignificant statistical improvements on the results obtained by the BEM:\nRMSFE=0.8% per year for the GDP deflator and ~1.2% per year for CPI. For a two\nyear horizon, the VECMs improve RMSFEs by a factor of 2, with the smallest\nRMSFE=0.5% per year for the GDP deflator.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.1097v1"
    },
    {
        "title": "The multiplex structure of interbank networks",
        "authors": [
            "Leonardo Bargigli",
            "Giovanni di Iasio",
            "Luigi Infante",
            "Fabrizio Lillo",
            "Federico Pierobon"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The interbank market has a natural multiplex network representation. We\nemploy a unique database of supervisory reports of Italian banks to the Banca\nd'Italia that includes all bilateral exposures broken down by maturity and by\nthe secured and unsecured nature of the contract. We find that layers have\ndifferent topological properties and persistence over time. The presence of a\nlink in a layer is not a good predictor of the presence of the same link in\nother layers. Maximum entropy models reveal different unexpected substructures,\nsuch as network motifs, in different layers. Using the total interbank network\nor focusing on a specific layer as representative of the other layers provides\na poor representation of interlinkages in the interbank market and could lead\nto biased estimation of systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4798v1"
    },
    {
        "title": "Unified Growth Theory: A puzzling collection of myths based on\n  hyperbolic illusions",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The Unified Growth Theory is a puzzling collection of myths based on\nillusions created by hyperbolic distributions. Some of these myths are\ndiscussed. The examination of data shows that the three stages of growth\n(Malthusian Regime, Post-Malthusian Regime and Modern Growth Regime) did not\nexist and that Industrial Revolution had no influence on the economic growth\nand on the growth of human population. All elaborate explanations revolving\naround phantom features created by hyperbolic illusions might be fascinating\nbut they are scientifically unacceptable and, consequently, they do not explain\nthe economic growth. The data clearly indicate that the economic growth was not\nas complicated as described by the Unified Growth Theory but elegantly simple.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.5511v3"
    },
    {
        "title": "Towards a microeconomic theory of the finance-driven business cycle",
        "authors": [
            "Alejandro Jenkins"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  I sketch a program for a microeconomic theory of the main component of the\nbusiness cycle as a recurring disequilibrium, driven by incompleteness of the\nfinancial market and by information asymmetries between borrowers and lenders.\nThis proposal seeks to incorporate five distinct but connected processes that\nhave been discussed at varying lengths in the literature: the leverage cycle,\nfinancial panic, debt deflation, debt overhang, and deleveraging of households.\nIn the wake of the 2007-08 financial crisis, policy responses by central banks\nhave addressed only financial panic and debt deflation. Debt overhang and the\nslowness of household deleveraging account for the Keynesian \"excessive saving\"\nseen in recessions, which raises questions about the suitability of the\nstandard Keynesian remedies.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.0323v2"
    },
    {
        "title": "Optimal Trading Strategies as Measures of Market Disequilibrium",
        "authors": [
            "Valerii Salov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  For classification of the high frequency trading quantities, waiting times,\nprice increments within and between sessions are referred to as the a-, b-, and\nc-increments. Statistics of the a-b-c-increments are computed for the Time &\nSales records posted by the Chicago Mercantile Exchange Group for the futures\ntraded on Globex. The Weibull, Kumaraswamy, Riemann and Hurwitz Zeta,\nparabolic, Zipf-Mandelbrot distributions are tested for the a- and\nb-increments. A discrete version of the Fisher-Tippett distribution is\nsuggested for approximating the extreme b-increments. Kolmogorov and Uspenskii\nclassification of stochastic, typical, and chaotic random sequences is reviewed\nwith regard to the futures price limits. Non-parametric L1 and log-likelihood\ntests are applied to check dependencies between the a- and b-increments. The\nmaximum profit strategies and optimal trading elements are suggested as\nmeasures of frequency and magnitude of the market offers and disequilibrium.\nEmpirical cumulative distribution functions of optimal profits are reported. A\nfew classical papers are reviewed with more details in order to trace the\norigin and foundation of modern finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2004v1"
    },
    {
        "title": "Fiscal shocks and asymmetric effects: a comparative analysis",
        "authors": [
            "Ioannis Praggidis",
            "Periklis Gogas",
            "Vasilios Plakandaras",
            "Theophilos Papadimitriou"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We empirically test the effects of unanticipated fiscal policy shocks on the\ngrowth rate and the cyclical component of real private output and reveal\ndifferent types of asymmetries in fiscal policy implementation. The data used\nare quarterly U.S. observati ons over the period 1967:1 to 2011:4. In doing so,\nwe use both a vector autoregressive and the novel support vector machines\nsystems in order to extract the fiscal policy shocks series. The latter has\nnever been used before in a similar macroeconomic setting. Within our research\nframework, in order to test the robustness of our results to alternative\naggregate money supply definitions we use two alternative moentary aggregates.\nThese are the commonly reported by central banks and policy makers simple sum\nmonetary aggregates at the MZM level of aggregation and the alternative CFS\nDivisia MZM aggregate. From each of these four systems we extracted four types\nof shocks: a negative and a positive government spending shock and a negative\nand a positive government revenue shock. These eight different types of\nunanticipated fiscal policy shocks are next used to empirically examine their\neffects on the growth rate and the cyclical component of real private GNP in\ntwo sets of regressions: one that assumes only contemporaneous effects of the\nshocks on output and one that is augmented with four lags of each fiscal shock.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2693v1"
    },
    {
        "title": "Modelling of the European Union income distribution by extended\n  Yakovenko formula",
        "authors": [
            "Maciej Jagielski",
            "Ryszard Kutner"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We found a unified formula for description of the household incomes of all\nsociety classes, for instance, for the European Union in years 2005-2010. The\nformula is more general than well known that of Yakovenko et al. because, it\nsatisfactorily describes not only the household incomes of low- and\nmedium-income society classes but also the household incomes of the high-income\nsociety class. As a striking result, we found that the high-income society\nclass almost disappeared in year 2009, in opposite to situation in remaining\nyears, where this class played a significant role.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2722v1"
    },
    {
        "title": "The process of macroprudential oversight in Europe",
        "authors": [
            "Peter Sarlin",
            "Henrik J. Nyman"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The 2007--2008 financial crisis has paved the way for the use of\nmacroprudential policies in supervising the financial system as a whole. This\npaper views macroprudential oversight in Europe as a process, a sequence of\nactivities with the ultimate aim of safeguarding financial stability. To\nconceptualize a process in this context, we introduce the notion of a public\ncollaborative process (PCP). PCPs involve multiple organizations with a common\nobjective, where a number of dispersed organizations cooperate under various\nunstructured forms and take a collaborative approach to reaching the final\ngoal. We argue that PCPs can and should essentially be managed using the tools\nand practices common for business processes. To this end, we conduct an\nassessment of process readiness for macroprudential oversight in Europe. Based\nupon interviews with key European policymakers and supervisors, we provide an\nanalysis model to assess the maturity of five process enablers for\nmacroprudential oversight. With the results of our analysis, we give clear\nrecommendations on the areas that need further attention when macroprudential\noversight is being developed, in addition to providing a general purpose\nframework for monitoring the impact of improvement efforts.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.7545v2"
    },
    {
        "title": "The Political Economy of FDI flows into Developing Countries: Does the\n  depth of International Trade Agreements Matter?",
        "authors": [
            "Arslan Tariq Rana",
            "Mazen Kebewar"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  There is considerable debate whether the domestic political institutions\n(specifically, the country s level of democracy) of the host developing country\ntoward foreign investors are effective in establishing the credibility of\ncommitments are still underway, researchers have also analyzed the effect of\ninternational institutions such as (GATT-WTO) membership and Bilateral\nInvestment treaties (BIT) in their role of establishing the credibility of\ncommitment to attract foreign investments. We argue that there are qualitative\ndifferences among various types of trade agreements and full-fledged trade\nagreements (FTA-CU) provide credibility to foreign investors and democracy\nlevel in the host country conditions this effect whereas the partial scope\nagreements (PSA) are not sufficient in providing credibility of commitments and\nnot moderated by democracy. This paper analyses the impact of heterogeneous\nTAs, and their interaction with domestic institutions, on FDI inflows.\nStatistical analyses for 122 developing countries from 1970 to 2005 support\nthis argument. The method adopted relies on fixed effects estimator which is\nrobust to control endogeneity on a large panel dataset. The strict erogeneity\nof results by using a method suggested by Baier and Bergstrand (2007) and no\nfeedback effect found in sample. The results state that (1) More the FTA-CU\nconcluded, larger the amount of FDI inflows are attracted into the developing\ncountries and PSA are insignificant in determining the FDI inflow; (2) FTA CU\nare complementary to democratic regime whereas the conditional effect of PSA\nwith democracy on levels of FDI inflows is insignificant.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.0139v1"
    },
    {
        "title": "The $500.00 AAPL close: Manipulation or hedging? A quantitative analysis",
        "authors": [
            "Yavni Bar-Yam",
            "Marcus A. M. de Aguiar",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Why do a market's prices move up or down? Claims about causes are made\nwithout actual information, and accepted or dismissed based upon poor or\nnon-existent evidence. Here we investigate the price movements that ended with\nApple stock closing at \\$500.00 on January 18, 2013. There is a ready\nexplanation for this price movement: market manipulation by those who sold\nstock options, who stood to directly benefit from this closing price. Indeed,\none web commentator predicted this otherwise unlikely event publicly. This\nexplanation was subsequently dismissed by press articles that claim that stock\nprices end near such round numbers based upon legitimate hedging activity. But\nhow can we know? We show that the accepted model that points to hedging as the\ndriving cause of prices is not quantitatively consistent with the price\nmovement on that day. The price moved upward too quickly over a period in which\nthe hedgers' position would require selling rather than buying. Under these\nconditions hedgers would have driven the price away from the strike price\nrather than toward it. We also show that a long published theory of the role of\nhedging is incomplete mathematically, and that the correct theory results in\nmuch weaker price movements. This evidence substantially weakens the case of\nthose who claim hedging as cause of anomalous market price movements. The\nexplanation that market manipulation is responsible for the final close cannot\nbe dismissed based upon unsubstantiated, even invalid, hedging claims. Such\nproffered explanations shield potential illegal activity from further inquiry\neven though the claims behind those explanations have not been demonstrated.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.0910v1"
    },
    {
        "title": "Reference Vectors in Economic Choice",
        "authors": [
            "Teycir Abdelghani Goucha"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  In this paper the introduction of notion of reference vector paves the way\nfor a combination of classical and social approaches in the framework of\nreferential preferences given by matrix groups. It is shown that individual\ndemand issue from rational decision does not depend on that reference.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.3424v1"
    },
    {
        "title": "Systemic Losses Due to Counter Party Risk in a Stylized Banking System",
        "authors": [
            "Annika Birch",
            "Tomaso Aste"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We report a study of a stylized banking cascade model investigating systemic\nrisk caused by counter party failure using liabilities and assets to define\nbanks' balance sheet. In our stylized system, banks can be in two states:\nnormally operating or distressed and the state of a bank changes from normally\noperating to distressed whenever its liabilities are larger than the banks'\nassets. The banks are connected through an interbank lending network and,\nwhenever a bank is distressed, its creditor cannot expect the loan from the\ndistressed bank to be repaid, potentially becoming distressed themselves. We\nsolve the problem analytically for a homogeneous system and test the robustness\nand generality of the results with simulations of more complex systems. We\ninvestigate the parameter space and the corresponding distribution of operating\nbanks mapping the conditions under which the whole system is stable or\nunstable. This allows us to determine how financial stability of a banking\nsystem is influenced by regulatory decisions, such as leverage; we discuss the\neffect of central bank actions, such as quantitative easing and we determine\nthe cost of rescuing a distressed banking system using re-capitalisation.\nFinally, we estimate the stability of the UK and US banking systems in the\nyears 2007 and 2012 showing that both banking systems were more unstable in\n2007 and connectedness on the interbank market partly caused the banking\ncrisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.3688v1"
    },
    {
        "title": "Technology Parks Potential for Small and Medium Enterprises",
        "authors": [
            "Anna V. Vilisova",
            "Qiang Fu"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Being one of the most important factors of economic growth of the country,\ninnovations became one of the key vectors in Russian economic policy. In this\nfield technology parks are one of the most effective instruments which can\nprovide growth of innovative activity in sectors, regions and economies. In\nthis paper, we made a model that allows us to evaluate the effect of technology\nparks in the economy of the country and its potential for small and medium\nenterprises. The model is based on a system of coupled equations, whose\nparameters are estimated on the statistical data that reflect the activity of\nthe economic entity, in an environment of this entity the technology parks are\nacting. Typically, there are regression equations linking a number of economic\nfactors with some output indicators. We analyzed the property of increasing the\nshare of surviving small and medium enterprises for Russian conditions as one\nof the effect of technology parks and built a working model for estimating the\nmaximum (limit) values of the effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.5373v1"
    },
    {
        "title": "Micro to macro models for income distribution in the absence and in the\n  presence of tax evasion",
        "authors": [
            "Maria Letizia Bertotti",
            "Giovanni Modanese"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We investigate the effect of tax evasion on the income distribution and the\ninequality index of a society through a kinetic model described by a set of\nnonlinear ordinary differential equations. The model allows to compute the\nglobal outcome of binary and multiple microscopic interactions between\nindividuals. When evasion occurs, both individuals involved in a binary\ninteraction take advantage of it, while the rest of the society is deprived of\na part of the planned redistribution. In general, the effect of evasion on the\nincome distribution is to decrease the population of the middle classes and\nincrease that of the poor and rich classes. We study the dependence of the Gini\nindex on several parameters (mainly taxation rates and evasion rates), also in\nthe case when the evasion rate increases proportionally to a taxation rate\nwhich is perceived by citizens as unfair. Finally, we evaluate the relative\nprobability of class advancement of individuals due to direct interactions and\nwelfare provisions, and some typical temporal rates of convergence of the\nincome distribution to its equilibrium state.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.0015v1"
    },
    {
        "title": "To bail-out or to bail-in? Answers from an agent-based model",
        "authors": [
            "Peter Klimek",
            "Sebastian Poledna",
            "J. Doyne Farmer",
            "Stefan Thurner"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Since beginning of the 2008 financial crisis almost half a trillion euros\nhave been spent to financially assist EU member states in taxpayer-funded\nbail-outs. These crisis resolutions are often accompanied by austerity programs\ncausing political and social friction on both domestic and international\nlevels. The question of how to resolve failing financial institutions under\nwhich economic preconditions is therefore a pressing and controversial issue of\nvast political importance. In this work we employ an agent-based model to study\nthe economic and financial ramifications of three highly relevant crisis\nresolution mechanisms. To establish the validity of the model we show that it\nreproduces a series of key stylized facts if the financial and real economy.\nThe distressed institution can either be closed via a purchase & assumption\ntransaction, it can be bailed-out using taxpayer money, or it may be bailed-in\nin a debt-to-equity conversion. We find that for an economy characterized by\nlow unemployment and high productivity the optimal crisis resolution with\nrespect to financial stability and economic productivity is to close the\ndistressed institution. For economies in recession with high unemployment the\nbail-in tool provides the most efficient crisis resolution mechanism. Under no\ncircumstances do taxpayer-funded bail-out schemes outperform bail-ins with\nprivate sector involvement.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.1548v1"
    },
    {
        "title": "The role of the \"Maximizing Output Growth Inflation Rate\" in monetary\n  policy",
        "authors": [
            "Dominique Pepin"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The paper discusses the role of monetary policy when potential output depends\non the inflation rate. If the intention of the central bank is to maximize\nactual output growth, then it has to be credibly committed to a strict\ninflation targeting rule, and to take the MOGIR (the Maximizing Output Growth\nInflation Rate) as the target.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.6112v1"
    },
    {
        "title": "Contextual and Structural Representations of Market-mediated Economic\n  Value",
        "authors": [
            "Bradly Alicea"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  How do we assign value to economic transactions? To answer this question, we\nmust consider whether the value of objects is inherent, is a product of social\ninteraction, or involves other mechanisms. Economic theory predicts that there\nis an optimal price for any market transaction, and can be observed during\nauctions or other bidding processes. However, there are also social, cultural,\nand cognitive components to the assignation of value, which can be observed in\nboth human and non-human Primate societies. While behaviors related to these\nfactors are embedded in market interactions, they also involve a biological\nsubstrate for the assignation of value (valuation). To synthesize this\ndiversity of perspectives, we will propose that the process of valuation can be\nmodeled computationally and conceived of as a set of interrelated cultural\nevolutionary, cognitive, and neural processes. To do this, contextual geometric\nstructures (CGS) will be placed in an agent-based context (minimal and\ncompositional markets). Objects in the form of computational propositions can\nbe acquired and exchanged, which will determine the value of both singletons\nand linked propositions. Expected results of this model will be evaluated in\nterms of their contribution to understanding human economic phenomena. The\npaper will focus on computational representations and how they correspond to\nreal-world concepts. The implications for evolutionary economics and our\ncontemporary understanding of valuation and market dynamics will also be\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.7021v1"
    },
    {
        "title": "Anatomy of a Bail-In",
        "authors": [
            "Thomas Conlon",
            "John Cotter"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  To mitigate potential contagion from future banking crises, the European\nCommission recently proposed a framework which would provide for the\n$\\textit{bail-in}$ of bank creditors in the event of failure. In this study, we\nexamine this framework retrospectively in the context of failed European banks\nduring the global financial crisis. Empirical findings suggest that equity and\nsubordinated bond holders would have been the main losers from the 535 billion\neuro impairment losses realized by failed European banks. Losses attributed to\nsenior debt holders would, on aggregate, have been proportionally small, while\nno losses would have been imposed on depositors. Cross-country analysis,\nincorporating stress-tests, reveals a divergence of outcomes with subordinated\ndebt holders wiped out in a number of countries, while senior debt holders of\nGreek, Austrian and Irish banks would have required bail-in.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.7628v1"
    },
    {
        "title": "Evolution of wealth in a nonconservative economy driven by local Nash\n  equilibria",
        "authors": [
            "Pierre Degond",
            "Jian-Guo Liu",
            "Christian Ringhofer"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We develop a model for the evolution of wealth in a non-conservative economic\nenvironment, extending a theory developed earlier by the authors. The model\nconsiders a system of rational agents interacting in a game theoretical\nframework. This evolution drives the dynamic of the agents in both wealth and\neconomic configuration variables. The cost function is chosen to represent a\nrisk averse strategy of each agent. That is, the agent is more likely to\ninteract with the market, the more predictable the market, and therefore the\nsmaller its individual risk. This yields a kinetic equation for an effective\nsingle particle agent density with a Nash equilibrium serving as the local\nthermodynamic equilibrium. We consider a regime of scale separation where the\nlarge scale dynamics is given by a hydrodynamic closure with this local\nequilibrium. A class of generalized collision invariants (GCIs) is developed to\novercome the difficulty of the non-conservative property in the hydrodynamic\nclosure derivation of the large scale dynamics for the evolution of wealth\ndistribution. The result is a system of gas dynamics-type equations for the\ndensity and average wealth of the agents on large scales. We recover the\ninverse Gamma distribution, which has been previously considered in the\nliterature, as a local equilibrium for particular choices of the cost function.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.7800v1"
    },
    {
        "title": "The Naive Extrapolation Hypothesis and the Rosy-Gloomy Forecasts",
        "authors": [
            "Vasileios Barmpoutis"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  I study the behavior and the performance of the long-term forecasts issued by\nfinancial analysts with respect to the Extrapolation Hypothesis. That\nhypothesis states that investors, extrapolating from the firms' recent\nperformances, are too optimistic about growth and large firms and too\npessimistic about value and small firms. I find that the forecasting errors are\nhigher for the growth firms and large firms, thus providing support for the\nExtrapolation Hypothesis. However, in addition to the rosy picture of the\ngrowth and large firms, the forecasts of the value and small firms are not so\ngloomy in many cases. My analysis also reveals that expectations move together\nfor all categories of book-to-market and all sizes of firms. I proceed by\ninvestigating some common factors that may influence analysts' long-term\nforecasts, including co-movement and excessive optimism. I find that macro\nfactors beyond a firm's recent performance may influence the formation of\nexpectations.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.1733v1"
    },
    {
        "title": "Decoding Stock Market Behavior with the Topological Quantum Computer",
        "authors": [
            "Ovidiu Racorean"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  A surprising image of the stock market arises if the price time series of all\nDow Jones Industrial Average stock components are represented in one chart at\nonce. The chart evolves into a braid representation of the stock market by\ntaking into account only the crossing of stocks and fixing a convention\ndefining overcrossings and undercrossings. The braid of stocks prices has a\nremarkable connection with the topological quantum computer. Using pairs of\nquasi-particles, called non-abelian anyons, having their trajectories braided\nin time, topological quantum computer can effectively simulate the stock market\nbehavior encoded in the braiding of stocks. In a typically topological quantum\ncomputation process the trajectories of non-abelian anyons are manipulated\naccording to the braiding of stocks and the outcome reflects the probability of\nthe future state of stock market. The probability depends only on the Jones\npolynomial of the knot formed by plat closing the quantum computation. The\nJones polynomial of the knotted stock market acts, making a parallel with the\ncommon financial literature, in a topological quantum computation as a\ncounterpart of a classical technical indicator in trading the stock market. The\ntype of knot stock market formed is an indicator of its future tendencies.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.3531v1"
    },
    {
        "title": "Climate Events and Insurance Demand - The effect of potentially\n  catastrophic events on insurance demand in Italy",
        "authors": [
            "Alessandro Chieppa",
            "Andrea Ricca",
            "Gianluca Rosso"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Climate extreme events are constantly increasing. What is the effect of these\npotentially catastrophic events on insurance demand in Italy, with particular\nreference to the economic activities? Extreme precipitation events over most of\nthe midlatitude land masses and over wet tropical regions will very likely\nbecome more intense and more frequent by the end of this century, as global\nmean surface temperature increases. If we look to Italy, examination of the\nprecipitation time series shows a sensitive and highly significant decrease in\nthe total number of precipitation events in Italy, with a trend of events\nintense dissimilar as regards to low and high intensity, with a decline of\nfirsts and an increase of seconds. The risk related to hydrological natural\ndisasters is in Italy one of the most important problem for both damage and\nnumber of victims. How evolves the ability to pay for damages, with a view to\nsafeguarding work and economic activities, and employment protection?\n",
        "pdf_link": "http://arxiv.org/pdf/1406.4114v1"
    },
    {
        "title": "Realization Utility with Reference-Dependent Preferences",
        "authors": [
            "Jonathan E. Ingersoll Jr.",
            "Lawrence J. Jin"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We develop a tractable model of realization utility that studies the role of\nreference-dependent S-shaped preferences in a dynamic investment setting with\nreinvestment. Our model generates both voluntarily realized gains and losses.\nIt makes specific predictions about the volume of gains and losses, the holding\nperiods, and the sizes of both realized and paper gains and losses that can be\ncalibrated to a variety of statistics, including the Odean measure of the\ndisposition effect. Our model also predicts several anomalies including, among\nothers, the flattening of the capital market line and a negative price for\nidiosyncratic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.2859v1"
    },
    {
        "title": "Hierarchical causality in financial economics",
        "authors": [
            "Diane Wilcox",
            "Tim Gebbie"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Hierarchical analysis is considered and a multilevel model is presented in\norder to explore causality, chance and complexity in financial economics. A\ncoupled system of models is used to describe multilevel interactions,\nconsistent with market data: the lowest level is occupied by agents generating\nthe prices of individual traded assets; the next level entails aggregation of\nstocks into markets; the third level combines shared risk factors with\ninformation variables and bottom-up, agent-generated structure, consistent with\nconditions for no-arbitrage pricing theory; the fourth level describes market\nfactors which originate in the greater economy and the highest levels are\ndescribed by regulated market structure and the customs and ethics which define\nthe nature of acceptable transactions. A mechanism for emergence or innovation\nis considered and causal sources are discussed in terms of five causation\nclasses.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.5585v2"
    },
    {
        "title": "A General Equilibrium Theorem for the Economy of Giving",
        "authors": [
            "W. P. Weijland"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  In [1] we presented a model for transactions when goods are given away in the\nexpectation of a later settlement. In settings where people keep track of their\nsocial accounts we were able to redefine concepts like account balance, yield\ncurve and the law of diminishing returns. In this paper we establish a general\nequilibrium theorem, conjectured in [1], by developing sufficient conditions\nfor any instance of the standard model (or Gift Economy Model) to have a unique\nequilibrium. The convergence to that equilibrium is exponential and for each\npair of entities P and Q the total sum of yields from all mutual transactions\nis equal to zero.\n  [1] W.P. Weijland, Mathematical Foundations for the Economy of Giving, ArXiv\nCategories: q-fin.GN, Report 1401.4664, 2014.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1929v1"
    },
    {
        "title": "Theories of Accounting: Evolution & Developments, Income-Determination\n  and Diversities in Use",
        "authors": [
            "Angus O. Unegbu"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Accounting frameworks follow stipulations of existing Accounting Theories.\nThis exploratory research sets out to trace the evolution of accounting\ntheories of Charge and Discharge Syndrome and the Corollary of Double Entry.\nFurthermore, it dives into the theories of Income Determination, garnishing it\nwith areas of diversities in the use of Accounting Information while review of\ntheories of recent growths and developments in Accounting are not left out. The\nmethod of research adopted is exploratory review of existing accounting\nliterature. It is observed that the emergence of these theories exist to\nminimize fraud, errors, misappropriations and pilfering of Corporate assets. It\nis recommended that implementation prescriptions of these theories by\nInternational Financial Reporting Standard Committee and Practicing Accountants\nshould be adhered to and simplified so as to avoid confusing and scandalous\nreporting of financial statements\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4633v1"
    },
    {
        "title": "Direct Foreign Investment in Kurdistan Region of Middle-East: Non-Oil\n  Sector Analysis",
        "authors": [
            "Angus O. Unegbu",
            "Augustine Okanlawon"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Kurdistan Region is a tourist hub. This research analyzes other Non-Oil\nSectors that have huge attractions of Foreign Direct Investments into the\nKurdistan Region from 2005 to 2013. Comparative analysis was carried out\nbetween Iraq and the Region, and among influential Sectors of the Economy.\nT-test and ANOVA are statistical tools employed in testing the research\nhypotheses. The research identify that there exist significant Foreign Direct\nInvestment inflows across the governorates in the region and among influential\nsectors of the Economy. The research also highlighted areas of high level of\ninvestment needs, sectors that have been crowded out and business opportunities\nin the region that requires huge Foreign Direct Investments. It is recommended\nthat the Regional Kurdistan Government should embark on fiscal Cashless\npolicies in order to stimulate further spill-off effects of attracting enormous\nNon-Oil Sectors of Foreign Direct Investments into the region.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00218v1"
    },
    {
        "title": "Mergers and acquisitions transactions strategies in diffusion - type\n  financial systems in highly volatile global capital markets with\n  nonlinearities",
        "authors": [
            "Dimitri O. Ledenyov",
            "Viktor O. Ledenyov"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The M and A transactions represent a wide range of unique business\noptimization opportunities in the corporate transformation deals, which are\nusually characterized by the high level of total risk. The M and A transactions\ncan be successfully implemented by taking to an account the size of\ninvestments, purchase price, direction of transaction, type of transaction, and\nusing the modern comparable transactions analysis and the business valuation\ntechniques in the diffusion type financial systems in the finances. We\ndeveloped the MicroMA software program with the embedded optimized\nnear-real-time artificial intelligence algorithm to create the winning virtuous\nM and A strategies, using the financial performance characteristics of the\ninvolved firms, and to estimate the probability of the M and A transaction\ncompletion success. We believe that the fluctuating dependence of M and A\ntransactions number over the certain time period is quasi periodic. We think\nthat there are many factors, which can generate the quasi periodic oscillations\nof the M and A transactions number in the time domain, for example: the stock\nmarket bubble effects. We performed the research of the nonlinearities in the M\nand A transactions number quasi-periodic oscillations in Matlab, including the\nideal, linear, quadratic, and exponential dependences. We discovered that the\naverage of a sum of random numbers in the M and A transactions time series\nrepresents a time series with the quasi periodic systematic oscillations, which\ncan be finely approximated by the polynomial numbers. We think that, in the\ncourse of the M and A transaction implementation, the ability by the companies\nto absorb the newly acquired knowledge and to create the new innovative\nknowledge bases, is a key predeterminant of the M and A deal completion success\nas in Switzerland.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.02537v1"
    },
    {
        "title": "Forecasting the term structure of crude oil futures prices with neural\n  networks",
        "authors": [
            "Jozef Barunik",
            "Barbora Malinska"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The paper contributes to the rare literature modeling term structure of crude\noil markets. We explain term structure of crude oil prices using dynamic\nNelson-Siegel model, and propose to forecast them with the generalized\nregression framework based on neural networks. The newly proposed framework is\nempirically tested on 24 years of crude oil futures prices covering several\nimportant recessions and crisis periods. We find 1-month, 3-month, 6-month and\n12-month-ahead forecasts obtained from focused time-delay neural network to be\nsignificantly more accurate than forecasts from other benchmark models. The\nproposed forecasting strategy produces the lowest errors across all times to\nmaturity.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.04819v1"
    },
    {
        "title": "Generalization of the Aoki-Yoshikawa sectoral productivity model based\n  on extreme physical information principle",
        "authors": [
            "Ilona Bednarek",
            "Marcin Makowski",
            "Edward W. Piotrowski",
            "Jan Sładkowski",
            "Jacek Syska"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  This paper presents a continuous variable generalization of the\nAoki-Yoshikawa sectoral productivity model. Information theoretical methods\nfrom the Frieden-Soffer extreme physical information statistical estimation\nmethodology were used to construct exact solutions. Both approaches coincide in\nfirst order approximation. The approach proposed here can be successfully\napplied in other fields of research.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.07604v1"
    },
    {
        "title": "Phase Transitions, Renormalization and Yang-Lee Zeros in Stock Markets",
        "authors": [
            "J. L. Subias"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The present paper analyses the formal parallelism existing between the laws\nof thermodynamics and some economic principles. Based on previous works, we\nshall show how the existence in Economics of principles analogous to those in\nthermodynamics involves the occurrence of economic events that remind of\nwell-known phenomenological thermodynamic paradigms (i.e., the magnetocaloric\neffect and population inversion). We shall also show how the phase transition\nand renormalization theory provides a natural framework to understand and\npredict trend changes in stock markets. Finally, current negotiation strategies\nin financial markets are briefly reviewed.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.00471v1"
    },
    {
        "title": "A Profit-maximization Model for a Company that Sells an Arbitrary Number\n  of Products",
        "authors": [
            "Dragos-Patru Covei"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  One of the problems faced by a firm that sells certain commodities is to\ndetermine the number of products that it must supply in order to maximize its\nprofit. In this article, the authors give an answer to this problem of economic\ninterest. The proposed problem is a generalization of the results obtained by\nStirzaker (Probability and Random Variables: A Beginner's Guide, 1999) and\nKupferman (Lecture Notes in Probability, 2009) where the authors do not present\na situation where the sale of a quantity from some commodities is constrained\nby the marketing of another. In addition, the described procedure is simple and\ncan be successfully applied to any number of commodities. The obtained results\ncan be easily put into practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.02644v2"
    },
    {
        "title": "Forecasting Financial Extremes: A Network Degree Measure of\n  Super-exponential Growth",
        "authors": [
            "Wanfeng Yan",
            "Edgar van Tuyll van Serooskerken"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Investors in stock market are usually greedy during bull markets and scared\nduring bear markets. The greed or fear spreads across investors quickly. This\nis known as the herding effect, and often leads to a fast movement of stock\nprices. During such market regimes, stock prices change at a super-exponential\nrate and are normally followed by a trend reversal that corrects the previous\nover reaction. In this paper, we construct an indicator to measure the\nmagnitude of the super-exponential growth of stock prices, by measuring the\ndegree of the price network, generated from the price time series. Twelve major\ninternational stock indices have been investigated. Error diagram tests show\nthat this new indicator has strong predictive power for financial extremes,\nboth peaks and troughs. By varying the parameters used to construct the error\ndiagram, we show the predictive power is very robust. The new indicator has a\nbetter performance than the LPPL pattern recognition indicator.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04060v1"
    },
    {
        "title": "Quantum Gates and Quantum Circuits of Stock Portfolio",
        "authors": [
            "Ovidiu Racorean"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In quantum computation, series of quantum gates have to be arranged in a\npredefined sequence that led to a quantum circuit in order to solve a\nparticular problem. What if the sequence of quantum gates is known but both the\nproblem to be solved and the outcome of the so defined quantum circuit remain\nin the shadow? This is the situation of the stock market. The price time series\nof a portfolio of stocks are organized in braids that effectively simulate\nquantum gates in the hypothesis of Ising anyons quantum computational model.\nFollowing the prescriptions of Ising anyons model, 1-qubit quantum gates are\nconstructed for portfolio composed of four stocks. Adding two additional stocks\nat the initial portfolio result in 2-qubits quantum gates and circuits.\nHadamard gate, Pauli gates or controlled-Z gate are some of the elementary\nquantum gates that are identified in the stock market structure. Addition of\nother pairs of stocks, that eventually represent a market index, like Dow Jones\nindustrial Average, it results in a sequence of n-qubits quantum gates that\nform a quantum code. Deciphering this mysterious quantum code of the stock\nmarket is an issue for future investigations.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.02310v2"
    },
    {
        "title": "Measuring Financial Sentiment to Predict Financial Instability: A New\n  Approach based on Text Analysis",
        "authors": [
            "Paul Ormerod",
            "Rickard Nyman",
            "David Tuckett"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Following the financial crisis of the late 2000s, policy makers have shown\nconsiderable interest in monitoring financial stability. Several central banks\nnow publish indices of financial stress, which are essentially based upon\nmarket related data. In this paper, we examine the potential for improving the\nindices by deriving information about emotion shifts in the economy. We report\non a new approach, based on the content analysis of very large text databases,\nand termed directed algorithmic text analysis. The algorithm identifies, very\nrapidly, shifts through time in the relations between two core emotional\ngroups. The method is robust. The same word-list is used to identify the two\nemotion groups across different studies. Membership of the words in the lists\nhas been validated in psychological experiments. The words consist of everyday\nEnglish words with no specific economic meaning. Initial results show promise.\nAn emotion index capturing shifts between the two emotion groups in texts\npotentially referring to the whole US economy improves the one-quarter ahead\nconsensus forecasts for real GDP growth. More specifically, the same indices\nare shown to Granger cause both the Cleveland and St Louis Federal Reserve\nIndices of Financial Stress.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05357v1"
    },
    {
        "title": "The effect of stock market indexing on corporate tax avoidance",
        "authors": [
            "Alex Young"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Membership in the Russell 1000 and 2000 Indices is based on a ranking of\nmarket capitalization in May. Each index is separately value weighted such that\nfirms just inside the Russell 2000 are comparable in size to firms just outside\n(i.e. at the bottom of the Russell 1000) but have much higher index weights.\nThese features allow for the the annual reconstitution of these indices to be\nused as part of a regression discontinuity design to identify the effect of\nstock market indexing. Using this design, I investigate whether stock market\nindexing affects corporate tax avoidance. I find no evidence that firms just\ninside the Russell 2000 have significantly different effective tax rates than\nfirms just outside.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.00136v1"
    },
    {
        "title": "Impact of Artificial Intelligence on Economic Theory",
        "authors": [
            "Tshilidzi Marwala"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Artificial intelligence has impacted many aspects of human life. This paper\nstudies the impact of artificial intelligence on economic theory. In particular\nwe study the impact of artificial intelligence on the theory of bounded\nrationality, efficient market hypothesis and prospect theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01213v1"
    },
    {
        "title": "An Introduction to Business Mathematics",
        "authors": [
            "Henk van Elst"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  These lecture notes provide a self-contained introduction to the mathematical\nmethods required in a Bachelor degree programme in Business, Economics, or\nManagement. In particular, the topics covered comprise real-valued vector and\nmatrix algebra, systems of linear algebraic equations, Leontief's stationary\ninput-output matrix model, linear programming, elementary financial\nmathematics, as well as differential and integral calculus of real-valued\nfunctions of one real variable. A special focus is set on applications in\nquantitative economical modelling.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.04333v2"
    },
    {
        "title": "Managing Cellular Billing Plan Switchings",
        "authors": [
            "Valery Vilisov"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Here we shall consider a very popular practical applied problem of managing\nmode switching (in this work we are considering managing billing plans). Out of\nthe two parties (service provider and service consumer), participating in the\nprocesses modelled here, we shall consider only a consumer type of a problem.\nHerein we provide formal characterization of the problem as well as the\nelements necessary for its solution. We shall consider full predicted costs,\noriginating when switching to a billing plan as a target index. The work\ncontains an example that provides a detailed view of the application technology\nreferring to the suggested problem solution algorithm. Using the example's data\nwe have performed the analysis measuring the problem's sensitivity in relation\nto the growth of the traffic volume. Herein we provided a polynomial\napproximation of the target index value depending on the traffic volume.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.05943v1"
    },
    {
        "title": "Gender income disparity in the USA: analysis and dynamic modelling",
        "authors": [
            "Ivan Kitov",
            "Oleg Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We analyze and develop a quantitative model describing the evolution of\npersonal income distribution, PID, for males and females in the U.S. between\n1930 and 2014. The overall microeconomic model, which we introduced ten years\nago, accurately predicts the change in mean income as a function of age as well\nas the dependence on age of the portion of people distributed according to the\nPareto law. As a result, we have precisely described the change in Gini ratio\nsince the start of income measurements in 1947. The overall population consists\nof two genders, however, which have different income distributions. The\ndifference between incomes earned by male and female population has been\nexperiencing dramatic changes over time. Here, we model the internal dynamics\nof men and women PIDs separately and then describe their relative contribution\nto the overall PID. Our original model is refined to match all principal\ngender-dependent observations. We found that women in the U.S. are deprived of\nhigher job positions. This is the cause of the long term income inequality\nbetween males and females in the U.S. It is unjust to women and has a negative\neffect on real economic growth. Women have been catching up since the 1960s and\nthat improves the performance of the U.S. economy. It will take decades,\nhowever, to full income equality between genders. There are no new defining\nparameters included in the model except the critical age, when people start to\nlose their incomes, was split into two critical ages for low-middle incomes and\nthe highest incomes, which obey a power law distribution. Such an extension\nbecomes necessary in order to match the observation that the female population\nin the earlier 1960s was practically not represented in the highest incomes.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.02752v1"
    },
    {
        "title": "How universal is the law of income distribution? Cross country\n  comparison",
        "authors": [
            "Ivan Kitov",
            "Oleg Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The evolution of personal income distribution (PID) in four countries:\nCanada, New Zealand, the UK, and the USA follows a unique trajectory. We have\nrevealed precise match in the shape of two age-dependent features of the PID:\nmean income and the portion of people with the highest incomes (2 to 5% of the\nworking age population). Because of the U.S. economic superiority, as expressed\nby real GDP per head, the curves of mean income and the portion of rich people\ncurrently observed in three chasing countries one-to-one reproduce the curves\nmeasured in the USA 15 to 25 years before. This result of cross country\ncomparison implies that the driving force behind the PID evolution is the same\nin four studied countries. Our parsimonious microeconomic model, which links\nthe change in PID only with one exogenous parameter - real GDP per capita,\naccurately predicts all studied features for the U.S. This study proves that\nour quantitative model, based on one first-order differential equation, is\nuniversal. For example, new observations in Canada, New Zealand, and the UK\nconfirm our previous finding that the age of maximum mean income is defined by\nthe root-square dependence on real GDP per capita.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.02754v1"
    },
    {
        "title": "The Insecure Future of the World Economic Growth",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Growth rate of the world Growth Domestic Product (GDP) is analysed to\ndetermine possible pathways of the future economic growth. The analysis is\nbased on using the latest data of the World Bank and it reveals that the growth\nrate between 1960 and 2014 was following a trajectory approaching\nasymptotically a constant value. The most likely prediction is that the world\neconomic growth will continue to increase exponentially and that it will become\nunsustainable possibly even during the current century. A more optimistic but\nless realistic prediction is based on the assumption that the growth rate will\nstart to decrease linearly. In this case, the world economic growth is\npredicted to reach a maximum, if the growth rate is going to decrease linearly\nwith time, or to follow a logistic trajectory, if the growth rate is going to\ndecrease linearly with the size of the world GDP.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07928v2"
    },
    {
        "title": "Computer-Suported Risk Identification for the Holistic Management of\n  Risks",
        "authors": [
            "Jochen L. Leidner"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Risk is part of the fabric of every business; surprisingly, there is little\nwork on establishing best practices for systematic, repeatable risk\nidentification, arguably the first step of any risk management process. In this\npaper, we present a proposal that constitutes a more holistic risk management\napproach, a methodology for computer-supported risk identification is proposed\nthat may lead to more consistent (objective, repeatable) risk analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.08285v1"
    },
    {
        "title": "Should employers pay their employees better? An asset pricing approach",
        "authors": [
            "Sebastien Valeyre",
            "Denis Grebenkov",
            "Sofiane Aboura",
            "Francois Bonnin"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We uncover a new anomaly in asset pricing that is linked to the remuneration:\nthe more a company spends on salaries and benefits per employee, the better its\nstock performs, on average. Moreover, the companies adopting similar\nremuneration policies share a common risk, which is comparable to that of the\nvalue premium. For this purpose,we set up an original methodology that uses\nfirm financial characteristics to build factors that are less correlated than\nin the standard asset pricing methodology. We quantify the importance of these\nfactors from an asset pricing perspective by introducing the factor correlation\nlevel as a directly accessible proxy of eigenvalues of the correlation matrix.\nA rational explanation of the remuneration anomaly involves the positive\ncorrelation between pay and employee performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.00931v2"
    },
    {
        "title": "Bunching of numbers in a non-ideal roulette: the key to winning\n  strategies",
        "authors": [
            "A. V. Kavokin",
            "A. S. Sheremet",
            "M. Yu. Petrov"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Chances of a gambler are always lower than chances of a casino in the case of\nan ideal, mathematically perfect roulette, if the capital of the gambler is\nlimited and the minimum and maximum allowed bets are limited by the casino.\nHowever, a realistic roulette is not ideal: the probabilities of realisation of\ndifferent numbers slightly deviate. Describing this deviation by a statistical\ndistribution with a width {\\delta} we find a critical {\\delta} that equalizes\nchances of gambler and casino in the case of a simple strategy of the game: the\ngambler always puts equal bets to the last N numbers. For up-critical {\\delta}\nthe expected return of the roulette becomes positive. We show that the dramatic\nincrease of gambler's chances is a manifestation of bunching of numbers in a\nnon-ideal roulette. We also estimate the critical starting capital needed to\nensure the low risk game for an indefinite time.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.06943v1"
    },
    {
        "title": "Contagion in the world's stock exchanges seen as a set of coupled\n  oscillators",
        "authors": [
            "Lucia Bellenzier",
            "Jørgen Vitting Andersen",
            "Giulia Rotundo"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We study how the phenomenon of contagion can take place in the network of the\nworld's stock exchanges due to the behavioral trait \"blindeness to small\nchanges\". On large scale individual, the delay in the collective response may\nsignificantly change the dynamics of the overall system. We explicitely insert\na term describing the behavioral phenomenon in a system of equations that\ndescribe the build and release of stress across the worldwide stock markets. In\nthe mathematical formulation of the model, each stock exchange acts as an\nintegrate-and-fire oscillator. Calibration on market data validate the model.\n  One advantage of the integrate-and-fire dynamics is that it enables for a\ndirect identification of cause and effect of price movements, without the need\nfor statistical tests such as for example Granger causality tests often used in\nthe identification of causes of contagion. Our methodology can thereby identify\nthe most relevant nodes with respect to onset of contagion in the network of\nstock exchanges, as well as identify potential periods of high vulnerability of\nthe network. The model is characterized by a separation of time scales created\nby a slow build up of stresses, for example due to (say monthly/yearly)\nmacroeconomic factors, and then a fast (say hourly/daily) release of stresses\nthrough \"price-quakes\" of price movements across the worlds network of stock\nexchanges.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07452v1"
    },
    {
        "title": "No such thing as a risk-neutral market",
        "authors": [
            "D. L. Wilcox"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  A very brief history of relative valuation in neoclassical finance since 1973\nis presented, with attention to core currency issues for emerging economies.\nPrice formation is considered in the context of hierarchical causality, with\ndiscussion focussed on identifying mathematical modelling challenges for robust\nand transparent regulation of interactions.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.08429v1"
    },
    {
        "title": "Microscopic models for the study of taxpayer audit effects",
        "authors": [
            "M. L. Bertotti",
            "G. Modanese"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  A microscopic dynamic model is here constructed and analyzed, describing the\nevolution of the income distribution in the presence of taxation and\nredistribution in a society in which also tax evasion and auditing processes\noccur. The focus is on effects of enforcement regimes, characterized by\ndifferent choices of the audited taxpayer fraction and of the penalties imposed\nto noncompliant individuals. A complex systems perspective is adopted: society\nis considered as a system composed by a large number of heterogeneous\nindividuals. These are divided into income classes and may as well have\ndifferent tax evasion behaviors. The variation in time of the number of\nindividuals in each class is described by a system of nonlinear differential\nequations of the kinetic discretized Boltzmann type involving transition\nprobabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.08467v1"
    },
    {
        "title": "Corruption and Wealth: Unveiling a national prosperity syndrome in\n  Europe",
        "authors": [
            "Juan C. Correa",
            "Klaus Jaffe"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Data mining revealed a cluster of economic, psychological, social and\ncultural indicators that in combination predicted corruption and wealth of\nEuropean nations. This prosperity syndrome of self-reliant citizens, efficient\ndivision of labor, a sophisticated scientific community, and respect for the\nlaw, was clearly distinct from that of poor countries that had a diffuse\nrelationship between high corruption perception, low GDP/capita, high social\ninequality, low scientific development, reliance on family and friends, and\nlanguages with many words for guilt. This suggests that there are many ways for\na nation to be poor, but few ones to become rich, supporting the existence of\nsynergistic interactions between the components in the prosperity syndrome\nfavoring economic growth. No single feature was responsible for national\nprosperity. Focusing on synergies rather than on single features should improve\nour understanding of the transition from poverty and corruption to prosperity\nin European nations and elsewhere.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.00283v1"
    },
    {
        "title": "The Topology of African Exports: emerging patterns on spanning trees",
        "authors": [
            "Tanya Araújo",
            "M. Ennes Ferreira"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  This paper is a contribution to interweaving two lines of research that have\nprogressed in separate ways: network analyses of international trade and the\nliterature on African trade and development. Gathering empirical data on\nAfrican countries has important limitations and so does the space occupied by\nAfrican countries in the analyses of trade networks. Here, these limitations\nare dealt with by the definition of two independent bipartite networks: a\ndestination share network and\\ a\\ commodity share network. These networks -\ntogether with their corresponding minimal spanning trees - allow to uncover\nsome ordering emerging from African exports in the broader context of\ninternational trade. The emerging patterns help to understand important\ncharacteristics of African exports and its binding relations to other economic,\ngeographic and organizational concerns as the recent literature on African\ntrade, development and growth has shown.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.03522v1"
    },
    {
        "title": "Risk Aversion and Catastrophic Risks: the Pill Experiment",
        "authors": [
            "Julien Blasco",
            "Graciela Chichilnisky"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  This article focuses on the work of O. Chanel and G. Chichilnisky (2013) on\nthe flaws of expected utility theory while assessing the value of life.\nExpected utility is a fundamental tool in decision theory. However, it does not\nfit with the experimental results when it comes to catastrophic outcomes\n---see, for example, Chichilnisky (2009) for more details. In the experiments\nconducted by Olivier Chanel in 1998 and 2009, several subjects are ask to\nimagine they are presented 1 billion identical pills. They are paid \\$220,000\nto take and swallow one, knowing that one out of 1 billion is deadly. The\nobjective of this article is to show that risk aversion phenomenon cannot\nexplain the experimental results found. This is an additional reason why a new\nkind of utility function is necessary: the axioms proposed by Graciela\nChichilnisky will be briefly presented, and it will be shown that it better\nfits with experiments than any risk aversion utility function.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.05672v1"
    },
    {
        "title": "On the \"usual\" misunderstandings between econophysics and finance: some\n  clarifications on modelling approaches and efficient market hypothesis",
        "authors": [
            "Marcel Ausloos",
            "Franck Jovanovic",
            "Christophe Schinckus"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  In line with the recent research and debates about econophysics and financial\neconomics, this article discusses on usual misunderstandings between the two\ndisciplines in terms of modelling and basic hypotheses. In the literature\ndevoted to econophysics, the methodology used by financial economists is\nfrequently considered as a top-down approach (starting from a priori \"first\nprinciples\") while econophysicists rather present themselves as scholars\nworking with a (empirical data prone) bottom-up approach. Although this dualist\nperspective is very common in the econophysics literature, this paper claims\nthat the distinction is very confusing and does not permit to reveal the\nessence of the differences between finance and econophysics. The distinction\nbetween these two fields is mainly investigated here through the lens of the\nEfficient Market Hypothesis in order to show that, in substance, econophysics\nand financial economics tend to have a similar approach implying that the\nmisunderstanding between these two fields at the modelling level can therefore\nbe overstepped.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.02045v1"
    },
    {
        "title": "Local Operators in Kinetic Wealth Distribution",
        "authors": [
            "M. Andrecut"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The statistical mechanics approach to wealth distribution is based on the\nconservative kinetic multi-agent model for money exchange, where the local\ninteraction rule between the agents is analogous to the elastic particle\nscattering process. Here, we discuss the role of a class of conservative local\noperators, and we show that, depending on the values of their parameters, they\ncan be used to generate all the relevant distributions. We also show\nnumerically that in order to generate the power-law tail an heterogeneous risk\naversion model is required. By changing the parameters of these operators one\ncan also fine tune the resulting distributions in order to provide support for\nthe emergence of a more egalitarian wealth distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.04790v1"
    },
    {
        "title": "A New Currency of the Future: The Novel Commodity Money with Attenuation\n  Coefficient Based on the Logistics Cost of Anchor",
        "authors": [
            "Boliang Lin",
            "Ruixi Lin"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  In this paper, we reveal the attenuation mechanism of anchor of the commodity\nmoney from the perspective of logistics warehousing costs, and propose a novel\nDecayed Commodity Money (DCM) for the store of value across time and space.\nConsidering the logistics cost of commodity warehousing by the third financial\ninstitution such as London Metal Exchange, we can award the difference between\nthe original and the residual value of the anchor to the financial institution.\nThis type of currency has the characteristic of self-decaying value over time.\nTherefore DCM has the advantages of both the commodity money which has the\nfunction of preserving wealth and credit currency without the logistics cost.\nIn addition, DCM can also avoid the defects that precious metal money is\nhoarded by market and credit currency often leads to excessive liquidity. DCM\nis also different from virtual currency, such as bitcoin, which does not have a\ncorresponding commodity anchor. As a conclusion, DCM can provide a new way of\nstoring wealth for nations, corporations and individuals effectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.06948v1"
    },
    {
        "title": "Evolutionary Model of Stock Markets",
        "authors": [
            "Joachim Kaldasch"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The paper presents an evolutionary economic model for the price evolution of\nstocks. Treating a stock market as a self-organized system governed by a fast\npurchase process and slow variations of demand and supply the model suggests\nthat the short term price distribution has the form a logistic (Laplace)\ndistribution. The long term return can be described by Laplace-Gaussian mixture\ndistributions. The long term mean price evolution is governed by a Walrus\nequation, which can be transformed into a replicator equation. This allows\nquantifying the evolutionary price competition between stocks. The theory\nsuggests that stock prices scaled by the price over all stocks can be used to\ninvestigate long-term trends in a Fisher-Pry plot. The price competition that\nfollows from the model is illustrated by examining the empirical long-term\nprice trends of two stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.01248v1"
    },
    {
        "title": "Toward an integrated workforce planning framework using structured\n  equations",
        "authors": [
            "Marie Doumic",
            "Benoît Perthame",
            "Edouard Ribes",
            "Delphine Salort",
            "Nathan Toubiana"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Strategic Workforce Planning is a company process providing best in class,\neconomically sound, workforce management policies and goals. Despite the\nabundance of literature on the subject, this is a notorious challenge in terms\nof implementation. Reasons span from the youth of the field itself to broader\ndata integration concerns that arise from gathering information from financial,\nhuman resource and business excellence systems. This paper aims at setting the\nfirst stones to a simple yet robust quantitative framework for Strategic\nWorkforce Planning exercises. First a method based on structured equations is\ndetailed. It is then used to answer two main workforce related questions: how\nto optimally hire to keep labor costs flat? How to build an experience\nconstrained workforce at a minimal cost?\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02349v2"
    },
    {
        "title": "Tail protection for long investors: Trend convexity at work",
        "authors": [
            "Tung-Lam Dao",
            "Trung-Tu Nguyen",
            "Cyril Deremble",
            "Yves Lempérière",
            "Jean-Philippe Bouchaud",
            "Marc Potters"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The performance of trend following strategies can be ascribed to the\ndifference between long-term and short-term realized variance. We revisit this\ngeneral result and show that it holds for various definitions of trend\nstrategies. This explains the positive convexity of the aggregate performance\nof Commodity Trading Advisors (CTAs) which -- when adequately measured -- turns\nout to be much stronger than anticipated. We also highlight interesting\nconnections with so-called Risk Parity portfolios. Finally, we propose a new\nportfolio of strangle options that provides a pure exposure to the long-term\nvariance of the underlying, offering yet another viewpoint on the link between\ntrend and volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02410v1"
    },
    {
        "title": "Secular bipolar growth rate of the real US GDP per capita: implications\n  for understanding past and future economic growth",
        "authors": [
            "Sandro Lera",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We present a quantitative characterisation of the fluctuations of the\nannualized growth rate of the real US GDP per capita growth at many scales,\nusing a wavelet transform analysis of two data sets, quarterly data from 1947\nto 2015 and annual data from 1800 to 2010. Our main finding is that the\ndistribution of GDP growth rates can be well approximated by a bimodal function\nassociated to a series of switches between regimes of strong growth rate\n$\\rho_\\text{high}$ and regimes of low growth rate $\\rho_\\text{low}$. The\nsuccession of such two regimes compounds to produce a remarkably stable long\nterm average real annualized growth rate of 1.6\\% from 1800 to 2010 and\n$\\approx 2.0\\%$ since 1950, which is the result of a subtle compensation\nbetween the high and low growth regimes that alternate continuously. Thus, the\noverall growth dynamics of the US economy is punctuated, with phases of strong\ngrowth that are intrinsically unsustainable, followed by corrections or\nconsolidation until the next boom starts. We interpret these findings within\nthe theory of \"social bubbles\" and argue as a consequence that estimations of\nthe cost of the 2008 crisis may be misleading. We also interpret the absence of\nstrong recovery since 2008 as a protracted low growth regime $\\rho_\\text{low}$\nassociated with the exceptional nature of the preceding large growth regime.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04136v1"
    },
    {
        "title": "Sectoral co-movements in the Indian stock market: A mesoscopic network\n  analysis",
        "authors": [
            "Kiran Sharma",
            "Shreyansh Shah",
            "Anindya S. Chakrabarti",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  In this article we review several techniques to extract information from\nstock market data. We discuss recurrence analysis of time series, decomposition\nof aggregate correlation matrices to study co-movements in financial data,\nstock level partial correlations with market indices, multidimensional scaling\nand minimum spanning tree. We apply these techniques to daily return time\nseries from the Indian stock market. The analysis allows us to construct\nnetworks based on correlation matrices of individual stocks in one hand and on\nthe other, we discuss dynamics of market indices. Thus both micro level and\nmacro level dynamics can be analyzed using such tools. We use the\nmulti-dimensional scaling methods to visualize the sectoral structure of the\nstock market, and analyze the comovements among the sectoral stocks. Finally,\nwe construct a mesoscopic network based on sectoral indices. Minimum spanning\ntree technique is seen to be extremely useful in order to separate\ntechnologically related sectors and the mapping corresponds to actual\nproduction relationship to a reasonable extent.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.05514v1"
    },
    {
        "title": "The Rank Effect for Commodities",
        "authors": [
            "Ricardo T. Fernholz",
            "Christoffer Koch"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We uncover a large and significant low-minus-high rank effect for commodities\nacross two centuries. There is nothing anomalous about this anomaly, nor is it\nclear how it can be arbitraged away. Using nonparametric econometric methods,\nwe demonstrate that such a rank effect is a necessary consequence of a\nstationary relative asset price distribution. We confirm this prediction using\ndaily commodity futures prices and show that a portfolio consisting of\nlower-ranked, lower-priced commodities yields 23% higher annual returns than a\nportfolio consisting of higher-ranked, higher-priced commodities. These excess\nreturns have a Sharpe ratio nearly twice as high as the U.S. stock market yet\nare uncorrelated with market risk. In contrast to the extensive literature on\nasset pricing factors and anomalies, our results are structural and rely on\nminimal and realistic assumptions for the long-run behavior of relative asset\nprices.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.07510v1"
    },
    {
        "title": "What is the Contribution of Intra-household Inequality to Overall Income\n  Inequality? Evidence from Global Data, 1973-2013",
        "authors": [
            "Deepak Malghan",
            "Hema Swaminathan"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Intra-household inequality continues to remain a neglected corner despite\nrenewed focus on income and wealth inequality. Using the LIS micro data, we\npresent evidence that this neglect is equivalent to ignoring up to a third of\ntotal inequality. For a wide range of countries and over four decades, we show\nthat at least 30 per cent of total inequality is attributable to inequality\nwithin the household. Using a simple normative measure of inequality, we\ncomment on the welfare implications of these trends.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.08210v1"
    },
    {
        "title": "The loss of interest for the euro in Romania",
        "authors": [
            "Claudiu Albulescu",
            "Dominique Pépin"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We generalize a money demand micro-founded model to explain Romanians' recent\nloss of interest for the euro. We show that the reason behind this loss of\ninterest is a severe decline in the relative degree of the euro liquidity\nagainst that of the Romanian leu.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.01900v1"
    },
    {
        "title": "A Simple Model of Credit Expansion",
        "authors": [
            "Alexander Smirnov"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The proposed model is aimed to reveal important patterns in the behavior of a\nsimplified financial system. The patterns could be detected as regular cycles\nconsisting of debt bubbles and crises. Financial cycles have a well defined\nstructure and form periodic sequences along the axis of credit expansion while\nretaining stochastic nature in terms of time.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05055v1"
    },
    {
        "title": "Copula-Based Univariate Time Series Structural Shift Identification Test",
        "authors": [
            "Henry Penikas"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  An approach is proposed to determine structural shift in time-series assuming\nnon-linear dependence of lagged values of dependent variable. Copulas are used\nto model non-linear dependence of time series components.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05056v1"
    },
    {
        "title": "Clustering Approaches for Financial Data Analysis: a Survey",
        "authors": [
            "Fan Cai",
            "Nhien-An Le-Khac",
            "Tahar Kechadi"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Nowadays, financial data analysis is becoming increasingly important in the\nbusiness market. As companies collect more and more data from daily operations,\nthey expect to extract useful knowledge from existing collected data to help\nmake reasonable decisions for new customer requests, e.g. user credit category,\nconfidence of expected return, etc. Banking and financial institutes have\napplied different data mining techniques to enhance their business performance.\nAmong these techniques, clustering has been considered as a significant method\nto capture the natural structure of data. However, there are not many studies\non clustering approaches for financial data analysis. In this paper, we\nevaluate different clustering algorithms for analysing different financial\ndatasets varied from time series to transactions. We also discuss the\nadvantages and disadvantages of each method to enhance the understanding of\ninner structure of financial datasets as well as the capability of each\nclustering method in this context.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.08520v1"
    },
    {
        "title": "Inventory growth cycles with debt-financed investment",
        "authors": [
            "Matheus Grasselli",
            "Adrien Nguyen-Huu"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We propose a continuous-time stock-flow consistent model for inventory\ndynamics in an economy with firms, banks, and households. On the supply side,\nfirms decide on production based on adaptive expectations for sales demand and\na desired level of inventories. On the demand side, investment is determined as\na function of utilization and profitability and can be financed by debt,\nwhereas consumption is independently determined as a function of income and\nwealth. Prices adjust sluggishly to both changes in labour costs and inventory.\nDisequilibrium between expected sales and demand is absorbed by unplanned\nchanges in inventory. This results in a five-dimensional dynamical system for\nwage share, employment rate, private debt ratio, expected sales, and capacity\nutilization. We analyze two limiting cases: the long-run dynamics provides a\nversion of the Keen model with effective demand and varying inventories,\nwhereas the short-run dynamics gives rise to behaviour that we interpret as\nKitchin cycles.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00955v1"
    },
    {
        "title": "Administration Costs in the Management of Research Funds; A Case Study\n  of a Public Fund for the Promotion of Industrial Innovation",
        "authors": [
            "David R Walwyn"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Research funding agencies routinely use a proportion of their total revenues\nto support internal administration and marketing costs. The ratio of\nadministration to total costs, referred to as the administration ratio, is\nhighly variable and within any single fund depends on many factors including\nthe number and average size of projects and the overall efficiency of the\nfunding agency. In this study, the standard agency activities have been\nidentified and used to develop a model of administration costs against expected\noutcomes. In particular, the model has been designed to estimate the optimum\nportfolio success rate and administration ratio as a function of a range of key\ninput variables including the project size, the complexity of proposal\nevaluation and project management, the risk tolerance of the sponsor and the\ntargeted research domain.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.01645v1"
    },
    {
        "title": "Understanding the Tracking Errors of Commodity Leveraged ETFs",
        "authors": [
            "Kevin Guo",
            "Tim Leung"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Commodity exchange-traded funds (ETFs) are a significant part of the rapidly\ngrowing ETF market. They have become popular in recent years as they provide\ninvestors access to a great variety of commodities, ranging from precious\nmetals to building materials, and from oil and gas to agricultural products. In\nthis article, we analyze the tracking performance of commodity leveraged ETFs\nand discuss the associated trading strategies. It is known that leveraged ETF\nreturns typically deviate from their tracking target over longer holding\nhorizons due to the so-called volatility decay. This motivates us to construct\na benchmark process that accounts for the volatility decay, and use it to\nexamine the tracking performance of commodity leveraged ETFs. From empirical\ndata, we find that many commodity leveraged ETFs underperform significantly\nagainst the benchmark, and we quantify such a discrepancy via the novel idea of\n\\emph{realized effective fee}. Finally, we consider a number of trading\nstrategies and examine their performance by backtesting with historical price\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09404v1"
    },
    {
        "title": "Are Trump and Bitcoin Good Partners?",
        "authors": [
            "Jamal Bouoiyour",
            "Refk Selmi"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  During times of extreme market turmoil, it is acknowledged that there is a\ntendency towards \"flight to safety\". A strong (weak) safe haven is defined as\nan asset that has a significant positive (negative) return in periods where\nanother asset is in distress, while hedge has to be negatively correlated\n(uncorrelated) on average. The Bitcoin's surge alongside the aftermath of\nTrump's win in the 2016 U.S. presidential elections has strengthened its status\nas the modern safe haven. This paper uses a truly noise-assisted data analysis\nmethod, termed as Ensemble Empirical Mode Decomposition-based approach, to\nexamine whether Bitcoin can act as a hedge and safe haven for U.S. stock price\nindex. The results document that the Bitcoin's safe-haven property is\ntime-varying and that it has primarily been a weak safe haven in the short term\nand the long-term. We also demonstrate that precious metals lost their safe\nhaven properties over time as the correlation between gold/silver and U.S.\nstock price declines from short-to long-run horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00308v1"
    },
    {
        "title": "Harry Potter and the Goblin Bank of Gringotts",
        "authors": [
            "Zachary Feinstein"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Gringotts Wizarding Bank is well known as the only financial institution in\nall of the Wizarding UK as documented in the works recounting the heroics of\nHarry Potter. The concentration of power and wealth in this single bank needs\nto be weighed against the financial stability of the entire Wizarding economy.\nThis study will consider the impact to financial risk of breaking up Gringotts\nWizarding Bank into five component pieces, along the lines of the\nGlass-Steagall Act in the United States. The emphasis of this work is to\ncalibrate and simulate a model of the banking and financial systems within\nWizarding UK under varying stress test scenarios simulating rumors of Lord\nVoldemort's return or the release of magical creatures into an unsuspecting\nmuggle populace. We conclude by comparing the economic fallout from financial\ncrises under the two systems: (i) Gringotts Wizarding Bank as a monopoly and\n(ii) the split-up financial system. We do this comparison on the level of\nminimal system-wide capital injections that would be needed to prevent the\nfinancial crisis from surpassing the damage caused by Lord Voldemort.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.10469v1"
    },
    {
        "title": "The Wandering of Corn",
        "authors": [
            "Valerii Salov"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Time and Sales of corn futures traded electronically on the CME Group Globex\nare studied. Theories of continuous prices turn upside down reality of\nintra-day trading. Prices and their increments are discrete and obey lattice\nprobability distributions. A function for systematic evolution of futures\ntrading volume is proposed. Dependence between sample skewness and kurtosis of\nwaiting times does not support hypothesis of Weibull distribution. Kumaraswamy\ndistribution is more suitable for waiting times. Relationships between trading\nvolume and maximum profit strategies are presented. Frequencies of absolute\nb-increments are approximated by a Hurwitz Zeta distribution. Relative\nb-increments are non-Gaussian too. Dependence between b- and a-increments\nallows to interpret the sample variances of b-increments as a stochastic\nprocess. Mean sample variance of b-increments vs. a-increments is presented.\nThe L1 distance and Log-likelihood statistics for independence between a- and\nb-increments are controversial. Corn price jumps remind of chain branching\nreactions. Bi-logarithmic plots of the empirical frequencies of extreme\nb-increments vs. ranks are presented. Corresponding distributions resemble\nsnakes forked tongues. The maximum profit strategy is discussed as a measure of\nnon-equilibrium.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01179v1"
    },
    {
        "title": "The micro-foundations of an open economy money demand: An application to\n  the Central and Eastern European countries",
        "authors": [
            "Claudiu Tiberiu Albulescu",
            "Dominique Pépin",
            "Stephen Miller"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  This paper investigates and compares currency substitution between the\ncurrencies of Central and Eastern European (CEE) countries and the euro. In\naddition, we develop a model with microeconomic foundations, which identifies\ndifference between currency substitution and money demand sensitivity to\nexchange rate variations. More precisely, we posit that currency substitution\nrelates to money demand sensitivity to the interest rate spread between the CEE\ncountries and the euro area. Moreover, we show how the exchange rate affects\nmoney demand, even absent a currency substitution effect. This model applies to\nany country where an international currency offers liquidity services to\ndomestic agents. The model generates empirical tests of long-run money demand\nusing two complementary cointegrating equations. The opportunity cost of\nholding the money and the scale variable, either household consumption or\noutput, explain the long-run money demand in CEE countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01840v1"
    },
    {
        "title": "A generalized Bayesian framework for the analysis of subscription based\n  businesses",
        "authors": [
            "Rahul Madhavan",
            "Ankit Baraskar"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  We have created a framework for analyzing subscription based businesses in\nterms of a unified metric which we call SCV (single customer value). The major\nadvance in this paper is to model customer churn as an exponential decay\nvariable, which directly follows from experimental data relating to\nsubscription based businesses. This Bayesian probabilistic model was used to\ncompute an expected value for the revenue contribution of a single user. We\nobtain an exact closed-form solution for the constant churn model, and an\napproximate closed-form solution for the exponential decay model. In addition,\nwe define a general methodology for decision making processes using sensitivity\nanalysis of the model equation, which we illustrate with a real-life case study\nfor a food based subscription business.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05729v1"
    },
    {
        "title": "The effect of heterogeneity on financial contagion due to overlapping\n  portfolios",
        "authors": [
            "Opeoluwa Banwo",
            "Fabio Caccioli",
            "Paul Harrald",
            "Francesca Medda"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  We consider a model of financial contagion in a bipartite network of assets\nand banks recently introduced in the literature, and we study the effect of\npower law distributions of degree and balance-sheet size on the stability of\nthe system. Relative to the benchmark case of banks with homogeneous degrees\nand balance-sheet sizes, we find that if banks have a power-law degree\ndistribution the system becomes less robust with respect to the initial failure\nof a random bank, and that targeted shocks to the most specialised banks (i.e.\nbanks with low degrees) or biggest banks increases the probability of observing\na cascade of defaults. In contrast, we find that a power-law degree\ndistribution for assets increases stability with respect to random shocks, but\nnot with respect to targeted shocks. We also study how allocations of capital\nbuffers between banks affects the system's stability, and we find that\nassigning capital to banks in relation to their level of diversification\nreduces the probability of observing cascades of defaults relative to size\nbased allocations. Finally, we propose a non-capital based policy that improves\nthe resilience of the system by introducing disassortative mixing between banks\nand assets.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.06791v1"
    },
    {
        "title": "Propensity to spending of an average consumer over a brief period",
        "authors": [
            "Roberto De Luca",
            "Marco Di Mauro",
            "Angelo Falzarano",
            "Adele Naddeo"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Understanding consumption dynamics and its impact on the whole economy and\nwelfare within the present economic crisis is not an easy task. Indeed the\nlevel of consumer demand for different goods varies with the prices, consumer\nincomes and demographic factors. Furthermore crisis may trigger different\nbehaviors which result in distortions and amplification effects. In the present\nwork we propose a simple model to quantitatively describe the time evolution\nover a brief period of the amount of money an average consumer decides to\nspend, depending on his/her available budget. A simple hydrodynamical analog of\nthe model is discussed. Finally, perspectives of this work are briefly\noutlined.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.03848v1"
    },
    {
        "title": "Application of Differential Equations in Projecting Growth Trajectories",
        "authors": [
            "Ron W. Nielsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Mathematical method based on a direct or indirect analysis of growth rates is\ndescribed. It is shown how simple assumptions and a relatively easy analysis\ncan be used to describe mathematically complicated trends and to predict\ngrowth. Only rudimentary knowledge of calculus is required. Projected\ntrajectories based on such simple initial assumptions are easier to accept and\nto understand than alternative complicated projections based on more\ncomplicated assumptions and on more intricate computational procedures.\nExamples of the growth of population and of the growth of the Gross Domestic\nProduct are used to illustrate the application of this method of forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.06557v2"
    },
    {
        "title": "Compressing Over-the-Counter Markets",
        "authors": [
            "Marco D'Errico",
            "Tarik Roukny"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Over-the-counter markets are at the center of the postcrisis global reform of\nthe financial system. We show how the size and structure of such markets can\nundergo rapid and extensive changes when participants engage in portfolio\ncompression, a post-trade netting technology. Tightly-knit and concentrated\ntrading structures, as featured by many large over-the-counter markets, are\nespecially susceptible to reductions of notional and reconfigurations of\nnetwork structure resulting from compression activities. Using\ntransaction-level data on credit-default-swaps markets, we estimate reduction\nlevels consistent with the historical development observed in these markets\nsince the Global Financial Crisis. Finally, we study the effect of a mandate to\ncentrally clear over-the-counter markets. When participants engage in both\ncentral clearing and portfolio compression, we find large netting failures if\nclearinghouses proliferate. Allowing for compression across clearinghouses\nby-and-large offsets this adverse effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07155v2"
    },
    {
        "title": "Exact probability distribution function for the volatility of cumulative\n  production",
        "authors": [
            "Rubina Zadourian",
            "Andreas Klümper"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  In this paper we study the volatility and its probability distribution\nfunction for the cumulative production based on the experience curve\nhypothesis. This work presents a generalization of the study of volatility in\n[1], which addressed the effects of normally distributed noise in the\nproduction process. Due to its wide applicability in industrial and\ntechnological activities we present here the mathematical foundation for an\narbitrary distribution function of the process, which we expect will pave the\nfuture research on production and market strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02605v1"
    },
    {
        "title": "Kinetic theory and Brazilian income distribution",
        "authors": [
            "Igor D. S. Siciliani",
            "Marcelo H. R. Tragtenberg"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  We investigate the Brazilian personal income distribution using data from\nNational Household Sample Survey (PNAD), an annual research available by the\nBrazilian Institute of Geography and Statistics (IBGE). It provides general\ncharacteristics of the country's population. Using PNAD data background we also\nconfirm the effectiveness of a semi-empirical model that reconciles Pareto\npower-law for high-income people and Boltzmann- Gibbs distribution for the rest\nof population. We use three measures of income inequality: the Pareto index,\nthe average income and the crossover income. In order to cope with many\ndimensions of the income inequality, we calculate these three indices and also\nthe Gini coefficient for the general population as well as for two kinds of\npopulation dichotomies: black / indigenous / mixed race versus white / yellow;\nand men versus women. We also followed the time series of these indices for the\nperiod 2001-2014. The results suggest a decreasing of Brazilian income\ninequality over the selected period. Another important result is that\nhistorically-disadvantaged subgroups (Women and black / indigenous / mixed\nrace),that are the majority of the population, have a more equalitarian income\ndistribution. These groups have also a smaller monthly income than the others\nand this social structure remained virtually unchanged in the period of time.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06480v1"
    },
    {
        "title": "Option Pricing with Greed and Fear Factor: The Rational Finance Approach",
        "authors": [
            "Svetlozar Rachev",
            "Frank J. Fabozzi",
            "Boryana Racheva-Iotova",
            "Abootaleb Shirvani"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  We explain the main concepts of Prospect Theory and Cumulative Prospect\nTheory within the framework of rational dynamic asset pricing theory. We derive\noption pricing formulas when asset returns are altered with a generalized\nProspect Theory value function or a modified Prelec weighting probability\nfunction and introduce new parametric classes for Prospect Theory value\nfunctions and weighting probability functions consistent with rational dynamic\npricing Theory. We study the behavioral finance notion of greed and fear from\nthe point of view of rational dynamic asset pricing theory and derive the\ncorresponding option pricing formulas in the case of asset returns that follow\ncontinuous diffusion or discrete binomial trees.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08134v2"
    },
    {
        "title": "Kinetic models for goods exchange in a multi-agent market",
        "authors": [
            "Carlo Brugna",
            "Giuseppe Toscani"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  We introduce a system of kinetic equations describing an exchange market\nconsisting of two populations of agents (dealers and speculators) expressing\nthe same preferences for two goods, but applying different strategies in their\nexchanges. We describe the trading of the goods by means of some fundamental\nrules in price theory, in particular by using Cobb-Douglas utility functions\nfor the exchange. The strategy of the speculators is to recover maximal utility\nfrom the trade by suitably acting on the percentage of goods which are\nexchanged. This microscopic description leads to a system of linear\nBoltzmann-type equations for the probability distributions of the goods on the\ntwo populations, in which the post-interaction variables depend from the\npre-interaction ones in terms of the mean quantities of the goods present in\nthe market. In this case, it is shown analytically that the strategy of the\nspeculators can drive the price of the two goods towards a zone in which there\nis a marked utility for their group. Also, the general system of nonlinear\nkinetic equations of Boltzmann type for the probability distributions of the\ngoods on the two populations is described in details. Numerical experiments\nthen show how the policy of speculators can modify the final price of goods in\nthis nonlinear setting.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.09495v1"
    },
    {
        "title": "Customer Selection Model with Grouping and Hierarchical Ranking Analysis",
        "authors": [
            "Bowen Cai"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  The purpose of this study was to build a customer selection model based on 20\ndimensions, including customer codes, total contribution, assets, deposit,\nprofit, profit rate, trading volume, trading amount, turnover rate, order\namount, withdraw amount, withdraw rate, process fee, process fee submitted,\nprocess fee retained, net process fee retained, interest revenue, interest\nreturn, exchange house return I and exchange house return II to group and rank\ncustomers. The traditional way to group customers in securities or futures\ncompanies is simply based on their assets. However, grouping customers with\nrespect to only one dimension cannot give us a full picture about customers'\nattributions. It is hard to group customers' with similar attributions or\nvalues into one group if we just consider assets as the only grouping\ncriterion. Nowadays, securities or futures companies usually group customers\nbased on managers' experience with lack of quantitative analysis, which is not\neffective. Therefore, we use kmeans unsupervised learning methods to group\ncustomers with respect to significant dimensions so as to cluster customers\nwith similar attributions together. Grouping is our first step. It is the\nhorizontal analysis in customer study. The next step is customer ranking. It is\nthe longitudinal analysis. It ranks customers by assigning each customer with a\ncertain score given by our weighted customer value calculation formula.\nTherefore, by grouping and ranking customers, we can differentiate our\ncustomers and rank them based on values instead of blindly reaching everyone.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.05598v1"
    },
    {
        "title": "Trading Strategies with Position Limits",
        "authors": [
            "Valerii Salov"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Whether you trade futures for yourself or a hedge fund, your strategy is\ncounted. Long and short position limits make the number of unique strategies\nfinite. Formulas of the numbers of strategies, transactions, do nothing actions\nare derived. A discrete distribution of actions, corresponding probability\nmass, cumulative distribution and characteristic functions, moments, extreme\nvalues are presented. Strategies time slice distributions are determined.\nVector properties of trading strategies are studied. Algebraic not associative,\ncommutative, initial magmas with invertible elements control trading positions\nand strategies. Maximum profit strategies, MPS, and optimal trading elements\ncan define trading patterns. Dynkin introduced the term interpreted in English\nas \"Markov time\" in 1963. Neftci applied it for the formalization of Technical\nAnalysis in 1991.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.07649v1"
    },
    {
        "title": "Foreign Portfolio Investment and Economy: The Network Perspective",
        "authors": [
            "Muhammad Mohsin Hakeem",
            "Ken-ichi Suzuki"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  The European Union and Eurozone present an inquisitive case of strongly\ninterconnected network with high degree of dependence among nodes. This\nresearch focused on investment network of European Union and its major trading\npartners for specific time period 2001 to 2014. The changing investment\npatterns within Eurozone suggest strong financial and trade links with central\nand large economies. This study is about the association between portfolio\ninvestment and economic indicators with respect to financial networks. The\nanalysis used the strongly connected investment network of Eurozone and its\nlarge trading partners. A strong correlation between, increasing or decreasing\ninvestment patterns with economic indicators of particular economy was found.\nInterestingly correlation patterns for network members other than Eurozone\nstates were not as strong and depicted mild behavior. This as well, explains\nthe significance of interconnectedness level among nodes of one network with\nvarying centrality measures. Investment network visualization techniques helped\nto validate the results based on network`s statistical measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.10274v1"
    },
    {
        "title": "The Information Content of Sarbanes-Oxley in Predicting Security\n  Breaches",
        "authors": [
            "J. Christopher Westland"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We investigated publicly reported security breaches of internal controls in\ncorporate systems to determine whether SOX assessments are information bearing\nwith respect to breaches which can lead to materially significant losses and\nmisstatements. SOX Section 404 adverse decisions on effectiveness of controls\noccurred in 100% of credit card data breaches and around 33% of insider\nbreaches. SOX 404 audits provided a contrarian \"effective\" control decisions on\n88% of situations where there was a control breach concerning a portable\ndevice. We found that management and SOX 404 auditors do not general agree on\nthe underlying internal control situation at any time; instead the SOX 404 team\nwas likely to discover material weaknesses and \"educate\" management and\ninternal audit teams about the importance of these control weaknesses. SOX\nattestations were poor at identifying control weaknesses from unintended\ndisclosures, physical losses, hacking and malware. Hazard and occupancy models\nshowed that both SOX 302 and 404 section audits provided information on the\nfrequency of breaches, with SOX 404 being three times as informative as section\n302 reports. The hazard model found an expected 2.88% reduction in breaches\nwhen SOX 302 controls are effective; management \"material weakness'\nattestations provided no information in this structural model, whereas there\nwould be around a 1% increase in breach occurrence when there are significant\ndeficiencies. SOX 404 attestations were the most informative, and a negative\nSOX 404 attestation is projected to increase the frequency of breaches by\naround 8.5%.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.10001v1"
    },
    {
        "title": "Predictive modeling of stock indices closing from web search trends",
        "authors": [
            "Arjun R",
            "Suprabha KR"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  The study aims to explore the strength of causal relationship between stock\nprice search interest and real stock market outcomes on worldwide equity market\nindices. Such a phenomenon could also be mediated by investor behavior and\nextent of news coverage. The stock-specific internet search trends data and\ncorresponding index close values from different countries stock exchanges are\ncollected and analyzed. Empirical findings show global stock price search\ninterests correlates more with developing economies with fewer effects in south\nasian stock exchanges apart from strong influence in western countries. Finally\nthis study calls for development in expert decision support systems with the\nsynthesis of using big data sources on forecasting market outcomes\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01676v1"
    },
    {
        "title": "A derivation of the Black-Scholes option pricing model using a central\n  limit theorem argument",
        "authors": [
            "Rajeshwari Majumdar",
            "Phanuel Mariano",
            "Lowen Peng",
            "Anthony Sisti"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  The Black-Scholes model (sometimes known as the Black-Scholes-Merton model)\ngives a theoretical estimate for the price of European options. The price\nevolution under this model is described by the Black-Scholes formula, one of\nthe most well-known formulas in mathematical finance. For their discovery,\nMerton and Scholes have been awarded the 1997 Nobel prize in Economics. The\nstandard method of deriving the Black-Scholes European call option pricing\nformula involves stochastic differential equations. This approach is out of\nreach for most students learning the model for the first time. We provide an\nalternate derivation using the Lindeberg-Feller central limit theorem under\nsuitable assumptions. Our approach is elementary and can be understood by\nundergraduates taking a standard undergraduate course in probability.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03290v2"
    },
    {
        "title": "Warranty Cost Analysis with an Alternating Geometric Process",
        "authors": [
            "Richard Arnold",
            "Stefanka Chukova",
            "Yu Hayakawa",
            "Sarah Marshall"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  In this study we model the warranty claims process and evaluate the warranty\nservicing costs under non-renewing and renewing free repair warranties. We\nassume that the repair time for rectifying the claims is non-zero and the\nrepair cost is a function of the length of the repair time. To accommodate the\nageing of the product and repair equipment, we use a decreasing geometric\nprocess to model the consecutive operational times and an increasing geometric\nprocess to model the consecutive repair times. We identify and study the\nalternating geometric process (AGP), which is an alternating process with\ncycles consisting of the item's operational time followed by the corresponding\nrepair time. We derive new results for the AGP in finite horizon and use them\nto evaluate the warranty costs over the warranty period and over the life cycle\nof the product under a non-renewing free repair warranty (NRFRW), a renewing\nfree repair warranty (RFRW) and a restricted renewing free repair warranty\n(RRFRW(n)). Properties of the model are demonstrated using a simulation study.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06707v1"
    },
    {
        "title": "An Attempt at Analyzing the Information Nature of Money",
        "authors": [
            "Haibo Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Money was invented to address the difficulty in the double coincidence of\nwants between the supply and demand when people exchanged their goods and\nservices. There are two information states in society: one is the initial state\nthat people have goods and services due to division of labor; the other is the\nfinal state that people have different goods and services with the initial\nstate due to exchange of goods and services between them. The process is that\nthe initial state is changed to the final state with the help of money. Because\nthe direct exchanges of goods and services are difficult to achieve the double\ncoincidence of wants in time and space, it can be achieved with the help of\nmoney which is as a medium and bridge. In this paper the changing process of\nthe state information is analyzed through the matrix representation of money,\nand then the nature of money with a kind of information of reliable ledger is\nmore apparently shown. This paper also analyzes the common characteristics of\nphysical money, electronic money and digital currency, that is, reliable ledger\nand explores the future trend of money development from the perspective of\nhistory and security technology of money.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06710v1"
    },
    {
        "title": "The CCI30 Index",
        "authors": [
            "Igor Rivin",
            "Carlo Scevola"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We describe the design of the CCI30 cryptocurrency index.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06711v1"
    },
    {
        "title": "Critical analysis of human progress: Its negative and positive sides in\n  the late-capitalism",
        "authors": [
            "Mario Coccia",
            "Matteo Bellitto"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  The concept of progress has characterized human society from millennia.\nHowever, this concept is elusive and too often given for certain. The goal of\nthis paper is to suggest a general definition of human progress that satisfies,\nwhenever possible the conditions of independence, generality, epistemological\napplicability and empirical correctness. This study proposes, within a\npragmatic approach, human progress as an inexhaustible process driven by an\nideal of maximum wellbeing of purposeful people which, on attainment of any of\nits goals or objectives for increasing wellbeing, then seek another\nconsequential goal and objective, endlessly, which more closely approximates\nits ideal fixed in new socioeconomic contexts over time and space. The human\nprogress, in the global, capitalistic, and post-humanistic Era, improves the\nfundamental life-interests represented by health, wealth, expansion of\nknowledge, technology and freedom directed to increase wellbeing throughout the\nsociety. These factors support the acquisition by humanity of better and more\ncomplex forms of life. However, this study shows the inconsistency of the\nequation economic growth= progress because human progress also generates,\nduring its continuous process without limit, negative effects for human being,\nenvironment and society.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09550v1"
    },
    {
        "title": "Can Insider Trading Be Committed Without Trading?",
        "authors": [
            "Russell Stanley Q. Geronimo"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Before a person can be prosecuted and convicted for insider trading, he must\nfirst execute the overt act of trading. If no sale of security is consummated,\nno crime is also consummated. However, through a complex and insidious\ncombination of various financial instruments, one can capture the same amount\nof gains from insider trading without undertaking an actual trade. Since the\ncrime of insider trading involves buying or selling a security, a more\nsophisticated insider can circumvent the language of the Securities Regulation\nCode by replicating the economic equivalent of a sale without consummating a\nsale as defined by law.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06080v1"
    },
    {
        "title": "Factor endowment--commodity output relationships in a three-factor\n  two-good general equilibrium trade model: Further analysis",
        "authors": [
            "Yoshiaki Nakada"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  The position of the EWS (economy-wide substitution)-ratio vector determines\nthe Rybczynski sign pattern, which expresses the factor endowment--commodity\noutput relationships, and the Stolper-Samuelson sign pattern, which expresses\nthe commodity price--factor price relationships in a three-factor two-good\ngeneral equilibrium trade model (see Nakada (2016a)). In this article, we show\nthat the EWS-ratio vector exists on the line segment. Using this relationship,\nwe develop a method to estimate the position of the EWS-ratio vector. We derive\na sufficient condition for extreme factors to be economy-wide complements,\nwhich implies \"a strong Rybczynski result.\" Additionally, we derive a\nsufficient condition for a specific Stolper-Samuelson sign pattern to hold. We\nassume factor-intensity ranking is constant. This article provides a basis for\nfurther applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06129v2"
    },
    {
        "title": "Unravelling Airbnb Predicting Price for New Listing",
        "authors": [
            "Paridhi Choudhary",
            "Aniket Jain",
            "Rahul Baijal"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This paper analyzes Airbnb listings in the city of San Francisco to better\nunderstand how different attributes such as bedrooms, location, house type\namongst others can be used to accurately predict the price of a new listing\nthat optimal in terms of the host's profitability yet affordable to their\nguests. This model is intended to be helpful to the internal pricing tools that\nAirbnb provides to its hosts. Furthermore, additional analysis is performed to\nascertain the likelihood of a listings availability for potential guests to\nconsider while making a booking. The analysis begins with exploring and\nexamining the data to make necessary transformations that can be conducive for\na better understanding of the problem at large while helping us make\nhypothesis. Moving further, machine learning models are built that are\nintuitive to use to validate the hypothesis on pricing and availability and run\nexperiments in that context to arrive at a viable solution. The paper then\nconcludes with a discussion on the business implications, associated risks and\nfuture scope.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.12101v1"
    },
    {
        "title": "Report for the Edinburgh Tram Inquiry",
        "authors": [
            "Bent Flyvbjerg",
            "Alexander Budzier"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This report reviews the Edinburgh tram project's risk management. Projects\nfrequently overrun their cost and timelines and fall short on intended\nbenefits. Cost, schedule, and benefit risk of projects need to be carefully\nconsidered to avoid this. The report describes and evaluates risk assessment\nand management for the Edinburgh tram. The report was produced as part of the\nEdinburgh Tram Inquiry.\n  Keywords: risk assessment, risk management, infrastructure, megaprojects,\noptimism bias, strategic misrepresentation, planning fallacy, behavioral\nscience.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.12106v1"
    },
    {
        "title": "Are Biotechnology Startups Different?",
        "authors": [
            "Hervé Lebret"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  In the domain of technology startups, biotechnology has often been considered\nas specific. Their unique technology content, the type of founders and managers\nthey have, the amount of venture capital they raise, the time it takes them to\nreach an exit as well as the technology clusters they belong to are seen as\nsuch unique features. Based on extensive research from new databases, the\nauthor claims that the biotechnology startups are not as different as it might\nhave been claimed: the amount of venture capital raised, the time to exit,\ntheir geography are indeed similar and even their equity structure to founders\nand managers have similarities. The differences still exist, for example the\nexperience of the founders, the revenue and profit level at exit.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.12108v1"
    },
    {
        "title": "Critical factors and enablers of food quality and safety compliance risk\n  management in the Vietnamese seafood supply chain",
        "authors": [
            "Thi Huong Tran"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Recently, along with the emergence of food scandals, food supply chains have\nto face with ever-increasing pressure from compliance with food quality and\nsafety regulations and standards. This paper aims to explore critical factors\nof compliance risk in food supply chain with an illustrated case in Vietnamese\nseafood industry. To this end, this study takes advantage of both primary and\nsecondary data sources through a comprehensive literature research of\nindustrial and scientific papers, combined with expert interview. Findings\nshowed that there are three main critical factor groups influencing on\ncompliance risk including challenges originating from Vietnamese food supply\nchain itself, characteristics of regulation and standards, and business\nenvironment. Furthermore, author proposed enablers to eliminate compliance\nrisks to food supply chain managers as well as recommendations to government\nand other influencers and supporters.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.12109v1"
    },
    {
        "title": "A Data-Driven Approach for Modeling Stochasticity in Oil Market",
        "authors": [
            "Sina Aghaei"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Global oil price is an important factor in determining many economic\nvariables in the world's economy. It is generally modeled as a stochastic\nprocess and have been studied through different techniques by comparing the\nhistoric time series of demand, supply and the price itself. However, there are\nmany historic events where the demand or supply changes are not sufficient in\nexplaining the price changes. In such cases, it is the expectations on the\nfuture changes of demand or supply that causes heavy and quick influences on\nthe price. There are many parameters and variables that shape these\nexpectations, and are usually neglected in traditional models. In this paper,\nwe have proposed a model based on System Dynamics approach that takes into\naccount these non-traditional factors. The validity of the proposed model is\nthen evaluated using real and potential scenarios in which the proposed model\nfollows the trend of the real data.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.12110v1"
    },
    {
        "title": "The logic of uncertainty as a logic of experience and chance and the\n  co~event-based Bayes' theorem",
        "authors": [
            "Oleg Yu Vorobyev"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  The logic of uncertainty is not the logic of experience and as well as it is\nnot the logic of chance. It is the logic of experience and chance. Experience\nand chance are two inseparable poles. These are two dual reflections of one\nessence, which is called co~event. The theory of experience and chance is the\ntheory of co~events. To study the co~events, it is not enough to study the\nexperience and to study the chance. For this, it is necessary to study the\nexperience and chance as a single entire, a co~event. In other words, it is\nnecessary to study their interaction within a co~event. The new co~event\naxiomatics and the theory of co~events following from it were created precisely\nfor these purposes. In this work, I am going to demonstrate the effectiveness\nof the new theory of co~events in a studying the logic of uncertainty. I will\ndo this by the example of a co~event splitting of the logic of the Bayesian\nscheme, which has a long history of fierce debates between Bayesianists and\nfrequentists. I hope the logic of the theory of experience and chance will make\nits modest contribution to the application of these old dual debaters.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.01310v1"
    },
    {
        "title": "Challenges in approximating the Black and Scholes call formula with\n  hyperbolic tangents",
        "authors": [
            "Michele Mininni",
            "Giuseppe Orlando",
            "Giovanni Taglialatela"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  In this paper we introduce the concept of standardized call function and we\nobtain a new approximating formula for the Black and Scholes call function\nthrough the hyperbolic tangent. This formula is useful for pricing and risk\nmanagement as well as for extracting the implied volatility from quoted\noptions. The latter is of particular importance since it indicates the risk of\nthe underlying and it is the main component of the option's price. Further we\nestimate numerically the approximating error of the suggested solution and, by\ncomparing our results in computing the implied volatility with the most common\nmethods available in literature we discuss the challenges of this approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04623v1"
    },
    {
        "title": "Symmetry, Entropy, Diversity and (why not?) Quantum Statistics in\n  Society",
        "authors": [
            "J. Rosenblatt"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We describe society as a nonequilibrium probabilistic system: N individuals\noccupy W resource states in it and produce entropy S over definite time\nperiods. Resulting thermodynamics is however unusual because a second entropy,\nH, measures a typically social feature, inequality or diversity in the\ndistribution of available resources. A symmetry phase transition takes place at\nGini values 1/3, where realistic distributions become asymmetric. Four\nconstraints act on S: expectedly, N and W, and new ones, diversity and\ninteractions between individuals; the latter result from the two coordinates of\na single point in the data, the peak. The occupation number of a job is either\nzero or one, suggesting Fermi-Dirac statistics for employment. Contrariwise, an\nindefinite nujmber of individuals can occupy a state defined as a quantile of\nincome or of age, so Bose-Einstein statistics may be required.\nIndistinguishability rather than anonymity of individuals and resources is thus\nneeded. Interactions between individuals define define classes of equivalence\nthat happen to coincide with acceptable definitions of social classes or\nperiods in human life. The entropy S is non-extensive and obtainable from data.\nTheoretical laws are compared to data in four different cases of economical or\nphysiological diversity. Acceptable fits are found for all of them.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04624v1"
    },
    {
        "title": "Advertising and Brand Attitudes: Evidence from 575 Brands over Five\n  Years",
        "authors": [
            "Rex Yuxing Du",
            "Mingyu Joo",
            "Kenneth C. Wilbur"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Little is known about how different types of advertising affect brand\nattitudes. We investigate the relationships between three brand attitude\nvariables (perceived quality, perceived value and recent satisfaction) and\nthree types of advertising (national traditional, local traditional and\ndigital). The data represent ten million brand attitude surveys and $264\nbillion spent on ads by 575 regular advertisers over a five-year period,\napproximately 37% of all ad spend measured between 2008 and 2012. Inclusion of\nbrand/quarter fixed effects and industry/week fixed effects brings parameter\nestimates closer to expectations without major reductions in estimation\nprecision. The findings indicate that (i) national traditional ads increase\nperceived quality, perceived value, and recent satisfaction; (ii) local\ntraditional ads increase perceived quality and perceived value; (iii) digital\nads increase perceived value; and (iv) competitor ad effects are generally\nnegative.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.07783v1"
    },
    {
        "title": "Economic Impact of Wind Generation Penetration in the Colombian\n  Electricity Market",
        "authors": [
            "Alvaro Gonzalez-Castellanos",
            "David Pozo",
            "Sergio Martinez",
            "Luis Lopez",
            "Ingrid Oliveros"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  The creation of the Renewable Energy Law (Law 1715 of 2014) promotes the\nintroduction of large-scale renewable energy generation in the Colombian\nelectricity market. The new legislation aims to diversify the country's\ngeneration matrix, mainly composed of hydro and fuel-based generation, with a\nshare of 66% and 34% respectively. Currently, three wind generation projects,\nwith an aggregated capacity of 500 MW, have been commissioned in the North of\nthe country. This study analyses the economic impact of the large-scale\nintroduction of wind generation on both, the market spot price and conventional\ngeneration plants operation. For this purpose, the study builds a unit\ncommitment model to mimic the current market legislation and the system's\ngeneration data. We show that the introduction of wind energy into the\nColombian electricity market would impact the generation share of large hydro\nand gas-fired power plants. The hydro generation has an important role in\nbalancing the generation for fluctuations on the wind resource. Meanwhile, the\ngas-fired plants would decrease their participation in the market,\nproportionally to the introduction of wind generation in the system, by as low\nas 20% of its current operation.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11458v1"
    },
    {
        "title": "Asymmetric Network Connectedness of Fears",
        "authors": [
            "Jozef Barunik",
            "Mattia Bevilacqua",
            "Radu Tunaru"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This paper introduces forward-looking measures of the network connectedness\nof fears in the financial system, arising due to the good and bad beliefs of\nmarket participants about uncertainty that spreads unequally across a network\nof banks. We argue that this asymmetric network structure extracted from call\nand put traded option prices of the main U.S. banks contains valuable\ninformation for predicting macroeconomic conditions and economic uncertainty,\nand it can serve as a tool for forward-looking systemic risk monitoring.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.12022v2"
    },
    {
        "title": "The Rank Effect",
        "authors": [
            "Ricardo T. Fernholz",
            "Christoffer Koch"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We decompose returns for portfolios of bottom-ranked, lower-priced assets\nrelative to the market into rank crossovers and changes in the relative price\nof those bottom-ranked assets. This decomposition is general and consistent\nwith virtually any asset pricing model. Crossovers measure changes in rank and\nare smoothly increasing over time, while return fluctuations are driven by\nvolatile relative price changes. Our results imply that in a closed,\ndividend-free market in which the relative price of bottom-ranked assets is\napproximately constant, a portfolio of those bottom-ranked assets will\noutperform the market portfolio over time. We show that bottom-ranked relative\ncommodity futures prices have increased only slightly, and confirm the\nexistence of substantial excess returns predicted by our theory. If these\nexcess returns did not exist, then top-ranked relative prices would have had to\nbe much higher in 2018 than those actually observed -- this would imply a\nradically different commodity price distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06000v1"
    },
    {
        "title": "Phase Diagram for Roegenian Economics",
        "authors": [
            "Constantin Udriste",
            "Massimiliano Ferrara",
            "Ionel Tevy",
            "Dorel Zugravescu",
            "Florin Munteanu"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We recall the similarities between the concepts and techniques of\nThermodynamics and Roegenian Economics. The Phase Diagram for a Roegenian\neconomic system highlights a triple point and a critical point, with related\nexplanations. These ideas can be used to improve our knowledge and\nunderstanding of the nature of development and evolution of Roegenian economic\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07959v1"
    },
    {
        "title": "Economic Cycles of Carnot Type",
        "authors": [
            "Constantin Udriste",
            "Vladimir Golubyatnikov",
            "Ionel Tevy"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Originally, the Carnot cycle is a theoretical thermodynamic cycle that\nprovides an upper limit on the efficiency that any classical thermodynamic\nengine can achieve during the conversion of heat into work, or conversely, the\nefficiency of a refrigeration system in creating a temperature difference by\nthe application of work to the system. The aim of this paper is to introduce\nand study the economic Carnot cycles into a Roegenian economy, using our\nThermodynamic-Economic Dictionary. Of course, the most difficult questions are:\nwhat is the economic significance of such a cycle? Roegenian economics is\nacceptable or not, in terms of practical applications? Our answer is yes for\nboth questions.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07960v1"
    },
    {
        "title": "Geobiodynamics and Roegenian Economic Systems",
        "authors": [
            "Constantin Udriste",
            "Massimiliano Ferrara",
            "Dorel Zugravescu",
            "Florin Munteanu",
            "Ionel Tevy"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This mathematical essay brings together ideas from Economics, Geobiodynamics\nand Thermodynamics. Its purpose is to obtain real models of complex\nevolutionary systems. More specifically, the essay defines Roegenian Economy\nand links Geobiodynamics and Roegenian Economy. In this context, we discuss the\nisomorphism between the concepts and techniques of Thermodynamics and\nEconomics. Then we describe a Roegenian economic system like a Carnot group.\nAfter we analyse the phase equilibrium for two heterogeneous economic systems.\nThe European Union Economics appears like Cartesian product of Roegenian\neconomic systems and its Balance is analysed in details. A Section at the end\ndescribes the \"economic black holes\" as small parts of a a global economic\nsystem in which national income is so great that it causes others poor\nenrichment. These ideas can be used to improve our knowledge and understanding\nof the nature of development and evolution of thermodynamic-economic systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07961v1"
    },
    {
        "title": "The gruesome murder of Jamal Khashoggi : Saudi Arabia's new economy\n  dream at risk ?",
        "authors": [
            "Jamal Bouoiyour",
            "Refk Selmi"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  With the horrific Jamal Khashoggi killing, Mohammed Bin Salman's image in the\ninternational community has been damaged. This study seeks to test whether\nKhashoggi murder discourage businesses from investing in Saudi Arabia. We use\nan event-study methodology and asset pricing model to assess, at sectoral\nlevel, the dynamics of stock prices surrounding the killing of the Saudi\njournalist on 2 October at the kingdom's consulate in Istanbul. A series of\nrobustness tests, including the Corrado ranking test and the non-parametric\nconditional distribution approach, have been conducted. We consistently show\nthat the khashoggi killing had the most adverse impact on banks and financial\nservices, materials, and technology. Oil and gas companies, however, were\nmoderately or insignificantly affected. Overall, our results suggest that the\ncrown prince's ambitious project for a Saudi Arabian economy moving beyond oil\nwealth are threatened as this recent event dampened foreign interest in\ninvesting in the kingdom.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.11336v1"
    },
    {
        "title": "Implementing a financial derivative as smart contract",
        "authors": [
            "Christian Fries",
            "Peter Kohl-Landgraf",
            "Björn Paffen",
            "Stefanie Weddigen",
            "Luca Del Re",
            "Wilfried Schütte",
            "David Bacher",
            "Rebecca Declara",
            "Daniel Eichsteller",
            "Florian Weichand",
            "Michael Streubel"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In this note we describe the application of existing smart contract\ntechnologies with the aim to construct a new digital representation of a\nfinancial derivative contract. We compare several existing DLT based\ntechnologies. We provide a detailed description of two separate prototypes\nwhich are able to be executed on a centralized and on a DLT platform\nrespectively. Beyond that we highlight some insights on legal aspects as well\nas on common integration challenges regarding existing process and system\nlandscapes. For a further introductory note and motivation on the theoretical\nconcept we refer to\nhttps://www.law.ox.ac.uk/business-law-blog/blog/2018/12/smart-derivative-contract-constructing-digital-financial-derivative\n. A very detailed methodological overview of the concept of a smart derivative\ncontract can be found in doi:10.2139/ssrn.3163074.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.00067v2"
    },
    {
        "title": "Quantitative evaluation of consecutive resilience cycles in stock market\n  performance: A systems-oriented approach",
        "authors": [
            "Junqing Tang",
            "Hans R. Heinimann"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Financial markets can be seen as complex systems that are constantly evolving\nand sensitive to external disturbance, such as systemic risks and economic\ninstabilities. Analysis of resilient market performance, therefore, becomes\nuseful for investors. From a systems perspective, this paper proposes a novel\nfunction-based resilience metric that considers the effect of two\nfault-tolerance thresholds: the Robustness Range (RR) and the Elasticity\nThreshold (ET). We examined the consecutive resilience cycles and their\ndynamics in the performance of two stock markets, NASDAQ and SSE. The proposed\nmetric was also compared with three well-documented resilience models. The\nresults showed that this new metric could satisfactorily quantify the\ntime-varying resilience cycles in the multi-cycle volatile performance of stock\nmarkets while also being more feasible in comparative analysis. Furthermore,\nanalysis of dynamics revealed that those consecutive resilience cycles in\nmarket performance were distributed non-linearly, following a power-law\nbehavior in the upper tail. Finally, sensitivity tests demonstrated the\nlarge-value resilience cycles were relatively sensitive to changes in RR. In\npractice, RR could indicate investors' psychological capability to withstand\ndownturns. It supports the observation that perception on the market's\nresilient responses may vary among investors. This study provides a new tool\nand valuable insight for researchers, practitioners, and investors when\nevaluating market performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.03201v1"
    },
    {
        "title": "The Changing Geopolitics in the Arab World: Implications of the 2017\n  Gulf Crisis for Business",
        "authors": [
            "Jamal Bouoiyour",
            "Refk Selmi"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The international community was caught by surprise on 5 June 2017 when Saudi\nArabia, the United Arab Emirates (UAE), Bahrain and Egypt severed diplomatic\nties with Qatar, accusing it of destabilizing the region. More than one year\nafter this diplomatic rift, several questions remain unaddressed. This study\nfocuses on the regional business costs of the year-long blockade on Qatar. We\nsplit the sample to compare the stock market performances of Qatar and its\nMiddle Eastern neighbors before and after the Saudi-led Qatar boycott. We focus\nour attention on the conditional volatility process of stock market returns and\nrisks related to financial interconnectedness. We show that the Gulf crisis had\nthe most adverse impact on Qatar together with Saudi Arabia and the UAE.\nAlthough not to the same degree as these three countries, Bahrain and Egypt\nwere also harmfully affected. But shocks to the volatility process tend to have\nshort-lasting effects. Moreover, the total volatility spillovers to and from\nothers increase but moderately after the blockade. Overall, the quartet\nlobbying efforts did not achieve the intended result. Our findings underscore\nQatar's economic vulnerability but also the successful resilience strategy of\nthis tiny state. The coordinated diplomatic efforts of Qatar have been able to\nfight the economic and political embargo.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.08076v1"
    },
    {
        "title": "Modeling, discretization, and hyperchaos detection of conformable\n  derivative approach to a financial system with market confidence and ethics\n  risk",
        "authors": [
            "Baogui Xin",
            "Wei Peng",
            "Yekyung Kwon",
            "Yanqin Liu"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  A new chaotic financial system is proposed by considering ethics involvement\nin a four-dimensional financial system with market confidence. A\nfive-dimensional conformable derivative financial system is presented by\nintroducing conformable fractional calculus to the integer-order system. A\ndiscretization scheme is proposed to calculate numerical solutions of\nconformable derivative systems. The scheme is illustrated by testing hyperchaos\nfor the system.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.12267v2"
    },
    {
        "title": "The impact of financial risks on economic growth in EU-15",
        "authors": [
            "Ionut Jianu",
            "Laura-Madalina Pirscoveanu",
            "Maria-Daniela Tudorache"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This paper examines the impact of financial risks on economic growth in the\nfirst 15 Member States of the European Union, considering 1995-2014 period and\naims to lay down a new explanatory model of economic growth, based mainly on\nthe behavioral reactivity of the financial disruptions mentioned above. The\nmodel was estimated through the panel estimated generalized least squares\nmethod and included additional control variables in order to strengthen the\nresearch conducted. Our goal consists in the examination of the financial risks\nin the European Union and in the estimation of their impact on economic growth.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.02166v1"
    },
    {
        "title": "Developing a real estate yield investment deviceusing granular data and\n  machine learning",
        "authors": [
            "Monica Azqueta-Gavaldon",
            "Gonzalo Azqueta-Gavaldon",
            "Inigo Azqueta-Gavaldon",
            "Andres Azqueta-Gavaldon"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This project aims at creating an investment device to help investors\ndetermine which real estate units have a higher return to investment in Madrid.\nTo do so, we gather data from Idealista.com, a real estate web-page with\nmillions of real estate units across Spain, Italy and Portugal. In this\npreliminary version, we present the road map on how we gather the data;\ndescriptive statistics of the 8,121 real estate units gathered (rental and\nsale); build a return index based on the difference in prices of rental and\nsale units(per neighbourhood and size) and introduce machine learning\nalgorithms for rental real estate price prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.02629v1"
    },
    {
        "title": "Obamacare and a Fix for the IRS Iteration",
        "authors": [
            "Samuel J. Ferguson"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We model the quantities appearing in Internal Revenue Service (IRS) tax\nguidance for calculating the health insurance premium tax credit created by the\nPatient Protection and Affordable Care Act, also called Obamacare. We ask the\nquestion of whether there is a procedure, computable by hand, which can\ncalculate the appropriate premium tax credit for any household with\nself-employment income. We motivate current IRS tax guidance, which has had\nself-employed taxpayers use a fixed point iteration to calculate their premium\ntax credits since 2014. Then, we give an example showing that the IRS iteration\ncan lead to a divergent sequence of iterates. As a consequence, IRS guidance\ndoes not calculate appropriate premium tax credits for tax returns in certain\nincome intervals, adversely affecting eligible beneficiaries. A bisection\nprocedure for calculating premium tax credits is proposed. We prove that this\nprocedure calculates appropriate premium tax credits for a model of simple tax\nreturns. This is generalized to the case where premium tax credits are received\nin advance, which is the most common one in applications. We outline the\nproblem of calculating appropriate premium tax credits for models of general\ntax returns. While the bisection procedure will work with the tax code in its\ncurrent configuration, it could fail, eg, in states which have not expanded\nMedicaid, if a new deduction with certain properties were to arise.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.03355v1"
    },
    {
        "title": "Corporate Governance and Firms Financial Performance in the United\n  Kingdom",
        "authors": [
            "Martin Kyere",
            "Marcel Ausloos"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The objective of this study is to examine empirically the impact of good\ncorporate governance on financial performance of United Kingdom non-financial\nlisted firms. Agency theory and stewardship theory serve as the bases of a\nconceptual model. Five corporate governance mechanisms are examined on two\nfinancial performance indicators, return on assets (ROA) and Tobin's Q,\nemploying cross-sectional regression methodology. The conclusion drawn from\nempirical test so performed on 252 firms listed on London Stock Exchange for\nthe year 2014 indicates a positive or a negative relationship, but also\nsometimes no effect, of corporate governance mechanisms impact on financial\nperformance. The implications are discussed. Thereby, so distinguishing effects\ndue to causes, we present a proof that, when the right corporate governance\nmechanisms are chosen, the finances of a firm can be improved. The results of\nthis research should have some implication on academia and policy makers\nthoughts.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04048v1"
    },
    {
        "title": "Insider Ownership and Dividend Payout Policy: The Role of Business Cycle",
        "authors": [
            "Asmar Aliyeva"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We investigate how the relationship between managerial stock incentives and\nthe dividend payout policy is impacted by the business cycle by using the data\nof S&P 1500 companies during 2000-2018. We find a strong negative relationship\nbetween managerial stock options and annual dividend payouts of companies for\nthe full sample. Although the direction of the relationship is also negative\nfor the recession period, the coefficient is found to be insignificant. We also\nfind that the mentioned relationship may vary during the recession depending on\nthe size of the company. The impact of stock options on the dividend payout is\nnegative for medium-sized companies and the coefficient is both economically\nand statistically significant. The direction of impact changes for large-cap\ncompanies indicating to deterioration of the CEO voting power in those\ncompanies and less agency problem. We also determine that the percentage of\nshares held by the CEO has a positive impact on annual dividends distributed\nfor large-cap companies, whereas this relationship changes in times of\nrecession.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04069v1"
    },
    {
        "title": "Short Term Stress of Covid-19 On World Major Stock Indices",
        "authors": [
            "Muhammad Rehan",
            "Jahanzaib Alvi",
            "Suleyman Serdar Karaca"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The main objective of this study is to check short term stress of COVID-19 on\nthe American, European, Asian, and Pacific stock market indices, furthermore,\nthe correlation between all the stock markets during the pandemic. Secondary\ndata of 41 stock exchange from 32 countries have been collected from\ninvesting.com website from 1st July 2019 to 14th May 2020 for the stock market\nand the COVID-19 data has been collected according to the first cases reported\nin the country, stocks market are classified either developed or emerging\neconomy, further divided according to the subcontinent i.e. America, Europe,\nand Pacific/Asia, the main focus in the data is the report of first COVID-19\ncases. The study reveals that there is volatility in the all the 41 stock\nmarket (American, Europe, Asia, and Pacific) after reporting of the first case\nand volatility increase with the increase of COVID-19 cases, moreover, there is\na significant negative relationship between the number of COVID-19 cases and 41\nmajor stock indices of American, Europe, Asia and Pacific, European\nsubcontinent market found more effected from the COVID-19 than another\nsubcontinent, there is Clustering effect of COVID-19 on all the stock market\nexcept American's stock market due to smart capital investing.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06450v1"
    },
    {
        "title": "Modelling uncertainty in coupled electricity and gas systems -- is it\n  worth the effort?",
        "authors": [
            "Iegor Riepin",
            "Thomas Möbius",
            "Felix Müsgens"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The interdependence of electricity and natural gas markets is becoming a\nmajor topic in energy research. Integrated energy models are used to assist\ndecision-making for businesses and policymakers addressing challenges of energy\ntransition and climate change. The analysis of complex energy systems requires\nlarge-scale models, which are based on extensive databases, intertemporal\ndynamics and a multitude of decision variables. Integrating such energy system\nmodels results in increased system complexity. This complexity poses a\nchallenge for energy modellers to address multiple uncertainties that affect\nboth markets. Stochastic optimisation approaches enable an adequate\nconsideration of uncertainties in investment and operation planning; however,\nstochastic modelling of integrated large-scale energy systems further scales\nthe level of complexity. In this paper, we combine integrated and stochastic\noptimisation problems and parametrise our model for European electricity and\ngas markets. We analyse and compare the impact of uncertain input parameters,\nsuch as gas and electricity demand, renewable energy capacities and fuel and\nCO2 prices, on the quality of the solution obtained in the integrated\noptimisation problem. Our results quantify the value of encoding uncertainty as\na part of a model. While the methodological contribution should be of interest\nfor energy modellers, our findings are relevant for industry experts and\nstakeholders with an empirical interest in the European energy system.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.07221v2"
    },
    {
        "title": "The behavior of stock market prices throughout the episodes of capital\n  inflows",
        "authors": [
            "Boubekeur Baba",
            "Guven Sevil"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This study aims to investigate the behavior of stock prices throughout the\nepisodes of foreign capital flows using data of daily stock prices and\nquarterly foreign capital flows from 14 EMEs. To this end, the episodes of\ncapital flows are identified using the threshold and the k-means clustering\napproaches. Next, the stock index changepoints are detected using the Pruned\nExact Linear Time (PELT) method. Finally, we combine the results by\ndistributing the detected changepoints over the identified capital flows. The\nresults reveal that the stock indices have been rarely pushed further during\nthe entire surge episodes identified by both approaches, and thus surges of\ncapital flows do not necessarily lead to further appreciation of stock prices.\nIn the meantime, a significant appreciation of stock prices is observed during\nthe normal state of capital flows. On the other hand, it is noticed that the\nstock prices have not often depreciated during the episodes of foreign capital\noutflows in all the selected EMEs, which means that stock prices have been less\nvulnerable to reversals of foreign capital flows\n",
        "pdf_link": "http://arxiv.org/pdf/2008.13472v1"
    },
    {
        "title": "Should we build more large dams? The actual costs of hydropower\n  megaproject development",
        "authors": [
            "Atif Ansar",
            "Bent Flyvbjerg",
            "Alexander Budzier",
            "Daniel Lunn"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  A brisk building boom of hydropower mega-dams is underway from China to\nBrazil. Whether benefits of new dams will outweigh costs remains unresolved\ndespite contentious debates. We investigate this question with the \"outside\nview\" or \"reference class forecasting\" based on literature on decision-making\nunder uncertainty in psychology. We find overwhelming evidence that budgets are\nsystematically biased below actual costs of large hydropower dams - excluding\ninflation, substantial debt servicing, environmental, and social costs. Using\nthe largest and most reliable reference data of its kind and multilevel\nstatistical techniques applied to large dams for the first time, we were\nsuccessful in fitting parsimonious models to predict cost and schedule\noverruns. The outside view suggests that in most countries large hydropower\ndams will be too costly in absolute terms and take too long to build to deliver\na positive risk-adjusted return unless suitable risk management measures\noutlined in this paper can be affordably provided. Policymakers, particularly\nin developing countries, are advised to prefer agile energy alternatives that\ncan be built over shorter time horizons to energy megaprojects.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.0002v1"
    },
    {
        "title": "What You Should Know About Megaprojects, and Why: An Overview",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  This paper takes stock of megaproject management, an emerging and hugely\ncostly field of study. First, it answers the question of how large megaprojects\nare by measuring them in the units mega, giga, and tera, concluding we are\npresently entering a new \"tera era\" of trillion-dollar projects. Second, total\nglobal megaproject spending is assessed, at USD 6-9 trillion annually, or 8\npercent of total global GDP, which denotes the biggest investment boom in human\nhistory. Third, four \"sublimes\" - political, technological, economic, and\naesthetic - are identified to explain the increased size and frequency of\nmegaprojects. Fourth, the \"iron law of megaprojects\" is laid out and\ndocumented: Over budget, over time, over and over again. Moreover, the\n\"break-fix model\" of megaproject management is introduced as an explanation of\nthe iron law. Fifth, Albert O. Hirschman's theory of the Hiding Hand is\nrevisited and critiqued as unfounded and corrupting for megaproject thinking in\nboth the academy and policy. Sixth, it is shown how megaprojects are\nsystematically subject to \"survival of the unfittest,\" explaining why the worst\nprojects get built instead of the best. Finally, it is argued that the\nconventional way of managing megaprojects has reached a \"tension point,\" where\ntradition is challenged and reform is emerging.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.0003v1"
    },
    {
        "title": "Beyond the Power Law: Uncovering Stylized Facts in Interbank Networks",
        "authors": [
            "Benjamin Vandermarliere",
            "Alexei Karas",
            "Jan Ryckebusch",
            "Koen Schoors"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We use daily data on bilateral interbank exposures and monthly bank balance\nsheets to study network characteristics of the Russian interbank market over\nAug 1998 - Oct 2004. Specifically, we examine the distributions of (un)directed\n(un)weighted degree, nodal attributes (bank assets, capital and\ncapital-to-assets ratio) and edge weights (loan size and counterparty\nexposure). We search for the theoretical distribution that fits the data best\nand report the \"best\" fit parameters. We observe that all studied distributions\nare heavy tailed. The fat tail typically contains 20% of the data and can be\nmostly described well by a truncated power law. Also the power law, stretched\nexponential and log-normal provide reasonably good fits to the tails of the\ndata. In most cases, however, separating the bulk and tail parts of the data is\nhard, so we proceed to study the full range of the events. We find that the\nstretched exponential and the log-normal distributions fit the full range of\nthe data best. These conclusions are robust to 1) whether we aggregate the data\nover a week, month, quarter or year; 2) whether we look at the \"growth\" versus\n\"maturity\" phases of interbank market development; and 3) with minor\nexceptions, whether we look at the \"normal\" versus \"crisis\" operation periods.\nIn line with prior research, we find that the network topology changes greatly\nas the interbank market moves from a \"normal\" to a \"crisis\" operation period.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.3738v2"
    },
    {
        "title": "Risk Premia: Asymmetric Tail Risks and Excess Returns",
        "authors": [
            "Y. Lempérière",
            "C. Deremble",
            "T. T. Nguyen",
            "P. Seager",
            "M. Potters",
            "J. P. Bouchaud"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We present extensive evidence that ``risk premium'' is strongly correlated\nwith tail-risk skewness but very little with volatility. We introduce a new,\nintuitive definition of skewness and elicit an approximately linear relation\nbetween the Sharpe ratio of various risk premium strategies (Equity,\nFama-French, FX Carry, Short Vol, Bonds, Credit) and their negative skewness.\nWe find a clear exception to this rule: trend following has both positive\nskewness and positive excess returns. This is also true, albeit less markedly,\nof the Fama-French ``Value'' factor and of the ``Low Volatility'' strategy.\nThis suggests that some strategies are not risk premia but genuine market\nanomalies. Based on our results, we propose an objective criterion to assess\nthe quality of a risk-premium portfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.7720v3"
    },
    {
        "title": "An Investigation into Laboucheres Betting System to Improve Odds of\n  Favorable Outcomes to Generate a Positive Externality Empirically",
        "authors": [
            "Jake Billings",
            "Sebastian Del Barco"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  The Labouchere gambling system is hypothesized to increase the probability of\nwinning a predetermined arbitrary profit in a gambling system such as a coin\nflip or a roulette game in which both payouts and odds are 1:1. However, use of\nthe system increases the downside monetary risk in the event of a streak of\nmultiple losses. To begin, a player creates an arbitrary series of consecutive\nintegers with a sum equal to the desired profit from multiple rounds of\nbetting. Using the system, a player will either win an amount equal to the sum\nof the elements of the initial series or lose all of their available capital.\nThis sequence was simulated multiple times to determine the statistical\ncharacteristics of both the return and of the loss in an average round of\nbetting. By running the simulations of millions of rounds of Labouchere, it was\npossible to discern the probable outcomes of running the system using the\nLabouchere gambling sequence and plotting the results on a graph to map the\naverage return on the initial capital investment. The Labouchere system is very\npsychologically appealing to players because when applied over time it provides\nvery consistent linear returns. However, there is eventually a critical moment\nat which the available capital for betting is exceeded and a player loses all\nof their available capital. It was found that as the number of bets increased,\nthe outcome of applying the sequence approached zero.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.00529v1"
    },
    {
        "title": "Equity Default Clawback Swaps to Implement Venture Banking",
        "authors": [
            "Brian P. Hanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  In this theoretical paper, I propose creation of a venture bank, able to\nmultiply the capital of a venture capital firm by at least 47 times, without\nrequiring access to the Federal Reserve or other central bank apart from\nsettlement. This concept rests on obtaining default swap instruments on loans\nin order to create the capital required, and expand Tier 1 and 2 base capital.\nProfitability depends on overall portfolio performance, availability of equity\ndefault swaps, cost of default swap, and the multiple of original capital (MOC)\nadopted by the venture bank. A new derivative financial instrument, the equity\ndefault swap (EDS), to cover loans made as venture investments. An EDS is\nsimilar to a credit default swap (CDS) but with some unique features. The\nfeatures and operation of these new derivative instruments are outlined along\nwith audit requirements. This instrument would be traded on open-outcry\nexchanges with special features to ensure orderly operation of the market. It\nis the creation of public markets for EDSs that makes possible the use of\npublic market pricing to indirectly provide a potential market capitalization\nfor the underlying venture-bank investment. Full coverage insulates the\nventure-bank from losses in most situations, and multiplies profitability quite\ndramatically in all scenarios. Ten year returns above 20X are attainable.\nFurther, a new feature for EDS derivatives, a clawback lien, closes out the\nequity default swap. Here it is optimized at 77%, and is to be paid back to the\nunderwriter at a future date to prevent perverse incentive to deliberately\nfail. This new feature creates an Equity Default Clawback Swap (EDCS) which can\nbe used safely. This proposal also solves an old problem in banking, because it\nmatches the term of the loan with the term of the investment. I show that the\nventure-bank investment and the EDCS underwriting business are profitable.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.08078v9"
    },
    {
        "title": "Knowledge Diffusion Process & Common Islamic Banking Governance\n  Principles: Integrative Perspective (s) of Managers and Shariah Scholars",
        "authors": [
            "Adnan Malik",
            "Karim Ullah",
            "Shakir Ullah"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Islamic banks being commercial entities strive to earn profit within shariah\nambit. Therefore, they seem to be basing themselves upon two knowledge streams\nnamely i) Islamic jurisprudence principles, and ii) banking principles. Islamic\njurisprudence principles primarily aim at bringing shariah compliance while\nbanking principles focus profitability. These principles, making two schools of\nthought in the discipline, however, have their unique philosophies, principles,\nand practices, which are now gradually diffusing into an emergent set of\ngovernance principles basing the contemporary Islamic banking theory and\npractice. Governance systems of Islamic banks have elements of both\nconventional as well as Shariah, and need to have principles having components\nof banking and shariah sufficiently diffused for their successful operations in\na longer term. Aim of this research is to review the literature about the\nknowledge diffusion process of islamic banking principles which guides the\ngovernance of Islamic banks. This study review the literature using a method in\nwhich focus remain on bridging different areas which in this case are knowledge\ndiffusion and islamic banking governance principles.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04067v1"
    },
    {
        "title": "D-Brane solutions under market panic",
        "authors": [
            "R. Pincak"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The relativistic quantum mechanic approach is used to develop a stock market\ndynamics. The relativistic is conceptional here as the meaning of big external\nvolatility or volatility shock on a financial market. We used a differential\ngeometry approach with the parallel transport of the prices to obtain a direct\nshift of the stock price movement. The prices are represented here as electrons\nwith different spin orientation. Up and down orientations of the spin particle\nare likened here as an increase or a decrease of stock prices. The paralel\ntransport of stock prices is enriched about Riemann curvature which describes\nsome arbitrage opportunities in the market. To solve the stock-price dynamics,\nwe used the Dirac equation for bispinors on the spherical brane-world. We found\nthat when a spherical brane is abbreviated to the disk on the equator, we\nconverge to the ideal behaviour of financial market where Black Scholes as well\nas semi-classical equations are sufficient. Full spherical brane-world\nscenarios can descibe a non-equilibrium market behaviour were all arbitrage\nopportunities as well as transaction costs are take into account.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0190v1"
    },
    {
        "title": "Revisiting the Merit-Order Effect of Renewable Energy Sources",
        "authors": [
            "Marcus Hildmann",
            "Andreas Ulbig",
            "Göran Andersson"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  An on-going debate in the energy economics and power market community has\nraised the question if energy-only power markets are increasingly failing due\nto growing feed-in shares from subsidized renewable energy sources (RES). The\nshort answer to this is: No, they are not failing. Energy-based power markets\nare, however, facing several market distortions, namely from the gap between\nthe electricity volume traded at day-ahead markets versus the overall\nelectricity consumption as well as the (wrong) regulatory assumption that\nvariable RES generation, i.e., wind and photovoltaic (PV), truly have zero\nmarginal operation costs. In this paper we show that both effects over-amplify\nthe well-known merit-order effect of RES power feed-in beyond a level that is\nexplainable by underlying physical realities, i.e., thermal power plants being\nwilling to accept negative electricity prices to be able to stay online due to\nconsiderations of wear & tear and start-stop constraints. We analyze the\nimpacts of wind and PV power feed-in on the day-ahead market for a region that\nis already today experiencing significant feed-in tariff (FIT)-subsidized RES\npower feed-in, the EPEX German-Austrian market zone ($\\approx\\,$20% FIT share).\nOur analysis shows that, if the necessary regulatory adaptations are taken,\ni.e., increasing the day-ahead market's share of overall load demand and using\nthe true marginal costs of RES units in the merit-order, energy-based power\nmarkets can remain functional despite high RES power feed-in.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0444v3"
    },
    {
        "title": "Evolution of the distribution of wealth in an economic environment\n  driven by local Nash equilibria",
        "authors": [
            "Pierre Degond",
            "Jian-Guo Liu",
            "Christian Ringhofer"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We present and analyze a model for the evolution of the wealth distribution\nwithin a heterogeneous economic environment. The model considers a system of\nrational agents interacting in a game theoretical framework, through fairly\ngeneral assumptions on the cost function. This evolution drives the dynamic of\nthe agents in both wealth and economic configuration variables. We consider a\nregime of scale separation where the large scale dynamics is given by a\nhydrodynamic closure with a Nash equilibrium serving as the local thermodynamic\nequilibrium. The result is a system of gas dynamics-type equations for the\ndensity and average wealth of the agents on large scales. We recover the\ninverse gamma distribution as an equilibrium in the particular case of\nquadratic cost functions which has been previously considered in the\nliterature.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.1685v1"
    },
    {
        "title": "Cost overruns in Large-Scale Transportation Infrastructure Projects:\n  Explanations and Their Theoretical Embeddedness",
        "authors": [
            "Chantal C. Cantarelli",
            "Bent Flybjerg",
            "Eric J. E. Molin",
            "Bert van Wee"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Managing large-scale transportation infrastructure projects is difficult due\nto frequent misinformation about the costs which results in large cost overruns\nthat often threaten the overall project viability. This paper investigates the\nexplanations for cost overruns that are given in the literature. Overall, four\ncategories of explanations can be distinguished: technical, economic,\npsychological, and political. Political explanations have been seen to be the\nmost dominant explanations for cost overruns. Agency theory is considered the\nmost interesting for political explanations and an eclectic theory is also\nconsidered possible. Nonpolitical explanations are diverse in character,\ntherefore a range of different theories (including rational choice theory and\nprospect theory), depending on the kind of explanation is considered more\nappropriate than one all-embracing theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2176v1"
    },
    {
        "title": "Lock-in and Its Influence on the Project Performance of Large-Scale\n  Transportation Infrastructure Projects. Investigating the Way in Which\n  Lock-in Can Emerge and Affect Cost Overruns",
        "authors": [
            "Chantal C. Cantarelli",
            "Bent Flybjerg",
            "Bert van Wee",
            "Eric J. E. Molin"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Lock-in, the escalating commitment of decision-makers to an ineffective\ncourse of action, has the potential to explain the large cost overruns in large\nscale transportation infrastructure projects. Lock-in can occur both at the\ndecision-making level (before the decision to build) and at the project level\n(after the decision to build) and can influence the extent of overruns in two\nways. The first involves the methodology of calculating cost overruns according\nto the formal decision to build. Due to lock-in, however, the real decision to\nbuild is made much earlier in the decision-making process and the costs\nestimated at that stage are often much lower than those that are estimated at a\nlater stage in the decision-making process, thus increasing cost overruns. The\nsecond way that lock-in can affect cost overruns is through practice. Although\ndecisions about the project (design and implementation) need to be made,\nlock-in can lead to inefficient decisions that involve higher costs. Sunk costs\n(in terms of both time and money), the need for justification, escalating\ncommitment, and inflexibility and the closure of alternatives are indicators of\nlock-in. In this paper, two case studies, of the Betuweroute and the HSL-South\nprojects in the Netherlands, demonstrate the presence of lock-in and its\ninfluence on the extent of cost overruns at both the decision-making and\nproject levels. This suggests that recognition of lock-in as an explanation for\ncost overruns significantly contributes to the understanding of the inadequate\nplanning process of projects and allows development of more appropriate means.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2177v1"
    },
    {
        "title": "Characteristics of Cost Overruns for Dutch Transport Infrastructure\n  Projects and the Importance of the Decision to Build and Project Phases",
        "authors": [
            "Chantal C. Cantarelli",
            "Eric J. E. Molin",
            "Bert van Wee",
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Using a methodology similar to that used the in the worldwide research, the\ncost performance of Dutch large-scale transport infrastructure projects is\ndetermined. In the Netherlands, cost overruns are as common as cost underruns\nbut because cost overruns are larger than cost underruns projects on average\nhave a cost overrun of 16.5%. The focus on one country further enabled to\nconsider cost overruns during different project development phases. It turned\nout that in the Netherlands the majority of the cost overrun occurs in the\npre-construction phase (the period between the formal decision to build and the\nstart of construction). The frequency as well as the magnitude of\npre-construction cost overrun is significantly higher than in the construction\nphase. The used methodology of calculating cost overruns does however not take\nlock-in into account. This phenomenon shows that the real decision to build was\ntaken much earlier in the decision-making process. Since estimated costs are\nusually lower during these earlier stages, the cost overruns based on this real\ndecision to build are likely to be much higher. Cost overruns presented in\nstudies are therefore often underestimated and the problem of cost overruns is\nmuch larger than we think.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2178v2"
    },
    {
        "title": "Different Cost Performance: Different Determinants? The Case of Cost\n  Overruns in Dutch Transportation Infrastructure Projects",
        "authors": [
            "Chantal C. Cantarelli",
            "Bert van Wee",
            "Eric J. E. Molin",
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper examines three independent explanatory variables and their\nrelation with cost overrun in order to decide whether this is different for\nDutch infrastructure projects compared to worldwide findings. The three\nindependent variables are project type (road, rail, and fixed link projects),\nproject size (measured in terms of estimated costs) and the length of the\nproject implementation phase. For Dutch projects, average cost overrun is 10.6%\nfor rail, 18.6% for roads and 21.7% for fixed links. For project size, small\nDutch projects have the largest average percentage cost overruns but in terms\nof total overrun, large projects have a larger share. The length of the\nimplementation phase and especially the length of the pre-construction phase\nare important determinants of cost overruns in the Netherlands. With each\nadditional year of pre-construction, percentage cost overrun increases by five\npercentage points. In contrast, the length of the construction phase has hardly\nany influence on cost overruns. This is an important contribution to current\nknowledge about cost overruns, because the period in which projects are most\nprone to cost overruns is narrowed down considerably, at least in the\nNetherlands. This means that period can be focused on to determine the causes\nand cures of overruns.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2179v2"
    },
    {
        "title": "Explaining Cost Overruns of Large-Scale Transportation Infrastructure\n  Projects using a Signalling Game",
        "authors": [
            "Chantal C. Cantarelli",
            "Caspar G. Chorus",
            "Scott W. Cunningham"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Strategic behaviour is one of the main explanations for cost overruns. It can\ntheoretically be supported by agency theory, in which strategic behaviour is\nthe result of asymmetric information between the principal and agent. This\npaper gives a formal account of this relation by a signalling game. This is a\ngame with incomplete information which considers the way in which parties\nanticipate upon other parties' behaviour in choosing a course of action. The\ngame shows how cost overruns are the result of an inappropriate signal. This\nmakes it impossible for the principal to distinguish between the types of\nagents, and hence, allows for strategic behaviour. It is illustrated how cost\noverruns can be avoided by means of two policy measures, e.g. an accountability\nstructure and benchmarking.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2180v1"
    },
    {
        "title": "Geographical Variation in Project Cost Performance: The Netherlands\n  versus Worldwide",
        "authors": [
            "Chantal C. Cantarelli",
            "Bent Flyvbjerg",
            "Søren L. Buhl"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Cost overruns in transport infrastructure projects know no geographical\nlimits, overruns are a global phenomenon. Nevertheless, the size of cost\noverruns varies with location. In the Netherlands, cost overruns appear to be\nsmaller compared to the rest of the world. This paper tests whether Dutch\nprojects perform significantly better in terms of cost overruns than other\ngeographical areas. It is concluded that for road and tunnel projects, the\nNetherlands performs similarly to the rest of the world. For rail projects,\nDutch projects perform considerably better, with projects having significantly\nlower percentage cost overruns in real terms (11%) compared to projects in\nother North West European countries (27%) and in other geographical areas\n(44%). Bridge projects also have considerably smaller cost overruns: 7% in the\nNetherlands compared with 45% in other NW European countries and 27% in other\ngeographical areas. In explaining cost overruns, geography should therefore\nclearly be taken into consideration.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2181v1"
    },
    {
        "title": "Uncertainty and absence of arbitrage opportunity",
        "authors": [
            "Yaroslav Ivanenko",
            "Illya Pasichnichenko"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  It is shown that absence of arbitrage opportunity in financial markets is a\nparticular case of existence of uncertainty in decision system. Absence of\narbitrage opportunity is considered in the sense of the Arrow-Debreu model of\nfinancial market with a riskless asset, while uncertainty (or ambiguity) is\ndefined on the basis of the principle of internal coherence of M. Allais.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.5602v1"
    },
    {
        "title": "Correct usage of transmission coefficient for timing the market",
        "authors": [
            "Ovidiu Racorean"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Traders and investors involved in an option contract having the underlying\nstock in range bound are likely to lose their initial investment. Timing in\nbuying an option contract is of capital importance. In a recent article [1] the\nhypothesis of range bound market is used in conjunction to Black-Scholes\nequation to find the transmission coefficient relation that help market\nprofessionals to correctly timing their investment and risk taking decisions.\nThe present paper explores the theoretical basis of transmission coefficient\nand its empirical evidence on the market.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.5975v1"
    },
    {
        "title": "A Spectral Model of Turnover Reduction",
        "authors": [
            "Zura Kakushadze"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We give a simple explicit formula for turnover reduction when a large number\nof alphas are traded on the same execution platform and trades are crossed\ninternally. We model turnover reduction via alpha correlations. Then, for a\nlarge number of alphas, turnover reduction is related to the largest eigenvalue\nand the corresponding eigenvector of the alpha correlation matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.5050v4"
    },
    {
        "title": "Towards a Monotonicity-Compliant Price Index for the Art Market",
        "authors": [
            "Ventura Charlin",
            "Arturo Cifuentes"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Notwithstanding almost forty years of efforts, the market for paintings still\nlacks a widely accepted price index. In this paper, we introduce a simple and\nintuitive metric to construct such index. Our metric is based on the price of a\npainting divided by its area. This formulation rests on a solid mathematical\nfoundation as it corresponds to a particular type of hedonic model. However,\nunlike indexes based on the time-dummy coefficients of conventional hedonic\nmodels, this index satisfies the monotonicity condition. We demonstrate with a\nsimple example the advantages of our metric. We also show the dangers of\nrelying on the time-dummy coefficients of conventional hedonic models to\nestimate returns and generate price indexes.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.5203v1"
    },
    {
        "title": "Measurement and Internalization of Systemic Risk in a Global Banking\n  Network",
        "authors": [
            "Xiaobing Feng",
            "Haibo Hu"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The negative externalities from an individual bank failure to the whole\nsystem can be huge. One of the key purposes of bank regulation is to\ninternalize the social costs of potential bank failures via capital charges.\nThis study proposes a method to evaluate and allocate the systemic risk to\ndifferent countries/regions using a SIR type of epidemic spreading model and\nthe Shapley value in game theory. The paper also explores features of a\nconstructed bank network using real globe-wide banking data.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.5689v1"
    },
    {
        "title": "A Multi-Entity Input Output (MEIO) Approach to Sustainability -\n  Water-Energy-GHG (WEG) Footprint Statements in Use Cases from Auto and Telco\n  Industries",
        "authors": [
            "Reza Farrahi Moghaddam",
            "Fereydoun Farrahi Moghaddam",
            "Mohamed Cheriet"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  A new Input-Output model, called the Multi-Entity Input-Output (MEIO) model,\nis introduced to estimate the responsibility of entities of an ecosystem on the\nfootprint of each other. It assumed that the ecosystem is comprised of end\nusers, service providers, and utilities. The proposed MEIO modeling approach\ncan be seen as a realization of the Everybody-in-the-Loop (EitL) framework,\nwhich promotes a sustainable future using behaviors and actions that are aware\nof their ubiquitous eco-socio-environment impacts. In this vision, the\nbehavioral changes could be initiated by providing all actors with their\nfootprint statement, which would be estimated using the MEIO models. First, a\nnaive MEIO model is proposed in the form of a graph of actions and\nresponsibility by considering interactions and goods transfers among the\nentities and actors along four channels. Then, the unnormalized responsibility\nand also the final responsibility among the actors are introduced, and then are\nused to re-allocate immediate footprint of actors among themselves. The\nfootprint in the current model is limited to three major impacts: Water,\nEnergy, and GHG emissions. The naive model is then generalized to\nProvider-perspective (P-perspective) and End User-perspective (E-perspective)\nMEIO models in order to make it more suitable to cases where a large number of\nend users are served by a provider. The E-perspective modeling approach\nparticularly allows estimating the footprint associated to a specific end user.\nIn two use cases from the auto and Telco industries, it has been observed that\nthe proposed MEIO models are practical and dependable in allocating footprint\nto the provider and also to the end user, while i) avoiding footprint leakage\nto the end users and ii) handling the large numbers end users. In addition, it\nwill be shown that the MEIO models could be sued to integrate Scope-3 and LCA\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.6227v2"
    },
    {
        "title": "Many-to-one contagion of economic growth rate across trade credit\n  network of firms",
        "authors": [
            "Natasa Golo",
            "Guy Kelman",
            "David S. Bree",
            "Leanne Usher",
            "Marco Lamieri",
            "Sorin Solomon"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We propose a novel approach and an empirical procedure to test direct\ncontagion of growth rate in a trade credit network of firms. Our hypotheses are\nthat the use of trade credit contributes to contagion (from many customers to a\nsingle supplier - \"many to one\" contagion) and amplification (through their\ninteraction with the macrocopic variables, such as interest rate) of growth\nrate. In this paper we test the contagion hypothesis, measuring empirically the\nmesoscopic \"many-to-one\" effect. The effect of amplification has been dealt\nwith in another paper.\n  Our empirical analysis is based on the delayed payments between trading\npartners across many different industrial sectors, intermediated by a large\nItalian bank during the year 2007. The data is used to create a weighted and\ndirected trade credit network. Assuming that the linkages are static, we look\nat the dynamics of the nodes/firms. On the ratio of the 2007 trade credit in\nSales and Purchases items on the profit and loss statements, we estimate the\ntrade credit in 2006 and 2008.\n  Applying the \"many to one\" approach we compare such predicted growth of trade\n(demand) aggregated per supplier, and compare it with the real growth of Sales\nof the supplier. We analyze the correlation of these two growth rates over two\nyearly periods, 2007/2006 and 2008/2007, and in this way we test our contagion\nhypotheses. We could not find strong correlations between the predicted and the\nactual growth rates. We provide an evidence of contagion only in restricted\nsub-groups of our network, and not in the whole network. We do find a strong\nmacroscopic effect of the crisis, indicated by a coincident negative drift in\nthe growth of sales of nearly all the firms in our sample.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01734v1"
    },
    {
        "title": "Too dynamic to fail. Empirical support for an autocatalytic model of\n  Minsky's financial instability hypothesis",
        "authors": [
            "Natasa Golo",
            "David S. Bree",
            "Guy Kelman",
            "Leanne Usher",
            "Marco Lamieri",
            "Sorin Solomon"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Solomon and Golo [1] have recently proposed an autocatalytic\n(self-reinforcing) feedback model which couples a macroscopic system parameter\n(the interest rate), a microscopic parameter that measures the distribution of\nthe states of the individual agents (the number of firms in financial\ndifficulty) and a peer-to-peer network effect (contagion across supply chain\nfinancing). In this model, each financial agent is characterized by its\nresilience to the interest rate. Above a certain rate the interest due on the\nfirm's financial costs exceeds its earnings and the firm becomes susceptible to\nfailure (ponzi). For the interest rate levels under a certain threshold level,\nthe firm loans are smaller then its earnings and the firm becomes 'hedge.' In\nthis paper, we fit the historical data (2002-2009) on interest rate data into\nour model, in order to predict the number of the ponzi firms. We compare the\nprediction with the data taken from a large panel of Italian firms over a\nperiod of 9 years. We then use trade credit linkages to discuss the connection\nbetween the ponzi density and the network percolation.\n  We find that the 'top-down'-'bottom-up' positive feedback loop accounts for\nmost of the Minsky crisis accelerator dynamics. The peer-to-peer ponzi\ncompanies contagion becomes significant only in the last stage of the crisis\nwhen the ponzi density is above a critical value. Moreover the ponzi contagion\nis limited only to the companies that were not dynamic enough to substitute\ntheir distressed clients with new ones. In this respect the data support a view\nin which the success of the economy depends on substituting the static\n'supply-network' picture with an interacting dynamic agents one.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.07582v2"
    },
    {
        "title": "Do investors trade too much? A laboratory experiment",
        "authors": [
            "Joao da Gama Batista",
            "Domenico Massaro",
            "Jean-Philippe Bouchaud",
            "Damien Challet",
            "Cars Hommes"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We run experimental asset markets to investigate the emergence of excess\ntrading and the occurrence of synchronised trading activity leading to crashes\nin the artificial markets. The market environment favours early investment in\nthe risky asset and no posterior trading, i.e. a buy-and-hold strategy with a\nmost probable return of over 600%. We observe that subjects trade too much, and\ndue to the market impact that we explicitly implement, this is detrimental to\ntheir wealth. The asset market experiment was followed by risk aversion\nmeasurement. We find that preference for risk systematically leads to higher\nactivity rates (and lower final wealth). We also measure subjects' expectations\nof future prices and find that their actions are fully consistent with their\nexpectations. In particular, trading subjects try to beat the market and make\nprofits by playing a buy low, sell high strategy. Finally, we have not detected\nany major market crash driven by collective panic modes, but rather a weaker\nbut significant tendency of traders to synchronise their entry and exit points\nin the market.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.03743v1"
    },
    {
        "title": "The Role of Time in Making Risky Decisions and the Function of Choice",
        "authors": [
            "Valerii Salov"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The prospects of Kahneman and Tversky, Mega Million and Powerball lotteries,\nSt. Petersburg paradox, premature profits and growing losses criticized by\nLivermore are reviewed under an angle of view comparing mathematical\nexpectations with awards received. Original prospects have been formulated as a\none time opportunity. An award value depends on the number of times the game is\nplayed. The random sample mean is discussed as a universal award. The role of\ntime in making a risky decision is important as long as the frequency of games\nand playing time affect their number. A function of choice mapping properties\nof two-point random variables to fractions of respondents choosing them is\nproposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.08792v1"
    },
    {
        "title": "Rational Decision-Making Under Uncertainty: Observed Betting Patterns on\n  a Biased Coin",
        "authors": [
            "Victor Haghani",
            "Richard Dewey"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  What would you do if you were invited to play a game where you were given\n\\$25 and allowed to place bets for 30 minutes on a coin that you were told was\nbiased to come up heads 60% of the time? This is exactly what we did, gathering\n61 young, quantitatively trained men and women to play this game. The results,\nin a nutshell, were that the majority of these 61 players did not place their\nbets very well, displaying a broad panoply of behaviorial and cognitive biases.\nAbout 30% of the subjects actually went bust, losing their full \\$25 stake. We\nalso discuss optimal betting strategies, valuation of the opportunity to play\nthe game and its similarities to investing in the stock market. The main\nimplication of our study is that people need to be better educated and trained\nin how to approach decision making under uncertainty. If these quantitatively\ntrained players, playing the simplest game we can think of involving\nuncertainty and favourable odds, did not play well, what hope is there for the\nrest of us when it comes to playing the biggest and most important game of all:\ninvesting our savings? In the words of Ed Thorp, who gave us helpful feedback\non our research: \"This is a great experiment for many reasons. It ought to\nbecome part of the basic education of anyone interested in finance or\ngambling.\"\n",
        "pdf_link": "http://arxiv.org/pdf/1701.01427v1"
    },
    {
        "title": "Predicting Economic Recessions Using Machine Learning Algorithms",
        "authors": [
            "Rickard Nyman",
            "Paul Ormerod"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Even at the beginning of 2008, the economic recession of 2008/09 was not\nbeing predicted. The failure to predict recessions is a persistent theme in\neconomic forecasting. The Survey of Professional Forecasters (SPF) provides\ndata on predictions made for the growth of total output, GDP, in the United\nStates for one, two, three and four quarters ahead since the end of the 1960s.\nOver a three quarters ahead horizon, the mean prediction made for GDP growth\nhas never been negative over this period. The correlation between the mean SPF\nthree quarters ahead forecast and the data is very low, and over the most\nrecent 25 years is not significantly different from zero.\n  Here, we show that the machine learning technique of random forests has the\npotential to give early warning of recessions. We use a small set of\nexplanatory variables from financial markets which would have been available to\na forecaster at the time of making the forecast. We train the algorithm over\nthe 1970Q2-1990Q1 period, and make predictions one, three and six quarters\nahead. We then re-train over 1970Q2-1990Q2 and make a further set of\npredictions, and so on. We did not attempt any optimisation of predictions,\nusing only the default input parameters to the algorithm we downloaded in the\npackage R.\n  We compare the predictions made from 1990 to the present with the actual\ndata. One quarter ahead, the algorithm is not able to improve on the SPF\npredictions. Three and six quarters ahead, the correlations between actual and\npredicted are low, but they are very significantly different from zero.\nAlthough the timing is slightly wrong, a serious downturn in the first half of\n2009 could have been predicted six quarters ahead in late 2007. The algorithm\nnever predicts a recession when one did not occur.\n  We obtain even stronger results with random forest machine learning\ntechniques in the case of the United Kingdom.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.01428v1"
    },
    {
        "title": "Political elections and uncertainty -Are BRICS markets equally exposed\n  to Trump's agenda?",
        "authors": [
            "Jamal Bouoiyour",
            "Refk Selmi"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  There certainly is little or no doubt that politicians, sometimes consciously\nand sometimes not, exert a significant impact on stock markets. The evolving\nvolatility over the Republican Donald Trump's surprise victory in the US\npresidential election is a perfect example when politicians, through announced\npolicies, send signals to financial markets. The present paper seeks to address\nwhether BRICS (Brazil, Russia, India, China and South Africa) stock markets\nequally vulnerable to Trump's plans. For this purpose, two methods were\nadopted. The first presents an event-study methodology based on regression\nestimation of abnormal returns. The second is based on vote intentions by\nintegrating data from social media (Twitter), search queries (Google Trends)\nand public opinion polls. Our results robustly reveal that although some\nmarkets emerged losers, others took the opposite route. China took the biggest\nhit with Brazil, while the damage was much more limited for India and South\nAfrica. These adverse responses can be explained by the Trump's\nneo-mercantilist attitude revolving around tearing up trade deals, instituting\ntariffs, and labeling China a \"currency manipulator\". However, Russia looks to\nbe benefiting due to Trump's sympathetic attitude towards Vladimir Putin and\nexpectations about the scaling down of sanctions imposed on Russia over its\nrole in the conflict in Ukraine.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.02182v1"
    },
    {
        "title": "The Influence of Collaboration in Procurement Relationships",
        "authors": [
            "Wesley S. Boyce",
            "Haim Mano",
            "John L. Kent"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Supply Chain Management often requires independent organizations to work\ntogether to achieve shared objectives. This collaboration is necessary when\ncoordinated actions benefit the group more than the uncoordinated efforts of\nindividual firms. Despite the commonly reported benefits that can be gained in\nclose relationships, recent research has indicated that collaboration attempts\nbetween purchasing firms and their suppliers have not been as widespread as\nanticipated. Using a survey of procurement professionals, this research\ninvestigates how the purchasing function utilizes collaboration in its supply\nchain relationships. Structural equation modeling is used to identify how\ninformation sharing, decision synchronization, incentive alignment,\ncollaborative communication, and trust impact collaboration, as well as how\ncollaboration impacts performance. Results from 86 survey responses indicate\nthat firms are still not fully utilizing collaborative relationships.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.02647v1"
    },
    {
        "title": "Mathematical models describing the effects of different tax evasion\n  behaviors",
        "authors": [
            "M. L. Bertotti",
            "G. Modanese"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Microscopic models describing a whole of economic interactions in a closed\nsociety are considered. The presence of a tax system combined with a\nredistribution process is taken into account, as well as the occurrence of tax\nevasion. In particular, the existence is postulated, in relation to the level\nof evasion, of different individual taxpayer behaviors. The effects of the\nmentioned different behaviors on shape and features of the emerging income\ndistribution profile are investigated qualitatively and quantitatively.\nNumerical solutions show that the Gini inequality index of the total population\nincreases when the evasion level is higher, but does not depend significantly\non the evasion spread. For fixed spread, the relative difference between the\naverage incomes of the worst evaders and honest taxpayers increases\napproximately as a quadratic function of the evasion level.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.02662v1"
    },
    {
        "title": "Business Dynamics in KPI Space. Some thoughts on how business analytics\n  can benefit from using principles of classical physics",
        "authors": [
            "Alex Ushveridze"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  The biggest problem with the methods of machine learning used today in\nbusiness analytics is that they do not generalize well and often fail when\napplied to new data. One of the possible approaches to this problem is to\nenrich these methods (which are almost exclusively based on statistical\nalgorithms) with some intrinsically deterministic add-ons borrowed from\ntheoretical physics. The idea proposed in this note is to divide the set of Key\nPerformance Indicators (KPIs) characterizing an individual business into the\nfollowing two distinct groups: 1) highly volatile KPIs mostly determined by\nexternal factors and thus poorly controllable by a business, and 2) relatively\nstable KPIs identified and controlled by a business itself. It looks like,\nwhereas the dynamics of the first group can, as before, be studied using\nstatistical methods, for studying and optimizing the dynamics of the second\ngroup it is better to use deterministic principles similar to the Principle of\nLeast Action of classical mechanics. Such approach opens a whole bunch of new\ninteresting opportunities in business analytics, with numerous practical\napplications including diverse aspects of operational and strategic planning,\nchange management, ROI optimization, etc. Uncovering and utilizing dynamical\nlaws of the controllable KPIs would also allow one to use dynamical invariants\nof business as the most natural sets of risk and performance indicators, and\nfacilitate business growth by using effects of parametric resonance with\nnatural business cycles.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.01742v1"
    },
    {
        "title": "For Whom the Bell (Curve) Tolls: A to F, Trade Your Grade Based on the\n  Net Present Value of Friendships with Financial Incentives",
        "authors": [
            "Ravi Kashyap"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We discuss a possible solution to an unintended consequence of having grades,\ncertificates, rankings and other diversions in the act of transferring\nknowledge; and zoom in specifically to the topic of having grades, on a curve.\nWe conduct a thought experiment, taking a chapter (and some more?) from the\nfinancial markets, (where we trade pollution and what not?), to create a\nmarketplace, where we can trade our grade, similar in structure to the interest\nrate swap. We connect this to broader problems that are creeping up,\nunintentionally, due to artificial labels we are attaching, to ourselves. The\npolicy and philosophical implications of our arguments are to suggest that all\ntrophies that we collect (including certificates, grades, medals etc.) should\nbe viewed as personal equity or private equity (borrowing another widely used\nterm in finance) and we should not use them to determine the outcomes in any\nselection criteria except have a cutoff point: either for jobs, higher studies,\nor, financial scholarships, other than for entertainment or spectator sports.\nWe suggest alternate methods for grading and performance assessment and put\nforth tests for teaching and learning similar to the Turing Test for\nintelligence.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.00960v2"
    },
    {
        "title": "A Stock Prediction Model Based on DCNN",
        "authors": [
            "Qiao Zhou",
            "Ningning Liu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The prediction of a stock price has always been a challenging issue, as its\nvolatility can be affected by many factors such as national policies, company\nfinancial reports, industry performance, and investor sentiment etc.. In this\npaper, we present a prediction model based on deep CNN and the candle charts,\nthe continuous time stock information is processed. According to different\ninformation richness, prediction time interval and classification method, the\noriginal data is divided into multiple categories as the training set of CNN.\nIn addition, the convolutional neural network is used to predict the stock\nmarket and analyze the difference in accuracy under different classification\nmethods.\n  The results show that the method has the best performance when the forecast\ntime interval is 20 days. Moreover, the Moving Average Convergence Divergence\nand three kinds of moving average are added as input. This method can\naccurately predict the stock trend of the US NDAQ exchange for 92.2%.\nMeanwhile, this article distinguishes three conventional classification methods\nto provide guidance for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03239v1"
    },
    {
        "title": "The impact of COVID-19 on the stock market crash risk in China",
        "authors": [
            "Zhifeng Liu",
            "Toan Luu Duc Huynh",
            "Peng-Fei Dai"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This study investigates the impact of the COVID-19 pandemic on the stock\nmarket crash risk in China. For this purpose, we first estimated the\nconditional skewness of the return distribution from a GARCH with skewness\n(GARCH-S) model as the proxy for the equity market crash risk of the Shanghai\nStock Exchange. We then constructed a fear index for COVID-19 using data from\nthe Baidu Index. Based on the findings, conditional skewness reacts negatively\nto daily growth in total confirmed cases, indicating that the pandemic\nincreases stock market crash risk. Moreover, the fear sentiment exacerbates\nsuch risk, especially with regard to the impact of COVID-19. In other words,\nwhen the fear sentiment is high, the stock market crash risk is more strongly\naffected by the pandemic. Our evidence is robust for the number of daily deaths\nand global cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08030v2"
    },
    {
        "title": "How Market Ecology Explains Market Malfunction",
        "authors": [
            "Maarten P. Scholl",
            "Anisoara Calinescu",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Standard approaches to the theory of financial markets are based on\nequilibrium and efficiency. Here we develop an alternative based on concepts\nand methods developed by biologists, in which the wealth invested in a\nfinancial strategy is like the abundance of a species. We study a toy model of\na market consisting of value investors, trend followers and noise traders. We\nshow that the average returns of strategies are strongly density dependent,\ni.e. they depend on the wealth invested in each strategy at any given time. In\nthe absence of noise the market would slowly evolve toward an efficient\nequilibrium, but the statistical uncertainty in profitability (which is\nadjusted to match real markets) makes this noisy and uncertain. Even in the\nlong term, the market spends extended periods of time away from perfect\nefficiency. We show how core concepts from ecology, such as the community\nmatrix and food webs, give insight into market behavior. The wealth dynamics of\nthe market ecology explain how market inefficiencies spontaneously occur and\ngives insight into the origins of excess price volatility and deviations of\nprices from fundamental values.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09454v2"
    },
    {
        "title": "A note on the impact of news on US household inflation expectations",
        "authors": [
            "Ben Zhe Wang",
            "Jeffrey Sheen",
            "Stefan Trück",
            "Shih-Kang Chao",
            "Wolfgang Karl Härdle"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Monthly disaggregated US data from 1978 to 2016 reveals that exposure to news\non inflation and monetary policy helps to explain inflation expectations. This\nremains true when controlling for household personal characteristics,\nperceptions of government policy effectiveness, future interest rates and\nunemployment expectations, and sentiment. We find an asymmetric impact of news\non inflation and monetary policy after 1983, with news on rising inflation and\neasier monetary policy having a stronger effect in comparison to news on\nlowering inflation and tightening monetary policy. Our results indicate the\nimpact on inflation expectations of monetary policy news manifested through\nconsumer sentiment during the lower bound period.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.11557v1"
    },
    {
        "title": "Liquidations: DeFi on a Knife-edge",
        "authors": [
            "Daniel Perez",
            "Sam M. Werner",
            "Jiahua Xu",
            "Benjamin Livshits"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The trustless nature of permissionless blockchains renders\novercollateralization a key safety component relied upon by decentralized\nfinance (DeFi) protocols. Nonetheless, factors such as price volatility may\nundermine this mechanism. In order to protect protocols from suffering losses,\nundercollateralized positions can be liquidated. In this paper, we present the\nfirst in-depth empirical analysis of liquidations on protocols for loanable\nfunds (PLFs). We examine Compound, one of the most widely used PLFs, for a\nperiod starting from its conception to September 2020. We analyze participants'\nbehavior and risk-appetite in particular, to elucidate recent developments in\nthe dynamics of the protocol. Furthermore, we assess how this has changed with\na modification in Compound's incentive structure and show that variations of\nonly 3% in an asset's dollar price can result in over 10m USD becoming\nliquidable. To further understand the implications of this, we investigate the\nefficiency of liquidators. We find that liquidators' efficiency has improved\nsignificantly over time, with currently over 70% of liquidable positions being\nimmediately liquidated. Lastly, we provide a discussion on how a false sense of\nsecurity fostered by a misconception of the stability of non-custodial\nstablecoins, increases the overall liquidation risk faced by Compound\nparticipants.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13235v6"
    },
    {
        "title": "Predicting Non Farm Employment",
        "authors": [
            "Tarun Bhatia"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  U.S. Nonfarm employment is considered one of the key indicators for assessing\nthe state of the labor market. Considerable deviations from the expectations\ncan cause market moving impacts. In this paper, the total U.S. nonfarm payroll\nemployment is predicted before the release of the BLS employment report. The\ncontent herein outlines the process for extracting predictive features from the\naggregated payroll data and training machine learning models to make accurate\npredictions. Publically available revised employment report by BLS is used as a\nbenchmark. Trained models show excellent behaviour with R2 of 0.9985 and 99.99%\ndirectional accuracy on out of sample periods from January 2012 to March 2020.\n  Keywords Machine Learning; Economic Indicators; Ensembling; Regression, Total\nNonfarm Payroll\n",
        "pdf_link": "http://arxiv.org/pdf/2009.14282v1"
    },
    {
        "title": "Delusions of Success: Comment on Dan Lovallo and Daniel Kahneman",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Dan Lovallo and Daniel Kahneman must be commended for their clear\nidentification of causes and cures to the planning fallacy in \"Delusions of\nSuccess: How Optimism Undermines Executives' Decisions\" (HBR July 2003). Their\nlook at overoptimism, anchoring, competitor neglect, and the outside view in\nforecasting is highly useful to executives and forecasters. However, Lovallo\nand Kahneman underrate one source of bias in forecasting - the deliberate\n\"cooking\" of forecasts to get ventures started.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0741v1"
    },
    {
        "title": "Are Financial Markets an aspect of Quantum World?",
        "authors": [
            "Ovidiu Racorean"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Writing the article-Time independent pricing of options in range bound\nmarkets; the question in the title came naturally to my mind. It is stated, in\nthe above article, that in certain market conditions the stock price is\nsubjected to an equation that exactly matches a time independent Schrodinger\nequation. The time independent equation for options valuation is used further\nto explain a stock market phenomenon that resembles an alpha particle decay\ntunneling effect. The transmission coefficient for the stock price tunneling\neffect it is also deduced. Although, it may not have important impact in\nquantum physics, the philosophical aspects residing in the use of quantum\nmechanics for stock market specific are very important.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.1559v1"
    },
    {
        "title": "Mathematical Analysis of Money in the Scope of Austerity",
        "authors": [
            "Peter Stallinga"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This summarizes the study of the financial and economic crisis in Europe. The\nstarting questions were: 1) Why do we have a crisis? Unde venis? 2) What will\nbe the outcome? Quo vadis? Here is the reasoning which touches many areas,\nranging from financial to politics and from psychology and economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.5373v3"
    },
    {
        "title": "To the problem of turbulence in quantitative easing transmission\n  channels and transactions network channels at quantitative easing policy\n  implementation by central banks",
        "authors": [
            "Dimitri O. Ledenyov",
            "Viktor O. Ledenyov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In agreement with the recent research findings in the econophysics, we\npropose that the nonlinear dynamic chaos can be generated by the turbulent\ncapital flows in both the quantitative easing transmission channels and the\ntransaction networks channels, when there are the laminar turbulent capital\nflows transitions in the financial system. We demonstrate that the capital\nflows in both the quantitative easing transmission channels and the transaction\nnetworks channels in the financial system can be accurately characterized by\nthe Reynolds numbers. We explain that the transition to the nonlinear dynamic\nchaos regime can be realized through the cascade of the Landau, Hopf\nbifurcations in the turbulent capital flows in both the quantitative easing\ntransmission channels and the transaction networks channels in the financial\nsystem. We completed the computer modeling, using both the Nonlinear Dynamic\nStochastic General Equilibrium Theory (NDSGET) and the Hydrodynamics Theory\n(HT), to accurately characterize the US economy in the conditions of the QE\npolicy implementation by the US Federal Reserve. We found that the ability of\nthe US financial system to adjust to the different levels of liquidity depends\non the nonlinearities appearance in the QE transmission channels, and is\nlimited by the laminar turbulent capital flows transitions in the QE\ntransmission channels and the transaction networks channels in the US financial\nsystem. The proposed computer model allows us to make the accurate forecasts of\nthe US economy performance in the cases, when there are the different levels of\nliquidity in the US financial system.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.5656v2"
    },
    {
        "title": "Goodhart, Charles A.E. and Tsomocos, Dimitros P.: The challenge of\n  financial stability: a new model and its applications",
        "authors": [
            "Jean-Bernard Chatelain"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This review of the book \"The Challenge of Financial Stability: A New Model\nand its Applications\" by Goodhart C.A.E. and Tsomocos D.P. highlights the\npotential of the framework of strategic partial default of banks with credit\nchain on the interbank market for further theoretical and applied research on\nfinancial stability.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.6148v1"
    },
    {
        "title": "Working Paper on Organizational Dynamics within Corporate Venture\n  Capital Firms",
        "authors": [
            "Michael Rolfes",
            "Alex \"Sandy\" Pentland"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Corporate venture capital is in the midst of a renaissance. The end of 2015\nmarked all-time highs both in the number of corporate firms participating in VC\ndeals and in the amount of capital being deployed by corporate VCs. This paper\nexplores, rather than defines, how these firms find success in the wake of this\nsudden influx of corporate investors. A series of interviews was conducted in\norder to capture the direct and indirect objectives, philosophies, and modes of\noperation within some of these corporate VC organizations. During the course of\nthis exploration, numerous operational coherency issues were discovered. Many\nfirms were implicitly incentivizing conflicting and inconsistent behavior among\ntheir investment team. Perhaps most surprising, the worst offenders were the\nmore mature corporate VCs who have been in the game for some time. As will be\ndiscussed, fundamental evidence suggests that this misalignment is due to lack\nof attention and commitment at the executive level as corporate strategy\nevolves.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.00970v1"
    },
    {
        "title": "Properties of the financial break-even point in a simple investment\n  project as a function of the discount rate",
        "authors": [
            "Domingo A. Tarzia"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We consider a simple investment project with the following parameters: I>0:\nInitial investment which is amortizable in n years; n: Number of years the\ninvestment allows production with constant output per year; A>0: Annual\namortization (A=I/n); Q>0: Quantity of products sold per year; Cv>0: Variable\ncost per unit; p>0: Price of the product with p>Cv; Cf>0: Annual fixed costs;\nte: Tax of earnings; r: Annual discount rate. We also assume inflation is\nnegligible. We derive a closed expression of the financial break-even point Qf\n(i.e. the value of Q for which the net present value (NPV) is zero) as a\nfunction of the parameters I, n, Cv, Cf, te, r, p. We study the behavior of Qf\nas a function of the discount rate r and we prove that: (i) For r negligible Qf\nequals the accounting break-even point Qc (i.e. the earnings before taxes (EBT)\nis null) ; (ii) When r is large the graph of the function Qf=Qf(r) has an\nasymptotic straight line with positive slope. Moreover, Qf(r) is an strictly\nincreasing and convex function of the variable r; (iii) From a sensitivity\nanalysis we conclude that, while the influence of p and Cv on Qf is strong, the\ninfluence of Cf on Qf is weak.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03740v1"
    },
    {
        "title": "The Tragedy of Your Upstairs Neighbors: Is the Airbnb Negative\n  Externality Internalized?",
        "authors": [
            "John J. Horton"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  A commonly expressed concern about the rise of the peer-to-peer rental market\nAirbnb is that hosts---those renting out their properties---impose costs on\ntheir unwitting neighbors. I consider the question of whether apartment\nbuilding owners will, in a competitive rental market, set a building-specific\nAirbnb hosting policy that is socially efficient. I find that if tenants can\nsort across apartments based on the owners policy then the equilibrium fraction\nof buildings allowing Airbnb listing would be socially efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.05688v1"
    },
    {
        "title": "A decomposition algorithm for computing income taxes with pass-through\n  entities and its application to the Chilean case",
        "authors": [
            "Javiera Barrera",
            "Eduardo Moreno",
            "Sebastian Varas"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Income tax systems with pass-through entities transfer a firm's incomes to\nthe shareholders, which are taxed individually. In 2014, a Chilean tax reform\nintroduced this type of entity and changed to an accrual basis that distributes\nincomes (but not losses) to shareholders. A crucial step for the Chilean\ntaxation authority is to compute the final income of each individual, given the\ncomplex network of corporations and companies, usually including cycles between\nthem. In this paper, we show the mathematical conceptualization and the\nsolution to the problem, proving that there is only one way to distribute\nincomes to taxpayers. Using the theory of absorbing Markov chains, we define a\nmathematical model for computing the taxable incomes of each taxpayer, and we\npropose a decomposition algorithm for this problem. This allows us to compute\nthe solution accurately and with the efficient use of computational resources.\nFinally, we present some characteristics of the Chilean taxpayers' network and\ncomputational results of the algorithm using this network.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.05690v1"
    },
    {
        "title": "A Multifaceted Panel Data Gravity Model Analysis of Peru's Foreign Trade",
        "authors": [
            "Xu Wang",
            "Ryan P. Badman"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Peru's abundant natural resources and friendly trade policies has made the\ncountry a major economic player in both South America and the global community.\nConsequently, exports are playing an increasingly important role in Peru's\nnational economy. Indeed, growing from 13.1% as of 1994, exports now contribute\napproximately 21% of the GDP of Peru as of 2015. Given Peru's growing global\ninfluence, the time is ripe for a thorough analysis of the most important\nfactors governing its export performance. Thus, within the framework of the\naugmented gravity model of trade, this paper examines Peru's export performance\nand attempts to identify the dominant economic factors that should be further\ndeveloped to increase the value of exports. The analysis was conducted from\nthree different aspects: (1) general economic parameters' effect on Peru's\nexport value, (2) more specific analysis into a major specific trade good,\ncopper, and (3) the impact that regional trade agreements have had on Peru's\nexport performance. Our panel data analysis results for each dataset revealed\ninteresting economic trends and were consistent with the theoretical\nexpectations of the gravity model: namely positive coefficients for economic\nsize and negative coefficients for distance. This report's results can be a\nreference for the proper direction of Peruvian economic policy so as to enhance\neconomic growth in a sustainable direction.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.01155v1"
    },
    {
        "title": "Co-movements in financial fluctuations are anchored to economic\n  fundamentals: A mesoscopic mapping",
        "authors": [
            "Kiran Sharma",
            "Balagopal Gopalakrishnan",
            "Anindya S. Chakrabarti",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We demonstrate the existence of an empirical linkage between the nominal\nfinancial networks and the underlying economic fundamentals across countries.\nWe construct the nominal return correlation networks from daily data to\nencapsulate sector-level dynamics and figure the relative importance of the\nsectors in the nominal network through a measure of centrality and clustering\nalgorithms. The eigenvector centrality robustly identifies the backbone of the\nminimum spanning tree defined on the return networks as well as the primary\ncluster in the multidimensional scaling map. We show that the sectors that are\nrelatively large in size, defined with the metrics market capitalization,\nrevenue and number of employees, constitute the core of the return networks,\nwhereas the periphery is mostly populated by relatively smaller sectors.\nTherefore, sector-level nominal return dynamics is anchored to the real size\neffect, which ultimately shapes the optimal portfolios for risk management. Our\nresults are reasonably robust across 27 countries of varying degrees of\nprosperity and across periods of market turbulence (2008-09) as well as\nrelative calmness (2015-16).\n",
        "pdf_link": "http://arxiv.org/pdf/1612.05952v2"
    },
    {
        "title": "The Price of Political Uncertainty: Evidence from the 2016 U.S.\n  Presidential Election and the U.S. Stock Markets",
        "authors": [
            "Jamal Bouoiyour",
            "Refk Selmi"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  There is bountiful evidence that political uncertainty stemming from\npresidential elections or doubt about the direction of future policy make\nfinancial markets significantly volatile, especially in proximity to close\nelections or elections that may prompt radical policy changes. Although several\nstudies have examined the association between presidential elections and stock\nreturns, very little attention has been given to the impacts of elections and\nelection induced uncertainty on stock markets. This paper explores, at sectoral\nlevel, the uncertain information hypothesis (UIH) as a means of explaining the\nreaction of markets to the arrival of unanticipated information. This\nhypothesis postulates that political uncertainty is greater prior to the\nelections (relative to pre-election period) but is resolved once the outcome of\nthe elections is determined (relative to post-election period). To this end, we\nadopt an event-study methodology that examines abnormal return behavior around\nthe election date. We show that collapsing stock returns around the election\nresult is reversed by positive abnormal return on the next day, except some\ncases where we note negative responses following the vote count. Although\nTrump's win plunges US into uncertain future, positive reactions of abnormal\nreturn are found. Therefore, our results do not support the UIH hypothesis.\nBesides, the effect of political uncertainty is sector-specific. While some\nsectors emerged winners (healthcare, oil and gas, real estate, defense,\nfinancials and consumer goods and services), others took the opposite route\n(technology and utilities). The winning industries are generally those that\nwill benefit from the new administration's focus on rebuilding infrastructure,\nrenegotiating trade agreements, reforming tax policy and labour laws,\nincreasing defense funding, easing restrictions on energy production, and\nrolling back Obamacare.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.06200v2"
    },
    {
        "title": "Crisis' Heritage Management - New Business Opportunities Out of the\n  Financial Collapse",
        "authors": [
            "Hristian Daskalov"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  This paper intends to present the opportunities emerging for the national\neconomy, out of the financial crisis. In particular the management of those,\nwhich arise from the commercial real estate owned property sector, defined by\nthe author as crisis heritage management. On one hand, as real estate property\nprices are subject of wide fluctuations, the longer possession of such assets\ncan seriously impact the financial condition of the already shattered financial\ninstitutions, but on the on other - with the help of professional and proactive\nmanagement, and the right kind of attitude by all the stakeholders, the\nheritage left out of the financial collapse, can not only help stabilize the\nsystem - bringing liquidity into it, but can also support its healthy corporate\ngovernance in the long-term. The properties themselves (business buildings,\nwarehouses, retail-and-office spaces), being an object of optimization of\nmaintenance costs, re-engineering, intensive marketing, as a result of the\ncrisis, can serve as a solid base for number of new and profitable business and\ninvestment opportunities, described in the article, as a proof of the healing\neffect of the financial crisis and the second chance it gives.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08689v1"
    },
    {
        "title": "State and Network Structures of Stock Markets around the Global\n  Financial Crisis",
        "authors": [
            "Jae Woo Lee",
            "Ashadun Nobi"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We consider the effects of the 2008 global financial crisis on the global\nstock market before, during, and after the crisis. We generate complex networks\nfrom a cross-correlation matrix such as the threshold network (TN) and the\nminimal spanning tree (MST). In the threshold network, we assign a threshold\nvalue by using the mean and standard deviation of cross-correlation\ncoefficients. When the threshold is equal to the mean of these coefficients, we\nobserve a giant cluster composed of three economic zones in all three periods.\nWe find that during the crisis, the countries in the Asian zone were weakly\nconnected and those in the American zone were tightly linked to the countries\nin the European zone. At a large threshold, the three economic zones were\nfragmented. The European countries connected tightly, but the Asian countries\nbound weakly. The MST constructed from the distance matrix. In the MST, France\nremained a hub node in all three periods. The size of the MST shrank slightly\nduring the crisis. We observe a scaling relation between the network distance\nof nodes from the central hub (France) and the geometrical distance. We observe\nthe topological change of the financial network structure during the global\nfinancial crisis. The TN and MST are complementary roles to understand the\nconnecting structure of financial complex networks. The TN reveals to observe\nthe clustering effects and robustness of the cluster during the financial\ncrisis. The MST shows the central hub and connecting node among the economic\nzones.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04363v1"
    },
    {
        "title": "What is Wrong with Net Promoter Score",
        "authors": [
            "Nicholas I Fisher",
            "Raymond E Kordupleski"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Net-Promoter Score (NPS) is now ubiquitous as an easily-collected market\nresearch metric, having displaced many serious market research processes.\nUnfortunately, this has been its sole success. It possesses few, if any, of the\ncharacteristics that might be regarded as highly desirable in a high-level\nmarket research metric; on the contrary, it has done considerable damage both\nto companies and to their customers.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.10452v1"
    },
    {
        "title": "A Policy Compass for Ecological Economics",
        "authors": [
            "Michèle Friend"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  A policy compass indicates the direction in which an institution is going in\nterms of three general qualities. The three qualities are: suppression, harmony\nand passion. Any formal institution can develop a policy compass to examine the\ndiscrepancy between what the institution would like to do (suggested in its\nmandate) and the actual performance and situation it finds itself in. The\nlatter is determined through an aggregation of statistical data and facts.\nThese are made robust and stable using meta-requirements of convergence. Here,\nI present a version of the compass adapted to embed the central ideas of\necological economics: that society is dependent on the environment, and that\neconomic activity is dependent on society; that we live in a world subject to\nat least the first two laws of thermodynamics; that the planet we live on is\nlimited in space and resources; that some of our practices have harmful and\nirreversible consequences on the natural environment; that there are values\nother than value in exchange, such as intrinsic value and use value. In this\npaper, I explain how to construct a policy compass in general. This is followed\nby the adaptation for ecological economics. The policy compass is original, and\nso is the adaptation. The compass is inspired by the work of Anthony Friend,\nRob Hoffman, Satish Kumar, Georgescu-Roegen, Stanislav Schmelev, Peter\nS\\\"oderbaum and Arild Vatn. In the conclusion, I discuss the accompanying\nconception of sustainability.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03338v1"
    },
    {
        "title": "Privatizaciones, fusiones y adquisiciones: las grandes empresas en\n  México",
        "authors": [
            "Rigoberto Pérez Ramírez"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The present work has as principal objective analyze the evolution of the\nprocess of privatization, mergers and acquisitions of the big companies in the\ncountry in the last decades, to understand the conductive threads that formed\nthe structural changes of the economy, in order world oligop\\'olicas to insert\nit to the global market characterized by formations of strategic alliances,\nacross the mergers and acquisitions that they favour to the transnational\ncompanies.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03339v1"
    },
    {
        "title": "Initial Crypto-asset Offerings (ICOs), tokenization and corporate\n  governance",
        "authors": [
            "Stéphane Blémus",
            "Dominique Guegan"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This paper discusses the potential impacts of the so-called `initial coin\nofferings', and of several developments based on distributed ledger technology\n(`DLT'), on corporate governance. While many academic papers focus mainly on\nthe legal qualification of DLT and crypto-assets, and most notably in relation\nto the potential definition of the latter as securities/financial instruments,\nthe authors analyze some of the use cases based on DLT technology and their\npotential for significant changes of the corporate governance analyses. This\narticle studies the consequences due to the emergence of new kinds of firm\nstakeholders, i.e. the crypto-assets holders, on the governance of small and\nmedium-sized enterprises (`SMEs') as well as of publicly traded companies.\nSince early 2016, a new way of raising funds has rapidly emerged as a major\nissue for FinTech founders and financial regulators. Frequently referred to as\ninitial coin offerings, Initial Token Offerings (`ITO'), Token Generation\nEvents (`TGE') or simply `token sales', we use in our paper the terminology\nInitial Crypto-asset Offerings (`ICO'), as it describes more effectively than\n`initial coin offerings' the vast diversity of assets that could be created and\nwhich goes far beyond the payment instrument issue.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03340v1"
    },
    {
        "title": "Unconventional Exchange: Methods for Statistical Analysis of Virtual\n  Goods",
        "authors": [
            "Oliver James Scholten",
            "Peter Cowling",
            "Kenneth A. Hawick",
            "James Alfred Walker"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Hyperinflation and price volatility in virtual economies has the potential to\nreduce player satisfaction and decrease developer revenue. This paper describes\nintuitive analytical methods for monitoring volatility and inflation in virtual\neconomies, with worked examples on the increasingly popular multiplayer game\nOld School Runescape. Analytical methods drawn from mainstream financial\nliterature are outlined and applied in order to present a high level overview\nof virtual economic activity of 3467 price series over 180 trading days.\nSix-monthly volume data for the top 100 most traded items is also used both for\nmonitoring and value estimation, giving a conservative estimate of exchange\ntrading volume of over {\\pounds}60m in real value. Our worked examples show\nresults from a well functioning virtual economy to act as a benchmark for\nfuture work. This work contributes to the growing field of virtual economics\nand game development, describing how data transformations and statistical tests\ncan be used to improve virtual economic design and analysis, with applications\nin real-time monitoring systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06721v2"
    },
    {
        "title": "The professional trader's paradox",
        "authors": [
            "Andrea Berdondini"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In this article, I will present a paradox whose purpose is to draw your\nattention to an important topic in finance, concerning the non-independence of\nthe financial returns (non-ergodic hypothesis). In this paradox, we have two\npeople sitting at a table separated by a black sheet so that they cannot see\neach other and are playing the following game: the person we call A flip a coin\nand the person we'll call B tries to guess the outcome of the coin flip. At the\nend of the game, both people are asked to estimate the compound probability of\nthe result obtained. The two people give two different answers, one estimates\nthe events as independent and the other one considers the events as dependent\ntherefore they calculate the conditional probability differently. This paradox\nshow how the erroneous estimation of conditional probability implies a strong\ndistortion of the forecasting skill that can lead us to bear excessive risks.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06722v1"
    },
    {
        "title": "Options to Receive Retirement Gratuity",
        "authors": [
            "Reason Machete"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Retirement gratuity is the money companies typically pay their employees at\nthe end of their contracts or at the time of leaving the company. It is a\ndefined benefit plan and is often given as an alternative to a pension plan. In\nBotswana, there is now a new pattern whereby companies give their employees the\noption to receive their gratuity at various stages before the end of their\ncontracts. In particular, some companies give their employees an option to\nreceive their gratuity on a monthly basis rather than having them wait for a\nyear or more. Many employees find this option attractive, but is it\neconomically sound? This paper sheds light on this question by quantifying the\neconomic benefits of the tax relief provided by government relative to\ninvesting the monthly-received funds in a risk-free savings account or helping\nrepay a loan. The principles and methods used herein can be adapted and applied\nto different taxation systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06733v2"
    },
    {
        "title": "Robust measurement of innovation performances in Europe with a hierarchy\n  of interacting composite indicators",
        "authors": [
            "Salvatore Corrente",
            "Ana Garcia-Bernabeu",
            "Salvatore Greco",
            "Teemu Makkonen"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  For long time the measurement of innovation has been in the forefront of\npolicy makers' and researchers' agenda worldwide. Therefore, there is an\nongoing debate about which indicators should be used to measure innovation.\nRecent approaches have favoured the use of composite innovation indicators.\nHowever, there is no consensus about the appropriate methodology to aggregate\nthe varying dimensions of innovation into a single summary indicator. One of\nthe best known examples of composite innovation indicators is the European\nInnovation Scoreboard (EIS). It is a relevant tool for benchmarking innovation\nin Europe. Still, the EIS lacks a proper scheme for weighting the included\nindicators according to their relative importance. In this context, we propose\nan appraisal methodology permitting to take into consideration the interaction\nof criteria and robustness concerns related to the elicitation of the weights\nassigned to the elementary indicators. With this aim, we apply the\nhierarchical-SMAA-Choquet integral approach. This integrated multicriteria\ndecision making (MCDM) method helps the users to rank and benchmark countries'\ninnovation performance taking into account the importance and interaction of\ncriteria assigned by themselves, rather than equal weights or weights\nexogenously fixed by external experts.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.12705v1"
    },
    {
        "title": "DAY TRADE: across the statistics | DAY TRADE: do outro lado das\n  estatisticas",
        "authors": [
            "Roberto Ernani Porcher Junior"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This paper questions some current ideas about the practice of specific\ncapital market operations - the so-called day trading operations. The text\nadvanced from theoretical propositions to a detailed analysis of the study\nentitled \"Is it possible to live by day-trading?\" (CHAGUE and GIOVANNETTI,\n2019), to which it offers a counterpoint. This investigation reveal the\nexistence of important elements that are not yet properly weighed in the\ntreatment of the current theme, leading to loss of dimensions that are - or\nshould be - inseparable from this type of research. The conclusion reached is\nthat the existing scientific evidence does not show that the adoption of the\nday trade as an occupation is economically unsustainable, nor does it prove the\nimpossibility of evolution of the day traders' operational performance over\ntime.\n  -----\n  O presente trabalho coloca em questionamento algumas ideias atualmente\ndifundidas sobre a pratica de operacoes especificas do mercado de capitais - as\nchamadas operacoes day trade. Partindo de proposicoes teoricas, o texto avanca\na uma analise detalhada do estudo intitulado \"E possivel viver de day-trading?\"\n(CHAGUE e GIOVANNETTI, 2019), ao qual oferece contraponto. Essa investigacao\nrevela a existencia de importantes elementos que ainda nao estao devidamente\nsopesados no tratamento do tema em voga, acarretando perda de dimensoes que sao\n- ou deveriam ser - indissociaveis desse tipo de pesquisa. A conclusao\nalcancada e de que as evidencias cientificas ate entao existentes nao\ndemonstram que a adocao do day trade como profissao seja insustentavel\neconomicamente, nem comprovam a impossibilidade de evolucao da performance\noperacional dos day traders ao longo do tempo.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04274v1"
    },
    {
        "title": "Assessment of Financial Potential as a Determinant of Enterprise\n  Development",
        "authors": [
            "Dmytro Zherlitsyn",
            "Stanislav Levytskyi",
            "Denys Mykhailyk",
            "Victoriia Ogloblina"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Financial potential is an important part of enterprise activities. The\ntechnique of the enterprise's financial potential assessment is offered in the\npaper. It is presented by particular stages, where each stage is related to a\ncertain task. The characteristics of the company's financial potential, based\non the analysis of the related literature, are determined. The implementation\nof each task is carried out. Thus, the study proposes a mechanism for managing\nthe financial potential of enterprises, which allows to emphasize the elements\nthat can be useful for economic development. It is based on the general\nstrategic principles of the enterprise management. The study results can be\nused to assess enterprise purposes and develop the formation goals of its\nfinancial potential. It can also help to forecast and separate main directions\nof accumulation, formation, and distribution of financial resources. It should\nbe noted, that analysis and control over the financial potential formation\nstrategy, as well as the use of analysis results for specifying the strategic\ndirections of the enterprise development, are of high importance. Therefore,\nthe management of the financial potential is a system of rational management of\nbusiness financing, which includes the formation of financial relations,\nemerging as a result of finance resources flow.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.05635v1"
    },
    {
        "title": "Argentum: a collaborative saving and investment platform for unstable\n  countries",
        "authors": [
            "Leonardo Belen",
            "Alejandro Baranek",
            "Xavier Gonzalez"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  A crypto coin designed to provide a stabilization instrument backed up by\nminded like financial investments instruments to maintain the purchase value of\nsavings across time, in order to construct new tools for unstable economies.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09569v1"
    },
    {
        "title": "A Stochastic Investment Model for Actuarial Use in South Africa",
        "authors": [
            "Şule Şahin",
            "Shaun Levitan"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In this paper, we propose a stochastic investment model for actuarial use in\nSouth Africa by modelling price inflation rates, share dividends, long term and\nshort-term interest rates for the period 1960-2018 and inflation-linked bonds\nfor the period 2000-2018. Possible bi-directional relations between the\neconomic series have been considered, the parameters and their confidence\nintervals have been estimated recursively to examine their stability and the\nmodel validation has been tested. The model is designed to provide long-term\nforecasts that should find application in long-term modelling for institutions\nsuch as pension funds and life insurance companies in South Africa\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12113v2"
    },
    {
        "title": "A review of the Dividend Discount Model: from deterministic to\n  stochastic models",
        "authors": [
            "Guglielmo D'Amico",
            "Riccardo De Blasis"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This chapter presents a review of the dividend discount models starting from\nthe basic models (Williams 1938, Gordon and Shapiro 1956) to more recent and\ncomplex models (Ghezzi and Piccardi 2003, Barbu et al. 2017, D'Amico and De\nBlasis 2018) with a focus on the modelling of the dividend process rather than\nthe discounting factor, that is assumed constant in most of the models. The\nChapter starts with an introduction of the basic valuation model with some\ngeneral aspects to consider when performing the computation. Then, Section 1.3\npresents the Gordon growth model (Gordon 1962) with some of its extensions\n(Malkiel 1963, Fuller and Hsia 1984, Molodovsky et al. 1965, Brooks and Helms\n1990, Barsky and De Long 1993), and reports some empirical evidence. Extended\nreviews of the Gordon stock valuation model and its extensions can be found in\nKamstra (2003) and Damodaran (2012). In Section 1.4, the focus is directed to\nmore recent advancements which make us of the Markov chain to model the\ndividend process (Hurley and Johnson 1994, Yao 1997, Hurley and Johnson 1998,\nGhezzi and Piccardi 2003, Barbu et al. 2017, D'Amico and De Blasis 2018). The\nadvantage of these models is the possibility to obtain a different valuation\nthat depends on the state of the dividend series, allowing the model to be\ncloser to reality. In addition, these models permit to obtain a measure of the\nrisk of the single stock or a portfolio of stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.00465v1"
    },
    {
        "title": "On the probability flow in the Stock market I: The Black-Scholes case",
        "authors": [
            "Ivan Arraut",
            "Alan Au",
            "Alan Ching-biu Tse",
            "Joao Alexandre Lobo Marques"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  It is known that the probability is not a conserved quantity in the stock\nmarket, given the fact that it corresponds to an open system. In this paper we\nanalyze the flow of probability in this system by expressing the ideal\nBlack-Scholes equation in the Hamiltonian form. We then analyze how the\nnon-conservation of probability affects the stability of the prices of the\nStocks. Finally, we find the conditions under which the probability might be\nconserved in the market, challenging in this way the non-Hermitian nature of\nthe Black-Scholes Hamiltonian.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.00516v1"
    },
    {
        "title": "Ownership Structure Variation and Firm Efficiency",
        "authors": [
            "Sallahuddin Hassan",
            "Zalila Othman",
            "Mukaramah Harun"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Firms with different ownership structures could be argued to have different\nlevels of efficiency.Highly concentrated firms are expected to be more\nefficient as this type of ownership structure may alleviate the conflict of\ninterest between managers and shareholders.In Malaysia, public-listed firms\nhave been found to have highly concentrated ownership structure.However,\nwhether this evidence holds for every industry has not been established.Hence,\nthe objective of this paper is to investigate whether there are variations in\nownership structure and firm's efficiency across sectors.To achieve this\nobjective, the frequency distributions of ownership structure were calculated\nand firms efficiency scores for consumer products, industrial products,\nconstruction and trading/services sectors were measured.Data Envelopment\nAnalysis(DEA) under the assumptions of constant returns to scale(CRS) and\nvariable returns to scale(VRS) was employed to estimate firms efficiency\nscores.A sample of 156 firms listed on the Kuala Lumpur Stock Exchange(KLSE)\nwas selected using the stratified random sampling method. The findings have\nshown that there are variations in firm ownership structure and efficiency\nacross sectors.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.05575v1"
    },
    {
        "title": "Choosing the Right Return Distribution and the Excess Volatility Puzzle",
        "authors": [
            "Abootaleb Shirvani",
            "Frank J. Fabozzi"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Proponents of behavioral finance have identified several \"puzzles\" in the\nmarket that are inconsistent with rational finance theory. One such puzzle is\nthe \"excess volatility puzzle\". Changes in equity prices are too large given\nchanges in the fundamentals that are expected to change equity prices. In this\npaper, we offer a resolution to the excess volatility puzzle within the context\nof rational finance. We empirically show that market inefficiency attributable\nto the volatility of excess return across time is caused by fitting an improper\ndistribution to the historical returns. Our results indicate that the variation\nof gross excess returns is attributable to poorly fitting the tail of the\nreturn distribution and that the puzzle disappears by employing a more\nappropriate distribution for the return data. The new distribution that we\nintroduce in this paper that better fits the historical return distribution of\nstocks explains the excess volatility in the market and thereby explains the\nvolatility puzzle. Failing to estimate the historical returns using the proper\ndistribution is only one possible explanation for the existence of the\nvolatility puzzle. However, it offers statistical models within the rational\nfinance framework which can be used without relying on behavioral finance\nassumptions when searching for an explanation for the volatility puzzle.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.08865v1"
    },
    {
        "title": "Investment behavior and firms' financial performance: A comparative\n  analysis using firm-level data from the wine industry",
        "authors": [
            "Claudiu Albulescu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This paper assesses the role of financial performance in explaining firms'\ninvestment dynamics in the wine industry from the three European Union (EU)\nlargest producers. The wine sector deserves special attention to investigate\nfirms' investment behavior given the high competition imposed by the\nlatecomers. More precisely, we investigate how the capitalization, liquidity\nand profitability influence the investment dynamics using firm-level data from\nthe wine industry from France (331 firms), Italy (335) firms and Spain (442)\nfirms. We use data from 2007 to 2014, drawing a comparison between these\ncountries, and relying on difference-and system-GMM estimators. Specifically,\nthe impact of profitability is positive and significant, while the\ncapitalization has a significant and negative impact on the investment dynamics\nonly in France and Spain. The influence of the liquidity ratio is negative and\nsignificant only in the case of Spain. Therefore, we notice different\ninvestment strategies for wine companies located in the largest producer\ncountries. It appears that these findings are in general robust to different\nspecifications of liquidity and profitability ratios, and to the different\nestimators we use.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10432v1"
    },
    {
        "title": "Some Issues In Securitization And Disintermediation",
        "authors": [
            "Michael C. Nwogugu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Securitization has become prevalent in many countries, and has substantial\nimpact on government monetary policy and fiscal policy which have not yet been\nadequately analyzed in the existing literature. This article develops optimal\nconditions for efficient securitization, identifies constraints on\nsecuritization, and analyzes the interactions of capital-reserve requirements\nand securitization. This article introduces new decision models and theories of\nasset-securitization.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01706v1"
    },
    {
        "title": "On The Choice Between A Sale-Leaseback And Debt",
        "authors": [
            "Michael C. Nwogugu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This article introduces decision models for commercial real estate leasing.\nThe concepts and models developed in the article can also be applied to\nequipment leasing and other types of leasing.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01707v1"
    },
    {
        "title": "Decision-Making, Sub-Additive Recursive \"Matching\" Noise And Biases In\n  Risk-Weighted Stock/Bond Index Calculation Methods In Incomplete Markets With\n  Partially Observable Multi-Attribute Preferences",
        "authors": [
            "Michael C. Nwogugu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  While Indices, Index tracking funds and ETFs have grown in popularity during\nthen last ten years, there are many structural problems inherent in Index\ncalculation methodologies and the legal/economic structure of ETFs. These\nproblems raise actionable issues of Suitability and fraud under US securities\nlaws, because most Indices and ETFs are misleading, have substantial tracking\nerrors and dont reflect what they are supposed to track. This article\ncontributes to the existing literature by: a) introducing and characterizing\nthe errors and Biases inherent in risk-adjusted index weighting methods and the\nassociated adverse effects; b) showing how these biases/effects inherent in\nIndex calculation methods reduce social welfare, and can form the basis for\nharmful arbitrage activities.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01708v1"
    },
    {
        "title": "Regret Theory And Asset Pricing Anomalies In Incomplete Markets With\n  Dynamic Un-Aggregated Preferences",
        "authors": [
            "Michael Nwogugu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Although the CML (Capital Market Line), the Intertemporal-CAPM, the CAPM/SML\n(Security Market Line) and the Intertemporal Arbitrage Pricing Theory (IAPT)\nare widely used in portfolio management, valuation and capital markets\nfinancing; these theories are inaccurate and can adversely affect risk\nmanagement and portfolio management processes. This article introduces several\nempirically testable financial theories that provide insights, and can be\ncalibrated to real data and used to solve problems, and contributes to the\nliterature by: i) explaining the conditions under which ICAPM/CAPM, IAPT and\nCML may be accurate, and why such conditions are not feasible; and explaining\nwhy the existence of incomplete markets and dynamic un-aggregated markets\nrender CML, IAPT and ICAPM inaccurate; ii) explaining why the\nConsumption-Savings-InvestmentProduction framework is insufficient for asset\npricing and analysis of changes in risk and asset values; and introducing a\nunified approach to asset pricing that simultaneously considers six factors,\nand the conditions under which this approach will work; iii) explaining why\nleisure, taxes and housing are equally as important as consumption and\ninvestment in asset pricing; iv) introducing the Marginal Rate of Intertemporal\nJoint Substitution (MRIJS) among Consumption, Taxes, Investment, Leisure,\nIntangibles and Housing - this model incorporates Regret Theory and captures\nfeatures of reality that dont fit well into standard asset pricing models, and\nthis framework can support specific or very general finance theories and or\nvery complicated models; v) showing why the Elasticity of Intertemporal\nSubstitution (EIS) is inaccurate and is insufficient for asset pricing and\nanalysis of investor preferences.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01709v1"
    },
    {
        "title": "Issues In Disintermediation In The Real Estate Brokerage Sector",
        "authors": [
            "Michael C. Nwogugu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This article introduces new models of disintermediation of the real estate\nbroker by the buyer or the seller. The decision to retain a real estate broker\nis critical in the property purchase/sale process. The existing literature does\nnot contain analysis of: 1) information asymmetry, 2) the conditions under\nwhich it will be optimal to disintermediate the broker, 3) social capital and\nreputation, 4) the impact of different types of real estate brokerage\ncontracts. The article shows that dis-intermediation of the real estate broker\nby the seller or buyer may be optimal in certain conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01710v1"
    },
    {
        "title": "Can Volatility Solve the Naive Portfolio Puzzle?",
        "authors": [
            "Michael Curran",
            "Patrick O'Sullivan",
            "Ryan Zalla"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We investigate whether sophisticated volatility estimation improves the\nout-of-sample performance of mean-variance portfolio strategies relative to the\nnaive 1/N strategy. The portfolio strategies rely solely upon second moments.\nUsing a diverse group of econometric and portfolio models across multiple\ndatasets, most models achieve higher Sharpe ratios and lower portfolio\nvolatility that are statistically and economically significant relative to the\nnaive rule, even after controlling for turnover costs. Our results suggest\nbenefits to employing more sophisticated econometric models than the sample\ncovariance matrix, and that mean-variance strategies often outperform the naive\nportfolio across multiple datasets and assessment criteria.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03204v4"
    },
    {
        "title": "India Growth Forecast for 2020-21",
        "authors": [
            "Amarendra Das",
            "Subhankar Mishra"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  COVID-19 has put a severe dent on the global economy and Indian Economy.\nInternational Monetary Fund has projected 1.9 percent for India. However, we\nbelieve that due to extended lockdown, the output in the first quarter is\nalmost wiped out. The situation may improve in the second quarter onwards.\nNevertheless, due to demand and supply constraints, input constraints and\ndisruption in the supply chain, except agriculture, no other sector would be\nable to achieve full capacity of production in 2020-21. The signals from power\nconsumption, GST collection, contraction in the core sectors hint towards a\nslump in the total output production in 2020-21. We derive the quarterly GVA\nfor 2020-21 by using certain assumptions on the capacity utilisation in\ndifferent sectors and using the quarterly data of 2019-20. We provide quarterly\nestimates of Gross Value Addition for 2020-21 under two scenarios. We have also\nestimated the fourth quarter output for 2019-20 under certain assumptions. We\nestimate\n",
        "pdf_link": "http://arxiv.org/pdf/2005.06461v1"
    },
    {
        "title": "Parameters of Profitability: Evidence From Conventional and Islamic\n  Banks of Bangladesh",
        "authors": [
            "K. M. Golam Muhiuddin",
            "Nusrat Jahan"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This paper evaluates the commercial banks of Bangladesh in terms of\nprofitability dimension of performance and also examines the impact of selected\ndeterminants and banking system on this dimension of performance. Evaluation of\ntrend in profitability of listed commercial banks of Bangladesh reveals that,\non an average, profitability is exhibiting a decreasing trend over the selected\nperiod; however, the profitability performance of Islamic banks remained rather\nhigh compared to Conventional banks. Profitability measured by Return on Asset\nis found to be significantly affected by the bank-specific factors,\nindustry-specific factor and the banking system. However, macro-economic\nfactors evidently have no significant impact on profitability of commercial\nbanks of Bangladesh.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.07732v1"
    },
    {
        "title": "Evaluation of Accounting and Market Performance: A Study on Listed\n  Islamic Banks of Bangladesh",
        "authors": [
            "Nusrat Jahan",
            "M. Ayub Islam"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This study compared accounting performance of Islamic banks with their market\nperformance and also assessed the effect of firm-specific determinants and\ncross-sectional effect on accounting and market performance. This study\nselected all six listed Islamic banks of Chittagong Stock Exchange and the data\nwere collected for the period of 2009 to 2013. This study reported that Social\nIslamic Bank Limited exhibits superior accounting performance whereas Islami\nBank Bangladesh Limited holds better market performance. However, banks\nexhibiting superior accounting performance reported to have inferior market\nperformance. Further, random-effect model for ROA reports that there exist\nsignificant entity or crosssectional effect on ROA; and operational efficiency\nand bank size are significantly negatively associated with ROA. However,\nrandom-effect model for Tobins Q failed to ascertain entity or cross-sectional\neffect on Tobins Q and also reveals that firm-specific determinants have no\nsignificant impact on Tobins Q.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.08734v1"
    },
    {
        "title": "Determinants of Profitability of Banks: Evidence from Islamic Banks of\n  Bangladesh",
        "authors": [
            "Nusrat Jahan"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This empirical study is conducted on randomly selected six Islamic banks of\nBangladesh. This study utilizes widely used Measures of banks profitability\nwhich are Return on Asset (ROA), Return on Equity (ROE) and Return on Deposit\n(ROD) and these are also commonly suggested tools by Bangladesh Bank to\nevaluate banks performance. In addition, this study examined the relationship\nof ROA with Asset Utilization (AU), Operational Efficiency (OE)and ROD. The\nresult reveals that EXIM Bank Limited is performing very good in terms of all\nprofitability measures ROA, ROE and ROD even though average asset size of\nIslami Bank Bangladesh Limited is found to be largest among all six Islamic\nBanks. The result of regression found the explanatory variable ROD is\nsignificantly associated with ROA but failed to establish any significant\nassociation with operational efficiency and asset utilization.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.08759v1"
    },
    {
        "title": "An Empirical Investigation of Cash Conversion Cycle of Manufacturing\n  Firms and its Association with Firm Size and Profitability",
        "authors": [
            "Nusrat Jahan"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The purpose of this empirical study is to investigate Cash Conversion Cycle\nof thirty manufacturing firms listed in Dhaka Stock Exchanges under six\ndifferent categories, which are, Food and allied, Pharmaceuticals and chemical,\nCement, Textile, Engineering and Miscellaneous. This paper sets industry\naverage Cash Conversion Cycle for these six industries and examines the\nrelationship of Cash Conversion Cycle with firm size and profitability. This\nstudy did not find statistically significant differences among the Cash\nConversion Cycle of varying manufacturing industries. The result of this study\nindicates a statistically significant negative relationship between the Cash\nConversion Cycle and profitability, especially in terms of Return on Equity.\nThe result also shows that the Cash Conversion Cycle of manufacturing firm also\nhas significant negative relationship with firm size, when measured in terms of\nnet sales. The present study contributes to the literature on working capital\nmanagement written in the context of Bangladesh.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09482v1"
    },
    {
        "title": "An Investigation into the Equivalency of Three Performance Dimensions:\n  Evidence from Commercial Banks in Bangladesh",
        "authors": [
            "Nusrat Jahan"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This study evaluated the three dimensions of performance of commercial banks\nin Bangladesh by analyzing the trend of the Malmquist Productivity Index (MPI)\nof the Total Factor Productivity (TFP), Return on Asset (ROA) and Total Stock\nReturn (TSR) over the period 2011 to 2015. The study developed an empirical\nframework with the intention to examine the equivalency of three dimensions of\nperformance. Since, the measures of performance are different, they cannot be\ntested in their original form; hence, the growth rate of each category of\nperformance measures were estimated and tested to examine the comparability\namong them. Evaluation of profitability revealed a decreasing trend and\nevaluation of stock performance suggests that investors are incurring losses on\ntheir investment over the selected period. Evaluation of productivity indicates\nthat productivity regress was recorded initially but at the end of the studied\nperiod a modest productivity growth was recorded. Finally, this study was able\nto ascertain the anticipated equivalency of outcome of the three dimensions of\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09483v1"
    },
    {
        "title": "Coronavirus: Case for Digital Money?",
        "authors": [
            "Zura Kakushadze",
            "Jim Kyung-Soo Liew"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We discuss the pros of adopting government-issued digital currencies as well\nas a supranational digital iCurrency. One such pro is to get rid of paper money\n(and coinage), a ubiquitous medium for spreading germs, as highlighted by the\nrecent coronavirus outbreak. We set forth three policy recommendations for\nadapting mobile devices as new digital wallets, regulatory oversight of\nsovereign digital currencies and user data protection, and a supranational\ndigital iCurrency for facilitating international digital monetary linkages.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.10154v1"
    },
    {
        "title": "Applying the Nash Bargaining Solution for a Reasonable Royalty",
        "authors": [
            "David M. Kryskowski",
            "David Kryskowski"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  There has been limited success applying the Nash Bargaining Solution (NBS) in\nassigning intellectual property damages due to the difficulty of relating it to\nthe specific facts of the case. Because of this, parties are not taking\nadvantage of Georgia-Pacific factor fifteen. This paper intends to bring\nclarity to the NBS so it can be applied to the facts of a case. This paper\nnormalizes the NBS and provides a methodology for determining the bargaining\nweight in Nash's solution. Several examples demonstrate this normalized form,\nand a nomograph is added for computational ease.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.10158v2"
    },
    {
        "title": "Value relevance of the components of oil and gas reserve quantity change\n  disclosures of upstream oil and gas companies in the london stock exchange",
        "authors": [
            "Tega Anighoro"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The high level of risk and uncertainty in harnessing oil and gas reserves\nposes an accounting dilemma in the reporting of reserves quantity information;\ninformation which is critical and relied on by investors for decision making.\nDifferent studies have indicated that reserves disclosure information is\nfundamental to understanding the value of the firm. This study attempts to\ncontribute to the growing value relevance literature on reserves disclosures by\nexamining the value relevance of the components of oil and gas reserve quantity\nchange disclosures of upstream oil and gas companies in the London Stock\nExchange. Particularly, it investigates the relationship between average\nhistorical share returns and changes in reserves from explorations,\nacquisitions, production, revisions and sale. It also examines the value\nrelevance of the quality of these disclosures. Using archival data from LSE,\ndatabases and annual reports, and applying a multifactor framework, the\nempirical results suggested that changes in reserves as well as the components\nof these changes where associated with share returns though insignificantly due\nto the significant impact of oil price and longitudinal effect posed by\napplying the measurement approach with utilizes historical returns. However,\nthe quality of reserves disclosures has a positively significant relationship\nwith share returns. The volatility and decline in oil price is also reflected\nin both low average share returns at -0.4% and low average growth in reserves\nat 8.94% for the last 8 years in the sector.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.14659v1"
    },
    {
        "title": "Heterogeneity, correlations and financial contagion",
        "authors": [
            "Fabio Caccioli",
            "Thomas A. Catanach",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We consider a model of contagion in financial networks recently introduced in\nthe literature, and we characterize the effect of a few features empirically\nobserved in real networks on the stability of the system. Notably, we consider\nthe effect of heterogeneous degree distributions, heterogeneous balance sheet\nsize and degree correlations between banks. We study the probability of\ncontagion conditional on the failure of a random bank, the most connected bank\nand the biggest bank, and we consider the effect of targeted policies aimed at\nincreasing the capital requirements of a few banks with high connectivity or\nbig balance sheets. Networks with heterogeneous degree distributions are shown\nto be more resilient to contagion triggered by the failure of a random bank,\nbut more fragile with respect to contagion triggered by the failure of highly\nconnected nodes. A power law distribution of balance sheet size is shown to\ninduce an inefficient diversification that makes the system more prone to\ncontagion events. A targeted policy aimed at reinforcing the stability of the\nbiggest banks is shown to improve the stability of the system in the regime of\nhigh average degree. Finally, disassortative mixing, such as that observed in\nreal banking networks, is shown to enhance the stability of the system.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.1213v1"
    },
    {
        "title": "The efficient index hypothesis and its implications in the BSM model",
        "authors": [
            "Vladimir Vovk"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This note studies the behavior of an index I_t which is assumed to be a\ntradable security, to satisfy the BSM model dI_t/I_t = \\mu dt + \\sigma dW_t,\nand to be efficient in the following sense: we do not expect a prespecified\ntrading strategy whose value is almost surely always nonnegative to outperform\nthe index greatly. The efficiency of the index imposes severe restrictions on\nits growth rate; in particular, for a long investment horizon we should have\n\\mu\\approx r+\\sigma^2, where r is the interest rate. This provides another\npartial solution to the equity premium puzzle. All our mathematical results are\nextremely simple.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.2327v1"
    },
    {
        "title": "Okun's law revisited. Is there structural unemployment in developed\n  countries?",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Okun's law for the biggest developed countries is re-estimated using the most\nrecent data on real GDP per capita and the rate of unemployment. Our results\nshow that the change in unemployment rate can be predicted with a high\naccuracy. The link needs the introduction of a structural break which might be\ncaused by the change in monetary policy or/and in measurement units.\nStatistically, the link between the studied variables is characterized by the\ncoefficient of determination between 0.40 (Australia) and 0.84 (the USA). The\nresidual errors can be associated with measurement errors. The obtained results\nsuggest the absence of structural unemployment in the studied developed\ncountries.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4383v1"
    },
    {
        "title": "Employment, unemployment and real economic growth",
        "authors": [
            "Ivan Kitov",
            "Oleg Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We have modeled the employment/population ratio in the largest developed\ncountries. Our results show that the evolution of the employment rate since\n1970 can be predicted with a high accuracy by a linear dependence on the\nlogarithm of real GDP per capita. All empirical relationships estimated in this\nstudy need a structural break somewhere between 1975 and 1995. Such breaks\nmight be caused by revisions to monetary policy (e.g. inflation targeting)\nor/and changes in measurement units. Statistically, the link between measured\nand predicted rate of employment is characterized by the coefficient of\ndetermination from 0.84 (Australia) to 0.95 (Japan). The model residuals are\nlikely to be associated with measurement errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4399v1"
    },
    {
        "title": "General Equilibrium as a Topological Field Theory",
        "authors": [
            "Eric Kemp-Benedict"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  General equilibrium is the dominant theoretical framework for economic policy\nanalysis at the level of the whole economy. In practice, general equilibrium\ntreats economies as being always in equilibrium, albeit in a sequence of\nequilibria as driven by external changes in parameters. This view is sometimes\ndefended on the grounds that internal dynamics are fast, while external changes\nare slow, so that the economy can be viewed as adjusting instantaneously to any\nchanged conditions. However, the argument has not been presented in a rigorous\nway. In this paper we show that when conditions are such that: a) economies do\nrespond essentially instantaneously to external influences; b) the external\nchanges are small compared to the values that characterize the economy; and c)\nthe economy's dynamics are continuous and first-order in time (as for Walrasian\ntatonnement), the resulting economic theory is equivalent to a topological\nfield theory. Because it is a topological theory it has no dynamics in a strict\nsense, and so perturbatively---that is, when examining dynamics in the region\nof a critical point---the field theory behaves as general equilibrium posits.\nHowever, the field-theoretic form of the theory admits non-perturbative\ninstanton solutions that link different critical points. Thus, in this theory,\nand in contrast to general equilibrium, the internal dynamics of the model\noccasionally make an appearance in the form of abrupt, noise-driven transitions\nbetween critical points.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.1705v1"
    },
    {
        "title": "Roles of discount rate, risk premium, and device performance in\n  estimating the cost of energy for photovoltaics",
        "authors": [
            "Sergei Manzhos"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We show that different rates should be used for borrowing and discount rates,\nand that the risk-free rate should be used for discounting when assessing and\ncomparing the cost of energy accross diffferent producers and technologies, on\nthe example of photovoltaics. Recent quantitative models using the same rate\nfor borrowing and discounting lead to an underestimation of the cost for risky\nborrowers and to distorted sensitivities of the cost to financial and\nnon-financial factors. Specifically, it is shown that they may lead to gross\nunderestimation of the importance of solar-to-electricity conversion\nefficiency. The importance of device efficiency is re-established under the\ntreatment of the discount rate proposed here. The effects on the cost of energy\nof the installation efficiency and degradation rate, on the discount rate and\nrisk premium as well as on the project lifetime are estimated.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.1903v1"
    },
    {
        "title": "The competitiveness versus the wealth of a country",
        "authors": [
            "Boris Podobnik",
            "Davor Horvatic",
            "Dror Y. Kenett",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Politicians world-wide frequently promise a better life for their citizens.\nWe find that the probability that a country will increase its {\\it per capita}\nGDP ({\\it gdp}) rank within a decade follows an exponential distribution with\ndecay constant $\\lambda = 0.12$. We use the Corruption Perceptions Index (CPI)\nand the Global Competitiveness Index (GCI) and find that the distribution of\nchange in CPI (GCI) rank follows exponential functions with approximately the\nsame exponent as $\\lambda$, suggesting that the dynamics of {\\it gdp}, CPI, and\nGCI may share the same origin. Using the GCI, we develop a new measure, which\nwe call relative competitiveness, to evaluate an economy's competitiveness\nrelative to its {\\it gdp}. For all European and EU countries during the\n2008-2011 economic downturn we find that the drop in {\\it gdp} in more\ncompetitive countries relative to {\\it gdp} was substantially smaller than in\nrelatively less competitive countries, which is valuable information for\npolicymakers.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.2813v1"
    },
    {
        "title": "Positive skewness, anti-leverage, reverse volatility asymmetry, and\n  short sale constraints: Evidence from the Chinese markets",
        "authors": [
            "Liang Wu",
            "Jingyi Luo",
            "Yingkai Tang",
            "Gregory Bardes"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  There are some statistical anomalies in the Chinese stock market, i.e.,\npositive return skewness, anti-leverage effect (positive returns induce higher\nvolatility than negative returns); and reverse volatility asymmetry\n(contemporaneous return-volatility correlation is positive). In this paper, we\nfirst confirm the existence of these anomalies using daily firm-level stock\nreturn data on the raw returns, excess returns and normalized excess returns.\nWe empirically show that the asymmetry response of investors to news is one\ncause of the statistical anomalies if short sales are constrained. Then in the\ncontext of slow adoption of security lending policy, we conduct panel analysis\nand empirically verify that the lifting of short sale constraints leads to\nsignificantly less skewness, less anti-leverage effect and less reverse\nvolatility asymmetry. Positive skewness is a feature of lottery. Investors are\nencouraged to bet on the upside lottery like potentials in the Chinese markets\nwhere the stocks skew more to the upside when short sales are constrained.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.01824v1"
    },
    {
        "title": "Deleveraging, short sale constraints and market crash",
        "authors": [
            "Liang Wu",
            "Lei Zhang",
            "Zhiming Fu"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In this paper, we develop a theory of market crashes resulting from a\ndeleveraging shock. We consider two representative investors in a market\nholding different opinions about the public available information. The\ndeleveraging shock forces the high confidence investors to liquidate their\nrisky assets to pay back their margin loans. When short sales are constrained,\nthe deleveraging shock creates a liquidity vacuum in which no trades can occur\nbetween the two representative investors until the price drop to a threshold\nbelow which low confidence investors take over the reduced demands. There are\ntwo roles short sellers could play to stabilize the market. First, short\nsellers provide extra supply in a bullish market so that the price of the asset\nis settled lower than otherwise. Second, short sellers catch the falling price\nearlier in the deleveraging process if they are previously allowed to hold a\nlarger short position. We apply our model to explain the recent deleveraging\ncrisis of the Chinese market with great success.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.03777v1"
    },
    {
        "title": "A Generalized Probability Framework to Model Economic Agents' Decisions\n  Under Uncertainty",
        "authors": [
            "Emmanuel Haven",
            "Sandro Sozzo"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The applications of techniques from statistical (and classical) mechanics to\nmodel interesting problems in economics and finance has produced valuable\nresults. The principal movement which has steered this research direction is\nknown under the name of `econophysics'. In this paper, we illustrate and\nadvance some of the findings that have been obtained by applying the\nmathematical formalism of quantum mechanics to model human decision making\nunder `uncertainty' in behavioral economics and finance. Starting from\nEllsberg's seminal article, decision making situations have been experimentally\nverified where the application of Kolmogorovian probability in the formulation\nof expected utility is problematic. Those probability measures which by\nnecessity must situate themselves in Hilbert space (such as `quantum\nprobability') enable a faithful representation of experimental data. We thus\nprovide an explanation for the effectiveness of the mathematical framework of\nquantum mechanics in the modeling of human decision making. We want to be\nexplicit though that we are not claiming that decision making has microscopic\nquantum mechanical features.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06734v1"
    },
    {
        "title": "Early Warning Signs of the Economic Crisis in Greece: A Warning for\n  Other Countries and Regions",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Warning signs about the developing economic crisis in Greece were present in\nthe growth rate of the Gross Domestic Product (GDP) and in the growth of the\nGDP well before the economic collapse. The growth rate was strongly unstable.\nOn average, in less than 50 years, it decreased 10-folds but after reaching a\nlow minimum it quickly increased 6-folds only to crash before completing the\nfull cycle. The decreasing growth rate was leading to an asymptotic maximum of\nthe GDP but it was soon replaced by a fast-increasing growth rate propelling\nthe GDP along a pseudo-hyperbolic trajectory, which if continued would have\nescaped to infinity in 2017. Such a growth could not have been possibly\nsupported. Under these conditions, the economic collapse in Greece was\ninevitable.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06992v1"
    },
    {
        "title": "It's a Trap: Emperor Palpatine's Poison Pill",
        "authors": [
            "Zachary Feinstein"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In this paper we study the financial repercussions of the destruction of two\nfully armed and operational moon-sized battle stations (\"Death Stars\") in a\n4-year period and the dissolution of the galactic government in Star Wars. The\nemphasis of this work is to calibrate and simulate a model of the banking and\nfinancial systems within the galaxy. Along these lines, we measure the level of\nsystemic risk that may have been generated by the death of Emperor Palpatine\nand the destruction of the second Death Star. We conclude by finding the\neconomic resources the Rebel Alliance would need to have in reserve in order to\nprevent a financial crisis from gripping the galaxy through an optimally\nallocated banking bailout.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.09054v1"
    },
    {
        "title": "Unified Growth Theory Contradicted by the GDP/cap Data",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Mathematical properties of the historical GDP/cap distributions are discussed\nand explained. These distributions are frequently incorrectly interpreted and\nthe Unified Growth Theory is an outstanding example of such common\nmisconceptions. It is shown here that the fundamental postulates of this theory\nare contradicted by the data used in its formulation. The postulated three\nregimes of growth did not exist and there was no takeoff at any time. It is\ndemonstrated that features interpreted as three regimes of growth represent\njust mathematical properties of a single, monotonically-increasing\ndistribution, indicating that a single mechanism should be used to explain the\nhistorical economic growth. It is shown that using different socio-economic\nconditions for different perceived parts of the historical GDP/cap data is\nirrelevant and scientifically unjustified. The GDP/cap growth was indeed\nincreasing slowly over a long time and fast over a short time but these\nfeatures represent a single, uniform and uninterrupted growth process, which\nshould be interpreted as whole using a single mechanism of growth.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.09323v2"
    },
    {
        "title": "Analysis of Advisor Portfolio using Multivariate Time Series and Cosine\n  Similarity",
        "authors": [
            "Gayatri Pradhan"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  In mutual fund, an investment adviser gives advice to clients about investing\nin securities such as stocks, bonds, mutual funds, or exchange traded funds.\nSome investment advisers manage portfolios of securities. In this paper, we\nanalyze advisor portfolio for each advisor so as to recognize the pattern in\neach adviser's portfolio. Such analysis helps the sales people to sell the fund\ncompany products to the suitable advisors desirable to the nature of the\nproduct they want to sell. This is done by analyzing the kind of products\nadvisors have been interested in which will help to boost the sales of the\nproducts as sales people will be reaching the appropriate advisors.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.06546v1"
    },
    {
        "title": "SME investment best strategies. Outliers for assessing how to optimize\n  performance",
        "authors": [
            "Marcel Ausloos",
            "Roy Cerqueti",
            "Francesca Bartolacci",
            "Nicola G. Castellano"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Any research on strategies for reaching business excellence aims at revealing\nthe appropriate course of actions any executive should consider. Thus,\ndiscussions take place on how effective a performance measurement system can be\nestimated, or/and validated. Can one find an adequate measure (i) on the\nperformance result due to whatever level of investment, and (ii) on the timing\nof such investments? We argue that extreme value statistics provide the answer.\nWe demonstrate that the level and timing of investments allow to be forecasting\nsmall and medium size enterprises (SME) performance, - at financial crisis\ntimes. The \"investment level\" is taken as the yearly total tangible asset\n(TTA). The financial/economic performance indicators defining growth are the\nsales or total assets variations; profitability is defined from returns on\ninvestments or returns on sales. Companies on the Italian Stock Exchange STAR\nMarket serve as example. It is found from the distributions extreme values that\noutlier companies (with positive performance) are those with the lowest but\ngrowing TTA. In contrast, the SME with low TTA, but which did not increase its\nTTA, before the crisis, became a negative outlier. The outcome of these\nstatistical findings should suggest strategies to SME board members.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.09583v1"
    },
    {
        "title": "Key Borrowers Detection by Long-Range Interactions",
        "authors": [
            "Fuad Aleskerov",
            "Natalia Meshcheryakova",
            "Alisa Nikitina",
            "Sergey Shvydun"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We propose a new method for assessing agents' influence in financial network\nstructures, which takes into consideration the intensity of interactions. A\ndistinctive feature of this approach is that it considers not only direct\ninteractions of agents of the first level and indirect interactions of the\nsecond level, but also long-range indirect interactions. At the same time we\ntake into account the attributes of agents as well as the possibility of impact\nto a single agent from a group of other agents. This approach helps us to\nidentify systemically important elements which cannot be detected by classical\ncentrality measures or other indices. The proposed method was used to analyze\nthe banking foreign claims for the end of 1Q 2015. Under the approach, two\ntypes of key borrowers were detected: a) major players with high ratings and\npositive credit history; b) intermediary players, which have a great scale of\nfinancial activities through the organization of favorable investment\nconditions and positive business climate.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.10115v1"
    },
    {
        "title": "Momentum and liquidity in cryptocurrencies",
        "authors": [
            "Stjepan Begušić",
            "Zvonko Kostanjčar"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The goal of this paper is to explore the relationship between momentum\neffects and liquidity in cryptocurrency markets. Portfolios based on\nmomentum-liquidity bivariate sorts are formed and rebalanced on a varying\nnumber of cryptocurrencies through time. We find a strong momentum effect in\nthe most liquid cryptocurrencies, which supports the theories of investor\nherding behavior. Moreover, we propose two profitable long-only strategies: the\nilliquid losers and liquid winners, which exhibit improved risk adjusted\nperformance over the market capitalization weighted portfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00890v1"
    },
    {
        "title": "Fat Tails in Financial Return Distributions Revisited: Evidence from the\n  Korean Stock Market",
        "authors": [
            "Cheoljun Eom",
            "Taisei Kaizoji",
            "Enrico Scalas"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This study empirically re-examines fat tails in stock return distributions by\napplying statistical methods to an extensive dataset taken from the Korean\nstock market. The tails of the return distributions are shown to be much fatter\nin recent periods than in past periods and much fatter for small-capitalization\nstocks than for large-capitalization stocks. After controlling for the 1997\nKorean foreign currency crisis and using the GARCH filter models to control for\nvolatility clustering in the returns, the fat tails in the distribution of\nresiduals are found to persist. We show that market crashes and volatility\nclustering may not sufficiently account for the existence of fat tails in\nreturn distributions. These findings are robust regardless of period or type of\nstock group.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.02567v1"
    },
    {
        "title": "The Economics of Enlightenment: Time Value of Knowledge and the Net\n  Present Value (NPV) of Knowledge Machines, A Proposed Approach Adapted from\n  Finance",
        "authors": [
            "Ravi Kashyap"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We formulate one methodology to put a value or price on knowledge using well\naccepted techniques from finance. We provide justifications for these finance\nprinciples based on the limitations of the physical world we live in. We start\nwith the intuition for our method to value knowledge and then formalize this\nidea with a series of axioms and models. To the best of our knowledge this is\nthe first recorded attempt to put a numerical value on knowledge. The\nimplications of this valuation exercise, which places a high premium on any\npiece of knowledge, are to ensure that participants in any knowledge system are\nbetter trained to notice the knowledge available from any source. Just because\nsomeone does not see a connection does not mean that there is no connection. We\nneed to try harder and be more open to acknowledging the smallest piece of new\nknowledge that might have been brought to light by anyone from anywhere about\nanything.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.03233v4"
    },
    {
        "title": "Advanced Mathematical Business Strategy Formulation Design",
        "authors": [
            "Song-Kyoo Kim"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This paper deals with the explicit design of strategy formulations to make\nthe best strategic choices from a conventional matrix form of representing\nstrategic choices. The explicit strategy formulation is an analytical model\nwhich is targeted to provide a mathematical strategy framework to find the best\nmoment for strategy shifting to prepare rapid market changes. This theoretical\nmodel could be adapted into practically any strategic decision making situation\nwhen a strategic formulation is described as a matrix form with quantitative\nmeasured decision parameters. Analytically tractable results are obtained by\nusing the fluctuation theory and these results are capable to predict the best\nmoments of changing strategies in a matrix form. This research helps strategy\ndecision makers who want to find the optimal moments of shifting present\nstrategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.06890v3"
    },
    {
        "title": "Industrial Concentration of the Brazilian Automobile Market and\n  Positioning in the World Market",
        "authors": [
            "Zionam E. L. Rolim",
            "Rafaël R. de Oliveira",
            "Hélio M. de Oliveira"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This paper surveys the evolution of industrial concentration of the Brazilian\nautomotive market as well as its positioning in the worldmarket. Data available\nby OICA (International Organization of Motor Vehicle Manufacturers) were used\nto better understand the characteristics of the Brazilian market on the world\nstage. A cluster analysis algorithm (by the k-means technique) ranks Brazil\nwith a concentration profile in a group of countries like US and South Korea,\nin contrast to countries such as Germany, Canada and Japan, or even France and\nItaly. It is rather usual to characterize the market structure through\nindustrial concentration indices: we revisit CR ratios (concentration ratios),\nHHI (Herfindahl-Hirschman index), B (Rosenbluth index), and CCI (Horvath\ncomprehensive concentration index). Data of Anfavea-Brazil (Associacao Nacional\ndos Fabricantes de Veiculos Automotores) were used to estimate these indices in\nthe period 2012-2018 for the national automobile industry. The values obtained\nindicate that by 1998 the automotive sector was behaving as an\noligopoly-differentiated. However, the values of more recent periods\n(particularly CR4) strongly indicate that the sector is currently moderately\nconcentrated and is changing for a quasi-devolved market.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.09686v1"
    },
    {
        "title": "Considering pricing and uncertainty in designing a reverse logistics\n  network",
        "authors": [
            "Mohsen Zamani",
            "Mahdi Abolghasemi",
            "Seyed Mohammad Seyed Hosseini",
            "Mir Saman Pishvaee"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Companies try to maximize their profits by recovering returned products of\nhighly uncertain quality and quantity. In this paper, a reverse logistics\nnetwork for an Original Equipment Manufacturer (OEM) is presented. Returned\nproducts are selected for remanufacturing or scrapping, based on their quality\nand proportional prices are offered to customers. A Mixed Integer Non-linear\nProgramming (MINLP) model is proposed to determine the location of collection\ncenters, the optimum price of returned products and the sorting policy. The\nrisk in the objective function is measured using the Conditional Value at Risk\n(CVaR) metric. CVaR measures the risk of an investment in a conservative way by\nconsidering the maximum lost. The results are analyzed for various values of\nthe risk parameters ({\\alpha}, and {\\lambda}). These parameters indicate that\nconsidering risk affects prices, the classification of returned products, the\nlocation of collection centers and, consequently, the objective function. The\nmodel performs more conservatively when the weight of the CVaR part ({\\lambda})\nand the value of the confidence level {\\alpha} are increased. The results show\nthat better profits are obtained when we take CVaR into account.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.11633v1"
    },
    {
        "title": "Effects of Regional Trade Agreement to Local and Global Trade Purity\n  Relationships",
        "authors": [
            "Siyu Huang",
            "Wensha Gou",
            "Hongbo Cai",
            "Xiaomeng Li",
            "Qinghua Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In contrast to the rapid integration of the world economy, many regional\ntrade agreements (RTAs) have also emerged since the early 1990s. This seeming\ncontradiction has encouraged scholars and policy makers to explore the true\neffects of RTAs, including both regional and global trade relationships. This\npaper defines synthesized trade resistance and decomposes it into natural and\nartificial factors. Here, we separate the influence of geographical distance,\neconomic volume, overall increases in transportation and labor costs and use\nthe expectation maximization algorithm to optimize the parameters and quantify\nthe trade purity indicator, which describes the true global trade environment\nand relationships among countries. This indicates that although global and most\nregional trade relations gradually deteriorated during the period 2007-2017,\nRTAs generate trade relations among members, especially contributing to the\nrelative prosperity of EU and NAFTA countries. In addition, we apply the\nnetwork to reflect the purity of the trade relations among countries. The\neffects of RTAs can be analyzed by comparing typical trade unions and trade\ncommunities, which are presented using an empirical network structure. This\nanalysis shows that the community structure is quite consistent with some trade\nunions, and the representative RTAs constitute the core structure of\ninternational trade network. However, the role of trade unions has weakened,\nand multilateral trade liberalization has accelerated in the past decade. This\nmeans that more countries have recently tended to expand their trading partners\noutside of these unions rather than limit their trading activities to RTAs.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.07329v1"
    },
    {
        "title": "Investment Disputes and Abnormal Volatility of Stocks",
        "authors": [
            "Jozef Barunik",
            "Zdenek Drabek",
            "Matej Nevrla"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Dramatic growth of investment disputes between foreign investors and host\nstates rises serious questions about the impact of those disputes on investors.\nThis paper is the first to explain increased uncertainty of investors about the\noutcome of arbitration, which may or may not lead to compensation for damages\nclaimed by the investor. We find robust evidence that investment disputes lead\nto abnormal share fluctuations of companies involved in disputes with host\ncountries. Importantly, while a positive outcome for an investor decreases\nuncertainty back to original levels, we document strong increase in the\nvolatility of companies with negative outcome for the investor. We find that\nseveral variables including size of the award, political instability, location\nof arbitration, country of origin of investor or public policy considerations\nin host country explain large portion of the investor's uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.10505v1"
    },
    {
        "title": "Corporate Social Responsibility and Corporate Governance: A cognitive\n  approach",
        "authors": [
            "Rania Béji",
            "Ouidad Yousfi",
            "Abdelwahed Omri"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This chapter aims to critically review the existing literature on the\nrelationship between corporate social responsibility (CSR) and corporate\ngovernance features. Drawn on management and corporate governance theories, we\ndevelop a theoretical model that makes explicit the links between board\ndiversity, CSR committees' attributes, CSR and financial performance.\nParticularly, we show that focusing on the cognitive and demographic\ncharacteristics of board members could provide more insights on the link\nbetween corporate governance and CSR. We also highlight how the functioning and\nthe composition of CSR committees, could be valuable to better understand the\nrelationship between corporate governance and CSR.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.09218v1"
    },
    {
        "title": "How Decentralized is the Governance of Blockchain-based Finance:\n  Empirical Evidence from four Governance Token Distributions",
        "authors": [
            "Johannes Rude Jensen",
            "Victor von Wachter",
            "Omri Ross"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Novel blockchain technology provides the infrastructure layer for the\ncreation of decentralized appli-cations. A rapidly growing ecosystem of\napplications is built around financial services, commonly referred to as\ndecentralized finance. Whereas the intangible concept of decentralization is\npresented as a key driver for the applications, defining and measuring\ndecentralization is multifaceted. This pa-per provides a framework to quantify\ndecentralization of governance power among blockchain appli-cations. Governance\nof the applications is increasingly important and requires striking a balance\nbe-tween broad distribution, fostering user activity, and financial incentives.\nTherefore, we aggregate, parse, and analyze empirical data of four finance\napplications calculating coefficients for the statistical dispersion of the\ngovernance token distribution. The gauges potentially support IS scholars for\nan objective evaluation of the capabilities and limitations of token governance\nand for fast iteration in design-driven governance mechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10096v2"
    },
    {
        "title": "The Impact of COVID-19 on Stock Market Volatility in Pakistan",
        "authors": [
            "Ateeb Akhter Shah Syed",
            "Kaneez Fatima"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This paper examines the impact of coronavirus (COVID-19) on stock market\nvolatility (SMV) in Pakistan by controlling the effect of exchange rate,\ninterest rate and government/central bank interventions to combat the pandemic.\nWe used the vector autoregressive (VAR) model over a sample period ranging from\nFebruary 25, 2020 to December 7, 2020. We find that a shock to total daily\ncoronavirus cases in Pakistan lead to a significant increase in SMV. This\nresult is aligned with a vast literature on pandemics and investors uncertainty\nand remains robust to several robustness checks applied in our analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03219v1"
    },
    {
        "title": "Locational Marginal Pricing: Towards a Free Market in Power",
        "authors": [
            "Martin Higgins"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Nothing has done more to empower the free market, enterprise, and meritocracy\nthan the spread of electricity and power to everyone. The power system has been\nthe precursor to the greatest period of innovation in our history and has meant\nthat visionaries with revolutionary ideas can compete with those with capital,\npolitical power, and means. Electricity, therefore, has been the great\nequalising force of the last 150 years, enhancing the productivity of the\nmasses and granting prosperity to whole swathes of our nation. Whilst\nelectricity has been one of the single largest innovations in enhancing the\npower of free markets, it is somewhat ironic that the way power is sold to\nconsumers is largely unfree. The market is highly regulated, centralised, and\nis often used for political football by cynical politicians on both sides of\nthe political spectrum. Introducing Locational Marginal Pricing into the UK\ngrid system will increase economic freedom in the consumer markets for power,\nreduce prices for the poorest in the UK, decrease transmission losses, increase\nthe permeation of low carbon generation in the grid, and incentivise investment\nin the UK's Northern Powerhouse initiative.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.10937v1"
    },
    {
        "title": "The black hole of logistics costs of digitizing commodity money",
        "authors": [
            "Boliang Lin",
            "Ruixi Lin"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  In this paper, we reveal the depreciation mechanism of representative money\n(banknotes) from the perspective of logistics warehousing costs. Although it\nhas long been the dream of economists to stabilize the buying power of the\nmonetary units, the goal we have honest money always broken since the central\nbank depreciate the currency without limit. From the point of view of modern\nlogistics, the key functions of money are the store of value and low logistics\n(circulation and warehouse) cost. Although commodity money (such as gold and\nsilver) has the advantages of a wealth store, its disadvantage is the high\nlogistics cost. In comparison to commodity money, credit currency and digital\ncurrency cannot protect wealth from loss over a long period while their\nlogistics costs are negligible. We proved that there is not such honest money\nfrom the perspective of logistics costs, which is both the store of value like\nprecious metal and without logistics costs in circulation like digital\ncurrency. The reason hidden in the back of the depreciation of banknotes is the\nblack hole of storage charge of the anchor overtime after digitizing commodity\nmoney. Accordingly, it is not difficult to infer the inevitable collapse of the\nBretton woods system. Therefore, we introduce a brand-new currency named honest\ndevalued stable-coin and built a attenuation model of intrinsic value of the\nhonest money based on the change mechanism of storage cost of anchor assets,\nlike gold, which will lay the theoretical foundation for a stable monetary\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.11772v1"
    },
    {
        "title": "Elastic Cash",
        "authors": [
            "Anup Rao"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Elastic Cash is a new decentralized mechanism for regulating the money\nsupply. The mechanism operates by modifying the supply so that an interest rate\ndetermined by a public market is kept approximately fixed. It can be\nincorporated into the conventional monetary system to improve the elasticity of\nthe US Dollar, and it can be used to design new elastic cryptocurrencies that\nremain decentralized.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04244v2"
    },
    {
        "title": "Modeling international crisis synchronization in the World Trade Web",
        "authors": [
            "Pau Erola",
            "Albert Diaz-Guilera",
            "Sergio Gomez",
            "Alex Arenas"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Trade is a fundamental pillar of economy and a form of social organization.\nIts empirical characterization at the worldwide scale is represented by the\nWorld Trade Web (WTW), the network built upon the trade relationships between\nthe different countries. Several scientific studies have focused on the\nstructural characterization of this network, as well as its dynamical\nproperties, since we have registry of the structure of the network at different\ntimes in history. In this paper we study an abstract scenario for the\ndevelopment of global crises on top of the structure of connections of the WTW.\nAssuming a cyclic dynamics of national economies and the interaction of\ndifferent countries according to the import-export balances, we are able to\ninvestigate, using a simple model of pulse-coupled oscillators, the\nsynchronization phenomenon of crises at the worldwide scale. We focus on the\nlevel of synchronization measured by an order parameter at two different\nscales, one for the global system and another one for the mesoscales defined\nthrough the topology. We use the WTW network structure to simulate a network of\nIntegrate-and-Fire oscillators for six different snapshots between years 1950\nand 2000. The results reinforce the idea that globalization accelerates the\nglobal synchronization process, and the analysis at a mesoscopic level shows\nthat this synchronization is different before and after globalization periods:\nafter globalization, the effect of communities is almost inexistent.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.2024v1"
    },
    {
        "title": "To lag or not to lag? How to compare indices of stock markets that\n  operate at different times",
        "authors": [
            "Leonidas Sandoval Junior"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Financial markets worldwide do not have the same working hours. As a\nconsequence, the study of correlation or causality between financial market\nindices becomes dependent on wether we should consider in computations of\ncorrelation matrices all indices in the same day or lagged indices. The answer\nthis article proposes is that we should consider both. In this work, we use 79\nindices of a diversity of stock markets across the world in order to study\ntheir correlation structure, and discover that representing in the same network\noriginal and lagged indices, we obtain a better understanding of how indices\nthat operate at different hours relate to each other.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4586v3"
    },
    {
        "title": "Mathematical Constraints on Financially Viable Public Policy",
        "authors": [
            "Martin Gremm",
            "Mark B. Wise"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Social Security and other public policies can be viewed as a series of cash\nin and outflows that depend on parameters such as the age distribution of the\npopulation and the retirement age. Given forecasts of these parameters,\npolicies can be designed to be financially stable, i.e., to terminate with a\nzero balance. If reality deviates from the forecasts, policies normally\nterminate with a surplus or a deficit. We derive constraints on the cash flows\nof robust policies that terminate with zero balance even in the presence of\nforecasting errors. Social Security and most similar policies are not robust.\nWe show that non-trivial robust policies exist and provide a recipe for\nconstructing robust extensions of non-robust policies. An example illustrates\nour results.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.6340v1"
    },
    {
        "title": "IIGHGINT: A generalization to the modified GHG intensity universal\n  indicator toward a production/consumption insensitive border carbon tax",
        "authors": [
            "Reza Farrahi Moghaddam",
            "Fereydoun Farrahi Moghaddam",
            "Mohamed Cheriet"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  A global agreement on how to reduce and cap human footprint, especially their\nGHG emissions, is very unlikely in near future. At the same time, bilateral\nagreements would be inefficient because of their neural and balanced nature.\nTherefore, unilateral actions would have attracted attention as a practical\noption. However, any unilateral action would most likely fail if it is not fair\nand also if it is not consistent with the world trade organization's (WTO's)\nrules, considering highly heterogeneity of the global economy. The modified GHG\nintensity (MGHGINT) indicator, hereafter called Inequality-adjusted\nProduction-based GHGINT (IPGHGINT), was put forward to address this need in the\nform of a universal indicator applicable to every region regardless of its\neconomic and social status. Nonetheless, the original MGHGINT indicator ignores\nhidden consumption-related emissions, and therefore it could be unfair to some\nproduction-oriented regions in the current bipolar production/consumption\nworld. Here, we propose two generalizations, called Inequality-adjusted\nConsumption-based GHGINT (ICGHGINT) and Inequality-adjusted\nProduction/Consumption-Insensitive GHGINT (IIGHGINT), to the IPGHGINT in order\nto combine both production and consumption emissions in a unified and balanced\nmanner. The impact of this generalizations on the associated border carbon tax\nrates is evaluated in order to validate their practicality.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0301v2"
    },
    {
        "title": "Bayesian analysis of redistribution policy with a fixed scale",
        "authors": [
            "Guy Cirier"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  A government has to finance a risk for its population. It shares the charges\namong the population with a fixed scale based on economic criteria. Various\norganisms have to collect and to redistribute fairly the subsidies. Under these\nconditions, when the size of the organisms is varied, the distribution's laws\nof the criteria are exponential families and criteria are semi linear\nsufficient statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.2867v1"
    },
    {
        "title": "Mathematical Foundations for the Economy of Giving",
        "authors": [
            "W. P. Weijland"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  This paper shows how we can build a model for transactions when goods are\ngiven away in the expectation of a later settlement. In settings where people\nkeep track of their social accounts we are able to redefine concepts like\naccount balance, yield curve and the law of diminishing returns. The model\nprovides us with a result that expresses how people have a structural\npreference for one recipient over the other regardless the actual account\nbalance. Hence a building block in the social fabric of a community. Finally, a\nfundamental theorem is presented to show how suppliers and recipients use their\naccount balance in order to reach an equilibrium in the exchange of goods much\nlike the traditional balance between supply and demand.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.4664v2"
    },
    {
        "title": "On Convergence in the Spatial AK Growth Models",
        "authors": [
            "Gani Aldashev",
            "Serik Aldashev",
            "Timoteo Carletti"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Recent research in economic theory attempts to study optimal economic growth\nand spatial location of economic activity in a unified framework. So far, the\nkey result of this literature - asymptotic convergence, even in the absence of\ndecreasing returns to capital - relies on specific assumptions about the\nobjective of the social planner. We show that this result does not depend on\nsuch restrictive assumptions and obtains for a broader class of objective\nfunctions. We also generalize this finding, allowing for the time-varying\ntechnology parameter, and provide an explicit solution for the dynamics of\nspatial distribution of the capital stock.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.4887v1"
    },
    {
        "title": "Testing for rational speculative bubbles in the Brazilian residential\n  real-estate market",
        "authors": [
            "Marcelo M. de Oliveira",
            "Alexandre C. L. Almeida"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Speculative bubbles have been occurring periodically in local or global real\nestate markets and are considered a potential cause of economic crises. In this\ncontext, the detection of explosive behaviors in the financial market and the\nimplementation of early warning diagnosis tests are of critical importance. The\nrecent increase in Brazilian housing prices has risen concerns that the\nBrazilian economy may have a speculative housing bubble. In the present paper,\nwe employ a recently proposed recursive unit root test in order to identify\npossible speculative bubbles in data from the Brazilian residential real-estate\nmarket. The empirical results show evidence for speculative price bubbles both\nin Rio de Janeiro and Sao Paulo, the two main Brazilian cities.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.7615v1"
    },
    {
        "title": "Field Theory of Macroeconomics",
        "authors": [
            "Heribert Genreith"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  In this article we will show that the Macro-Economy and its growth can be\nmodelled and explained exactly in principle by commonly known Field Theory from\ntheoretical physics. We will show the main concepts and calculations needed and\nshow that calculation and prediction of economic growth then gets indeed\npossible in Dollars and Cents. As every field theory it is based on an equation\nof continuity, which in economic terms means the full balance of all sources\nand sinks of Capital (Assets) and real Goods (GDP) in the bulk. Uniqueness of\nfield theory of macroeconomics then can be derived from adapting Noether's\nTheorems, which is based on the notion of invariants to derive unique field\nequations. We will show that the only assumption which is needed for a\nself-consistent non-linear macro-economic theory is that the well known\nQuantity Equation, used in corrected formulation, holds at least locally in\ntime.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.6334v1"
    },
    {
        "title": "Grid Integration Costs of Fluctuating Renewable Energy Sources",
        "authors": [
            "Jonas Müller",
            "Marcus Hildmann",
            "Andreas Ulbig",
            "Göran Andersson"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The grid integration of intermittent Renewable Energy Sources (RES) causes\ncosts for grid operators due to forecast uncertainty and the resulting\nproduction schedule mismatches. These so-called profile service costs are\nmarginal cost components and can be understood as an insurance fee against RES\nproduction schedule uncertainty that the system operator incurs due to the\nobligation to always provide sufficient control reserve capacity for power\nimbalance mitigation. This paper studies the situation for the German power\nsystem and the existing German RES support schemes. The profile service costs\nincurred by German Transmission System Operators (TSOs) are quantified and\nmeans for cost reduction are discussed. In general, profile service costs are\ndependent on the RES prediction error and the specific workings of the power\nmarkets via which the prediction error is balanced. This paper shows both how\nthe prediction error can be reduced in daily operation as well as how profile\nservice costs can be reduced via optimization against power markets and/or\nactive curtailment of RES generation.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.7237v1"
    },
    {
        "title": "Interbank markets and multiplex networks: centrality measures and\n  statistical null models",
        "authors": [
            "Leonardo Bargigli",
            "Giovanni di Iasio",
            "Luigi Infante",
            "Fabrizio Lillo",
            "Federico Pierobon"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The interbank market is considered one of the most important channels of\ncontagion. Its network representation, where banks and claims/obligations are\nrepresented by nodes and links (respectively), has received a lot of attention\nin the recent theoretical and empirical literature, for assessing systemic risk\nand identifying systematically important financial institutions. Different\ntypes of links, for example in terms of maturity and collateralization of the\nclaim/obligation, can be established between financial institutions. Therefore\na natural representation of the interbank structure which takes into account\nmore features of the market, is a multiplex, where each layer is associated\nwith a type of link. In this paper we review the empirical structure of the\nmultiplex and the theoretical consequences of this representation. We also\ninvestigate the betweenness and eigenvector centrality of a bank in the\nnetwork, comparing its centrality properties across different layers and with\nMaximum Entropy null models.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.05751v1"
    },
    {
        "title": "Hyperinflation in Brazil, Israel, and Nicaragua revisited",
        "authors": [
            "M. A. Szybisz",
            "L. Szybisz"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The aim of this work is to address the description of hyperinflation regimes\nin economy. The spirals of hyperinflation developed in Brazil, Israel, and\nNicaragua are revisited. This new analysis of data indicates that the episodes\noccurred in Brazil and Nicaragua can be understood within the frame of the\nmodel available in the literature, which is based on a nonlinear feedback (NLF)\ncharacterized by an exponent $\\beta>0$. In the NLF model the accumulated\nconsumer price index carries a finite time singularity of the type\n$1/(t_c-t)^{(1- \\beta)/\\beta}$ determining a critical time $t_c$ at which the\neconomy would crash. It is shown that in the case of Brazil the entire episode\ncannot be described with a unique set of parameters because the time series was\nstrongly affected by a change of policy. This fact gives support to the \"so\ncalled\" Lucas critique, who stated that model's parameters usually change once\npolicy changes. On the other hand, such a model is not able to provide any\n$t_c$ in the case of the weaker hyperinflation occurred in Israel. It is shown\nthat in this case the fit of data yields $\\beta \\to 0$. This limit leads to the\nlinear feedback formulation which does not predict any $t_c$. An extension for\nthe NLF model is suggested.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00092v3"
    },
    {
        "title": "Long memory and multifractality: A joint test",
        "authors": [
            "John Goddard",
            "Enrico Onali"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The properties of statistical tests for hypotheses concerning the parameters\nof the multifractal model of asset returns (MMAR) are investigated, using Monte\nCarlo techniques. We show that, in the presence of multifractality,\nconventional tests of long memory tend to over-reject the null hypothesis of no\nlong memory. Our test addresses this issue by jointly estimating long memory\nand multifractality. The estimation and test procedures are applied to exchange\nrate data for 12 currencies. In 11 cases, the exchange rate returns are\naccurately described by compounding a NIID series with a multifractal\ntime-deformation process. There is no evidence of long memory.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00903v1"
    },
    {
        "title": "Fighting Uncertainty with Uncertainty: A Baby Step",
        "authors": [
            "Ravi Kashyap"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We can overcome uncertainty with uncertainty. Using randomness in our choices\nand in what we control, and hence in the decision making process, could\npotentially offset the uncertainty inherent in the environment and yield better\noutcomes. The example we develop in greater detail is the news-vendor inventory\nmanagement problem with demand uncertainty. We briefly discuss areas, where\nsuch an approach might be helpful, with the common prescription, \"Don't Simply\nOptimize, Also Randomize; perhaps best described by the term -\nRandoptimization\".\n  1. News-vendor Inventory Management\n  2. School Admissions\n  3. Journal Submissions\n  4. Job Candidate Selection\n  5. Stock Picking\n  6. Monetary Policy\n  This methodology is suitable for the social sciences since the primary source\nof uncertainty are the members of the system themselves and presently, no\nmethods are known to fully determine the outcomes in such an environment, which\nperhaps would require being able to read the minds of everyone involved and to\nanticipate their actions continuously. Admittedly, we are not qualified to\nrecommend whether such an approach is conducive for the natural sciences,\nunless perhaps, bounds can be established on the levels of uncertainty in a\nsystem and it is shown conclusively that a better understanding of the system\nand hence improved decision making will not alter the outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04043v8"
    },
    {
        "title": "Puzzling properties of the historical growth rate of income per capita\n  explained",
        "authors": [
            "Ron W Nielsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Galor discovered many mysteries of the growth process. He lists them in his\nUnified Growth Theory and wonders how they can be explained. Close inspection\nof his mysteries reveals that they are of his own creation. They do not exist.\nHe created them by his habitually distorted presentation of data. One of his\nself-created mysteries is the mystery of the alleged sudden spurt in the growth\nrate of income per capita. This sudden spurt never happened. In order to\nunderstand the growth rate of income per capita, its mathematical properties\nare now explored and explained. The explanation is illustrated using the\nhistorical world economic growth. Galor also wonders about the sudden spurt in\nthe growth rate of population. We show that this sudden spurt was also created\nby the distorted presentation of data. The mechanism of the historical economic\ngrowth and of the growth of human population is yet to be explained but it\nwould be unproductive to try to explain the non-existing and self-created\nmysteries of the growth process described in the scientifically unacceptable\nUnified Growth Theory. However, the problem is much deeper than just the\nexamination of this theory. Demographic Growth Theory is based on the incorrect\nbut deeply entrenched postulates developed by accretion over many years and now\ngenerally accepted in the economic and demographic research, postulates\nrevolving around the concept of Malthusian stagnation and around a transition\nfrom stagnation to growth. The study presented here and earlier similar\npublications show that these postulates need to be replaced by interpretations\nbased on the mathematical analysis of data and on the correct understanding of\nhyperbolic distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00736v2"
    },
    {
        "title": "Cyclical properties of supply-side and demand-side shocks in oil-based\n  commodity markets",
        "authors": [
            "Tomas Krehlik",
            "Jozef Barunik"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Oil markets profoundly influence world economies through determination of\nprices of energy and transports. Using novel methodology devised in frequency\ndomain, we study the information transmission mechanisms in oil-based commodity\nmarkets. Taking crude oil as a supply-side benchmark and heating oil and\ngasoline as demand-side benchmarks, we document new stylized facts about\ncyclical properties of the transmission mechanism generated by volatility\nshocks with heterogeneous frequency responses. Our first key finding is that\nshocks to volatility with response shorter than one week are increasingly\nimportant to the transmission mechanism over the studied period. Second,\ndemand-side shocks to volatility are becoming increasingly important in\ncreating short-run connectedness. Third, the supply-side shocks to volatility\nresonating in both the long run and short run are important sources of\nconnectedness.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.07020v2"
    },
    {
        "title": "Depreciation and the Time Value of Money",
        "authors": [
            "Brendon Farrell"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Generally accepted depreciation methods do not compute the intrinsic value of\nan asset, as they do not factor for the Time Value of Money, a key principle\nwithin financial theory. This is disadvantageous, as knowing the intrinsic\nvalue of an asset can assist with making effective purchase and sale decisions.\n  By applying the Time Value of Money principle to deprecation and book\nvaluation, methods can be formulated to approximate the intrinsic valuation of\na depreciable asset, which improves the capacity for buyers and sellers of\nassets to make rational decisions.\n  A deprecation method is formulated within, which aims to better match book\nvalue with intrinsic value. While this method makes many assumptions and thus\nhas limitations, more complex formulas, which factor for a greater number of\nvariables, can be created using a similar approach, to produce better\napproximations for intrinsic value.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.00080v2"
    },
    {
        "title": "Stochastic Effects in a Discretized Kinetic Model of Economic Exchange",
        "authors": [
            "M. L. Bertotti",
            "A. K. Chattopadhyay",
            "G. Modanese"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Linear stochastic models and discretized kinetic theory are two complementary\nanalytical techniques used for the investigation of complex systems of economic\ninteractions. The former employ Langevin equations, with an emphasis on stock\ntrade; the latter is based on systems of ordinary differential equations and is\nbetter suited for the description of binary interactions, taxation and welfare\nredistribution. We propose a new framework which establishes a connection\nbetween the two approaches by introducing stochastic effects into the kinetic\nmodel based on Langevin and Fokker-Planck formalisms. Numerical simulations of\nthe resulting model indicate positive correlations between the Gini index and\nthe total wealth, that suggests a growing inequality with increasing income.\nFurther analysis shows a simultaneous decrease in inequality as social mobility\nincreases in presence of a conserved total wealth, in conformity with economic\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04943v3"
    },
    {
        "title": "General Equilibrium Under Convex Portfolio Constraints and Heterogeneous\n  Risk Preferences",
        "authors": [
            "Tyler Abbot"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  This paper characterizes the equilibrium in a continuous time financial\nmarket populated by heterogeneous agents who differ in their rate of relative\nrisk aversion and face convex portfolio constraints. The model is studied in an\napplication to margin constraints and found to match real world observations\nabout financial variables and leverage cycles. It is shown how margin\nconstraints increase the market price of risk and decrease the interest rate by\nforcing more risk averse agents to hold more risky assets, producing a higher\nequity risk premium. In addition, heterogeneity and margin constraints are\nshown to produce both pro- and counter-cyclical leverage cycles. Beyond two\ntypes, it is shown how constraints can cascade and how leverage can exhibit\nhighly non-linear dynamics. Finally, empirical results are given, documenting a\nnovel stylized fact which is predicted by the model, namely that the leverage\ncycle is both pro- and counter-cyclical.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.05877v3"
    },
    {
        "title": "Análisis de cointegración con una aplicación al mercado de deuda\n  en Estados Unidos, Canadá y México",
        "authors": [
            "Emiliano Diaz"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Certain theoretical aspects of vector autoregression (VAR) as tools to model\neconomic time series are revised, in particular their capacity to include both\nshort term and long term information. The VAR model, in its error correction\nform, is derived and the permanent-transitory decomposition of factors proposed\nby Gonzalo and Granger (1995) studied. An introductory exposition of estimation\ntheory for reduced rank models, necessary to estimate the error correction\nmodel, is given. Cointegration analysis using the VAR model is carried out for\ngovernment bond interest rates (short, medium and long term) of the United\nStates, Mexico and Canada, with the objective of finding the long-term common\nfactors that drive the system. The error correction model of this system is\nestimated using Johansen's method. Using this estimation the\npermanent-transitory decomposition of the system is calculated. Hypothesis\ntests are carried out on permanent factors to determine which of the nine rates\nstudied drive the system.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.05912v1"
    },
    {
        "title": "Token Economics in Energy Systems: Concept, Functionality and\n  Applications",
        "authors": [
            "Jun Zhang",
            "Fei-Yue Wang",
            "Siyuan Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Traditional centralized energy systems have the disadvantages of difficult\nmanagement and insufficient incentives. Blockchain is an emerging technology,\nwhich can be utilized in energy systems to enhance their management and\ncontrol. Integrating token economy and blockchain technology, token economic\nsystems in energy possess the characteristics of strong incentives and low\ncost, facilitating integrating renewable energy and demand side management, and\nproviding guarantees for improving energy efficiency and reducing emission.\nThis article describes the concept and functionality of token economics, and\nthen analyzes the feasibility of applying token economics in the energy\nsystems, and finally discuss the applications of token economics with an\nexample in integrated energy systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.01261v1"
    },
    {
        "title": "A Dichotomous Analysis of Unemployment Benefits",
        "authors": [
            "Xingwei Hu"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Using equal employment opportunity as a fairness hypothesis, real-time\nbalanced-budget rule as a constraint, and policy stability as an objective, we\nderive a scientific formula for taxation policy and formulate a fair allocation\nfor unemployment benefits. The general setting is a coalitional game in which a\nrandom subset of the players is selected to take a task, resulting in a random\npayoff; we attempt to divide the payoff among the players fairly. The formula\ndescribes a fair, debt-free, and asymptotic risk-free payroll tax rate for\ngiven unemployment and spending levels. Also robust to the choice of other\nobjectives, the tax rate stimulates employment and boosts productivity within\nthis fair and sustainable framework. With additional assumptions on\nproductivity, the tax rate results in equality of outcome, reducing poverty and\neconomic inequality. The fair division rule and the valuation approach could be\napplied to similar profit- or cost-sharing situations, including voting rights,\nhealth insurance, road sharing, and machine learning.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08563v5"
    },
    {
        "title": "Econophysics as conceived by Meghnad Saha",
        "authors": [
            "Bikas K. Chakrabarti"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We trace the initiative by Professor Meghnad Saha to develop a (statistical)\nphysics model of market economy and his search for the mechanism to constrain\nthe entropy maximized width of the income distribution in a society such that\nthe spread of inequality can be minimized.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09279v1"
    },
    {
        "title": "Model Risk in Real Option Valuation",
        "authors": [
            "Carol Alexander",
            "Xi Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We introduce a general decision tree framework to value an option to\ninvest/divest in a project, focusing on the model risk inherent in the\nassumptions made by standard real option valuation methods. We examine how real\noption values depend on the dynamics of project value and investment costs, the\nfrequency of exercise opportunities, the size of the project relative to\ninitial wealth, the investor's risk tolerance (and how it changes with wealth)\nand several other choices about model structure. For instance, contrary to\nstylized facts from previous literature, real option values can actually\ndecrease with the volatility of the underlying project value and increase with\ninvestment costs.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.00817v2"
    },
    {
        "title": "Cryptocurrencies, Mainstream Asset Classes and Risk Factors - A Study of\n  Connectedness",
        "authors": [
            "George Milunovich"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We investigate connectedness within and across two major groups or assets: i)\nfive popular cryptocurrencies, and ii) six major asset classes plus two\ncommonly employed risk factors. Granger-causality tests uncover six direct\nchannels of causality from the elements of the mainstream assets/risk factors\ngroup to digital assets. On the other hand there are two statistically\nsignificant causal links going in the other direction. In order to provide some\nperspective on the magnitude of the uncovered linkages we supplement the\nanalysis by estimating networks from forecast error variance decompositions.\nThe estimated connectedness within the groups is relatively large, whereas the\nlinkages across the two groups are small in comparison. Namely, less than 2.2\npercent of future uncertainty of any cryptocurrency is sourced from all\nnon-crypto assets combined, while the joint contribution of all digital assets\nto non-crypto uncertainty does not exceed 1.5 percent.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.03072v1"
    },
    {
        "title": "A Big data analytical framework for portfolio optimization",
        "authors": [
            "Dhanya Jothimani",
            "Ravi Shankar",
            "Surendra S. Yadav"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  With the advent of Web 2.0, various types of data are being produced every\nday. This has led to the revolution of big data. Huge amount of structured and\nunstructured data are produced in financial markets. Processing these data\ncould help an investor to make an informed investment decision. In this paper,\na framework has been developed to incorporate both structured and unstructured\ndata for portfolio optimization. Portfolio optimization consists of three\nprocesses: Asset selection, Asset weighting and Asset management. This\nframework proposes to achieve the first two processes using a 5-stage\nmethodology. The stages include shortlisting stocks using Data Envelopment\nAnalysis (DEA), incorporation of the qualitative factors using text mining,\nstock clustering, stock ranking and optimizing the portfolio using heuristics.\nThis framework would help the investors to select appropriate assets to make\nportfolio, invest in them to minimize the risk and maximize the return and\nmonitor their performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.07188v2"
    },
    {
        "title": "Elementary Microeconomics of the Talmudic Rule",
        "authors": [
            "Anton Salikhmetov"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This paper takes a look at the Talmudic rule aka the 1/N rule aka the uniform\ninvestment strategy from the viewpoint of elementary microeconomics.\nSpecifically, we derive the cardinal utility function for a Talmud-obeying\nagent which happens to have the Cobb-Douglas form. Further, we investigate\nindividual supply and demand due to rebalancing and compare them to market\ndepth of an exchange. Finally, we discuss how operating as a liquidity provider\ncan benefit the Talmud-obeying agent with every exchange transaction in terms\nof the identified utility function.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.00814v1"
    },
    {
        "title": "The Arrival of News and Return Jumps in Stock Markets: A Nonparametric\n  Approach",
        "authors": [
            "Juho Kanniainen",
            "Ye Yue"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This paper introduces a non-parametric framework to statistically examine how\nnews events, such as company or macroeconomic announcements, contribute to the\npre- and post-event jump dynamics of stock prices under the intraday\nseasonality of the news and jumps. We demonstrate our framework, which has\nseveral advantages over the existing methods, by using data for i) the S&P 500\nindex ETF, SPY, with macroeconomic announcements and ii) Nasdaq Nordic\nLarge-Cap stocks with scheduled and non-scheduled company announcements. We\nprovide strong evidence that non-scheduled company announcements and some\nmacroeconomic announcements contribute jumps that follow the releases and also\nsome evidence for pre-jumps that precede the scheduled arrivals of public\ninformation, which may indicate non-gradual information leakage. Especially\ninterim reports of Nordic large-cap companies are found containing important\ninformation to yield jumps in stock prices. Additionally, our results show that\nreleases of unexpected information are not reacted to uniformly across Nasdaq\nNordic markets, even if they are jointly operated and are based on the same\nexchange rules.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.02691v1"
    },
    {
        "title": "Report for the Commission of Inquiry Respecting the Muskrat Falls\n  Project",
        "authors": [
            "Bent Flyvbjerg",
            "Alexander Budzier"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This report was commissioned by the Commission of Inquiry Respecting the\nMuskrat Falls Project to provide the national and international context in\nwhich the Muskrat Falls Project took place. The Commission asked for the report\nto cover three specific topics of questions: (1) What is the national and\ninternational context of the Muskrat Falls Project with regards to cost overrun\nand schedule overrun? (What are the typical cost and schedule overruns of\nhydro-electric dam projects? How do hydro-electric dams compare to other\ncapital investment projects? How do Canadian projects compare to other\ncountries?), (2) What are the causes and root causes of cost and schedule\noverruns? (3) What are recommendations, based on international experience and\nresearch into capital investment projects, to prevent cost and schedule\noverruns in hydro-electric dam projects and other capital investment projects?\n  Keywords: Hydroelectric Dams, Megaprojects, Cost Overrun, Schedule Overrun,\nOptimism Bias, Strategic Misrepresentation, Infrastructure, Capital Investment\nProjects, Canada, Muskrat Falls\n",
        "pdf_link": "http://arxiv.org/pdf/1901.03698v1"
    },
    {
        "title": "PROOF OF VALUE ALIENATION (PoVA) - a concept of a cryptocurrency\n  issuance protocol",
        "authors": [
            "Tim Shuliar",
            "Nikita Goldsmit"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In this paper, we will describe a concept of a cryptocurrency issuance\nprotocol which supports digital currencies in a Proof-of-Work (< PoW >) like\nmanner. However, the methods assume alternative utilization of assets used for\ncryptocurrency creation (rather than purchasing electricity necessary for <\nmining >).\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04928v1"
    },
    {
        "title": "Quantitative Cost and Schedule Risk Analysis of Nuclear Waste Storage",
        "authors": [
            "Alexander Budzier",
            "Bent Flyvbjerg",
            "Andi Garavaglia",
            "Andreas Leed"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This study provides an independent, outside-in estimate of the cost and\nschedule risks of nuclear waste storage projects. Based on a reference class of\n216 past, comparable projects, risk of cost overrun was found to be 202% or\nless, with 80% certainty, i.e., 20% risk of an overrun above 202%. Based on a\nreference class of 200 past, comparable projects, risk of schedule overrun was\nfound to be 104% or less, with 80% certainty, i.e., 20% risk of overrun above\n104%. Cost risk and schedule risk are both substantial for nuclear waste\nstorage projects.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.11123v1"
    },
    {
        "title": "Taxation of a GMWB Variable Annuity in a Stochastic Interest Rate Model",
        "authors": [
            "Andrea Molent"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Modeling taxation of Variable Annuities has been frequently neglected but\naccounting for it can significantly improve the explanation of the withdrawal\ndynamics and lead to a better modeling of the financial cost of these insurance\nproducts. The importance of including a model for taxation has first been\nobserved by Moenig and Bauer (2016) while considering a GMWB Variable Annuity.\nIn particular, they consider the simple Black-Scholes dynamics to describe the\nunderlying security. Nevertheless, GMWB are long term products and thus\naccounting for stochastic interest rate has relevant effects on both the\nfinancial evaluation and the policy holder behavior, as observed by Gouden\\`ege\net al. (2018). In this paper we investigate the outcomes of these two elements\ntogether on GMWB evaluation. To this aim, we develop a numerical framework\nwhich allows one to efficiently compute the fair value of a policy. Numerical\nresults show that accounting for both taxation and stochastic interest rate has\na determinant impact on the withdrawal strategy and on the cost of GMWB\ncontracts. In addition, it can explain why these products are so popular with\npeople looking for a protected form of investment for retirement.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.11296v2"
    },
    {
        "title": "Regularities in stock markets",
        "authors": [
            "Abhin Kakkad",
            "Harsh Vasoya",
            "Arnab K. Ray"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  From the stock markets of six countries with high GDP, we study the stock\nindices, S&P 500 (NYSE, USA), SSE Composite (SSE, China), Nikkei (TSE, Japan),\nDAX (FSE, Germany), FTSE 100 (LSE, Britain) and NIFTY (NSE, India). The daily\nmean growth of the stock values is exponential. The daily price fluctuations\nabout the mean growth are Gaussian, but with a non-zero asymptotic convergence.\nThe growth of the monthly average of stock values is statistically self-similar\nto their daily growth. The monthly fluctuations of the price follow a Wiener\nprocess, with a decline of the volatility. The mean growth of the daily volume\nof trade is exponential. These observations are globally applicable and\nunderline regularities across global stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.00371v2"
    },
    {
        "title": "Resolution of the St. Petersburg paradox using Von Mises axiom of\n  randomness",
        "authors": [
            "Andrea Berdondini"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In this article we will propose a completely new point of view for solving\none of the most important paradoxes concerning game theory. The solution\ndevelop shifts the focus from the result to the strategy s ability to operate\nin a cognitive way by exploiting useful information about the system. In order\nto determine from a mathematical point of view if a strategy is cognitive, we\nuse Von Mises' axiom of randomness. Based on this axiom, the knowledge of\nuseful information consequently generates results that cannot be reproduced\nrandomly. Useful information in this case may be seen as a significant datum\nfor the recipient, for their present or future decision-making process.\nFinally, by resolving the paradox from this new point of view, we will\ndemonstrate that an expected gain that tends toward infinity is not always a\nconsequence of a cognitive and non-random strategy. Therefore, this result\nleads us to define a hierarchy of values in decision-making, where the\ncognitive aspect, whose statistical consequence is a divergence from random\nbehaviour, turns out to be more important than the expected gain.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11054v1"
    },
    {
        "title": "Does the short-term boost of renewable energies guarantee their stable\n  long-term growth? Assessment of the dynamics of feed-in tariff policy",
        "authors": [
            "Milad Mousavian H.",
            "Hamed Shakouri G.",
            "Alinaghi Mashayekhi",
            "Aliyeh Kazemi"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Feed in tariff (FiT) is one of the most efficient ways that many governments\nthroughout the world use to stimulate investment in renewable energies (REs)\ntechnology. For governments, financial management of the policy is very\nchallenging as that it needs a considerable amount of budget to support RE\nproducers during the long remuneration period. In this paper, we illuminate\nthat the early growth of REs capacity could be a temporary boost and the system\nelements would backlash the policy if financial circumstances are not handled\nwell. To show this, we chose Iran as the case, which is in the infancy period\nof FiT implementation. Iran started the implementation of FiT policy in 2015\naiming to achieve 5 GW of renewable capacity until 2021. Analyses show that the\nprobable financial crisis will not only lead to inefficient REs development\nafter the target time (2021), but may also cause the existing plants to fail.\nSocial tolerance for paying REs tax and potential investors trust emanated from\nbudget related mechanisms are taken into consideration in the system dynamics\nmodel developed in this research to reflect those financial effects, which have\nrarely been considered in the previous researches. To prevent the financial\ncrisis of the FiT funding and to maintain the stable growth in long term, three\npolicy scenarios are analyzed: continuation of the current program with higher\nFiT rates, adjusting the FiT rates based on the budget status, and adjusting\nthe tax on electricity consumption for the development of REs based on the\nbudget status. The results demonstrate that adjusting the tax on electricity\nconsumption for the development of REs based on budget status leads to the best\npolicy result for a desired installed capacity development without any negative\nsocial effects and financial crises.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11224v1"
    },
    {
        "title": "Stability of Equilibrium Asset Pricing Models: A Necessary and\n  Sufficient Condition",
        "authors": [
            "Jaroslav Borovicka",
            "John Stachurski"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We obtain an exact necessary and sufficient condition for the existence and\nuniqueness of equilibrium asset prices in infinite horizon, discrete-time,\narbitrage free environments. Through several applications we show how the\ncondition sharpens and improves on previous results. We connect the condition,\nand hence the problem of existence and uniqueness of asset prices, with the\nrecent literature on stochastic discount factor decompositions. Finally, we\ndiscuss computation of the test value associated with our condition, providing\na Monte Carlo method that is naturally parallelizable.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00778v2"
    },
    {
        "title": "Concepts, Components and Collections of Trading Strategies and Market\n  Color",
        "authors": [
            "Ravi Kashyap"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This paper acts as a collection of various trading strategies and useful\npieces of market information that might help to implement such strategies. This\nlist is meant to be comprehensive (though by no means exhaustive) and hence we\nonly provide pointers and give further sources to explore each strategy\nfurther. To set the stage for this exploration, we consider the factors that\ndetermine good and bad trades, the notions of market efficiency, the real\nprospect amidst the seemingly high expectations of homogeneous expectations\nfrom human beings and the catch-22 connotations that arise while comprehending\nthe true meaning of rational investing. We can broadly classify trading ideas\nand client market color material into Delta-One and Derivative strategies since\nthis acts as a natural categorization that depends on the expertise of the\nvarious trading desks that will implement these strategies. For each strategy,\nwe will have a core idea and we will present different flavors of this central\ntheme to demonstrate that we can easily cater to the varying risk appetites,\nregional preferences, asset management styles, investment philosophies,\nliability constraints, investment horizons, notional trading size, trading\nfrequency and other preferences of different market participants.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.02144v2"
    },
    {
        "title": "The Possible Effects of Personal Income Tax and Value Added Tax on\n  Consumer Behaviors",
        "authors": [
            "Ahmet Ak",
            "Oner Gumus"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In economics literature, it is accepted that all people are rational and they\ntry to maximize their utilities as possible as they can. In addition, economic\ntheories are formed with the assumptions not suitable to real life. For\ninstance, indifference curves are drawn with the assumptions that there are two\ngoods, people are rational, more is preferred to less and so on. Hence, the\nconsumer behaviors are guessed according to this analysis. Nevertheless, these\nare invalid in real life. And this inconsistencey are examined by behavioral\neconomics and neuroeconomics. Behavioral economics claims that people can\nbehave what they are not expected since people can be irrational, their\nwillpower is limited and altruistic behaviors can be seen and they can give\nmore value to what they own. As a result of these, consumer behaviors become\nmore different than that of economic theory. In addition to behavioral\neconomics, neuroeconomics also examines consumer behaviors more differently\nthan mainstream economic theory. It emphasizes the people using prefrontial\ncortex of the brain are more rational than the people using hippocampus of the\nbrain. Therefore, people can make illogical choices compared to economic\ntheory. In these cases, levying taxes such as personal income tax or value\nadded tax can be ineffective or effective. In other words, the effect becomes\nambigious. Hence,the hypothesis that if government desires to levy personal\nincome tax or value added tax, it makes a detailed research in terms of\nproductivity of taxes forms the fundamental of this study.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03141v1"
    },
    {
        "title": "Creating a unique mobile financial services framework for Myanmar: A\n  Review",
        "authors": [
            "Dr Ma Nang Laik",
            "Chester Mark Hong Wei"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Myanmar is languishing at the bottom of key international indexes. United\nNations considers the country as a structurally weak and vulnerable economy.\nYet, from 2011 when Myanmar ended decades of military rule and isolationism and\ntransited towards democracy, its breakneck development has led to many\nconsidering the country to be one of the final frontiers for growth in the Asia\nregion. One such industry that has benefitted from the opening of the country\nis telecommunications. The mobile penetration rate at 4.8% in 2011 has\nincreased significantly to 90% in 2016. Despite renewed optimism and\ndevelopment in the economy, one statistic remains disappointing. According to a\nreport by Asian Development Bank (ADB), only 23% of the adult population have\naccess to a bank account. This highlights a need to reach out and increase\naccess to financial resources to a population that is severely unbanked and\nunderbanked. This creates an interesting proposition of allowing both the\ntelecommunications and financial sector to form the mobile financial services\n(MFS) sector and meet the need of improving access to financial resources for\nthe population. This report explores the government role in supporting, growing\nand sustaining the MFS sector and conducts a comparative research into\nSingapore, Malaysia and Thailand to understand the steps taken by these\ngovernments to develop their own Financial Technology (FinTech), specifically\nMFS, industry. Finally, the report will present preliminary recommendations\nthat the Myanmar government could consider implementing to drive growth in its\nMFS sector.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03793v2"
    },
    {
        "title": "Art Pricing with Computer Graphic Techniques",
        "authors": [
            "Lan Ju",
            "Zhiyong Tu",
            "Changyong Xue"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This paper makes the first attempt to introduce the tools from computer\ngraphics into the art pricing research. We argue that the creation of a\npainting calls for a combination of conceptual effort and painting effort from\nthe artist. However, as the important price determinants, both efforts are long\nmissing in the traditional hedonic model because they are hard to measure. This\npaper draws on the digital pictures of auctioned paintings from various\nrenowned artists, and applies the image recognition techniques to measure the\nvariances of lines and colors of these paintings. We then use them as the\nproxies for the artist's painting effort, and include them in the hedonic\nregression to test their significance. Our results show that the variances of\nlines and colors of a painting can significantly positively explain the sales\nprice in a general context. Our suggested measurements can better capture the\ncontent heterogeneity of paintings hence improving on the traditional art\npricing methodology. Our approach also provides a quantitative perspective for\nboth valuation and authentication of paintings.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03800v1"
    },
    {
        "title": "Fundamental Analysis in China: An Empirical Study of the Relationship\n  between Financial Ratios and Stock Prices",
        "authors": [
            "Lijuan Ma",
            "Marcel Ausloos",
            "Christophe Schinckus",
            "H. L. Felicia Chong"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The informational context is regularly questioned in a transitional economic\nregime like the one implemented in China or Vietnam. This article investigates\nthis issue and the predictive power of fundamental analysis in such context and\nmore precisely in a Chinese context with an analysis of 3 different industries\n(media, power, and steel). Through 3 different kinds of correlation, we examine\n25 financial determinants for 60 Chinese listed companies between 2011 and\n2015. Our results show that fundamental analysis can effectively be used as an\ninvestment tool in transitional economic context. Contrasting with the EMH for\nwhich the accounting information is instantaneously integrated into the\nfinancial information (stock prices), our study suggests that these two levels\nof information are not synchronized in China opening therefore a door for a\nfundamental analysis based prediction. Furthermore, our results also indicate\nthat accounting information illustrates quite well the economic reality since\nfinancial reports in each industry can disclose a part of stock value\ninformation in line with the economic situation of the industry under\nconsideration.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06746v1"
    },
    {
        "title": "An AGI Modifying Its Utility Function in Violation of the Orthogonality\n  Thesis",
        "authors": [
            "James D. Miller",
            "Roman Yampolskiy",
            "Olle Häggström"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  An artificial general intelligence (AGI) might have an instrumental drive to\nmodify its utility function to improve its ability to cooperate, bargain,\npromise, threaten, and resist and engage in blackmail. Such an AGI would\nnecessarily have a utility function that was at least partially observable and\nthat was influenced by how other agents chose to interact with it. This\ninstrumental drive would conflict with the orthogonality thesis since the\nmodifications would be influenced by the AGI's intelligence. AGIs in highly\ncompetitive environments might converge to having nearly the same utility\nfunction, one optimized to favorably influencing other agents through game\ntheory.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.00812v1"
    },
    {
        "title": "Financial replicator dynamics: emergence of systemic-risk-averting\n  strategies",
        "authors": [
            "Indrajit Saha",
            "Veeraruna Kavitha"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We consider a random financial network with a large number of agents. The\nagents connect through credit instruments borrowed from each other or through\ndirect lending, and these create the liabilities. The settlement of the debts\nof various agents at the end of the contract period can be expressed as\nsolutions of random fixed point equations. Our first step is to derive these\nsolutions (asymptotically), using a recent result on random fixed point\nequations. We consider a large population in which agents adapt one of the two\navailable strategies, risky or risk-free investments, with an aim to maximize\ntheir expected returns (or surplus). We aim to study the emerging strategies\nwhen different types of replicator dynamics capture inter-agent interactions.\nWe theoretically reduced the analysis of the complex system to that of an\nappropriate ordinary differential equation (ODE). We proved that the\nequilibrium strategies converge almost surely to that of an attractor of the\nODE. We also derived the conditions under which a mixed evolutionary stable\nstrategy (ESS) emerges; in these scenarios the replicator dynamics converges to\nan equilibrium at which the expected returns of both the populations are equal.\nFurther the average dynamics (choices based on large observation sample) always\naverts systemic risk events (events with large fraction of defaults). We\nverified through Monte Carlo simulations that the equilibrium suggested by the\nODE method indeed represents the limit of the dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.00886v3"
    },
    {
        "title": "Equity-Based Incentives, Production/Service Functions And Game Theory",
        "authors": [
            "Michael C. Nwogugu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  EBIs/ESOs substantially change the traditional production/service function\nbecause ESOs/EBIs can have different psychological effects(motivation or\nde-motivation), and can create intangible capital and different economic\npayoffs. Although Game Theory is flawed, it can be helpful in describing\ninteractions in ESO/EBIs transactions. ESOs/EBIs involve two-stage games and\nthere are no perfect Nash Equilibria for the two sub-games. The large number of\nactual and potential participants in these games significantly complicates\nresolution of equilibria and increases the dynamism of the games given that\nplayers are more sensitive to other peoples moves in such games. This article:\na) analyzes how ESOs/EBIs affect traditional assumptions of production\nfunctions (in both the manufacturing and service sectors), b) analyzes\nESOs/EBIs transactions using game theory concepts, c) illustrates some of the\nlimitations of game theory.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.01855v1"
    },
    {
        "title": "Reanimating a Dead Economy: Financial and Economic Analysis of a Zombie\n  Outbreak",
        "authors": [
            "Zachary Feinstein"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this paper, we study the financial and economic implications of a zombie\nepidemic on a major industrialized nation. We begin with a consideration of the\nepidemiological modeling of the zombie contagion. The emphasis of this work is\non the computation of direct and indirect financial consequences of this\ncontagion of the walking dead. A moderate zombie outbreak leaving 1 million\npeople dead in a major industrialized nation could result in GDP losses of\n23.44% over the subsequent year and a drop in financial market of 29.30%. We\nconclude by recommending policy actions necessary to prevent this potential\neconomic collapse.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.09943v1"
    },
    {
        "title": "EB-dynaRE: Real-Time Adjustor for Brownian Movement with Examples of\n  Predicting Stock Trends Based on a Novel Event-Based Supervised Learning\n  Algorithm",
        "authors": [
            "Yang Chen",
            "Emerson Li"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Stock prices are influenced over time by underlying macroeconomic factors.\nJumping out of the box of conventional assumptions about the unpredictability\nof the market noise, we modeled the changes of stock prices over time through\nthe Markov Decision Process, a discrete stochastic control process that aids\ndecision making in a situation that is partly random. We then did a \"Region of\nInterest\" (RoI) Pooling of the stock time-series graphs in order to predict\nfuture prices with existing ones. Generative Adversarial Network (GAN) is then\nused based on a competing pair of supervised learning algorithms, to regenerate\nfuture stock price projections on a real-time basis. The supervised learning\nalgorithm used in this research, moreover, is original to this study and will\nhave wider uses. With the ensemble of these algorithms, we are able to\nidentify, to what extent, each specific macroeconomic factor influences the\nchange of the Brownian/random market movement. In addition, our model will have\na wider influence on the predictions of other Brownian movements.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11473v1"
    },
    {
        "title": "Tall wheatgrass (Thinopyrum ponticum (Podp)) in a real farm context, a\n  sustainable perennial alternative to rye (Secale cereale L.) cultivation in\n  marginal lands",
        "authors": [
            "Carlos S. Ciria",
            "Carlos M. Sastre",
            "Juan Carrasco",
            "Pilar Ciria"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In order to face the expected increasing demand of energy crops without\ncreating conflicts of land occupation sustainability, farmers need to find\nreliable alternatives in marginal agricultural areas where the production of\nfood hardly ever is economically and environmentally sustainable. The purpose\nof this work was the study of the viability of the introduction of new non food\ncrops in marginal areas of real farms. This study compares the profit margin\nand the energy and environmental performance of growing tall wheatgrass, in the\nmarginal area of a rainfed farm versus rye, the annual crop sowed traditionally\nin the marginal area of the farm. The cited farm owned 300 ha of which about 13\npercent was marginal. The methodology was based on the use of the profit margin\nof the crops as indicator for the economic assessment and Life Cycle Assessment\nLCA as technique for the energy and the environmental evaluations. Results of\nthe economic analysis showed a slight enhancement of the profit margin for tall\nwheatgrass 156 Euro ha-1 y-1 compared to rye 145 Euro ha-1 y-1. Environmental\nLCA was driven by CO2 fixation due to soil organic matter increase and reduced\ninputs consumption for tall wheatgrass that produced a Global Warming Potential\nGWP of -1.9 Mg CO2 eq ha-1 y-1 versus 1.6 Mg CO2 eq ha-1 y-1 obtained for rye.\nTall wheatgrass cultivation primary energy consumption was less than 40 percent\nof rye s consumption. According to the results achieved it was concluded that\ntall wheatgrass is better option than rye from the energy and the environmental\npoint of views and slight better option from the economic view. Considering\nthese results, monetarization of the CO2 eq reductions of tall wheatgrass\ncompared to rye is essential to improve its profit margin and promote the\nimplantation of this new crop in marginal areas of farms.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13395v1"
    },
    {
        "title": "Sustainable Banking; Evaluation of the European Business Models",
        "authors": [
            "Saeed Nosratabadi",
            "Gergo Pinter",
            "Amir Mosavi",
            "Sandor Semperger"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Sustainable business models also offer banks competitive advantages such as\nincreasing brand reputation and cost reduction. However, no framework is\npresented to evaluate the sustainability of banking business models. To bridge\nthis theoretical gap, the current study using A Delphi-Analytic Hierarchy\nProcess method, firstly, developed a sustainable business model to evaluate the\nsustainability of the business model of banks. In the second step, the\nsustainability performance of sixteen banks from eight European countries\nincluding Norway, the UK, Poland, Hungary, Germany, France, Spain, and Italy,\nassessed. The proposed business model components of this study were ranked in\nterms of their impact on achieving sustainability goals. Consequently, the\nproposed model components of this study, based on their impact on\nsustainability, are respectively value proposition, core competencies,\nfinancial aspects, business processes, target customers, resources, technology,\ncustomer interface, and partner network. The results of the comparison of the\nbanks studied by each country disclosed that the sustainability of the\nNorwegian and German banks business models is higher than in other counties.\nThe studied banks of Hungary and Spain came in second, the banks of the UK,\nPoland, and France ranked third, and finally, the Italian banks ranked fourth\nin the sustainability of their business models.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13423v1"
    },
    {
        "title": "Information Token Driven Machine Learning for Electronic Markets:\n  Performance Effects in Behavioral Financial Big Data Analytics",
        "authors": [
            "Jim Samuel"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Conjunct with the universal acceleration in information growth, financial\nservices have been immersed in an evolution of information dynamics. It is not\njust the dramatic increase in volumes of data, but the speed, the complexity\nand the unpredictability of big-data phenomena that have compounded the\nchallenges faced by researchers and practitioners in financial services. Math,\nstatistics and technology have been leveraged creatively to create analytical\nsolutions. Given the many unique characteristics of financial bid data (FBD) it\nis necessary to gain insights into strategies and models that can be used to\ncreate FBD specific solutions. Behavioral finance data, a subset of FBD, is\nseeing exponential growth and this presents an unprecedented opportunity to\nstudy behavioral finance employing big data analytics methodologies. The\npresent study maps machine learning (ML) techniques and behavioral finance\ncategories to explore the potential for using ML techniques to address\nbehavioral aspects in FBD. The ontological feasibility of such an approach is\npresented and the primary purpose of this study is propositioned- ML based\nbehavioral models can effectively estimate performance in FBD. A simple machine\nlearning algorithm is successfully employed to study behavioral performance in\nan artificial stock market to validate the propositions.\n  Keywords: Information; Big Data; Electronic Markets; Analytics; Behavior\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06642v1"
    },
    {
        "title": "Refining Understanding of Corporate Failure through a Topological Data\n  Analysis Mapping of Altman's Z-Score Model",
        "authors": [
            "Wanling Qiu",
            "Simon Rudkin",
            "Pawel Dlotko"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Corporate failure resonates widely leaving practitioners searching for\nunderstanding of default risk. Managers seek to steer away from trouble, credit\nproviders to avoid risky loans and investors to mitigate losses. Applying\nTopological Data Analysis tools this paper explores whether failing firms from\nthe United States organise neatly along the five predictors of default proposed\nby the Z-score models. Firms are represented as a point cloud in a five\ndimensional space, one axis for each predictor. Visualising that cloud using\nBall Mapper reveals failing firms are not often neighbours. As new modelling\napproaches vie to better predict firm failure, often using black boxes to\ndeliver potentially over-fitting models, a timely reminder is sounded on the\nimportance of evidencing the identification process. Value is added to the\nunderstanding of where in the parameter space failure occurs, and how firms\nmight act to move away from financial distress. Further, lenders may find\nopportunity amongst subsets of firms that are traditionally considered to be in\ndanger of bankruptcy but actually sit in characteristic spaces where failure\nhas not occurred.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.10318v1"
    },
    {
        "title": "Some Applications of Lie Groups in Theory of Technical Progress",
        "authors": [
            "Le Anh Vu",
            "Duong Quang Hoa",
            "Nguyen Minh Tri",
            "Ha Van Hieu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In recent decades, we have known some interesting applications of Lie theory\nin the theory of technological progress. Firstly, we will discuss some results\nof R. Saito in \\cite{rS1980} and \\cite{rS1981} about the application modeling\nof Lie groups in the theory of technical progress. Next, we will describe the\nresult on Romanian economy of G. Zaman and Z. Goschin in \\cite{ZG2010}.\nFinally, by using Sato's results and applying the method of G. Zaman and Z.\nGoschin, we give an estimation of the GDP function of Viet Nam for the\n1995-2018 period and give several important observations about the impact of\ntechnical progress on economic growth of Viet Nam.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11118v1"
    },
    {
        "title": "Optimizing the reliability of a bank with Logistic Regression and\n  Particle Swarm Optimization",
        "authors": [
            "Vadlamani Ravi",
            "Vadlamani Madhav"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  It is well-known that disciplines such as mechanical engineering, electrical\nengineering, civil engineering, aerospace engineering, chemical engineering and\nsoftware engineering witnessed successful applications of reliability\nengineering concepts. However, the concept of reliability in its strict sense\nis missing in financial services. Therefore, in order to fill this gap, in a\nfirst-of-its-kind-study, we define the reliability of a bank/firm in terms of\nthe financial ratios connoting the financial health of the bank to withstand\nthe likelihood of insolvency or bankruptcy. For the purpose of estimating the\nreliability of a bank, we invoke a statistical and machine learning algorithm\nnamely, logistic regression (LR). Once, the parameters are estimated in the 1st\nstage, we fix them and treat the financial ratios as decision variables. Thus,\nin the 1st stage, we accomplish the hitherto unknown way of estimating the\nreliability of a bank. Subsequently, in the 2nd stage, in order to maximize the\nreliability of the bank, we formulate an unconstrained optimization problem in\na single-objective environment and solve it using the well-known particle swarm\noptimization (PSO) algorithm. Thus, in essence, these two stages correspond to\npredictive and prescriptive analytics respectively. The proposed 2-stage\nstrategy of using them in tandem is beneficial to the decision-makers within a\nbank who can try to achieve the optimal or near-optimal values of the financial\nratios in order to maximize the reliability which is tantamount to safeguarding\ntheir bank against solvency or bankruptcy.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11122v1"
    },
    {
        "title": "Trading characteristics of member firms on the Korea Exchange",
        "authors": [
            "Min-Young Lee",
            "Woo-Sung Jung",
            "Gabjin Oh"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this paper, we study the characteristics of the member firms on the Korea\nExchange. The member firms intermediate between the market participants and the\nexchange, and all the participants should trade stocks through members. To\nidentify the characteristics of member firms, all member firms are categorized\ninto three groups, such as the domestic members similar to individuals (DIMs),\nthe domestic members similar to institutions (DSMs), and the foreign members\n(FRMs), in terms of the type of investor. We examine the dynamics of the member\nfirms. The trading characteristics of members are revealed through the\ndirectionality and trend. While FRMs tend to trade one-way and move with the\nprice change, DIMs are the opposite. In the market, DIMs and DSMs do herd and\nthe herding moves in the opposite direction of the price change. One the other\nhand, FRMs do herd in the direction of the price change. The network analysis\nsupports that the members are clustered into three groups similar to DIMs,\nDSMs, and FRMs. Finally, random matrix theory and a cross-sectional regression\nshow that the inventory variation of members possesses significant information\nabout stock prices and that member herding helps to price the stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11148v1"
    },
    {
        "title": "On the multiplicity of the martingale condition: Spontaneous symmetry\n  breaking in Quantum Finance",
        "authors": [
            "Ivan Arraut",
            "Alan Au",
            "Alan Ching-biu Tse"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We demonstrate that the martingale condition in the stock market can be\ninterpreted as a vacuum condition when we express the financial equations in\nthe Hamiltonian form. We then show that the symmetry under the changes of the\nprices is spontaneously broken in general and the symmetry under changes in the\nvolatility, for the case of the Merton-Garman (MG) equation, is also\nspontaneously broken. This reproduces a vacuum degeneracy for the system. In\nthis way, we find the conditions under which, the martingale condition can be\nconsidered to be a non-degenerate vacuum. This gives us a surprising connection\nbetween spontaneous symmetry breaking and the flow of information through the\nboundaries for the financial systems. Subsequently, we find an extended\nmartingale condition for the MG equation, depending not only prices but also on\nthe volatility and finally, we show what happens if we include additional\nnon-derivative terms on the Black Scholes and on the MG equations, breaking\nthen some other symmetries of the system spontaneously.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11270v1"
    },
    {
        "title": "Estimating the Demand Factors and Willingness to Pay for Agricultural\n  Insurance",
        "authors": [
            "Osman Gulseven"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This article investigates the effect of prices and socio-demographic\nvariables on the farmers decision to purchase agricultural insurance. A survey\nhas been conducted to 200 farmers most of whom are engaged in diversified\nincome-generating activities. The logistic estimation results suggest that\neducation and household income from farming activities positively affect the\nlikelihood of purchasing insurance. The demand for insurance is negatively\ncorrelated with the premium paid per insured value, suggesting that insurance\nis a normal good. Farmers are willing to pay (WTP) increasingly higher premiums\nfor contracts with a higher coverage ratio. According to the valuation model,\nthe WTP declines sharply for coverage ratios under 70%.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11279v1"
    },
    {
        "title": "Econophysics Approach and Model on Mixed Economy",
        "authors": [
            "Ion Spanulescu",
            "Anca Gheorghiu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this paper the general principles and categories of mixed economy that\ncurrently exist in almost all countries of the world are presented. The paper\nalso presents an Advanced Model of Mixed Economy with Threshold (AMMET), which\nis characterized by a reduced value (approx. 10-15%) of the State and public\nsector participation in the national economy and proposes and analyzes an\neconophysics model for the mixed economy.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.13008v1"
    },
    {
        "title": "Race and gender income inequality in the USA: black women vs. white men",
        "authors": [
            "Ivan Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Income inequality between different races in the U.S. is especially large.\nThis difference is even larger when gender is involved. In a complementary\nstudy, we have developed a dynamic microeconomic model accurately describing\nthe evolution of male and female incomes since 1930. Here, we extend our\nanalysis and model the disparity between black and white population in the\nU.S., separately for males and females. Unfortunately, income microdata\nprovided by the U.S. Census Bureau for other races and ethnic groups are not\ntime compatible or too short for modelling purposes. We are forced to constrain\nour analysis to black and white population, but all principal results can be\nextrapolated to other races and ethnicities. Our analysis shows that black\nfemales and white males are two poles of the overall income inequality. The\nprediction of income distribution for two extreme cases with one model is the\nmain challenge of this study.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.06530v1"
    },
    {
        "title": "How does stock market reflect the change in economic demand? A study on\n  the industry-specific volatility spillover networks of China's stock market\n  during the outbreak of COVID-19",
        "authors": [
            "Fu Qiao",
            "Yan Yan"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Using the carefully selected industry classification standard, we divide 102\nindustry securities indices in China's stock market into four demand-oriented\nsector groups and identify demand-oriented industry-specific volatility\nspillover networks. The \"deman-oriented\" is a new idea of reconstructing the\nstructure of the networks considering the relationship between industry sectors\nand the economic demand their outputs meeting. Networks with the new structure\nhelp us improve the understanding of the economic demand change, especially\nwhen the macroeconomic is dramatically influenced by exogenous shocks like the\noutbreak of COVID-19. At the beginning of the outbreak of COVID-19, in China's\nstock market, spillover effects from industry indices of sectors meeting the\ninvestment demand to those meeting the consumption demands rose significantly.\nHowever, these spillover effects fell after the outbreak containment in China\nappeared to be effective. Besides, some services sectors including utility,\ntransportation and information services have played increasingly important\nroles in the networks of industry-specific volatility spillovers as of the\nCOVID-19 out broke. By implication, firstly, being led by Chinese government,\nthe COVID-19 is successfully contained and the work resumption is organized\nwith a high efficiency in China. The risk of the investment demand therefore\nwas controlled and eliminated relatively fast. Secondly, the intensive using of\nnon-pharmaceutical interventions (NPIs) led to supply restriction in services\nin China. It will still be a potential threat for the Chinese economic recovery\nin the next stage.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.07487v1"
    },
    {
        "title": "Proposal for a Comprehensive (Crypto) Asset Taxonomy",
        "authors": [
            "Thomas Ankenbrand",
            "Denis Bieri",
            "Roland Cortivo",
            "Johannes Hoehener",
            "Thomas Hardjono"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Developments in the distributed ledger technology have led to new types of\nassets with a broad range of purposes. Although some classification frameworks\nfor common instruments from traditional finance and some for these new, so\ncalled cryptographic assets already exist and are used, a holistic approach to\nintegrate both worlds is missing. The present paper fills this research gap by\nidentifying 14 attributes, each of which is assigned different characteristics,\nthat can be used to classify all types of assets in a structured manner. Our\nproposed taxonomy which is an extension of existing classification frameworks,\nsummarises these findings in a morphological box and is tested for\npracticability by classifying exemplary assets like cash and bitcoin. The final\nclassification framework can help to ensure that the various stakeholders, such\nas investors or supervisors, have a consistent view of the different types of\nassets, and in particular of their characteristics, and also helps to establish\nstandardised terminology.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11877v1"
    },
    {
        "title": "Use Cases of Quantum Optimization for Finance",
        "authors": [
            "Samuel Mugel",
            "Enrique Lizaso",
            "Roman Orus"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this paper we briefly review two recent use-cases of quantum optimization\nalgorithms applied to hard problems in finance and economy. Specifically, we\ndiscuss the prediction of financial crashes as well as dynamic portfolio\noptimization. We comment on the different types of quantum strategies to carry\non these optimizations, such as those based on quantum annealers, universal\ngate-based quantum processors, and quantum-inspired Tensor Networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.01312v1"
    },
    {
        "title": "Thermodynamics of markets",
        "authors": [
            "Sergey Rashkovskiy"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We consider the thermodynamic approach to the description of economic systems\nand processes. The first and second laws of thermodynamics as applied to\neconomic systems are derived and analyzed. It is shown that there is a deep\nanalogy between the parameters of thermodynamic and economic systems (markets);\nin particular, each thermodynamic parameter can be associated with a certain\neconomic parameter or indicator. The economic meaning of such primordially\nthermodynamic concepts as pressure, volume, internal energy, heat, etc. has\nbeen established. The thermostatistics of the market is considered. It is shown\nthat, as in conventional thermostatistics, many market parameters, such as\nprice of goods, quantity of goods, etc., as well as their fluctuations can be\ncalculated formally using the partition function of an economic system.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.10260v1"
    },
    {
        "title": "Determinants of Lending to Small and Medium Enterprises by Commercial\n  Banks in Kenya",
        "authors": [
            "David Haritone Shikumo",
            "Mwangi Mirie"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Small and Medium Enterprises (SMEs) access to external finance is an issue of\nsignificant research interest to academicians. Commercial banks consider many\nSMEs not to be credit worthy because of their inability to meet some banking\nrequirements. Hence, the objective of this study was to investigate what\ndetermines lending to SMEs by commercial banks in Kenya. To achieve the study\nobjectives, a descriptive research design was employed. The study undertook a\ncensus of the 43 commercial banks in Kenya, with full data being obtained for\n36 institutions. The study used secondary data from the annual published\nreports of commercial banks in Kenya for a period of 5 years from 2010-2014.\nThe data collected was analyzed through the multiple linear regression using\nthe Statistical Package for Social Studies version 20.The study established\nthat bank size and liquidity significantly influences (positively and\nnegatively, respectively) lending to SMEs by commercial banks in Kenya while\ncredit risk and interest rates have no significant influence on lending to SMEs\nby commercial banks in Kenya. The study recommends that lending to SMEs by\ncommercial banks in Kenya be enhanced by adopting policies that grow the\ncommercial banks.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12550v1"
    },
    {
        "title": "Determinants of Financial Performance of Microfinance Banks in Kenya",
        "authors": [
            "King'ori S. Ngumo",
            "Kioko W. Collins",
            "Shikumo H. David"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Microfinance provides strength to boost the economic activities of low-income\nearners and thus contributes to eradication of poverty. However, microfinance\ninstitutions face stringent competition from commercial banks; the growth of\nmicroloan activities of commercial banks may confront microfinance institutions\nwith increased competition for borrowers. In Kenya, the micro finance sector\nhas extremely high competition indicated by the shifting market share and\nprofitability. This study sought to examine the determinants of financial\nperformance of Microfinance banks in Kenya. The study adopted a descriptive\nresearch design and used secondary data from 7 Microfinance banks for a period\nof 5 years from 2011 to 2015. The data collected was analyzed using correlation\nand regression analysis. The study found a positive and statistically\nsignificant relationship between operational efficiency, capital adequacy, firm\nsize and financial performance of microfinance banks in Kenya. However, the\nstudy found an insignificant negative relationship between liquidity risk,\ncredit risk and financial performance of microfinance banks in Kenya. The study\nconcluded that there is direct relationship between operational efficiency,\ncapital adequacy, firm size and financial performance of microfinance banks in\nKenya.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12569v1"
    },
    {
        "title": "Effect of Long-Term Debt on the Financial Growth of Non-Financial Firms\n  Listed at the Nairobi Securities Exchange",
        "authors": [
            "David Haritone Shikumo",
            "Oluoch Oluoch",
            "Joshua Matanda Wepukhulu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  A significant number of the non-financial firms listed at Nairobi Securities\nExchange (NSE) have been experiencing declining financial performance which\ndeter investors from investing in such firms. The lenders are also not willing\nto lend to such firms. As such, the firms struggle to raise funds for their\noperations. Prudent financing decisions can lead to financial growth of the\nfirm. The purpose of this study is to assess the effect of Long-term debt on\nthe financial growth of Non-financial firms listed at Nairobi Securities\nExchange. Financial firms were excluded because of their specific sector\ncharacteristics and stringent regulatory framework. The study is guided by\nTrade-Off Theory and Theory of Growth of the Firm. Explanatory research design\nwas adopted. The population of the study comprised of 45 non-financial firms\nlisted at the NSE for a period of ten years from 2008 to 2017. The study\nconducted both descriptive statistics analysis and panel data analysis. The\nresult indicates that Long term debt explains 21.6% and 5.16% of variation in\nfinancial growth as measured by growth in earnings per share and growth in\nmarket capitalization respectively. Long term debt positively and significantly\ninfluences financial growth measured using both growth in earnings per share\nand growth in market capitalization. The study recommends that, the management\nof non-financial firms listed at Nairobi Securities Exchange to employ\nfinancing means that can improve the earnings per share, market capitalization\nand enhance the value of the firm for the benefit of its stakeholders.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12596v3"
    },
    {
        "title": "Effect of Short-Term Debt on Financial Growth of Non-Financial Firms\n  Listed at Nairobi Securities Exchange",
        "authors": [
            "David Haritone Shikumo",
            "Oluoch Oluoch",
            "Joshua Matanda Wepukhulu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  A significant number of the non-financial firms listed at Nairobi Securities\nExchange (NSE) have been experiencing declining financial performance which\ndeter investors from investing in such firms. The lenders are also not willing\nto lend to such firms. As such, the firms struggle to raise funds for their\noperations. Prudent financing decisions can lead to financial growth of the\nfirm. The purpose of this study is to assess the effect of short-term debt on\nfinancial growth of non-financial firms listed at Nairobi Securities Exchange\nfor a period of ten years from 2008 to 2017. Financial firms were excluded\nbecause of their specific sector characteristics and stringent regulatory\nframework. The study is guided by Agency Theory and Theory of Growth of the\nFirm. Explanatory research design was adopted. The target population of the\nstudy comprised of 45 non-financial firms listed at the NSE for a period of ten\nyears from 2008 to 2017. The study conducted both descriptive statistics\nanalysis and panel data analysis. The result indicates that, short term debt\nexplains 45.99% and 25.6% of variations in financial growth as measured by\ngrowth in earnings per share and growth in market capitalization respectively.\nShort term debt positively and significantly influences financial growth\nmeasured using both growth in earnings per share and growth in market\ncapitalization. The study recommends that, the management of non-financial\nfirms listed at Nairobi Securities Exchange to employ financing means that can\nimprove the earnings per share, market capitalization and enhance the value of\nthe firm for the benefit of its stakeholders.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.03339v1"
    },
    {
        "title": "Spontaneous symmetry breaking in Quantum Finance",
        "authors": [
            "Ivan Arraut",
            "Alan Au",
            "Alan Ching-biu Tse"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We analyze the phenomena of spontaneous symmetry breaking in Quantum Finance\nby using as a starting point the Black-Scholes (BS) and the Merton-Garman (MG)\nequations expressed in the Hamiltonian form. In this scenario the martingale\ncondition (state) corresponds to the vacuum state which becomes degenerate when\nthe symmetry of the system is spontaneously broken. We then analyze the broken\nsymmetries of the system and we interpret from the perspective of Financial\nmarkets the possible appearance of the Nambu-Goldstone bosons.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.05278v1"
    },
    {
        "title": "Solution to the Equity Premium Puzzle",
        "authors": [
            "Atilla Aras"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This study provides a solution of the equity premium puzzle. Questioning the\nvalidity of the Arrow-Pratt measure of relative risk aversion for detecting the\nrisk behavior of investors under all conditions, a new tool, that is, the\nsufficiency factor of the model was developed to analyze the risk behavior of\ninvestors. The calculations of this newly tested model show that the value of\nthe coefficient of relative risk aversion is 1.033526 by assuming the value of\nthe subjective time discount factor as 0.99. Since these values are compatible\nwith the existing empirical studies, they confirm the validity of the newly\nderived model that provides a solution to the equity premium puzzle.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.05458v5"
    },
    {
        "title": "Shadow economy and populism-risk and uncertainty factors for\n  establishing low-carbon economy of Balkan countries-case study for Bulgaria",
        "authors": [
            "Shteryo Nozharov",
            "Nina Nikolova"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The main purpose of the current publication is to formulate a scenario model\nfor analysis of the opportunities for low-carbon economy establishment in the\ncountries with transition economies.The model studies risk factors such as\nshadow economy level and populism based on the implementation and development\nof Balkan countries economic policy and at the same time shows future climate\nchanges tendencies and uncertainties of climate models.A transdisciplinary\napproach is implemented in the study. Climate change perception and\nunderstanding about low-carbon economy are examined through the public opinion\nand analysis of mass-media publications.The results of the research are\nimportant in order to clarify the multicultural divergences as a factor for\nrisk and uncertainty in the implementation process of the policy for climate\nchange.In this way geographical aspects of risk and uncertainty, which are not\nonly related to the economic development of the relevant countries, could be\nbrought out.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.06592v1"
    },
    {
        "title": "Pricing the Information Quantity in Artworks",
        "authors": [
            "Lan Ju",
            "Zhiyong Tu",
            "Changyong Xue"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In the traditional art pricing models, the variables that capture the\npainting's content are often missing. Recent research starts to apply the\ncomputer graphic techniques to extract the information from the painting\ncontent. Most of the research concentrates on the reading of the color\ninformation from the painting images and analyzes how different color\ncompositions can affect the sales prices of paintings. This paper takes a\ndifferent approach, and tries to abstract away from the interpretation of the\ncontent information, while only focus on measuring the quantity of information\ncontained. We extend the concept of Shannon entropy in information theory to\nthe painting's scenario, and suggest using the variances of a painting's\ncomposing elements, i.e., line, color, value, shape/form and space, to measure\nthe amount of information in the painting. These measures are calculated at the\npixel level based on a picture's digital image. We include them into the\ntraditional hedonic regression model to test their significance based on the\nauction samples from two famous artists (Picasso and Renoir). We find that all\nthe variance measurements can significantly explain the sales price either at\n1% or 5% level. The adjusted R square is also increased by more than ten\npercent. Our method greatly improves the traditional pricing models, and may\nalso find applications in other areas such as art valuation and authentication.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.09129v1"
    },
    {
        "title": "Comparison of the effects of investor attention using search volume data\n  before and after mobile device popularization",
        "authors": [
            "Jonghyeon Min"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  In this study, we will study investor attention measurement using the Search\nVolume Index in the recent market. Since 2009, the popularity of mobile devices\nand the spread of the Internet have made the speed of information delivery\nfaster and the investment information retrieval data for obtaining investment\ninformation has increased dramatically. In these circumstances, investor\nattention measurement using search volume data can be measured more accurately\nand faster than before mobile device popularization. To confirm this, we will\ncompare the effect of measuring investor attention using search volume data\nbefore and after mobile device popularization. In addition, it is confirmed\nthat the measured investor attention is that of retail traders, not\ninstitutional traders or professional traders, and the relationship between\ninvestor attention and short-term price pressure theory. Using SVI data\nprovided by Google Trends, we will experiment with Russell 3000 stocks and IPO\nstocks and compare the results. In addition, the results of investigating the\ninvestor's interest using the search volume data from various angles through\nexperiments such as the comparison of the results based on the inclusion of the\nnoise ticker group, the comparison of the limitations of the existing investor\nattention measurement method, and the comparison of explanatory variables with\nexisting IPO related studies. We would like to verify its practicality and\nsignificance.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03239v1"
    },
    {
        "title": "Challenging Practical Features of Bitcoin by the Main Altcoins",
        "authors": [
            "Andrew Spurr",
            "Marcel Ausloos"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We study the fundamental differences that separate: Litecoin; Bitcoin Gold;\nBitcoin Cash; Ethereum; and Zcash from Bitcoin, and draw analysis to how these\nfeatures are appreciated by the market, to ultimately make an inference as to\nhow future successful cryptocurrencies may behave. We use Google Trend data, as\nwell as price, volume and market capitalization data sourced from\ncoinmarketcap.com to support this analysis. We find that Litecoin's shorter\nblock times offer benefits in commerce, but drawbacks in the mining process\nthrough orphaned blocks. Zcash holds a niche use for anonymous transactions,\nbenefitting areas of the world lacking in economic freedom. Bitcoin Cash\nsuffers from centralization in the mining process, while the greater\ndecentralization of Bitcoin Gold has generally left it to stagnate. Ether's\ngreater functionality offers the greatest threat to Bitcoin's dominance in the\nmarket. A coin that incorporates several of these features can be technically\nbetter than Bitcoin, but the first-to-marketadvantage of Bitcoin should keep\nits dominant position in the market.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03891v1"
    },
    {
        "title": "Four Ways to Scale Up: Smart, Dumb, Forced, and Fumbled",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Scale-up is the process of growing a venture in size. The paper identifies\nmodularity and speed as keys to successful scale-up. On that basis four types\nof scale-up are identified: Smart, dumb, forced, and fumbled. Smart scale-up\ncombines modularity and speed. Dumb scale-up is bespoke and slow, and very\ncommon. The paper presents examples of each type of scale-up, explaining why\nthey were successful or not. Whether you are a small startup or Elon Musk\ntrying to grow Tesla and SpaceX or Jeff Bezos scaling up Amazon - or you are\nthe US, UK, Chinese, or other government trying to increase power production,\nexpand your infrastructure, or make your health, education, and social services\nwork better - modularity and speed are the answer to effective delivery, or so\nthe paper argues. How well you deal with modularity and speed decides whether\nyour efforts succeed or fail. Most ventures, existing or planned, are neither\nfully smart nor fully dumb, but have elements of both. Successful organizations\nwork to tip the balance towards smart by (a) introducing elements of smart\nscale-up into existing ventures and (b) starting new, fully smart-scaled\nventures, to make themselves less dumb and ever smarter.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.11104v1"
    },
    {
        "title": "An Empirical Assessment of Characteristics and Optimal Portfolios",
        "authors": [
            "Christopher G. Lamoureux",
            "Huacheng Zhang"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We analyze characteristics' joint predictive information through the lens of\nout-of-sample power utility functions. Linking weights to characteristics to\nform optimal portfolios suffers from estimation error which we mitigate by\nmaximizing an in-sample loss function that is more concave than the utility\nfunction. While no single characteristic can be used to enhance utility by all\ninvestors, conditioning on momentum, size, and residual volatility produces\nportfolios with significantly higher certainty equivalents than benchmarks for\nall investors. Characteristic complementarities produce the benefits, for\nexample momentum mitigates overfitting inherent in other characteristics.\nOptimal portfolios' returns lie largely outside the span of traditional\nfactors.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.12975v4"
    },
    {
        "title": "Where are the opportunities for growth in the professional services\n  space?",
        "authors": [
            "Edouard Ribes"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  The professional services industry (legal, accounting, consulting,\narchitectural services{\\ldots}) employs an important share of the active\npopulation in mature countries. However, after decades of undisputed growth,\nthe sector appears to be at a turning point in certain geographies. This\narticle therefore proposes a simple framework to help diagnose where growth\nopportunities (if any) may lie.When applied to the US economic context, the\nmodel indicates that at a macro-economic national level the sector should\nstall, which concurs with the trend observed over the past decade. However, it\nalso highlights that a few industrial sectors (e.g. the US beverage industry)\nstill offer pockets of growth for a variety of professional expertises.\nReplicating and fine-tuning those findings could be interesting for\npractitioners to steer their marketing and business development efforts. On the\nother hand, the quantitative framework presented in this study could pave the\nway for future research in the academic community.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02781v1"
    },
    {
        "title": "Can an Agency Role-Reversal Lead to an Organizational Collapse?; A Study\n  Proposal",
        "authors": [
            "Yossi Haimberg"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  The Principal-Agent Theory model is widely used to explain governance role\nwhere there is a separation of ownership and control, as it defines clear\nboundaries between governance and executives. However, examination of recent\ncorporate failure reveals the concerning contribution of the Board of Directors\nto such failures and calls into question governance effectiveness in the\npresence of a powerful and charismatic CEO. This study proposes a framework for\nanalyzing the relationship between the Board of Directors and the CEO, and how\ncertain relationships affect the power structure and behavior of the Board,\nwhich leads to a role reversal in the Principal-Agent Theory, as the Board\nassumes the role of the CEO's agent. This study's results may help create a red\nflag for a board and leader's behavior that may result in governance failure.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.04667v1"
    },
    {
        "title": "Winterization of Texan power system infrastructure is profitable but\n  risky",
        "authors": [
            "Katharina Gruber",
            "Tobias Gauster",
            "Peter Regner",
            "Gregor Laaha",
            "Johannes Schmidt"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We deliver the first analysis of the 2021 cold spell in Texas which combines\ntemperature dependent load estimates with temperature dependent estimates of\npower plant outages to understand the frequency of loss of load events, using a\n71 year long time series of climate data. The expected revenue from full\nwinterization is 11.74bn$ over a 30 years investment period. We find that\nlarge-scale winterization, in particular of gas infrastructure and gas power\nplants, would be profitable, as related costs for winterization are\nsubstantially lower. At the same moment, the associated investment risks are\nhigh due to the low-frequency of events - the 2021 event was the largest and we\nobserve only 8 other similar ones for the simulated 71 years. As risks to\ninvestors are considerable, regulatory measures may be necessary to enforce\nwinterization.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.05148v1"
    },
    {
        "title": "The Problems of Personal Income Tax on Revenue Generation in Gombe State",
        "authors": [
            "Abubakar Bala",
            "Esther Yusuf Enoch",
            "Salisu Yakubu"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This study examined the problems of personal income tax on revenue generation\nin Gombe state. The methodology used in data collection is survey, which\nutilized both primary and secondary types of data. Purposive sampling technique\nwas adopted in selecting a sample of 150 respondents from both employees of\nstate board of internal revenue service and tax payers in the state. The chi\nsquare statistics test was used in testing the hypotheses. The study found that\ntax avoidance/evasion and complete absences of information technology are\nserious problems affecting revenue generation in the state. It recommends that\ngovernment should device strict measures in dealing and punishing individuals\nengage in tax avoidance and evasion. It should also employ the use of\ninformation technology as it is the only way problems experience in personal\nincome tax collection can be reduced drastically.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.10995v1"
    },
    {
        "title": "Empirical Analysis of Service Quality, Reliability and End-User\n  Satisfaction on Electronic Banking in Nigeria",
        "authors": [
            "Esther Enoch Yusuf",
            "Abubakar Bala"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Today, almost all banks have adopted ICT as a means of enhancing their\nbanking service quality. These banks provide ICT based electronic service which\nis also called electronic banking, internet banking or online banking etc to\ntheir customers. Despite the increasing adoption of electronic banking and it\nrelevance towards end users satisfaction, few investigations has been conducted\non factors that enhanced end users satisfaction perception. In this research,\nan empirical analysis has been conducted on factors that influence electronic\nbanking user's satisfaction perception and the relationship between these\nfactors and the customer's satisfaction. The study will help bank industries in\nimproving the level of their customer's satisfaction and increase the bond\nbetween a bank and its customer.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.11184v1"
    },
    {
        "title": "How does economic policy uncertainty comove with stock markets: New\n  evidence from symmetric thermal optimal path method",
        "authors": [
            "Ying-Hui Shao",
            "Yan-Hong Yang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We revisit the dynamic relationship between domestic economic policy\nuncertainty and stock markets using the symmetric thermal optimal path (TOPS)\nmethod. We observe different interaction patterns in emerging and developed\nmarkets. Economic policy uncertainty drives the stock market in China, while\nstock markets play a leading role in the UK and the US. Meanwhile, the lead-lag\nrelationship of the three countries reacts significantly to extreme events. Our\nfindings have important implications for investors and policy makers.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.04421v2"
    },
    {
        "title": "The Effect of Client Appraisal on the Efficiency of Micro Finance Bank",
        "authors": [
            "Esther Yusuf Enoch",
            "Usman Abubakar Arabo",
            "Abubakar Mahmud Digil"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  One of the major problems confronting financial institutions most especially\nmicrofinance institutions is the increasing incidence of loan defaults and\nconsequence loan losses which manifested in their financial performance with\nhuge uncollectible loans and advances. This study assessed the effects of\ncredit management on financial performance on microfinance institutions in\nAdamawa State, Nigeria. Specifically, we examine the effect of client appraisal\non the efficiency of microfinance banks in Adamawa State. The methodology\nemployed in this study is the survey method in which both primary and secondary\nsources were used in the collection of data. A multi-stage sampling method was\nadopted in selecting a sample of 21 respondents from a total population of 52\ncredit officers. Questionnaires were used in the due collection of data from\nthe respondents. Descriptive statistics (simple percentage) and inferential\nstatistics (regression analysis) were used to analyze the data collected and in\ntesting the hypotheses. The study showed that client appraisal has a positive\neffect on efficiency and productivity.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.07679v1"
    },
    {
        "title": "XRP Network and Proposal of Flow Index",
        "authors": [
            "Hideaki Aoyama"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  XRP is a modern crypto-asset (crypto-currency) developed by Ripple Labs,\nwhich has been increasing its financial presence. We study its transaction\nhistory available as ledger data. An analysis of its basic statistics,\ncorrelations, and network properties are presented. Motivated by the behavior\nof some nodes with histories of large transactions, we propose a new index: the\n``Flow Index.'' The Flow Index is a pair of indices suitable for characterizing\ntransaction frequencies as a source and destination of a node. Using this Flow\nIndex, we study the global structure of the XRP network and construct\nbow-tie/walnut structure.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10012v1"
    },
    {
        "title": "Robust Decisions for Heterogeneous Agents via Certainty Equivalents",
        "authors": [
            "Anne G. Balter",
            "Nikolaus Schweizer"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We study the problem of a planner who resolves risk-return trade-offs - like\nfinancial investment decisions - on behalf of a collective of agents with\nheterogeneous risk preferences. The planner's objective is a two-stage utility\nfunctional where an outer utility function is applied to the distribution of\nthe agents' certainty equivalents from a given decision. Assuming lognormal\nrisks and heterogeneous power utility preferences for the agents, we\ncharacterize optimal behavior in a setting where the planner can let each agent\nchoose between different options from a fixed menu of possible decisions,\nleading to a grouping of the agents by risk preferences. These optimal decision\nmenus are derived first for the case where the planner knows the distribution\nof preferences exactly and then for a case where he faces uncertainty about\nthis distribution, only having access to upper and lower bounds on agents'\nrelative risk aversion. Finally, we provide tight bounds on the welfare loss\nfrom offering a finite menu of choices rather than fully personalized\ndecisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.13059v1"
    },
    {
        "title": "Investment AUM Fee Costs: Evaluating a Simple Formula",
        "authors": [
            "Joseph Levine"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  How much do financial management fees cost investors? This article studies an\napproximate formula for the cumulative costs of annual Assets Under Management\n(AUM) fees. The formula states that an investment paying an annual fee of\n$\\epsilon$% of AUM over $N$ years loses almost $N\\epsilon$% of its value,\ncompared to an investment with the same returns paying no fee. The article\nexplains this formula intuitively, derives it analytically, and studies its\napproximation error. The article concludes with a discussion of the formula's\nuses and limitations.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.00837v3"
    },
    {
        "title": "A decision support tool for ship biofouling management in the Baltic Sea",
        "authors": [
            "Emilia Luoma",
            "Mirka Laurila-Pant",
            "Elias Altarriba",
            "Inari Helle",
            "Lena Granhag",
            "Maiju Lehtiniemi",
            "Greta Srėbalienė",
            "Sergej Olenin",
            "Annukka Lehikoinen"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Biofouling of ships causes major environmental and economic consequences all\nover the world. In addition, biofouling management of ship hulls causes both\nsocial, environmental and economic risks that should all be considered reaching\nwell-balanced decisions. In addition, each case is unique and thus optimal\nmanagement strategy must be considered case-specifically. We produced a novel\ndecision support tool using Bayesian networks to promote the comprehensive\nunderstanding about the complex biofouling management issue in the Baltic Sea\nand to identify potential management options and their consequences. The tool\ncompares the biofouling management strategies in relation to NIS\n(non-indigenous species) introduction risk, eco-toxicological risk due to\nbiocidal coating, carbon dioxide emissions resulting from fuel consumption and\ncosts related to fuel consumption, in-water cleaning and coating. According to\nthe results, the optimal biofouling management strategy would consist of a\nbiocidal-free coating with regular in-water cleaning and with devices\ncollecting the material. However, the best biocidal-free coating type and the\noptimal in-water cleaning interval varies and depends e.g. on the operational\nprofile of the ship. The decision support tool can increase the\nmulti-perspective understanding about the issue and support the implementation\nof the optimal biofouling management strategies in the Baltic Sea.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06810v1"
    },
    {
        "title": "Moore's law, Wright's law and the Countdown to Exponential Space",
        "authors": [
            "Daniel Berleant",
            "Venkat Kodali",
            "Richard Segall",
            "Hyacinthe Aboudja",
            "Michael Howell"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Technologies have often been observed to improve exponentially over time. In\npractice this often means identifying a constant known as the doubling time,\ndescribing the time period over which the technology roughly doubles in some\nmeasure of performance or of performance per dollar. Moore's law is,\nclassically, the empirical observation that the number of electronic components\nthat can be put on a chip doubles every 18 months to 2 years. Today it is\nfrequently stated as the number of computations available per unit of cost.\nGeneralized to the appropriate doubling time, it describes the rate of\nadvancement in many technologies. A frequently noted competitor to Moore's law\nis known as Wright's law, which has aeronautical roots. Wright's law (also\ncalled power law, experience curve and Henderson's law) relates some quality of\na manufactured unit (for Wright, airplanes) to the volume of units\nmanufactured. The Wright's law equation expresses the idea that performance -\nprice or a quality metric - improves according to a power of the number\nproduced, or alternatively stated, improves by a constant percentage for every\ndoubling of the total number produced.\n  Does exploration of outer space conform to Moore's law or Wright's law-like\nbehavior? Our results below are broadly consistent with these laws. This is\ntrue for many technologies. Although the two laws can make somewhat different\npredictions, Sahal found that they converge to the same predictions when\nmanufacturing volume increases exponentially over time. When space exploration\ntransitions into an independent commercial sector, as many people hope and\nexpect, spacecraft technology will then likely enter an era of unambiguously\nexponential advancement.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.09637v1"
    },
    {
        "title": "Forecasting performance of workforce reskilling programmes",
        "authors": [
            "Evan Hurwitz",
            "George Cevora"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Estimating success rates for programmes aiming to reintegrate theunemployed\ninto the workforce is essential for good stewardship of publicfinances. At the\ncurrent moment, the methods used for this task arebased on the historical\nperformance of comparable programmes. In lightof Brexit and Covid-19\nsimultaneously causing a shock to the labourmarket in the UK we developed an\nestimation method that is basedon fundamental factors involved - workforce\ndemand and supply - asopposed to the historical values which are quickly\nbecoming irrelevant.With an average error of 3.9% of the re-integration success\nrate, ourmodel outperforms the best benchmark known to us by 53%\n",
        "pdf_link": "http://arxiv.org/pdf/2107.10001v1"
    },
    {
        "title": "Mortality in Germany during the Covid-19 pandemic",
        "authors": [
            "Alois Pichler",
            "Dana Uhlig"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  The Covid-19 pandemic still causes severe impacts on society and the economy.\nThis paper studies excess mortality during the pandemic years 2020 and 2021 in\nGermany empirically with a special focus on the life insurer's perspective. Our\nconclusions are based on official counts of German governmental offices on the\nliving and deaths of the entire population. Conclusions, relevant for actuaries\nand specific insurance business lines, including portfolios of pension, life,\nand health insurance contracts, are provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.12899v2"
    },
    {
        "title": "Financial intermediation and risk in decentralized lending protocols",
        "authors": [
            "Carlos Castro-Iragorri",
            "Julian Ramirez",
            "Sebastian Velez"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We provide an overview of decentralized protocols like Compound and Aave that\noffer collateralized loans for cryptoasset investors. Compound and Aave are two\nof the most important application in the decentralized finance (DeFi)\necosystem. Using publicly available information on rates, supply and borrow\nactivity, and accounts we analyze different elements of the protocols. In\nparticular, we estimate ex-post margins that give a comprehensive account of\nthe cost of financial intermediation. We find that ex-post margins considering\nall markets are 1% and lower for stablecoin markets. In addition, we estimate\nquarterly indicators regarding solvency, asset quality, earnings and market\nrisk similar to the ones used in traditional banking. This provides a first\nlook at the use of these metrics and a comparison between the similarities and\nchallenges to our understanding of financial intermediation in these protocols\nbased on tools used for traditional banking.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14678v1"
    },
    {
        "title": "Features of international taxation and its impact on business entities\n  of Georgia",
        "authors": [
            "George Abuselidze",
            "Mariam Msakhuradze"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  The work \"International Taxation and its impact on Georgian Business\nSubjects\" discusses the essence, types of international taxation and ways to\nprevent it. Object of international taxation, taxable base and rates, features\nbased on the taxpayer. The approaches of states and its impact on the\nactivities of business entities. The aim of the work was to study the\ntheoretical and methodological bases of international taxation in the tax\nsystem of Georgia and to present the existing problems. To get acquainted with\nthe activities of the free industrial zones in our country and to evaluate\nthem. Sharing opinions and expressing one's attitude towards it. The work\npresents the opinion on the impact of the approaches and recommendations of our\ncountry's legislation on international taxation on the business sector of\nGeorgia to correct the current situation.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.02956v1"
    },
    {
        "title": "Specifics of formation tax revenues and ways to improve it in Georgia",
        "authors": [
            "George Abuselidze",
            "Rusudan Zoidze"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  In the research there is reviewed the peculiarities of the formation of tax\nrevenues of the state budget, analysis of the recent past and present periods\nof tax system in Georgia, there is reviewed the influence of existing factors\non the revenues, as well as the role and the place of direct and indirect taxes\nin the state budget revenues. In addition, the measures of stimulating action\non formation of tax revenues and their impact on the state budget revenues are\nestablished. At the final stage, there are examples of foreign developed\ncountries, where the tax system is perfectly developed, where various\nstimulating measures are successfully stimulating and consequently it promotes\nmobilization of the amount of money required in the state budget. The exchange\nof foreign experience is very important for Georgia, the existing tax model\nthat is based on foreign experience is greatly successful. For the formation of\ntax policy, it is necessary to take into consideration all the factors\naffecting on it, a complex analysis of the tax system and the steps that will\nbe really useful and perspective for our country.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.03027v1"
    },
    {
        "title": "About inevitability of budgetary code receiving for fiscal politics",
        "authors": [
            "George Abuselidze"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Since the end of 90s till today when all the elements confirming Georgian\nState System have practically been established, budget system and policy\nremains as the most difficult Georgian macroeconomics challenge and even still\nhalf-and-half unsolved problem. One side of the fiscal policy is quite\ncrucially formulated and administrative Tax Code, and the other side is the\nweak, unmanaged and incomplete law on Budget System. According to the\nabove-mentioned the elaboration and adoption of the Budget Code having equal\nforce as Tax Code is necessary by which the following are to be determined:\nexcellence of government responsibility when it will not perform the budget\nobligations specified by the law permanently; the rights and responsibilities\nof the state, the optimal distribution of the funds mobilized by the tax\ntowards each member of the society. For optimization of the budget system\neffective correlation between the state, regional and local budgets revenues\nand expenditures is particularly important as the social-economic development\nof the regions and territorial units of the country is impossible without the\nfinancial relations. For it the just differentiation of tax base in the section\nof state, regional and local budgets and transfers system for support of the\nbudgets of the territorial units from the central budget are necessary. Solving\nthe most part of these problems is possible by the adoption of the budget code\nwhich, in our opinion, is to be considered as the closest decisive task for the\ncurrent legislative and executive authority.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.04739v1"
    },
    {
        "title": "Elephants or Goldfish? An Empirical Analysis of Carrier Reciprocity in\n  Dynamic Freight Markets",
        "authors": [
            "Angela Acocella",
            "Chris Caplice",
            "Yossi Sheffi"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Dynamic macroeconomic conditions and non-binding truckload freight contracts\nenable both shippers and carriers to behave opportunistically. We present an\nempirical analysis of carrier reciprocity in the US truckload transportation\nsector to demonstrate whether consistent performance and fair pricing by\nshippers when markets are in their favor result in maintained primary carrier\ntender acceptance when markets turn. The results suggest carriers have short\nmemories; they do not remember shippers' previous period pricing, tendering\nbehavior, or performance when making freight acceptance decisions. However,\ncarriers appear to be myopic and respond to shippers' current market period\nbehaviors, ostensibly without regard to shippers' previous behaviors.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.07348v1"
    },
    {
        "title": "Effect of Share Capital on Financial Growth of Non-Financial Firms\n  Listed at the Nairobi Securities Exchange",
        "authors": [
            "David Haritone Shikumo"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Purpose: A significant number of the non-financial firms listed at the\nNairobi Securities Exchange (NSE) have been experiencing declining financial\nperformance which deters investors from investing in such firms. The lenders\nare also not willing to lend to such firms. As such, the firms struggle to\nraise funds for their operations. Prudent financing decisions can lead to\nfinancial growth of the firm. The purpose of this study is to assess the effect\nof Share capital on financial growth of Non-financial firms listed at the\nNairobi Securities Exchange. Financial firms were excluded because of their\nspecific sector characteristics and stringent regulatory framework. The study\nis guided by Market Timing Theory and Theory of Growth of the Firm.\nMethodology: Explanatory research design was adopted. The target population of\nthe study comprised of 45 non-financial firms listed at NSE for a period of ten\nyears from 2008 to 2017. The study conducted both descriptive statistics\nanalysis and panel data analysis. Findings: The result indicates that, share\ncapital explains 32.73% and 11.62% of variations in financial growth as measure\nby growth in earnings per share and growth in market capitalization\nrespectively. Share capital positively and significantly influences financial\ngrowth as measured by both growth in earnings per share and growth in market\ncapitalization. Implications: The study recommends for the Non-financial firms\nto utilize equity financing as a way of raising capital for major expansions,\nasset growth or acquisitions which may require heavy funding. In this way,\nfirms will be assured of improved performance as well as high financial growth.\nThe study also recommends for substantial firm financing through equity. Value:\nEquity financing is important to any firm, if the proceeds are used to invest\nin projects which eventually bring growth to the firm.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.10244v1"
    },
    {
        "title": "Cooling Measures and Housing Wealth: Evidence from Singapore",
        "authors": [
            "Wolfgang Karl Härdle",
            "Rainer Schulz",
            "Taojun Sie"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Excessive house price growth was at the heart of the financial crisis in\n2007/08. Since then, many countries have added cooling measures to their\nregulatory frameworks. It has been found that these measures can indeed control\nprice growth, but no one has examined whether this has adverse consequences for\nthe housing wealth distribution. We examine this for Singapore, which started\nin 2009 to target price growth over ten rounds in total. We find that welfare\nfrom housing wealth in the last round might not be higher than before 2009.\nThis depends on the deflator used to convert nominal into real prices.\nIrrespective of the deflator, we can reject that welfare increased\nmonotonically over the different rounds.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.11915v1"
    },
    {
        "title": "Future Photovoltaic Electricity Production Targets and The Link to\n  Consumption per Capita on The Policy Level in MENA Region",
        "authors": [
            "Mostafa Abdelrashied",
            "Dikshita Bhattacharya"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This paper provides an overview of the status of the electricity market in\nthe region, indicating the nexus between electricity consumption with\npopulation growth and GDP. It also analyzes the policy portfolio in different\ncountries, indicating some of the in-action policies' effectiveness and\nrecommended alternatives. World Bank datasets were used for the analysis\nbetween 2000 and 2014. We found that the MENA region is at an early stage for\nrenewable energy with a high potential for solar energy, making it attractive\nfor investors. However, the high dependency on oil for consumption and\nexporting might not provide a prosperous environment for renewable technologies\nto grow. Therefore, a greater focus on decoupling economic growth from energy\nconsumption will have a long-lasting impact on fiscal revenues for net-oil\nexporting countries. Moreover, the consequences of the decoupling will allow\nmore renewables penetration in the current energy mix enabling many countries\nto reach their Paris Agreement goals. For short-term energy policy actions,\nstarting a subsidy reform towards the final repeal of subsidies is a must as\nthese measures relate to all end-use sectors and impact fiscal stability in\nmany countries. With its 1.65GW Benban Solar Park in Aswan, Egypt has shown an\nexample of shifting from subsidizing fossil fuel products to commissioning\nrenewable projects to get closer to its Paris Agreement targets.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.02129v1"
    },
    {
        "title": "Net Buying Pressure and the Information in Bitcoin Option Trades",
        "authors": [
            "Carol Alexander",
            "Jun Deng",
            "Jianfen Feng",
            "Huning Wan"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  How do supply and demand from informed traders drive market prices of bitcoin\noptions? Deribit options tick-level data supports the limits-to-arbitrage\nhypothesis about the market maker's supply. The main demand-side effects are\nthat at-the-money option prices are largely driven by volatility traders and\nout-of-the-money options are simultaneously driven by volatility traders and\nthose with proprietary information about the direction of future bitcoin price\nmovements. The demand-side trading results contrast with prior studies on\nestablished options markets in the US and Asia, but we also show that Deribit\nis rapidly evolving into a more efficient channel for aggregating information\nfrom informed traders.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.02776v2"
    },
    {
        "title": "Behavioral Bias Benefits: Beating Benchmarks By Bundling Bouncy Baskets",
        "authors": [
            "Ravi Kashyap"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We consider in detail an investment strategy, titled \"The Bounce Basket\",\ndesigned for someone to express a bullish view on the market by allowing them\nto take long positions on securities that would benefit the most from a rally\nin the markets. We demonstrate the use of quantitative metrics and large\namounts of historical data towards decision making goals. This investment\nconcept combines macroeconomic views with characteristics of individual\nsecurities to beat the market returns. The central idea of this theme is to\nidentity securities from a regional perspective that are heavily shorted and\nyet are fundamentally sound with at least a minimum buy rating from a consensus\nof stock analysts covering the securities. We discuss the components of\ncreating such a strategy including the mechanics of constructing the portfolio.\nUsing simulations, in which securities lending data is modeled as geometric\nbrownian motions, we provide a few flavors of creating a ranking of securities\nto identity the ones that are heavily shorted.\n  An investment strategy of this kind will be ideal in market scenarios when a\ndownturn happens due to unexpected extreme events and the markets are\nanticipated to bounce back thereafter. This situation is especially applicable\nto incidents being observed, and relevant proceedings, during the Coronavirus\npandemic in 2020-2021. This strategy is one particular way to overcome a\npotential behavioral bias related to investing, which we term the \"rebound\neffect\".\n",
        "pdf_link": "http://arxiv.org/pdf/2109.03740v1"
    },
    {
        "title": "Risk-Adjusted Valuation for Real Option Decisions",
        "authors": [
            "Carol Alexander",
            "Xi Chen",
            "Charles Ward"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We model investor heterogeneity using different required returns on an\ninvestment and evaluate the impact on the valuation of an investment. By\nassuming no disagreement on the cash flows, we emphasize how risk preferences\nin particular, but also the costs of capital, influence a subjective evaluation\nof the decision to invest now or retain the option to invest in future. We\npropose a risk-adjusted valuation model to facilitate investors' subjective\ndecision making, in response to the market valuation of an investment\nopportunity. The investor's subjective assessment arises from their perceived\nmisvaluation of the investment by the market, so projected cash flows are\ndiscounted using two different rates representing the investor's and the\nmarket's view. This liberates our model from perfect or imperfect hedging\nassumptions and instead, we are able to illustrate the hedging effect on the\nreal option value when perceptions of risk premia diverge. During crises\nperiods, delaying an investment becomes more valuable as the idiosyncratic risk\nof future cash flows increases, but the decision-maker may rush to invest too\nquickly when the risk level is exceptionally high. Our model verifies features\nestablished by classical real-option valuation models and provides many new\ninsights about the importance of modelling divergences in decision-makers risk\npremia, especially during crisis periods. It also has many practical advantages\nbecause it requires no more parameter inputs than basic discounted cash flow\napproaches, such as the marketed asset disclaimer method, but the outputs are\nmuch richer. They allow for complex interactions between cost and revenue\nuncertainties as well as an easy exploration of the effects of hedgeable and\nun-hedgeable risks on the real option value. Furthermore, we provide\nfully-adjustable Python code in which all parameter values can be chosen by the\nuser.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04793v1"
    },
    {
        "title": "Coulomb-like Model for International Trade Flow and Derivation of\n  Distribution Function for Trade Flow Strength",
        "authors": [
            "Mikrajuddin Abdullah"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  To describe international trade flows, we propose the coulomb force\nformulation, in which the magnitude of the charge represents gross domestic\nproduct (GDP) and the distance between countries is the bilateral distance, the\nproduct of spatial distance and \"dielectric constant,\" rather than the spatial\ndistance as used in the gravitation model, allowing it to be time dependent.\nThe \"dielectric constant\" is influenced by factors such as warfare,\ntransportation disruptions, trade agreements, social, geography, politics,\nculture, and others. The GDP and distance power parameters were estimated using\ndata from high-GDP countries' export-import transactions. We also developed a\ntrade strength distribution equation that fits World Bank data reasonably well\nover a decade.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14554v2"
    },
    {
        "title": "Uncertainty, volatility and the persistence norms of financial time\n  series",
        "authors": [
            "Simon Rudkin",
            "Wanling Qiu",
            "Pawel Dlotko"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Norms of Persistent Homology introduced in topological data analysis are seen\nas indicators of system instability, analogous to the changing predictability\nthat is captured in financial market uncertainty indexes. This paper\ndemonstrates norms from the financial markets are significant in explaining\nfinancial uncertainty, whilst macroeconomic uncertainty is only explainable by\nmarket volatility. Meanwhile, volatility is insignificant in the determination\nof norms when uncertainty enters the regression. Persistence norms therefore\nhave potential as a further tool in asset pricing, and also as a means of\ncapturing signals from financial time series beyond volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.00098v1"
    },
    {
        "title": "Study of The Relationship Between Public and Private Venture Capitalists\n  in France: A Qualitative Approach",
        "authors": [
            "Jonathan Labbe"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This research focuses on the study of relationships between public and\nprivate equity investors in France. In this regard, we need to apprehend the\nformal or informal nature of interactions that can sometimes take place within\ntraditional innovation networks (Djellal \\& Gallouj, 2018). For this, our\narticle mobilizes a public-private partnerships approach (PPPs) and the\nresource-based view theory. These perspectives emphasize the complementary role\nof disciplinary and incentive mechanisms as well as the exchange of specific\nresources as levers for value creation. Moreover, these orientations crossed\nwith the perspective of a hybrid form of co-investment allow us to build a\ncoherent and explanatory framework of the mixed syndication phenomenon. Our\nmethodology is based on a qualitative approach with an interpretative aim,\nwhich includes twenty-seven semi-structured interviews. These data were\nsubjected to a thematic content analysis using Nvivo software. The results\nsuggest that the relationships between public and private Venture capitalists\n(VCs) of a formal or informal nature, more specifically in a syndication\ncontext, at a national or regional level, are representative of an\n''economico-cognitive'' (Farrugia, 2014, page 6) approach to networking and\ninnovation. Moreover, the phenomenon of mixed syndication reveals a context of\nhybridization of public and private actors that would allow the private VCs to\nbenefit from the distribution of wealth when the company develops its\ninnovation. We can also identify a process related to a quest for legitimacy on\nthe part of the public actor characterized by its controlling role within the\npublic-private partnership (Beuve and Saussier, 2019). Finally, our study has\nsome limitations. One example is the measurement of the effects of\nrelationships on ''visible'' or ''invisible'' innovation (Djellal \\& Gallouj,\n2018, page 90).\n",
        "pdf_link": "http://arxiv.org/pdf/2110.09098v1"
    },
    {
        "title": "Media abnormal tone, earnings announcements, and the stock market",
        "authors": [
            "David Ardia",
            "Keven Bluteau",
            "Kris Boudt"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We conduct a tone-based event study to examine the aggregate abnormal tone\ndynamics in media articles around earnings announcements. We test whether they\nconvey incremental information that is useful for price discovery for\nnonfinancial S&P 500 firms. The relation we find between the abnormal tone and\nabnormal returns suggests that media articles provide incremental information\nrelative to the information contained in earnings press releases and earnings\ncalls.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.10800v1"
    },
    {
        "title": "Solution to the Equity Premium Puzzle Using the Sufficiency Factor of\n  the Model",
        "authors": [
            "Atilla Aras"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This study provides the solution to the equity premium puzzle. The new model\nwas developed by including the behavior of investors toward risk in financial\nmarkets in prior studies. The calculations of this newly tested model show that\nthe value of the coefficient of relative risk aversion is 1.033526 by assuming\nthe value of the subjective time discount factor to be 0.99. Since these values\nare compatible with the existing empirical studies, they confirm the validity\nof the newly derived model that provides the solution to the equity premium\npuzzle.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.14405v2"
    },
    {
        "title": "Risk-Free Rate in the Covid-19 Pandemic: Application Mistakes and\n  Conclusions for Traders",
        "authors": [
            "Magomet Yandiev"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This short paper is intended to demonstrate a crucial omission made by\ntraders in setting the risk-free interest rate, especially in times of crisis:\ninstead of increasing the risk-free rate, traders undercut it en masse on the\ncontrary. This results in incorrect investment and financial decisions,\nespecially those involving CAPM models, option pricing models and portfolio\ntheory.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.07075v1"
    },
    {
        "title": "The Cost-Benefit Fallacy: Why Cost-Benefit Analysis Is Broken and How to\n  Fix It",
        "authors": [
            "Bent Flyvbjerg",
            "Dirk W. Bester"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Most cost-benefit analyses assume that the estimates of costs and benefits\nare more or less accurate and unbiased. But what if, in reality, estimates are\nhighly inaccurate and biased? Then the assumption that cost-benefit analysis is\na rational way to improve resource allocation would be a fallacy. Based on the\nlargest dataset of its kind, we test the assumption that cost and benefit\nestimates of public investments are accurate and unbiased. We find this is not\nthe case with overwhelming statistical significance. We document the extent of\ncost overruns, benefit shortfalls, and forecasting bias in public investments.\nWe further assess whether such inaccuracies seriously distort effective\nresource allocation, which is found to be the case. We explain our findings in\nbehavioral terms and explore their policy implications. Finally, we conclude\nthat cost-benefit analysis of public investments stands in need of reform and\nwe outline four steps to such reform.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.03171v1"
    },
    {
        "title": "Potential utilization of Battery Energy Storage Systems (BESS) in the\n  major European electricity markets",
        "authors": [
            "Yu Hu",
            "Miguel Armada",
            "Maria Jesus Sanchez"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Given the declining cost of battery technology in the last decade, nowadays\nBESS becomes a more attractive solution in electrical power systems. The\nobjective of this work is to analyze the potential utilization of BESS in the\nmajor European electricity markets. A general payoff model for BESS operation\nis proposed to correctly address the operational flexibility of battery\nsystems. Utilization factors such as potentially profitable utilization time\nand rate are calculated for common applications including energy arbitrage and\nfrequency support services using real market information. The result shows that\nunder the current empirical estimation of the battery cost and lifetime, BESS\nis not feasible for energy arbitrage in most of the European electricity\nmarkets. However, BESS shows clearly and significantly higher potential in\nproviding frequency support services. The result suggests that, when the\nfrequency containment reserve is remunerable, the potentially profitable\nutilization of BESS has become already accretive in most of the European\ncountries. For example from January to September 2021, the potentially\nprofitable utilization rate has reached almost 100% for the FCR-N service in\nthe Danish market. Comparing the regional electricity markets in Europe, BESS\nhas shown significant potential in becoming a feasible solution in Central\nWestern Europe and parts of Northern Europe by providing frequency regulation\nservices. Meanwhile, in the British Isles and some other islanded local\nmarkets, a remarkable level of scarcity of flexibility has been revealed by the\ninvestigation, and the potential of BESS would also be considerably\nencouraging.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.09816v2"
    },
    {
        "title": "The theory of quantitative trading",
        "authors": [
            "Andrea Berdondini"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Abstract: This book consists of a selection of articles divided into three\nmain themes: Statistics, Quantitative Trading, Psychology. These three\narguments are indispensable for the development of a quantitative trading\nsystem. The order of the articles was chosen so as to constitute a single\nlogical reasoning that develops progressively.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11070v1"
    },
    {
        "title": "Cashing Out: Assessing the risk of localised financial exclusion as the\n  UK moves towards a cashless society",
        "authors": [
            "George Sullivan",
            "Luke Burns"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Whilst academic, commercial and policy literature on financial exclusion is\nextensive and wide-ranging, there have been very few attempts to quantify and\nmeasure localised financial exclusion anywhere in the world. This is a subject\nof growing importance in modern UK society with the withdrawal of cash\ninfrastructure and a shift towards online banking. This research develops a\ncomposite indicator using a wide-range of input variables, including the\nlocations of existing cash infrastructure, various demographic factors (such as\nincome and housing tenure) and other freely available lifestyle data to\nidentify areas at greatest risk of financial exclusion, thereby aiding\norganisations to develop intervention strategies to tackle the problem. The\nindicator illustrates that whilst there is no apparent correlation between\nfinancial exclusion and deprivation, pockets of extreme financial exclusion are\ngenerally found in deprived communities, and affluent, suburban areas tend to\nscore consistently more favourably and consequently carry less risk. The\nattributing causes vary, from a lack of infrastructure, to low car\navailability, but income levels have a pronounced influence. Three policy\nproposals are put forward, including offering banking services at PayPoint\noutlets, and converting cash machines to cash recyclers, but improving digital\nadoption was found to be the most effective intervention, provided that it is\nimplemented by community organisations. Policies purely targeting\ninfrastructure provision or addressing social exclusion are unlikely to be\neffective, as community-based initiatives coupled with wider reforms to the\nfinancial system are needed.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05674v1"
    },
    {
        "title": "The Evolution of Blockchain: from Lit to Dark",
        "authors": [
            "Agostino Capponi",
            "Ruizhe Jia",
            "Ye Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Transactions submitted through the blockchain peer-to-peer (P2P) network may\nleak out exploitable information. We study the economic incentives behind the\nadoption of blockchain dark venues, where users' transactions are observable\nonly by miners on these venues. We show that miners may not fully adopt dark\nvenues to preserve rents extracted from arbitrageurs, hence creating execution\nrisk for users. The dark venue neither eliminates frontrunning risk nor reduces\ntransaction costs. It strictly increases the payoff of miners, weakly increases\nthe payoff of users, and weakly reduces arbitrageurs' profits. We provide\nempirical support for our main implications, and show that they are\neconomically significant. A 1% increase in the probability of being frontrun\nraises users' adoption rate of the dark venue by 0.6%. Arbitrageurs'\ncost-to-revenue ratio increases by a third with a dark venue.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05779v1"
    },
    {
        "title": "Constructing a NFT Price Index and Applications",
        "authors": [
            "Hugo Schnoering",
            "Hugo Inzirillo"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  We are witnessing the emergence of a new digital art market, the art market\n3.0. Blockchain technology has taken on a new sector which is still not well\nknown, Non-Fungible tokens (NFT). In this paper we propose a new methodology to\nbuild a NFT Price Index that represents this new market on the whole. In\naddition, this index will allow us to have a look on the dynamics and\nperformances of NFT markets, and to diagnose them.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.08966v2"
    },
    {
        "title": "The impact of accumulative pension policy on welfare of individuals",
        "authors": [
            "Marika Khozrevanidze"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Many countries around the world have had to carry out radical reforms\nperiodically in their pension systems. Global experience shows that it is\nimportant to optimize the costs of pension and social security systems in order\nto ensure a decent old age in addition to reducing the pressure on budgetary\nresources. By Georgia is changing demographic situation, special attention is\npaid to proper functioning of the pension policy. The pension reform carried\nout in Georgia in 2019 caused a difference of opinion among experts. This issue\nin today is conditions does not lose relevance. The presented thesis discusses\nthe impact of the mandatory funded pension system on the well-being of people.\nThesis includes the following issues: peculiarities of the formation of pension\nsystems in Georgia. It is presented a small historical excursion in terms of\nthe development of pension systems. In addition, are discussed the\ninternational experience of pension systems and comparative analysis in\nrelation to Georgia. This paper specifically focuses on the essence of the\nmandatory funded pension system and assesses the current situation in terms of\ninvestment potential of the resource accumulated in the pension fund. In\nconclusion, are presented the challenges of this system and the ways of\nperfection.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12721v1"
    },
    {
        "title": "A Robust Statistical Analysis of the Role of Hydropower on the System\n  Electricity Price and Price Volatility",
        "authors": [
            "Olukunle O. Owolabi",
            "Kathryn Lawson",
            "Sanhita Sengupta",
            "Yingsi Huang",
            "Lan Wang",
            "Chaopeng Shen",
            "Mila Getmansky Sherman",
            "Deborah A. Sunter"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Hydroelectric power (hydropower) is unique in that it can function as both a\nconventional source of electricity and as backup storage (pumped hydroelectric\nstorage) for providing energy in times of high demand on the grid. This study\nexamines the impact of hydropower on system electricity price and price\nvolatility in the region served by the New England Independent System Operator\n(ISONE) from 2014 - 2020. We perform a robust holistic analysis of the mean and\nquantile effects, as well as the marginal contributing effects of hydropower in\nthe presence of solar and wind resources. First, the price data is adjusted for\ndeterministic temporal trends, correcting for seasonal, weekend, and diurnal\neffects that may obscure actual representative trends in the data. Using\nmultiple linear regression and quantile regression, we observe that hydropower\ncontributes to a reduction in the system electricity price and price\nvolatility. While hydropower has a weak impact on decreasing price and\nvolatility at the mean, it has greater impact at extreme quantiles (> 70th\npercentile). At these higher percentiles, we find that hydropower provides a\nstabilizing effect on price volatility in the presence of volatile resources\nsuch as wind. We conclude with a discussion of the observed relationship\nbetween hydropower and system electricity price and volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.02089v1"
    },
    {
        "title": "Vulnerability-CoVaR: Investigating the Crypto-market",
        "authors": [
            "Martin Waltz",
            "Abhay Kumar Singh",
            "Ostap Okhrin"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  This paper proposes an important extension to Conditional Value-at-Risk\n(CoVaR), the popular systemic risk measure, and investigates its properties on\nthe cryptocurrency market. The proposed Vulnerability-CoVaR (VCoVaR) is defined\nas the Value-at-Risk (VaR) of a financial system or institution, given that at\nleast one other institution is equal or below its VaR. The VCoVaR relaxes\nnormality assumptions and is estimated via copula. While important theoretical\nfindings of the measure are detailed, the empirical study analyzes how\ndifferent distressing events of the cryptocurrencies impact the risk level of\neach other. The results show that Litecoin displays the largest impact on\nBitcoin and that each cryptocurrency is significantly affected if an event of\njoint distress among the remaining market participants occurs. The VCoVaR is\nshown to capture domino effects better than other CoVaR extensions.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.10777v1"
    },
    {
        "title": "Grandes fraudes y gobiernos corporativos en la Economía desde mediados\n  del siglo XX",
        "authors": [
            "I Martín-de-Santos"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The international financial system is currently not yet prepared to face a\nforeseeable crisis mainly motivated by the dichotomy between the real economy\nand the virtual economy. Skepticism is widespread even when it comes to\ninvestments in sustainable economy. The concentration of capital in a few\npersons is one of the greatest risks for the possible reiteration of economic\ncrises. The benevolent sentences of the courts to some of the fraudsters do not\ncontribute to dispelling the ghost of fraud nor to the disappearance of tax\nhavens. From the diachronic perspective, it is observed that economic crises\nare increasingly frequent and incidents always in the financial field; which\nforces us to rethink an economic model on an international scale in which there\nis a greater weight of the economic policy of governments over the power of\nmultinational companies in the context of globalization. In the context of\nCorporate Social Responsibility, Corporate Governance is listed as one of the\nfundamental levers to curb large business fraud, but its efficiency seems\ninsufficient due to the lack of international regulations and the ignorance of\nhidden forces in what has been known as fiscal and financial engineering. The\napplication of liberal policies in an unorthodox way is causing real social\ngaps in the distribution of income and is undermining the current capitalist\nsystem. The need to implement corporate governments is recommended as one of\nthe essential formulas for sustaining the international economic system.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12605v1"
    },
    {
        "title": "A Case Study on Nutek India Limited, Regarding Deep Fall in Share Price",
        "authors": [
            "Gurjeet Singh",
            "Pankaj Nagar"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Manipulating the security price is an act of artificially inflating or\ndeflating the price of a security. Generally, manipulation is defined as a\nseries of transactions designed to raise or lower a price of a security or to\ngive the appearance of trading for the purpose of inducing others to buy or\nsell. In essence, a manipulation is intentional interference with the free\nforces of supply and demand. In this paper we have tried to study the reasons\nbehind drastic fall in share price of Nutek India Limited.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12657v1"
    },
    {
        "title": "A Structured Survey of Quantum Computing for the Financial Industry",
        "authors": [
            "Franco D. Albareti",
            "Thomas Ankenbrand",
            "Denis Bieri",
            "Esther Hänggi",
            "Damian Lötscher",
            "Stefan Stettler",
            "Marcel Schöngens"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Quantum computers can solve specific problems that are not feasible on\n\"classical\" hardware. Harvesting the speed-up provided by quantum computers\ntherefore has the potential to change any industry which uses computation,\nincluding finance. First quantum applications for the financial industry\ninvolving optimization, simulation, and machine learning problems have already\nbeen proposed and applied to use cases such as portfolio management, risk\nmanagement, and pricing derivatives. This survey reviews platforms, algorithms,\nmethodologies, and use cases of quantum computing for various applications in\nfinance in a structured way. It is aimed at people working in the financial\nindustry and serves to gain an overview of the current development and\ncapabilities and understand the potential of quantum computing in the financial\nindustry.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10026v1"
    },
    {
        "title": "A systematic analysis of biotech startups that went public in the first\n  half of 2021",
        "authors": [
            "Sebastian G. Huayamares",
            "Melissa P. Lokugamage",
            "Alejandro J. Da Silva Sanchez",
            "James E. Dahlman"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Biotechnologies are being commercialized at historic rates. In 2020, 74\nbiotech startups went public through an Initial Public Offering (IPO), and 60\nwent through the IPO process in the first six months of 2021. However, the\ntraits associated with biotech startups obtaining recent IPOs have not been\nreported. Here we build a database of biotechs that underwent an IPO in the\nfirst half of 2021. By analyzing leadership, technological focus, clinical\ntrials, and financing, we found that advanced degrees among the leadership,\nclinical trials, and intellectual property are important factors for biotech\nstartups. The data also suggest that large private rounds can decrease\ntime-to-IPO and affect post-IPO stock performance. Notably, these traits were\noften exhibited by the 138 biotech IPOs in 2018-2019, suggesting 2021 data were\nnot driven by COVID.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00993v1"
    },
    {
        "title": "Financial Markets and the Real Economy: A Statistical Field Perspective\n  on Capital Allocation and Accumulation",
        "authors": [
            "Pierre Gosselin",
            "Aïleen Lotz",
            "Marc Wambst"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  This paper provides a general method to directly translate a classical\neconomic framework with a large number of agents into a field-formalism model.\nThis type of formalism allows the analytical treatment of economic models with\nan arbitrary number of agents, while preserving the system's interactions and\nmicroeconomic features of the individual level.We apply this methodology to\nmodel the interactions between financial markets and the real economy,\ndescribed in a classical framework of a large number of heterogeneous agents,\ninvestors and firms. Firms are spread among sectors but may shift between\nsectors to improve their returns. They compete by producing differentiated\ngoods and reward their investors by paying dividends and through their stocks'\nvaluation. Investors invest in firms and move along sectors based on firms'\nexpected long-run returns.The field-formalism model derived from this framework\nallows for collective states to emerge. We show that the number of firms in\neach sector depends on the aggregate financial capital invested in the sector\nand its firms' expected long-term returns. Capital accumulation in each sector\ndepends both on short-term returns and expected long-term returns relative to\nneighbouring sectors.For each sector, three patterns of accumulation emerge. In\nthe first pattern, the dividend component of short-term returns is determinant\nfor sectors with small number of firms and low capital. In the second pattern,\nboth short and long-term returns in the sector drive intermediate-to-high\ncapital. In the third pattern, higher expectations of long-term returns drive\nmassive inputs of capital.Instability in capital accumulation may arise among\nand within sectors. We therefore widen our approach and study the dynamics of\nthe collective configurations, in particular interactions between average\ncapital and expected long-term returns, and show that overall stability\ncrucially depends on the expectations' formation process.Expectations that are\nhighly reactive to capital variations stabilize high capital configurations,\nand drive low-to-moderate capital sectors towards zero or a higher level of\ncapital, depending on their initial capital. Inversely, low-to moderate capital\nconfigurations are stabilized by expectations moderately reactive to capital\nvariations, and drive high capital sectors towards more moderate level of\ncapital equilibria.Eventually, the combination of expectations both highly\nsensitive to exogenous conditions and highly reactive to variations in capital\nimply that large fluctuations of capital in the system, at the possible expense\nof the real economy.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.03087v1"
    },
    {
        "title": "Bitcoin Returns and Public Attention to COVID-19: Do Timing and\n  Individualism Matter?",
        "authors": [
            "Huaxin Wang-Lu"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The evolution of the pandemic and people's concern over it have an impact on\nthe Bitcoin market, while the extent of individualism could differentiate\ninvestor behaviors in the financial market during the pandemic. This paper\nexamines whether public attention to COVID-19 in individualistic countries\nversus collectivistic countries Granger causes Bitcoin returns between February\n11, 2020 and May 09, 2022. To this end, eight large economies with a\nindividualistic or collectivistic tradition are chosen for analyses. By using\nrolling and recursive-evolving algorithms, it accounts for the timing of\nCOVID-19 issues that vary by country and circumvents the potential estimation\nbias that a traditional Granger causality test may suffer due largely to\nGoogle's sampling variation for different time frames. In general,\ncollectivistic countries are found to have stronger causal impacts on Bitcoin\nreturns than individualistic countries.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04290v3"
    },
    {
        "title": "The role of investor attention in global asset price variation during\n  the invasion of Ukraine",
        "authors": [
            "Martina Halousková",
            "Daniel Stašek",
            "Matúš Horváth"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  We study the impact of event-specific attention indices -- based on Google\nTrends -- in predictive price variation models before and during the Russian\ninvasion of Ukraine in February 2022. We extend our analyses to the importance\nof geographical proximity and economic openness to Russia within 51 global\nequity markets. Our results demonstrate that 36 countries show significant\nattention to the conflict at the onset of and during the invasion, which helps\npredict volatility. We find that the impact of attention is more significant in\ncountries with a higher degree of economic openness to Russia and those nearer\nto it.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.05985v3"
    },
    {
        "title": "The Evolution of Investor Activism in Japan",
        "authors": [
            "Ryo Sakai"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Activist investors have gradually become a catalyst for change in Japanese\ncompanies. This study examines the impact of activist board representation on\nfirm performance in Japan. I focus on the only two Japanese companies with\nactivist board representation: Kawasaki Kisen Kaisha, Ltd. (\"Kawasaki\") and\nOlympus Corporation (\"Olympus\"). Overall, I document significant benefits from\nthe decision to engage with activists at these companies. The target companies\nexperience greater short- and long-term abnormal stock returns following the\nactivist engagement. Moreover, I show operational improvements as measured by\nreturn on assets and return on equity. Activist board members also associate\nwith important changes in payout policy that help explain the positive stock\nreturns. My findings support the notion that Japanese companies should consider\nengagements with activist investors to transform and improve their businesses.\nSuch interactions can lead to innovative and forward-thinking policies that\ncreate value for Japanese businesses and their stakeholders.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.00640v1"
    },
    {
        "title": "Topological Data Analysis Ball Mapper for Finance",
        "authors": [
            "Pawel Dlotko",
            "Wanling Qiu",
            "Simon Rudkin"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Finance is heavily influenced by data-driven decision-making. Meanwhile, our\nability to comprehend the full informational content of data sets remains\nimpeded by the tools we apply in analysis, especially where the data is\nhigh-dimensional. Presenting the Topological Data Analysis Ball Mapper\nalgorithm this paper illuminates a new means of seeing the detail in data from\ndata shape. With comparisons to existing approaches and illustrative examples,\nthe value of the new tool is shown. Directions for employing Ball Mapper in\npractice are given and the benefits are reviewed.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.03622v1"
    },
    {
        "title": "Repenser le financement des entreprises vertueuses et les politiques\n  prudentielles en int{é}grant la solvabilit{é} socio-environnementale",
        "authors": [
            "Laura Chémali",
            "Camille Souffron"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Despite the amount of savings available and the money supply managed by\nfinancial institutions, significant market failures and the failure of carbon\npricing strategies prevent sufficient financing of the transition, notably\nthrough bank credit. Aware of the links between natural, monetary and\nproductive aggregates, we propose the development of ''eco-systemic''\nprudential policies by exposing the interdependence between macro, micro and\nenvironmental prudential measures. These would be based on a reorientation of\ncorporate accounting standards towards the concept of socio-environmental\nsolvency, notably the CARE-TDL model (integration of human and natural capital\nalongside financial capital on the liabilities side of the balance sheet). In\nan ecosystemic framework, this solvency of virtuous companies would compensate\nin accounting terms for the lack of financial solvency. The State would then be\nthe guarantor in order to facilitate their access to financing, also reduced by\nBasel III and Solvency II. This policy develops a system of reallocation of\nfinancing capacities from non-virtuous companies to the most virtuous ones with\npublic guarantees, aiming to reduce the debt ratio while increasing green\ninvestments, with monetary policies of rates but also of volumes and ratios\ndifferentiated according to the types of assets and the greening of bank\nbalance sheets, and finally forms of public-private partnership. Facilitating\nthe financing of green companies would green capital but increase it, partly\nneutralising the positive environmental impact. It is therefore necessary to\nlimit the credit expansion of ''brown'' companies. This would reduce risky\noperations and favour less leveraged investments more connected to the real\neconomy, reducing systemic financial risk. -- The Agenda 2030 Policy Briefs\nseries (PoCFiN Kedge Business School - SDSN France - Institut Rousseau)\nmobilises economists and practitioners to identify an agenda of economic and\nfinancial reforms to achieve the 2030 Agenda, at territorial, national and\nsupranational levels. These are published after peer review.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.06820v1"
    },
    {
        "title": "The probability flow in the Stock market and Spontaneous symmetry\n  breaking in Quantum Finance",
        "authors": [
            "Ivan Arraut",
            "Joao Alexandre Lobo Marques",
            "Sergio Gomes"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The Spontaneous Symmetry breaking in Quantum Finance considers the martingale\ncondition in the stock market as a vacuum state if we express the financial\nequations in the Hamiltonian form. The original analysis for this phenomena\nignores completely the kinetic terms in the neighborhood of the minimal of the\npotential terms. This is correct in most of the cases. However, when we deal\nwith the Martingale condition, it comes out that the kinetic terms can also\nbehave as potential terms and then reproduce a shift on the effective location\nof the vacuum (Martingale). In this paper we analyze the effective symmetry\nbreaking patterns and the connected vacuum degeneracy for these special\ncircumstances. Within the same scenario, we analyze the connection between the\nflow of information and the multiplicity of martingale states, providing in\nthis way powerful tools for analyzing the dynamic of the stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.07130v1"
    },
    {
        "title": "Parameter Estimation Methods of Required Rate of Return on Stock",
        "authors": [
            "Battulga Gankhuu"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  In this study, we introduce new estimation methods for the required rate of\nreturn of the stochastic dividend discount model (DDM) and the private company\nvaluation model, which will appear below. To estimate the required rate of\nreturn, we use the maximum likelihood method, the Bayesian method, and the\nKalman filtering. We apply the model to a set of firms from the S\\&P 500 index\nusing historical dividend and price data over a 32--year period. Overall,\nsuggested methods can be used to estimate the required rate of return.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.09657v2"
    },
    {
        "title": "The dynamics of the prices of the companies of the STOXX Europe 600\n  Index through the logit model and neural network",
        "authors": [
            "Federico Mecchia",
            "Marcellino Gaudenzi"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The aim of the present work is analysing and understanding the dynamics of\nthe prices of companies, depending on whether they are included or excluded\nfrom the STOXX Europe 600 Index. For this reason, data regarding the companies\nof the Index in question was collected and analysed also through the use of\nlogit models and neural networks in order to find the independent variables\nthat affect the changes in prices and thus determine the dynamics over time.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.09899v1"
    },
    {
        "title": "AI in Asset Management and Rebellion Research",
        "authors": [
            "Jimei Shen",
            "Yihan Mo",
            "Christopher Plimpton",
            "Mustafa Kaan Basaran"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  On October 30th, 2021, Rebellion Research's CEO announced in a Q3 2021 Letter\nto Investors that Rebellion's AI Global Equity strategy returned +6.8% gross\nfor the first three quarters of 2021. \"It's no surprise\", Alex told us, \"Our\nMachine Learning global strategy has a history of outperforming the S&P 500 for\n14 years\". In 2021, Rebellion's brokerage accounts can be opened in over 70\ncountries, and Rebellion's research covers over 50 countries. Besides being an\nAI asset management company, Rebellion also defines itself as a top-tier,\nglobal machine learning think tank. Alex planned to build a Rebellion ML & AI\necosystem. Should Rebellion stay in the asset management area or jump into\nother areas? How could the Rebellion strategically move towards a more broad\narea? What were Rebellion's new or alternative business models?\n",
        "pdf_link": "http://arxiv.org/pdf/2206.14876v1"
    },
    {
        "title": "Most claimed statistical findings in cross-sectional return\n  predictability are likely true",
        "authors": [
            "Andrew Y. Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  I develop simple and intuitive bounds for the false discovery rate (FDR) in\ncross-sectional return predictability publications. The bounds can be\ncalculated by plugging in summary statistics from previous papers and reliably\nbound the FDR in simulations that closely mimic cross-predictor correlations.\nMost bounds find that at least 75% of findings are true. The tightest bound\nfinds that at least 91% of findings are true. Surprisingly, the estimates in\nHarvey, Liu, and Zhu (2016) imply a similar FDR. I explain how Harvey et al.'s\nconclusion that most findings are false stems from misinterpreting\n``insignificant factor'' as ``false discovery.''\n",
        "pdf_link": "http://arxiv.org/pdf/2206.15365v6"
    },
    {
        "title": "A study on Determinants of Dividend Policy and its Impact on Financial\n  Performances: A Panel Data Analysis for Indian Listed Firms",
        "authors": [
            "Suresh N",
            "Pooja M"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Determination of the correct mix of dividend and retained earnings and its\neffect on profitability has been a subject of controversy in financial\nmanagement literature. This paper seeks to contribute to the ongoing debate by\nexamining the relationship between dividend payout policy and the financial\nperformance of 60 firms listed on the National Stock Exchange between\n2009-2018. The Return on Assets (ROA) served as a surrogate for the dependent\nvariable, profitability, while the Dividend Pay-out ratio proxied for dividend\npolicy and was the only explanatory variable. Control variables include firm\nsize, asset tangibility, and leverage. Regression result reveals a positive and\nsignificant relationship between dividend payout policy (DPO) and firm\nperformance (ROA). It is recommended that companies should endeavor to put in\nplace a robust dividend payout policy that would encourage investment in\nprojects that give positive Net Present Value.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00715v1"
    },
    {
        "title": "A Study on Impact of Environmental Accounting on Profitability of\n  Companies listed in Bombay Stock Exchange",
        "authors": [
            "Nandini E. S",
            "Sudharani R",
            "Suresh N"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The study focuses on the Impact of Environmental Accounting on the\nProfitability of Companies listed on the Bombay Stock Exchange. The study has\nconsidered the Amount spent on Environmental protection as an Independent\nvariable and Return on Capital Employed, Return on Assets, Return on Net\nworth/equity, Net Profit Margin, and Dividend per Share as the Dependent\nvariable. The present study is to analyses the relationship between Amounts\nspent on Environmental protection costs and Return on Capital Employed, Return\non Assets, Return on Net worth/equity, Net Profit Margin, and Dividend per\nShare. The data is collected from 18 companies listed on the Bombay Stock\nExchange for 10 years from the Annual reports of companies. The data collected\nwere analysed using Panel data Regression in E-Views. Results revealed that\nthere is a significant Relationship between Environmental protection Cost and\nReturn on Capital Employed, Return on Assets, Return on Net worth/equity, Net\nProfit Margin, and Dividend per Share. The study shows that Environmental\naccounting impact positively on Firms profitability.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00716v1"
    },
    {
        "title": "A Study on Impact of Capital Structure on Profitability of Companies\n  Listed in Indian Stock Exchange with respect to Automobile Industry",
        "authors": [
            "P. Aishwarya",
            "Sudharani R",
            "Suresh N"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Current research helps in understanding both positive and negative impacts of\ncapital structure on profits of Indian automobile companies by using variables\nlike Return on Capital Employed, Return on Long Term Funds, Return on Net\nWorth, Gross Profit Margin, and Operating Profit, and Return on Asset. The\nstudy hypothesized that RoCE, RoLT, and RoNW have a positive effect and GP, OP\nand ROA have a negative impact on debt-equity and interest coverage ratios i.e\ncapital structure of the companies. Also, the study proves that the\nrelationship between profitability and capital structure variables is strongly\nsignificant. The hypothesis was tested by using fixed effect and random effect\nmodels by considering 10 years of data (from 2010-2019) from 17 automobile\ncompanies. The result of the study recommends that the firms can improve their\nperformance by using an optimal capital structure. Also, a fair mix of debt and\nequity should be established to ensure that the firm maintains capital\nadequacy. Firms can thus be able to meet their financial compulsions and\ninvestments that can promise attractive returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00720v1"
    },
    {
        "title": "A Study on the Impact of Human Resource Accounting on Firms Value with\n  Respect to Companies Listed in National Stock Exchange",
        "authors": [
            "Anil S",
            "Sudharani R",
            "Suresh N"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The study focuses on the Impact of Employment Benefit Cots on the\nProfitability of Companies listed in the National Stock Exchange. The study has\nconsidered the Amount spent on Employment Benefit Cots as an Independent\nvariable and Profit after tax, Total Assets, Return on Equity, and Return on\nAsset and Debt equity Ration as the Dependent variable. The present study is to\nanalyses the relationship between Employment Benefit Cots and Profit after tax,\nTotal Assets, Return on Equity, Return on Asset, and Debt equity Ration. The\ndata is collected from 20 companies listed on the National Stock Exchange for\n10 years from the Annual reports of companies. The data collected were analyzed\nusing Panel data Regression in E-Views. Results revealed that there is a\nsignificant Relationship between Employment Benefit Cots and Profit after tax,\nTotal Assets, Return on Equity, Return on Asset, and Debt equity Ration. The\nstudy shows that Employment Benefit Cots impact positively on Firms\nprofitability.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00722v1"
    },
    {
        "title": "Mortgage-Rate-Adjusted Home Prices",
        "authors": [
            "Honggao Cao"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  In this paper, we investigate the impact of mortgage rates on home prices,\nand how the impact may be used to help property purchase discussions at\nindividual buyer level and to adjust home price indices across time. A\nmortgage-rate-adjusted \"effective price\" is derived to measure near term\nproperty price in the presence of (expected) mortgage rate changes. A\nprice-mortgage rate neutrality line is then constructed based on the \"effective\nprice\" to help differentiate various market scenarios in the near term, which\ncan be used by prospective buyers in their \"to-buy or not-to-buy\"\ndeliberations. At the market level, effective home prices allow for\nneutralization of mortgage rates on the movement of the housing market. An\napplication of the neutralization strategy to the Case-Shiller Home Price Index\n(HPI) indicates that the U.S. housing market has been considerably affected by\nthe dynamics of mortgage rates in a long run. But mortgage rates do no appear a\nprimary driver of the extraordinary home price increase during the COVID-19\npandemic.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.02896v1"
    },
    {
        "title": "Effect of Demonetisation of on Indian High Denomination Currencies on\n  Indian Stock Market and its Relationship with Foreign Exchange Rate",
        "authors": [
            "N. Suresh",
            "N. R. Bharathi"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  This study examines the impact of the foreign exchange rate, i.e., US Dollar\nto Indian Rupee (USD/INR) on the Indian Stock Market Index (Nifty 50) during\nthe demonetization of high denomination Indian currencies. A daily rate of\nreturn of Foreign exchange rate (USD/INR) and the Indian Stock Market Index\n(Nifty 50) were considered for the study. The Dummy variable was used to\nmeasure the effect of demonetization during Nov/Dec 2016. The period of study\nwas restricted to 243 days from 1st April 2016 to 31st March 2017. The study\nreveals that there was an upward trend observed in the Indian Stock Market and\nthe Indian currency was strengthened with the decrease in the Foreign exchange\nrate (USD/INR).\n",
        "pdf_link": "http://arxiv.org/pdf/2207.06963v1"
    },
    {
        "title": "A Study on Impact of Dividend Policy on Initial Public Offering Price\n  Performance",
        "authors": [
            "S. Meghna",
            "N. Suresh",
            "J. C. Usha"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  This study examines the impact of dividend policy on the performance of\ninitial public offerings in India. The period of study is from the year\n2011-2014. Monthly returns of the IPOs issued in the considered period and the\nIndian Stock Market Index (Nifty 50) were considered for the long-run\nperformance study. The methodological tools used are long-run performance\nstatistics and the GARCH model. The Dummy variable was used to measure the\neffect of dividends on the IPOs. The study reveals that the dividend policy has\nno significant effect on the stock prices of IPO.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07227v1"
    },
    {
        "title": "A Study on Impact of Downsizing on Profitability of Construction\n  Industries listed in Bombay Stock Exchange (BSE) India",
        "authors": [
            "D Reshma",
            "Sudharani R",
            "Suresh N"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The study investigates the impact of downsizing layoffs on the profitability\nof construction industries listed in BSE India. In India, construction\nindustries have adopted downsizing long back in the organization to improve the\nfirms performance. For the purpose of the study, Secondary data of 15\nConstruction companies listed in BSE India have been considered for a period of\n10 years from FY.2010 to FY2019. Data has been taken from the companys official\nwebsite. The variable considered for the analysis is Other Expenses, Returns on\nNet Worth, Employee Expenses, Number of Employees, and Profit Per Employee. The\nstudy has used the Co-integration test to see co-integration between the\nvariables, Ordinary Least Square (OLS) and Vector Auto Regression (VAR) the\nmodel used for estimating the impact of downsizing on the profitability of\nconstruction companies. OLS and VAR model has been used to draw a conclusion\nbased on the P values and R square. From the result, it can be concluded that,\nExpect Profit Per Employees are the downsizing variable that has no significant\nimpact on the profitability of the firms performance. Whereas the other\nDownsizing variables Employee Expenses and the Number of Employee has a\nsignificant impact on the profitability of the firms performance\n",
        "pdf_link": "http://arxiv.org/pdf/2207.11546v1"
    },
    {
        "title": "Indian Derivatives Market Evolution and Challenge",
        "authors": [
            "Shruthi B. C.",
            "N. Suresh"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The study is conducted to establish the framework for comparing the relation\nin performance of the derivatives of BSE and NSE in India and to analyse the\nrelationship of derivatives with the cash market and the market volatility. The\nexchange-traded equity derivatives were considered for the study. It was found\nthat the performance of derivatives in NSE is a lot higher than in BSE and the\nNSE is on par with the global exchanges compared to BSE in terms of the number\nof contracts traded for Stock Index Options and Futures and also Stock Futures.\nHence derivatives market needs to strengthen further with the number of\ncontracts traded and turnover in all the derivative instruments with more\nstrong regulations and a robust framework protecting the interest of the\ninvestors. This study enables Derivative Industry to progress towards its goals\nand objectives in a more efficient way.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.13123v1"
    },
    {
        "title": "Interrogation of A Bubble in the Indian Market",
        "authors": [
            "Ganapathy G Gangadharan",
            "N. Suresh"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Emerging markets such as India provide investors with returns far greater\nthan those in developed markets; taking the average returns from the period\n1995 to 2014 the returns are 4.714% to 3.276% of the developed market. The\nmajority of emerging markets commenced joining with the capital market of the\nworld, thus allowing a huge inflow of capital which in turn paved the path for\neconomic growth. Even though the emerging markets provide high returns these\nmay also be an indication of a bubble formation. Detection of a bubble is a\ntedious task primarily due to the fundamental value of the security being\nuncertain, and the randomness of the fundamentals of the market makes detecting\nbubbles an arduous task. Ratios that foretold the financial crisis of 2007-\nMarket Capitalization to GDP, Price to Earnings Ratio, Price to Book Value,\nTobins Q. Data is collected from 1999-2000 from various Indian indices such as\nNIFTY 50, NIFTY NEXT 50, NIFTY BANK, NIFTY 500 S and PBSE SENSEX, S and P BSE\n100. The paper utilizes the ratios mentioned above to detect and backtrack\nvarious bubble episodes in the Indian market; the methodology used is the\nPhilips et al 2015 right-tailed unit test. The paper is also inclined to take\nsteps to mitigate the effects of a bubble by amending the financial policies\nand the monetary liquidity of the financial system.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.13444v1"
    },
    {
        "title": "Assessing the different aspects of consuming fashion and the role of\n  self-confidence on the buying behaviour of fashion consumers in the clothing\n  market as a mediator",
        "authors": [
            "Milad Zam",
            "Mohammadhosein Tavakoli",
            "Hasan Ramezanian",
            "Amin Rezasoltani"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  As a mediator variable, self-confidence is one of the most effective elements\nof the decision-making process of consumer behaviour. This research has studied\nthe effects of different aspects of consuming fashion on the self-confidence\nand behaviour of consumers in Tehran's clothing market. This study has\nconsidered the acceptance of new products, interest in mode and fashion,\nutilitarianism, and personal taste in its analysis. This research aims to\nunderstand the fashion buying behaviour amongst Iranian consumers in\nconsideration of their attitude towards self-confidence and aspects of fashion\nconsumption. The statistic sample is 400 consumers from Tehran's clothing\nmarket who have been chosen based on the random availability procedure. The\nprimary tool in this research was a questionary used to testify the assumptions\nand a model fit created by using structural equations and factor analysis. This\nresearch showed that the interest in mode and fashion, personal taste,\nutilitarianism, and new products positively impact self-confidence. In\naddition, the positive impact of self-confidence on fashion buying behaviour\nwas confirmed.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.02367v1"
    },
    {
        "title": "Cryptocurrency bubbles, the wealth effect, and non-fungible token\n  prices: Evidence from metaverse LAND",
        "authors": [
            "Kanis Saengchote"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The rapid rise of cryptocurrency prices led to concerns (e.g. the Financial\nStability Board) that this wealth accumulation could detrimentally spill over\ninto other parts of the economy, but evidence is limited. We exploit the\ntendency for metaverses to issue their own cryptocurrencies along with\nnon-fungible tokens (NFTs) representing virtual real estate ownership (LAND) to\nprovide evidence of the wealth effect. Cryptocurrency prices and their\ncorresponding real estate prices are highly correlated (more than 0.96), and\ncryptocurrency prices Granger cause LAND prices. This metaverse bubble\nreminisces the 1920s American real estate bubble that preceded the 1929 stock\nmarket crash.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.04385v1"
    },
    {
        "title": "The Sign of Risk for Present Value of Future Losses",
        "authors": [
            "Brian P. Hanley",
            "Steve Keen"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  In the ongoing debate over discount rates and climate change, William\nNordhaus has championed a higher discount rate to account for risk. Nicholas\nStern has championed a lower rate. Here we prove that in the case of a stream\nof future losses, risk can only be represented by a lower discount rate, never\na higher one.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.06654v1"
    },
    {
        "title": "Credit Information in Earnings Calls",
        "authors": [
            "Harry Mamaysky",
            "Yiwen Shen",
            "Hongyu Wu"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  We develop a novel technique to extract credit-relevant information from the\ntext of quarterly earnings calls. This information is not spanned by\nfundamental or market variables and forecasts future credit spread changes. One\nreason for such forecastability is that our text-based measure predicts future\ncredit spread risk and firm profitability. More firm- and call-level complexity\nincrease the forecasting power of our measure for spread changes. Out-of-sample\nportfolio tests show the information in our measure is valuable for investors.\nBoth results suggest that investors do not fully internalize the\ncredit-relevant information contained in earnings calls.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.11914v2"
    },
    {
        "title": "Discount Puzzle Of Closed-End Mutual Funds: A Case Of Bangladesh",
        "authors": [
            "Farhana Rahman"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The paper intends to perform a relevant study on the closed-end fund puzzle\nin the perspective of an emerging market. Quarterly data of 36 closed-end\nmutual funds traded in Dhaka Stock Exchange are collected over the sample\nperiod of 2016 to 2019. Dependent and independent variables are mapped down by\nexploring previous researches. Weight of top 10 investments, fund size, fund\nage, fund maturity, turnover and dividend yield are taken as explanatory\nvariable to analyze the impact on CEF discount. A fixed effects panel\nregression is performed on the data set with few diagnostic tests to ensure the\nreliability of the analysis conducted. The results show that, the variable fund\nsize and fund maturity have a significant positive and turnover has a\nsignificant negative impact on CEF discount while the impact of weight of top\n10 investments, dividend yield and fund age are found insignificant.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.13102v1"
    },
    {
        "title": "Publication Bias in Asset Pricing Research",
        "authors": [
            "Andrew Y. Chen",
            "Tom Zimmermann"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Researchers are more likely to share notable findings. As a result, published\nfindings tend to overstate the magnitude of real-world phenomena. This bias is\na natural concern for asset pricing research, which has found hundreds of\nreturn predictors and little consensus on their origins.\n  Empirical evidence on publication bias comes from large scale meta-studies.\nMeta-studies of cross-sectional return predictability have settled on four\nstylized facts that demonstrate publication bias is not a dominant factor: (1)\nalmost all findings can be replicated, (2) predictability persists\nout-of-sample, (3) empirical $t$-statistics are much larger than 2.0, and (4)\npredictors are weakly correlated. Each of these facts has been demonstrated in\nat least three meta-studies.\n  Empirical Bayes statistics turn these facts into publication bias\ncorrections. Estimates from three meta-studies find that the average correction\n(shrinkage) accounts for only 10 to 15 percent of in-sample mean returns and\nthat the risk of inference going in the wrong direction (the false discovery\nrate) is less than 10%.\n  Meta-studies also find that $t$-statistic hurdles exceed 3.0 in multiple\ntesting algorithms and that returns are 30 to 50 percent weaker in alternative\nportfolio tests. These facts are easily misinterpreted as evidence of\npublication bias effects. We clarify these misinterpretations and others,\nincluding the conflating of ``mostly false findings'' with ``many insignificant\nfindings,'' ``data snooping'' with ``liquidity effects,'' and ``failed\nreplications'' with ``insignificant ad-hoc trading strategies.''\n  Meta-studies outside of the cross-sectional literature are rare. The four\nfacts from cross-sectional meta-studies provide a framework for future\nresearch. We illustrate with a preliminary re-examination of equity premium\npredictability.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.13623v3"
    },
    {
        "title": "Digital financial services and open banking innovation: are banks\n  becoming invisible?",
        "authors": [
            "Valeria Stefanelli",
            "Francesco Manta",
            "Pierluigi Toma"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Digitalization in many economic sectors drove the financial system to adapt\nto new paradigms of technological transformation. Moreover, the extant\nregulatory framework forced the financial system to reconsider its business\nmodels and its relationship with the market. Such reasons generated also in the\nbanking sector a new model of competition within the ecosystem never seen\nbefore in this sector. The new ecosystem of banks and financial institutions\nlacks a common framework that not only synthesizes the development lines of\nopen innovation in the banking sector, but also regarding the planification of\nstrategic choices and organisation within the new ecosystems. The present study\naims to inquire the strategic positioning of European banks toward their\ndigital transformation strategies, by analysing the decision-making processes\nthat occurred between 2015 and 2019. A qualitative analysis on partnerships and\nthe adoption of Application Programming Interfaces (APIs) development in\nsupport of new service models was carried out. Results have relevant policy\nimplications for regulators, linked to the business evolution and the risks of\noutsourcing, and managerial implications for the followers, specifically on the\nplan of service integration to diversify and boost their activities in the\nsegment of customer relationship management and care, providing a better user\nexperience.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.01109v2"
    },
    {
        "title": "Impact of WACC on Firm Profitability: Evidence from Food and Allied\n  Industry of Bangladesh",
        "authors": [
            "Farhana Rahman"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The research paper aims to analyze the underlying relationship in between the\nprofitability and cost of funds of a firm. A total of twelve companies were\nselected as a sample for this study which are listed in Dhaka Stock Exchange\nunder Food and Allied Industry. A panel data set of 15 years from 2005 to 2019\nwas used to conduct the necessary analysis. In this paper, Return on Asset\n(ROA) is used as the accounting criteria of profitability. WACC is the\nindependent variable while Firm Size, Firm Age and Firm Leverage are used as\ncontrol variable for the study. Fixed Effects Panel Regression Model is used to\nanalyze the dataset. The result of the analysis shows that WACC is negatively\nrelated with the profitability measure and this relationship is significant.\nThe study has potential to be replicated by other industries like textile,\ncement, pharmaceutical & chemical, fuel & power, tannery etc.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.07955v1"
    },
    {
        "title": "Which Factors Matter Most? Can Startup Valuation be Micro-Targeted?",
        "authors": [
            "Max Berre"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  While startup valuations are influenced by revenues, risks, age, and\nmacroeconomic conditions, specific causality is traditionally a black box.\nBecause valuations are not disclosed, roles played by other factors (industry,\ngeography, and intellectual property) can often only be guessed at. VC\nvaluation research indicates the importance of establishing a factor-hierarchy\nto better understand startup valuations and their dynamics, suggesting the\nwisdom of hiring data-scientists for this purpose. Bespoke understanding can be\nestablished via construction of hierarchical prediction models based on\ndecision trees and random forests. These have the advantage of understanding\nwhich factors matter most. In combination with OLS, the also tell us the\ncircumstances of when specific causalities apply. This study explores the\ndeterministic role of categorical variables on the valuation of start-ups (i.e.\nthe joint-combination geographic, urban, and sectoral denomination-variables),\nin order to be able to build a generalized valuation scorecard approach. Using\na dataset of 1,091 venture-capital investments, containing 1,044 unique EU and\nEEA, this study examines microeconomic, sectoral, and local-level impacts on\nstartup valuation. In principle, the study relies on Fixedeffects and\nJoint-fixed-effects regressions as well as the analysis and exploration of\ndivergent micropopulations and fault-lines by means of non-parametric\napproaches combining econometric and machinelearning techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.14518v1"
    },
    {
        "title": "State-dependent Asset Allocation Using Neural Networks",
        "authors": [
            "Reza Bradrania",
            "Davood Pirayesh Neghab"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Changes in market conditions present challenges for investors as they cause\nperformance to deviate from the ranges predicted by long-term averages of means\nand covariances. The aim of conditional asset allocation strategies is to\novercome this issue by adjusting portfolio allocations to hedge changes in the\ninvestment opportunity set. This paper proposes a new approach to conditional\nasset allocation that is based on machine learning; it analyzes historical\nmarket states and asset returns and identifies the optimal portfolio choice in\na new period when new observations become available. In this approach, we\ndirectly relate state variables to portfolio weights, rather than firstly\nmodeling the return distribution and subsequently estimating the portfolio\nchoice. The method captures nonlinearity among the state (predicting) variables\nand portfolio weights without assuming any particular distribution of returns\nand other data, without fitting a model with a fixed number of predicting\nvariables to data and without estimating any parameters. The empirical results\nfor a portfolio of stock and bond indices show the proposed approach generates\na more efficient outcome compared to traditional methods and is robust in using\ndifferent objective functions across different sample periods.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00871v1"
    },
    {
        "title": "Does Peer-Reviewed Research Help Predict Stock Returns?",
        "authors": [
            "Andrew Y. Chen",
            "Alejandro Lopez-Lira",
            "Tom Zimmermann"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Mining 29,000 accounting ratios for t-statistics over 2.0 leads to\ncross-sectional return predictability similar to the peer review process. For\nboth methods, about 50% of predictability remains after the original sample\nperiods. Predictors supported by peer-reviewed risk explanations or equilibrium\nmodels underperform other predictors post-sample, suggesting peer review\nsystematically mislabels mispricing as risk, though only 20% of predictors are\nlabelled as risk. Data mining generates other features of peer review including\nthe rise in returns as original sample periods end and the speed of post-sample\ndecay. It also uncovers themes like investment, issuance, and accruals --\ndecades before they are published.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.10317v5"
    },
    {
        "title": "The Impact of Regulation Regime Changes on ChiNext IPOs: Effects of 2013\n  and 2020 Reforms on Pricing and Overreaction",
        "authors": [
            "Qi Deng",
            "Lunge Dai",
            "Zixin Yang",
            "Zhong-guo Zhou",
            "Monica Hussein",
            "Dingyi Chen",
            "Mick Swartz"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Since its inauguration, ChiNext has gone through three time periods with two\ndifferent regulation regimes and three different sets of listing day trading\nrestrictions. This paper studies the impact of regulation regimes and listing\nday trading restrictions on the initial return of ChiNext IPOs. We hypothesize\nthat the initial return of a ChiNext IPO contains the issuers intrinsic value\nand the investors overreaction. The intrinsic value is represented by the IPOs\n21st day return (monthly return), and the difference between the monthly and\ninitial returns (intramonth return) is a proxy of the overreaction. We find\nthat all significant variables for all three returns in all three time periods\nfall into four categories: pre-listing demand, post-listing demand, market\ncondition and pre-listing issuer value. We observe stark contrasts among\nvariable categories for each of the returns in the three time periods, which\nreveals an evolution of the investors behavior with regard to the progression\nof regulation regimes. Based on our findings, we argue that the differences\namong the levels and determinants of initial return, monthly return (intrinsic\nvalue) and intramonth return (overreaction) in different time periods can be\nlargely explained by regulation regime changes along two dimensions: 1)\napproval vs. registration and 2) listing day trading curbs and return limits.\nWe find that IPO pricing is demand-driven under the approval regime, but\nvalue-driven under the registration regime. We further compare the impact of\nregulation regime changes on ChiNext IPO pricing practice, and propose a future\nresearch plan on ChiNext IPO pricing efficiency with policy implication.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.11779v1"
    },
    {
        "title": "Stock Market Prediction via Deep Learning Techniques: A Survey",
        "authors": [
            "Jinan Zou",
            "Qingying Zhao",
            "Yang Jiao",
            "Haiyao Cao",
            "Yanxi Liu",
            "Qingsen Yan",
            "Ehsan Abbasnejad",
            "Lingqiao Liu",
            "Javen Qinfeng Shi"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Existing surveys on stock market prediction often focus on traditional\nmachine learning methods instead of deep learning methods. This motivates us to\nprovide a structured and comprehensive overview of the research on stock market\nprediction. We present four elaborated subtasks of stock market prediction and\npropose a novel taxonomy to summarize the state-of-the-art models based on deep\nneural networks. In addition, we also provide detailed statistics on the\ndatasets and evaluation metrics commonly used in the stock market. Finally, we\npoint out several future directions by sharing some new perspectives on stock\nmarket prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.12717v2"
    },
    {
        "title": "Facts of US Firm Scale and Growth 1970-2019: An Illustrated Guide",
        "authors": [
            "Robert Parham"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This work analyzes data on all public US firms in the 50 year period\n1970-2019, and presents 18 stylized facts of their scale, income, growth,\nreturn, investment, and dynamism. Special attention is given to (i) identifying\ndistributional forms; and (ii) scale effects -- systematic difference between\nfirms based on their scale of operations. Notable findings are that the\nDifference-of-Log-Normals (DLN) distribution has a central role in describing\nfirm data, scale-dependent heteroskedasticity is rampant, and small firms are\nsystematically different from large firms.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.02485v1"
    },
    {
        "title": "Decentralized Exchanges: The Profitability Frontier of Constant Product\n  Market Makers",
        "authors": [
            "Tobias Bitterli",
            "Fabian Schär"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  In this paper we analyze constant product market makers (CPMMs). We formalize\nthe liquidity providers' profitability conditions and introduce a concept we\ncall the profitability frontier in the xyk-space. We study the effect of mint\nand burn fees on the profitability frontier, consider various pool types, and\ncompile a large data set from all Uniswap V2 transactions. We use this data to\nfurther study our theoretical framework and the profitability conditions. We\nshow how the profitability of liquidity provision is severely affected by the\ncosts of mint and burn events relative to the portfolio size and the\ncharacteristics of the trading pair.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.05219v2"
    },
    {
        "title": "Control of Emerging-Market Target, Abnormal Stock Return: Evidence in\n  Vietnam",
        "authors": [
            "Quyen Van",
            "Vy Tran"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Joining with the upward trend of Global Foreign direct investment and FDI in\nemerging economies and emerging Asian economies, FDI to Vietnam, especially\nM&As have increased significantly in both numbers and value of deals from 1995\nto 2015...\n",
        "pdf_link": "http://arxiv.org/pdf/2302.07117v2"
    },
    {
        "title": "The financial health of a company and the risk of its default: Back to\n  the future",
        "authors": [
            "Gianmarco Bet",
            "Francesco Dainelli",
            "Eugenio Fabrizi"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We theorize the financial health of a company and the risk of its default. A\ncompany is financially healthy as long as its equilibrium in the financial\nsystem is maintained, which depends on the cost attributable to the probability\nthat equilibrium may decay. The estimate of that probability is based on the\ncredibility and uncertainty of the company's financial forecasts. Accordingly,\nwe develop an equilibrium model establishing ranges of interest rates as a\nfunction of predictable corporate performance and of its credit supply\nconditions. As a result, our model estimates idiosyncratic default risk and\nprovides intrinsically forward-looking PD.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.10140v1"
    },
    {
        "title": "DeFi Lending During The Merge",
        "authors": [
            "Lioba Heimbach",
            "Eric Schertenleib",
            "Roger Wattenhofer"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Lending protocols in decentralized finance enable the permissionless exchange\nof capital from lenders to borrowers without relying on a trusted third party\nfor clearing or market-making. Interest rates are set by the supply and demand\nof capital according to a pre-defined function. In the lead-up to The Merge:\nEthereum blockchain's transition from proof-of-work (PoW) to proof-of-stake\n(PoS), a fraction of the Ethereum ecosystem announced plans of continuing with\na PoW-chain. Owners of ETH - whether their ETH was borrowed or not - would hold\nthe native tokens on each chain. This development alarmed lending protocols.\nThey feared spiking ETH borrowing rates would lead to mass liquidations which\ncould undermine their viability. Thus, the decentralized autonomous\norganization running the protocols saw no alternative to intervention -\nrestricting users' ability to borrow.\n  We investigate the effects of the merge and the aforementioned intervention\non the two biggest lending protocols on Ethereum: AAVE and Compound. Our\nanalysis finds that borrowing rates were extremely volatile, jumping by two\norders of magnitude, and borrowing at times reached 100% of the available\nfunds. Despite this, no spike in mass liquidations or irretrievable loans\nmaterialized. Further, we are the first to quantify and analyze\nhard-fork-arbitrage, profiting from holding debt in the native blockchain token\nduring a hard fork. We find that arbitrageurs transferred tokens to centralized\nexchanges which at the time were worth more than 13 Mio US$, money that was\neffectively extracted from the platforms' lenders.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.08748v3"
    },
    {
        "title": "Financial Structure, Firm Size and Financial Growth of Non-Financial\n  Firms Listed at the Nairobi Securities Exchange",
        "authors": [
            "David Haritone Shikumo",
            "Oluoch Oluoch",
            "Joshua Matanda Wepukhulu"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  A significant number of the non-financial firms listed at the Nairobi\nSecurities Exchange have been experiencing declining financial performance and\nfinancial growth, which deter investors from investing in such firms. Hence,\nthe study aimed at establishing the effect of financial structure on the\nfinancial growth of non-financial firms listed at the Nairobi Securities\nExchange.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10910v1"
    },
    {
        "title": "Company Competition Graph",
        "authors": [
            "Yanci Zhang",
            "Yutong Lu",
            "Haitao Mao",
            "Jiawei Huang",
            "Cien Zhang",
            "Xinyi Li",
            "Rui Dai"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Financial market participants frequently rely on numerous business\nrelationships to make investment decisions. Investors can learn about potential\nrisks and opportunities associated with other connected entities through these\ncorporate connections. Nonetheless, human annotation of a large corpus to\nextract such relationships is highly time-consuming, not to mention that it\nrequires a considerable amount of industry expertise and professional training.\nMeanwhile, we have yet to observe means to generate reliable knowledge graphs\nof corporate relationships due to the lack of impartial and granular data\nsources. This study proposes a system to process financial reports and\nconstruct the public competitor graph to fill the void. Our method can retrieve\nmore than 83\\% competition relationship of the S\\&P 500 index companies. Based\non the output from our system, we construct a knowledge graph with more than\n700 nodes and 1200 edges. A demo interactive graph interface is available.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.00323v1"
    },
    {
        "title": "Decentralized Finance (DeFi)",
        "authors": [
            "Mansur Bestas"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Decentralized finance, powered by blockchain technology, is growing day by\nday. This field, which emerged a few years ago, today manages $70 billion in\nassets. In this study, the concept of decentralized finance is discussed and\nexplained the differences from traditional finance. Then, compliance with the\nlegal regulations and the requirements to ensure compliance are mentioned. An\nevaluation has been made about the financial services offered by the\ndecentralized finance field and the stock market and stablecoins that it uses\nas a tool while providing these services. Its economic effects, security and,\nprivacy dimensions are examined. In the study, the differences between\ncentralized and decentralized finance, which generally covers legal, economic,\nsecurity, privacy, and market manipulation, are systematically analyzed. A\nstructured methodology is presented to distinguish between centralized and\ndecentralized financial services. Keywords: decentralized finance, FinTech,\nfinancial regulation, blockchain, distributed ledger technology.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.01918v1"
    },
    {
        "title": "Patents and intellectual property assets as non-fungible tokens: key\n  technologies and challenges",
        "authors": [
            "Seyed Mojtaba Hosseini Bamakan",
            "Nasim Nezhadsistani",
            "Omid Bodaghi",
            "Qiang Qu"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  With the explosive development of decentralized finance, we witness a\nphenomenal growth in tokenization of all kinds of assets, including equity,\nfunds, debt, and real estate. By taking advantage of blockchain technology,\ndigital assets are broadly grouped into fungible and non-fungible tokens (NFT).\nHere non-fungible tokens refer to those with unique and non-substitutable\nproperties. NFT has widely attracted attention, and its protocols, standards,\nand applications are developing exponentially. It has been successfully applied\nto digital fantasy artwork, games, collectibles, etc. However, there is a lack\nof research in utilizing NFT in issues such as Intellectual Property. Applying\nfor a patent and trademark is not only a time-consuming and lengthy process but\nalso costly. NFT has considerable potential in the intellectual property\ndomain. It can promote transparency and liquidity and open the market to\ninnovators who aim to commercialize their inventions efficiently. The main\nobjective of this paper is to examine the requirements of presenting\nintellectual property assets, specifically patents, as NFTs. Hence, we offer a\nlayered conceptual NFT-based patent framework. Furthermore, a series of open\nchallenges about NFT-based patents and the possible future directions are\nhighlighted. The proposed framework provides fundamental elements and guidance\nfor businesses in taking advantage of NFTs in real-world problems such as grant\npatents, funding, biotechnology, and so forth.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10490v1"
    },
    {
        "title": "Why Students Trade? The Analysis of Young Investors behavior",
        "authors": [
            "Jones Pontoh"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Interestingly the numbers of young traders in Jakarta Stock Exchange had been\nincreasing in recent years. Even in the middle of the global crisis caused by\ncovid19 pandemic, in December 2021 according to KSEI, Individual investors were\ndominated by young investors. Data presented by KSEI showed that 60 percent of\nthe investors listed in Indonesian Stock Exchange were young investors. Other\ndata shows that 28 percent of the investors listed were shockingly students. It\nwas interesting to study the behavior of young and Rookie investors at the\nbranch of stock market in Manado State University. Basically, they varied in\nhow to make decision to trade on the stock exchange. The problems were\ndiscussed by qualitative approach. Descriptive analysis was conducted prior to\ninterviews. Data will be collected through data observation techniques and\ninterviews. The study succeeded in investigating the investment behavior of\nyoung or Rookie investors at Manado State University in accordance with\ninvestment decision making and the perception of behavioral control. The\nperception of behavioral control greatly influenced investors decision making.\nStudents were greatly influenced by lecturer, friends and more experienced\ninvestors. The results of the interview provide information that before they\ndetermine their behavior, first they do stock analysis, both technical and\nfundamental analysis. These facts shows that students investors were well\nliterate.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04703v1"
    },
    {
        "title": "Searching for the \"Holy Grail\" of sponsorship-linked marketing: A\n  generalizable sponsorship ROI model",
        "authors": [
            "Jonathan A. Jensen"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Marketers routinely allocate a significant portion of their budget to\nsponsorship. However, isolating the return on investment from such efforts has\nremained a challenge. Thus, a dataset of more than 5,800 sponsorships is\nanalyzed using survival analysis approaches that utilizes the sponsor's renewal\nof the sponsorship as a proxy for positive ROI. In addition to its contribution\nto the sponsorship-linked marketing literature, the resulting model can be\nutilized by managers to generate predicted values relative to the sponsor's\nprobability of renewal and the ultimate duration of time the sponsor will\nremain with the property, representing a novel managerial contribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.09473v1"
    },
    {
        "title": "Gauge symmetries and the Higgs mechanism in Quantum Finance",
        "authors": [
            "Ivan Arraut"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  By using the Hamiltonian formulation, we demonstrate that the Merton-Garman\nequation emerges naturally from the Black-Scholes equation after imposing\ninvariance (symmetry) under local (gauge) transformations over changes in the\nstock price. This is the case because imposing gauge symmetry implies the\nappearance of an additional field, which corresponds to the stochastic\nvolatility. The gauge symmetry then imposes some constraints over the\nfree-parameters of the Merton-Garman Hamiltonian. Finally, we analyze how the\nstochastic volatility gets massive dynamically via Higgs mechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.03237v1"
    },
    {
        "title": "'Ergodicity Economics' is Pseudoscience",
        "authors": [
            "Alexis Akira Toda"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  In a series of papers, Ole Peters and his collaborators claim that the\n'conceptual basis of mainstream economic theory' is 'flawed' and that the\napproach they call 'ergodicity economics' gives 'reason to hope for a future\neconomic science that is more parsimonious, conceptually clearer and less\nsubjective' (Peters, 2019). This paper argues that 'ergodicity economics' is\npseudoscience because it has not produced falsifiable implications and should\nbe taken with skepticism.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.03275v1"
    },
    {
        "title": "Exploring Dynamic Asset Pricing within Bachelier Market Model",
        "authors": [
            "Nancy Asare Nyarko",
            "Bhathiya Divelgama",
            "Jagdish Gnawali",
            "Blessing Omotade",
            "Svetlozar Rachev",
            "Peter Yegon"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This paper delves into the dynamics of asset pricing within Bachelier market\nmodel, elucidating the representation of risky asset price dynamics and the\ndefinition of riskless assets.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.04059v1"
    },
    {
        "title": "Adjusting the nuclear reactor's neutron transport and diffusion theory\n  for an alternative description and modelling of postage or supplies delivery\n  processes",
        "authors": [
            "Nick P. Petropoulos"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  There seems to exist significant similarities between a reactor system and a\nsupply chain from collection to delivery. In the reactor case, neutrons are\ncontinuously produced and absorbed in nuclear fuel. In a supply system case,\nitems are continuously collected and continuously delivered to destinations.\nStable reactor operation is ensured by keeping the ratio of neutrons produced\nto neutrons absorbed in the reactor equal to one. Profitable and qualitative\nsupply operation is ensured by keeping the ratio of items delivered to items\ncollected as close to unity as possible. The analogy between the two systems is\nobvious. This text, which is provided as is and has not undergone any peer\nreview process, proposes transferring parts of the nuclear reactor's neutron\ntransport and diffusion theory to deterministically model supply processes. To\nthis end a set of assumptions and definitions are provided as needed along with\nthe introduction of reactions or interactions like collections, deliveries, and\nlosses occurring into the supply chain. The interaction rates are calculated\nwith the method used in reactors employing analogy factors and interactors with\nwhich the items in the chain interact. The main aim is to describe losses\nescape in steady state and in parallel estimate the analogy factors and\noptimize for the correct selection of the interactors pool. The model as\nproposed seems to be a tool for a different insight method into supply\nproblems. The model, if proven and applied, is discussed to be a strong\noptimization tool, which could deterministically pinpoint flaws in existing\nsupply systems or stochastically efficiently organize proposed supplied chains.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.07867v1"
    },
    {
        "title": "The Great Deception: A Comprehensive Study of Execution Strategies in\n  Corporate Share Buy-Backs",
        "authors": [
            "Michael Seigne",
            "Joerg Osterrieder"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We delve into the intricate world of share buy-backs, a strategic corporate\ncapital allocation tool that has gained significant prominence over the past\nfew decades. Despite being the subject of extensive research and debate, the\nexecution phase of these transactions remains an underexplored area. This lack\nof research into the execution phase is surprising, especially when compared to\nthe extensive literature on other capital allocation decisions, such as\nacquisition pricing. We aim to shed light on the execution practices of share\nbuy-backs, highlighting challenges and opportunities that arise and presenting\na comparative analysis with other aspects of capital allocation. From an\noutsider's perspective, this paper analyses the seemingly disparate practices\nand aims to uncover the \"dark arts\" of execution. The intention is to assist\ncorporations, investors, and regulators in better understanding the intricacies\nof executing share buy-backs, emphasizing the need for a cost-efficient and\nregulatory compliant service for corporations trading their own listed\nsecurities. We question the seemingly disproportionate fees charged to\ncorporations for similar or even inferior outcomes, compared to retail and\ninstitutional investors. We illustrate potential inefficiencies and frictional\ncosts in the current execution phase. The examples highlight the need for\ngreater transparency and fairness in share buy-back executions, advocating for\nmore equitable processes that benefit all stakeholders in the capital markets\necosystem. With the surge in regulatory attention and political pressure fueled\nby the rising prominence of buy-backs and evolving ESG considerations, it is\ncrucial that we enhance understanding of this key area. This paper seeks to\nfoster dialogue and encourage transparency, thus promoting the efficient use of\nresources in capital markets, and ultimately, benefiting shareholders.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.09617v1"
    },
    {
        "title": "Dynamic Function Market Maker",
        "authors": [
            "Arman Abgaryan",
            "Utkarsh Sharma"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Decentralised automated market makers (AMMs) have gained significant\nattention recently. We propose an adaptive and automated Dynamic Function\nMarket Maker (DFMM) that addresses challenges in this space. Our DFMM protocol\nincludes a data aggregator and an order routing mechanism. It synchronises\nprice-sensitive market information, asserting the principle of one price, and\nensuring market efficiency. The data aggregator includes a virtual order book,\nasserting efficient asset pricing by staying synchronised with information from\nexternal venues, including competitors. The protocol's rebalancing and order\nrouting method optimises inventory risk through arbitrageurs, who are more\nlikely to assist DFMM, enhancing protocol stability. DFMM incorporates\nprotective buffers with non-linear derivative instruments to manage risk and\nmitigate losses caused by market volatility. The protocol employs an\nalgorithmic accounting-asset, connecting all pools and resolving the issue of\nsegregated pools and risk transfer. The settlement process is entirely\nprotocol-driven, maximising risk management efficiency, and eliminating\nsubjective market risk assessments. In essence, DFMM offers a fully automated,\ndecentralised, and robust solution for automated market making. It aims to\nprovide long-term viability and stability in an asset class that demands\nrobustness.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.13624v1"
    },
    {
        "title": "Exploiting the dynamics of commodity futures curves",
        "authors": [
            "Robert J Bianchi",
            "John Hua Fan",
            "Joelle Miffre",
            "Tingxi Zhang"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  The Nelson-Siegel framework is employed to model the term structure of\ncommodity futures prices. Exploiting the information embedded in the level,\nslope and curvature parameters, we develop novel investment strategies that\nassume short-term continuation of recent parallel, slope or butterfly movements\nof futures curves. Systematic strategies based on the change in the slope\ngenerate significant profits that are unrelated to previously documented risk\nfactors and can survive reasonable transaction costs. Further analysis\ndemonstrates that the profitability of the slope strategy increases with\ninvestor sentiment and is in part a compensation for the drawdowns incurred\nduring economic slowdowns. The profitability can also be magnified through\ntiming and persists under alternative specifications of the Nelson-Siegel\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00383v1"
    },
    {
        "title": "An Empirical Study on the Holiday Effect of China's Time-Honored\n  Companies",
        "authors": [
            "Xianyang Li",
            "Jiayi Xu",
            "Haoxuan Xu",
            "Yunxuan Ma",
            "Yu Zhong",
            "Lei Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  The stock segment of China's time-honored brand enterprises has an important\nposition in our securities stock market. The holiday effect is one of the\nmarket anomalies that occur in the securities market, which refers to the\nphenomenon that the stock market has significantly different returns than other\ntrading days around festivals. The study of the holiday effect of China's\ntime-honored brand enterprises can provide fresh ideas for the revitalization\nof our time-honored brands and the revitalization of time-honored enterprises.\nThis paper takes listed companies of China's time-honored brand enterprises as\nthe research object and focuses on the impact of the holiday effect on listed\ncompanies of China's time-honored brands with the help of the event study, and\nempirically analyses the changes in the return of listed companies of China\ntime-honored brands during the Spring Festival period from 2012 to 2021. The\nempirical results reveal that: the time-honored brand concept stocks have a\nsignificant post-holiday effect during the Chinese New Year period, the\ntime-honored alcoholic beverage enterprises are more sensitive to the Chinese\nNew Year reflection, while the holiday effect of the time-honored\npharmaceutical manufacturing enterprises is not significant.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00702v1"
    },
    {
        "title": "Digital Real Estate in the Metaverse: An Empirical Analysis of Retail\n  Investor Motivations",
        "authors": [
            "Lennart Ante",
            "Friedrich-Philipp Wazinski",
            "Aman Saggu"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This paper investigates retail investor motivations for digital real estate\nownership in the crypto-metaverse. Utilizing a detailed financial behavior\nsurvey of metaverse landowners' intrinsic and extrinsic motivations, we apply\nprincipal components analysis to uncover four distinct motivational groups: (1)\nAesthetics and Identity, (2) Social and Community, (3) Speculation and\nInvestment, and (4) Innovation and Technology. Our findings reveal that age,\neducation, investment knowledge, risk-taking, and impulsivity significantly\ninfluence investor group membership. This research provides valuable insights\nto investors and developers, underscoring the potential of a platform to\nattract retail investors with speculative intentions, engagement longevity, and\npassive or active trading characteristics, contingent on unique\ncrypto-metaverse attributes.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.10309v1"
    },
    {
        "title": "Capital Structure Dynamics and Financial Performance in Indian Banks (An\n  Analysis of Mergers and Acquisitions)",
        "authors": [
            "Kurada T S S Satyanarayana",
            "Addada Narasimha Rao",
            "Kumpatla jaya surya"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This research investigates the multifaceted relationship underlying capital\nstructure dynamics along with financial performance as a result of mergers and\nacquisitions, or M&As, in Indian banks. In the face of increasing competition,\nbanks have deliberately embraced M&A as a strategy of improving commercial\nprospects and maintaining financial stability. The primary goal of this study\nis to examine the changes in the capital framework and financial results of\nbanks before and after M&A transactions. The investigation, which employs a\npaired t-test as a method of statistical analysis, is based on a review of\nannual reports from selected banks over a two-year period before and after M&A\ntransactions. The paired t-test approach allows for a thorough statistical\nanalysis of interconnected datasets, revealing the subtle influence of M&A\nattempts on both bank financial performance as well as capital structure\ndynamics. The study's findings have the potential to add to the current body of\nknowledge on organisational planning, managing finances, and capital structure\noptimisation. The research has practical significance for financial companies,\nlegislators, and scholars interested in understanding the profound effects of\nM&A inside the arena of financial institutions that operate within fiercely\ncompetitive landscapes because it provides comprehensive insights regarding the\ncomplex consequences of banking merger and acquisition (M&A) deals on capital\nstructure as well as financial performance. Finally, the goal of this research\nis to provide the banking sector with educated decision-making capabilities and\nstrategic guidance to businesses facing heightened competition while coping\nwith the complexities of capital structure.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.16054v1"
    },
    {
        "title": "A hidden Markov model for statistical arbitrage in international crude\n  oil futures markets",
        "authors": [
            "Viviana Fanelli",
            "Claudio Fontana",
            "Francesco Rotondi"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  In this work, we study statistical arbitrage strategies in international\ncrude oil futures markets. We analyse strategies that extend classical pairs\ntrading strategies, considering the two benchmark crude oil futures (Brent and\nWTI) together with the newly introduced Shanghai crude oil futures. We document\nthat the time series of these three futures prices are cointegrated and we\nmodel the resulting cointegration spread by a mean-reverting regime-switching\nprocess modulated by a hidden Markov chain. By relying on our stochastic model\nand applying online filter-based parameter estimators, we implement and test a\nnumber of statistical arbitrage strategies. Our analysis reveals that\nstatistical arbitrage strategies involving the Shanghai crude oil futures are\nprofitable even under conservative levels of transaction costs and over\ndifferent time periods. On the contrary, statistical arbitrage strategies\ninvolving the three traditional crude oil futures (Brent, WTI, Dubai) do not\nyield profitable investment opportunities. Our findings suggest that the\nShanghai futures, which has already become the benchmark for the Chinese\ndomestic crude oil market, can be a valuable asset for international investors.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.00875v2"
    },
    {
        "title": "Common Firm-level Investor Fears: Evidence from Equity Options",
        "authors": [
            "Jozef Barunik",
            "Mattia Bevilacqua",
            "Michael Ellington"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We identify a new type of risk, common firm-level investor fears, from\ncommonalities within the cross-sectional distribution of individual stock\noptions. We define firm-level fears that link with upward price movements as\ngood fears, and those relating to downward price movements as bad fears. Such\ninformation is different to market fears that we extract from index options.\nStocks with high sensitivities to common firm-level investor fears earn lower\nreturns, with investors demanding a higher compensation for exposure to common\nbad fears relative to common good fears. Risk premium estimates for common bad\nfears range from -5.63% to -4.92% per annum.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.03968v1"
    },
    {
        "title": "Handling missing data in Burundian sovereign bond market",
        "authors": [
            "Irène Irakoze",
            "Rédempteur Ntawiratsa",
            "David Niyukuri"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Constructing an accurate yield curve is essential for evaluating financial\ninstruments and analyzing market trends in the bond market. However, in the\ncase of the Burundian sovereign bond market, the presence of missing data poses\na significant challenge to accurately constructing the yield curve. In this\npaper, we explore the limitations and data availability constraints specific to\nthe Burundian sovereign market and propose robust methodologies to effectively\nhandle missing data. The results indicate that the Linear Regression method,\nand the Previous value method perform consistently well across variables,\napproximating a normal distribution for the error values. The non parametric\nMissing Value Imputation using Random Forest (miss-Forest) method performs well\nfor coupon rates but poorly for bond prices, and the Next value method shows\nmixed results. Ultimately, the Linear Regression (LR) method is recommended for\nimputing missing data due to its ability to approximate normality and\npredictive capabilities. However, filling missing values with previous values\nhas high accuracy, thus, it will be the best choice when we have less\ninformation to be able to increase accuracy for LR. This research contributes\nto the development of financial products, trading strategies, and overall\nmarket development in Burundi by improving our understanding of the yield curve\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.17379v1"
    },
    {
        "title": "Modeling the yield curve of Burundian bond market by parametric models",
        "authors": [
            "Rédempteur Ntawiratsa",
            "David Niyukuri",
            "Irène Irakoze",
            "Menus Nkurunziza"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  The term structure of interest rates (yield curve) is a critical facet of\nfinancial analytics, impacting various investment and risk management\ndecisions. It is used by the central bank to conduct and monitor its monetary\npolicy. That instrument reflects the anticipation of inflation and the risk by\ninvestors. The rates reported on yield curve are the cornerstone of valuation\nof all assets. To provide such tool for Burundi financial market, we collected\nthe auction reports of treasury securities from the website of the Central Bank\nof Burundi. Then, we computed the zero-coupon rates, and estimated actuarial\nrates of return by applying the Nelson-Siegel and Svensson models. This paper\nconducts a rigorous comparative analysis of these two prominent parametric\nyield curve models and finds that the Nelson-Siegel model is the optimal choice\nfor modeling the Burundian yield curve. The findings contribute to the body of\nknowledge on yield curve modeling, enhancing its precision and applicability in\nfinancial markets. Furthermore, this research holds implications for investment\nstrategies, risk management, second market pricing, financial decision-making,\nand the forthcoming establishment of the Burundian stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00321v2"
    },
    {
        "title": "A systematic review of early warning systems in finance",
        "authors": [
            "Ali Namaki",
            "Reza Eyvazloo",
            "Shahin Ramtinnia"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Early warning systems (EWSs) are critical for forecasting and preventing\neconomic and financial crises. EWSs are designed to provide early warning signs\nof financial troubles, allowing policymakers and market participants to\nintervene before a crisis expands. The 2008 financial crisis highlighted the\nimportance of detecting financial distress early and taking preventive measures\nto mitigate its effects. In this bibliometric review, we look at the research\nand literature on EWSs in finance. Our methodology included a comprehensive\nexamination of academic databases and a stringent selection procedure, which\nresulted in the final selection of 616 articles published between 1976 and\n2023. Our findings show that more than 90\\% of the papers were published after\n2006, indicating the growing importance of EWSs in financial research.\nAccording to our findings, recent research has shifted toward machine learning\ntechniques, and EWSs are constantly evolving. We discovered that research in\nthis area could be divided into four categories: bankruptcy prediction, banking\ncrisis, currency crisis and emerging markets, and machine learning forecasting.\nEach cluster offers distinct insights into the approaches and methodologies\nused for EWSs. To improve predictive accuracy, our review emphasizes the\nimportance of incorporating both macroeconomic and microeconomic data into EWS\nmodels. To improve their predictive performance, we recommend more research\ninto incorporating alternative data sources into EWS models, such as social\nmedia data, news sentiment analysis, and network analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00490v1"
    },
    {
        "title": "Market Crowds' Trading Behaviors, Agreement Prices, and the Implications\n  of Trading Volume",
        "authors": [
            "Leilei Shi",
            "Bing Han",
            "Yingzi Zhu",
            "Liyan Han",
            "Yiwen Wang",
            "Yan Piao"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  It has been long that literature in financial academics focuses mainly on\nprice and return but much less on trading volume. In the past twenty years, it\nhas already linked both price and trading volume to economic fundamentals, and\nexplored the behavioral implications of trading volume such as investor's\nattitude toward risks, overconfidence, disagreement, and attention etc.\nHowever, what is surprising is how little we really know about trading volume.\nHere we show that trading volume probability represents the frequency of market\ncrowd's trading action in terms of behavior analysis, and test two adaptive\nhypotheses relevant to the volume uncertainty associated with price in China\nstock market. The empirical work reveals that market crowd trade a stock in\nefficient adaptation except for simple heuristics, gradually tend to achieve\nagreement on an outcome or an asset price widely on a trading day, and generate\nsuch a stationary equilibrium price very often in interaction and competition\namong themselves no matter whether it is highly overestimated or\nunderestimated. This suggests that asset prices include not only a fundamental\nvalue but also private information, speculative, sentiment, attention, gamble,\nand entertainment values etc. Moreover, market crowd adapt to gain and loss by\ntrading volume increase or decrease significantly in interaction with\nenvironment in any two consecutive trading days. Our results demonstrate how\ninteraction between information and news, the trading action, and return\noutcomes in the three-term feedback loop produces excessive trading volume\nwhich includes various internal and external causes.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.05322v1"
    },
    {
        "title": "From the Top Down: Does Corruption Affect Performance?",
        "authors": [
            "Maurizio La Rocca",
            "Tiziana La Rocca",
            "Francesco Fasano",
            "Javier Sanchez-Vidal"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Corruption, fraud, and unethical activities have emerged as significant\nobstacles to global economic, political, and social progress. Although many\nempirical studies have focused on country-level corruption metrics, this study\nis the first to utilize a substantial international dataset to assess the\neffects of illicit and unethical managerial practices on firm performance.\nEmploying cross-sectional data, this research examines the influence of\ncorruption on corporate outcomes. Our definition of corruption evaluates the\ndegree to which managers engage in mismanagement, misconduct, or corrupt\nactivities. The repercussions for corporate governance, especially concerning\nthe process of appointing managers, are both crucial and strategic.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.20028v1"
    },
    {
        "title": "Impact of Investing Characteristics on Financial Performance of\n  Individual Investors: An Exploratory Study",
        "authors": [
            "Poompak Kusawat",
            "Nopadol Rompho"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This exploratory study examines which investing characteristics determine\nsuccess in an equity market. Based on data from 403 respondents, exploratory\nfactor analysis results in 13 factors: middle/long time horizon, qualitative\nanalyst, open-minded/disciplined, organized, emotional stability, na\\\"ive,\ngrowth stock, concentrated portfolio, contrarian, value stock, globalized,\nintrinsic value, and price-independent. Multiple linear regression of\nindividual investors' excess return on these factors show statistically\nsignificant relationship. These results deepen our knowledge on what sort of\ninvesting characteristics are required to survive in equity markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.00384v1"
    },
    {
        "title": "Analysis of Decentralization in Governance and Financial Efficiency of\n  Companies: Studying the Relationship in the Field of Decentralized Finance",
        "authors": [
            "Kirill Kolmykov"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Currently, the advantages of decentralization through blockchain technology\nin the financial sector are actively discussed. In this article, we investigate\nthe decentralization in the governance of Decentralized Autonomous\nOrganizations (DAO) using the Gini coefficient as an indicator of inequality\namong the token owners. This metric is analyzed in the context of Return on\nInvestment (ROI) for companies in the decentralized finance (DeFi) sector. Our\ngoal is to understand whether the level of \"real\" decentralization in\nblockchain-based governance affects financial efficiency, and to explore the\nbenefits and possible limitations of such an approach. This analysis allows for\na deeper understanding of the significance and impact of decentralization on\nthe functioning and productivity of organizations in the DeFi sector, and to\ndetermine the extent to which this impact is positively or negatively reflected\nin their success and profitability. Additionally, the results of this analysis\nwill provide a fuller understanding of the dynamics and potential of blockchain\nfor organization governance.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.02434v1"
    },
    {
        "title": "Intermediating DFMM Asset (IDA)",
        "authors": [
            "Arman Abgaryan",
            "Utkarsh Sharma"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  The Dynamic Function Market Maker (DFMM) introduced a fully automated\nframework for operating a multi-asset market, wherein an algorithmic accounting\nasset was used to connect different liquidity pools and ensure efficient\nrebalancing of risks, and internal accounting processes. In the DFMM design,\nthis asset was not tradaeble; however, in this work, we explore the\ncharacteristics of this asset, if it were to be made tradeable. Named the\nIntermediating DFMM Asset (IDA), this asset serves as a unit of account in\ncross-chain finance, functioning as an intermediating asset for predictable\nbudgeting, and efficient multichain transfers and settlements. Harnessing its\nrobust liquidity as the key counterpart asset in DFMM, it achieves capital\nefficiency through the strategic repurposing of its asset base, while\nsimultaneously mitigating risk via the dynamic optimisation of its\nmulticollateral foundation. We outline key characteristics of the proposed\nasset, unique risk mitigation aspects enabled by the adopting AMM (DFMM), and\ncontrol levers enabling the protocol's tactical asset and liability management\ntoolkit to harmonise the asset's objectives with its real-world realisation,\nthrough a novel prudential market operation to incentivise productive use of a\nfinite asset and dynamic AMM fee to ensure alignment of behaviours. The\nproposed design has the potential to harmonise the interests of diverse market\nparticipants, leading to synergetic reactions to informational flow, aiding IDA\nprotocol in achieving its objectives.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.05234v1"
    },
    {
        "title": "Audit fees in auditor switching",
        "authors": [
            "Sarit Agami"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  The auditor work is examining that a company's financial statements\nfaithfully reflect its financial situation. His wage, the audit fees, are not\nfixed among all companies, but can be affected by the financial and structural\ncharacteristics of the company, as well as the characteristics of the firm he\nbelongs to. Another factor that may affect his wage in an auditor switching,\nwhich can be resulted from changes in the company that may influence the fees.\nThis paper examines the effect nature of the auditor switching on his wage, and\nthe factors of the company characteristics and the economy data which determine\nthe wage at switching. A product of the research are tools for predicting and\nevaluating the auditor wage at switching. These tools are important for the\nauditor himself, but also for the company manager to correctly determine the\nwage due to the possibility that the quality of the audit work depends on its\nfees. Two main results are obtained. First, the direction of the wage change in\nthe switching year depends on the economic stability of the economy. Second,\nthe switching effect on the direction and the change size in wage depends on\nthe change size in the company characteristics before and after switching - a\nlarge change versus a stable one. We get that forecasting the change size in\nwage for companies with a larger change is their characteristics is paralleled\nto forecasting a wage increasing. And vice versa, forecasting the change size\nin wage for companies with a stable change in their characteristics is\nparalleled to forecasting a wage decreasing. But, whereas the former can be\nachieved based on the company characteristics and macroeconomics factors, the\npredictably of these characteristics and factors is negligible for the letter.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.08250v1"
    },
    {
        "title": "Bank Performance Determinants: State of the Art and Future Research\n  Avenues",
        "authors": [
            "Anas Azzabi",
            "Younes Lahrichi"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Banks' performance is an important topic for both professionals and\nresearchers. Given the important literature on this subject, this paper aims to\nbring an up-to-date and organized review of literature on the determinants of\nbanks performance. This paper discusses the main approaches that molded the\ndebate on banks performance and their main determinants. An in-depth\nunderstanding of these latter may allow on the one hand, bank managers and\nregulators to improve the sector efficiency and to deal with the new trends\nshaping the future of their industry and on the other hand, academicians to\nenrich research and knowledge on this field. Through the analysis of 54 studies\npublished in 42 peer-reviewed journals, we show that despite the importance of\nthe existent literature, the subject of bank performance factors did not reveal\nall its secrets and still constitute a fertile field for critical debates,\nespecially since the COVID-19 and the increasingly pressing rise in power of\ndigital transformation and artificial intelligence in general and FinTechs in\nparticular. The study concludes by suggesting new promising research avenues.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.08617v3"
    },
    {
        "title": "Does religiosity influence corporate greenwashing behavior?",
        "authors": [
            "Mathieu Gomes",
            "Sylvain Marsat",
            "Jonathan Peillex",
            "Guillaume Pijourlet"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We analyze the influence of religious social norms on corporate greenwashing\nbehavior. Specifically, we focus on a specific form of greenwashing: selective\ndisclosure. Using a large sample of US firms between 2005 and 2019, we show\nthat firms located in counties where religious adherence is high are less\nlikely to engage in greenwashing. We also find that a stronger religious\nadherence within the county in which a company is located reduces the magnitude\nof greenwashing, when observed. We further analyze the mechanism underlying\nthis relationship and show that religious adherence impacts greenwashing\nbehaviors through the channel of risk aversion. A comprehensive set of\nrobustness tests aimed at addressing potential endogeneity concerns confirms\nthat religion is a relevant driver of corporate greenwashing behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14515v1"
    },
    {
        "title": "A Statistical Field Perspective on Capital Allocation and Accumulation",
        "authors": [
            "Pierre Gosselin",
            "Aïleen Lotz"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This paper provides a general method to translate a standard economic model\nwith a large number of agents into a field-formalism model. This formalism\npreserves the system's interactions and microeconomic features at the\nindividual level but reveals the emergence of collective states.We apply this\nmethod to a simple microeconomic framework of investors and firms. Both macro\nand micro aspects of the formalism are studied.At the macro-scale, the field\nformalism shows that, in each sector, three patterns of capital accumulation\nmay emerge. A distribution of patterns across sectors constitute a collective\nstate. Any change in external parameters or expectations in one sector will\naffect neighbouring sectors, inducing transitions between collective states and\ngenerating permanent fluctuations in patterns and flows of capital. Although\nchanges in expectations can cause abrupt changes in collective states,\ntransitions may be slow to occur. Due to its relative inertia, the real economy\nis bound to be more affected by these constant variations than the financial\nmarkets.At the micro-scale we compute the transition functions of individual\nagents and study their probabilistic dynamics in a given collective state, as a\nfunction of their initial state. We show that capital accumulation of an\nindividual agent depends on various factors. The probability associated with\neach firm's trajectories is the result of several contradictory effects: the\nfirm tends to shift towards sectors with the greatest long-term return, but\nmust take into account the impact of its shift on its attractiveness for\ninvestors throughout its trajectory. Since this trajectory depends largely on\nthe average capital of transition sectors, a firm's attractiveness during its\nrelocation depends on the relative level of capital in those sectors. Moreover,\nthe firm must also consider the effects of competition in the intermediate\nsectors that tends to oust under-capitalized firm towards sectors with lower\naverage capital. For investors, capital allocation depends on their short and\nlong-term returns and investors will tend to reallocate their capital to\nmaximize both. The higher their level of capital, the stronger the\nre-allocation will be.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.16173v1"
    },
    {
        "title": "Displaying risk in mergers: a diagrammatic approach for exchange ratio\n  determination",
        "authors": [
            "Alessandra Mainini",
            "Enrico Moretto",
            "Daniela Visetti"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This article extends, in a stochastic setting, previous results in the\ndetermination of feasible exchange ratios for merging companies. A first\noutcome is that shareholders of the companies involved in the merging process\nface both an upper and a lower bounds for acceptable exchange ratios. Secondly,\nin order for the improved `bargaining region' to be intelligibly displayed, the\ndiagrammatic approach developed by Kulpa is exploited.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.02681v1"
    },
    {
        "title": "Proof of Efficient Liquidity: A Staking Mechanism for Capital Efficient\n  Liquidity",
        "authors": [
            "Arman Abgaryan",
            "Utkarsh Sharma",
            "Joshua Tobkin"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The Proof of Efficient Liquidity (PoEL) protocol, designed for specialised\nProof of Stake (PoS) consensus-based blockchains that incorporate intrinsic\nDeFi applications, aims to support sustainable liquidity bootstrapping and\nnetwork security. This concept seeks to efficiently utilise budgeted staking\nrewards to attract and sustain liquidity through a risk-structuring engine and\nincentive allocation strategy, both of which are designed to maximise capital\nefficiency. The proposed protocol serves the dual objective of: (i) capital\ncreation by attracting risk capital efficiently and maximising its operational\nutility for intrinsic DeFi applications, thereby asserting sustainability; and\n(ii) enhancing the adopting blockchain network's economic security by\naugmenting their staking (PoS) mechanism with a harmonious layer seeking to\nattract a diversity of digital assets. Finally, the protocol's conceptual\nframework, as detailed in the appendix, is extended to encompass service fee\ncredits. This extension capitalises on the network's auxiliary services to\ndisperse incentives and attract liquidity, ensuring the network achieves and\nmaintains the critical usage threshold essential for its sustained operational\nviability and progressive growth.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.04521v2"
    },
    {
        "title": "Token vs Equity for Startup Financing",
        "authors": [
            "Guangye Cao"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Why would a blockchain-based startup and its venture capital investors choose\nto finance by issuing tokens instead of equity? What would be their rates of\nreturn for each asset? This paper focuses on the liquidity difference between\nthe two fundraising methods. I build a three-period model of an entrepreneur,\ntwo types of investors, and users. Some investors have unforeseen liquidity\nneeds in the middle period that can only be met with tokens. The entrepreneur\nobtains higher payoff by issuing tokens instead of equity, and the payoff\ndifference increases with investors risk-aversion and need for liquidity in the\nmiddle period, as well as the depth of the token market.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.04662v1"
    },
    {
        "title": "Prioritizing Investments in Cybersecurity: Empirical Evidence from an\n  Event Study on the Determinants of Cyberattack Costs",
        "authors": [
            "Daniel Celeny",
            "Loïc Maréchal",
            "Evgueni Rousselot",
            "Alain Mermoud",
            "Mathias Humbert"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Along with the increasing frequency and severity of cyber incidents,\nunderstanding their economic implications is paramount. In this context, listed\nfirms' reactions to cyber incidents are compelling to study since they (i) are\na good proxy to estimate the costs borne by other organizations, (ii) have a\ncritical position in the economy, and (iii) have their financial information\npublicly available. We extract listed firms' cyber incident dates and\ncharacteristics from newswire headlines. We use an event study over 2012--2022,\nusing a three-day window around events and standard benchmarks. We find that\nthe magnitude of abnormal returns around cyber incidents is on par with\nprevious studies using newswire or alternative data to identify cyber\nincidents. Conversely, as we adjust the standard errors accounting for\nevent-induced variance and residual cross-correlation, we find that the\npreviously claimed significance of abnormal returns vanishes. Given these\nresults, we run a horse race of specifications, in which we test for the\nmarginal effects of type of cyber incidents, target firm sector, periods, and\ntheir interactions. Data breaches are the most detrimental incident type with\nan average loss of -1.3\\% or (USD -1.9 billion) over the last decade. The\nhealth sector is the most sensitive to cyber incidents, with an average loss of\n-5.21\\% (or USD -1.2 billion), and even more so when these are data breaches.\nInstead, we cannot show any time-varying effect of cyber incidents or a\nspecific effect of the type of news as had previously been advocated.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.04773v1"
    },
    {
        "title": "Manager Characteristics and SMEs' Restructuring Decisions: In-Court vs.\n  Out-of-Court Restructuring",
        "authors": [
            "Rachid Achbah"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This study aims to empirically investigate the impact of managers'\ncharacteristics on their choice between in-court and out-of-court\nrestructuring. Based on the theory of upper echelons, we tested the preferences\nof 342 managers of financially distressed French firms regarding restructuring\ndecisions. The overall findings of this study provide empirical support for the\nupper echelons theory. Specifically, managers with a long tenure and those with\na high level of education are less likely to restructure before the court and\nare more likely to restructure privately. The findings also indicate that\nmanagers' age and gender do not significantly affect their choice between\nin-court and out-of-court restructuring. This study contributes to the\nliterature on bankruptcy and corporate restructuring by turning the focus from\nfirm characteristics to manager characteristics to explain restructuring\ndecisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18135v1"
    },
    {
        "title": "Testing Business Cycle Theories: Evidence from the Great Recession",
        "authors": [
            "Bo Li"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Empirical business cycle studies using cross-country data usually cannot\nachieve causal relationships while within-country studies mostly focus on the\nbust period. We provide the first causal investigation into the boom period of\nthe 1999-2010 U.S. cross-metropolitan business cycle. Using a novel research\ndesign, we show that credit expansion in private-label mortgages causes a\ndifferentially stronger boom (2000-2006) and bust (2007-2010) cycle in the\nhouse-related industries in the high net-export-growth areas. Most importantly,\nour unique research design enables us to perform the most comprehensive tests\non theories (hypotheses) regarding the business cycle. We show that the\nfollowing theories (hypotheses) cannot explain the cause of the 1999-2010 U.S.\nbusiness cycle: the speculative euphoria hypothesis, the real business cycle\ntheory, the collateral-driven credit cycle theory, the business uncertainty\ntheory, and the extrapolative expectation theory.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.04104v1"
    },
    {
        "title": "Capital Structure Adjustment Speed and Expected Returns: Examination of\n  Information Asymmetry as a Moderating Role",
        "authors": [
            "Masoud Taherinia",
            "Mehrdad Matin",
            "Jamal Valipour",
            "Kavian Abdolahi",
            "Peyman Shouryabi",
            "Mohammad Mahdi Barzegar"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Shareholders' expectations of stock returns and fluctuations are constantly\nchanging due to restrictions in financial status and undesirable capital\nstructure, which constrain managers to limit the changes in price trends in\norder to cover the risk instigated and infused by the unfavorable situation.\nThe present research examines the moderating impact of information asymmetry on\nthe relationship between capital structure adjustment and expected returns. The\ndata from 120 companies approved in the Tehran Stock Exchange were extracted,\nand a hybrid data regression model was used to test the research hypotheses.\nFindings indicate that the capital structure adjustment speed correlates with\nthe expected returns. Moreover, the information asymmetry positively affects\nthe relationship between capital structure adjustment speed and expected\nreturns.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06035v1"
    },
    {
        "title": "Study of the Impact of the Big Data Era on Accounting and Auditing",
        "authors": [
            "Yuxiang Sun",
            "Jingyi Li",
            "Mengdie Lu",
            "Zongying Guo"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Big data revolutionizes accounting and auditing, offering deep insights but\nalso introducing challenges like data privacy and security. With data from IoT,\nsocial media, and transactions, traditional practices are evolving.\nProfessionals must adapt to these changes, utilizing AI and machine learning\nfor efficient data analysis and anomaly detection. Key to overcoming these\nchallenges are enhanced analytics tools, continuous learning, and industry\ncollaboration. By addressing these areas, the accounting and auditing fields\ncan harness big data's potential while ensuring accuracy, transparency, and\nintegrity in financial reporting. Keywords: Big Data, Accounting, Audit, Data\nPrivacy, AI, Machine Learning, Transparency.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.07180v1"
    },
    {
        "title": "The UK Universities Superannuation Scheme valuations 2014-2023: gilt\n  yield dependence, self-sufficiency and metrics",
        "authors": [
            "Jackie Grant"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This review considers the Universities Superannuation Scheme (USS) valuations\nfrom 2014 to 2023. USS is a 70-80 billion GBP Defined Benefit pension scheme\nwith over 500,000 members who are employed (or have been employed) at around 70\nUK universities. Disputes over USS have led to a decade of industrial action.\nNew results are presented showing the high dependence of USS pension\ncontributions on the return from UK government bonds (the gilt yield). The two\nconditions of the USS-specific 'self-sufficiency' (SfS) definition are\nexamined. USS data are presented along with new analysis. It is shown that the\nsecond SfS condition of 'maintaining a high funding ratio' dominates USS\nmodelling to amplify gilt yield dependence, inflating the SfS liabilities\nbeyond the regulatory requirements, and leading to excessive prudence. The Red,\nAmber and Green status of USS metrics 'Actual' and 'Target' Reliance are also\nexamined. It is shown that Target Reliance tethers the cost of future pensions\nto the SfS definition and that Actual Reliance can simultaneously be Green and\nRed. Implications for regulatory intervention are considered. An aim of this\nreview is to support evidence-based decision making and consensus building.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.08811v2"
    },
    {
        "title": "Voting Participation and Engagement in Blockchain-Based Fan Tokens",
        "authors": [
            "Lennart Ante",
            "Aman Saggu",
            "Benjamin Schellinger",
            "Friedrich Wazinksi"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This paper investigates the potential of blockchain-based fan tokens, a class\nof crypto asset that grants holders access to voting on club decisions and\nother perks, as a mechanism for stimulating democratized decision-making and\nfan engagement in the sports and esports sectors. By utilizing an extensive\ndataset of 3,576 fan token polls, we reveal that fan tokens engage an average\nof 4,003 participants per poll, representing around 50% of token holders,\nunderscoring their relative effectiveness in boosting fan engagement. The\nanalyses identify significant determinants of fan token poll participation,\nincluding levels of voter (dis-)agreement, poll type, sports sectors,\ndemographics, and club-level factors. This study provides valuable stakeholder\ninsights into the current state of adoption and voting trends for fan token\npolls. It also suggests strategies for increasing fan engagement, thereby\noptimizing the utility of fan tokens in sports. Moreover, we highlight the\nbroader applicability of fan token principles to any community, brand, or\norganization focused on customer engagement, suggesting a wider potential for\nthis digital innovation.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.08906v1"
    },
    {
        "title": "Piercing the Veil of TVL: DeFi Reappraised",
        "authors": [
            "Yichen Luo",
            "Yebo Feng",
            "Jiahua Xu",
            "Paolo Tasca"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Total value locked (TVL) is widely used to measure the size and popularity of\ndecentralized finance (DeFi). However, TVL can be easily manipulated and\ninflated through \"double counting\" activities such as wrapping and leveraging.\nAs existing methodologies addressing double counting are inconsistent and\nflawed, we propose a new framework, termed \"total value redeemable (TVR)\", to\nassess the true underlying value of DeFi. Our formal analysis reveals how\nDeFi's complex network spreads financial contagion via derivative tokens,\nincreasing TVL's sensitivity to external shocks. To quantify double counting,\nwe construct the DeFi multiplier, which mirrors the money multiplier in\ntraditional finance (TradFi). This measurement reveals substantial double\ncounting in DeFi, finding that the gap between TVL and TVR reached \\$139.87\nbillion during the peak of DeFi activity on December 2, 2021, with a TVL-to-TVR\nratio of approximately 2. We conduct sensitivity tests to evaluate the\nstability of TVL compared to TVR, demonstrating the former's significantly\nhigher level of instability than the latter, especially during market\ndownturns: A 25% decline in the price of Ether (ETH) leads to a \\$1 billion\ngreater non-linear decrease in TVL compared to TVR via the liquidations\ntriggered by derivative tokens. We also document that the DeFi money multiplier\nis positively correlated with crypto market indicators and negatively\ncorrelated with macroeconomic indicators. Overall, our findings suggest that\nTVR is more reliable and stable than TVL.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.11745v4"
    },
    {
        "title": "Joint Liability Model with Adaptation to Climate Change",
        "authors": [
            "Jiayue Zhang",
            "Ken Seng Tan",
            "Tony S. Wirjanto",
            "Lysa Porth"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This paper extends the application of ESG score assessment methodologies from\nlarge corporations to individual farmers' production, within the context of\nclimate change. Our proposal involves the integration of crucial agricultural\nsustainability variables into conventional personal credit evaluation\nframeworks, culminating in the formulation of a holistic sustainable credit\nrating referred to as the Environmental, Social, Economics (ESE) score. This\nESE score is integrated into theoretical joint liability models, to gain\nvaluable insights into optimal group sizes and individual-ESE score\nrelationships. Additionally, we adopt a mean-variance utility function for\nfarmers to effectively capture the risk associated with anticipated profits.\nThrough a set of simulation exercises, the paper investigates the implications\nof incorporating ESE scores into credit evaluation systems, offering a nuanced\ncomprehension of the repercussions under various climatic conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13818v2"
    },
    {
        "title": "Transforming Investment Strategies and Strategic Decision-Making:\n  Unveiling a Novel Methodology for Enhanced Performance and Risk Management in\n  Financial Markets",
        "authors": [
            "Tian Tian",
            "Ricky Cooper",
            "Jiahao Deng",
            "Qingquan Zhang"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This paper introduces a novel methodology for index return forecasting,\nblending highly correlated stock prices, advanced deep learning techniques, and\nintricate factor integration. Departing from conventional cap-weighted\napproaches, our innovative framework promises to reimagine traditional\nmethodologies, offering heightened diversification, amplified performance\ncapture, and nuanced market depiction. At its core lies the intricate\nidentification of highly correlated company clusters, fueling predictive\naccuracy and robustness. By harnessing these interconnected constellations, we\nunlock a profound comprehension of market dynamics, bestowing both investment\nentities and individual enterprises with invaluable performance insights.\nMoreover, our methodology integrates pivotal factors such as indexes and ETFs,\nseamlessly woven with Hierarchical Risk Parity (HRP) portfolio optimization, to\nelevate performance and fortify risk management. This comprehensive\namalgamation refines risk diversification, fortifying portfolio resilience\nagainst turbulent market forces. The implications reverberate resoundingly.\nInvestment entities stand poised to calibrate against competitors with surgical\nprecision, tactically sidestepping industry-specific pitfalls, and sculpting\nbespoke investment strategies to capitalize on market fluctuations.\nConcurrently, individual enterprises find empowerment in aligning strategic\nendeavors with market trajectories, discerning key competitors, and navigating\nvolatility with steadfast resilience. In essence, this research marks a pivotal\nmoment in economic discourse, unveiling novel methodologies poised to redefine\ndecision-making paradigms and elevate performance benchmarks for both\ninvestment entities and individual enterprises navigating the intricate\ntapestry of financial realms.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.01892v1"
    },
    {
        "title": "The Impact of Financial Literacy, Social Capital, and Financial\n  Technology on Financial Inclusion of Indonesian Students",
        "authors": [
            "Gen Norman Thomas",
            "Siti Mutiara Ramadhanti Nur",
            "Lely Indriaty"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This study aims to analyze the impact of financial literacy, social capital\nand financial technology on financial inclusion. The research method used a\nquantitative research method, in which questionnaires were distributed to 100\nactive students in the economics faculty at 7 private colleges in Tangerang,\nIndonesia. Based on the results of data processing using SPSS version 23, it\nresults that financial literacy, social capital and financial technology\npartially have a positive and significant influence on financial inclusion. The\nresults of this study provide input that financial literacy needs to be\nincreased because it is not yet equivalent to financial inclusion, and reducing\nthe gap between financial literacy and financial inclusion is only 2.74%.\nAnother benefit of this research is to give an understanding to students that\nstudents should be independent actors or users of financial technology products\nand that students should become pioneers in delivering financial knowledge,\nfinancial behavior and financial attitudes to the wider community.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06570v1"
    },
    {
        "title": "Visualization of Board of Director Connections for Analysis in Socially\n  Responsible Investing",
        "authors": [
            "Alice Da Fonseca",
            "Peter Lake",
            "Ariana Barrenechea"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This project is a collaboration between industry and academia to delve into\nFinance Social Networks, specifically the Board of Directors of public\ncompanies. Knowing the connections between Directors and Executives in\ndifferent companies can generate powerful stories and meaningful insights on\ninvestments. A proof of concept in the form of a Data Visualization tool\nreveals its strength in investigating corporate governance and sustainability,\nas well as in the partnership between industry and academic institutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.20522v1"
    },
    {
        "title": "Essays on Responsible and Sustainable Finance",
        "authors": [
            "Baridhi Malakar"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The dissertation consists of three essays on responsible and sustainable\nfinance. I show that local communities should be seen as stakeholders to\ndecisions made by corporations. In the first essay, I examine whether the\nimposition of fiduciary duty on municipal advisors affects bond yields and\nadvising fees. Using a difference-in-differences analysis, I show that bond\nyields reduce by 9\\% after the imposition of the SEC Municipal Advisor Rule. In\nthe second essay, we analyze the impact of USD 40 billion of corporate\nsubsidies given by U.S. local governments on their borrowing costs. We find\nthat winning counties experience a 15 bps increase in bond yield spread as\ncompared to the losing counties. In the third essay, we provide new evidence\nthat the bankruptcy filing of a locally-headquartered and publicly-listed\nmanufacturing firm imposes externalities on the local governments. Compared to\nmatched counties with similar economic trends, municipal bond yields for\naffected counties increase by 10 bps within a year of the firm filing for\nbankruptcy. The final essay examines whether managers walk the talk on the\nenvironmental and social discussion. We train a deep-learning model on various\ncorporate sustainability frameworks to construct a comprehensive Environmental\nand Social (E and S) dictionary. Using this dictionary, we find that the\ndiscussion of environmental topics in the earnings conference calls of U.S.\npublic firms is associated with higher pollution abatement and more future\ngreen patents.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.12995v1"
    },
    {
        "title": "Fiduciary Duty in the Municipal Bonds Market",
        "authors": [
            "Baridhi Malakar"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  I examine whether the imposition of fiduciary duty on municipal advisors\naffects bond yields and advising fees. Using a difference-in-differences\nanalysis, I show that bond yields reduce by $\\sim$9\\% after the imposition of\nthe SEC Municipal Advisor Rule due to lower underwriting spreads. Larger\nmunicipalities are more likely to recruit advisors after the rule is effective\nand experience a greater reduction in yields. However, smaller issuers do not\nexperience a reduction in offering yields after the SEC Rule. Instead, their\nborrowing cost increases if their primary advisor exits the market. Using novel\nhand-collected data, I find that the average advising fees paid by issuers does\nnot increase after the regulation. Overall, my results suggest that while\nfiduciary duty may mitigate the principal-agent problem between some issuers\nand advisors, there is heterogeneity among issuers.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15197v1"
    },
    {
        "title": "Entropy and Economics",
        "authors": [
            "Martin Pomares Calero"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Entropy is a very useful concept from physics that tries to explain how a\nsystem behaves from a point of view of the thermodynamics. However, there are\ntwo ways to explain entropy, and it depends on if we are studying a microsystem\nor a microsystem. From a macroscopically point of view, it is important to\ndescribe if the system is a reversible system or not. However, form the\nmicroscopically point of view, the concept of chaos is related to entropy. In\nsuch case, entropy measures the amount of disorder into the system. Otherwise,\nthe idea of connecting at the same time the analysis of the macro and micro\nsystem with the use of entropy it is not very common.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.00022v1"
    },
    {
        "title": "Regulating Cryptocurrency and Decentralized Finance for an Inclusive\n  Economy",
        "authors": [
            "Amrutha Muralidhar",
            "Muralidhar Lakkanna"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The evolution of cryptocurrency and decentralized finance (DeFi) marks a\nsignificant shift in the financial landscape, making it more accessible,\ninclusive, and participative for various societal groups. However, this\ntransition from traditional financial institutions to DeFi demands a meticulous\npolicy framework that strikes a balance between innovation and safeguarding\nconsumer interests, security, and regulatory compliance. In this script we\nexplore the imperative need for regulatory frameworks overseeing\ncryptocurrencies and DeFi, aiming to leverage their potential for inclusive\neconomic advancement. It underscores the prevalent challenges within\nconventional financial systems, juxtaposing them with the transformative\npotential offered by these emergent financial paradigms. By highlighting the\nrole of robust regulations, we examine their capacity to ensure user security,\nfortify market resilience, and spur innovative strides. We aim to proffer\nviable strategies for formulating regulatory structures that harmonize the twin\nobjectives of fostering innovation and upholding fairness within financial\necosystems.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.01532v1"
    },
    {
        "title": "Household Leverage Cycle Around the Great Recession",
        "authors": [
            "Bo Li"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This paper provides the first causal evidence that credit supply expansion\ncaused the 1999-2010 U.S. business cycle mainly through the channel of\nhousehold leverage (debt-to-income ratio). Specifically, induced by net export\ngrowth, credit expansion in private-label mortgages, rather than\ngovernment-sponsored enterprise mortgages, causes a much stronger boom and bust\ncycle in household leverage in the high net-export-growth areas. In addition,\nsuch a stronger household leverage cycle creates a stronger boom and bust cycle\nin the local economy, including housing prices, residential construction\ninvestment, and house-related employment. Thus, our results are consistent with\nthe credit-driven household demand channel (Mian and Sufi, 2018). Further, we\nshow multiple pieces of evidence against the corporate channel, which is\nemphasized by other business cycle theories (hypotheses).\n",
        "pdf_link": "http://arxiv.org/pdf/2407.01539v1"
    },
    {
        "title": "Auction theory and demography",
        "authors": [
            "O. A. Malafeyev",
            "I. E. Khomenko"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  In economics, there are many ways to describe the interaction between a\n\"seller\" and a \"buyer\". The most common one, with which we interact almost\nevery day, is selling for a fixed price. This option is perfect for selling a\nmass product, when we have a number of sellers and many buyers, and the price\nfor the product varies depending on the conditions of the relationship between\nsupply and demand. Another situation meets us already in markets, where a\nproduct can be either mass-produced or more unique, so this option is already\ncloser to the object of our discussion.However, a one-on-one transaction is a\nmuch more unstable option, which is why it is also more difficult to model,\nsince it is determined not so much by algorithms as by psychology and the\ndifference in the bargaining ability of the two parties. An even closer example\nof an auction is price discrimination, when the price for the buyer is\ndetermined not only by supply and demand, but also by which group the buyer\nbelongs to. But in this case, the product is not unique, and the final seller\nis the only one. Thus, we have identified the main auction criteria and their\nfeatures of the \"game\".\n",
        "pdf_link": "http://arxiv.org/pdf/2407.06248v1"
    },
    {
        "title": "Central Bank Digital Currency: The Advent of its IT Governance in the\n  financial markets",
        "authors": [
            "Carlos Alberto Durigan Junior",
            "Mauro De Mesquita Spinola",
            "Rodrigo Franco Gonçalves",
            "Fernando José Barbin Laurindo"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Central Bank Digital Currency (CBDC) can be defined as a virtual currency\nbased on node network and digital encryption algorithm issued by a country\nwhich has a legal credit protection. CBDCs are supported by Distributed Ledger\nTechnologies (DLTs), and they may allow a universal means of payments for the\ndigital era. There are many ways to proceed, they all require central banks to\ndevelop technological expertise. Considering these points, it is important to\nunderstand the new IT governance in the financial markets due to CBDC and\ndigital economy. Information Technology is an essential driver that will allow\nthe new financial industry design. This paper has the objective to answer two\nquestions through an updated Systematic Literature Review (SLR). The first\nquestion is What IT resources and tools have been considered or applied to set\nthe governance of CBDC adoption? The second; Identify IT governance models in\nthe financial market due to CBDC adoption. Bank for International Settlements\n(BIS) publications, Scopus and Web of Science were considered as sources of\nstudies. After the strings and including criteria were applied, fourteen papers\nwere analyzed. This paper finds many IT resources used in the CBDC adoption and\nsome preliminary IT design related to the IT governance of CBDC, in the results\nand discussion section the findings are more detailed. Finally, limitations and\nfuture work are considered. Keywords: Blockchain, Central Bank Digital Currency\n(CBDC), Digital Economy, Distributed Ledger Technology (DLT), Information\nTechnology (IT), IT governance.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07898v1"
    },
    {
        "title": "Applying the Nash Bargaining Solution for a Reasonable Royalty II",
        "authors": [
            "David M. Kryskowski",
            "David Kryskowski"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This paper expands on the concepts presented in Applying the Nash Bargaining\nSolution for a Reasonable Royalty ( arXiv:2005.10158 ). The goal is to refine\nthe process for determining a reasonable royalty using statistical methods in\ncases where there is risk and uncertainty regarding each party's disagreement\npayoffs (opportunity costs) in the Nash Bargaining Solution (NBS). This paper\nuses a Bayes Cost approach to analyze Case 1, Case 2, and the Original Nash\nmodel from the authors' previous work. By addressing risk and uncertainty in\nthe NBS, the NBS emerges as a more reliable method for estimating a reasonable\nroyalty, aligning with the criteria outlined in Georgia Pacific factor fifteen.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.14642v1"
    },
    {
        "title": "Inefficiencies of Carbon Trading Markets",
        "authors": [
            "Nicola Borri",
            "Yukun Liu",
            "Aleh Tsyvinski",
            "Xi Wu"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The European Union Emission Trading System is a prominent market-based\nmechanism to reduce emissions. While the theory is well understood, we are the\nfirst to study the whole cap-and-trade mechanism as a financial market.\nAnalyzing the universe of transactions in 2005-2020 (more than one million\nrecords of granular transaction data), we show that this market features\nsignificant inefficiencies undermining its goals. First, about 40% of firms\nnever trade in a given year. Second, many firms only trade during surrendering\nmonths, when compliance is immediate and prices are predictably high. Third, a\nnumber of operators engage in speculative trading, exploiting private\ninformation.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.06497v2"
    },
    {
        "title": "Hydrogen Development in China and the EU: A Recommended Tian Ji's Horse\n  Racing Strategy",
        "authors": [
            "Hong Xu"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The global momentum towards establishing sustainable energy systems has\nbecome increasingly prominent. Hydrogen, as a remarkable carbon-free and\nrenewable energy carrier, has been endorsed by 39 countries at COP28 in the\nUAE, recognizing its essential role in global energy transition and industry\ndecarbonization. Both the European Union (EU) and China are at the forefront of\nthis shift, developing hydrogen strategies to enhance regional energy security\nand racing for carbon neutrality commitments by 2050 for the EU and 2060 for\nChina. The wide applications of hydrogen across hard-to-abate sectors and the\nflexibility of decentralized production and storage offer customized solutions\nutilizing local resources in a self-paced manner. To unveil the trajectory of\nhydrogen development in China and the EU, this paper proposes a comparative\nanalysis framework employing key factors to investigate hydrogen developments\nin both economic powerhouses. Beyond country-wise statistics, it dives into\nrepresentative hydrogen economic areas in China (Inner Mongolia, Capital\nEconomic Circle, Yangtze River Delta) and Europe (Delta Rhine Corridor) for\nunderstanding supply and demand, industrial synergy, and policy incentives for\nlocal hydrogen industries. The derived implications offer stakeholders an\nevolving hydrogen landscape across the Eurasian continent and insights for\nfuture policy developments facilitating the global green transition.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.08874v1"
    },
    {
        "title": "Information Asymmetry Index: The View of Market Analysts",
        "authors": [
            "Roberto Frota Decourt",
            "Heitor Almeida",
            "Philippe Protin",
            "Matheus R. C. Gonzalez"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The purpose of the research was to build an index of informational asymmetry\nwith market and firm proxies that reflect the analysts' perception of the level\nof informational asymmetry of companies. The proposed method consists of the\nconstruction of an algorithm based on the Elo rating and captures the\nperception of the analyst that choose, between two firms, the one they consider\nto have better information. After we have the informational asymmetry index, we\nrun a regression model with our rating as dependent variable and proxies used\nby the literature as the independent variable to have a model that can be used\nfor other researches that need to measure the level of informational asymmetry\nof a company. Our model presented a good fit between our index and the proxies\nused to measure informational asymmetry and we find four significant variables:\ncoverage, volatility, Tobin q, and size.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06272v1"
    },
    {
        "title": "Implicit Government Guarantee Measurement Based on PMC Index Model",
        "authors": [
            "Yan Zhang",
            "Yixiang Tian",
            "Lin Chen",
            "Qi Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The implicit government guarantee hampers the recognition and management of\nrisks by all stakeholders in the bond market, and it has led to excessive debt\nfor local governments or state-owned enterprises. To prevent the risk of local\ngovernment debt defaults and reduce investors' expectations of implicit\ngovernment guarantees, various regulatory departments have issued a series of\npolicy documents related to municipal investment bonds. By employing text\nmining techniques on policy documents related to municipal investment bond, and\nutilizing the PMC index model to assess the effectiveness of policy documents.\nThis paper proposes a novel method for quantifying the intensity of implicit\ngovernmental guarantees based on PMC index model. The intensity of implicit\ngovernmental guarantees is inversely correlated with the PMC index of policies\naimed at de-implicitizing governmental guarantees. Then as these policies\nbecome more effective, the intensity of implicit governmental guarantees\ndiminishes correspondingly. These findings indicate that recent policies\nrelated to municipal investment bond have indeed succeeded in reducing implicit\ngovernmental guarantee intensity, and these policies have achieved the goal of\nrisk management. Furthermore, it was showed that the intensity of implicit\ngovernmental guarantee affected by diverse aspects of these policies such as\neffectiveness, clarity, and specificity, as well as incentive and assurance\nmechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.12831v1"
    },
    {
        "title": "The Impact of Implicit Government Guarantee on Credit Rating of\n  Municipal Investment Bonds",
        "authors": [
            "Yan Zhang",
            "Yixiang Tian",
            "Lin Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  One type of bond with the most implicit government guarantee is municipal\ninvestment bonds. In recent years, there have been an increasing number of\ndowngrades in the credit ratings of municipal bonds, which has led some people\nto question whether the implicit government guarantee may affect the\nobjectivity of the bond ratings? This paper uses text mining methods to mine\nrelevant policy documents related to municipal investment bond issuance, and\ncalculates the implicit guarantee strength of municipal investment bonds based\non the PMC index model. It further analyzes the impact of the implicit\nguarantee strength of municipal bonds on their credit evaluation. The study\nfound that the implicit government guarantee on municipal investment bonds does\nindeed help to raise the credit ratings assigned by credit rating agencies. The\nstudy found that, moreover, the government's implicit guarantee has a more\npronounced effect in boosting credit ratings in less developed western regions.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13957v1"
    },
    {
        "title": "Assessment of the Financial Competitiveness of Publicly Listed Indian\n  Real Estate Companies Using the Entropy Method",
        "authors": [
            "Ritij Saini",
            "Aditya Deora",
            "Kirtesh Gadiya"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The real estate sector is one of the key drivers of India's national economy,\ncontributing about 7.3\\% to the GDP. As the market evolves, more players enter,\nand government policies become more stringent, Indian real estate companies\nface increasing competition. Improving financial competitiveness is crucial for\nthe survival and growth of these companies. This paper presents a financial\ncompetitiveness evaluation index system for Indian-listed real estate\ncompanies, covering profitability, solvency, and operational capacity. Using\nkey financial ratios and a scoring system, the financial competitiveness of\nvarious companies was evaluated, revealing that companies with high scores have\nstrong profitability and operational capacity. In contrast, those with lower\nscores struggle with solvency and working capital.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.06772v1"
    },
    {
        "title": "The aftermath of the Covid pandemic in the forest sector: new\n  opportunities for emerging wood products",
        "authors": [
            "Mojtaba Houballah",
            "Jean-Yves Courtonne",
            "Henri Cuny",
            "Antoine Colin",
            "Mathieu Fortin",
            "Jean-Baptiste Pichancourt",
            "Francis Colin"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Context: Over the last decade, the forestry sector has undergone substantial\nchanges, evolving from a post-2008 financial crisis landscape to incorporating\npolicies favoring sustainable and green alternatives, especially after the 2015\nParis agreement. This evolution was drastically disrupted with the advent of\nthe COVID-19 pandemic in 2020, causing unprecedented interruptions in supply\nchains, product markets, and data collection. Grasping the aftermath of the\nCOVID-19, regional instances of the forest supply chain sector need synthetic\npictures of their present state and future opportunities for emerging wood\nproducts and better regional-scale carbon balance. But given the impact of\nCOVID-19 lock-down on data collection, the production of such synthetic\npictures has become more complex, yet essential. This was the case for the\nregional supply chain of the Grand-Est region in France that we studied.\n  Aims: For this study, our aim was to demonstrate that an integrated\nmethodology could provide such synthetic picture even though we sued\nheterogenous sources of data and different analytical objectives: i.e. (1)\nretrospectively evaluate the aftermath of COVID-19 pandemic on the supply chain\noutcomes within the forestry sector; and then (2) retrospectively explore\npossible options of structural change of regional supply chain that would be\nrequired to simultaneously recover from COVID-19 and transit to new objectives\nin line with the extraction of new bio-molecules from wood biomass, and with\nthe reduction of the regional scale carbon footprint (in line with the IPCC\nParis Agreement)\n  Methods: To achieve this, our methodological approach was decomposed into\nthree steps. We first used a Material Flow Analysis (MFA) recently conducted on\nthe forestry sector in the Grand Est region to establish a Sankey diagram (i.e.\na schematic representation of industrial sectors and biomass flows along the\nsupply chain) for the pre-Covid-19 period (2014-2018). Then we compared\npre-Covid-19 Sankey diagram to the only source of data we could access from the\npost-Covid-19 period (2020-2021) in order to estimate the impact of Covid-19.\nFinally, we used as input the reconciled supply chain model into a\nconsequential Wood Product Model (WPMs), called CAT (carbon accounting tool) in\norder to compare three prospective scenarios: (1) a scenario that projected\n2020-2021 Covid-19 conditions and assumed pre-Covid-19 business as usual\npractices, (2) a scenario illustrating the consequence or rerouting some of the\nbiomass to satisfy the expected increase in pulp and paper production to\nsatisfy the needs of the industry after Covid-19, and (3) a scenario that\nexplored new opportunities in term of extraction of novel bio-molecules by the\nemerging biochemical wood industry. For every scenario we also evaluated the\nregional carbon gains and losses that these changes implied.\n  Results: Our study conducted a detailed analysis of the impacts of the\nCOVID-19 pandemic on the forestry sector's supply chain in the Grand Est\nregion, using a dynamic and integrated Wood Product Model. We found significant\ndisruptions during the pandemic period, with notable declines in industrial\nwood chips and timber hardwood production by 41.8% and 40%, respectively.\nConversely, there were substantial increases in fuelwood, timber sawdust, and\ntimber softwood, rising by 14.15%, 44.23%, and 15.29% respectively. These\nfluctuations underscore the resilience and vulnerabilities within the regional\nwood supply chain. Our findings also emphasize the potential for strategic\nrerouting of biomass flows to meet changing industry demands, which could play\na crucial role in supporting the sector's recovery and adaptation to\npost-pandemic conditions.\n  Discussion and conclusion: In addition, our study recognizes the limitations\nof the current approach combining MFA and WPM and suggests potential areas of\nenhancement. Ultimately, our findings shed light on the need to develop more\nintegrated analytical methods to provide useful synthetic pictures of regional\nscale supply chains, when there is a need to adapt it to evolving situations\nand complex data landscapes.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.07195v1"
    },
    {
        "title": "Exploring the Interplay of Skewness and Kurtosis: Dynamics in\n  Cryptocurrency Markets Amid the COVID-19 Pandemic",
        "authors": [
            "Ariston Karagiorgis",
            "Antonis Ballis",
            "Konstantinos Drakos"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  We examine how skewness interacts with kurtosis within the cryptocurrency\nmarket. We show that during the COVID-19 pandemic there are more clusters of\nobservations around the two flanks, highlighting the presence of a volatile\nbehavior. Moreover, we document the evolvement of the interrelationship as the\npandemic progresses, identifying the domination of the extremes. Our findings\nadvance the thinking that by exploiting the interrelationship between the two\nhigher moments of cryptocurrencies, investors and researchers can have in their\narsenal an additional analytic tool.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12801v1"
    },
    {
        "title": "Corporate Non-Disclosure Disputes: Equilibrium Settlements with a\n  Probabilistic Burden of Proof",
        "authors": [
            "Miles B. Gietzmann",
            "Adam J. Ostaszewski </footnote>"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The literature of damages-assessment under the US disclosure 10b-5 rule\nproposes multiple formulations for a \"proper measure\" based on the legal\n\"out-of-pocket\" principle, i.e. the difference between original and an observed\nlater price consequent on revealed information, with suspected prior disclosure\nomission. Key is the evolution of a putative \"price-but for the omission\".\nProposed procedures include back-casting and forward-casting (between two\ndates, or via multiple intervening dates) -- Ferrell and Saha (2011). We\nderive, consistently with fair-pricing, a new model of induced\nlitigation-behaviour referring to prices at and just before the revelation\ndate. Parameters incentivise withholding of price-sensitive news under a\n`sparing' strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.13878v1"
    },
    {
        "title": "Decentralized Finance: Impact on Financial Services and required DeFi\n  Literacy in 2034",
        "authors": [
            "Daniel Liebau"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This study explores the potential future impact of Decentralized Finance\n(DeFi) on the financial services industry and the competencies financial\nprofessionals will require in a decade from now, in 2034. Using a survey of 109\nexperts from industry, academia, and regulatory bodies, the research highlights\ngrowing confidence in DeFi's potential to reshape critical areas such as risk\nmanagement and operations. Over 40% of respondents anticipate high levels of\nDeFi adoption by 2034. However, the study identifies critical issues that need\nto be addressed, particularly in the areas of data management & privacy, and\nsecurity. These concerns, alongside regulatory challenges, underscore the need\nfor financial institutions to prepare carefully. The findings also suggest that\nstrategic competencies, sector-specific domain expertise, and technological\nskills will become increasingly vital. The insights offered are valuable for\nregulators, policymakers, and industry professionals, emphasizing the need for\ncontinuous upskilling to remain competitive in an evolving financial services\nlandscape.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.14173v2"
    },
    {
        "title": "Graph Signal Processing for Global Stock Market Volatility Forecasting",
        "authors": [
            "Zhengyang Chi",
            "Junbin Gao",
            "Chao Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The interconnectedness of global financial markets has brought increasing\nattention to modeling volatility spillover effects. Via incorporating Graph\nSignal Processing techniques, a novel multivariate framework, extending the\ntraditional Heterogeneous Auto-Regressive model, is developed in the spectral\ndomain constructed by the graph Fourier transformation method. Further, a set\nof convolution filters with learnable weights is employed to more flexibly\naggregate the past mid-term and long-term information. Using 24 global stock\nmarket indices, the effectiveness of the proposed model is demonstrated through\ncomprehensive empirical evaluations.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.22706v1"
    },
    {
        "title": "Sensitivity Analysis of emissions Markets: A Discrete-Time Radner\n  Equilibrium Approach",
        "authors": [
            "Stéphane Crépey",
            "Mekonnen Tadese",
            "Gauthier Vermandel"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Emissions markets play a crucial role in reducing pollution by encouraging\nfirms to minimize costs. However, their structure heavily relies on the\ndecisions of policymakers, on the future economic activities, and on the\navailability of abatement technologies. This study examines how changes in\nregulatory standards, firms' abatement costs, and emissions levels affect\nallowance prices and firms' efforts to reduce emissions. This is done in a\nRadner equilibrium framework encompassing inter-temporal decision-making,\nuncertainty, and a comprehensive assessment of the market dynamics and\noutcomes. The results of this research have the potential to assist\npolicymakers in enhancing the structure and efficiency of emissions trading\nsystems, through an in-depth comprehension of the reactions of market\nstakeholders towards different market situations.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.06185v1"
    },
    {
        "title": "The relationship between general equilibrium models with infinite-lived\n  agents and overlapping generations models, and some applications",
        "authors": [
            "Ngoc-Sang Pham"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  We prove that a two-cycle equilibrium in a general equilibrium model with\ninfinitely-lived agents also constitutes an equilibrium in an overlapping\ngenerations (OLG) model. Conversely, an equilibrium in an OLG model that\nsatisfies additional conditions is part of an equilibrium in a general\nequilibrium model with infinitely-lived agents. Applying this result, we\ndemonstrate that equilibrium indeterminacy and rational asset price bubbles may\narise in both types of models.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07674v1"
    },
    {
        "title": "Intergenerational cross-subsidies in UK collective defined contribution\n  (CDC) funds",
        "authors": [
            "John Armstrong",
            "James Dalby",
            "Catherine Donnelly"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  We evaluate the performance of and level of intergenerational cross subsidy\nin single-employer and multi-employer collective defined contribution (CDC)\nschemes which have been designed to be compatible with UK legislation. The\nsingle-employer scheme captures the essential features of the Royal Mail CDC\nscheme, which is currently the only UK CDC scheme. We find that the schemes can\nbe successful in smoothing pension outcomes while outperforming a DC + annuity\nscheme, but that this outperformance is not guaranteed in a single-employer\nscheme. There are significant intergenerational cross-subsidies in the\nsingle-employer scheme. These qualitatively mirror the cross-subsidies seen in\nexisting defined benefit schemes, but we find the magnitude of the\ncross-subsidies is much larger in single employer CDC schemes. The\nmulti-employer scheme is intended to minimize such cross-subsidies, but we find\nthat such subsidies still arise due to the approximate pricing methodology\nimplicit in the scheme design. These cross-subsidies tend to cancel out over\ntime, but in any given year they can be large, implying that it is important to\nuse a rigorous pricing methodology when valuing collective pension investments.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13565v2"
    },
    {
        "title": "The Value of Information from Sell-side Analysts",
        "authors": [
            "Linying Lv"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  I examine the value of information from sell-side analysts by analyzing a\nlarge corpus of their written reports. Using embeddings from state-of-the-art\nlarge language models, I show that textual information in analyst reports\nexplains 10.19% of contemporaneous stock returns out-of-sample, a value that is\neconomically more significant than quantitative forecasts. I then perform a\nShapley value decomposition to assess how much each topic within the reports\ncontributes to explaining stock returns. The results show that analysts' income\nstatement analyses account for more than half of the reports' explanatory\npower. Expressing these findings in economic terms, I estimate that early\nacquisition of analysts' reports can yield significant profits. Analysts'\ninformation value peeks in the first week following earnings announcements,\nhighlighting their vital role in interpreting new financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13813v3"
    },
    {
        "title": "IntraLayer: A Platform of Digital Finance Platforms",
        "authors": [
            "Arman Abgaryan",
            "Utkarsh Sharma"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  IntraLayer presents an innovative framework that enables comprehensive\ninterconnectivity in digital finance. The proposed framework comprises a core\nunderlying infrastructure and an overarching strategy to create a pioneering\n\"platform of platforms\", serving as an algorithmic fiduciary. By design, this\ninfrastructure optimises transactional efficiency for a broad spectrum of\nagents, thereby facilitating the sustainable creation of intrinsic economic\nvalue. Complementing the infrastructure, our forthcoming work will present an\noverarching adaptive fiscal policy to optimise IntraLayer's resources, striking\na balance between sustaining the network and enhancing the proposal herein.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.07348v1"
    },
    {
        "title": "EGT through Quantum Mechanics & from Statistical Physics to Economics",
        "authors": [
            "Esteban Guevara"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  By analyzing the relationships between a socioeconomical system modeled\nthrough evolutionary game theory and a physical system modeled through quantum\nmechanics we show how although both systems are described through two theories\napparently different both are analogous and thus exactly equivalents. The\nextensions of quantum mechanics to statistical physics and information theory\nlet us use some of their definitions for the best understanding of the behavior\nof economics and biology. The quantum analogue of the replicator dynamics is\nthe von Neumann equation. A system in where all its members are in Nash\nequilibrium is equivalent to a system in a maximum entropy state. Nature is a\ngame in where its players compete for a common welfare and the equilibrium of\nthe system that they are members. They act as a whole besides individuals like\nthey obey a rule in where they prefer to work for the welfare of the collective\nbesides the individual welfare.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.0029v1"
    },
    {
        "title": "Hiking the hypercube: producers and consumers",
        "authors": [
            "Tanya Araújo",
            "Gérard Weisbuch"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We study the dynamics of co-evolution of producers and customers described by\nbit-strings representing individual traits. Individual ''size-like'' properties\nare controlled by binary encounters which outcome depends upon a recognition\nprocess. Depending upon the parameter set-up, mutual selection of producers and\ncustomers results in different types of attractors, either an exclusive niches\nregime or a competition regime.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.1028v1"
    },
    {
        "title": "Heterogeneity and Increasing Returns May Drive Socio-Economic\n  Transitions",
        "authors": [
            "Gérard Weisbuch",
            "Vincent Buskens",
            "Luat Vuong"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  There are clear benefits associated with a particular consumer choice for\nmany current markets. For example, as we consider here, some products might\ncarry environmental or `green' benefits. Some consumers might value these\nbenefits while others do not. However, as evidenced by myriad failed attempts\nof environmental products to maintain even a niche market, such benefits do not\nnecessarily outweigh the extra purchasing cost. The question we pose is, how\ncan such an initially economically-disadvantaged green product evolve to hold\nthe greater share of the market? We present a simple mathematical model for the\ndynamics of product competition in a heterogeneous consumer population. Our\nmodel preassigns a hierarchy to the products, which designates the consumer\nchoice when prices are comparable, while prices are dynamically rescaled to\nreflect increasing returns to scale. Our approach allows us to model many\nscenarios of technology substitution and provides a method for generalizing\nmarket forces. With this model, we begin to forecast irreversible trends\nassociated with consumer dynamics as well as policies that could be made to\ninfluence transitions\n",
        "pdf_link": "http://arxiv.org/pdf/0706.1454v1"
    },
    {
        "title": "The minority game: An economics perspective",
        "authors": [
            "Willemien Kets"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  This paper gives a critical account of the minority game literature. The\nminority game is a simple congestion game: players need to choose between two\noptions, and those who have selected the option chosen by the minority win. The\nlearning model proposed in this literature seems to differ markedly from the\nlearning models commonly used in economics. We relate the learning model from\nthe minority game literature to standard game-theoretic learning models, and\nshow that in fact it shares many features with these models. However, the\npredictions of the learning model differ considerably from the predictions of\nmost other learning models. We discuss the main predictions of the learning\nmodel proposed in the minority game literature, and compare these to\nexperimental findings on congestion games.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.4432v1"
    },
    {
        "title": "The Product Space Conditions the Development of Nations",
        "authors": [
            "C. A. Hidalgo",
            "B. Klinger",
            "A. -L. Barabasi",
            "R. Hausmann"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Economies grow by upgrading the type of products they produce and export. The\ntechnology, capital, institutions and skills needed to make such new products\nare more easily adapted from some products than others. We study the network of\nrelatedness between products, or product space, finding that most upscale\nproducts are located in a densely connected core while lower income products\noccupy a less connected periphery. We show that countries tend to move to goods\nclose to those they are currently specialized in, allowing nations located in\nmore connected parts of the product space to upgrade their exports basket more\nquickly. Most countries can reach the core only if they jump over empirically\ninfrequent distances in the product space. This may help explain why poor\ncountries have trouble developing more competitive exports, failing to converge\nto the income levels of rich countries.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.2090v1"
    },
    {
        "title": "Analytical modelling of terminal properties in industrial growth",
        "authors": [
            "Arnabi Marjit",
            "Sudipto Marjit",
            "Arnab K. Ray"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  In this pedagogical study, carried out by adopting standard mathematical\nmethods of nonlinear dynamics, we have presented some simple analytical models\nto understand terminal behaviour in industrial growth. This issue has also been\naddressed from a dynamical systems perspective, with especial emphasis on the\nconcept of the Balanced Scorecard. Our study enables us to make the general\nclaim that although the fortunes of an industrial organization can rise with\nexponential rapidity on relatively short time scales, its growth will\nultimately and inevitably be saturated on long time scales by various factors\nwhich are nonlinear in character. We have mathematically demonstrated the\nlikely occurrence of this feature under various possible circumstances,\nincluding the Red Ocean and the Blue Ocean. Finally and most importantly, our\narguments and their associated mathematical modelling have received remarkable\nsupport from the growth pattern indicated by empirical data gathered from a\nwell-recognized global company like IBM.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.3467v3"
    },
    {
        "title": "On the Topological Properties of the World Trade Web: A Weighted Network\n  Analysis",
        "authors": [
            "Giorgio Fagiolo",
            "Javier Reyes",
            "Stefano Schiavo"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  This paper studies the topological properties of the World Trade Web (WTW)\nand its evolution over time by employing a weighted network analysis. We show\nthat the WTW, viewed as a weighted network, displays statistical features that\nare very different from those obtained by using a traditional binary-network\napproach. In particular, we find that: (i) the majority of existing links are\nassociated to weak trade relationships; (ii) the weighted WTW is only weakly\ndisassortative; (iii) countries holding more intense trade relationships are\nmore clustered.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.4359v1"
    },
    {
        "title": "Evolution of community structure in the world trade web",
        "authors": [
            "Irena Tzekina",
            "Karan Danthi",
            "Daniel N. Rockmore"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  In this note we study the bilateral merchandise trade flows between 186\ncountries over the 1948-2005 period using data from the International Monetary\nFund. We use Pajek to identify network structure and behavior across thresholds\nand over time. In particular, we focus on the evolution of trade \"islands\" in\nthe a world trade network in which countries are linked with directed edges\nweighted according to fraction of total dollars sent from one country to\nanother. We find mixed evidence for globalization.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2630v1"
    },
    {
        "title": "Innovation Success and Structural Change: An Abstract Agent Based Study",
        "authors": [
            "Tanya Araujo",
            "R. Vilela Mendes"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  A model is developed to study the effectiveness of innovation and its impact\non structure creation and structure change on agent-based societies. The\nabstract model that is developed is easily adapted to any particular field. In\nany interacting environment, the agents receive something from the environment\n(the other agents) in exchange for their effort and pay the environment a\ncertain amount of value for the fulfilling of their needs or for the very price\nof existence in that environment. This is coded by two bit strings and the\ndynamics of the exchange is based on the matching of these strings to those of\nthe other agents. Innovation is related to the adaptation by the agents of\ntheir bit strings to improve some utility function.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2694v1"
    },
    {
        "title": "Bidding Strategy with Forecast Technology Based on Support Vector\n  Machine in Electrcity Market",
        "authors": [
            "C. Gao",
            "E. Bompard",
            "R. Napoli",
            "Q. Wan"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  The participants of the electricity market concern very much the market price\nevolution. Various technologies have been developed for price forecast. SVM\n(Support Vector Machine) has shown its good performance in market price\nforecast. Two approaches for forming the market bidding strategies based on SVM\nare proposed. One is based on the price forecast accuracy, with which the being\nrejected risk is defined. The other takes into account the impact of the\nproducer's own bid. The risks associated with the bidding are controlled by the\nparameters setting. The proposed approaches have been tested on a numerical\nexample.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.3710v1"
    },
    {
        "title": "A Brief History of Economics: An Outsider's Account",
        "authors": [
            "Bikas K Chakrabarti"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  A dangerously brief history of the developments of the main ideas in\neconomics, as observed by a physicist, is given. This was published in\n'Econophysics of Stock and Other Markets', Eds. A. Chatterjee, B. K.\nChakrabarti, New Economic Windows Series, Springer, Milan, 2006, pp~219-224.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.4093v1"
    },
    {
        "title": "Quantum Auctions: Facts and Myths",
        "authors": [
            "E. W. Piotrowski",
            "J. Sladkowski"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Quantum game theory, whatever opinions may be held due to its abstract\nphysical formalism, have already found various applications even outside the\northodox physics domain. In this paper we introduce the concept of a quantum\nauction, its advantages and drawbacks. Then we describe the models that have\nalready been put forward. A general model involves Wigner formalism and\ninfinite dimensional Hilbert spaces - we envisage that the implementation might\nnot be an easy task. But a restricted model advocated by the Hewlett-Packard\ngroup seems to be much easier to implement. Simulations involving humans have\nalready been performed. We will focus on problems related to combinatorial\nauctions and technical assumptions that are made. Quantum approach offers at\nleast two important developments. Powerful quantum algorithms for finding\nsolutions would extend the range of possible applications. Quantum strategies,\nbeing qubits, can be teleported but are immune from cloning - therefore extreme\nprivacy of agent's activity could in principle be guaranteed. Then we point out\nsome key problem that have to be solved before commercial use would be\npossible. With present technology, optical networks, single photon sources and\ndetectors seems to be sufficient for experimental realization in the near\nfuture. We conclude by describing potential customers, estimating the potential\nmarket size and possible timing.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.4096v1"
    },
    {
        "title": "Agent Simulation of Chain Bankruptcy",
        "authors": [
            "Yuichi Ikeda",
            "Yoshi Fujiwara",
            "Wataru Souma",
            "Hideaki Aoyama",
            "Hiroshi Iyetomi"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We have conducted an agent-based simulation of chain bankruptcy. The\npropagation of credit risk on a network, i.e., chain bankruptcy, is the key to\nnderstanding largesized bankruptcies. In our model, decrease of revenue by the\nloss of accounts payable is modeled by an interaction term, and bankruptcy is\ndefined as a capital deficit. Model parameters were estimated using financial\ndata for 1,077 listed Japanese firms. Simulations of chain bankruptcy on the\nreal transaction network consisting of those 1,077 firms were made with the\nestimated model parameters. Given an initial bankrupt firm, a list of chain\nbankrupt firms was obtained. This model can be used to detect high-risk links\nin a transaction network, for the management of chain bankruptcy.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.4355v1"
    },
    {
        "title": "Mixing Kohonen Algorithm, Markov Switching Model and Detection of\n  Multiple Change-Points: An Application to Monetary History",
        "authors": [
            "Marie-Thérèse Boyer-Xambeu",
            "Ghislain Deleplace",
            "Patrice Gaubert",
            "Lucien Gillard",
            "Madalina Olteanu"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  The present paper aims at locating the breakings of the integration process\nof an international system observed during about 50 years in the 19th century.\nA historical study could link them to special events, which operated as\nexogenous shocks on this process. The indicator of integration used is the\nspread between the highest and the lowest among the London, Hamburg and Paris\ngold-silver prices. Three algorithms are combined to study this integration: a\nperiodization obtained with the SOM algorithm is confronted to the estimation\nof a two-regime Markov switching model, in order to give an interpretation of\nthe changes of regime; in the same time change-points are identified over the\nwhole period providing a more precise interpretation of the various types of\nregulation.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.0745v1"
    },
    {
        "title": "Kinetic Economies",
        "authors": [
            "Wan Ahmad Tajuddin Wan Abdullah",
            "Sidiq Mohamad Khidzir"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We study a minimalist kinetic model for economies. A system of agents with\nlocal trading rules display emergent demand behaviour. We examine the resulting\nwealth distribution to look for non-thermal behaviour. We compare and contrast\nthis model with other similar models.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.1139v1"
    },
    {
        "title": "Common Markets, Strong Currencies & the Collective Welfare",
        "authors": [
            "Esteban Guevara Hidalgo"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  The so called \"globalization\" process (i.e. the inexorable integration of\nmarkets, currencies, nation-states, technologies and the intensification of\nconsciousness of the world as a whole) has a behavior exactly equivalent to a\nsystem that is tending to a maximum entropy state. This globalization process\nobeys a collective welfare principle in where the maximum payoff is given by\nthe equilibrium of the system and its stability by the maximization of the\nwelfare of the collective besides the individual welfare. This let us predict\nthe apparition of big common markets and strong common currencies. They will\nreach the \"equilibrium\" by decreasing its number until they reach a state\ncharacterized by only one common currency and only one big common community\naround the world.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.1307v3"
    },
    {
        "title": "Information, Inflation, and Interest",
        "authors": [
            "Lane P. Hughston",
            "Andrea Macrina"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We propose a class of discrete-time stochastic models for the pricing of\ninflation-linked assets. The paper begins with an axiomatic scheme for asset\npricing and interest rate theory in a discrete-time setting. The first axiom\nintroduces a \"risk-free\" asset, and the second axiom determines the\nintertemporal pricing relations that hold for dividend-paying assets. The\nnominal and real pricing kernels, in terms of which the price index can be\nexpressed, are then modelled by introducing a Sidrauski-type utility function\ndepending on (a) the aggregate rate of consumption, and (b) the aggregate rate\nof real liquidity benefit conferred by the money supply. Consumption and money\nsupply policies are chosen such that the expected joint utility obtained over a\nspecified time horizon is maximised subject to a budget constraint that takes\ninto account the \"value\" of the liquidity benefit associated with the money\nsupply. For any choice of the bivariate utility function, the resulting model\ndetermines a relation between the rate of consumption, the price level, and the\nmoney supply. The model also produces explicit expressions for the real and\nnominal pricing kernels, and hence establishes a basis for the valuation of\ninflation-linked securities.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.2876v1"
    },
    {
        "title": "The k-generalized distribution: A new descriptive model for the size\n  distribution of incomes",
        "authors": [
            "F. Clementi",
            "T. Di Matteo",
            "M. Gallegati",
            "G. Kaniadakis"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  This paper proposes the k-generalized distribution as a model for describing\nthe distribution and dispersion of income within a population. Formulas for the\nshape, moments and standard tools for inequality measurement - such as the\nLorenz curve and the Gini coefficient - are given. A method for parameter\nestimation is also discussed. The model is shown to fit extremely well the data\non personal income distribution in Australia and the United States.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.3645v4"
    },
    {
        "title": "Application of Tuncay's language teacher model to business-customer\n  relations",
        "authors": [
            "Carmen Costea"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  It seems that what has been said by now about market and competitiveness do\nnot fit perfectly with competences of getting the best of profit. Sometimes,\nthe classical methods of fundamentals of management do not apply to individual\ncompanies that face irregular accommodation on the market. It is high time to\nreplace the perfect business with the right one. New approaches and models may\nhelp in identifying new competition trends, changes for better application of\npurposes and proposals.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.3909v1"
    },
    {
        "title": "Consumption processes and positively homogeneous projection properties",
        "authors": [
            "Tom Fischer"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We constructively prove the existence of time-discrete consumption processes\nfor stochastic money accounts that fulfill a pre-specified positively\nhomogeneous projection property (PHPP) and let the account always be positive\nand exactly zero at the end. One possible example is consumption rates forming\na martingale under the above restrictions. For finite spaces, it is shown that\nany strictly positive consumption strategy with restrictions as above possesses\nat least one corresponding PHPP and could be constructed from it. We also\nconsider numeric examples under time-discrete and -continuous account\nprocesses, cases with infinite time horizons and applications to income\ndrawdown and bonus theory.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.4225v1"
    },
    {
        "title": "Financial Variables Effect on the U.S. Gross Private Domestic Investment\n  (GPDI) 1959-2001",
        "authors": [
            "Byron E. Bell"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  I studied what role the US stock markets and money markets have possibly\nplayed in the Gross Private Domestic Investment (GPDI) of the United States\nfrom the year 1959 to the year 2001, Gross Private Domestic Investment refers\nto the total amount of investment spending by businesses and firms located\nwithin the borders of a nation. It includes both the values of the purchases of\nnon-residential fixed investment, which include capital goods used for\nproduction, and the values of the purchases of residential fixed investment,\nwhich include construction spending for factories or offices. And I created a\nMultiple Linear Regression Model of the GDPI. To see if companies and private\ncitizens use the stock market and money markets as a way of financing capital\nprojects (business ventures, buying commercial and noncommercial property,\netc).\n  Keywords: Gross Private Domestic Investment, Pearson Correlation, SP 500, TB3\n",
        "pdf_link": "http://arxiv.org/pdf/0712.2088v1"
    },
    {
        "title": "Phase transition in the rich-get-richer mechanism due to finite-size\n  effects",
        "authors": [
            "James P. Bagrow",
            "Jie Sun",
            "Daniel ben-Avraham"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  The rich-get-richer mechanism (agents increase their ``wealth'' randomly at a\nrate proportional to their holdings) is often invoked to explain the Pareto\npower-law distribution observed in many physical situations, such as the degree\ndistribution of growing scale free nets. We use two different analytical\napproaches, as well as numerical simulations, to study the case where the\nnumber of agents is fixed and finite (but large), and the rich-get-richer\nmechanism is invoked a fraction r of the time (the remainder of the time wealth\nis disbursed by a homogeneous process). At short times, we recover the Pareto\nlaw observed for an unbounded number of agents. In later times, the (moving)\ndistribution can be scaled to reveal a phase transition with a Gaussian\nasymptotic form for r < 1/2 and a Pareto-like tail (on the positive side) and a\nnovel stretched exponential decay (on the negative side) for r > 1/2.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.2220v2"
    },
    {
        "title": "The universal shape of economic recession and recovery after a shock",
        "authors": [
            "Damien Challet",
            "Sorin Solomon",
            "Gur Yaari"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We show that a simple and intuitive three-parameter equation fits remarkably\nwell the evolution of the gross domestic product (GDP) in current and constant\ndollars of many countries during times of recession and recovery. We then argue\nthat this equation is the response function of the economy to isolated shocks,\nhence that it can be used to detect large and small shocks, including those\nwhich do not lead to a recession; we also discuss its predictive power.\nFinally, a two-sector toy model of recession and recovery illustrates how the\nseverity and length of recession depends on the dynamics of transfer rate\nbetween the growing and failing parts of the economy.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.2004v5"
    },
    {
        "title": "Gamma-distribution and wealth inequality",
        "authors": [
            "Anirban Chakraborti",
            "Marco Patriarca"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We discuss the equivalence between kinetic wealth-exchange models, in which\nagents exchange wealth during trades, and mechanical models of particles,\nexchanging energy during collisions. The universality of the underlying\ndynamics is shown both through a variational approach based on the minimization\nof the Boltzmann entropy and a complementary microscopic analysis of the\ncollision dynamics of molecules in a gas. In various relevant cases the\nequilibrium distribution is the same for all these models, namely a\ngamma-distribution with suitably defined temperature and number of dimensions.\nThis in turn allows one to quantify the inequalities observed in the wealth\ndistributions and suggests that their origin should be traced back to very\ngeneral underlying mechanisms: for instance, it follows that the smaller the\nfraction of the relevant quantity (e.g. wealth or energy) that agents can\nexchange during an interaction, the closer the corresponding equilibrium\ndistribution is to a fair distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.4410v1"
    },
    {
        "title": "Balance, growth and diversity of financial markets",
        "authors": [
            "Constantinos Kardaras"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  A financial market comprising of a certain number of distinct companies is\nconsidered, and the following statement is proved: either a specific agent will\nsurely beat the whole market unconditionally in the long run, or (and this \"or\"\nis not exclusive) all the capital of the market will accumulate in one company.\nThus, absence of any \"free unbounded lunches relative to the total capital\"\nopportunities lead to the most dramatic failure of diversity in the market: one\ncompany takes over all other until the end of time. In order to prove this, we\nintroduce the notion of perfectly balanced markets, which is an equilibrium\nstate in which the relative capitalization of each company is a martingale\nunder the physical probability. Then, the weaker notion of balanced markets is\ndiscussed where the martingale property of the relative capitalizations holds\nonly approximately, we show how these concepts relate to growth-optimality and\nefficiency of the market, as well as how we can infer a shadow interest rate\nthat is implied in the economy in the absence of a bank.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.1858v1"
    },
    {
        "title": "Business Cycle and Conserved Quantity in Economics",
        "authors": [
            "Masa-aki Taniguchi",
            "Masako Bando",
            "Akihiro Nakayama"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We propose a dynamical model for business cycle based on an optimal DI model.\nIn the model there exists a conserved quantity, which corresponds to the total\nenergy in a dynamical system. We found that the business cycle with the period\n6 or 7 years is nicely reproduced, since the model predicts a periodic motion\nin the conservative system.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.1916v1"
    },
    {
        "title": "Continuous growth models in terms of generalized logarithm and\n  exponential functions",
        "authors": [
            "Alexandre Souto Martinez",
            "Rodrigo Silva Gonzalez",
            "Cesar Augusto Sangaletti Tercariol"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Consider the one-parameter generalizations of the logarithmic and exponential\nfunctions which are obtained from the integration of non-symmetrical\nhyperboles. These generalizations coincide to the one obtained in the context\nof non-extensive thermostatistics. We show that these functions are suitable to\ndescribe and unify the great majority of continuous growth models, which we\nbriefly review. Physical interpretation to the generalization function\nparameter is given for the Richards' model, which has an underlying microscopic\nmodel to justify it.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2635v2"
    },
    {
        "title": "The virtues and vices of equilibrium and the future of financial\n  economics",
        "authors": [
            "J. Doyne Farmer",
            "John Geanakoplos"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  The use of equilibrium models in economics springs from the desire for\nparsimonious models of economic phenomena that take human reasoning into\naccount. This approach has been the cornerstone of modern economic theory. We\nexplain why this is so, extolling the virtues of equilibrium theory; then we\npresent a critique and describe why this approach is inherently limited, and\nwhy economics needs to move in new directions if it is to continue to make\nprogress. We stress that this shouldn't be a question of dogma, but should be\nresolved empirically. There are situations where equilibrium models provide\nuseful predictions and there are situations where they can never provide useful\npredictions. There are also many situations where the jury is still out, i.e.,\nwhere so far they fail to provide a good description of the world, but where\nproper extensions might change this. Our goal is to convince the skeptics that\nequilibrium models can be useful, but also to make traditional economists more\naware of the limitations of equilibrium models. We sketch some alternative\napproaches and discuss why they should play an important role in future\nresearch in economics.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2996v1"
    },
    {
        "title": "Constructing the Optimal Solutions to the Undiscounted Continuous-Time\n  Infinite Horizon Optimization Problems",
        "authors": [
            "Dapeng CAI",
            "Takashi Gyoshin NITTA"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We aim to construct the optimal solutions to the undiscounted continuous-time\ninfinite horizon optimization problems, the objective functionals of which may\nbe unbounded. We identify the condition under which the limit of the solutions\nto the finite horizon problems is optimal for the infinite horizon problems\nunder the overtaking criterion.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.4046v1"
    },
    {
        "title": "Limit of the Solutions for the Finite Horizon Problems as the Optimal\n  Solution to the Infinite Horizon Optimization Problems",
        "authors": [
            "Dapeng CAI",
            "Takashi Gyoshin NITTA"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We aim to generalize the results of Cai and Nitta (2007) by allowing both the\nutility and production function to depend on time. We also consider an\nadditional intertemporal optimality criterion. We clarify the conditions under\nwhich the limit of the solutions for the finite horizon problems is optimal\namong all attainable paths for the infinite horizon problems under the\novertaking criterion, as well as the conditions under which such a limit is the\nunique optimum under the sum-of-utilities criterion. The results are applied to\na parametric example of the one-sector growth model to examine the impacts of\ndiscounting on optimal paths.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.4050v1"
    },
    {
        "title": "Demand forecasting for companies with many branches, low sales numbers\n  per product, and non-recurring orderings",
        "authors": [
            "Sascha Kurz",
            "Joerg Rambau"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We propose the new Top-Dog-Index to quantify the historic deviation of the\nsupply data of many small branches for a commodity group from sales data. On\nthe one hand, the common parametric assumptions on the customer demand\ndistribution in the literature could not at all be supported in our real-world\ndata set. On the other hand, a reasonably-looking non-parametric approach to\nestimate the demand distribution for the different branches directly from the\nsales distribution could only provide us with statistically weak and unreliable\nestimates for the future demand. Based on real-world sales data from our\nindustry partner we provide evidence that our Top-Dog-Index is statistically\nrobust. Using the Top-Dog-Index, we propose a heuristics to improve the\nbranch-dependent proportion between supply and demand. Our approach cannot\nestimate the branch-dependent demand directly. It can, however, classify the\nbranches into a given number of clusters according to an historic oversupply or\nundersupply. This classification of branches can iteratively be used to adapt\nthe branch distribution of supply and demand in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.1414v1"
    },
    {
        "title": "A note on wealth in a volatile economy",
        "authors": [
            "M. Marsili"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  I show that if the capital accumulation dynamics is stochastic a new term, in\naddition to that given by accounting prices, has to be introduced in order to\nderive a correct estimate of the genuine wealth of an economy. In a simple\nmodel with multiplicative accumulation dynamics I show that: 1) the value\nfunction is always a decreasing function of volatility 2) the accounting prices\nare affected by volatility 3) the new term always gives a negative contribution\nto wealth changes. I discuss results for models with constant elasticity\nutility functions. When the elasticity of marginal utility is larger than one,\naccounting prices increase with volatility whereas when it is less than one\naccounting prices decrease with volatility. These conclusions are not altered\nwhen adopting optimal saving rates.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.2772v1"
    },
    {
        "title": "Global recessions as a cascade phenomenon with heterogenous, interacting\n  agents",
        "authors": [
            "Paul Ormerod"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  I examine global recessions as a cascade phenomenon. In other words, how\nrecessions arising in one or more countries might percolate across a network of\nconnected economies. A heterogeneous agent based model is set up in which the\nagents are Western economies. A country has a probability of entering a\nrecession in any given year and one of emerging from it the next. In addition,\nthe agents have a unique threshold propensity to import a recession from the\nagents with which they have the strongest connections. They are connected on a\nsmall world topology, and an agent's neighbours at any time are either in\n(state 1) or out (state 0) of recession. If the weighted sum exceeds the\nthreshold, the agent goes into recession. Annual real GDP growth for 17 Western\ncountries 1871-2006 is used as the data set. The distribution of the number of\ncountries in recession in any given year is exponential, as is the duration of\nrecessions within individual countries. The model is calibrated against these\ntwo facts, plus the 'wait time' between recessions. It is able to replicate\nthem successfully. The network structure is essential for the agents to\nreplicate the stylised facts. The country-specific probabilities of entering\nand emerging from recession by themselves give results very different to the\nactual data.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1639v1"
    },
    {
        "title": "Agent-based model of competition in a social structure",
        "authors": [
            "Erika Fille Legara",
            "Anthony Longjas",
            "Rene Batac"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Indirect competition emerged from the complex organization of human\nsocieties, and knowledge of the existing network topology may aid in developing\neffective strategies for success. Here, we propose an agent-based model of\ncompetition with systems co-existing in a `small-world' social network. We show\nthat within the range of parameter values obtained from the model and empirical\ndata, the network evolution is highly dependent on $k$, the local parameter\ndescribing the density of neighbors in the network. The model applied to\nlanguage death and competition of telecommunication companies show strong\ncorrespondence with empirical data.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.3059v2"
    },
    {
        "title": "Modelling interest rates by correlated multi-factor CIR-like processes",
        "authors": [
            "L. Bertini",
            "L. Passalacqua"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We investigate the joint description of the interest-rate term stuctures of\nItaly and an AAA-rated European country by mean of a --here proposed--\ncorrelated CIR-like bivariate model where one of the state variables is\ninterpreted as a benchmark risk-free rate and the other as a credit spread. The\nmodel is constructed by requiring the strict positivity of interest rates and\nthe asymptotic decoupling of the joint distribution of the two state variables\non a long time horizon. The second condition is met by imposing the\nreversibility of the process with respect to a product measure, the first is\nthen implemented by using the tools of potential theory. It turns out that\nthese conditions select a class of non-affine models, out of which we choose\none that is quadratic in the two state variables both in the drift and\ndiffusion matrix. We perform a numerical analysis of the model by investigating\na cross section of the term structures comparing the results with those\nobtained with an uncoupled bivariate CIR model.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.3898v1"
    },
    {
        "title": "Taxes in a simple wealth distribution model by inelastically scattering\n  particles",
        "authors": [
            "Sebastian D. Guala"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  In this work we use an inelastic scattering process of particles to propose a\nmodel able to reproduce the salient features of the wealth distribution in an\neconomy by including taxes to each trading process and redistributing that\ncollected among the population according to a given criterion. Additionally, we\nshow that different optimal levels of taxes may exist depending on the\nredistribution criterion.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.4484v1"
    },
    {
        "title": "Shelf space strategy in long-tail markets",
        "authors": [
            "R. Alexander Bentley",
            "Paul Ormerod",
            "Mark E. Madsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  The Internet is known to have had a powerful impact on on-line retailer\nstrategies in markets characterised by long-tail distribution of sales. Such\nretailers can exploit the long tail of the market, since they are effectively\nwithout physical limit on the number of choices on offer. Here we examine two\nextensions of this phenomenon. First, we introduce turnover into the long-tail\ndistribution of sales. Although over any given period such as a week or a\nmonth, the distribution is right-skewed and often power law distributed, over\ntime there is considerable turnover in the rankings of sales of individual\nproducts. Second, we establish some initial results on the implications for\nshelf-space strategy of physical retailers in such markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.1655v2"
    },
    {
        "title": "Network effects in a human capital based economic growth model",
        "authors": [
            "Teresa Vaz Martins",
            "Tanya Araujo",
            "Maria Augusta Santos",
            "Miguel St Aubyn"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We revisit a recently introduced agent model[ACS {\\bf 11}, 99 (2008)], where\neconomic growth is a consequence of education (human capital formation) and\ninnovation, and investigate the influence of the agents' social network, both\non an agent's decision to pursue education and on the output of new ideas.\nRegular and random networks are considered. The results are compared with the\npredictions of a mean field (representative agent) model.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.3418v2"
    },
    {
        "title": "Hedging and production decisions under uncertainty: A survey",
        "authors": [
            "Moawia Alghalith"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  This paper synthesizes and analyzes some important current and recent\ncontributions to the theory of the firm under uncertainty. In so doing, it\nexamines the production and hedging decisions of the competitive firm under a\nsingle source and multiple sources of uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.0917v1"
    },
    {
        "title": "Trust! Why it Has Been Lost and How to Regain It",
        "authors": [
            "D. Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  This essay suggests that a proper assessment of the presently unfolding\nfinancial crisis, and its cure, requires going back at least to the late 1990s,\naccounting for the cumulative effect of the ITC, real-estate and financial\nderivative bubbles. We focus on the deep loss of trust, not only in Wall\nStreet, but more importantly in Main Street, and how to recover it on the short\nand long terms. A multi-disciplinary approach is needed to deal with the\nnonlinear complex systems of the present world, in order to develop a culture\nof fairness, and of upside opportunities associated with a risky world.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.4608v1"
    },
    {
        "title": "Economics need a scientific revolution",
        "authors": [
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  I argue that the current financial crisis highlights the crucial need of a\nchange of mindset in economics and financial engineering, that should move away\nfrom dogmatic axioms and focus more on data, orders of magnitudes, and\nplausible, albeit non rigorous, arguments.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.5306v1"
    },
    {
        "title": "Evolution of the personal income distribution in the USA: High incomes",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  The personal income distribution (PID) above the Pareto threshold is studied\nand modeled. A microeconomic model is proposed to simulate the PID and its\nevolution below and above the Pareto income threshold. The model balances\nprocesses of income production and dissipation for any person above 15 years of\nage. The model accurately predicts the observed dependence of the number of\npeople reaching the Pareto threshold on work experience and the functional\ndependence of the relationship on the per capita real GDP growth for the period\nfrom 1994 to 2002. Predictions of the income distribution depending on age are\ngiven for past and future. In future, relatively less rich people are observed\nin the younger age groups and the peak of the relative number shifts to older\nages with time. The effect of the power law distribution extending itself to\nvery high incomes is speculated to be the cause of low performance of socialist\ncountries.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0352v1"
    },
    {
        "title": "Modeling the evolution of Gini coefficient for personal incomes in the\n  USA between 1947 and 2005",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  The evolution of Gini coefficient for personal incomes in the USA between\n1947 and 2005 is analyzed and modeled. There are several versions of personal\nincome distribution (PID) provided by the US Census Bureau (US CB) for this\nperiod with various levels of resolution. Effectively, these PIDs result in\ndifferent Gini coefficients due to the differences between discrete and\ncontinuous representations. When all persons of 15 years of age and over are\nincluded in the PIDs, Gini coefficient drops from 0.64 in 1947 to 0.54 in 1990.\nThis effect is observed due to a significant decrease in the portion of people\nwithout income. For the PIDs not including persons without income, Gini\ncoefficient is varying around 0.51 between 1960 and 2005 with standard\ndeviation of 0.004, i.e. is in fact constant. This Gini coefficient is\npractically independent on the portion of population included in the PIDs\naccording to any revision of income definitions. The driving force of the model\ndescribing the evolution of individual incomes (microeconomic level) and their\naggregate value (macroeconomic level) is the change in nominal GDP per capita.\nThe model accurately predicts the evolution of Gini coefficient for the PIDs\nfor people with income. The model gives practically unchanged (normalized) PIDs\nand Gini coefficient between 1947 and 2005. The empirical Gini curves converge\nto the predicted one when the number of people without income decreases.\nAsymptotically, the empirical curves should collapse to the theoretical one\nwhen all the working age population obtains an appropriate definition of\nincome. Therefore the model Gini coefficient potentially better describes true\nbehavior of inequality in the USA because the definitions of income used by the\nUS Census Bureau apparently fail to describe true income distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0356v1"
    },
    {
        "title": "Modelling the average income dependence on work experience",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  The average and median income dependence on work experience and time is\nanalyzed and modeled for the USA. The original data set providing the mean and\nmedian income estimates in 10 year long intervals spans a long time period of\nalmost 35 years ? from 1967 to 2003. A microeconomic model linking personal\nincome, population age structure and GDP per capita is used to predict the mean\nincome values in various age groups and their relative evolution in time. Also\nmodeled is the value of work experience where the mean income growth ends and\nit starts to drop exponentially with increasing age. This work experience\nincreases through time as the square root of the per capita GDP growth.\nPrediction for the following 20 years is given for each age group considering\npotential per capita growth rate of 1.6%. The USA mean income dependence on\nwork experience for 1987 coincides with that for 2002 in the UK ? the years\nwhen per capita GDP were equal in the countries.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0489v1"
    },
    {
        "title": "Modelling real GDP per capita in the USA: cointegration test",
        "authors": [
            "Ivan O. Kitov",
            "Oleg I. Kitov",
            "Svetlana A. Dolinskaya"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  A two-component model for the evolution of real GDP per capita in the USA is\npresented and tested. The first component of the GDP growth rate represents an\neconomic trend and is inversely proportional to the attained level of real GDP\nper capita itself, with the nominator being constant through time. The second\ncomponent is responsible for fluctuations around the economic trend and is\ndefined as a half of the growth rate of the number of 9-year-olds. This\nnonlinear relationship between the growth rate of real GDP per capita and the\nnumber of 9-year-olds in the USA is tested for cointegration. For linearization\nof the problem, a predicted population time series is calculated using the\noriginal relationship. Both single year of age population time series, the\nmeasured and predicted one, are shown to be integrated of order 1. The\nEngel-Granger approach is applied to the difference of the measured and\npredicted time series and to the residuals or corresponding linear regression.\nBoth tests show the existence of a cointegrating relation. The Johansen test\nresults in the cointegrating rank 1. Since a cointegrating relation between the\nmeasured and predicted number of 9-year-olds does exist, the VAR, VECM, and\nlinear regression are used in estimation of the goodness of fit and root\nmean-square errors, RMSE. The highest R2=0.95 and the best RMSE is obtained in\nthe VAR representation. Econometrically, the tests for cointegration show that\nthe deviations of real economic growth in the USA from the economic trend, as\ndefined by the constant annual increment of real per capita GDP, are driven by\nthe change in the number of 9-year-olds.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0490v1"
    },
    {
        "title": "Real GDP per capita in developed countries",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Growth rate of real GDP per capita is represented as a sum of two components\n-- a monotonically decreasing economic trend and fluctuations related to a\nspecific age population change. The economic trend is modeled by an inverse\nfunction of real GDP per capita with a numerator potentially constant for the\nlargest developed economies. Statistical analysis of 19 selected OECD countries\nfor the period between 1950 and 2004 shows a very weak linear trend in the\nannual GDP per capita increment for the largest economies: the USA, Japan,\nFrance, Italy, and Spain. The UK, Australia, and Canada show a larger positive\nlinear trend. The fluctuations around the trend values are characterized by a\nquasi-normal distribution with potentially Levy distribution for far tails.\nDeveloping countries demonstrate the increment values far below the mean\nincrement for the most developed economies. This indicates an underperformance\nin spite of large relative growth rates.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0889v1"
    },
    {
        "title": "Inflation as a function of labor force change rate: cointegration test\n  for the USA",
        "authors": [
            "Ivan O. Kitov",
            "Oleg I. Kitov",
            "Svetlana A. Dolinskaya"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  A linear and lagged relationship between inflation and labor force change\nrate, p(t)= A1dLF(t-t1)/LF(t-t1)+A2 was found for developed economies. For the\nUSA, A1=4.0, A2=-0.03075, and t1=2 years. It provides a RMS forecasting error\n(RMFSE) of 0.8% at a two-year horizon for the period between 1965 and 2002 (the\nbest among other inflation forecasting models). This relationship is tested for\ncointegration. Both variables are integrated of order one according to the\npresence of a unit root in the series and its absence in their first\ndifferences. Two methods of cointegration testing are applied: the\nEngle-Granger one based on the unit root test of the residuals including a\nvariety of specification tests and the Johansen cointegration rank test based\non the VAR representation. Both approaches demonstrate that the variables are\ncointegrated and the long-run equilibrium relation revealed in previous study\nholds. According to the Granger causality test, the labor force change is\nproved to be a weakly exogenous variable - a natural result considering the\ntime lead and the existence of a cointegrating relation. VAR and VECM\nrepresentations do not provide any significant improvement in RMSFE. There are\nnumerous applications of the equation: from purely theoretical - a robust\nfundamental relation between macroeconomic and population variables, to a\npractical one - an accurate out-of-sample inflation forecasting at a two-year\nhorizon and a long-term prediction based on labor force projections. The\npredictive power of the relationship is inversely proportional to the\nuncertainty of labor force estimates. Therefore, future inflation research\nprograms should start from a significant improvement in the accuracy of labor\nforce estimations.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0892v1"
    },
    {
        "title": "Relationship between inflation, unemployment and labor force change rate\n  in France: cointegration test",
        "authors": [
            "Ivan O. Kitov",
            "Oleg I. Kitov",
            "Svetlana A. Dolinskaya"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  A linear and lagged relationship between inflation, unemployment and labor\nforce change rate, p(t)=A0UE(t-t0)+A1dLF(t-t1)/LF(t-t1)+ A2, where A0, A1, and\nA2 are empirical country-specific coefficients, was found for developed\neconomies. The relationship obtained for France is characterized by A0=-1,\nA1=4, A2=0.095, t0=4 years, and t1=4 years. For GDP deflator, it provides a RMS\nforecasting error (RMFSE) of 1.0% at a four-year horizon for the period between\n1971 and 2004. The relationship is tested for cointegration. All three\nvariables involved in the relationship are proved to be integrated of order\none. Two methods of cointegration testing are used. First is the Engle-Granger\napproach based on the unit root test in the residuals of linear regression,\nwhich also includes a number of specification tests. Second method is the\nJohansen cointegration rank test based on a VAR representation, which is also\nproved to be an adequate one via a set of appropriate tests. Both approaches\ndemonstrate that the variables are cointegrated and the long-run equilibrium\nrelation revealed in previous study holds together with statistical estimates\nof goodness-of-fit and RMSFE. Relationships between inflation and labor force\nand between unemployment and labor force are tested separately in appropriate\ntime intervals, where the Banque de France monetary policy introduced in 1995\ndoes not disturb the long-term links. All the individual relationships are\ncointegrated in corresponding intervals. The VAR and vector error correction\n(VEC) models are estimated and provide just a marginal improvement in RMSFE at\nthe four-year horizon both for GDP deflator (down to 0.9%) and CPI (~1.1%) on\nthe results obtained in the regression study.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0896v1"
    },
    {
        "title": "Modelling the transition from a socialist to capitalist economic system",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  The transition of several East and Central European countries and the\ncountries of the Former Soviet Union from the socialist economic system to the\ncapitalist one is studied. A recently developed microeconomic model for the\npersonal income distribution and its evolution and a simple functional\nrelationship between the rate of the per capita GDP growth and the attained\nlevel of the per capita GDP are used to describe the transition process. The\ndeveloped transition model contains only three defining parameters and\ndescribes the process of real GDP per capita evolution during the last 15\nyears. It is found that the transition process finished in the Central European\ncountries several years ago and their economic evolution is defined by pure\ncapitalist rules. In the long run, this means that the future of these\ncountries has to follow the same path, i.e. dependence on the per capita GDP\ngrowth rate of the per capita GDP itself, as the developed countries have had\nin the past. If the best GDP evolution scenario occurs for the studied\ncountries, they will be able to maintain the absolute lag in per capita GDP\nrelative to most developed countries including the USA. But they will never\ncatch the advanced countries if they follow the same rules of development. In\nRussia and some countries of the Former Soviet Union the transition process is\nstill far from complete.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.1182v1"
    },
    {
        "title": "The driving force of labor productivity",
        "authors": [
            "Ivan O. Kitov",
            "Oleg I. kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Labor productivity in developed countries is analyzed and modeled. Modeling\nis based on our previous finding that the rate of labor force participation is\na unique function of GDP per capita. Therefore, labor productivity is fully\ndetermined by the rate of economic growth, and thus, is a secondary economic\nvariable. Initially, we assess a model for the U.S. and then test it using data\nfor Japan, France, the UK, Italy, and Canada. Results obtained for these\ncountries validate those for the U.S. The evolution of labor force productivity\nis predictable at least at an 11-year horizon\n",
        "pdf_link": "http://arxiv.org/pdf/0811.2124v1"
    },
    {
        "title": "GDP growth rate and population",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Real GDP growth rate in developed countries is found to be a sum of two\nterms. The first term is the reciprocal value of the duration of the period of\nmean income growth with work experience, Tcr. The current value of Tcr in the\nUSA is 40 years. The second term is inherently related to population and\ndefined by the relative change in the number of people with a specific age (9\nyears in the USA), (1/2)*dN9(t) /N9(t), where N9(t) is the number of\n9-year-olds at time t. The Tcr grows as the square root of real GDP per capita.\nHence, evolution of real GDP is defined by only one parameter - the number of\npeople of the specific age. Predictions for the USA, the UK, and France are\npresented and discussed. A similar relationship is derived for real GDP per\ncapita. Annual increment of GDP per capita is also a combination of economic\ntrend term and the same specific age population term. The economic trend term\nduring last 55 years is equal to $400 (2002 US dollars) divided by the attained\nlevel of real GDP per capita. Thus, the economic trend term has an asymptotic\nvalue of zero. Inversion of the measured GDP values is used to recover the\ncorresponding change of the specific age population between 1955 and 2003. The\npopulation recovery method based on GDP potentially is of a higher accuracy\nthan routine censuses.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.2125v1"
    },
    {
        "title": "An Apology for Money",
        "authors": [
            "Karl Svozil"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  This review is about the convenience, the benefits, as well as the\ndestructive capacities of money. It deals with various aspects of money\ncreation, with its value, and its appropriation. All sorts of money tend to get\ncorrupted by eventually creating too much of them. In the long run, this\nrenders money worthless and deprives people holding it. This misuse of money\ncreation is inevitable and should come as no surprise. Abusive money creation\ncomes in various forms. In the present fiat money system \"suspended in free\nthought\" and sustained merely by our belief in and our conditioning to it,\nmoney is conveniently created out of \"thin air\" by excessive government\nspending and speculative credit creation. Alas, any too tight money supply\ncould ruin an economy by inviting all sorts of unfriendly takeovers, including\nwars or competition. Therefore the ambivalence of money as benefactor and\ndestroyer should be accepted as destiny.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.3130v6"
    },
    {
        "title": "Mathematics underlying the 2008 financial crisis, and a possible remedy",
        "authors": [
            "V. P. Maslov",
            "V. E. Nazaikinskii"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  This paper has been withdrawn by the authors, because it has been made\nobsolete by the detailed expositions in our papers in arXiv:0812.4885 (the\nmathematics part) and arXiv:0812.4737 (the economics part).\n",
        "pdf_link": "http://arxiv.org/pdf/0811.4678v2"
    },
    {
        "title": "Structure and temporal change of the credit network between banks and\n  large firms in Japan",
        "authors": [
            "Yoshi Fujiwara",
            "Hideaki Aoyama",
            "Yuichi Ikeda",
            "Hiroshi Iyetomi",
            "Wataru Souma"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We present a new approach to understanding credit relationships between\ncommercial banks and quoted firms, and with this approach, examine the temporal\nchange in the structure of the Japanese credit network from 1980 to 2005. At\neach year, the credit network is regarded as a weighted bipartite graph where\nedges correspond to the relationships and weights refer to the amounts of\nloans. Reduction in the supply of credit affects firms as debtor, and failure\nof a firm influences banks as creditor. To quantify the dependency and\ninfluence between banks and firms, we propose a set of scores of banks and\nfirms, which can be calculated by solving an eigenvalue problem determined by\nthe weight of the credit network. We found that a few largest eigenvalues and\ncorresponding eigenvectors are significant by using a null hypothesis of random\nbipartite graphs, and that the scores can quantitatively describe the stability\nor fragility of the credit network during the 25 years.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.2377v3"
    },
    {
        "title": "Visualizing a large-scale structure of production network by N-body\n  simulation",
        "authors": [
            "Yoshi Fujiwara"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Our recent study of a nation-wide production network uncovered a community\nstructure, namely how firms are connected by supplier-customer links into\ntightly-knit groups with high density in intra-groups and with lower\nconnectivity in inter-groups. Here we propose a method to visualize the\ncommunity structure by a graph layout based on a physical analogy. The layout\ncan be calculated in a practical computation-time and is possible to be\naccelerated by a special-purpose device of GRAPE (gravity pipeline) developed\nfor astrophysical N-body simulation. We show that the method successfully\nidentifies the communities in a hierarchical way by applying it to the\nmanufacturing sector comprising tenth million nodes and a half million edges.\nIn addition, we discuss several limitations of this method, and propose a\npossible way to avoid all those problems.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.2381v1"
    },
    {
        "title": "Timed tuplix calculus and the Wesseling and van den Bergh equation",
        "authors": [
            "J. A. Bergstra",
            "C. A. Middelburg"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We develop an algebraic framework for the description and analysis of\nfinancial behaviours, that is, behaviours that consist of transferring certain\namounts of money at planned times. To a large extent, analysis of financial\nproducts amounts to analysis of such behaviours. We formalize the cumulative\ninterest compliant conservation requirement for financial products proposed by\nWesseling and van den Bergh by an equation in the framework developed and\ndefine a notion of financial product behaviour using this formalization. We\nalso present some properties of financial product behaviours. The development\nof the framework has been influenced by previous work on the process algebra\nACP.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.3003v4"
    },
    {
        "title": "The role of a matchmaker in buyer-vendor interactions",
        "authors": [
            "Linyuan Lü",
            "Matus Medo",
            "Yi-Cheng Zhang"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We consider a simple market where a vendor offers multiple variants of a\ncertain product and preferences of both the vendor and potential buyers are\nheterogeneous and possibly even antagonistic. Optimization of the joint benefit\nof the vendor and the buyers turns the toy market into a combinatorial matching\nproblem. We compare the optimal solutions found with and without a matchmaker,\nexamine the resulting inequality between the market participants, and study the\nimpact of correlations on the system.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.0504v2"
    },
    {
        "title": "Backbone of complex networks of corporations: The flow of control",
        "authors": [
            "J. B. Glattfelder",
            "S. Battiston"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We present a methodology to extract the backbone of complex networks based on\nthe weight and direction of links, as well as on nontopological properties of\nnodes. We show how the methodology can be applied in general to networks in\nwhich mass or energy is flowing along the links. In particular, the procedure\nenables us to address important questions in economics, namely, how control and\nwealth are structured and concentrated across national markets. We report on\nthe first cross-country investigation of ownership networks, focusing on the\nstock markets of 48 countries around the world. On the one hand, our analysis\nconfirms results expected on the basis of the literature on corporate control,\nnamely, that in Anglo-Saxon countries control tends to be dispersed among\nnumerous shareholders. On the other hand, it also reveals that in the same\ncountries, control is found to be highly concentrated at the global level,\nnamely, lying in the hands of very few important shareholders. Interestingly,\nthe exact opposite is observed for European countries. These results have\npreviously not been reported as they are not observable without the kind of\nnetwork analysis developed here.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.0878v2"
    },
    {
        "title": "A Paradigm Shift from Production Function to Production Copula:\n  Statistical Description of Production Activity of Firms",
        "authors": [
            "H. Iyetomi",
            "H. Aoyama",
            "Y. Fujiwara",
            "Y. Ikeda",
            "W. Souma"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Heterogeneity of economic agents is emphasized in a new trend of\nmacroeconomics. Accordingly the new emerging discipline requires one to replace\nthe production function, one of key ideas in the conventional economics, by an\nalternative which can take an explicit account of distribution of firms'\nproduction activities. In this paper we propose a new idea referred to as\nproduction copula; a copula is an analytic means for modeling dependence among\nvariables. Such a production copula predicts value added yielded by firms with\ngiven capital and labor in a probabilistic way. It is thereby in sharp contrast\nto the production function where the output of firms is completely\ndeterministic. We demonstrate empirical construction of a production copula\nusing financial data of listed firms in Japan. Analysis of the data shows that\nthere are significant correlations among their capital, labor and value added\nand confirms that the values added are too widely scattered to be represented\nby a production function. We employ four models for the production copula, that\nis, trivariate versions of Frank, Gumbel and survival Clayton and\nnon-exchangeable trivariate Gumbel; the last one works best.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.1576v3"
    },
    {
        "title": "A dynamic nonlinear model for saturation in industrial growth",
        "authors": [
            "Arnab K. Ray"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  A general nonlinear logistic equation has been proposed to model long-time\nsaturation in industrial growth. An integral solution of this equation has been\nderived for any arbitrary degree of nonlinearity. A time scale for the onset of\nnonlinear saturation in industrial growth can be estimated from an\nequipartition condition between nonlinearity and purely exponential growth.\nPrecise predictions can be made about the limiting values of the annual revenue\nand the human resource content that an industrial organisation may attain.\nThese variables have also been modelled to set up an autonomous first-order\ndynamical system, whose equilibrium condition forms a stable node (an attractor\nstate) in a related phase portrait. The theoretical model has received close\nsupport from all relevant data pertaining to the well-known global company,\nIBM.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.0282v1"
    },
    {
        "title": "Financial Atoms and Molecules",
        "authors": [
            "Yik Wen Goo",
            "Tong Wei Lian",
            "Wei Guang Ong",
            "Wen Ting Choi",
            "Siew-Ann Cheong"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Atoms and molecules are important conceptual entities we invented to\nunderstand the physical world around us. The key to their usefulness lies in\nthe organization of nuclear and electronic degrees of freedom into a single\ndynamical variable whose time evolution we can better imagine. The use of such\neffective variables in place of the true microscopic variables is possible\nbecause of the separation between nuclear/electronic and atomic/molecular time\nscales. Where separation of time scales occurs, identification of analogous\nobjects in financial markets can help advance our understanding of their\ndynamics. To detect separated time scales and identify their associated\neffective degrees of freedom in financial markets, we devised a two-stage\nstatistical clustering scheme to analyze the price movements of stocks in\nseveral equity markets. Through this two-time-scale clustering analysis, we\ndiscovered a hierarchy of levels of self-organization in real financial\nmarkets. We call these statistically robust self-organized dynamical structures\nfinancial atoms, financial molecules, and financial supermolecules. In general,\nthe detailed compositions of these dynamical structures cannot be deduced based\non raw financial intuition alone, and must be explained in terms of the\nunderlying portfolios, and investment strategies of market players. More\ninterestingly, we find that major market events such as the Chinese Correction\nand the Subprime Crisis leave many tell-tale signs within the correlational\nstructures of financial molecules.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.2099v1"
    },
    {
        "title": "Numéraire-invariant preferences in financial modeling",
        "authors": [
            "Constantinos Kardaras"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We provide an axiomatic foundation for the representation of\nnum\\'{e}raire-invariant preferences of economic agents acting in a financial\nmarket. In a static environment, the simple axioms turn out to be equivalent to\nthe following choice rule: the agent prefers one outcome over another if and\nonly if the expected (under the agent's subjective probability) relative rate\nof return of the latter outcome with respect to the former is nonpositive. With\nthe addition of a transitivity requirement, this last preference relation has\nan extension that can be numerically represented by expected logarithmic\nutility. We also treat the case of a dynamic environment where consumption\nstreams are the objects of choice. There, a novel result concerning a canonical\nrepresentation of unit-mass optional measures enables us to explicitly solve\nthe investment--consumption problem by separating the two aspects of investment\nand consumption. Finally, we give an application to the problem of optimal\nnum\\'{e}raire investment with a random time-horizon.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.3736v5"
    },
    {
        "title": "Unemployment and inflation in Western Europe: solution by the boundary\n  element method",
        "authors": [
            "Ivan Kitov",
            "Oleg Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Using an analog of the boundary element method in engineering and science, we\nanalyze and model unemployment rate in Austria, Italy, the Netherlands, Sweden,\nSwitzerland, and the United States as a function of inflation and the change in\nlabor force. Originally, the model linking unemployment to inflation and labor\nforce was developed and successfully tested for Austria, Canada, France,\nGermany, Japan, and the United States. Autoregressive properties of neither of\nthese variables are used to predict their evolution. In this sense, the model\nis a self-consistent and completely deterministic one without any stochastic\ncomponent (external shocks) except that associated with measurement errors and\nchanges in measurement units. Nevertheless, the model explains between 65% and\n95% of the variability in unemployment and inflation. For Italy, the rate of\nunemployment is predicted at a time horizon of nine years with pseudo\nout-of-sample root-mean-square forecasting error of 0.55% for the period\nbetween 1973 and 2006. One can expect that the u nemployment will be growing\nsince 2008 and will reach 11.4% near 2012. After 2012, unemployment in Italy\nwill start to descend.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.5064v1"
    },
    {
        "title": "The (unfortunate) complexity of the economy",
        "authors": [
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This article is a follow-up of a short essay that appeared in Nature 455,\n1181 (2008) [arXiv:0810.5306]. It has become increasingly clear that the\nerratic dynamics of markets is mostly endogenous and not due to the rational\nprocessing of exogenous news. I elaborate on the idea that spin-glass type of\nproblems, where the combination of competition and heterogeneities generically\nleads to long epochs of statis interrupted by crises and hyper-sensitivity to\nsmall changes of the environment, could be metaphors for the complexity of\neconomic systems. I argue that the most valuable contribution of physics to\neconomics might end up being of methodological nature, and that simple models\nfrom physics and agent based numerical simulations, although highly stylized,\nare more realistic than the traditional models of economics that assume\nrational agents with infinite foresight and infinite computing abilities.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.0805v1"
    },
    {
        "title": "An Introduction to Hedge Funds",
        "authors": [
            "Sovan Mitra"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This report was originally written as an industry white paper on Hedge Funds.\nThis paper gives an overview to Hedge Funds, with a focus on risk management\nissues. We define and explain the general characteristics of Hedge Funds, their\nmain investment strategies and the risk models employed. We address the\nproblems in Hedge Fund modelling, survey current Hedge Funds available on the\nmarket and those that have been withdrawn. Finally, we summarise the supporting\nand opposing arguments for Hedge Fund usage. A unique value of this paper,\ncompared to other Hedge Fund literature freely available on the internet, is\nthat this review is fully sourced from academic references (such as peer\nreviewed journals) and is thus a bona fide study. This paper will be of\ninterest to: Hedge Fund and Mutual Fund Managers, Quantitative Analysts,\n\"Front\" and \"Middle\" office banking functions e.g. Treasury Management,\nRegulators concerned with Hedge Fund Financial Risk Management, Private and\nInstitutional Investors, Academic Researchers in the area of Financial Risk\nManagement and the general Finance community.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.2731v2"
    },
    {
        "title": "Generalized supermartingale deflators under limited information",
        "authors": [
            "Constantinos Kardaras"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We undertake a study of markets from the perspective of a financial agent\nwith limited access to information. The set of wealth processes available to\nthe agent is structured with reasonable economic properties, instead of the\nusual practice of taking it to consist of stochastic integrals against a\nsemimartingale integrator. We obtain the equivalence of the boundedness in\nprobability of the set of terminal wealth outcomes (which in turn is equivalent\nto the weak market viability condition of absence of arbitrage of the first\nkind) with the existence of at least one strictly positive deflator that makes\nthe deflated wealth processes have a generalized supermartingale property.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.2913v4"
    },
    {
        "title": "Macroeconomic Phase Transitions Detected from the Dow Jones Industrial\n  Average Time Series",
        "authors": [
            "Wong Jian Cheng",
            "Lian Heng",
            "Cheong Siew Ann"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  In this paper, we perform statistical segmentation and clustering analysis of\nthe Dow Jones Industrial Average time series between January 1997 and August\n2008. Modeling the index movements and log-index movements as stationary\nGaussian processes, we find a total of 116 and 119 statistically stationary\nsegments respectively. These can then be grouped into between five to seven\nclusters, each representing a different macroeconomic phase. The macroeconomic\nphases are distinguished primarily by their volatilities. We find the US\neconomy, as measured by the DJI, spends most of its time in a low-volatility\nphase and a high-volatility phase. The former can be roughly associated with\neconomic expansion, while the latter contains the economic contraction phase in\nthe standard economic cycle. Both phases are interrupted by a\nmoderate-volatility market, but extremely-high-volatility market crashes are\nfound mostly within the high-volatility phase. From the temporal distribution\nof various phases, we see a high-volatility phase from mid-1998 to mid-2003,\nand another starting mid-2007 (the current global financial crisis).\nTransitions from the low-volatility phase to the high-volatility phase are\npreceded by a series of precursor shocks, whereas the transition from the\nhigh-volatility phase to the low-volatility phase is preceded by a series of\ninverted shocks. The time scale for both types of transitions is about a year.\nWe also identify the July 1997 Asian Financial Crisis to be the trigger for the\nmid-1998 transition, and an unnamed May 2006 market event related to\ncorrections in the Chinese markets to be the trigger for the mid-2007\ntransition.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.3004v1"
    },
    {
        "title": "Income and Poverty in a Developing Economy",
        "authors": [
            "Amit K Chattopadhyay",
            "Graeme J Ackland",
            "Sushanta K Mallick"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We present a stochastic agent-based model for the distribution of personal\nincomes in a developing economy. We start with the assumption that incomes are\ndetermined both by individual labour and by stochastic effects of trading and\ninvestment. The income from personal effort alone is distributed about a mean,\nwhile the income from trade, which may be positive or negative, is proportional\nto the trader's income. These assumptions lead to a Langevin model with\nmultiplicative noise, from which we derive a Fokker-Planck (FP) equation for\nthe income probability density function (IPDF) and its variation in time. We\nfind that high earners have a power-law income distribution while the low\nincome groups have a Levy IPDF. Comparing our analysis with the Indian survey\ndata (obtained from the world bank website) taken over many years we obtain a\nnear-perfect data collapse onto our model's equilibrium IPDF. The theory\nquantifies the economic notion of \"given other things\". Using survey data to\nrelate the IPDF to actual food consumption we define a poverty index, which is\nconsistent with traditional indices, but independent of an arbitrarily chosen\n\"poverty line\" and therefore less susceptible to manipulation.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3803v1"
    },
    {
        "title": "Complementarity between private and public investment in R&D: A Dynamic\n  Panel Data analysis",
        "authors": [
            "Sadraoui Tarek",
            "Naceur Ben Zina"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This paper investigates the relationship between private and public\ninvestment in R&D, while taking into account the effect of several instruments\npolicies such as subsidies and taxes. We design a new look of knowledge\nspillovers and R&D cooperation to explain the contribution of public and\nprivate R&D on growth. We propose a heterogeneous dynamic panel data model to\nconsider the endogenous effect of R&D investment. We also distinguish between\nthe estimated long and short run results. Our results based on a sample of 23\ncountries over the period 1992-2004 indicate that both public and private\ninvestments in R&D are complementary. By establishing an endogenous growth\nmodel, the estimates indicate that public and private R&D depends on the host\ncountry's human capital investment. Results indicate that foreign direct\ninvestment is a more significant spillover channel than imports.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4272v1"
    },
    {
        "title": "Economic interactions and the distribution of wealth",
        "authors": [
            "Davide Fiaschi",
            "Matteo Marsili"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This paper analyzes the equilibrium distribution of wealth in an economy\nwhere firms' productivities are subject to idiosyncratic shocks, returns on\nfactors are determined in competitive markets, dynasties have linear\nconsumption functions and government imposes taxes on capital and labour\nincomes and equally redistributes the collected resources to dynasties. The\nequilibrium distribution of wealth is explicitly calculated and its shape\ncrucially depends on market incompleteness. In particular, a Paretian law in\nthe top tail only arises if capital markets are incomplete. The Pareto exponent\ndepends on the saving rate, on the net return on capital, on the growth rate of\npopulation and on portfolio diversification. On the contrary, the\ncharacteristics of the labour market mostly affects the bottom tail of the\ndistribution of wealth. The analysis also suggests a positive relationship\nbetween growth and wealth inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.1512v1"
    },
    {
        "title": "De Finetti's dividend problem and impulse control for a two-dimensional\n  insurance risk process",
        "authors": [
            "Irmina Czarna",
            "Zbigniew Palmowski"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Consider two insurance companies (or two branches of the same company) that\nreceive premiums at different rates and then split the amount they pay in fixed\nproportions for each claim (for simplicity we assume that they are equal). We\nmodel the occurrence of claims according to a Poisson process. The ruin is\nachieved when the corresponding two-dimensional risk process first leaves the\npositive quadrant. We will consider two scenarios of the controlled process:\nrefraction and impulse control. In the first case the dividends are payed out\nwhen the two-dimensional risk process exits the fixed region. In the second\nscenario, whenever the process hits the horizontal line, it is reduced by\npaying dividends to some fixed point in the positive quadrant where it waits\nfor the next claim to arrive. In both models we calculate the discounted\ncumulative dividend payments until the ruin. This paper is the first attempt to\nunderstand the effect of dependencies of two portfolios on the joint optimal\nstrategy of paying dividends. For example in case of proportional reinsurance\none can observe the interesting phenomenon that choice of the optimal barrier\ndepends on the initial reserves. This is in contrast with the one-dimensional\nCram\\'{e}r-Lundberg model where the optimal choice of the barrier is uniform\nfor all initial reserves.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.2100v3"
    },
    {
        "title": "Utility Function and Optimum Consumption in the models with Habit\n  Formation and Catching up with the Joneses",
        "authors": [
            "Roman Naryshkin",
            "Matt Davison"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This paper analyzes popular time-nonseparable utility functions that describe\n\"habit formation\" consumer preferences comparing current consumption with the\ntime averaged past consumption of the same individual and \"catching up with the\nJoneses\" (CuJ) models comparing individual consumption with a cross-sectional\naverage consumption level.\n  Few of these models give reasonable optimum consumption time series. We\nintroduce theoretically justified utility specifications leading to a plausible\nconsumption behavior to show that habit formation preferences must be described\nby a power CRRA utility function different from the exponential CARA used for\nCuJ.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.3655v1"
    },
    {
        "title": "The Building Blocks of Economic Complexity",
        "authors": [
            "Cesar A. Hidalgo",
            "Ricardo Hausmann"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  For Adam Smith, wealth was related to the division of labor. As people and\nfirms specialize in different activities, economic efficiency increases,\nsuggesting that development is associated with an increase in the number of\nindividual activities and with the complexity that emerges from the\ninteractions between them. Here we develop a view of economic growth and\ndevelopment that gives a central role to the complexity of a country's economy\nby interpreting trade data as a bipartite network in which countries are\nconnected to the products they export, and show that it is possible to quantify\nthe complexity of a country's economy by characterizing the structure of this\nnetwork. Furthermore, we show that the measures of complexity we derive are\ncorrelated with a country's level of income, and that deviations from this\nrelationship are predictive of future growth. This suggests that countries tend\nto converge to the level of income dictated by the complexity of their\nproductive structures, indicating that development efforts should focus on\ngenerating the conditions that would allow complexity to emerge in order to\ngenerate sustained growth and prosperity.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.3890v1"
    },
    {
        "title": "Financial crises and the evaporation of trust",
        "authors": [
            "Kartik Anand",
            "Prasanna Gai",
            "Matteo Marsili"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Trust lies at the crux of most economic transactions, with credit markets\nbeing a notable example. Drawing on insights from the literature on\ncoordination games and network growth, we develop a simple model to clarify how\ntrust breaks down in financial systems. We show how the arrival of bad news\nabout a financial agent can lead others to lose confidence in it and how this,\nin turn, can spread across the entire system. Our results emphasize the role of\nhysteresis -- it takes considerable effort to regain trust once it has been\nbroken. Although simple, the model provides a plausible account of the credit\nfreeze that followed the global financial crisis of 2007/8, both in terms of\nthe sequence of events and the measures taken (and being proposed) by the\nauthorities.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.3099v1"
    },
    {
        "title": "On the Existence of Consistent Price Systems",
        "authors": [
            "Erhan Bayraktar",
            "Mikko S. Pakkanen",
            "Hasanjan Sayit"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We formulate a sufficient condition for the existence of a consistent price\nsystem (CPS), which is weaker than the conditional full support condition (CFS)\nintroduced by Guasoni, Rasonyi, and Schachermayer [Ann. Appl. Probab.,\n18(2008), pp. 491-520] . We use the new condition to show the existence of CPSs\nfor certain processes that fail to have the CFS property. In particular this\ncondition gives sufficient conditions, under which a continuous function of a\nprocess with CFS admits a CPS, while the CFS property might be lost.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.3789v5"
    },
    {
        "title": "Causal Links Between US Economic Sectors",
        "authors": [
            "Gladys Hui Ting Lee",
            "Yiting Zhang",
            "Jian Cheng Wong",
            "Manamohan Prusty",
            "Siew Ann Cheong"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  In this paper, we perform a comparative segmentation and clustering analysis\nof the time series for the ten Dow Jones US economic sector indices between 14\nFebruary 2000 and 31 August 2008. From the temporal distributions of clustered\nsegments, we find that the US economy took one and a half years to recover from\nthe mid-1998-to-mid-2003 financial crisis, but only two months to completely\nenter the present financial crisis. We also find the oil & gas and basic\nmaterials sectors leading the recovery from the previous financial crisis,\nwhile the consumer goods and utilities sectors led the descent into the present\nfinancial crisis. On a macroscopic level, we find sectors going earlier into a\ncrisis emerge later from it, whereas sectors going later into the crisis emerge\nearlier. On the mesoscopic level, we find leading sectors experiencing stronger\nand longer volatility shocks, while trailing sectors experience weaker and\nshorter volatility shocks. In our shock-by-shock causal-link analysis, we also\nfind shorter delays between corresponding shocks in more closely related\neconomic sectors. In addition, our analysis reveals evidences for complex\nsectorial structures, as well as nonlinear amplification in the propagating\nvolatility shocks. From a perspective relevant to public policy, our study\nsuggests an endogeneous sectorial dynamics during the mid-2003 economic\nrecovery, in contrast to strong exogeneous driving by Federal Reserve interest\nrate cuts during the mid-2007 onset. Most interestingly, we find for the\nsequence of closely spaced interest rate cuts instituted in 2007/2008, the\nfirst few cuts effectively lowered market volatilities, while the next few cuts\ncounter-effectively increased market volatilities. Subsequent cuts evoked\nlittle response from the market.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4763v3"
    },
    {
        "title": "Fluctuation-Dissipation Theory of Input-Output Interindustrial\n  Correlations",
        "authors": [
            "Hiroshi Iyetomi",
            "Yasuhiro Nakayama",
            "Hideaki Aoyama",
            "Yoshi Fujiwara",
            "Yuichi Ikeda",
            "Wataru Souma"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  In this study, the fluctuation-dissipation theory is invoked to shed light on\ninput-output interindustrial relations at a macroscopic level by its\napplication to IIP (indices of industrial production) data for Japan.\nStatistical noise arising from finiteness of the time series data is carefully\nremoved by making use of the random matrix theory in an eigenvalue analysis of\nthe correlation matrix; as a result, two dominant eigenmodes are detected. Our\nprevious study successfully used these two modes to demonstrate the existence\nof intrinsic business cycles. Here a correlation matrix constructed from the\ntwo modes describes genuine interindustrial correlations in a statistically\nmeaningful way. Further it enables us to quantitatively discuss the\nrelationship between shipments of final demand goods and production of\nintermediate goods in a linear response framework. We also investigate\ndistinctive external stimuli for the Japanese economy exerted by the current\nglobal economic crisis. These stimuli are derived from residuals of moving\naverage fluctuations of the IIP remaining after subtracting the long-period\ncomponents arising from inherent business cycles. The observation reveals that\nthe fluctuation-dissipation theory is applicable to an economic system that is\nsupposed to be far from physical equilibrium.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.1985v2"
    },
    {
        "title": "Levy Random Bridges and the Modelling of Financial Information",
        "authors": [
            "Edward Hoyle",
            "Lane P. Hughston",
            "Andrea Macrina"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  The information-based asset-pricing framework of Brody, Hughston and Macrina\n(BHM) is extended to include a wider class of models for market information. In\nthe BHM framework, each asset is associated with a collection of random cash\nflows. The price of the asset is the sum of the discounted conditional\nexpectations of the cash flows. The conditional expectations are taken with\nrespect to a filtration generated by a set of \"information processes\". The\ninformation processes carry imperfect information about the cash flows. To\nmodel the flow of information, we introduce in this paper a class of processes\nwhich we term Levy random bridges (LRBs). This class generalises the Brownian\nbridge and gamma bridge information processes considered by BHM. An LRB is\ndefined over a finite time horizon. Conditioned on its terminal value, an LRB\nis identical in law to a Levy bridge. We consider in detail the case where the\nasset generates a single cash flow $X_T$ occurring at a fixed date $T$. The\nflow of market information about $X_T$ is modelled by an LRB terminating at the\ndate $T$ with the property that the (random) terminal value of the LRB is equal\nto $X_T$. An explicit expression for the price process of such an asset is\nfound by working out the discounted conditional expectation of $X_T$ with\nrespect to the natural filtration of the LRB. The prices of European options on\nsuch an asset are calculated.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.3652v2"
    },
    {
        "title": "Diagnosis and Prediction of Tipping Points in Financial Markets: Crashes\n  and Rebounds",
        "authors": [
            "Wanfeng Yan",
            "Ryan Woodard",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  By combining (i) the economic theory of rational expectation bubbles, (ii)\nbehavioral finance on imitation and herding of investors and traders and (iii)\nthe mathematical and statistical physics of bifurcations and phase transitions,\nthe log-periodic power law (LPPL) model has been developed as a flexible tool\nto detect bubbles. The LPPL model considers the faster-than-exponential (power\nlaw with finite-time singularity) increase in asset prices decorated by\naccelerating oscillations as the main diagnostic of bubbles. It embodies a\npositive feedback loop of higher return anticipations competing with negative\nfeedback spirals of crash expectations. The power of the LPPL model is\nillustrated by two recent real-life predictions performed recently by our\ngroup: the peak of the Oil price bubble in early July 2008 and the burst of a\nbubble on the Shanghai stock market in early August 2009. We then present the\nconcept of \"negative bubbles\", which are the mirror images of positive bubbles.\nWe argue that similar positive feedbacks are at work to fuel these accelerated\ndownward price spirals. We adapt the LPPL model to these negative bubbles and\nimplement a pattern recognition method to predict the end times of the negative\nbubbles, which are characterized by rebounds (the mirror images of crashes\nassociated with the standard positive bubbles). The out-of-sample tests\nquantified by error diagrams demonstrate the high significance of the\nprediction performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.0265v2"
    },
    {
        "title": "Diverse Beliefs",
        "authors": [
            "Angus A Brown",
            "L C G Rogers"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This paper presents a general framework for studying diverse beliefs in\ndynamic economies. Within this general framework, the characterization of a\ncentral-planner general equilbrium turns out to be very easy to derive, and\nleads to a range of interesting applications. We show how for an economy with\nlog investors holding diverse beliefs, rational overconfidence is to be\nexpected; volume-of-trade effects are effectively modelled; the Keynesian\n`beauty contest' can be modelled and analysed; and bubbles and crashes arise\nnaturally. We remark that models where agents receive private information can\nformally be considered as models of diverse beliefs.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1450v1"
    },
    {
        "title": "Mesure de l'incertitude tendancielle sur la mortalité ? application\n  à un régime de rentes",
        "authors": [
            "Frédéric Planchet",
            "Marc Juillard"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The aim of this paper is to propose a realistic and operational model to\nquantify the systematic risk of mortality included in an engagement of\nretirement. The model presented is built on the basis of model of Lee-Carter.\nThe stochastic prospective tables thus built make it possible to project the\nevolution of the random mortality rates in the future and to quantify the\nsystematic risk of mortality.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1921v1"
    },
    {
        "title": "Etude du risque systématique de mortalité",
        "authors": [
            "Frédéric Planchet",
            "Laurent Faucillon",
            "Marc Juillard"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The aim of this paper is to propose a realistic and operational model to\nquantify the systematic risk of mortality included in an engagement of\nretirement. The model presented is built on the basis of model of Lee-Carter.\nThe stochastic prospective tables thus built make it possible to project the\nevolution of the random mortality rates in the future and to quantify the\nsystematic risk of mortality.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1922v1"
    },
    {
        "title": "Quantum Model of Bertrand Duopoly",
        "authors": [
            "Salman Khan",
            "M. Ramzan",
            "M. K. Khan"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We present the quantum model of Bertrand duopoly and study the entanglement\nbehavior on the profit functions of the firms. Using the concept of optimal\nresponse of each firm to the price of the opponent, we found only one Nash\nequilibirum point for maximally entangled initial state. The very presence of\nquantum entanglement in the initial state gives payoffs higher to the firms\nthan the classical payoffs at the Nash equilibrium. As a result the dilemma\nlike situation in the classical game is resolved.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.2831v3"
    },
    {
        "title": "Universal Laws and Economic Phenomena",
        "authors": [
            "Austin Gerig"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This is a short commentary piece that discusses how the methods used in the\nnatural sciences can apply to economics in general and financial markets\nspecifically.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.0377v2"
    },
    {
        "title": "Markets are efficient if and only if P = NP",
        "authors": [
            "Philip Maymin"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  I prove that if markets are weak-form efficient, meaning current prices fully\nreflect all information available in past prices, then P = NP, meaning every\ncomputational problem whose solution can be verified in polynomial time can\nalso be solved in polynomial time. I also prove the converse by showing how we\ncan \"program\" the market to solve NP-complete problems. Since P probably does\nnot equal NP, markets are probably not efficient. Specifically, markets become\nincreasingly inefficient as the time series lengthens or becomes more frequent.\nAn illustration by way of partitioning the excess returns to momentum\nstrategies based on data availability confirms this prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2284v2"
    },
    {
        "title": "Boolean delay equations on networks: An application to economic damage\n  propagation",
        "authors": [
            "B. Coluzzi",
            "M. Ghil",
            "S. Hallegatte",
            "G. Weisbuch"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We introduce economic models based on Boolean Delay Equations: this formalism\nmakes easier to take into account the complexity of the interactions between\nfirms and is particularly appropriate for studying the propagation of an\ninitial damage due to a catastrophe. Here we concentrate on simple cases, which\nallow to understand the effects of multiple concurrent production paths as well\nas the presence of stochasticity in the path time lengths or in the network\nstructure.\n  In absence of flexibility, the shortening of production of a single firm in\nan isolated network with multiple connections usually ends up by attaining a\nfinite fraction of the firms or the whole economy, whereas the interactions\nwith the outside allow a partial recovering of the activity, giving rise to\nperiodic solutions with waves of damage which propagate across the structure.\nThe damage propagation speed is strongly dependent upon the topology. The\nexistence of multiple concurrent production paths does not necessarily imply a\nslowing down of the propagation, which can be as fast as the shortest path.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.0793v1"
    },
    {
        "title": "Micro-Macro Relation of Production - The Double Scaling Law for\n  Statistical Physics of Economy -",
        "authors": [
            "Hideaki Aoyama",
            "Yoshi Fujiwara",
            "Mauro Gallegati"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We show that an economic system populated by multiple agents generates an\nequilibrium distribution in the form of multiple scaling laws of conditional\nPDFs, which are sufficient for characterizing the probability distribution. The\nexistence of the double scaling law is demonstrated empirically for the sales\nand the labor of one million Japanese firms. Theoretical study of the scaling\nlaws suggests lognormal joint distributions of sales and labor and a scaling\nlaw for labor productivity, both of which are confirmed empirically. This\nframework offers characterization of the equilibrium distribution with a small\nnumber of scaling indices, which determine macroscopic quantities, thus setting\nthe stage for an equivalence with statistical physics, bridging micro- and\nmacro-economics.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.2321v1"
    },
    {
        "title": "The Validity of Company Valuation Using Discounted Cash Flow Methods",
        "authors": [
            "Florian Steiger"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This paper closely examines theoretical and practical aspects of the widely\nused discounted cash flows (DCF) valuation method. It assesses its potentials\nas well as several weaknesses. A special emphasize is being put on the\nvaluation of companies using the DCF method. The paper finds that the\ndiscounted cash flow method is a powerful tool to analyze even complex\nsituations. However, the DCF method is subject to massive assumption bias and\neven slight changes in the underlying assumptions of an analysis can\ndrastically alter the valuation results. A practical example of these\nimplications is given using a scenario analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4881v2"
    },
    {
        "title": "Overview of utility-based valuation",
        "authors": [
            "David German"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We review the utility-based valuation method for pricing derivative\nsecurities in incomplete markets. In particular, we review the practical\napproach to the utility-based pricing by the means of computing the first order\nexpansion of marginal utility-based prices with respect to a small number of\nrandom endowments.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.5712v1"
    },
    {
        "title": "Laplace transform analysis of a multiplicative asset transfer model",
        "authors": [
            "Andrey Sokolov",
            "Andrew Melatos",
            "Tien Kieu"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We analyze a simple asset transfer model in which the transfer amount is a\nfixed fraction $f$ of the giver's wealth. The model is analyzed in a new way by\nLaplace transforming the master equation, solving it analytically and\nnumerically for the steady-state distribution, and exploring the solutions for\nvarious values of $f\\in(0,1)$. The Laplace transform analysis is superior to\nagent-based simulations as it does not depend on the number of agents, enabling\nus to study entropy and inequality in regimes that are costly to address with\nsimulations. We demonstrate that Boltzmann entropy is not a suitable (e.g.\nnon-monotonic) measure of disorder in a multiplicative asset transfer system\nand suggest an asymmetric stochastic process that is equivalent to the asset\ntransfer model.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.5169v1"
    },
    {
        "title": "Crude oil and motor fuel: Fair price revisited",
        "authors": [
            "Ivan O. Kitov",
            "Oleg I. Kitov"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  In April 2009, we introduced a model representing the evolution of motor fuel\nprice (a subcategory of the consumer price index of transportation) relative to\nthe overall CPI as a linear function of time. Under our framework, all price\ndeviations from the linear trend are transient and the price must promptly\nreturn to the trend. Specifically, the model predicted that \"the price for\nmotor fuel in the US will also grow by 50% by the end of 2009. Oil price is\nexpected to rise by ~50% as well, from its current value of ~$50 per barrel\".\nThe behavior of actual price has shown that this prediction is accurate in both\namplitude and trajectory shape. Hence, one can conclude that the concept of\nprice decomposition into a short-term (oscillating) and long-term (linear\ntrend) components is valid. According to the model, the price of motor fuel and\ncrude oil will be falling to the level of $30 per barrel during the next 5 to 8\nyears.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0051v1"
    },
    {
        "title": "Rough paths in idealized financial markets",
        "authors": [
            "Vladimir Vovk"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This paper considers possible price paths of a financial security in an\nidealized market. Its main result is that the variation index of typical price\npaths is at most 2, in this sense, typical price paths are not rougher than\ntypical paths of Brownian motion. We do not make any stochastic assumptions and\nonly assume that the price path is positive and right-continuous. The\nqualification \"typical\" means that there is a trading strategy (constructed\nexplicitly in the proof) that risks only one monetary unit but brings infinite\ncapital when the variation index of the realized price path exceeds 2. The\npaper also reviews some known results for continuous price paths and lists\nseveral open problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0279v3"
    },
    {
        "title": "Deterministic criteria for the absence of arbitrage in one-dimensional\n  diffusion models",
        "authors": [
            "Aleksandar Mijatović",
            "Mikhail Urusov"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We obtain a deterministic characterisation of the \\emph{no free lunch with\nvanishing risk}, the \\emph{no generalised arbitrage} and the \\emph{no relative\narbitrage} conditions in the one-dimensional diffusion setting and examine how\nthese notions of no-arbitrage relate to each other.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1861v1"
    },
    {
        "title": "An empirical study of the tails of mutual fund size",
        "authors": [
            "Yonathan Schwarzkopf",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The mutual fund industry manages about a quarter of the assets in the U.S.\nstock market and thus plays an important role in the U.S. economy. The question\nof how much control is concentrated in the hands of the largest players is best\nquantitatively discussed in terms of the tail behavior of the mutual fund size\ndistribution. We study the distribution empirically and show that the tail is\nmuch better described by a log-normal than a power law, indicating less\nconcentration than, for example, personal income. The results are highly\nstatistically significant and are consistent across fifteen years. This\ncontradicts a recent theory concerning the origin of the power law tails of the\ntrading volume distribution. Based on the analysis in a companion paper, the\nlog-normality is to be expected, and indicates that the distribution of mutual\nfunds remains perpetually out of equilibrium.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.4976v1"
    },
    {
        "title": "Econophysics studies in Estonia",
        "authors": [
            "M. Patriarca",
            "E. Heinsalu",
            "R. Kitt",
            "J. Kalda"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  A short review of the econophysics research done in Estonia, devoted to the\n15th anniversary of the term \"econophysics\".\n",
        "pdf_link": "http://arxiv.org/pdf/1006.3708v1"
    },
    {
        "title": "Econophysics on Real Economy -The First Decade of the Kyoto Econophysics\n  Group-",
        "authors": [
            "Hideaki Aoyama",
            "Yoshi Fujiwara",
            "Yuichi Ikeda",
            "Hiroshi Iyetomi",
            "Wataru Souma"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Research activities of Kyoto Econophysics Group is reviewed. Strong emphasis\nhas been placed on real economy. While the initial stage of research was a\nfirst high-definition data analysis on personal income, it soon progressed to\nfirm dynamics, growth rate distribution and establishment of Pareto's law and\nGibrat's law. It then led to analysis and simulation of firm dynamics on\neconomic network. Currently it covers a wide rage of dynamics of firms and\nfinancial institutions on complex network, using Japanese large-scale network\ndata, some of which are not available in other countries. Activities of this\ngroup for publicising and promoting understanding of econophysics is also\nreviewed.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.5587v3"
    },
    {
        "title": "A simple model for asset price bubble formation and collapse",
        "authors": [
            "Alexander Kiselev",
            "Lenya Ryzhik"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We consider a simple stochastic differential equation for modeling bubbles in\nsocial context. A prime example is bubbles in asset pricing, but similar\nmechanisms may control a range of social phenomena driven by psychological\nfactors (for example, popularity of rock groups, or a number of students\npursuing a given major). Our goal is to study the simplest possible model in\nwhich every term has a clear meaning and which demonstrates several key\nbehaviors. The main factors that enter are tendency of mean reversion to a\nstable value, speculative social response triggered by trend following and\nrandom fluctuations. The interplay of these three forces may lead to bubble\nformation and collapse. Numerical simulations show that the equation has\ndistinct regimes depending on the values of the parameters. We perform rigorous\nanalysis of the weakly random regime, and study the role of change in\nfundamentals in igniting the bubble.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.0299v1"
    },
    {
        "title": "Topology of the correlation networks among major currencies using\n  hierarchical structure methods",
        "authors": [
            "Mustafa Keskin",
            "Bayram Deviren",
            "Yusuf Kocakaplan"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We studied the topology of correlation networks among 34 major currencies\nusing the concept of a minimal spanning tree and hierarchical tree for the full\nyears of 2007-2008 when major economic turbulence occurred. We used the USD (US\nDollar) and the TL (Turkish Lira) as numeraires in which the USD was the major\ncurrency and the TL was the minor currency. We derived a hierarchical\norganization and constructed minimal spanning trees (MSTs) and hierarchical\ntrees (HTs) for the full years of 2007, 2008 and for the 2007-2008 periods. We\nperformed a technique to associate a value of reliability to the links of MSTs\nand HTs by using bootstrap replicas of data. We also used the average linkage\ncluster analysis for obtaining the hierarchical trees in the case of the TL as\nthe numeraire. These trees are useful tools for understanding and detecting the\nglobal structure, taxonomy and hierarchy in financial data. We illustrated how\nthe minimal spanning trees and their related hierarchical trees developed over\na period of time. From these trees we identified different clusters of\ncurrencies according to their proximity and economic ties. The clustered\nstructure of the currencies and the key currency in each cluster were obtained\nand we found that the clusters matched nicely with the geographical regions of\ncorresponding countries in the world such as Asia or Europe. As expected the\nkey currencies were generally those showing major economic activity.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.5653v1"
    },
    {
        "title": "Size-Dependency of Income Distributions and Its Implications",
        "authors": [
            "Jiang Zhang",
            "You-Gui Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This paper highlights the size-dependency of income distributions, i.e. the\nincome distribution curves versus the population of a country systematically.\nBy using the generalized Lotka-Volterra model to fit the empirical income data\nin the United States during 1996-2007, we found an important parameter\n$\\lambda$ can scale with a $\\beta$ power of the size (population) of U.S. in\nthat year. We pointed out that the size-dependency of the income distributions,\nwhich is a very important property but seldom addressed by previous studies,\nhas two non-trivial implications: (1) the allometric growth pattern, i.e. the\npower law relationship between population and GDP in different years, which can\nbe mathematically derived from the size-dependent income distributions and also\nsupported by the empirical data; (2) the connection with the anomalous scaling\nfor the probability density function in critical phenomena since the re-scaled\nform of the income distributions has the exactly same mathematical expression\nfor the limit distribution of the sum of many correlated random variables\nasymptotically.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.2279v3"
    },
    {
        "title": "Fundamental and Real-World Challenges in Economics",
        "authors": [
            "Dirk Helbing",
            "Stefano Balietti"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  In the same way as the Hilbert Program was a response to the foundational\ncrisis of mathematics, this article tries to formulate a research program for\nthe socio-economic sciences. The aim of this contribution is to stimulate\nresearch in order to close serious knowledge gaps in mainstream economics that\nthe recent financial and economic crisis has revealed. By identifying weak\npoints of conventional approaches in economics, we identify the scientific\nproblems which need to be addressed. We expect that solving these questions\nwill bring scientists in a position to give better decision support and policy\nadvice. We also indicate, what kinds of insights can be contributed by\nscientists from other research fields such as physics, biology, computer and\nsocial science. In order to make a quick progress and gain a systemic\nunderstanding of the whole interconnected socio-economic-environmental system,\nusing the data, information and computer systems available today and in the\nnear future, we suggest a multi-disciplinary collaboration as most promising\nresearch approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.4446v1"
    },
    {
        "title": "On the Existence of Bertrand-Nash Equilibrium Prices Under Logit Demand",
        "authors": [
            "W. Ross Morrow",
            "Steven J. Skerlos"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This article presents a proof of the existence of Bertrand-Nash equilibrium\nprices with multi-product firms and under the Logit model of demand that does\nnot rely on restrictive assumptions on product characteristics, firm\nhomogeneity or symmetry, product costs, or linearity of the utility function.\nThe proof is based on conditions for the indirect utility function, fixed-point\nequations derived from the first-order conditions, and a direct analysis of the\nsecond-order conditions resulting in the uniqueness of profit-maximizing\nprices. Several subsequent results also demonstrate that price equilibrium\nunder the Logit model of demand cannot adequately describe multi-product\npricing.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.5832v2"
    },
    {
        "title": "Punctuated Equilibrium and Power Law in Economic Dynamics",
        "authors": [
            "Abhijit Kar Gupta"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  An interesting toy model has recently been proposed on Schumpeterian economic\ndynamics by Thurner {\\it et al.} following the idea of economist Joseph\nSchumpeter. Punctuated equilibrium dynamics is shown to emerge from this model\nand some detail analyses of the time series indicate SOC kind of behaviours.\nThe focus in the present work is to toss the idea whether the dynamics can\nreally be like a self organized critical (SOC) type. This study indicates that\nit is necessary to incorporate the concepts of 'fitness' and 'selection' in\nsuch a model in the line of the biological evolutionary model by Bak and\nSneppen in order to obtain power law and thus SOC behaviour.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.5896v1"
    },
    {
        "title": "An statistical analysis of stratification and inequity in the income\n  distribution",
        "authors": [
            "Juan C. Ferrero"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The analysis of the USA 2001 income distribution shows that it can be\ndescribed by at least two main components, which obey the generalized Tsallis\nstatistics with different values of the q parameter. Theoretical calculations\nusing the gas kinetics model with a distributed saving propensity factor and\ntwo ensembles reproduce the empirical data and provide further information on\nthe structure of the distribution, which shows a clear stratification. This\nstratification is amenable to different interpretations, which are analyzed.\nThe distribution function is invariant with the average individual income,\nwhich implies that the inequity of the distribution cannot be modified by\nincreasing the total income.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.5932v1"
    },
    {
        "title": "The Network Structure of Economic Output",
        "authors": [
            "Ricardo Hausmann",
            "Cesar A. Hidalgo"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Much of the analysis of economic growth has focused on the study of aggregate\noutput. Here, we deviate from this tradition and look instead at the structure\nof output embodied in the network connecting countries to the products that\nthey export.We characterize this network using four structural features: the\nnegative relationship between the diversification of a country and the average\nubiquity of its exports, and the non-normal distributions for product ubiquity,\ncountry diversification and product co-export. We model the structure of the\nnetwork by assuming that products require a large number of non-tradable\ninputs, or capabilities, and that countries differ in the completeness of the\nset of capabilities they have. We solve the model assuming that the probability\nthat a country has a capability and that a product requires a capability are\nconstant and calibrate it to the data to find that it accounts well for all of\nthe network features except for the heterogeneity in the distribution of\ncountry diversification. In the light of the model, this is evidence of a large\nheterogeneity in the distribution of capabilities across countries. Finally, we\nshow that the model implies that the increase in diversification that is\nexpected from the accumulation of a small number of capabilities is small for\ncountries that have a few of them and large for those with many. This implies\nthat the forces that help drive divergence in product diversity increase with\nthe complexity of the global economy when capabilities travel poorly.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.1707v2"
    },
    {
        "title": "Macrostate Parameter and Investment Risk Diagrams for 2008 and 2009",
        "authors": [
            "Anca Gheorghiu",
            "Ion Spânulescu"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  In this paper are made some considerations of the application of\nphenomenological thermodynamics in risk analysis for the transaction on\nfinancial markets, using the concept of economic entropy and the macrostate\nparameter introduced by us in a previous works [15,16]. The investment risk\ndiagrams for a number of Romanian listed companies in 2008 and 2009 years were\ncalculed. Also, the evolution of the macrostate parameter during financial and\neconomic crisis in Romania are studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.4674v1"
    },
    {
        "title": "Econophysical Approaches for the Direct Foreign Investments",
        "authors": [
            "Anca Gheorghiu",
            "Ion Spanulescu",
            "Anda Gheorghiu"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  In this paper will be applied some principles and methods from econophysics\nin the case of the direct foreign investitions (D.F.I.), particularised for the\nGreenfield type, and mixed firms of trade and industrial production (Joint\nVentures). To this aim will be used some similarities and parallelisms between\nthe mentioned economic domains and some phenomena and processes from physics,\nespecially from thermodynamics, solid state physics (the grow of crystals and\nthin policrystalline layers etc.), electromagnetism etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.4675v1"
    },
    {
        "title": "An Econophysics Model for the Stock-Markets' Analysis and Diagnosis",
        "authors": [
            "Ion Spanulescu",
            "Ion Popescu",
            "Victor Stoica",
            "Anca Gheorghiu",
            "Victor Velter"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  In this paper we present an econophysic model for the description of shares\ntransactions in a capital market. For introducing the fundamentals of this\nmodel we used an analogy between the electrical field produced by a system of\ncharges and the overall of economic and financial information of the shares\ntransactions from the stock-markets. An energetic approach of the rate\nvariation for the shares traded on the financial markets was proposed and\nstudied.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.4680v1"
    },
    {
        "title": "The Conflict between Economic Development and Planetary Ecosystem in the\n  Context of Sustainable Development",
        "authors": [
            "Corina-Maria Ene",
            "Anda Gheorghiu",
            "Cristina Burghelea",
            "Anca Gheorghiu"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The green area of economy is the key of healthy living. It is necessary to\nconvene economic and ecologic framework to establish a market attentive to\ndrastic reduction of emissions damaging our climate and landscapes in rural\nareas, to the protection of biological diversity of the planet, to stop\nproducing nuclear waste, etc. This paper tries to demonstrate human concern for\na waste recycling economy that will provide new jobs, will create economic and\nsocial stability and will ensure a healthier and cleaner environment. Green\nEconomy and its support system (planetary ecosystem) won't be in conflict\nanymore. Green Economy will be able to support economic progress for future.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5747v1"
    },
    {
        "title": "A Theoretical Approach for Dynamic Modelling of Sustainable Development",
        "authors": [
            "Corina-Maria Ene",
            "Anda Gheorghiu",
            "Anca Gheorghiu"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This article presents a theoretical model for a dynamic system based on\nsustainable development. Due to the relatively absence of theoretical studies\nand practical issues in the area of sustainable development, Romania aspires to\nthe principles of sustainable development. Based on the concept as a process in\nwhich economic, social, political and natural environment are combined in order\nto sustain planet management, our goal is to promote an economic tool for\nRomanian decision-makers in order to evaluate scenarios and planning options.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.5752v1"
    },
    {
        "title": "Arbitrage and Hedging in a non probabilistic framework",
        "authors": [
            "Alexander Alvarez",
            "Sebastian Ferrando",
            "Pablo Olivares"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The paper studies the concepts of hedging and arbitrage in a non\nprobabilistic framework. It provides conditions for non probabilistic arbitrage\nbased on the topological structure of the trajectory space and makes\nconnections with the usual notion of arbitrage. Several examples illustrate the\nnon probabilistic arbitrage as well perfect replication of options under\ncontinuous and discontinuous trajectories, the results can then be applied in\nprobabilistic models path by path. The approach is related to recent financial\nmodels that go beyond semimartingales, we remark on some of these connections\nand provide applications of our results to some of these models.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.1006v1"
    },
    {
        "title": "Ambiguous Volatility, Possibility and Utility in Continuous Time",
        "authors": [
            "Larry Epstein",
            "Shaolin Ji"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This paper formulates a model of utility for a continuous time framework that\ncaptures the decision-maker's concern with ambiguity about both the drift and\nvolatility of the driving process. At a technical level, the analysis requires\na significant departure from existing continuous time modeling because it\ncannot be done within a probability space framework. This is because ambiguity\nabout volatility leads invariably to a set of nonequivalent priors, that is, to\npriors that disagree about which scenarios are possible.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.1652v7"
    },
    {
        "title": "How Unlucky is 25-Sigma?",
        "authors": [
            "Kevin Dowd",
            "John Cotter",
            "Chris Humphrey",
            "Margaret Woods"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  One of the more memorable moments of last summer's credit crunch came when\nthe CFO of Goldman Sachs, David Viniar, announced in August that Goldman's\nflagship GEO hedge fund had lost 27% of its value since the start of the year.\nAs Mr. Viniar explained, \"We were seeing things that were 25-standard deviation\nmoves, several days in a row.\"\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5672v1"
    },
    {
        "title": "Utility Maximization, Risk Aversion, and Stochastic Dominance",
        "authors": [
            "Mathias Beiglboeck",
            "Johannes Muhle-Karbe",
            "Johannes Temme"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Consider an investor trading dynamically to maximize expected utility from\nterminal wealth. Our aim is to study the dependence between her risk aversion\nand the distribution of the optimal terminal payoff.\n  Economic intuition suggests that high risk aversion leads to a rather\nconcentrated distribution, whereas lower risk aversion results in a higher\naverage payoff at the expense of a more widespread distribution.\n  Dybvig and Wang [J. Econ. Theory, 2011, to appear] find that this idea can\nindeed be turned into a rigorous mathematical statement in one-period models.\nMore specifically, they show that lower risk aversion leads to a payoff which\nis larger in terms of second order stochastic dominance.\n  In the present study, we extend their results to (weakly) complete\ncontinuous-time models. We also complement an ad-hoc counterexample of Dybvig\nand Wang, by showing that these results are \"fragile\", in the sense that they\nfail in essentially any model, if the latter is perturbed on a set of\narbitrarily small probability. On the other hand, we establish that they hold\nfor power investors in models with (conditionally) independent increments.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.0761v2"
    },
    {
        "title": "Stability of the World Trade Web over Time - An Extinction Analysis",
        "authors": [
            "N. Foti",
            "S. Pauls",
            "Daniel N. Rockmore"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The World Trade Web (WTW) is a weighted network whose nodes correspond to\ncountries with edge weights reflecting the value of imports and/or exports\nbetween countries. In this paper we introduce to this macroeconomic system the\nnotion of extinction analysis, a technique often used in the analysis of\necosystems, for the purposes of investigating the robustness of this network.\nIn particular, we subject the WTW to a principled set of in silico \"knockout\nexperiments,\" akin to those carried out in the investigation of food webs, but\nsuitably adapted to this macroeconomic network. Broadly, our experiments show\nthat over time the WTW moves to a \"robust yet fragile\" configuration where it\nis robust to random failures but fragile under targeted attack. This change in\nstability is highly correlated with the connectance (edge density) of the\nnetwork. Moreover, there is evidence of a sharp change in the structure of the\nnetwork in the 1960s and 1970s, where most measures of robustness rapidly\nincrease before resuming a declining trend. We interpret these results in the\ncontext in the post-World War II move towards globalization. Globalization\ncoincides with the sharp increase in robustness but also with a rise in those\nmeasures (e.g., connectance and trade imbalances) which correlate with\ndecreases in robustness. The peak of robustness is reached after the onset of\nglobalization policy but before the negative impacts are substantial. These\nanalyses depend on a simple model of dynamics that rebalances the trade flow\nupon network perturbation, the most dramatic of which is node deletion. More\nsubtle and textured forms of perturbation lead to the definition of other\nmeasures of node importance as well as vulnerability. We anticipate that\nexperiments and measures like these can play an important role in the\nevaluation of the stability of economic systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.4380v2"
    },
    {
        "title": "The \"S\" Curve Relationship between Export Diversity and Economic Size of\n  Countries",
        "authors": [
            "Lunchao Hu",
            "Kailan Tian",
            "Xin Wang",
            "Jiang Zhang"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The highly detailed international trade data among all countries in the world\nduring 1971-2000 shows that the kinds of export goods and the logarithmic GDP\n(gross domestic production) of a country has an S-shaped relationship. This\nindicates all countries can be divided into three stages accordingly. First,\nthe poor countries always export very few kinds of products as we expect.\nSecond, once the economic size (GDP) of a country is beyond a threshold, its\nexport diversity may increase dramatically. However, this is not the case for\nrich countries because a ceiling on the export diversity is observed when their\nGDPs are higher than another threshold. This pattern is very stable for\ndifferent years although the concrete parameters of the fitting sigmoid\nfunctions may change with time. In addition, we also discussed other\nrelationships such as import diversity with respect to logarithmic GDP,\ndiversity of exporters with respect to the number of export goods etc., all of\nthese relationships show S-shaped or power law patterns. Although this paper\ndoes not explain the origin of the S-shaped curve, it may provide a basic\nempirical fact and insights for economic diversity.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.5891v1"
    },
    {
        "title": "Life time of correlation between stocks prices on established and\n  emerging markets",
        "authors": [
            "Andrzej Buda"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The correlation coefficient between stocks depends on price history and\nincludes information on hierarchical structure in financial markets. It is\nuseful for portfolio selection and estimation of risk. I introduce the Life\nTime of Correlation between stocks prices to know how far we should investigate\nthe price history to obtain the optimal durability of correlation. I carry out\nmy research on emerging (Poland) and established markets (in the USA, Great\nBritain and Germany). Other methods, including the Minimum Spanning Trees, tree\nhalf-life, decomposition of correlations and the Epps effect are also\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.6272v1"
    },
    {
        "title": "Archimedean Survival Processes",
        "authors": [
            "Edward Hoyle",
            "Levent Ali Menguturk"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Archimedean copulas are popular in the world of multivariate modelling as a\nresult of their breadth, tractability, and flexibility. A. J. McNeil and J.\nNe\\v{s}lehov\\'a (2009) showed that the class of Archimedean copulas coincides\nwith the class of multivariate $\\ell_1$-norm symmetric distributions. Building\nupon their results, we introduce a class of multivariate Markov processes that\nwe call `Archimedean survival processes' (ASPs). An ASP is defined over a\nfinite time interval, is equivalent in law to a multivariate gamma process, and\nits terminal value has an Archimedean survival copula. There exists a bijection\nfrom the class of ASPs to the class of Archimedean copulas. We provide various\ncharacterisations of ASPs, and a generalisation.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.2342v4"
    },
    {
        "title": "Learning, investments and derivatives",
        "authors": [
            "Andrei N. Soklakov"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The recent crisis and the following flight to simplicity put most derivative\nbusinesses around the world under considerable pressure. We argue that the\ntraditional modeling techniques must be extended to include product design. We\npropose a quantitative framework for creating products which meet the challenge\nof being optimal from the investors point of view while remaining relatively\nsimple and transparent.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.2882v1"
    },
    {
        "title": "On the Zipf strategy for short-term investments in WIG20 futures",
        "authors": [
            "B. Bieda",
            "P. Chodorowski",
            "D. Grech"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We apply the Zipf power law to financial time series of WIG20 index daily\nchanges (open-close). Thanks to the mapping of time series signal into the\nsequence of 2k+1 'spin-like' states, where k=0, 1/2, 1, 3/2, ..., we are able\nto describe any time series increments, with almost arbitrary accuracy, as the\none of such 'spin-like' states. This procedure leads in the simplest\nnon-trivial case (k = 1/2) to the binary data projection. More sophisticated\nprojections are also possible and mentioned in the article. The introduced\nformalism allows then to use Zipf power law to describe the intrinsic structure\nof time series. The fast algorithm for this implementation was constructed by\nus within Matlab^{TM} software. The method, called Zipf strategy, is then\napplied in the simplest case k = 1/2 to WIG 20 open and close daily data to\nmake short-term predictions for forthcoming index changes. The results of\nforecast effectiveness are presented with respect to different time window\nsizes and partition divisions (word lengths in Zipf language). Finally, the\nvarious investment strategies improving ROI (return of investment) for WIG20\nfutures are proposed. We show that the Zipf strategy is the appropriate and\nvery effective tool to make short-term predictions and therefore, to evaluate\nshort-term investments on the basis of historical stock index data. Our\nfindings support also the existence of long memory in financial data, exceeding\nthe known in literature 3 days span limit.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3287v1"
    },
    {
        "title": "On the Representation of General Interest Rate Models as Square\n  Integrable Wiener Functionals",
        "authors": [
            "Lane P. Hughston",
            "Francesco Mina"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  In the setting proposed by Hughston & Rafailidis (2005) we consider general\ninterest rate models in the case of a Brownian market information filtration\n$(\\mathcal{F}_t)_{t\\geq0}$. Let $X$ be a square-integrable\n$\\mathcal{F}_\\infty$-measurable random variable, and assume the non-degeneracy\ncondition that for all $t<\\infty$ the random variable $X$ is not\n$\\mathcal{F}_t$-measurable. Let ${\\sigma_t}$ denote the integrand appearing in\nthe representation of $X$ as a stochastic integral, write $\\pi_t$ for the\nconditional variance of $X$ at time $t$, and set $r_t = \\sigma^2_t / \\pi_t$.\nThen $\\pi_t$ is a potential, and as such can act as a model for a pricing\nkernel (or state price density), where $r_t$ is the associated interest rate.\nUnder the stated assumptions, we prove the following: (a) that the money market\naccount process defined by $B_t = \\exp (\\int_0^t r_s \\,ds)$ is finite almost\nsurely at all finite times; and (b) that the product of the money-market\naccount and the pricing kernel is a local martingale, and is a martingale\nprovided a certain integrability condition is satisfied. The fact that a\nmartingale is thus obtained shows that from any non-degenerate element of\nWiener space satisfying the integrability condition we can construct an\nassociated interest-rate model. The model thereby constructed is valid over an\ninfinite time horizon, with strictly positive interest, and satisfies the\nrelevant intertemporal relations associated with the absence of arbitrage. The\nresults thus stated pave the way for the use of Wiener chaos methods in\ninterest rate modelling, since any such square-integrable Wiener functional\nadmits a chaos expansion, the individual terms of which can be regarded as\nparametric degrees of freedom in the associated interest rate model to be fixed\nby calibration to appropriately liquid sectors of the interest rate derivatives\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3293v1"
    },
    {
        "title": "Econophysics: Bridges over a Turbulent Current",
        "authors": [
            "Shu-Heng Chen",
            "Sai-Ping Li"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  In this editorial guide for the special issue on econophysics, we give a\nunique review of this young but quickly growing discipline. A suggestive\ntaxonomy of the development is proposed by making a distinction between\nclassical econophysics and modern econophysics. For each of these two stages of\ndevelopment, we identify the key economic issues whose formulations and/or\ntreatments have been affected by physics or physicists, which includes value,\nbusiness fluctuations, economic growth, economic and financial time series, the\ndistribution of economic entities, interactions of economic agents, and\neconomic and social networks. The recent advancements in these issues of modern\neconophysics are demonstrated by nine articles selected from the papers\npresented at the Econophysics Colloquium 2010 held at Academia Sinica in\nTaipei.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.5373v1"
    },
    {
        "title": "Second-Order, Dissipative Tâtonnement: Economic Interpretation and\n  2-Point Limit Cycles",
        "authors": [
            "Eric Kemp-Benedict"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This paper proposes an alternative to the classical price-adjustment\nmechanism (called \"t\\^{a}tonnement\" after Walras) that is second-order in time.\nThe proposed mechanism, an analogue to the damped harmonic oscillator, provides\na dynamic equilibration process that depends only on local information. We show\nhow such a process can result from simple behavioural rules. The discrete-time\nform of the model can result in two-step limit cycles, but as the distance\ncovered by the cycle depends on the size of the damping, the proposed mechanism\ncan lead to both highly unstable and relatively stable behaviour, as observed\nin real economies.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.0188v3"
    },
    {
        "title": "Entropy and equilibrium state of free market models",
        "authors": [
            "J. R. Iglesias",
            "R. M. C. de Almeida"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Many recent models of trade dynamics use the simple idea of wealth exchanges\namong economic agents in order to obtain a stable or equilibrium distribution\nof wealth among the agents. In particular, a plain analogy compares the wealth\nin a society with the energy in a physical system, and the trade between agents\nto the energy exchange between molecules during collisions. In physical\nsystems, the energy exchange among molecules leads to a state of equipartition\nof the energy and to an equilibrium situation where the entropy is a maximum.\nOn the other hand, in the majority of exchange models, the system converges to\na very unequal condensed state, where one or a few agents concentrate all the\nwealth of the society while the wide majority of agents shares zero or almost\nzero fraction of the wealth. So, in those economic systems a minimum entropy\nstate is attained. We propose here an analytical model where we investigate the\neffects of a particular class of economic exchanges that minimize the entropy.\nBy solving the model we discuss the conditions that can drive the system to a\nstate of minimum entropy, as well as the mechanisms to recover a kind of\nequipartition of wealth.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5725v1"
    },
    {
        "title": "A framework for analyzing contagion in banking networks",
        "authors": [
            "Thomas R. Hurd",
            "James P. Gleeson"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  A probabilistic framework is introduced that represents stylized banking\nnetworks and aims to predict the size of contagion events. In contrast to\nprevious work on random financial networks, which assumes independent\nconnections between banks, the possibility of disassortative edge probabilities\n(an above average tendency for small banks to link to large banks) is\nexplicitly incorporated. We give a probabilistic analysis of the default\ncascade triggered by shocking the network. We find that the cascade can be\nunderstood as an explicit iterated mapping on a set of edge probabilities that\nconverges to a fixed point. A cascade condition is derived that characterizes\nwhether or not an infinitesimal shock to the network can grow to a finite size\ncascade, in analogy to the basic reproduction number $R_0$ in epidemic\nmodeling. It provides an easily computed measure of the systemic risk inherent\nin a given banking network topology. An analytic formula is given for the\nfrequency of global cascades, derived from percolation theory on the random\nnetwork. Two simple examples are used to demonstrate that edge-assortativity\ncan have a strong effect on the level of systemic risk as measured by the\ncascade condition. Although the analytical methods are derived for infinite\nnetworks, large-scale Monte Carlo simulations are presented that demonstrate\nthe applicability of the results to finite-sized networks. Finally, we propose\na simple graph theoretic quantity, which we call \"graph-assortativity\", that\nseems to best capture systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.4312v1"
    },
    {
        "title": "A Quantum-like Approach to the Stock Market",
        "authors": [
            "Diederik Aerts",
            "Bart D'Hooghe",
            "Sandro Sozzo"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Modern approaches to stock pricing in quantitative finance are typically\nfounded on the 'Black-Scholes model' and the underlying 'random walk\nhypothesis'. Empirical data indicate that this hypothesis works well in stable\nsituations but, in abrupt transitions such as during an economical crisis, the\nrandom walk model fails and alternative descriptions are needed. For this\nreason, several proposals have been recently forwarded which are based on the\nformalism of quantum mechanics. In this paper we apply the 'SCoP formalism',\nelaborated to provide an operational foundation of quantum mechanics, to the\nstock market. We argue that a stock market is an intrinsically contextual\nsystem where agents' decisions globally influence the market system and stocks\nprices, determining a nonclassical behavior. More specifically, we maintain\nthat a given stock does not generally have a definite value, e.g., a price, but\nits value is actualized as a consequence of the contextual interactions in the\ntrading process. This contextual influence is responsible of the\nnon-Kolmogorovian quantum-like behavior of the market at a statistical level.\nThen, we propose a 'sphere model' within our 'hidden measurement formalism'\nthat describes a buying/selling process of a stock and shows that it is\nintuitively reasonable to assume that the stock has not a definite price until\nit is traded. This result is relevant in our opinion since it provides a\ntheoretical support to the use of quantum models in finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5350v1"
    },
    {
        "title": "Sustainable Credit And Interest Rates",
        "authors": [
            "Andreas Hula"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  With negative growth in real production in many countries and debt levels\nwhich become an increasing burden on developed societies, the calls for a\nchange in economic policy and even the monetary system become louder and\nincreasingly impatient. We research the consequences of a system of credit and\ndebt, that still allows for the expansion of credit and fundamentally retains\nmany features of the present monetary system, without the instability inherent\nin the present system.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.3035v4"
    },
    {
        "title": "Interest Rates and Information Geometry",
        "authors": [
            "Dorje C. Brody",
            "Lane P. Hughston"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The space of probability distributions on a given sample space possesses\nnatural geometric properties. For example, in the case of a smooth parametric\nfamily of probability distributions on the real line, the parameter space has a\nRiemannian structure induced by the embedding of the family into the Hilbert\nspace of square-integrable functions, and is characterised by the Fisher-Rao\nmetric. In the nonparametric case the relevant geometry is determined by the\nspherical distance function of Bhattacharyya. In the context of term structure\nmodelling, we show that minus the derivative of the discount function with\nrespect to the maturity date gives rise to a probability density. This follows\nas a consequence of the positivity of interest rates. Therefore, by mapping the\ndensity functions associated with a given family of term structures to Hilbert\nspace, the resulting metrical geometry can be used to analyse the relationship\nof yield curves to one another. We show that the general arbitrage-free yield\ncurve dynamics can be represented as a process taking values in the convex\nspace of smooth density functions on the positive real line. It follows that\nthe theory of interest rate dynamics can be represented by a class of processes\nin Hilbert space. We also derive the dynamics for the central moments\nassociated with the distribution determined by the yield curve.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.3757v1"
    },
    {
        "title": "The minimal length uncertainty and the quantum model for the stock\n  market",
        "authors": [
            "Pouria Pedram"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We generalize the recently proposed quantum model for the stock market by\nZhang and Huang to make it consistent with the discrete nature of the stock\nprice. In this formalism, the price of the stock and its trend satisfy the\ngeneralized uncertainty relation and the corresponding generalized Hamiltonian\ncontains an additional term proportional to the fourth power of the trend. We\nstudy a driven infinite quantum well where information as the external field\nperiodically fluctuates and show that the presence of the minimal trading value\nof stocks results in a positive shift in the characteristic frequencies of the\nquantum system. The connection between the information frequency and the\ntransition probabilities is discussed finally.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.6859v2"
    },
    {
        "title": "Randomised Mixture Models for Pricing Kernels",
        "authors": [
            "Andrea Macrina",
            "Priyanka A. Parbhoo"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Numerous kinds of uncertainties may affect an economy, e.g. economic,\npolitical, and environmental ones. We model the aggregate impact by the\nuncertainties on an economy and its associated financial market by randomised\nmixtures of L\\'evy processes. We assume that market participants observe the\nrandomised mixtures only through best estimates based on noisy market\ninformation. The concept of incomplete information introduces an element of\nstochastic filtering theory in constructing what we term \"filtered Esscher\nmartingales\". We make use of this family of martingales to develop pricing\nkernel models. Examples of bond price models are examined, and we show that the\nchoice of the random mixture has a significant effect on the model dynamics and\nthe types of movements observed in the associated yield curves. Parameter\nsensitivity is analysed and option price processes are derived. We extend the\nclass of pricing kernel models by considering a weighted heat kernel approach,\nand develop models driven by mixtures of Markov processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2059v1"
    },
    {
        "title": "An Econophysics Model for the Migration Phenomena",
        "authors": [
            "Anca Gheorghiu",
            "Ion Spanulescu"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Knowing and modelling the migration phenomena and especially the social and\neconomic consequences have a theoretical and practical importance, being\nrelated to their consequences for development, economic progress (or as\nappropriate, regression), environmental influences etc. One of the causes of\nmigration, especially of the interregional and why not intercontinental, is\nthat resources are unevenly distributed, and from the human perspective there\nare differences in culture, education, mentality, collective aspirations etc.\nThis study proposes a new econophysics model for the migration phenomena.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.0996v1"
    },
    {
        "title": "Predatory trading and risk minimisation: how to (b)eat the competition",
        "authors": [
            "Anita Mehta"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We present a model of predatory traders interacting with each other in the\npresence of a central reserve (which dissipates their wealth through say,\ntaxation), as well as inflation. This model is examined on a network for the\npurposes of correlating complexity of interactions with systemic risk. We\nsuggest the use of selective networking to enhance the survival rates of\narbitrarily chosen traders. Our conclusions show that networking with 'doomed'\ntraders is the most risk-free scenario, and that if a trader is to network with\npeers, it is far better to do so with those who have less intrinsic wealth than\nhimself to ensure individual, and perhaps systemic stability.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.1374v1"
    },
    {
        "title": "Loan and nonloan flows in the Australian interbank network",
        "authors": [
            "Andrey Sokolov",
            "Rachel Webster",
            "Andrew Melatos",
            "Tien Kieu"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  High-value transactions between Australian banks are settled in the Reserve\nBank Information and Transfer System (RITS) administered by the Reserve Bank of\nAustralia. RITS operates on a real-time gross settlement (RTGS) basis and\nsettles payments sourced from the SWIFT, the Austraclear, and the interbank\ntransactions entered directly into RITS. In this paper, we analyse a dataset\nreceived from the Reserve Bank of Australia that includes all interbank\ntransactions settled in RITS on an RTGS basis during five consecutive weekdays\nfrom 19 February 2007 inclusive, a week of relatively quiescent market\nconditions. The source, destination, and value of each transaction are known,\nwhich allows us to separate overnight loans from other transactions (nonloans)\nand reconstruct monetary flows between banks for every day in our sample. We\nconduct a novel analysis of the flow stability and examine the connection\nbetween loan and nonloan flows. Our aim is to understand the underlying causal\nmechanism connecting loan and nonloan flows. We find that the imbalances in the\nbanks' exchange settlement funds resulting from the daily flows of nonloan\ntransactions are almost exactly counterbalanced by the flows of overnight\nloans. The correlation coefficient between loan and nonloan imbalances is about\n-0.9 on most days. Some flows that persist over two consecutive days can be\nhighly variable, but overall the flows are moderately stable in value. The\nnonloan network is characterised by a large fraction of persistent flows,\nwhereas only half of the flows persist over any two consecutive days in the\nloan network. Moreover, we observe an unusual degree of coherence between\npersistent loan flow values on Tuesday and Wednesday. We probe static\ntopological properties of the Australian interbank network and find them\nconsistent with those observed in other countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.3182v1"
    },
    {
        "title": "Agent-based Versus Macroscopic Modeling of Competition and Business\n  Processes in Economics and Finance",
        "authors": [
            "Aleksejus Kononovicius",
            "Vygintas Gontis",
            "Valentas Daniunas"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We present examples of agent-based and stochastic models of competition and\nbusiness processes in economics and finance. We start from as simple as\npossible models, which have microscopic, agent-based, versions and macroscopic\ntreatment in behavior. Microscopic and macroscopic versions of herding model\nproposed by Kirman and Bass diffusion of new products are considered in this\ncontribution as two basic ideas. Further we demonstrate that general herding\nbehavior can be considered as a background of nonlinear stochastic model of\nfinancial fluctuations.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.3533v2"
    },
    {
        "title": "Low-Frequency Waves and the Medium to Long-Term US Stock Market Outlook",
        "authors": [
            "Valeriy Zakamulin"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In this paper we provide compelling evidence of cyclical mean reversion and\nmultiperiod stock return predictability over horizons of about 30 years with a\nhalf-life of about 15 years. This implies that the US stock market follows a\nlong-term rhythm where a period of above average returns tends to be followed\nby a period of below average returns. We demonstrate that this long-term stock\nmarket rhythm moves in lockstep with corresponding long-term economic, social,\nand political rhythms in the US. Assuming that the past relationship between\nthese rhythms will hold unaltered in the future, we provide the medium to\nlong-term stock market outlook.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.2250v2"
    },
    {
        "title": "Evolutionary Model of the Personal Income Distribution",
        "authors": [
            "Joachim Kaldasch"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The aim of this work is to establish the personal income distribution from\nthe elementary constituents of a free market; products of a representative good\nand agents forming the economic network. The economy is treated as a\nself-organized system. Based on the idea that the dynamics of an economy is\ngoverned by slow modes, the model suggests that for short time intervals a\nfixed ratio of total labour income (capital income) to net income exists\n(Cobb-Douglas relation). Explicitly derived is Gibrat's law from an\nevolutionary market dynamics of short term fluctuations. The total private\nincome distribution is shown to consist of four main parts. From capital income\nof private firms the income distribution contains a lognormal distribution for\nsmall and a Pareto tail for large incomes. Labour income contributes an\nexponential distribution. Also included is the income from a social insurance\nsystem, approximated by a Gaussian peak. The evolutionary model is able to\nreproduce the stylized facts of the income distribution, shown by a comparison\nwith empirical data of a high resolution income distribution. The theory\nsuggests that in a free market competition between products is ultimately the\norigin of the uneven income distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.6507v1"
    },
    {
        "title": "A proposal for impact-adjusted valuation: Critical leverage and\n  execution risk",
        "authors": [
            "Fabio Caccioli",
            "Jean-Philippe Bouchaud",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The practice of valuation by marking-to-market with current trading prices is\nseriously flawed. Under leverage the problem is particularly dramatic: due to\nthe concave form of market impact, selling always initially causes the expected\nleverage to increase. There is a critical leverage above which it is impossible\nto exit a portfolio without leverage going to infinity and bankruptcy becoming\nlikely. Standard risk-management methods give no warning of this problem, which\neasily occurs for aggressively leveraged positions in illiquid markets. We\npropose an alternative accounting procedure based on the estimated market\nimpact of liquidation that removes the illusion of profit. This should curb the\nleverage cycle and contribute to an enhanced stability of financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.0922v2"
    },
    {
        "title": "Toehold Purchase Problem: A comparative analysis of two strategies",
        "authors": [
            "Iryna Banakh",
            "Taras Banakh",
            "Pavel Trisch",
            "Myroslava Vovk"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Toehold purchase, defined here as purchase of one share in a firm by an\ninvestor preparing a tender offer to acquire majority of shares in it, reduces\nby one the number of shares this investor needs for majority. In the paper we\nconstruct mathematical models for the toehold and no-toehold strategies and\ncompare the expected profits of the investor and the probabilities of takeover\nthe firm in both strategies. It turns out that the expected profits of the\ninvestor in both strategies coincide. On the other hand, the probability of\ntakeover the firm using the toehold strategy is considerably higher comparing\nto the no-toehold strategy. In the analysis of the models we apply the\napparatus of incomplete Beta functions and some refined bounds for central\nbinomial coefficients.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.2065v5"
    },
    {
        "title": "A finite-dimensional quantum model for the stock market",
        "authors": [
            "Liviu-Adrian Cotfas"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We present a finite-dimensional version of the quantum model for the stock\nmarket proposed in [C. Zhang and L. Huang, A quantum model for the stock\nmarket, Physica A 389(2010) 5769]. Our approach is an attempt to make this\nmodel consistent with the discrete nature of the stock price and is based on\nthe mathematical formalism used in the case of the quantum systems with\nfinite-dimensional Hilbert space. The rate of return is a discrete variable\ncorresponding to the coordinate in the case of quantum systems, and the\noperator of the conjugate variable describing the trend of the stock return is\ndefined in terms of the finite Fourier transform. The stock return in\nequilibrium is described by a finite Gaussian function, and the time evolution\nof the stock price, directly related to the rate of return, is obtained by\nnumerically solving a Schrodinger type equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.4614v2"
    },
    {
        "title": "Structural Hamiltonian of the international trade network",
        "authors": [
            "Agata Fronczak"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  It is common wisdom that no nation is an isolated economic island. All\nnations participate in the global economy and are linked together through trade\nand finance. Here we analyze international trade network (ITN), being the\nnetwork of import-export relationships between countries. We show that in each\nyear over the analyzed period of 50 years (since 1950) the network is a typical\nrepresentative of the ensemble of maximally random networks. Structural\nHamiltonians characterizing binary and weighted versions of ITN are formulated\nand discussed. In particular, given binary representation of ITN (i.e. binary\nnetwork of trade channels) we show that the network of partnership in trade is\nwell described by the configuration model. We also show that in the weighted\nversion of ITN, bilateral trade volumes (i.e. directed connections which\nrepresent trade/money flows between countries) are only characterized by the\nproduct of the trading countries' GDPs, like in the famous gravity model of\ntrade.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.4589v1"
    },
    {
        "title": "No-Arbitrage Pricing for Dividend-Paying Securities in Discrete-Time\n  Markets with Transaction Costs",
        "authors": [
            "Tomasz R. Bielecki",
            "Igor Cialenco",
            "Rodrigo Rodriguez"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We prove a version of First Fundamental Theorem of Asset Pricing under\ntransaction costs for discrete-time markets with dividend-paying securities.\nSpecifically, we show that the no-arbitrage condition under the efficient\nfriction assumption is equivalent to the existence of a risk-neutral measure.\nWe derive dual representations for the superhedging ask and subhedging bid\nprice processes of a derivative contract. Our results are illustrated with a\nvanilla credit default swap contract.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.6254v2"
    },
    {
        "title": "A physical theory of economic growth",
        "authors": [
            "Hans G. Danielmeyer",
            "Thomas Martinetz"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Economic growth is unpredictable unless demand is quantified. We solve this\nproblem by introducing the demand for unpaid spare time and a user quantity\nnamed human capacity. It organizes and amplifies spare time required for\nenjoying affluence like physical capital, the technical infrastructure for\nproduction, organizes and amplifies working time for supply. The sum of annual\nspare and working time is fixed by the universal flow of time. This yields the\nfirst macroeconomic equilibrium condition. Both storable quantities form\nstabilizing feedback loops. They are driven with the general and technical\nknowledge embodied with parts of the supply by education and construction.\nLinear amplification yields S-functions as only analytic solutions.\nDestructible physical capital controls medium-term recoveries from disaster.\nIndestructible human capacity controls the collective long-term industrial\nevolution. It is immune even to world wars and runs from 1800 to date parallel\nto the unisex life expectancy in the pioneering nations. This is the first\nquantitative information on long-term demand. The theory is self-consistent. It\nreproduces all peaceful data from 1800 to date without adjustable parameter. It\nhas full forecasting power since the decisive parameters are constants of the\nhuman species. They predict an asymptotic maximum for the economic level per\ncapita. Long-term economic growth appears as a part of natural science.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2494v1"
    },
    {
        "title": "Evolutionary Model of the Growth and Size of Firms",
        "authors": [
            "Joachim Kaldasch"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The key idea of this model is that firms are the result of an evolutionary\nprocess. Based on demand and supply considerations the evolutionary model\npresented here derives explicitly Gibrat's law of proportionate effects as the\nresult of the competition between products. Applying a preferential attachment\nmechanism for firms the theory allows to establish the size distribution of\nproducts and firms. Also established are the growth rate and price distribution\nof consumer goods. Taking into account the characteristic property of human\nactivities to occur in bursts, the model allows also an explanation of the\nsize-variance relationship of the growth rate distribution of products and\nfirms. Further the product life cycle, the learning (experience) curve and the\nmarket size in terms of the mean number of firms that can survive in a market\nare derived. The model also suggests the existence of an invariant of a market\nas the ratio of total profit to total revenue. The relationship between a\nneo-classic and an evolutionary view of a market is discussed. The comparison\nwith empirical investigations suggests that the theory is able to describe the\nmain stylized facts concerning the size and growth of firms.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.1123v1"
    },
    {
        "title": "Economic decision making: application of the theory of complex systems",
        "authors": [
            "Robert Kitt"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In this chapter the complex systems are discussed in the context of economic\nand business policy and decision making. It will be showed and motivated that\nsocial systems are typically chaotic, non-linear and/or non-equilibrium and\ntherefore complex systems. It is discussed that the rapid change in global\nconsumer behaviour is underway, that further increases the complexity in\nbusiness and management. For policy making under complexity, following\nprinciples are offered: openness and international competition, tolerance and\nvariety of ideas, self-reliability and low dependence on external help. The\nchapter contains four applications that build on the theoretical motivation of\ncomplexity in social systems. The first application demonstrates that small\neconomies have good prospects to gain from the global processes underway, if\nthey can demonstrate production flexibility, reliable business ethics and good\nrisk management. The second application elaborates on and discusses the\nopportunities and challenges in decision making under complexity from macro and\nmicro economic perspective. In this environment, the challenges for corporate\nmanagement are being also permanently changed: the balance between short term\nnoise and long term chaos whose attractor includes customers, shareholders and\nemployees must be found. The emergence of chaos in economic relationships is\ndemonstrated by a simple system of differential equations that relate the\nstakeholders described above. The chapter concludes with two financial\napplications: about debt and risk management. The non-equilibrium economic\nestablishment leads to additional problems by using excessive borrowing;\nunexpected downturns in economy can more easily kill companies. Finally, the\ndemand for quantitative improvements in risk management is postulated.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.1277v2"
    },
    {
        "title": "Inverse Thinking in Economic Theory: A Radical Approach to Economic\n  Thinking",
        "authors": [
            "Jaime Gomez-Ramirez"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The seriousness of the current crisis urgently demands new economic thinking\nthat breaks the austerity vs. deficit spending circle in economic policy. The\ncore tenet of the paper is that the most important problems that natural and\nsocial science are facing today are inverse problems, and that a new approach\nthat goes beyond optimization is necessary. The approach presented here is\nradical in the sense that identifies the roots in key assumptions in economic\ntheory such as optimal behavior and stability to provide an inverse thinking\nperspective to economic modeling of use in economic and financial stability\npolicy. The inverse problem provides a truly multidisciplinary platform where\nrelated problems from different disciplines can be studied under a common\napproach with comparable results.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.3460v1"
    },
    {
        "title": "On Global Stability of Financial Networks",
        "authors": [
            "Bhaskar DasGupta",
            "Lakshmi Kaligounder"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The recent financial crisis have generated renewed interests in fragilities\nof global financial networks among economists and regulatory authorities. In\nparticular, a potential vulnerability of the financial networks is the\n\"financial contagion\" process in which insolvencies of individual entities\npropagate through the \"web of dependencies\" to affect the entire system. In\nthis paper, we formalize an extension of a financial network model originally\nproposed by Nier et al. for scenarios such as the OTC derivatives market,\ndefine a suitable global stability measure for this model, and perform a\ncomprehensive empirical evaluation of this stability measure over more than\n700,000 combinations of networks types and parameter combinations. Based on our\nevaluations, we discover many interesting implications of our evaluations of\nthis stability measure, and derive topological properties and parameters\ncombinations that may be used to flag the network as a possible fragile\nnetwork. An interactive software FIN-STAB for computing the stability is\navailable from the website www2.cs.uic.edu/~dasgupta/financial-simulator-files\n",
        "pdf_link": "http://arxiv.org/pdf/1208.3789v5"
    },
    {
        "title": "Yard-Sale exchange on networks: Wealth sharing and wealth appropriation",
        "authors": [
            "R. Bustos-Guajardo",
            "Cristian F. Moukarzel"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Yard-Sale (YS) is a stochastic multiplicative wealth-exchange model with two\nphases: a stable one where wealth is shared, and an unstable one where wealth\ncondenses onto one agent. YS is here studied numerically on 1d rings, 2d square\nlattices, and random graphs with variable average coordination, comparing its\nproperties with those in mean field (MF). Equilibrium properties in the stable\nphase are almost unaffected by the introduction of a network. Measurement of\ndecorrelation times in the stable phase allow us to determine the critical\ninterface with very good precision, and it turns out to be the same, for all\nnetworks analyzed, as the one that can be analytically derived in MF. In the\nunstable phase, on the other hand, dynamical as well as asymptotic properties\nare strongly network-dependent. Wealth no longer condenses on a single agent,\nas in MF, but onto an extensive set of agents, the properties of which depend\non the network. Connections with previous studies of coalescence of immobile\nreactants are discussed, and their analytic predictions are successfully\ncompared with our numerical results.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.4409v1"
    },
    {
        "title": "Characterizing the development of sectoral Gross Domestic Product\n  composition",
        "authors": [
            "Raphael Lutz",
            "Michael Spies",
            "Dominik E. Reusser",
            "Jürgen P. Kropp",
            "Diego Rybski"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We consider the sectoral composition of a country's GDP, i.e. the\npartitioning into agrarian, industrial, and service sectors. Exploring a simple\nsystem of differential equations we characterize the transfer of GDP shares\nbetween the sectors in the course of economic development. The model fits for\nthe majority of countries providing 4 country-specific parameters. Relating the\nagrarian with the industrial sector, a data collapse over all countries and all\nyears supports the applicability of our approach. Depending on the parameter\nranges, country development exhibits different transfer properties. Most\ncountries follow 3 of 8 characteristic paths. The types are not random but show\ndistinct geographic and development patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.3543v3"
    },
    {
        "title": "Redistribution spurs growth by using a portfolio effect on human capital",
        "authors": [
            "Jan Lorenz",
            "Fabian Paetzel",
            "Frank Schweitzer"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We demonstrate by mathematical analysis and systematic computer simulations\nthat redistribution can lead to sustainable growth in a society. The human\ncapital dynamics of each agent is described by a stochastic multiplicative\nprocess which, in the long run, leads to the destruction of individual human\ncapital and the extinction of the individualistic society. When agents are\nlinked by fully-redistributive taxation the situation might turn to individual\ngrowth in the long run. We consider that a government collects a proportion of\nincome and reduces it by a fraction as costs for administration (efficiency\nlosses). The remaining public good is equally redistributed to all agents. We\nderive conditions under which the destruction of human capital can be turned\ninto sustainable growth, despite the losses from the random growth process and\ndespite the administrative costs. Sustainable growth is induced by\nredistribution. This effect could be explained by a simple portfolio-effect\nwhich re-balances individual stochastic processes.\n  The findings are verified for three different tax schemes: proportional tax,\ntaking proportional more from the rich, and proportionally more from the poor.\nWe discuss which of these tax schemes is optimal with respect to maximize\ngrowth under a fixed rate of administrative costs, or with respect to maximize\nthe governmental income. This leads us to some general conclusions about\ngovernmental decisions, the relation to public good games, and the use of\ntaxation in a risk taking society.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.3716v1"
    },
    {
        "title": "Towards international E-stat for monitoring the socio-economic\n  activities across the globe",
        "authors": [
            "Aki-Hiro Sato",
            "Ken Umeno"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We investigate relationship between annual electric power consumption per\ncapita and gross domestic production (GDP) per capita for 131 countries. We\nfound that the relationship can be fitted with a power-law function. We examine\nthe relationship for 47 prefectures in Japan. Furthermore, we investigate\nvalues of annual electric power production reported by four international\norganizations. We collected the data from U.S. Energy Information\nAdministration (EIA), Statistics by International Energy Agency (IEA), OECD\nFactbook (Economic, Environmental and Social Statistics), and United Nations\n(UN) Energy Statistics Yearbook. We found that the data structure, values, and\nunit depend on the organizations. This implies that it is further necessary to\nestablish data standards and an organization to collect, store, and distribute\nthe data on socio-economic systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.4129v1"
    },
    {
        "title": "Modeling Spatial Equilibrium in Cities: the Isobenefit Lines",
        "authors": [
            "Luca D'Acci"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  I propose and briefly define the concept of Urban Isobenefit Lines by using\nfunctions as easy as efficient, whose results can offer a rich tool to use into\nspatial equilibrium analysis involving cities. They are line joining urban\npoints with equal level of positional advantage from city amenities. The\nresults which one obtain by implementing a chosen function, gave specific\nscenarios: numerically described by indicators and graphically visualized by\nefficient city matrix views. This is also a theoretical concept for the Urban\nEconomics theory and Spatial Equilibrium analysis in cities.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.4461v3"
    },
    {
        "title": "Food for fuel: The price of ethanol",
        "authors": [
            "Dominic K. Albino",
            "Karla Z. Bertrand",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Conversion of corn to ethanol in the US since 2005 has been a major cause of\nglobal food price increases during that time and has been shown to be\nineffective in achieving US energy independence and reducing environmental\nimpact. We make three key statements to enhance understanding and communication\nabout ethanol production's impact on the food and fuel markets: (1) The amount\nof corn used to produce the ethanol in a gallon of regular gas would feed a\nperson for a day, (2) The production of ethanol is so energy intensive that it\nuses only 20% less fossil fuel than gasoline, and (3) The cost of gas made with\nethanol is actually higher per mile because ethanol reduces gasoline's energy\nper gallon.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.6080v1"
    },
    {
        "title": "Isobenefit Lines, Breaking Point of equal attraction, Uniformity\n  Benefit, Variety Value and Proximity Value, Preference Gap Gain",
        "authors": [
            "Luca D'Acci"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Isobenefit Lines can offer a certain range of applicability in Location\nTheory and Gravitational Models for Urban and Geography Economics, in\npositional decision processes made by citizens, and, last but not least, in\nland value and property market theories and analysis. The value of a land, or a\nproperty, in a generic k point, is, ceteris paribus, the mirror of the quality,\nattractiveness, benefit characterizing k. Preference Gap Gain (PGG) of a\nperson, is the difference between his Personal Isobenefit Lines and that of the\nmajority of people. In monetary terms, when buying or renting a property, it\ncan become an economic gain or vice versa, and PGG localizes and quantifies\nthis gain.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.7510v2"
    },
    {
        "title": "Can we explain unexpected fluctuations of long-term real interest rate?",
        "authors": [
            "Barbora Volná"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In this paper, we present own point of view how the unexpected fluctuations\nof the long-term real interest rate can be explained. We describe a\nmacroeconomic environment by the modification of the fundamental macroeconomic\nequilibrium model called the IS-LM model. Last but not least, we suggest a\npossible cooperation between the fiscal and monetary policy to reduce these\nfluctuations. Our modelling is demonstrated on an illustrative example.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.2709v2"
    },
    {
        "title": "Can we predict long-run economic growth?",
        "authors": [
            "Timothy J. Garrett"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  For those concerned with the long-term value of their accounts, it can be a\nchallenge to plan in the present for inflation-adjusted economic growth over\ncoming decades. Here, I argue that there exists an economic constant that\ncarries through time, and that this can help us to anticipate the more distant\nfuture: global economic wealth has a fixed link to civilization's overall rate\nof energy consumption from all sources; the ratio of these two quantities has\nnot changed over the past 40 years that statistics are available. Power\nproduction and wealth rise equally quickly because civilization, like any other\nsystem in the universe, must consume and dissipate its energy reserves in order\nto sustain its current size. One perspective might be that financial wealth\nmust ultimately collapse as we deplete our energy reserves. However, we can\nalso expect that highly aggregated quantities like global wealth have inertia,\nand that growth rates must persist. Exceptionally rapid innovation in the two\ndecades following 1950 allowed for unprecedented acceleration of\ninflation-adjusted rates of return. But today, real innovation rates are more\nstagnant. This means that, over the coming decade or so, global GDP and wealth\nshould rise fairly steadily at an inflation-adjusted rate of about 2.2% per\nyear.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.3102v1"
    },
    {
        "title": "Heterogeneous Enterprises in a Macroeconomic Agent-Based Model",
        "authors": [
            "Cornelia Metzig",
            "Mirta Gordon"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We present a macroeconomic agent-based model that combines several mechanisms\noperating at the same timescale, while remaining mathematically tractable. It\ncomprises enterprises and workers who compete in a job market and a commodity\ngoods market. The model is stock-flow consistent; a bank lends money charging\ninterest rates, and keeps track of equities. Important features of the model\nare heterogeneity of enterprises, existence of bankruptcies and creation of new\nenterprises, as well as productivity increase. The model's evolution reproduces\nempirically found regularities for firm size and growth rate distributions. It\ncombines probabilistic elements and deterministic dynamics, with relative\nweights that may be modified according to the considered problem or the belief\nof the modeler. We discuss statistical regularities on enterprises, the origin\nand the amplitude of endogeneous fluctuations of the system's steady state, as\nwell as the role of the interest rate and the credit volume. We also summarize\nobtained results which are not discussed in detail in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5575v1"
    },
    {
        "title": "Statistical Microeconomics",
        "authors": [
            "Belal E. Baaquie"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  A statistical generalization is made of microeconomics in the spirit of going\nfrom classical to statistical mechanics. The price and quantity of every\ncommodity1 traded in the market, at each instant of time, is considered to be\nan independent random variable: all prices and quantities are considered to be\nstochastic processes, with the observed market prices being a random sample of\nthe stochastic prices. The dynamics of market prices is determined by an action\nfunctional and, for concreteness, a specific model is proposed. The model can\nbe calibrated from the unequal time correlation of the market commodity prices.\nA perturbation expansion for the correlation functions is defined in powers of\nthe inverse of the total budget of the aggregate consumer and the propagator\nfor the market prices is evaluated.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.7172v1"
    },
    {
        "title": "The physics of business cycles and inflation",
        "authors": [
            "Hans G. Danielmeyer",
            "Thomas Martinetz"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We analyse four consecutive cycles observed in the USA for employment and\ninflation. They are driven by three oil price shocks and an intended interest\nrate shock. Non-linear coupling between the rate equations for consumer\nproducts as prey and consumers as predators provides the required instability,\nbut its natural damping is too high for spontaneous cycles. Extending the\nLotka-Volterra equations with a small term for collective anticipation yields a\nsecond analytic solution without damping. It predicts the base period, phase\nshifts, and the sensitivity to shocks for all six cyclic variables correctly.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.1282v1"
    },
    {
        "title": "Predicting economic growth with classical physics and human biology",
        "authors": [
            "Hans G. Danielmeyer",
            "Thomas Martinetz"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We collect and analyze the data for working time, life expectancy, and the\npair output and infrastructure of industrializing nations. During S-functional\nrecovery from disaster the pair's time shifts yield 25 years for the\ninfrastructure's physical lifetime. At G7 level the per capita outputs converge\nand the time shifts identify a heritable quantity with a reaction time of 62\nyears. It seems to control demand and the spare time required for enjoying G7\naffluence. The sum of spare and working time is fixed by the universal flow of\ntime. This yields analytic solutions for equilibrium, recovery, and long-term\nevolution for all six variables with biologically stabilized parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.1286v1"
    },
    {
        "title": "Econophysics in Belgium. The first (?) 15 years",
        "authors": [
            "Marcel Ausloos"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  This reviews the econophysics activities in Belgium from my admittedly biased\npoint of view. Unknown historical notes or facts are presented for the first\ntime explaining the aims, whence evolution of the research papers and friendly\nconnections with colleagues. Comments on endeavors are also provided. The lack\nof official, academic and private support is outlined.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.1946v1"
    },
    {
        "title": "Information content of financial markets: a practical approach based on\n  Bohmian quantum mechanics",
        "authors": [
            "F. Tahmasebi",
            "S. Meskini",
            "A. Namaki",
            "G. R. Jafari"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The Bohmian quantum approach is implemented to analyze the financial markets.\nIn this approach, there is a wave function that leads to a quantum potential.\nThis potential can explain the relevance and entanglements of the agent's\nbehaviors with the past. The light is shed by considering the relevance of the\nmarket conditions with the previous market conditions enabling the conversion\nof the local concepts to the global ones. We have shown that there are two\npotential limits for each market. In essence, these potential limits act as a\nboundary which limits the return values inside it. By estimating the difference\nbetween these two limits in each market, it is found that the quantum\npotentials of the return time series in different time scales, possess a\nscaling behavior. The slopes of the scaling behaviors in mature, emerging and\ncommodity markets show different patterns. The emerge market having a slope\ngreater than 0.5, has a higher value compared to the corresponding values for\nthe mature and commodity markets which is less than 0.5. The cut-off observed\nin the curve of the commodity market indicates the threshold for the efficiency\nof the global effects. While before the cut-off, local effects in the market\nare dominant, as in the case of the mature markets. The findings could prove\nadequate for investors in different markets to invest in different time\nhorizons.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.4293v1"
    },
    {
        "title": "Testing the Goodwin growth-cycle macroeconomic dynamics in Brazil",
        "authors": [
            "N. J. Moura Jr",
            "Marcelo B. Ribeiro"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper discusses the empirical validity of Goodwin's (1967) macroeconomic\nmodel of growth with cycles by assuming that the individual income distribution\nof the Brazilian society is described by the Gompertz-Pareto distribution\n(GPD). This is formed by the combination of the Gompertz curve, representing\nthe overwhelming majority of the population (~99%), with the Pareto power law,\nrepresenting the tiny richest part (~1%). In line with Goodwin's original\nmodel, we identify the Gompertzian part with the workers and the Paretian\ncomponent with the class of capitalists. Since the GPD parameters are obtained\nfor each year and the Goodwin macroeconomics is a time evolving model, we use\npreviously determined, and further extended here, Brazilian GPD parameters, as\nwell as unemployment data, to study the time evolution of these quantities in\nBrazil from 1981 to 2009 by means of the Goodwin dynamics. This is done in the\noriginal Goodwin model and an extension advanced by Desai et al. (2006). As far\nas Brazilian data is concerned, our results show partial qualitative and\nquantitative agreement with both models in the studied time period, although\nthe original one provides better data fit. Nevertheless, both models fall short\nof a good empirical agreement as they predict single center cycles which were\nnot found in the data. We discuss the specific points where the Goodwin\ndynamics must be improved in order to provide a more realistic representation\nof the dynamics of economic systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1090v3"
    },
    {
        "title": "The Role of Social Feedback in Financing of Technology Ventures",
        "authors": [
            "Aleksandar Bradic"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  This research examines relationship between staging of Venture Capital (VC)\ninvestments and social feedback visible in publicly available data on the Web.\nWe address the question of Venture Capital investment sensitivity to\nperformance and prospects of new venture, given as likelihood of obtaining\nfuture financing, available exit options and duration between investment\nrounds. We argue that in the case of Internet companies, publicly available\nsocial feedback data, such as search trends and website traffic information,\ncan be used as a proxy for some of company's internal metrics such as user base\ngrowth and product adoption. In order to answer questions of interest, we\ncompile unique dataset consisting of detailed information about Venture Capital\ninvestments in the Internet Technology sector over the period from 2004 to 2012\nand associated longitudinal search trend and website traffic data. By applying\nmethods of survival analysis, we find that positive trends in search and\nwebsite traffic volumes can lead to increased likelihood of future financing\nand shortening of duration between subsequent financing rounds. We also find\nevidence that social feedback only impacts company's ability to attract next\nround of financing or exit via IPO, while M&A exits seem relatively independent\nof such performance metrics and can occur at any stage of company development.\nSuch findings provide strong evidence in support of learning hypothesis and\nsuggest VC's ability to identify prospects of new venture early in it's\ndevelopment and allocate funding accordingly. Given research also provides\nmethodological contributions to the problem of evaluating the prospects of new\nstartup companies using only publicly available data, and as such should be of\ninterest in applications such as new investment screening and industry-level\nassessments by analysts or policy makers.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2196v1"
    },
    {
        "title": "The Community Structure of the Global Corporate Network",
        "authors": [
            "Stefania Vitali",
            "Stefano Battiston"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We investigate the community structure of the global ownership network of\ntransnational corporations. We find a pronounced organization in communities\nthat cannot be explained by randomness. Despite the global character of this\nnetwork, communities reflect first of all the geographical location of firms,\nwhile the industrial sector plays only a marginal role. We also analyze the\nnetwork in which the nodes are the communities and the links are obtained by\naggregating the links among firms belonging to pairs of communities. We analyze\nthe network centrality of the top 50 communities and we provide the first\nquantitative assessment of the financial sector role in connecting the global\neconomy.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2363v1"
    },
    {
        "title": "On Bankruptcy Game Theoretic Interval Rules",
        "authors": [
            "Rodica Branzei",
            "Marco Dall'Aglio",
            "Stef H. Tijs"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Interval bankruptcy problems arise in situations where an estate has to be\nliquidated among a fixed number of creditors and uncertainty about the amounts\nof the claims is modeled by intervals. We extend in the interval setting the\nclassical results by Curiel, Maschler and Tijs (1987) that characterize\ndivision rules which correspond to solutions of the cooperative bankruptcy\ngame. Finally, we analyze the difficulties with incorporating the uncertainty\nabout the estate.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.3096v1"
    },
    {
        "title": "Anticipatory Systems, Preferences, Averages: Inflation, Uncertain\n  Phenomena, Management",
        "authors": [
            "Leonid A. Shapiro"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Behavior of systems that are functions of anticipated behavior of other\nsystems, whose own behavior is also anticipatory but homeostatic and determined\nby hierarchical ordering, which changes over time, of sets of possible\nenvironments that are not co-possible, is proven to be highly non-linear and\nsensitively dependent on precise parameters. Averages and other kinds of\naggregates cannot be calculated for sets of measurements of behavior of\nsystems, defined in this essay, that are \"index complex\" in this way. This\nincludes many systems, for instance, social behavior, where anticipation of\nbehavior of other individuals plays a central role. Anticipation of preferences\nof economic actors are discussed in this way. Analysis by way of generalized\nfunctions of complex variables is done for these kinds of systems, and\nequations of change of state are formally described. Behavior that comprises of\nresponses to market interest rates is taken for example. Continuity assumptions\nin economics analyzed in this context. Anticipatory responses to inflation in\neconomics are discussed. Applications to theory of production are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.4207v2"
    },
    {
        "title": "Conservation laws, financial entropy and the Eurozone crisis",
        "authors": [
            "Paul Cockshott",
            "David Zachariah"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The report attempts of apply econophysics concepts to the Eurozone crisis. It\nstarts by examining the idea of conservation laws as applied to market\neconomies. It formulates a measure of financial entropy and gives numerical\nsimulations indicating that this tends to rise. We discuss an analogue for free\nenergy released during this process. The concepts of real and symbolic\nappropriation are introduced as a means to analyse debt and taxation.\n  We then examine the conflict between the conservation laws that apply to\ncommodity exchange with the exponential growth implied by capital accumulation\nand how these have necessitated a sequence of evolutionary forms for money, and\ngo on to present a simple stochastic model for the formation of rates of\ninterest and a model for the time evolution of the rate of profit.\n  Finally we apply the conservation law model to examining the Euro Crisis and\nthe European Stability pact, arguing that if the laws we hypothesise actually\nhold, then the goals of the stability pact are unobtainable.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.5974v1"
    },
    {
        "title": "Ab initio analysis of all income society classes in the European Union",
        "authors": [
            "Maciej Jagielski",
            "Ryszard Kutner"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We found a unified formula for description of the household incomes of all\nsociety classes, for instance, of those of the European Union in year 2007.\nThis formula is a stationary solution of the threshold Fokker-Planck equation\n(derived from the threshold nonlinear Langevin one). The formula is more\ngeneral than the well known that of Yakovenko et al. because it satisfactorily\ndescribes not only household incomes of low- and medium-income society classes\nbut also the household incomes of the high-income society class.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.6519v1"
    },
    {
        "title": "On return rate implied by behavioural present value",
        "authors": [
            "Krzysztof Piasecki"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The future value of a security is described as a random variable.\nDistribution of this random variable is the formal image of risk uncertainty.\nOn the other side, any present value is defined as a value equivalent to the\ngiven future value. This equivalence relationship is a subjective. Thus\nfollows, that present value is described as a fuzzy number, which is depend on\nthe investor's susceptibility to behavioural factors. All above reasons imply,\nthat return rate is given as a fuzzy probabilistic set. The basic properties of\nsuch image of return rate are studied. At the last the set of effective\nsecurities is distinguished as a fuzzy set.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.0538v1"
    },
    {
        "title": "Behavioural present value",
        "authors": [
            "Krzysztof Piasecki"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Impact of chosen behavioural factors on imprecision of present value is\ndiscussed here. The formal model of behavioural present value is offered as a\nresult of this discussion. Behavioural present value is described here by fuzzy\nset. These considerations were illustrated by means of extensive numerical case\nstudy. Finally there are shown that in proposed model the return rate is given,\nas a fuzzy probabilistic set.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.0539v1"
    },
    {
        "title": "Efficient Markets, Behavioral Finance and a Statistical Evidence of the\n  Validity of Technical Analysis",
        "authors": [
            "Marco Antonio Penteado"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This work tried to detect the existence of a relationship between the graphic\nsignals - or patterns - observed day by day in the Brazilian stock market and\nthe trends which happen after these signals, within a period of 8 years, for a\nnumber of securities. The results obtained from this study show evidence of the\nexistence of such a relationship, suggesting the validity of the Technical\nAnalysis as an instrument to predict the trend of security prices in the\nBrazilian stock market within that period.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.1228v1"
    },
    {
        "title": "From Nobel Prize to Project Management: Getting Risks Right",
        "authors": [
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  A major source of risk in project management is inaccurate forecasts of\nproject costs, demand, and other impacts. The paper presents a promising new\napproach to mitigating such risk, based on theories of decision making under\nuncertainty which won the 2002 Nobel prize in economics. First, the paper\ndocuments inaccuracy and risk in project management. Second, it explains\ninaccuracy in terms of optimism bias and strategic misrepresentation. Third,\nthe theoretical basis is presented for a promising new method called \"reference\nclass forecasting,\" which achieves accuracy by basing forecasts on actual\nperformance in a reference class of comparable projects and thereby bypassing\nboth optimism bias and strategic misrepresentation. Fourth, the paper presents\nthe first instance of practical reference class forecasting, which concerns\ncost forecasts for large transportation infrastructure projects. Finally,\npotentials for and barriers to reference class forecasting are assessed.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.3642v1"
    },
    {
        "title": "Impact Analysis for Risks in Informatics Systems",
        "authors": [
            "Floarea Baicu",
            "Maria Alexandra Baches"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In this paper are presented methods of impact analysis on informatics system\nsecurity accidents, qualitative and quantitative methods, starting with risk\nand informational system security definitions. It is presented the relationship\nbetween the risks of exploiting vulnerabilities of security system, security\nlevel of these informatics systems, probability of exploiting the weak points\nsubject to financial losses of a company, respectively impact of a security\naccident on the company. Herewith are presented some examples concerning losses\ncaused by excesses within informational systems and depicted from the study\ncarried out by CSI.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.1663v1"
    },
    {
        "title": "Nanotechnology and Innovation, Recent status and the strategic\n  implication for the formation of high tech clusters in Greece, in between a\n  global economic crisis",
        "authors": [
            "Evangelos I. Gkanas",
            "Vasso MagkouKriticou",
            "Sofoklis S. Makridis",
            "Athanasios K. Stubos",
            "Ioannis Bakouros"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Nanotechnology is the first major worldwide research initiative of the 21st\ncentury and probably is the solution vector in the economic environment. Also,\ninnovation is widely recognized as a key factor in the economic development of\nnations, and is essential for the competitiveness of the industrial firms as\nwell. Policy and management of innovation are necessary in order to develop\ninnovation and it involves processes. It is essential to develop new methods\nfor nanotechnology development for better understanding of nanotechnology based\ninnovation. Nanotechnologies reveal commercialization processes, from start ups\nto large firms in collaboration with public sector research. In the current\npaper, a study in the present status of innovation in nanotechnology and the\naffection of global economic crisis in this section is made and also the\npotential of increase the innovation via the presence of clusters in a small\ncountry like Greece which is in the eye of tornado from the global crisis is\nstudied.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.5290v2"
    },
    {
        "title": "Feedback models and stability analysis of three economic paradigms",
        "authors": [
            "Harris V. Georgiou"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In this paper, simple mathematical models from Control Theory are applied to\nthree very important economic paradigms, namely (a) minimum wages in\nself-regulating markets, (b) market-versus-true values and currency rates, and\n(c) government spending and taxation levels. Analytical solutions are provided\nin all three paradigms and some useful conclusions are drawn in terms of\nvariable analysis. This short study can be used as an example of how feedback\nmodels and stability analysis can be applied as a guideline of 'proofs' in the\ncontext of economic policies.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.5882v1"
    },
    {
        "title": "A Model for Scaling in Firms' Size and Growth Rate Distribution",
        "authors": [
            "Cornelia Metzig",
            "Mirta B. Gordon"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We introduce a simple agent-based model which allows us to analyze three\nstylized facts: a fat-tailed size distribution of companies, a `tent-shaped'\ngrowth rate distribution, the scaling relation of the growth rate variance with\nfirm size, and the causality between them. This is achieved under the simple\nhypothesis that firms compete for a scarce quantity (either aggregate demand or\nworkforce) which is allocated probabilistically. The model allows us to relate\nsize and growth rate distributions. We compare the results of our model to\nsimulations with other scaling relationships, and to similar models and relate\nit to existing theory. Effects arising from binning data are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.4311v3"
    },
    {
        "title": "Double Whammy - How ICT Projects are Fooled by Randomness and Screwed by\n  Political Intent",
        "authors": [
            "Alexander Budzier",
            "Bent Flyvbjerg"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The cost-benefit analysis formulates the holy trinity of objectives of\nproject management - cost, schedule, and benefits. As our previous research has\nshown, ICT projects deviate from their initial cost estimate by more than 10%\nin 8 out of 10 cases. Academic research has argued that Optimism Bias and Black\nSwan Blindness cause forecasts to fall short of actual costs. Firstly, optimism\nbias has been linked to effects of deception and delusion, which is caused by\ntaking the inside-view and ignoring distributional information when making\ndecisions. Secondly, we argued before that Black Swan Blindness makes\ndecision-makers ignore outlying events even if decisions and judgements are\nbased on the outside view. Using a sample of 1,471 ICT projects with a total\nvalue of USD 241 billion - we answer the question: Can we show the different\neffects of Normal Performance, Delusion, and Deception? We calculated the\ncumulative distribution function (CDF) of (actual-forecast)/forecast. Our\nresults show that the CDF changes at two tipping points - the first one\ntransforms an exponential function into a Gaussian bell curve. The second\ntipping point transforms the bell curve into a power law distribution with the\npower of 2. We argue that these results show that project performance up to the\nfirst tipping point is politically motivated and project performance above the\nsecond tipping point indicates that project managers and decision-makers are\nfooled by random outliers, because they are blind to thick tails. We then show\nthat Black Swan ICT projects are a significant source of uncertainty to an\norganisation and that management needs to be aware of.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.4590v1"
    },
    {
        "title": "Cross-border Portfolio Investment Networks and Indicators for Financial\n  Crises",
        "authors": [
            "Andreas Joseph",
            "Stephan Joseph",
            "Guanrong Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Cross-border equity and long-term debt securities portfolio investment\nnetworks are analysed from 2002 to 2012, covering the 2008 global financial\ncrisis. They serve as network-proxies for measuring the robustness of the\nglobal financial system and the interdependence of financial markets,\nrespectively. Two early-warning indicators for financial crises are identified:\nFirst, the algebraic connectivity of the equity securities network, as a\nmeasure for structural robustness, drops close to zero already in 2005, while\nthere is an over-representation of high-degree off-shore financial centres\namong the countries most-related to this observation, suggesting an\ninvestigation of such nodes with respect to the structural stability of the\nglobal financial system. Second, using a phenomenological model, the edge\ndensity of the debt securities network is found to describe, and even forecast,\nthe proliferation of several over-the-counter-traded financial derivatives,\nmost prominently credit default swaps, enabling one to detect potentially\ndangerous levels of market interdependence and systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0215v3"
    },
    {
        "title": "Reserve Requirement Analysis using a Dynamical System of a Bank based on\n  Monti-Klein model of Bank's Profit Function",
        "authors": [
            "Novriana Sumarti",
            "Iman Gunadi"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Commercial banks and other depository institutions in some countries are\nrequired to hold in reserve against deposits made by their customers at their\nCentral Bank or Federal Reserve. Although some countries have been eliminated\nit, this requirement is useful as one of many Central Bank's regulation made to\ncontrol rate of inflation and conditions of excess liquidity in banks which\ncould affect the monetary stability. The amount of this reserve is affected by\nthe volumes of the commercial bank's loan and deposit, and also by the bank's\nLoan to Deposit Ratio (LDR) value. In this research, a dynamical system of the\nvolume of deposits (dD/dt) and loans (dL/dt) of a bank is constructed from the\nbank profit equation by Monti-Klein. The model is implemented using the\nregulation of Bank of Indonesia, and analysed in terms of the behaviour of the\nsolution. Based on some simplifying assumptions in this model, the results show\nthat eventhough the LDR values at the initial points of two solutions are the\nsame, the behavior of solutions will be significantly different due to\ndifferent magnitude of L and D volumes.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0468v1"
    },
    {
        "title": "Thermodynamics of long-run economic innovation and growth",
        "authors": [
            "Timothy J. Garrett"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This article derives prognostic expressions for the evolution of globally\naggregated economic wealth, productivity, inflation, technological change,\ninnovation and growth. The approach is to treat civilization as an open,\nnon-equilibrium thermodynamic system that dissipates energy and diffuses matter\nin order to sustain existing circulations and to further its material growth.\nAppealing to a prior result that established a fixed relationship between a\nvery general representation of global economic wealth and rates of global\nprimary energy consumption, physically derived expressions for economic\nquantities follow. The analysis suggests that wealth can be expressed in terms\nof the length density of civilization's networks and the availability of energy\nresources. Rates of return on wealth are accelerated by energy reserve\ndiscovery, improvements to human and infrastructure longevity, and a more\ncommon culture, or a lowering of the amount of energy required to diffuse raw\nmaterials into civilization's bulk. According to a logistic equation, rates of\nreturn are slowed by past growth, and if rates of return approach zero, such\n\"slowing down\" makes civilization fragile with respect to externally imposed\nnetwork decay. If past technological change has been especially rapid, then\ncivilization is particularly vulnerable to newly unfavorable conditions that\nmight force a switch into a mode of accelerating collapse.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3554v1"
    },
    {
        "title": "Non-Equivalent Beliefs and Subjective Equilibrium Bubbles",
        "authors": [
            "Martin Larsson"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper develops a dynamic equilibrium model where agents exhibit a strong\nform of belief heterogeneity: they disagree about zero probability events. It\nis shown that, somewhat surprisingly, equilibrium exists in this setting, and\nthat the disagreement about nullsets naturally leads to equilibrium asset\npricing bubbles. The bubbles are subjective in the sense that they are\nperceived by some but not necessarily all agents. In contrast to existing\nmodels, bubbles arise with no restrictions on trade beyond a standard solvency\nconstraint.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.5082v1"
    },
    {
        "title": "Factorising equity returns in an emerging market through exogenous\n  shocks and capital flows",
        "authors": [
            "Diane Wilcox",
            "Tim Gebbie"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  A technique from stochastic portfolio theory [Fernholz, 1998] is applied to\nanalyse equity returns of Small, Mid and Large cap portfolios in an emerging\nmarket through periods of growth and regional crises, up to the onset of the\nglobal financial crisis. In particular, we factorize portfolios in the South\nAfrican market in terms of distribution of capital, change of stock ranks in\nportfolios, and the effect due to dividends for the period Nov 1994 to May\n2007. We discuss the results in the context of broader economic thinking to\nconsider capital flows as risk factors, turning around more established\napproaches which use macroeconomic and socio-economic conditions to explain\nForeign Direct Investment (into the economy) and Net Portfolio Investment (into\nequity and bond markets).\n",
        "pdf_link": "http://arxiv.org/pdf/1306.5302v2"
    },
    {
        "title": "Asymmetric connectedness of stocks: How does bad and good volatility\n  spill over the U.S. stock market?",
        "authors": [
            "Jozef Barunik",
            "Evzen Kocenda",
            "Lukas Vacha"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Asymmetries in volatility spillovers are highly relevant to risk valuation\nand portfolio diversification strategies in financial markets. Yet, the large\nliterature studying information transmission mechanisms ignores the fact that\nbad and good volatility may spill over at different magnitudes. This paper\nfills this gap with two contributions. One, we suggest how to quantify\nasymmetries in volatility spillovers due to bad and good volatility. Two, using\nhigh frequency data covering most liquid U.S. stocks in seven sectors, we\nprovide ample evidence of the asymmetric connectedness of stocks. We\nuniversally reject the hypothesis of symmetric connectedness at the\ndisaggregate level but in contrast, we document the symmetric transmission of\ninformation in an aggregated portfolio. We show that bad and good volatility is\ntransmitted at different magnitudes in different sectors, and the asymmetries\nsizably change over time. While negative spillovers are often of substantial\nmagnitudes, they do not strictly dominate positive spillovers. We find that the\noverall intra-market connectedness of U.S. stocks increased substantially with\nthe increased uncertainty of stock market participants during the financial\ncrisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1221v2"
    },
    {
        "title": "Complexity, Chaos, and the Duffing-Oscillator Model: An Analysis of\n  Inventory Fluctuations in Markets",
        "authors": [
            "Varsha S. Kulkarni"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Apparently random financial fluctuations often exhibit varying levels of\ncomplexity, chaos. Given limited data, predictability of such time series\nbecomes hard to infer. While efficient methods of Lyapunov exponent computation\nare devised, knowledge about the process driving the dynamics greatly\nfacilitates the complexity analysis. This paper shows that quarterly inventory\nchanges of wheat in the global market, during 1974-2012, follow a nonlinear\ndeterministic process. Lyapunov exponents of these fluctuations are computed\nusing sliding time windows each of length 131 quarters. Weakly chaotic behavior\nalternates with non-chaotic behavior over the entire period of analysis. More\nimportantly, in this paper, a cubic dependence of price changes on inventory\nchanges leads to establishment of deterministic Duffing-Oscillator-Model(DOM)\nas a suitable candidate for examining inventory fluctuations of wheat. DOM\nrepresents the interaction of commodity production cycle with an external\nintervention in the market. Parameters obtained for shifting time zones by\nfitting the Fourier estimated time signals to DOM are able to generate\nresponses that reproduce the true chaotic nature exhibited by the empirical\nsignal at that time. Endowing the parameters with suitable meanings, one may\ninfer that temporary changes in speculation reflect the pattern of inventory\nvolatility that drives the transitions between chaotic and non-chaotic\nbehavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1616v1"
    },
    {
        "title": "Econophysics Research in India in the last two Decades",
        "authors": [
            "Asim Ghosh"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We discuss here researches on econophysics done from India in the last two\ndecades. The term `econophysics' was formally coined in India (Kolkata) in\n1995. Since then many research papers, books, reviews, etc. have been written\nby scientists. Many institutions are now involved in this research field and\nmany conferences are being organized there. In this article we give an account\n(of papers, books, reviews, papers in proceedings volumes etc.) of this\nresearch from India.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.2191v4"
    },
    {
        "title": "The Interrupted Power Law and The Size of Shadow Banking",
        "authors": [
            "Davide Fiaschi",
            "Imre Kondor",
            "Matteo Marsili",
            "Valerio Volpati"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Using public data (Forbes Global 2000) we show that the asset sizes for the\nlargest global firms follow a Pareto distribution in an intermediate range,\nthat is ``interrupted'' by a sharp cut-off in its upper tail, where it is\ntotally dominated by financial firms. This flattening of the distribution\ncontrasts with a large body of empirical literature which finds a Pareto\ndistribution for firm sizes both across countries and over time. Pareto\ndistributions are generally traced back to a mechanism of proportional random\ngrowth, based on a regime of constant returns to scale. This makes our findings\nof an ``interrupted'' Pareto distribution all the more puzzling, because we\nprovide evidence that financial firms in our sample should operate in such a\nregime. We claim that the missing mass from the upper tail of the asset size\ndistribution is a consequence of shadow banking activity and that it provides\nan (upper) estimate of the size of the shadow banking system. This estimate --\nwhich we propose as a shadow banking index -- compares well with estimates of\nthe Financial Stability Board until 2009, but it shows a sharper rise in shadow\nbanking activity after 2010. Finally, we propose a proportional random growth\nmodel that reproduces the observed distribution, thereby providing a\nquantitative estimate of the intensity of shadow banking activity.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.2130v4"
    },
    {
        "title": "Reducing Financial Avalanches By Random Investments",
        "authors": [
            "Alessio Emanuele Biondo",
            "Alessandro Pluchino",
            "Andrea Rapisarda",
            "Dirk Helbing"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Building on similarities between earthquakes and extreme financial events, we\nuse a self-organized criticality-generating model to study herding and\navalanche dynamics in financial markets. We consider a community of interacting\ninvestors, distributed on a small-world network, who bet on the bullish\n(increasing) or bearish (decreasing) behavior of the market which has been\nspecified according to the S&P500 historical time series. Remarkably, we find\nthat the size of herding-related avalanches in the community can be strongly\nreduced by the presence of a relatively small percentage of traders, randomly\ndistributed inside the network, who adopt a random investment strategy. Our\nfindings suggest a promising strategy to limit the size of financial bubbles\nand crashes. We also obtain that the resulting wealth distribution of all\ntraders corresponds to the well-known Pareto power law, while the one of random\ntraders is exponential. In other words, for technical traders, the risk of\nlosses is much greater than the probability of gains compared to those of\nrandom traders.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.3639v2"
    },
    {
        "title": "Corporations and Regulators: The Game of Influence in Regulatory Capture",
        "authors": [
            "Dominic K. Albino",
            "Anzi Hu",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In a market system, regulations are designed to prevent or rectify market\nfailures that inhibit fair exchange, such as monopoly or transactions with\nhidden costs. Because regulations reduce profits to those possessing unfair\nadvantage, these advantaged corporations (whether individuals, companies, or\nother collective organizations) are motivated to influence regulators.\nRegulatory bodies created to protect the market are instead co-opted to advance\nthe interests of the corporations they are charged to regulate. This\nwide-spread influence, known as \"regulatory capture,\" has been recognized for\nover 100 years, and according to expectations of rational behavior, will exist\nwherever it is in the mutual self-interest of corporations and regulators. Here\nwe model the interaction between corporations and regulators using a new game\ntheory framework explicitly accounting for players' mutual influence, and\ndemonstrate the incentive for collusion. Communication between corporations and\nregulators enables them to collude and split the resulting profits. We identify\nwhen collusion is profitable for both parties. The intuitive results show that\ncapture occurs when the benefits to the corporation outweigh the costs to the\nregulator. Under these conditions, the corporation can compensate the regulator\nfor costs incurred and, further, provide a profit to both parties. In the real\nworld, benefits often far outweigh costs, providing large incentives to collude\nand making capture likely. Regulatory capture is inhibited by decreasing the\ninfluence between parties through strict separation, independent market\nknowledge and research by regulators, regulatory and market transparency,\nregulatory accountability for market failures, widely distributed regulatory\ncontrol, and anti-corruption enforcement.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.0057v1"
    },
    {
        "title": "Reciprocity as the foundation of Financial Economics",
        "authors": [
            "Timothy C. Johnson"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper argues that the fundamental principle of contemporary financial\neconomics is balanced reciprocity, not the principle of utility maximisation\nthat is important in economics more generally. The argument is developed by\nanalysing the mathematical Fundamental Theory of Asset Pricing with reference\nto the emergence of mathematical probability in the seventeenth century in the\ncontext of the ethical assessment of commercial contracts. This analysis is\nundertaken within a framework of Pragmatic philosophy and Virtue Ethics. The\npurpose of the paper is to mitigate future financial crises by reorienting\nfinancial economics to emphasise the objectives of market stability and social\ncohesion rather than individual utility maximisation.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.2798v1"
    },
    {
        "title": "Optimistic versus Pessimistic--Optimal Judgemental Bias with Reference\n  Point",
        "authors": [
            "Si Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper develops a model of reference-dependent assessment of subjective\nbeliefs in which loss-averse people optimally choose the expectation as the\nreference point to balance the current felicity from the optimistic\nanticipation and the future disappointment from the realisation. The choice of\nover-optimism or over-pessimism depends on the real chance of success and\noptimistic decision makers prefer receiving early information. In the portfolio\nchoice problem, pessimistic investors tend to trade conservatively, however,\nthey might trade aggressively if they are sophisticated enough to recognise the\nbiases since low expectation can reduce their fear of loss.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.2964v1"
    },
    {
        "title": "Double Cascade Model of Financial Crises",
        "authors": [
            "Thomas R. Hurd",
            "Davide Cellai",
            "Sergey Melnik",
            "Quentin Shao"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The scope of financial systemic risk research encompasses a wide range of\ninterbank channels and effects, including asset correlation shocks, default\ncontagion, illiquidity contagion, and asset fire sales. This paper introduces a\nfinancial network model that combines the default and liquidity stress\nmechanisms into a \"double cascade mapping\". The progress and eventual result of\nthe crisis is obtained by iterating this mapping to its fixed point. Unlike\nsimpler models, this model can therefore quantify how illiquidity or default of\none bank influences the overall level of liquidity stress and default in the\nsystem. Large-network asymptotic cascade mapping formulas are derived that can\nbe used for efficient network computations of the double cascade. Numerical\nexperiments then demonstrate that these asymptotic formulas agree qualitatively\nwith Monte Carlo results for large finite networks, and quantitatively except\nwhen the initial system is placed in an exceptional \"knife-edge\" configuration.\nThe experiments clearly support the main conclusion that when banks respond to\nliquidity stress by hoarding liquidity, then in the absence of asset fire\nsales, the level of defaults in a financial network is negatively related to\nthe strength of bank liquidity hoarding and the eventual level of stress in the\nnetwork.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.6873v4"
    },
    {
        "title": "The stochastic field of aggregate utilities and its saddle conjugate",
        "authors": [
            "Peter Bank",
            "Dmitry Kramkov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We describe the sample paths of the stochastic field $F = F_t(v,x,q)$ of\naggregate utilities parameterized by Pareto weights $v$ and total cash amounts\n$x$ and stocks' quantities $q$ in an economy. We also describe the sample paths\nof the stochastic field $G = G_t(u,y,q)$, which is conjugate to $F$ with\nrespect to the saddle arguments $(v,x)$, and obtain various conjugacy relations\nbetween these stochastic fields. The results of this paper play a key role in\nour study of a continuous-time price impact model.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.7280v1"
    },
    {
        "title": "On Agents and Equilibria",
        "authors": [
            "Ted Theodosopoulos"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This essay discusses the advantages of a probabilistic agent-based approach\nto questions in theoretical economics, from the nature of economic agents, to\nthe nature of the equilibria supported by their interactions. One idea we\npropose is that \"agents\" are meta-individual, hierarchically structured\nobjects, that include as irreducible components groupings of different\ndimensions. We also explore the effects of non-ergodicity, by constructing a\nsimple stochastic model for the contingent nature of economic interactions.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.0414v1"
    },
    {
        "title": "On the implicit interest rate in the Yunus equation",
        "authors": [
            "Marc Diener",
            "Pheakdei Mauk"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  In his book with Alan Jolis, Vers un monde sans pauvret\\'e (1997) Yunus gives\nthe example of a microcredit loan of 1000BDT reimbursed via 50 weekly\nsettlements of 22BDT and correctly claims that this corresponds to the annual\ninterest rate of 20%. But this is without taking into account that if the\nborrower has good reasons not to pay at one installment, she can postpone of\none week all remaining settlements, under the same conditions, so without extra\ncost. This of course leads to a lower implicit interest rate. Introducing a\nsimple geometric law model for the time between settlements, this turns the\nimplicit interest rate into a random variable, whose laws is still unknown but\nfor which we provide simulated empirical distribution density function. De ning\nby actuarial expected rate the real number r that satis es the expectation of\nthe random Yunus equation, we compute this number as a function of the\nprobability p of in-time installment. This allows in turn to compute the\nimplicit probability p which is to the value of p corresponding to the observed\n3% default rate, where in practice, \\default\" means \\more than four weeks\ndelay\". The mathematical tool used is the probability generating function, the\ncomputer tool is the Scilab algebraic equation solver.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2179v1"
    },
    {
        "title": "Minsky Financial Instability, Interscale Feedback, Percolation and\n  Marshall-Walras Disequilibrium",
        "authors": [
            "Sorin Solomon",
            "Natasa Golo"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We study analytically and numerically Minsky instability as a combination of\ntop-down, bottom-up and peer-to-peer positive feedback loops. The peer-to-peer\ninteractions are represented by the links of a network formed by the\nconnections between firms, contagion leading to avalanches and percolation\nphase transitions propagating across these links. The global parameter in the\ntop-bottom, bottom-up feedback loop is the interest rate. Before the Minsky\nmoment, in the Minsky Loans Accelerator stage, the relevant bottom parameter\nrepresenting the individual firms micro-states is the quantity of loans. After\nthe Minsky moment, in the Minsky Crisis Accelerator stage, the relevant bottom\nparameters are the number of ponzi units / quantity of failures, defaults. We\nrepresent the top-bottom, bottom-up interactions on a plot similar to the\nMarshal-Walras diagram for quantity-price market equilibrium (where the\ninterest rate is the analog of the price). The Minsky instability is then\nsimply emerging as a consequence of the fixed point (the intersection of the\nsupply and demand curves) being unstable (repulsive). In the presence of\nnetwork effects, one obtains more than one fixed point and a few dynamic\nregimes (phases). We describe them and their implications for understanding,\npredicting and steering economic instability.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.0176v1"
    },
    {
        "title": "Stock portfolio structure of individual investors infers future trading\n  behavior",
        "authors": [
            "Ludvig Bohlin",
            "Martin Rosvall"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Although the understanding of and motivation behind individual trading\nbehavior is an important puzzle in finance, little is known about the\nconnection between an investor's portfolio structure and her trading behavior\nin practice. In this paper, we investigate the relation between what stocks\ninvestors hold, and what stocks they buy, and show that investors with similar\nportfolio structures to a great extent trade in a similar way. With data from\nthe central register of shareholdings in Sweden, we model the market in a\nsimilarity network, by considering investors as nodes, connected with links\nrepresenting portfolio similarity. From the network, we find groups of\ninvestors that not only identify different investment strategies, but also\nrepresent groups of individual investors trading in a similar way. These\nfindings suggest that the stock portfolios of investors hold meaningful\ninformation, which could be used to earn a better understanding of stock market\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.2494v2"
    },
    {
        "title": "Reconstructing the world trade multiplex: the role of intensive and\n  extensive biases",
        "authors": [
            "Rossana Mastrandrea",
            "Tiziano Squartini",
            "Giorgio Fagiolo",
            "Diego Garlaschelli"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  In economic and financial networks, the strength of each node has always an\nimportant economic meaning, such as the size of supply and demand, import and\nexport, or financial exposure. Constructing null models of networks matching\nthe observed strengths of all nodes is crucial in order to either detect\ninteresting deviations of an empirical network from economically meaningful\nbenchmarks or reconstruct the most likely structure of an economic network when\nthe latter is unknown. However, several studies have proved that real economic\nnetworks and multiplexes are topologically very different from configurations\ninferred only from node strengths. Here we provide a detailed analysis of the\nWorld Trade Multiplex by comparing it to an enhanced null model that\nsimultaneously reproduces the strength and the degree of each node. We study\nseveral temporal snapshots and almost one hundred layers (commodity classes) of\nthe multiplex and find that the observed properties are systematically well\nreproduced by our model. Our formalism allows us to introduce the (static)\nconcept of extensive and intensive bias, defined as a measurable tendency of\nthe network to prefer either the formation of extra links or the reinforcement\nof link weights, with respect to a reference case where only strengths are\nenforced. Our findings complement the existing economic literature on (dynamic)\nintensive and extensive trade margins. More in general, they show that\nreal-world multiplexes can be strongly shaped by layer-specific local\nconstraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.4171v3"
    },
    {
        "title": "Mapping systemic risk: critical degree and failures distribution in\n  financial networks",
        "authors": [
            "Matteo Smerlak",
            "Brady Stoll",
            "Agam Gupta",
            "James S. Magdanz"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The 2008 financial crisis illustrated the need for a thorough, functional\nunderstanding of systemic risk in strongly interconnected financial structures.\nDynamic processes on complex networks being intrinsically difficult, most\nrecent studies of this problem have relied on numerical simulations. Here we\nreport analytical results in a network model of interbank lending based on\ndirectly relevant financial parameters, such as interest rates and leverage\nratios. Using a mean-field approach, we obtain a closed-form formula for the\n\"critical degree\", viz. the number of creditors per bank below which an\nindividual shock can propagate throughout the network. We relate the failures\ndistribution (probability that a single shock induces $F$ failures) to the\ndegree distribution (probability that a bank has $k$ creditors), showing in\nparticular that the former is fat-tailed whenever the latter is. Our criterion\nfor the onset of contagion turns out to be isomorphic to the condition for\ncooperation to evolve on graphs and social networks, as recently formulated in\nevolutionary game theory. This remarkable connection supports recent calls for\na methodological rapprochement between finance and ecology.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.4783v2"
    },
    {
        "title": "Stationarity, non-stationarity and early warning signals in economic\n  networks",
        "authors": [
            "Tiziano Squartini",
            "Diego Garlaschelli"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Economic integration, globalization and financial crises represent examples\nof processes whose understanding requires the analysis of the underlying\nnetwork structure. Of particular interest is establishing whether a real\neconomic network is in a state of (quasi)stationary equilibrium, i.e.\ncharacterized by smooth structural changes rather than abrupt transitions.\nWhile in the former case the behaviour of the system can be reasonably\ncontrolled and predicted, in the latter case this is generally impossible.\nHere, we propose a method to assess whether a real economic network is in a\nquasi-stationary state by checking the consistency of its structural evolution\nwith appropriate quasi-equilibrium maximum-entropy ensembles of graphs. As\nillustrative examples, we consider the International Trade Network (ITN) and\nthe Dutch Interbank Network (DIN). We find that the ITN is an almost perfect\nexample of quasi-equilibrium network, while the DIN is clearly\nout-of-equilibrium. In the latter, the entity of the deviation from\nquasi-stationarity contains precious information that allows us to identify\nremarkable early warning signals of the interbank crisis of 2008. These early\nwarning signals involve certain dyadic and triadic topological properties,\nincluding dangerous 'debt loops' with different levels of interbank\nreciprocity.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.4460v2"
    },
    {
        "title": "Modelling Returns and Volatilities During Financial Crises: a Time\n  Varying Coefficient Approach",
        "authors": [
            "Menelaos Karanasos",
            "Alexandros Paraskevopoulos",
            "Faek Menla Ali",
            "Michail Karoglou",
            "Stavroula Yfanti"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We examine how the most prevalent stochastic properties of key financial time\nseries have been affected during the recent financial crises. In particular we\nfocus on changes associated with the remarkable economic events of the last two\ndecades in the mean and volatility dynamics, including the underlying\nvolatility persistence and volatility spillovers structure. Using daily data\nfrom several key stock market indices we find that stock market returns exhibit\ntime varying persistence in their corresponding conditional variances.\nFurthermore, the results of our bivariate GARCH models show the existence of\ntime varying correlations as well as time varying shock and volatility\nspillovers between the returns of FTSE and DAX, and those of NIKKEI and Hang\nSeng, which became more prominent during the recent financial crisis. Our\ntheoretical considerations on the time varying model which provides the\nplatform upon which we integrate our multifaceted empirical approaches are also\nof independent interest. In particular, we provide the general solution for low\norder time varying specifications, which is a long standing research topic.\nThis enables us to characterize these models by deriving, first, their\nmultistep ahead predictors, second, the first two time varying unconditional\nmoments, and third, their covariance structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.7179v1"
    },
    {
        "title": "Opinion Dynamics and Price Formation: a Nonlinear Network Model",
        "authors": [
            "Marco D'Errico",
            "Gulnur Muradoglu",
            "Silvana Stefani",
            "Giovanni Zambruno"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Opinions and beliefs determine the evolution of social systems. This is of\nparticular interest in finance, as the increasing complexity of financial\nsystems is coupled with information overload. Opinion formation, therefore, is\nnot always the result of optimal information processing. On the contrary,\nagents are boundedly rational and naturally tend to observe and imitate others\nin order to gain further insights. Hence, a certain degree of interaction,\nwhich can be envisioned as a network, occurs within the system. Opinions, the\ninteraction network and prices in financial markets are then heavily\nintertwined and influence one another. We build on previous contributions on\nadaptive systems, where agents have hetereogenous beliefs, and introduce a\ndynamic confidence network that captures the interaction and shapes the opinion\npatterns. The analytical framework we adopt for modeling the interaction is\nrooted in the opinion dynamics problem. This will allow us to introduce a\nnonlinear model where the confidence network, opinion dynamics and price\nformation coevolve in time. A key aspect of the model is the classification of\nagents according to their topological role in the network, therefore showing\nthat topology matters in determining how of opinions and prices will coevolve.\nWe illustrate the dynamics via simulations, discussing the stylized facts in\nfinance that the model is able to capture. Last, we propose an empirical\nvalidation and calibration scheme that makes use of social network data.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.0308v1"
    },
    {
        "title": "Ranking the Economic Importance of Countries and Industries",
        "authors": [
            "Wei Li",
            "Dror Y. Kenett",
            "Kazuko Yamasaki",
            "H. Eugene Stanley",
            "Shlomo Havlin"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  In the current era of worldwide stock market interdependencies, the global\nfinancial village has become increasingly vulnerable to systemic collapse. The\nrecent global financial crisis has highlighted the necessity of understanding\nand quantifying interdependencies among the world's economies, developing new\neffective approaches to risk evaluation, and providing mitigating solutions. We\npresent a methodological framework for quantifying interdependencies in the\nglobal market and for evaluating risk levels in the world-wide financial\nnetwork. The resulting information will enable policy and decision makers to\nbetter measure, understand, and maintain financial stability. We use the\nmethodology to rank the economic importance of each industry and country\naccording to the global damage that would result from their failure. Our\nquantitative results shed new light on China's increasing economic dominance\nover other economies, including that of the USA, to the global economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.0443v1"
    },
    {
        "title": "A simple model of local prices and associated risk evaluation",
        "authors": [
            "Krzysztof Urbanowicz",
            "Peter Richmond",
            "Janusz A. Hołyst"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  A simple spin system is constructed to simulate dynamics of asset prices and\nstudied numerically. The outcome for the distribution of prices is shown to\ndepend both on the dimension of the system and the introduction of price into\nthe link measure. For dimensions below 2, the associated risk is high and the\nprice distribution is bimodal. For higher dimensions, the price distribution is\nGaussian and the associated risk is much lower. It is suggested that the\nresults are relevant to rare assets or situations where few players are\ninvolved in the deal making process.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.1352v1"
    },
    {
        "title": "Granger Causality Stock Market Networks: Temporal Proximity and\n  Preferential Attachment",
        "authors": [
            "Tomáš Výrost",
            "Štefan Lyócsa",
            "Eduard Baumöhl"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The structure of return spillovers is examined by constructing Granger\ncausality networks using daily closing prices of 20 developed markets from 2nd\nJanuary 2006 to 31st December 2013. The data is properly aligned to take into\naccount non-synchronous trading effects. The study of the resulting networks of\nover 94 sub-samples revealed three significant findings. First, after the\nrecent financial crisis the impact of the US stock market has declined. Second,\nspatial probit models confirmed the role of the temporal proximity between\nmarket closing times for return spillovers, i.e. the time distance between\nnational stock markets matters. Third, preferential attachment between stock\nmarkets exists, i.e. spillover from market j to market i is more likely if A)\nmarket j influences other markets other than i, or when B) market i is\ninfluenced by other markets other than j.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.2985v1"
    },
    {
        "title": "Income Distribution in the European Union Versus in the United States",
        "authors": [
            "Maciej Jagielski",
            "Rafał Duczmal",
            "Ryszard Kutner"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We prove that the refined approach -- our extension of the Yakovenko et al.\nformalism -- is universal in the sense that it describes well both household\nincomes in the European Union and the individual incomes in the United States\nfor social classes of any income. This formalism allowed the study of the\nimpact of the recent world-wide financial crisis on the annual incomes of\ndifferent social classes. Hence, we indicate the existence of a possible\nprecursor of a market crisis. Besides, we find the most painful impact of the\ncrisis on incomes of all social classes.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1560v1"
    },
    {
        "title": "An $H$ theorem for Boltzmann's equation for the Yard-Sale Model of asset\n  exchange",
        "authors": [
            "Bruce M. Boghosian",
            "Merek Johnson",
            "Jeremy Marcq"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  In recent work, Boltzmann and Fokker-Planck equations were derived for the\n\"Yard-Sale Model\" of asset exchange. For the version of the model without\nredistribution, it was conjectured, based on numerical evidence, that the\ntime-asymptotic state of the model was oligarchy -- complete concentration of\nwealth by a single individual. In this work, we prove that conjecture by\ndemonstrating that the Gini coefficient, a measure of inequality commonly used\nby economists, is an $H$ function of both the Boltzmann and Fokker-Planck\nequations for the model.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7227v1"
    },
    {
        "title": "Inflation and speculation in a dynamic macroeconomic model",
        "authors": [
            "Matheus Grasselli",
            "Adrien Nguyen Huu"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We study a monetary version of the Keen model by merging two alternative\nextensions, namely the addition of a dynamic price level and the introduction\nof speculation. We recall and study old and new equilibria, together with their\nlocal stability analysis. This includes a state of recession associated with a\ndeflationary regime and characterized by falling employment but constant wage\nshares, with or without an accompanying debt crisis. We also emphasize some new\nqualitative behavior of the extended model, in particular its ability to\nproduce and describe repeated financial crises as a natural pace of the\neconomy, and its suitability to describe the relationship between economic\ngrowth and financial activities.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7500v1"
    },
    {
        "title": "Optimal Digital Product Maintenance with a Continuous Revenue Stream",
        "authors": [
            "James Fan",
            "Christopher Griffin"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We use a control framework to analyze the digital vendor's profit\nmaximization problem. The vendor captures market share by focusing costly\neffort on post-launch product maintenance, which influences user perception of\nthe product and drives a revenue stream associated with product use. Our\ntheoretical results show necessary and sufficient conditions for product\nmaintenance to decline over a product's life-cycle, thus showing conditions\nwhen Lehman's 7th law of software evolution holds. We also numerically\nillustrate control paths under different market conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.8624v2"
    },
    {
        "title": "On the multiplicative effect of government spending (or any other\n  spending for that matter)",
        "authors": [
            "João P. da Cruz"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  There is, among the economist ecosystem, the idea of virtuous public spending\nas a form of promotion of economic growth. If we think on the way GDP is\nmeasured, it is not possible to get that conclusion because it becomes\ncircular: measuring the money flow obviously will detect directly the public\nspending but always mixed with the flow of money from other sources. The\nquestion is how virtuous is public spending per se? Can it promote economic\ngrowth? Is there multiplicative effect in GDP bigger than 1? In this paper, we\nmake use of the first principles of Economics to show that government spending\nis, at the most, as virtuous as private consumption and can be a source of\neconomic depression and inequality if it is not restricted to fundamental\nservices.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00808v1"
    },
    {
        "title": "How volatilities nonlocal in time affect the price dynamics in complex\n  financial systems",
        "authors": [
            "Lei Tan",
            "Bo Zheng",
            "Jun-Jie Chen",
            "Xiong-Fei Jiang"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  What is the dominating mechanism of the price dynamics in financial systems\nis of great interest to scientists. The problem whether and how volatilities\naffect the price movement draws much attention. Although many efforts have been\nmade, it remains challenging. Physicists usually apply the concepts and methods\nin statistical physics, such as temporal correlation functions, to study\nfinancial dynamics. However, the usual volatility-return correlation function,\nwhich is local in time, typically fluctuates around zero. Here we construct\ndynamic observables nonlocal in time to explore the volatility-return\ncorrelation, based on the empirical data of hundreds of individual stocks and\n25 stock market indices in different countries. Strikingly, the correlation is\ndiscovered to be non-zero, with an amplitude of a few percent and a duration of\nover two weeks. This result provides compelling evidence that past volatilities\nnonlocal in time affect future returns. Further, we introduce an agent-based\nmodel with a novel mechanism, that is, the asymmetric trading preference in\nvolatile and stable markets, to understand the microscopic origin of the\nvolatility-return correlation nonlocal in time.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00824v1"
    },
    {
        "title": "Estimation of Several Political Action Effects of Energy Prices",
        "authors": [
            "Andrew B. Whitford"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  One important effect of price shocks in the United States has been increased\npolitical attention paid to the structure and performance of oil and natural\ngas markets, along with some governmental support for energy conservation. This\npaper describes how price changes helped lead the emergence of a political\nagenda accompanied by several interventions, as revealed through Granger\ncausality tests on change in the legislative agenda.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.07265v1"
    },
    {
        "title": "Forest Fire Model as a Supercritical Dynamic Model in Financial Systems",
        "authors": [
            "Deokjae Lee",
            "Jae-Young Kim",
            "Jeho Lee",
            "B. Kahng"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Recently, large-scale cascading failures in complex systems have garnered\nsubstantial attention. Such extreme events have been treated as an integral\npart of the self-organized criticality (SOC). Recent empirical work has\nsuggested that some extreme events systematically deviate from the SOC\nparadigm, requiring a different theoretical framework. We shed additional\ntheoretical light on this possibility by studying financial crisis. We build\nour model of financial crisis on the well-known forest fire model in scale-free\nnetworks. Our analysis shows a non-trivial scaling feature indicating\nsupercritical behavior, which is independent of system size. Extreme events in\nthe supercritical state result from bursting of a fat bubble, seeds of which\nare sown by a protracted period of a benign financial environment with few\nshocks. Our findings suggest that policymakers can control the magnitude of\nfinancial meltdowns by keeping the economy operating within reasonable duration\nof a benign environment.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.04841v1"
    },
    {
        "title": "The intensity of the random variable intercept in the sector of negative\n  probabilities",
        "authors": [
            "Marcin Makowski",
            "Edward W. Piotrowski",
            "Jan Sładkowski",
            "Jacek Syska"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We consider properties of the measurement intensity $\\rho$ of a random\nvariable for which the probability density function represented by the\ncorresponding Wigner function attains negative values on a part of the domain.\nWe consider a simple economic interpretation of this problem. This model is\nused to present the applicability of the method to the analysis of the negative\nprobability on markets where there are anomalies in the law of supply and\ndemand (e.g. Giffen's goods). It turns out that the new conditions to optimize\nthe intensity $\\rho$ require a new strategy. We propose a strategy (so-called\n$\\grave{a}$ rebours strategy) based on the fixed point method and explore its\neffectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.07495v1"
    },
    {
        "title": "East africa in the Malthusian trap? A statistical analysis of financial,\n  economic, and demographic indicators",
        "authors": [
            "Andrey Korotayev",
            "Julia Zinkina"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  A statistical analysis of financial, economic, and demographic indicators\nperformed by the authors demonstrates (1) that the main countries of East\nAfrica (Uganda, Kenya, and Tanzania) have not escaped the Malthusian Trap yet;\n(2) that this countries are not likely to follow the \"North African path\" and\nto achieve this escape before they achieve serious successes in their fertility\ntransition; (3) that East Africa is unlikely to achieve this escape if it does\nnot follow the \"Bangladeshi path\" and does not achieve really substantial\nfertility declines in the foreseeable future, which would imply the\nintroduction of compulsory universal secondary education, serious family\nplanning programs of the Rwandan type, and the rise of legal age of marriage\nwith parental consent. Such measures should of course be accompanied by the\nsubstantial increases in the agricultural labor productivity and the decline of\nthe percentage of population employed in agriculture.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08441v1"
    },
    {
        "title": "Agent-based model with multi-level herding for complex financial systems",
        "authors": [
            "Jun-Jie Chen",
            "Lei Tan",
            "Bo Zheng"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In complex financial systems, the sector structure and volatility clustering\nare respectively important features of the spatial and temporal correlations.\nHowever, the microscopic generation mechanism of the sector structure is not\nyet understood. Especially, how to produce these two features in one model\nremains challenging. We introduce a novel interaction mechanism, i.e., the\nmulti-level herding, in constructing an agent-based model to investigate the\nsector structure combined with volatility clustering. According to the previous\nmarket performance, agents trade in groups, and their herding behavior\ncomprises the herding at stock, sector and market levels. Further, we propose\nmethods to determine the key model parameters from historical market data,\nrather than from statistical fitting of the results. From the simulation, we\nobtain the sector structure and volatility clustering, as well as the\neigenvalue distribution of the cross-correlation matrix, for the New York and\nHong Kong stock exchanges. These properties are in agreement with the empirical\nones. Our results quantitatively reveal that the multi-level herding is the\nmicroscopic generation mechanism of the sector structure, and provide new\ninsight into the spatio-temporal interactions in financial systems at the\nmicroscopic level.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.01811v1"
    },
    {
        "title": "The Equilibrium Statistical Model of Economic Systems using Concepts and\n  Theorems of Statistical Physics",
        "authors": [
            "Zhiwu Zheng"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Economic systems are similar with physic systems for their large number of\nindividuals and the exist of equilibrium. In this paper, we present a model\napplying the equilibrium statistical model in economic systems. Consistent with\nstatistical physics, we define a series of concepts, such as economic\ntemperature, economic pressure, economic potential, wealth and population.\nMoreover, we suggest that these parameters show pretty close relationship with\nthe concepts in economy. This paper presents related concepts in the\nequilibrium economic model and constructs significant theorems and corollaries,\nwhich is derived from the priori possibility postulate, getting theorems\nincluding the equilibrium theorem between open systems, the distribution\ntheorem of wealth and population along with related corollaries. More\nimportantly, we demonstrate a method constructing optimal density of states,\noptimizing a macroscopic parameter depending on need to get the distribution of\ndensity of states utilizing the variation method, which is significant for the\ndevelopment of a society. In addition, we calculate a simple economic system as\nan example, indicating that the system occupied mostly by the middle class\ncould develop stably and soundly, explaining the reason for resulting\ndistributions of macroscopic parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.04102v1"
    },
    {
        "title": "Mathematical modeling of physical capital using the spatial Solow model",
        "authors": [
            "Gilberto González-Parra",
            "Benito Chen-Charpentier",
            "Abraham J. Arenas",
            "Miguel Diaz-Rodriguez"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  This research deals with the mathematical modeling of the physical capital\ndiffusion through the borders of the countries. The physical capital is\nconsidered an important variable for the economic growth of a country. Here we\nuse an extension of the economic Solow model to describe how the smuggling\naffects the economic growth of the countries. In this study we rely on a\nproduction function that is non-concave instead of the classical Cobb-Douglas\nproduction function. In order to model the physical capital diffusion through\nthe borders of the country, we developed a model based on a parabolic partial\ndifferential equation that describes the dynamics of physical capital and\nboundary conditions of Neumann type. Smuggling is present in many borders\nbetween countries and may include fuel, machinery and food. This smuggling\nthrough the borders is a problematic issue for the country's economies. The\nsmuggling problem usually is related mainly to a non-official exchange rate\nthat is different than the official rate or subsides. Numerical simulations are\nobtained using an explicit finite difference scheme that shows how the physical\ncapital diffusion through the border of the countries. The study of physical\ncapital is a paramount issue for the economic growth of many countries for the\nnext years. The results show that the dynamics of the physical capital when\nboundary conditions of Neumann type are different than zero differ from the\nclassical economic behavior observed in the classical spatial Solow model\nwithout physical capital flux through the borders of countries. Finally, it can\nbe concluded that avoiding the smuggling through the frontiers is an important\nfactor that affects the economic growth of the countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.04388v1"
    },
    {
        "title": "Kinetic models of immediate exchange",
        "authors": [
            "Els Heinsalu",
            "Marco Patriarca"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We propose a novel kinetic exchange model differing from previous ones in two\nmain aspects. First, the basic dynamics is modified in order to represent\neconomies where immediate wealth exchanges are carried out, instead of\nreshufflings or uni-directional movements of wealth. Such dynamics produces\nwealth distributions that describe more faithfully real data at small values of\nwealth. Secondly, a general probabilistic trading criterion is introduced, so\nthat two economic units can decide independently whether to trade or not\ndepending on their profit. It is found that the type of the equilibrium wealth\ndistribution is the same for a large class of trading criteria formulated in a\nsymmetrical way with respect to the two interacting units. This establishes\nunexpected links between and provides a microscopic foundations of various\nkinetic exchange models in which the existence of a saving propensity is\npostulated. We also study the generalized heterogeneous version of the model in\nwhich units use different trading criteria and show that suitable sets of\ndiversified parameter values with a moderate level of heterogeneity can\nreproduce realistic wealth distributions with a Pareto power law.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01274v1"
    },
    {
        "title": "Jarzynski-type equalities in gambling: role of information in capital\n  growth",
        "authors": [
            "Yuji Hirono",
            "Yoshimasa Hidaka"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We study the capital growth in gambling with (and without) side information\nand memory effects. We derive several equalities for gambling, which are of\nsimilar form to the Jarzynski equality and its extension to systems with\nfeedback controls. Those relations provide us with new measures to quantify the\neffects of information on the statistics of capital growth in gambling. We\ndiscuss the implications of the equalities and show that they reproduce the\nknown upper bounds of average capital growth rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.06216v2"
    },
    {
        "title": "Violation of Invariance of Measurement for GDP Growth Rate and its\n  Consequences",
        "authors": [
            "Ali Hosseiny"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The aim here is to address the origins of sustainability for the real growth\nrate in the United States. For over a century of observations on the real GDP\nper capita of the United States a sustainable two percent growth rate has been\nobserved. To find an explanation for this observation I consider the impact of\nutility preferences and the effect of mobility of labor \\& capital on every\nprovided measurement. Mobility of labor results in heterogenous rates of\nincrease in prices which is called Baumol's cost disease phenomenon.\nHeterogeneous rates of inflation then make it impossible to define an invariant\nmeasure for the real growth rate. Paradoxical and ambiguous results already\nhave been observed when different measurements provided by the World Bank have\nbeen compared with the ones from the systems of national accounts (SNA). Such\nambiguity is currently being discussed in economy. I define a toy model for\ncaring out measurements in order to state that this ambiguity can be very\nsignificant. I provide examples in which GDP expands 5 folds while measurements\npercept an expansion around 2 folds. Violation of invariance of the\nmeasurements leads to state that it is hard to compare the growth rate of GDP\nfor a smooth growing country such as the U.S. with a fast growing country such\nas China. Besides, I state that to extrapolate the time that economy of China\npasses the economy of the US we need to consider local metric of the central\nbanks of both countries. Finally I conclude that it is our method of\nmeasurements that leads us to percept the sustainable growth rate.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04848v2"
    },
    {
        "title": "Quantile Correlations: Uncovering temporal dependencies in financial\n  time series",
        "authors": [
            "Thilo A. Schmitt",
            "Rudi Schäfer",
            "Holger Dette",
            "Thomas Guhr"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We conduct an empirical study using the quantile-based correlation function\nto uncover the temporal dependencies in financial time series. The study uses\nintraday data for the S\\&P 500 stocks from the New York Stock Exchange. After\nestablishing an empirical overview we compare the quantile-based correlation\nfunction to stochastic processes from the GARCH family and find striking\ndifferences. This motivates us to propose the quantile-based correlation\nfunction as a powerful tool to assess the agreements between stochastic\nprocesses and empirical data.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04990v1"
    },
    {
        "title": "Stochastic model of financial markets reproducing scaling and memory in\n  volatility return intervals",
        "authors": [
            "Vygintas Gontis",
            "Shlomo Havlin",
            "Aleksejus Kononovicius",
            "Boris Podobnik",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We investigate the volatility return intervals in the NYSE and FOREX markets.\nWe explain previous empirical findings using a model based on the interacting\nagent hypothesis instead of the widely-used efficient market hypothesis. We\nderive macroscopic equations based on the microscopic herding interactions of\nagents and find that they are able to reproduce various stylized facts of\ndifferent markets and different assets with the same set of model parameters.\nWe show that the power-law properties and the scaling of return intervals and\nother financial variables have a similar origin and could be a result of a\ngeneral class of non-linear stochastic differential equations derived from a\nmaster equation of an agent system that is coupled by herding interactions.\nSpecifically, we find that this approach enables us to recover the volatility\nreturn interval statistics as well as volatility probability and spectral\ndensities for the NYSE and FOREX markets, for different assets, and for\ndifferent time-scales. We find also that the historical S\\&P500 monthly series\nexhibits the same volatility return interval properties recovered by our\nproposed model. Our statistical results suggest that human herding is so strong\nthat it persists even when other evolving fluctuations perturbate the financial\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05203v2"
    },
    {
        "title": "Dynamical system theory of periodically collapsing bubbles",
        "authors": [
            "V. I. Yukalov",
            "E. P. Yukalova",
            "D. Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We propose a reduced form set of two coupled continuous time equations\nlinking the price of a representative asset and the price of a bond, the later\nquantifying the cost of borrowing. The feedbacks between asset prices and bonds\nare mediated by the dependence of their \"fundamental values\" on past asset\nprices and bond themselves. The obtained nonlinear self-referencing price\ndynamics can induce, in a completely objective deterministic way, the\nappearance of periodically exploding bubbles ending in crashes. Technically,\nthe periodically explosive bubbles arise due to the proximity of two types of\nbifurcations as a function of the two key control parameters $b$ and $g$, which\nrepresent, respectively, the sensitivity of the fundamental asset price on past\nasset and bond prices and of the fundamental bond price on past asset prices.\nOne is a Hopf bifurcation, when a stable focus transforms into an unstable\nfocus and a limit cycle appears. The other is a rather unusual bifurcation,\nwhen a stable node and a saddle merge together and disappear, while an unstable\nfocus survives and a limit cycle develops. The lines, where the periodic\nbubbles arise, are analogous to the critical lines of phase transitions in\nstatistical physics. The amplitude of bubbles and waiting times between them\nrespectively diverge with the critical exponents $\\gamma = 1$ and $\\nu = 1/2$,\nas the critical lines are approached.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05311v1"
    },
    {
        "title": "Why is GDP growth linear?",
        "authors": [
            "Jörg D. Becker"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In many European countries the growth of the real GDP per capita has been\nlinear since 1950. An explanation for this linearity is still missing. We\npropose that in artificial intelligence we may find models for a linear growth\nof performance. We also discuss possible consequences of the fact that in\nsystems with linear growth the percentage growth goes to zero.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.04246v1"
    },
    {
        "title": "Forecasting stock market returns over multiple time horizons",
        "authors": [
            "Dimitri Kroujiline",
            "Maxim Gusev",
            "Dmitry Ushanov",
            "Sergey V. Sharov",
            "Boris Govorkov"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In this paper we seek to demonstrate the predictability of stock market\nreturns and explain the nature of this return predictability. To this end, we\nintroduce investors with different investment horizons into the news-driven,\nanalytic, agent-based market model developed in Gusev et al. (2015). This\nheterogeneous framework enables us to capture dynamics at multiple timescales,\nexpanding the model's applications and improving precision. We study the\nheterogeneous model theoretically and empirically to highlight essential\nmechanisms underlying certain market behaviors, such as transitions between\nbull- and bear markets and the self-similar behavior of price changes. Most\nimportantly, we apply this model to show that the stock market is nearly\nefficient on intraday timescales, adjusting quickly to incoming news, but\nbecomes inefficient on longer timescales, where news may have a long-lasting\nnonlinear impact on dynamics, attributable to a feedback mechanism acting over\nthese horizons. Then, using the model, we design algorithmic strategies that\nutilize news flow, quantified and measured, as the only input to trade on\nmarket return forecasts over multiple horizons, from days to months. The\nbacktested results suggest that the return is predictable to the extent that\nsuccessful trading strategies can be constructed to harness this\npredictability.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.04332v2"
    },
    {
        "title": "Theory of pricing as relativistic kinematics",
        "authors": [
            "S. I. Melnyk",
            "I. G. Tuluzov"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The algebra of transactions as fundamental measurements is constructed on the\nbasis of the analysis of their properties and represents an expansion of the\nBoolean algebra. The notion of the generalized economic measurements of the\neconomic quantity and quality of objects of transactions is introduced. It has\nbeen shown that the vector space of economic states constructed on the basis of\nthese measurements is relativistic. The laws of kinematics of economic objects\nin this space have been analyzed and the stages of constructing the dynamics\nhave been formulated. In particular, the principle of maximum benefit, which\nrepresents an economic analog of the principle of least action in the classical\nmechanics, and the principle of relativity as the principle of equality of all\npossible consumer preferences have been formulated. The notion of economic\ninterval between two economic objects invariant to the selection of the vector\nof consumer preferences has been introduced. Methods of experimental\nverification of the principle of relativity in the space of economic states\nhave been proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.06225v1"
    },
    {
        "title": "Assessing Consistency of Consumer Confidence Data using Dynamic Latent\n  Class Analysis",
        "authors": [
            "Sunil Kumar",
            "Zakir Husain",
            "Diganta Mukherjee"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In many countries information on expectations collected through consumer\nconfidence surveys are used in macroeconomic policy formulation. Unfortunately,\nbefore doing so, the consistency of responses is often not taken into account,\nleading to biases creeping in and affecting the reliability of the indices\nhence created. This paper describes how latent class analysis may be used to\ncheck the consistency of responses and ensure a parsimonious questionnaire. In\nparticular, we examine how temporal changes may be incorporated into the model.\nOur methodology is illustrated using three rounds of Consumer Confidence Survey\n(CCS) conducted by Reserve Bank of India (RBI).\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01215v1"
    },
    {
        "title": "On the emergence of scale-free production networks",
        "authors": [
            "Stanislao Gualdi",
            "Antoine Mandel"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We propose a simple dynamical model of the formation of production networks\namong monopolistically competitive firms. The model subsumes the standard\ngeneral equilibrium approach \\`a la Arrow-Debreu but displays a wide set of\npotential dynamic behaviors. It robustly reproduces key stylized facts of\nfirms' demographics. Our main result is that competition between intermediate\ngood producers generically leads to the emergence of scale-free production\nnetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01483v2"
    },
    {
        "title": "Solar energy production: Short-term forecasting and risk management",
        "authors": [
            "Cédric Join",
            "Michel Fliess",
            "Cyril Voyant",
            "Frédéric Chaxel"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Electricity production via solar energy is tackled via short-term forecasts\nand risk management. Our main tool is a new setting on time series. It allows\nthe definition of \"confidence bands\" where the Gaussian assumption, which is\nnot satisfied by our concrete data, may be abandoned. Those bands are quite\nconvenient and easily implementable. Numerous computer simulations are\npresented.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.06295v1"
    },
    {
        "title": "Escaping the trap of 'blocking': a kinetic model linking economic\n  development and political competition",
        "authors": [
            "Marina Dolfin",
            "Damián Knopoff",
            "Leone Leonida",
            "Dario Maimone Ansaldo Patti"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In this paper we present a kinetic model with stochastic game-type\ninteractions, analyzing the relationship between the level of political\ncompetition in a society and the degree of economic liberalization. The above\nissue regards the complex interactions between economy and institutional\npolicies intended to introduce technological innovations in a society, where\ntechnological innovations are intended in a broad sense comprehending reforms\ncritical to production. A special focus is placed on the political replacement\neffect described in a macroscopic model by Acemoglu and Robinson (AR-model,\nhenceforth), which can determine the phenomenon of innovation 'blocking',\npossibly leading to economic backwardness. One of the goals of our modelization\nis to obtain a mesoscopic dynamical model whose macroscopic outputs are\nqualitatively comparable with stylized facts of the AR-model. A set of\nnumerical solutions is presented showing the non monotonous relationship\nbetween economic liberization and political competition, which can be\nconsidered as an emergent phenomenon of the complex socio-economic interaction\ndynamic.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.08442v1"
    },
    {
        "title": "Market Integration in the Prewar Japanese Rice Markets",
        "authors": [
            "Mikio Ito",
            "Kiyotaka Maeda",
            "Akihiko Noda"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  This paper examines the integration process of the Japanese major rice\nmarkets (Tokyo and Osaka) from 1881 to 1932. Using a non-Bayesian time-varying\nvector error correction model, we argue that the process strongly depended on\nthe government's policy on the network system of the telegram and telephone;\nrice traders with an intention to use modern communication tools were usually\naffected by the changes in policy. We find that (i) the Japanese rice markets\nhad been integrated in the 1910s; (ii) increasing use of telegraphs had\naccelerated rice market integration from the Meiji period in Japan; and (iii)\nlocal telephone system, which reduced the time spent by urban users sending and\nreceiving telegrams, promoted market integration.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.00148v6"
    },
    {
        "title": "Controllability Analyses on Firm Networks Based on Comprehensive Data",
        "authors": [
            "Hiroyasu Inoue"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Since governments give stimulus to firms and expect the spillover effect by\nfiscal policies, it is important to know the effectiveness that they can\ncontrol the economy. To clarify the controllability of the economy, we\ninvestigate a firm production network observed exhaustively in Japan and what\nfirms should be directly or indirectly controlled by using control theory. By\ncontrol theory, we can classify firms into three different types: (a) firms\nthat should be directly controlled; (b) firms that should be indirectly\ncontrolled; (c) neither of them (ordinary). Since there is a direction\n(supplier and client) in the production network, we can consider controls of\ntwo different directions: demand and supply sides. As analyses results, we\nobtain the following results: (1) Each industry has diverse share of firms that\nshould be controlled directly or indirectly. The configurations of the shares\nin industries are different between demand- and supply-sides; (2) Advancement\nof industries, such like, primary industries or other advanced industries, does\nnot show apparent difference in controllability; (3) If we clip a network in\ndescending order of capital size, we do not lose the control effect for both\ndemand- and supply-sides.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01322v1"
    },
    {
        "title": "Market Imitation and Win-Stay Lose-Shift strategies emerge as unintended\n  patterns in market direction guesses",
        "authors": [
            "Mario Gutiérrez-Roig",
            "Carlota Segura",
            "Jordi Duch",
            "Josep Perelló"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Decisions taken in our everyday lives are based on a wide variety of\ninformation so it is generally very difficult to assess what are the strategies\nthat guide us. Stock market therefore provides a rich environment to study how\npeople take decision since responding to market uncertainty needs a constant\nupdate of these strategies. For this purpose, we run a lab-in-the-field\nexperiment where volunteers are given a controlled set of financial information\n-based on real data from worldwide financial indices- and they are required to\nguess whether the market price would go up or down in each situation. From the\ndata collected we explore basic statistical traits, behavioural biases and\nemerging strategies. In particular, we detect unintended patterns of behavior\nthrough consistent actions which can be interpreted as {\\it Market Imitation}\nand {\\it Win-Stay Lose-Shift} emerging strategies, being {\\it Market Imitation}\nthe most dominant one. We also observe that these strategies are affected by\nexternal factors: the expert advice, the lack of information or an information\noverload reinforce the use of these intuitive strategies, while the probability\nto follow them significantly decreases when subjects spends more time to take a\ndecision. The cohort analysis shows that women and children are more prone to\nuse such strategies although their performance is not undermined. Our results\nare of interest for better handling clients expectations of trading companies,\navoiding behavioural anomalies in financial analysts decisions and improving\nnot only the design of markets but also the trading digital interfaces where\ninformation is set down. Strategies and behavioural biases observed can also be\ntranslated into new agent based modelling or stochastic price dynamics to\nbetter understand financial bubbles or the effects of asymmetric risk\nperception to price drops.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01557v1"
    },
    {
        "title": "The Affine Wealth Model: An agent-based model of asset exchange that\n  allows for negative-wealth agents and its empirical validation",
        "authors": [
            "Jie Li",
            "Bruce M. Boghosian",
            "Chengli Li"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We present a stochastic, agent-based, binary-transaction Asset-Exchange Model\n(AEM) for wealth distribution that allows for agents with negative wealth. This\nmodel retains certain features of prior AEMs such as redistribution and\nwealth-attained advantage, but it also allows for shifts as well as scalings of\nthe agent density function. We derive the Fokker-Planck equation describing its\ntime evolution and we describe its numerical solution, including a methodology\nfor solving the inverse problem of finding the model parameters that best match\nempirical data. Using this methodology, we compare the steady-state solutions\nof the Fokker-Planck equation with data from the United States Survey of\nConsumer Finances over a time period of 27 years. In doing so, we demonstrate\nagreement with empirical data of an average error less than 0.16\\% over this\ntime period. We present the model parameters for the US wealth distribution\ndata as a function of time under the assumption that the distribution responds\nto their variation adiabatically. We argue that the time series of model\nparameters thus obtained provides a valuable new diagnostic tool for analyzing\nwealth inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.02370v2"
    },
    {
        "title": "Convergence of Economic Growth and the Great Recession as Seen From a\n  Celestial Observatory",
        "authors": [
            "Eamon Duede",
            "Victor Zhorin"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Macroeconomic theories of growth and wealth distribution have an outsized\ninfluence on national and international social and economic policies. Yet, due\nto a relative lack of reliable, system wide data, many such theories remain, at\nbest, unvalidated and, at worst, misleading. In this paper, we introduce a\nnovel economic observatory and framework enabling high resolution comparisons\nand assessments of the distributional impact of economic development through\nthe remote sensing of planet earth's surface. Striking visual and empirical\nvalidation is observed for a broad, global macroeconomic sigma-convergence in\nthe period immediately following the end of the Cold War. What is more, we\nobserve strong empirical evidence that the mechanisms driving sigma-convergence\nfailed immediately after the financial crisis and the start of the Great\nRecession. Nevertheless, analysis of both cross-country and cross-state samples\nindicates that, globally, disproportionately high growth levels and excessively\nhigh decay levels have become rarer over time. We also see that urban areas,\nespecially concentrated within short distances of major capital cities were\nmore likely than rural or suburban areas to see relatively high growth in the\naftermath of the financial crisis. Observed changes in growth polarity can be\nattributed plausibly to post-crisis government intervention and subsidy\npolicies introduced around the world. Overall, the data and techniques we\npresent here make economic evidence for the rise of China, the decline of U.S.\nmanufacturing, the euro crisis, the Arab Spring, and various, recent, Middle\nEast conflicts visually evident for the first time.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04312v2"
    },
    {
        "title": "Physicists' approach to studying socio-economic inequalities: Can humans\n  be modelled as atoms?",
        "authors": [
            "Kiran Sharma",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  A brief overview of the models and data analyses of income, wealth,\nconsumption distributions by the physicists, are presented here. It has been\nfound empirically that the distributions of income and wealth possess fairly\nrobust features, like the bulk of both the income and wealth distributions seem\nto reasonably fit both the log-normal and Gamma distributions, while the tail\nof the distribution fits well to a power law (as first observed by sociologist\nPareto). We also present our recent studies of the unit-level expenditure on\nconsumption across multiple countries and multiple years, where it was found\nthat there exist invariant features of consumption distribution: the bulk is\nlog-normally distributed, followed by a power law tail at the limit. The\nmechanisms leading to such inequalities and invariant features for the\ndistributions of socio-economic variables are not well-understood. We also\npresent some simple models from physics and demonstrate how they can be used to\nexplain some of these findings and their consequences.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.06051v2"
    },
    {
        "title": "Online shopping key features analysis in Mures county",
        "authors": [
            "Elena-Iulia Apăvăloaie",
            "Liviu Onoriu Marian",
            "Elena Lucia Harpa"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The aim of this paper is to get an overview of the online buyer profile, and\nalso some key aspects in the way the online shopping is conducted. In this\nproject we conducted a quantitative research, consisting of a questionnaire\nbased survey. For data processing and interpretation we used SPSS statistical\nsoftware and Excel. For data analysis, we used the descriptive statistics\nindicators, and a series of bi-varied analysis for testing some statistical\nassumptions. Viewed at first with skepticism by the Internet users in Romania,\nbecause of the many news about how dangerous the credit card payments are, the\nonline stores have gained much ground and trust in the recent years. Since the\nstudy was conducted mainly in the online environment, we can not talk about the\nrepresentativeness of the sample, only about a trend observed in the studied\npopulation. The study helps us understand the population reactions and\nattitudes regarding the online shopping. The study revealed some important\nissues regarding the online shopping in Mures county, issues that are described\nin detail in the content of this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.07706v1"
    },
    {
        "title": "Asymmetric volatility connectedness on forex markets",
        "authors": [
            "Jozef Barunik",
            "Evzen Kocenda",
            "Lukas Vacha"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We show how bad and good volatility propagate through forex markets, i.e., we\nprovide evidence for asymmetric volatility connectedness on forex markets.\nUsing high-frequency, intra-day data of the most actively traded currencies\nover 2007 - 2015 we document the dominating asymmetries in spillovers that are\ndue to bad rather than good volatility. We also show that negative spillovers\nare chiefly tied to the dragging sovereign debt crisis in Europe while positive\nspillovers are correlated with the subprime crisis, different monetary policies\namong key world central banks, and developments on commodities markets. It\nseems that a combination of monetary and real-economy events is behind the net\npositive asymmetries in volatility spillovers, while fiscal factors are linked\nwith net negative spillovers.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.08214v1"
    },
    {
        "title": "Role of Intensive and Extensive Variables in a Soup of Firms in Economy\n  to Address Long Run Prices and Aggregate Data",
        "authors": [
            "Ali Hosseiny",
            "Mauro Gallegati"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We review the production function and the hypothesis of equilibrium in the\nneoclassical framework. We notify that in a soup of sectors in economy, while\ncapital and labor resemble extensive variables, wage and rate of return on\ncapital act as intensive variables. As a result, Baumol and Bowen's statement\nof equal wages is inevitable from the thermodynamics point of view. We try to\nsee how aggregation can be performed concerning the extensive variables in a\nsoup of firms. We provide a toy model to perform aggregation for production and\nthe labor income as extensive quantities in a neoclassical framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.02523v2"
    },
    {
        "title": "Emergent organization in a model market",
        "authors": [
            "Avinash Chand Yadav",
            "Kaustubh Manchanda",
            "Ramakrishna Ramaswamy"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We study the collective behavior of interacting agents in a simple model of\nmarket economics originally introduced by N{\\o}rrelykke and Bak. A general\ntheoretical framework for interacting traders on an arbitrary network is\npresented, with the interaction consisting of buying (namely, consumption) and\nselling (namely, production) of commodities. Extremal dynamics is introduced by\nhaving the agent with least profit in the market readjust prices, causing the\nmarket to self--organize. We study this model market on regular lattices in\ntwo--dimension as well as on random complex networks; in the critical state\nfluctuations in an activity signal exhibit properties that are characteristic\nof avalanches observed in models of self-organized criticality, and these can\nbe described by power--law distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03521v1"
    },
    {
        "title": "Property bubble in Hong Kong: A predicted decade-long slump (2016-2025)",
        "authors": [
            "Peter Richmond",
            "Bertrand M. Roehner"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Between 2003 and 2015 the prices of apartments in Hong Kong (adjusted for\ninflation) increased by a factor of 3.8. This is much higher than in the United\nStates prior to the so-called subprime crisis of 2007. The analysis of this\nspeculative episode confirms the mechanism and regularities already highlighted\nby the present authors in similar episodes in other countries. Based on these\nregularities, it is possible to predict the price trajectory over the time\ninterval 2016-2025. It suggests that, unless appropriate relief is provided by\nthe mainland, Hong Kong will experience a decade-long slump. Possible\nimplications for its relations with Beijing are discussed at the end of the\npaper.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03985v1"
    },
    {
        "title": "Poverty Index With Time Varying Consumption and Income Distributions",
        "authors": [
            "Amit K Chattopadhyay",
            "T Krishna Kumar",
            "Sushanta K Mallick"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  In a recent work (Chattopadhyay, A. K. et al, Europhys. Lett. {\\bf 91},\n58003, 2010) based on food consumption statistics, we showed how a stochastic\nagent based model could represent the time variation of the income distribution\nstatistics in a developing economy, thereby defining an alternative\n\\enquote{poverty index} (PI) that largely agreed with poverty gap index data.\nThis PI used two variables, the probability density function of the income\nstatistics and a consumption deprivation (CD) function, representing the\nshortfall in the minimum consumption needed for survival. Since the time\ndependence of the CD function was introduced there through data extrapolation\nonly and not through an endogenous time dependent series, this model left\nunexplained how the minimum consumption needed for survival varies with time.\nThe present article overcomes these limitations and arrives at a new unified\ntheoretical structure through time varying consumption and income distributions\nwhere trade is only allowed when the income exceeds consumption deprivation\n(CD). Our results reveal that such CD-dynamics reduces the threshold level of\nconsumption of basic necessities, suggesting a possible dietary transition in\nterms of lower saturation level of food-grain consumption. The new poverty\nindex conforms to recently observed trends more closely than conventional\nmeasures of poverty and allows probabilistic prediction of PI for future times.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05650v1"
    },
    {
        "title": "The Growth of Oligarchy in a Yard-Sale Model of Asset Exchange: A\n  Logistic Equation for Wealth Condensation",
        "authors": [
            "Bruce M. Boghosian",
            "Adrian Devitt-Lee",
            "Hongyan Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The addition of wealth-attained advantage (WAA) to the Yard-Sale Model (YSM)\nof asset exchange has been demonstrated to induce wealth condensation. In a\nmodel of WAA for which the bias is a continuous function of the wealth\ndifference of the transacting agents, the condensation was shown to arise from\na second-order phase transition to a coexistence regime. In this paper, we\npresent the first analytic time-dependent results for this model, by showing\nthat the condensed wealth obeys a logistic equation in time.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05851v1"
    },
    {
        "title": "Discrete hierarchy of sizes and performances in the exchange-traded fund\n  universe",
        "authors": [
            "Benjamin Vandermarliere",
            "Jan Ryckebusch",
            "Koen Schoors",
            "Peter Cauwels",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Using detailed statistical analyses of the size distribution of a universe of\nequity exchange-traded funds (ETFs), we discover a discrete hierarchy of sizes,\nwhich imprints a log-periodic structure on the probability distribution of ETF\nsizes that dominates the details of the asymptotic tail. This allows us to\npropose a classification of the studied universe of ETFs into seven size layers\napproximately organized according to a multiplicative ratio of 3.5 in their\ntotal market capitalization. Introducing a similarity metric generalising the\nHerfindhal index, we find that the largest ETFs exhibit a significantly\nstronger intra-layer and inter-layer similarity compared with the smaller ETFs.\nComparing the performance across the seven discerned ETF size layers, we find\nan inverse size effect, namely large ETFs perform significantly better than the\nsmall ones both in 2014 and 2015.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.08582v2"
    },
    {
        "title": "Multivariate Garch with dynamic beta",
        "authors": [
            "Matthias Raddant",
            "Friedrich Wagner"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We investigate a solution for the problems related to the application of\nmultivariate GARCH models to markets with a large number of stocks by\nrestricting the form of the conditional covariance matrix. The model is a\nfactor model and uses only six free GARCH parameters. One factor can be\ninterpreted as the market component, the remaining factors are equal. This\nallow the analytical calculation of the inverse covariance matrix. The\ntime-dependence of the factors enables the determination of dynamical beta\ncoefficients. We compare the results from our model with the results of other\nGARCH models for the daily returns from the S\\&P500 market and find that they\nare competitive. As applications we use the daily values of beta coefficients\nto confirm a transition of the market in 2006. Furthermore we discuss the\nrelationship of our model with the leverage effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07051v6"
    },
    {
        "title": "Information inefficiency in a random linear economy model",
        "authors": [
            "Joao Pedro Jerico",
            "Renato Vicente"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We study the effects of introducing information inefficiency in a model for a\nrandom linear economy with a representative consumer. This is done by\nconsidering statistical, instead of classical, economic general equilibria.\nEmploying two different approaches we show that inefficiency increases the\nconsumption set of a consumer but decreases her expected utility. In this\nscenario economic activity grows while welfare shrinks, that is the opposite of\nthe behavior obtained by considering a rational consumer.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.01270v2"
    },
    {
        "title": "$κ$-generalized models of income and wealth distributions: A survey",
        "authors": [
            "F. Clementi",
            "M. Gallegati",
            "G. Kaniadakis",
            "S. Landini"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The paper provides a survey of results related to the \"$\\kappa$-generalized\ndistribution\", a statistical model for the size distribution of income and\nwealth. Topics include, among others, discussion of basic analytical\nproperties, interrelations with other statistical distributions as well as\naspects that are of special interest in the income distribution field, such as\nthe Gini index and the Lorenz curve. An extension of the basic model that is\nmost able to accommodate the special features of wealth data is also reviewed.\nThe survey of empirical applications given in this paper shows the\n$\\kappa$-generalized models of income and wealth to be in excellent agreement\nwith the observed data in many cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08676v1"
    },
    {
        "title": "Income and wealth distribution of the richest Norwegian individuals: An\n  inequality analysis",
        "authors": [
            "Maciej Jagielski",
            "Kordian Czyżewski",
            "Ryszard Kutner",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Using the empirical data from the Norwegian tax office, we analyse the wealth\nand income of the richest individuals in Norway during the period 2010--2013.\nWe find that both annual income and wealth level of the richest individuals are\ndescribable using the Pareto law. We find that the robust mean Pareto exponent\nover the four-year period to be $\\approx 2.3$ for income and $\\approx 1.5$ for\nwealth.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08918v1"
    },
    {
        "title": "The Indirect Effects of FDI on Trade: A Network Perspective",
        "authors": [
            "Paolo Sgrignoli",
            "Rodolfo Metulini",
            "Zhen Zhu",
            "Massimo Riccaboni"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  The relationship between international trade and foreign direct investment\n(FDI) is one of the main features of globalization. In this paper we\ninvestigate the effects of FDI on trade from a network perspective, since FDI\ntakes not only direct but also indirect channels from origin to destination\ncountries because of firms' incentive to reduce tax burden, to minimize\ncoordination costs, and to break barriers to market entry. We use a unique data\nset of international corporate control as a measure of stock FDI to construct a\ncorporate control network (CCN) where the nodes are the countries and the edges\nare the corporate control relationships. Based on the CCN, the network\nmeasures, i.e., the shortest path length and the communicability, are computed\nto capture the indirect channel of FDI. Empirically we find that corporate\ncontrol has a positive effect on trade both directly and indirectly. The result\nis robust with different specifications and estimation strategies. Hence, our\npaper provides strong empirical evidence of the indirect effects of FDI on\ntrade. Moreover, we identify a number of interplaying factors such as regional\ntrade agreements and the region of Asia. We also find that the indirect effects\nare more pronounced for manufacturing sectors than for primary sectors such as\noil extraction and agriculture.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.02187v1"
    },
    {
        "title": "Risk Constrained Trading Strategies for Stochastic Generation with a\n  Single-Price Balancing Market",
        "authors": [
            "Jethro Browell"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Due to the limited predictability of wind power and other stochastic\ngeneration, trading this energy in competitive electricity markets is\nchallenging. This paper derives revenue-maximising and risk-constrained\nstrategies for stochastic generators participating in electricity markets with\na single-price balancing mechanism. Starting from the optimal---and\nimpractical---strategy of offering zero or nominal power, which exposes the\nparticipant to potentially large imbalance costs, we develop a number of\nstrategies that control risk by hedging against penalising balancing prices in\nfavour of rewarding ones. Trading strategies are formulated in a probabilistic\nframework in order to address asymmetry in balancing prices. The large-scale\ncommunication of system information characteristic of modern power systems is\nutilised to inputs for electricity price forecasts and probabilistic system\nlength forecasts. A case study using data from the GB market in the UK is\npresented and the ability of the proposed strategies to increase revenue and\nreduce risk is demonstrated and analysed.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02625v1"
    },
    {
        "title": "Oil economy phase plot: a physical analogy",
        "authors": [
            "Luciano Celi",
            "Claudio Della Volpe",
            "Luca Pardi",
            "Stefano Siboni"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  A phase plot of the oil economy is built using the literature data of world\noil production, price, and EROEI (Energy Returned on Energy Invested). An\nanalogy between the oil economy and the Benard convection is proposed; some\nmethods of interpretation and forecast of the system behavior are also shown\nbased on \"phase portrait\" using as main variables the price, production and\nEROEI values. A scenery is proposed on this basis.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.03533v2"
    },
    {
        "title": "GDP growth rates as confined Lévy flights",
        "authors": [
            "Sandro Claudio Lera",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  A new model that combines economic growth rate fluctuations at the\nmicroscopic and macroscopic level is presented. At the microscopic level, firms\nare growing at different rates while also being exposed to idiosyncratic shocks\nat the firm and sector level. We describe such fluctuations as independent\nL\\'evy-stable fluctuations, varying over multiple orders of magnitude. These\nfluctuations are aggregated and measured at the macroscopic level in averaged\neconomic output quantities such as GDP. A fundamental question is thereby to\nwhat extend individual firm size fluctuations can have a noticeable impact on\nthe overall economy. We argue that this question can be answered by considering\nthe L\\'evy fluctuations as embedded in a steep confining potential well,\nensuring nonlinear mean-reversal behavior, without having to rely on\nmicroscopic details of the system. The steepness of the potential well directly\ncontrols the extend towards which idiosyncratic shocks to firms and sectors are\ndamped at the level of the economy. Additionally, the theory naturally accounts\nfor business cycles, represented in terms of a bimodal economic output\ndistribution, and thus connects two so far unrelated fields in economics. By\nanalyzing 200 years of US GDP growth rates, we find that the model is in good\nagreement with the data.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.05594v1"
    },
    {
        "title": "Decomposition of the Inequality of Income Distribution by Income Types -\n  Application for Romania",
        "authors": [
            "Tudorel Andrei",
            "Bogdan Oancea",
            "Peter Richmond",
            "Gurjeet Dhesi",
            "Claudiu Herteliu"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  This paper identifies the salient factors that characterize the inequality\nincome distribution for Romania. Data analysis is rigorously carried out using\nsophisticated techniques borrowed from classical statistics (Theil).\nDecomposition of the inequalities measured by the Theil index is also\nperformed. This study relies on an exhaustive (11.1 million records for 2014)\ndata-set for total personal gross income of Romanian citizens.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.07960v1"
    },
    {
        "title": "The perverse incentive for insurance instruments that are derivatives:\n  solving the jackpot problem with a clawback lien for default insurance notes",
        "authors": [
            "Brian P. Hanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  When an insurance note is also a derivative a serious problem arises because\na derivative must be fulfilled immediately. This feature of derivatives\nprevents claims processing procedures that screen out ineligible claims. This,\nin turn, creates a perverse incentive for insured holders of notes to commit\nacts that result in payment. This problem first surfaced with CDS contracts,\nwhich are part of a class of loan insurance I term a default insurance note.\n  Without an address to this problem, within the average range of returns for a\nlarge venture capital portfolio, a venture-bank makes less money the better\ntheir investments do, in a continuous function. The highest rate of return is a\ntotal loss, 64% more than a top portfolio.\n  Here, a strategy for removing this perverse incentive is defined, consisting\nof a clawback lien that returns part of the payment value as a lien on the firm\nthat is the beneficiary of the insurance. This is presented as the final major\ncomponent for implementing a default insurance note system so that\nventure-banking can operate to maximum benefit. Removing the perverse incentive\nalso minimizes disincentive for underwriters to deny DIN coverage to new\nventure capital firms, or to those firms that have historical earnings which\nare below average.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02600v3"
    },
    {
        "title": "Rich or poor: Who should pay higher tax rates?",
        "authors": [
            "Paulo Murilo Castro de Oliveira"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  A dynamic agent model is introduced with an annual random wealth\nmultiplicative process followed by taxes paid according to a linear\nwealth-dependent tax rate. If poor agents pay higher tax rates than rich\nagents, eventually all wealth becomes concentrated in the hands of a single\nagent. By contrast, if poor agents are subject to lower tax rates, the economic\ncollective process continues forever.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.06164v1"
    },
    {
        "title": "Asymmetric return rates and wealth distribution influenced by the\n  introduction of technical analysis into a behavioral agent based model",
        "authors": [
            "F. M. Stefan",
            "A. P. F. Atman"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Behavioral Finance has become a challenge to the scientific community. Based\non the assumption that behavioral aspects of investors may explain some\nfeatures of the Stock Market, we propose an agent based model to study\nquantitatively this relationship. In order to approximate the simulated market\nto the complexity of real markets, we consider that the investors are connected\namong them through a small world network; each one has its own psychological\nprofile (Imitation, Anti-Imitation, Random); two different strategies for\ndecision making: one of them is based on the trust neighborhood of the investor\nand the other one considers a technical analysis, the momentum of the market\nindex technique. We analyze the market index fluctuations, the wealth\ndistribution of the investors according to their psychological profiles and the\nrate of return distribution. Moreover, we analyze the influence of changing the\npsychological profile of the hub of the network and report interesting results\nwhich show how and when anti-imitation becomes the most profitable strategy for\ninvestment. Besides this, an intriguing asymmetry of the return rate\ndistribution is explained considering the behavioral aspect of the investors.\nThis asymmetry is quite robust being observed even when a completely different\nalgorithm to calculate the decision making of the investors was applied to it,\na remarkable result which, up to our knowledge, has never been reported before.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08282v1"
    },
    {
        "title": "Some Physics Notions on Monetary Standard",
        "authors": [
            "Tiago Fernandes"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Regardless of the gold-standard being considered as outdated, it provides\nvaluable signs concerning the development of novel monetary standards, better\nadjusted to the current macroeconomic environment. By using a point of view of\nclassical physics, the intent of this work is doing a review of the concept of\nmonetary standard and show that the energy matrix of an economy together with a\nnew monetary standard, based on the energy supply capacity, can play an\nessential role in the sustainable growth.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00001v4"
    },
    {
        "title": "Hint of a Universal Law for the Financial Gains of Competitive Sport\n  Teams. The case of Tour de France cycle race",
        "authors": [
            "Marcel Ausloos"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  This short note is intended as a \"Letter to the Editor\" Perspective in order\nthat it serves as a contribution, in view of reaching the physics community\ncaring about rare events and scaling laws and unexpected findings, on a domain\nof wide interest: sport and money. It is apparent from the data reported and\ndiscussed below that the scarcity of such data does not allow to recommend a\ncomplex elaboration of an agent based model, - at this time. In some sense,\nthis also means that much data on sport activities is not necessarily given in\nterms of physics prone materials, but it could be, and would then attract much\nattention. Nevertheless the findings tie the data to well known scaling laws\nand physics processes. It is found that a simple scaling law describes the\ngains of teams in recent bicycle races, like the Tour de France. An analogous\ncase, ranking teams in Formula 1 races, is shown in an Appendix\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00130v1"
    },
    {
        "title": "Why Long-Term Debt Instruments Cannot Be Deposit Substitutes",
        "authors": [
            "Russell Stanley Q. Geronimo"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  The definition of deposit substitutes in Philippine tax law fails to consider\nthe maturity of a debt instrument. This makes it possible for long-term bonds\nto be considered as deposit substitutes if they meet the 20-lender rule,\ntaxable at 20% final tax. However, long-term debt instruments cannot\nrealistically function as deposit substitutes even if they fall in the hands of\n20 or more lenders. First, long-term debt instruments cannot simultaneously\nreplicate the twin features of capital preservation and liquidity, which are\nintegral to the nature of a deposit substitute. Second, deposit substitutes are\nan integral part of the maturity transformation process (i.e. short-term\nborrowing for the purpose of long-term lending) in financial intermediaries,\nwhich means that they should have low borrowing cost, made possible only by\nhaving short-term maturity. To prove these propositions, this paper situates\nthe function of deposit substitutes within the context of shadow banking, where\nsaid instruments originated and are generally used. To show the incompatibility\nbetween a deposit substitute and a long-term debt instrument, the paper applies\nthe fundamental theory of bond values to 10-year zero-coupon treasury notes\ncalled 'PEACe Bonds' in Banco De Oro, et al. vs. Republic (2015 and 2016). The\npaper recommends that deposit substitutes should be limited to debt instruments\nwith maturity of not more than 1 year.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09978v1"
    },
    {
        "title": "How Short Sales Circumvent the Capital Gains Tax System",
        "authors": [
            "Russell Stanley Q. Geronimo"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Through a short sale, a person borrows a share of stock from a lender, sells\nthe borrowed share to a third person at the current price, and purchases an\nidentical share in the market at a future date and at a future price to replace\nthe borrowed share of stock. This only makes sense if the short seller\nanticipates a downward trend in share price. The short seller incurs a gain if\nshare price decreases because the cost of replacing the borrowed share falls\nbelow the selling price. The reverse is true in an ordinary sale, where a\nperson owning a share of stock incurs a loss if price decreases because the\nselling price falls below the basis or acquisition cost. Therefore, when a\ntaxpayer simultaneously owns a share of stock and short sells an identical\nstock, any gain in an ordinary sale of the owned stock is offset by a\ncorresponding loss in the short sale of the borrowed identical stock, vice\nversa. This offsetting effect, in turn, creates an unexpected tax deferral\nopportunity abused in other jurisdictions and which remains unregulated in the\nPhilippine tax system.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09987v1"
    },
    {
        "title": "Global Income Inequality and Savings: A Data Science Perspective",
        "authors": [
            "Kiran Sharma",
            "Subhradeep Das",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  A society or country with income equally distributed among its people is\ntruly a fiction! The phenomena of socioeconomic inequalities have been plaguing\nmankind from times immemorial. We are interested in gaining an insight about\nthe co-evolution of the countries in the inequality space, from a data science\nperspective. For this purpose, we use the time series data for Gini indices of\ndifferent countries, and construct the equal-time cross-correlation matrix. We\nthen use this to construct a similarity matrix and generate a map with the\ncountries as different points generated through a multi-dimensional scaling\ntechnique. We also produce a similar map of different countries using the time\nseries data for Gross Domestic Savings (% of GDP). We also pose a different,\nyet significant, question: Can higher savings moderate the income inequality?\nIn this paper, we have tried to address this question through another data\nscience technique - linear regression, to seek an empirical linkage between the\nincome inequality and savings, mainly for relatively small or closed economies.\nThis question was inspired from an existing theoretical model proposed by\nChakraborti-Chakrabarti (2000), based on the principle of kinetic theory of\ngases. We tested our model empirically using Gini index and Gross Domestic\nSavings, and observed that the model holds reasonably true for many economies\nof the world.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00253v2"
    },
    {
        "title": "Coexistence of several currencies in presence of increasing returns to\n  adoption",
        "authors": [
            "Alex Lamarche-Perrin",
            "André Orléan",
            "Pablo Jensen"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We present a simplistic model of the competition between different\ncurrencies. Each individual is free to choose the currency that minimizes his\ntransaction costs, which arise whenever his exchanging relations have chosen a\ndifferent currency. We show that competition between currencies does not\nnecessarily converge to the emergence of a single currency. For large systems,\nwe prove that two distinct communities using different currencies in the\ninitial state will remain forever in this fractionalized state.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04218v1"
    },
    {
        "title": "Demographic Modeling Via 3-dimensional Markov Chains",
        "authors": [
            "Juan Jose Viquez",
            "Alexander Campos",
            "Jorge Loria",
            "Luis Alfredo Mendoza",
            "Jorge Aurelio Viquez"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  This article presents a new model for demographic simulation which can be\nused to forecast and estimate the number of people in pension funds\n(contributors and retirees) as well as workers in a public institution.\nFurthermore, the model introduces opportunities to quantify the financial ows\ncoming from future populations such as salaries, contributions, salary\nsupplements, employer contribution to savings/pensions, among others. The\nimplementation of this probabilistic model will be of great value in the\nactuarial toolbox, increasing the reliability of the estimations as well as\nallowing deeper demographic and financial analysis given the reach of the\nmodel. We introduce the mathematical model, its first moments, and how to\nadjust the required probabilities, showing at the end an example where the\nmodel was applied to a public institution with real data.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04841v1"
    },
    {
        "title": "Shooting High or Low: Do Countries Benefit from Entering Unrelated\n  Activities?",
        "authors": [
            "Flávio L. Pinheiro",
            "Aamena Alshamsi",
            "Dominik Hartmann",
            "Ron Boschma",
            "César A. Hidalgo"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Countries tend to diversify their exports by entering products that are\nrelated to their current exports. Yet this average behavior is not\nrepresentative of every diversification path. In this paper, we introduce a\nmethod to identify periods when countries enter unrelated products. We analyze\nthe economic diversification paths of 93 countries between 1965 and 2014 and\nfind that countries enter unrelated products in only about 7.2% of all\nobservations. We find that countries enter more unrelated products when they\nare at an intermediate level of economic development, and when they have higher\nlevels of human capital. Finally, we ask whether countries entering more\nunrelated products grow faster than those entering only related products. The\ndata shows that countries that enter more unrelated activities experience a\nsmall but significant increase in future economic growth, compared to countries\nwith a similar level of income, human capital, capital stock per worker, and\neconomic complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05352v3"
    },
    {
        "title": "Accurate Evaluation of Asset Pricing Under Uncertainty and Ambiguity of\n  Information",
        "authors": [
            "Farouq Abdulaziz Masoudy"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Since exchange economy considerably varies in the market assets, asset prices\nhave become an attractive research area for investigating and modeling\nambiguous and uncertain information in today markets. This paper proposes a new\ngenerative uncertainty mechanism based on the Bayesian Inference and\nCorrentropy (BIC) technique for accurately evaluating asset pricing in markets.\nThis technique examines the potential processes of risk, ambiguity, and\nvariations of market information in a controllable manner. We apply the new BIC\ntechnique to a consumption asset-pricing model in which the consumption\nvariations are modeled using the Bayesian network model with observing the\ndynamics of asset pricing phenomena in the data. These dynamics include the\nprocyclical deviations of price, the countercyclical deviations of equity\npremia and equity volatility, the leverage impact and the mean reversion of\nexcess returns. The key findings reveal that the precise modeling of asset\ninformation can estimate price changes in the market effectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06966v2"
    },
    {
        "title": "Dynamic Pricing and Learning with Competition: Insights from the Dynamic\n  Pricing Challenge at the 2017 INFORMS RM & Pricing Conference",
        "authors": [
            "Ruben van de Geer",
            "Arnoud V. den Boer",
            "Christopher Bayliss",
            "Christine Currie",
            "Andria Ellina",
            "Malte Esders",
            "Alwin Haensel",
            "Xiao Lei",
            "Kyle D. S. Maclean",
            "Antonio Martinez-Sykora",
            "Asbjørn Nilsen Riseth",
            "Fredrik Ødegaard",
            "Simos Zachariades"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This paper presents the results of the Dynamic Pricing Challenge, held on the\noccasion of the 17th INFORMS Revenue Management and Pricing Section Conference\non June 29-30, 2017 in Amsterdam, The Netherlands. For this challenge,\nparticipants submitted algorithms for pricing and demand learning of which the\nnumerical performance was analyzed in simulated market environments. This\nallows consideration of market dynamics that are not analytically tractable or\ncan not be empirically analyzed due to practical complications. Our findings\nimplicate that the relative performance of algorithms varies substantially\nacross different market dynamics, which confirms the intrinsic complexity of\npricing and learning in the presence of competition.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03219v1"
    },
    {
        "title": "Assessing the state of e-Readiness for Small and Medium Companies in\n  Mexico: a Proposed Taxonomy and Adoption Model",
        "authors": [
            "Guillermo Rodriguez-Abitia",
            "Susana Vidrio",
            "Claudia Montiel-Sanchez"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Emerging economies frequently show a large component of their Gross Domestic\nProduct to be dependant on the economic activity of small and medium\nenterprises. Nevertheless, e-business solutions are more likely designed for\nlarge companies. SMEs seem to follow a classical family-based management, used\nto traditional activities, rather than seeking new ways of adding value to\ntheir business strategy. Thus, a large portion of a nations economy may be at\ndisadvantage for competition. This paper aims at assessing the state of\ne-business readiness of Mexican SMEs based on already published e-business\nevolution models and by means of a survey research design. Data is being\ncollected in three cities with differing sizes and infrastructure conditions.\nStatistical results are expected to be presented. A second part of this\nresearch aims at applying classical adoption models to suggest potential causal\nrelationships, as well as more suitable recommendations for development.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06709v1"
    },
    {
        "title": "The impact of margin trading on share price evolution: A cascading\n  failure model investigation",
        "authors": [
            "Ya-Chun Gao",
            "Huai-Lin Tang",
            "Shi-Min Cai",
            "Jing-Jing Gao",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Margin trading in which investors purchase shares with money borrowed from\nbrokers is blamed to be a major cause of the 2015 Chinese stock market crash.\nWe propose a cascading failure model and examine how an increase in margin\ntrading increases share price vulnerability. The model is based on a bipartite\ngraph of investors and shares that includes four margin trading factors, (i)\ninitial margin $k$, (ii) minimum maintenance $r$, (iii) volatility $v$, and\n(iv) diversity $s$. We use our model to simulate margin trading and observe how\nthe share prices are affected by these four factors. The experimental results\nindicate that a stock market can be either vulnerable or stable. A stock market\nis vulnerable when an external shock can cause a cascading failure of its share\nprices. It is stable when its share prices are resilient to external shocks.\nFurthermore, we investigate how the cascading failure of share price is\naffected by these four factors, and find that by increasing $v$ and $r$ or\ndecreasing $k$ we increase the probability that the stock market will\nexperience a phase transition from stable to vulnerable. It is also found that\nincreasing $s$ decreases resilience and increases systematic risk. These\nfindings could be useful to regulators supervising margin trading activities.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07352v1"
    },
    {
        "title": "Planetary boundaries of consumption growth: Declining social discount\n  rates",
        "authors": [
            "Victor E. Gluzberg",
            "Yuri A. Katz"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We introduce the logistic model of consumption growth, which captures a\nnegative feedback loop preventing an unlimited growth of consumption due to\nfinite biophysical resources of our planet. This simple dynamic model allows\nfor derivation of the expression describing the declining long-term tail of a\nsocial discount curve. The latter plays a critical role in, e.g., climate\nfinance with benefits on current investments deferred to centuries from now.\nThe growth rate of consumption is irregularly evolving in time, which makes\nestimation of an expected term-structure of consumption growth and associated\nsocial discount rates a challenging task. Nonetheless, observations show that\nthe problem at hand is perturbative with the small parameter being the product\nof an average strength of fluctuations in the growth rate and its\nautocorrelation time. This fact permits utilization of the cumulant expansion\nmethod to derive remarkably simple expressions for the term-structure of\nexpected consumption growth and associated discount rates in the bounded\neconomy. Comparison with empirical data shows that the dynamic effect related\nto the planetary resource constrains could become a dominant mechanism\nresponsible for a declining long-term tail of a social discount curve at the\ntime horizon estimated here as about100 years from now (the lower boundary).\nThe derived results can help to shape a more realistic long-term social\ndiscounting policy. Furthermore, with the obvious redefinition of the key\nparameters of the model, obtained results are directly applicable for\ndescription of expected long-term population growth in stochastic environments.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.08021v2"
    },
    {
        "title": "Agents' beliefs and economic regimes polarization in interacting markets",
        "authors": [
            "Fausto Cavalli",
            "Ahmad Naimzada",
            "Nicolò Pecora",
            "Marina Pireddu"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  In the present paper a model of a market consisting of real and financial\ninteracting sectors is studied. Agents populating the stock market are assumed\nto be not able to observe the true underlying fundamental, and their beliefs\nare biased by either optimism or pessimism. Depending on the relevance they\ngive to beliefs, they select the best performing strategy in an evolutionary\nperspective. The real side of the economy is described within a\nmultiplier-accelerator framework with a nonlinear, bounded investment function.\nWe show that strongly polarized beliefs in an evolutionary framework can\nintroduce multiplicity of steady states, which, consisting in enhanced or\ndepressed levels of income, reflect and reproduce the optimistic or pessimistic\nnature of the agents' beliefs. The polarization of these steady states, which\ncoexist with an unbiased steady state, positively depends on that of the\nbeliefs and on their relevance. Moreover, with a mixture of analytical and\nnumerical tools, we show that such static characterization is inherited also at\nthe dynamical level, with possibly complex attractors that are characterized by\nendogenously fluctuating pessimistic and optimistic levels of national income\nand price. This framework, when stochastic perturbations are included, is able\nto account for stylized facts commonly observed in real financial markets, such\nas fat tails and excess volatility in the returns distributions, as well as\nbubbles and crashes for stock prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00387v1"
    },
    {
        "title": "Total, asymmetric and frequency connectedness between oil and forex\n  markets",
        "authors": [
            "Jozef Baruník",
            "Evžen Kočenda"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We analyze total, asymmetric and frequency connectedness between oil and\nforex markets using high-frequency, intra-day data over the period 2007 --\n2017. By employing variance decompositions and their spectral representation in\ncombination with realized semivariances to account for asymmetric and frequency\nconnectedness, we obtain interesting results. We show that divergence in\nmonetary policy regimes affects forex volatility spillovers but that adding oil\nto a forex portfolio decreases the total connectedness of the mixed portfolio.\nAsymmetries in connectedness are relatively small. While negative shocks\ndominate forex volatility connectedness, positive shocks prevail when oil and\nforex markets are assessed jointly. Frequency connectedness is largely driven\nby uncertainty shocks and to a lesser extent by liquidity shocks, which impact\nlong-term connectedness the most and lead to its dramatic increase during\nperiods of distress.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03980v2"
    },
    {
        "title": "'Bosons' and 'fermions' in social and economic systems",
        "authors": [
            "Sergey A. Rashkovskiy"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We analyze social and economic systems with a hierarchical structure and show\nthat for such systems, it is possible to construct thermostatistics, based on\nthe intermediate Gentile statistics. We show that in social and economic\nhierarchical systems there are elements that obey the Fermi-Dirac statistics\nand can be called fermions, as well as elements that are approximately subject\nto Bose-Einstein statistics and can be called bosons. We derive the first and\nsecond laws of thermodynamics for the considered economic system and show that\nsuch concepts as temperature, pressure and financial potential (which is an\nanalogue of the chemical potential in thermodynamics) that characterize the\nstate of the economic system as a whole, can be introduced for economic\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.05327v1"
    },
    {
        "title": "A new $κ$-deformed parametric model for the size distribution of\n  wealth",
        "authors": [
            "Adams Vallejos",
            "Ignacio Ormazabal",
            "Felix A. Borotto",
            "Hernan F. Astudillo"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  It has been pointed out by Patriarca et al. (2005) that the power-law tailed\nequilibrium distribution in heterogeneous kinetic exchange models with a\ndistributed saving parameter can be resolved as a mixture of Gamma\ndistributions corresponding to particular subsets of agents. Here, we propose a\nnew four-parameter statistical distribution which is a $\\kappa$-deformation of\nthe Generalized Gamma distribution with a power-law tail, based on the deformed\nexponential and logarithm functions introduced by Kaniadakis(2001). We found\nthat this new distribution is also an extension to the $\\kappa$-Generalized\ndistribution proposed by Clementi et al. (2007), with an additional shape\nparameter $\\nu$, and properly reproduces the whole range of the distribution of\nwealth in such heterogeneous kinetic exchange models. We also provide various\nassociated statistical measures and inequality measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06929v1"
    },
    {
        "title": "Forecasting financial crashes with quantum computing",
        "authors": [
            "Roman Orus",
            "Samuel Mugel",
            "Enrique Lizaso"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  A key problem in financial mathematics is the forecasting of financial\ncrashes: if we perturb asset prices, will financial institutions fail on a\nmassive scale? This was recently shown to be a computationally intractable\n(NP-hard) problem. Financial crashes are inherently difficult to predict, even\nfor a regulator which has complete information about the financial system. In\nthis paper we show how this problem can be handled by quantum annealers. More\nspecifically, we map the equilibrium condition of a toy-model financial network\nto the ground-state problem of a spin-1/2 quantum Hamiltonian with 2-body\ninteractions, i.e., a quadratic unconstrained binary optimization (QUBO)\nproblem. The equilibrium market values of institutions after a sudden shock to\nthe network can then be calculated via adiabatic quantum computation and, more\ngenerically, by quantum annealers. Our procedure could be implemented on\nnear-term quantum processors, thus providing a potentially more efficient way\nto assess financial equilibrium and predict financial crashes.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.07690v2"
    },
    {
        "title": "How production networks amplify economic growth",
        "authors": [
            "James McNerney",
            "Charles Savoie",
            "Francesco Caravelli",
            "Vasco M. Carvalho",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Technological improvement is the most important cause of long-term economic\ngrowth. We study the effects of technology improvement in the setting of a\nproduction network, in which each producer buys input goods and converts them\nto other goods, selling the product to households or other producers. We show\nhow this network amplifies the effects of technological improvements as they\npropagate along chains of production. Longer production chains for an industry\nbias it towards faster price reduction, and longer production chains for a\ncountry bias it towards faster GDP growth. These predictions are in good\nagreement with data and improve with the passage of time, demonstrating a key\ninfluence of production chains in price change and output growth over the long\nterm.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.07774v2"
    },
    {
        "title": "On dynamics of wage-price spiral and stagflation in some model economic\n  systems",
        "authors": [
            "Afifa Alintissar",
            "Abdelkader Intissar",
            "Jean-karim Intissar"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This article aims to present an elementary analytical solution to the\nquestion of the formation of a structure of differentiation of rates of return\nin a classical gravitation model and in a model of the dynamics of price-wage\nspirals.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.01707v1"
    },
    {
        "title": "In (Stochastic) Search of a Fairer Alife",
        "authors": [
            "Dmitriy Volinskiy",
            "Lana Cuthbertson",
            "Omid Ardakanian"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Economies and societal structures in general are complex stochastic systems\nwhich may not lend themselves well to algebraic analysis. An addition of\nsubjective value criteria to the mechanics of interacting agents will further\ncomplicate analysis. The purpose of this short study is to demonstrate\ncapabilities of agent-based computational economics to be a platform for\nfairness or equity analysis in both a broad and practical sense.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.02311v1"
    },
    {
        "title": "Quantification of market efficiency based on informational-entropy",
        "authors": [
            "Roland Rothenstein"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Since the 1960s, the question whether markets are efficient or not is\ncontroversially discussed. One reason for the difficulty to overcome the\ncontroversy is the lack of a universal, but also precise, quantitative\ndefinition of efficiency that is able to graduate between different states of\nefficiency. The main purpose of this article is to fill this gap by developing\na measure for the efficiency of markets that fulfill all the stated\nrequirements. It is shown that the new definition of efficiency, based on\ninformational-entropy, is equivalent to the two most used definitions of\nefficiency from Fama and Jensen. The new measure therefore enables steps to\nsettle the dispute over the state of efficiency in markets. Moreover, it is\nshown that inefficiency in a market can either arise from the possibility to\nuse information to predict an event with higher than chance level, or can\nemerge from wrong pricing/ quotes that do not reflect the right probabilities\nof possible events. Finally, the calculation of efficiency is demonstrated on a\nsimple game (of coin tossing), to show how one could exactly quantify the\nefficiency in any market-like system, if all probabilities are known.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.02371v1"
    },
    {
        "title": "Piketty's second fundamental law of capitalism as an emergent property\n  in a kinetic wealth-exchange model of economic growth",
        "authors": [
            "D. S. Quevedo",
            "C. J. Quimbay"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We propose in this work a kinetic wealth-exchange model of economic growth by\nintroducing saving as a non consumed fraction of production. In this new model,\nwhich starts also from microeconomic arguments, it is found that economic\ntransactions between pairs of agents leads the system to a macroscopic behavior\nwhere total wealth is not conserved and it is possible to have an economic\ngrowth which is assumed as the increasing of total production in time. This\nlast macroeconomic result, that we find both numerically through a Monte Carlo\nbased simulation method and analytically in the framework of a mean field\napproximation, corresponds to the economic growth scenario described by the\nwell known Solow model developed in the economic neoclassical theory. If\nadditionally to the income related with production due to return on individual\ncapital, it is also included the individual labor income in the model, then the\nThomas Piketty's second fundamental law of capitalism is found as a emergent\nproperty of the system. We consider that the results obtained in this paper\nshows how Econophysics can help to understand the connection between\nmacroeconomics and microeconomics.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.00952v2"
    },
    {
        "title": "Modern Asset Theory: A Framework for Successful Active Management",
        "authors": [
            "Corry Bedwell",
            "Ryan Guttridge"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Active management is a term that has many meanings and we have found the\ndefining characteristics needed for success as an \"active manager\" elusive\nwithin the literature. In this paper we offer a set of criteria that defines an\nactive manager and his success. In order to facilitate this, we introduce\nseveral definitions, which lead to a logically coherent evaluation framework.\nWe expand on the definitions of these six key concepts: Introduce a specific\nconcept of Ruin, Assets, Risk, Discount rate, Margin of safety, and\nOptimization.\n  Through these definitions a strong defense of active management emerges.\nFurthermore, to the extent one chooses to limit the definitions we offer, our\nframework reduces to that of Modern Portfolio Theory. Overall, we have aimed to\nconstruct a robust expansion of a framework for active management, one that has\nbeen found wanting in the current literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09683v1"
    },
    {
        "title": "Pricing foreseeable and unforeseeable risks in insurance portfolios",
        "authors": [
            "Weihong Ni",
            "Corina Constantinescu",
            "Alfredo Egídio dos Reis",
            "Véronique Maume-Deschamps"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this manuscript we propose a method for pricing insurance products that\ncover not only traditional risks, but also unforeseen ones. By considering the\nPoisson process parameter to be a mixed random variable, we capture the\nheterogeneity of foreseeable and unforeseeable risks. To illustrate, we\nestimate the weights for the two risk streams for a real dataset from a\nPortuguese insurer. To calculate the premium, we set the frequency and severity\nas distributions that belong to the linear exponential family. Under a Bayesian\nsetup , we show that when working with a finite mixture of conjugate priors,\nthe premium can be estimated by a mixture of posterior means, with updated\nparameters, depending on claim histories. We emphasise the riskiness of the\nunforeseeable trend, by choosing heavy-tailed distributions. After estimating\ndistribution parameters involved using the Expectation-Maximization algorithm,\nwe found that Bayesian premiums derived are more reactive to claim trends than\ntraditional ones.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.03123v1"
    },
    {
        "title": "Transparency versus Performance in Financial Markets: The Role of CSR\n  Communications",
        "authors": [
            "Rajiv Kashyap",
            "Mohamed Menisy",
            "Peter Caiazzo",
            "Jim Samuel"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Although companies are exhorted to provide more information to the financial\ncommunity, it is evident that they choose different paths based upon their\nstrategic emphasis and competitive environments. Our investigation explores the\nempirical boundary conditions under which firms choose to disclose versus\nwithhold information from investors based upon their strategic emphasis. We\nfound significant differences in terms of voluntary information disclosures\nbetween firms that consistently delivered positive earnings surprises versus\nthose that delivered negative earnings surprises. We investigated this effect\nin a more granular fashion by separately examining differences in\nenvironmental, social, and governance disclosures between the two pools of\nfirms. We found that in essence, the differences remained consistent and\npositive earnings firms were significantly more likely to disclose information\nabout their ESG activities than their counterparts. Interestingly, none of the\nmeasures of financial performance were instrumental in distinguishing between\nthe two pools of firms. However, our measures of reach -- as measured by the\nnumber of -- negative news stories lends credence to our findings. From a fund\nmanager-s perspective, this finding should raise an immediate red flag firms\nthat are likely to underperform are likely to be less transparent than\noverperformers.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.03443v1"
    },
    {
        "title": "Portfolio Optimization of 60 Stocks Using Classical and Quantum\n  Algorithms",
        "authors": [
            "Jeffrey Cohen",
            "Alex Khan",
            "Clark Alexander"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We continue to investigate the use of quantum computers for building an\noptimal portfolio out of a universe of 60 U.S. listed, liquid equities.\nStarting from historical market data, we apply our unique problem formulation\non the D-Wave Systems Inc. D-Wave 2000Q (TM) quantum annealing system\n(hereafter called D-Wave) to find the optimal risk vs return portfolio. We\napproach this first classically, then using the D-Wave, to select efficient buy\nand hold portfolios. Our results show that practitioners can use either\nclassical or quantum annealing methods to select attractive portfolios. This\nbuilds upon our prior work on optimization of 40 stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.08669v1"
    },
    {
        "title": "Rawls' Fairness, Income Distribution and Alarming Level of Gini\n  Coefficient",
        "authors": [
            "Yong Tao",
            "Xiangjun Wu",
            "Changshuai Li"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The argument that the alarming level of Gini coefficient is 0.4 is very\npopular, especially in the media industry, all around the world for a long\ntime. Although the 0.4 standard is widely accepted, the derivation of the value\nlacks rigid theoretical foundations. In fact, to the best of our knowledge, it\nis not based on any prevalent and convincing economic theories. In this paper,\nwe incorporate Rawls' principle of fair equality of opportunity into\nArrow-Debreu's framework of general equilibrium theory with heterogeneous\nagents, and derive the alarming level of Gini coefficient formally. Our theory\nreveals that the exponential distribution of income not only satisfies Pareto\noptimality, but also obeys social fairness in Rawls' sense. Therefore, we\nspecify the maximal value of the Gini coefficient when income follows\nexponential distribution as a possible alarming level. Our computations show\nthat the alarming level should be specified at least equal or larger than 0.5\nrather than 0.4. We empirically investigate if our model receives support from\na large data set of all kinds of countries all over the world from Word Bank in\n1990, 1995, 2000 and 2005 using the distribution fitting and statistical\ndecision methodology. The results suggest that the value of 0.4 is around the\nmean of the Gini coefficients, corresponding to the most probable event in a\npeaceful world, rather than the alarming level, while the two-sigma rule shows\nthat in our sample the alarming levels are all larger than 0.5, conforming to\nthe predictions of our theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.3979v1"
    },
    {
        "title": "Can Market Risk Perception Drive Inefficient Prices? Theory and Evidence",
        "authors": [
            "Matteo Formenti"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  This work presents an asset pricing model that under rational expectation\nequilibrium perspective shows how, depending on risk aversion and noise\nvolatility, a risky-asset has one equilibrium price that differs in term of\nefficiency: an informational efficient one (similar to Campbell and Kyle\n(1993)), and another one where price diverges from its informational efficient\nlevel. The former Pareto dominates (is dominated by) the latter in presence of\nlow (high) market risk perception. The estimates of the model using S&P 500\nIndex support the theoretical findings, and the estimated inefficient\nequilibrium price captures the higher risk premium and higher volatility\nobserved during the Dot.com bubble 1995--2000.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.4890v1"
    },
    {
        "title": "The Immediate Exchange model: an analytical investigation",
        "authors": [
            "Guy Katriel"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We study the Immediate Exchange model, recently introduced by Heinsalu and\nPatriarca [Eur. Phys. J. B 87: 170 (2014)], who showed by simulations that the\nwealth distribution in this model converges to a Gamma distribution with shape\nparameter $2$. Here we justify this conclusion analytically, in the\ninfinite-population limit. An infinite-population version of the model is\nderived, describing the evolution of the wealth distribution in terms of\niterations of a nonlinear operator on the space of probability densities. It is\nproved that the Gamma distributions with shape parameter $2$ are fixed points\nof this operator, and that, starting with an arbitrary wealth distribution, the\nprocess converges to one of these fixed points. We also discuss the mixed model\nintroduced in the same paper, in which exchanges are either bidirectional or\nunidirectional with fixed probability. We prove that, although, as found by\nHeinsalu and Patriarca, the equilibrium distribution can be closely fit by\nGamma distributions, the equilibrium distribution for this model is {\\it{not}}\na Gamma distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.6646v1"
    },
    {
        "title": "A GDP-driven model for the binary and weighted structure of the\n  International Trade Network",
        "authors": [
            "Assaf Almog",
            "Tiziano Squartini",
            "Diego Garlaschelli"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Recent events such as the global financial crisis have renewed the interest\nin the topic of economic networks. One of the main channels of shock\npropagation among countries is the International Trade Network (ITN). Two\nimportant models for the ITN structure, the classical gravity model of trade\n(more popular among economists) and the fitness model (more popular among\nnetworks scientists), are both limited to the characterization of only one\nrepresentation of the ITN. The gravity model satisfactorily predicts the volume\nof trade between connected countries, but cannot reproduce the observed missing\nlinks (i.e. the topology). On the other hand, the fitness model can\nsuccessfully replicate the topology of the ITN, but cannot predict the volumes.\nThis paper tries to make an important step forward in the unification of those\ntwo frameworks, by proposing a new GDP-driven model which can simultaneously\nreproduce the binary and the weighted properties of the ITN. Specifically, we\nadopt a maximum-entropy approach where both the degree and the strength of each\nnode is preserved. We then identify strong nonlinear relationships between the\nGDP and the parameters of the model. This ultimately results in a weighted\ngeneralization of the fitness model of trade, where the GDP plays the role of a\n`macroeconomic fitness' shaping the binary and the weighted structure of the\nITN simultaneously. Our model mathematically highlights an important asymmetry\nin the role of binary and weighted network properties, namely the fact that\nbinary properties can be inferred without the knowledge of weighted ones, while\nthe opposite is not true.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.6649v1"
    },
    {
        "title": "Tax Compliance and Public Goods Provision -- An Agent-based Econophysics\n  Approach",
        "authors": [
            "S. Hokamp",
            "G. Seibold"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We calculate the dynamics of tax evasion within a multi-agent econophysics\nmodel which is adopted from the theory of magnetism and previously has been\nshown to capture the main characteristics from agent-based based models which\nbuild on the standard Allingham and Sandmo approach. In particular, we\nimplement a feedback of public goods provision on the decision-making of\nselfish agents which aim to pursue their self interest. Our results imply that\nsuch a feedback enhances the moral attitude of selfish agents thus reducing the\npercentage of tax evasion. Two parameters govern the behavior of selfish\nagents, (i) the rate of adaption to changes in public goods provision and (ii)\nthe threshold of perception of public goods provision. Furtheron we analyze the\ntax evasion dynamics for different agent co mpositions and under the feedback\nof public goods provision. We conclude that policymakers may enhance tax\ncompliance behavior via the threshold of perception by means of targeted public\nrelations.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.8528v1"
    },
    {
        "title": "The Wealth of Nations: Complexity Science for an Interdisciplinary\n  Approach in Economics",
        "authors": [
            "Klaus Jaffe"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Classic economic science is reaching the limits of its explanatory powers.\nComplexity science uses an increasingly larger set of different methods to\nanalyze physical, biological, cultural, social, and economic factors, providing\na broader understanding of the socio-economic dynamics involved in the\ndevelopment of nations worldwide. The use of tools developed in the natural\nsciences, such as thermodynamics, evolutionary biology, and analysis of complex\nsystems, help us to integrate aspects, formerly reserved to the social\nsciences, with the natural sciences. This integration reveals details of the\nsynergistic mechanisms that drive the evolution of societies. By doing so, we\nincrease the available alternatives for economic analysis and provide ways to\nincrease the efficiency of decision-making mechanisms in complex social\ncontexts. This interdisciplinary analysis seeks to deepen our understanding of\nwhy chronic poverty is still common, and how the emergence of prosperous\ntechnological societies can be made possible. This understanding should\nincrease the chances of achieving a sustainable, harmonious and prosperous\nfuture for humanity. The analysis evidences that complex fundamental economic\nproblems require multidisciplinary approaches and rigorous application of the\nscientific method if we want to advance significantly our understanding of\nthem. The analysis reveals viable routes for the generation of wealth and the\nreduction of poverty, but also reveals huge gaps in our knowledge about the\ndynamics of our societies and about the means to guide social development\ntowards a better future for all.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02853v1"
    },
    {
        "title": "Network analysis of Japanese global business using quasi-exhaustive\n  micro-data for Japanese overseas subsidiaries",
        "authors": [
            "Jean-Pascal Bassino",
            "Pablo Jensen",
            "Matteo Morini"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Network analysis techniques remain rarely used for understanding\ninternational management strategies. Our paper highlights their value as\nresearch tool in this field of social science using a large set of micro-data\n(20,000) to investigate the presence of networks of subsidiaries overseas. The\nresearch question is the following: to what extent did/do global Japanese\nbusiness networks mirror organizational models existing in Japan? In\nparticular, we would like to assess how much the links building such business\nnetworks are shaped by the structure of big-size industrial conglomerates of\nfirms headquartered in Japan, also described as HK. The major part of the\nacademic community in the fields of management and industrial organization\nconsiders that formal links can be identified among firms belonging to HK. Miwa\nand Ramseyer (Miwa and Ramseyer 2002; Ramseyer 2006) challenge this claim and\nargue that the evidence supporting the existence of HK is weak. So far,\nquantitative empirical investigation has been conducted exclusively using data\nfor firms incorporated in Japan. Our study tests the Miwa-Ramseyer hypothesis\n(MRH) at the global level using information on the network of Japanese\nsubsidiaries overseas. The results obtained lead us to reject the MRH for the\nglobal dataset, as well as for subsets restricted to the two main\nregions/countries of destination of Japanese foreign investment. The results\nare robust to the weighting of the links, with different specifications, and\nare observed in most industrial sectors. The global Japanese network became\nincreasingly complex during the late 20th century as a consequence of increase\nin the number of Japanese subsidiaries overseas but the key features of the\nstructure remained rather stable. We draw implications of these findings for\nacademic research in international business and for professionals involved in\ncorporate strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05699v1"
    },
    {
        "title": "Contagious disruptions and complexity traps in economic development",
        "authors": [
            "Charles D. Brummitt",
            "Kenan Huremovic",
            "Paolo Pin",
            "Matthew H. Bonds",
            "Fernando Vega-Redondo"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Poor economies not only produce less; they typically produce things that\ninvolve fewer inputs and fewer intermediate steps. Yet the supply chains of\npoor countries face more frequent disruptions---delivery failures, faulty\nparts, delays, power outages, theft, government failures---that systematically\nthwart the production process. To understand how these disruptions affect\neconomic development, we model an evolving input--output network in which\ndisruptions spread contagiously among optimizing agents. The key finding is\nthat a poverty trap can emerge: agents adapt to frequent disruptions by\nproducing simpler, less valuable goods, yet disruptions persist. Growing out of\npoverty requires that agents invest in buffers to disruptions. These buffers\nrise and then fall as the economy produces more complex goods, a prediction\nconsistent with global patterns of input inventories. Large jumps in economic\ncomplexity can backfire. This result suggests why \"big push\" policies can fail,\nand it underscores the importance of reliability and of gradual increases in\ntechnological complexity.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05914v1"
    },
    {
        "title": "Impact of the Global Crisis on SME Internal vs. External Financing in\n  China",
        "authors": [
            "ShiXue He",
            "Marcel Ausloos"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Changes in the capital structure before and after the global financial crisis\nfor SMEs are studied, emphasizing their financing problems, distinguishing\nbetween internal financing and external financing determinants. The empirical\nresearch bears upon 158 small and medium-sized firms listed on Shenzhen and\nShanghai Stock Exchanges in China over the period of 2004-2014. A regression\nanalysis, along the lines of the Trade-Off Theory, shows that the leverage\ndecreases with profitability, non-debt tax shields and the liquidity, and\nincreases with firm size and tangibility. A positive relationship is found\nbetween firm growth and debt ratio, though not highly significantly. It is\nshown that the SMEs with high growth rates are those which will more easily\nobtain external financing after a financial crisis. It is recognized that the\nChina government should reconsider SMEs taxation laws.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.06635v1"
    },
    {
        "title": "Optimization by Hybridization of a Genetic Algorithm with the PROMOTHEE\n  Method: Management of Multicriteria Localization",
        "authors": [
            "Myriem Alijo",
            "Otman Abdoun",
            "Mostafa Bachran",
            "Amal Bergam"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The decision to locate an economic activity of one or several countries is\nmade taking into account numerous parameters and criteria. Several studies have\nbeen carried out in this field, but they generally use information in a reduced\ncontext. The majority are based solely on parameters, using traditional methods\nwhich often lead to unsatisfactory solutions.This work consists in hybridizing\nthrough genetic algorithms, economic intelligence (EI) and multicriteria\nanalysis methods (MCA) to improve the decisions of territorial localization.\nThe purpose is to lead the company to locate its activity in the place that\nwould allow it a competitive advantage. This work also consists of identifying\nall the parameters that can influence the decision of the economic actors and\nequipping them with tools using all the national and international data\navailable to lead to a mapping of countries, regions or departments favorable\nto the location. Throughout our research, we have as a goal the realization of\na hybrid conceptual model of economic intelligence based on multicriteria on\nwith genetic algorithms in order to optimize the decisions of localization, in\nthis perspective we opted for the method of PROMETHEE (Preference Ranking\nOrganization for Method of Enrichment Evaluation), which has made it possible\nto obtain the best compromise between the various visions and various points of\nview.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04068v1"
    },
    {
        "title": "A New Pricing Theory That Solves the St. Petersburg Paradox",
        "authors": [
            "Dahang Li"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The St. Petersburg Paradox, an important topic in probability theory, has not\nbeen solved in the last 280 years. Since Nicolaus Bernoulli proposed the St.\nPetersburg Paradox in 1738, many people had tried to solve it and had proposed\nvarious explanations, but all were not satisfactory. In this paper we propose a\nnew pricing theory with several rules, which incidentally resolves this\nparadox. The new pricing theory states that so-called fair (reasonable) pricing\nshould be judged by the seller and the buyer independently. Reasonable pricing\nfor the seller may not be appropriate for the buyer. The seller cares about\ncosts, while the buyer is concerned about the realistic prospect of returns.The\npricing theory we proposed can be applied to financial markets to solve the\nconfusion that financial asset return with fat tails distribution will cause\nthe option pricing formula to fail, thus making up the theoretical defects of\nquantitative financial pricing theory.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07116v4"
    },
    {
        "title": "Criptocurrencies, Fiat Money, Blockchains and Databases",
        "authors": [
            "Jorge Barrera"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Two taxonomies of money that include cryptocurrencies are analyzed. A\ndefinition of the term cryptocurrency is given and a taxonomy of them is\npresented, based on how its price is fixed. The characteristics of the use of\ncurrent fiat money and the operation of two-level banking systems are\ndiscussed. Cryptocurrencies are compared with fiat money and the aspects in\nwhich the latter cannot be overcome are indicated. The characteristics of\nblockchains and databases are described. The possible cases of use of both\ntechnologies are compared, and it is noted that blockchains, in addition to\ncryptocurrencies and certain records, have not yet shown their usefulness,\nwhile databases constitute the foundation of most of the automated systems in\noperation.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.08466v1"
    },
    {
        "title": "Analysing tax evasion dynamics via the Ising model",
        "authors": [
            "Georg Zaklan",
            "Frank Westerhoff",
            "Dietrich Stauffer"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We develop a model of tax evasion based on the Ising model. We augment the\nmodel using an appropriate enforcement mechanism that may allow policy makers\nto curb tax evasion. With a certain probability tax evaders are subject to an\naudit. If they get caught they behave honestly for a certain number of periods.\nSimulating the model for a range of parameter combinations, we show that tax\nevasion may be controlled effectively by using punishment as an enforcement\nmechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.2980v1"
    },
    {
        "title": "Boom and bust in continuous time evolving economic model",
        "authors": [
            "Lawrence Mitchell",
            "G. J. Ackland"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We show that a simple model of a spatially resolved evolving economic system,\nwhich has a steady state under simultaneous updating, shows stable oscillations\nin price when updated asynchronously. The oscillations arise from a gradual\ndecline of the mean price due to competition among sellers competing for the\nsame resource. This lowers profitability and hence population but is followed\nby a sharp rise as speculative sellers invade the large un-inhabited areas.\nThis cycle then begins again.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.3973v2"
    },
    {
        "title": "Emergence of firms in $(d+1)$-dimensional work space",
        "authors": [
            "G. Weisbuch",
            "D. Stauffer",
            "D. Mangalagiu",
            "R. Ben-Av",
            "S. Solomon"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Standard micro-economics concentrate on the description of markets but is\nseldom interested in production. Several economists discussed the concept of a\nfirm, as opposed to an open labour market where entrepreneurs would recrute\nworkers on the occasion of each business opportunity. Coase \\cite{Coase} is one\nof them, who explains the existence of firms as institution because they reduce\nthe transaction costs with respect to an open labour market. Whatever the\nrationale proposed by economists to account for the existence of firms, their\nperspective is based on efficiency and cost analysis. Little attention is paid\nto the dynamics of emergence and evolution of firms. The aim of the present\nmanuscript is to check the global dynamical properties of a very simple model\nbased on bounded rationality and reinforcement learning.\n  Workers and managers are localised on a lattice and they choose collaborators\non the basis of the success of previous work relations. The choice algorithm is\nlargely inspired rom the observation and modeling of long term customer/sellers\nrelationships observed on perishable goods markets discussed in Weisbuch\netal\\cite{Weisbuch} and Nadal etal\\cite{Nadal}. The model presented here is in\nno way an alternative to Coase. We describe the build-up of long term\nrelationships which do reduce transaction costs, and we deduce the dynamical\nproperties of networks built from our simple assumptions.\n  In conclusion, the present model explains the metastability of employment\nrelations in the firm, but something has to be added to it to explain the more\nefficient workload repartition observed in real firms.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.4337v1"
    },
    {
        "title": "A Statistical Test of Walrasian Equilibrium by Means of Complex Networks\n  Theory",
        "authors": [
            "Leonardo Bargigli",
            "Andrea Lionetto",
            "Stefano Viaggiu"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We represent an exchange economy in terms of statistical ensembles for\ncomplex networks by introducing the concept of market configuration. This is\ndefined as a sequence of nonnegative discrete random variables $\\{w_{ij}\\}$\ndescribing the flow of a given commodity from agent $i$ to agent $j$. This\nsequence can be arranged in a nonnegative matrix $W$ which we can regard as the\nrepresentation of a weighted and directed network or digraph $G$. Our main\nresult consists in showing that general equilibrium theory imposes highly\nrestrictive conditions upon market configurations, which are in most cases not\nfulfilled by real markets. An explicit example with reference to the e-MID\ninterbank credit market is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0817v2"
    },
    {
        "title": "Tipping points in macroeconomic Agent-Based models",
        "authors": [
            "Stanislao Gualdi",
            "Marco Tarzia",
            "Francesco Zamponi",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The aim of this work is to explore the possible types of phenomena that\nsimple macroeconomic Agent-Based models (ABM) can reproduce. We propose a\nmethodology, inspired by statistical physics, that characterizes a model\nthrough its 'phase diagram' in the space of parameters. Our first motivation is\nto understand the large macro-economic fluctuations observed in the 'Mark I'\nABM. Our major finding is the generic existence of a phase transition between a\n'good economy' where unemployment is low, and a 'bad economy' where\nunemployment is high. We introduce a simpler framework that allows us to show\nthat this transition is robust against many modifications of the model, and is\ngenerically induced by an asymmetry between the rate of hiring and the rate of\nfiring of the firms. The unemployment level remains small until a tipping\npoint, beyond which the economy suddenly collapses. If the parameters are such\nthat the system is close to this transition, any small fluctuation is amplified\nas the system jumps between the two equilibria. We have explored several\nnatural extensions of the model. One is to introduce a bankruptcy threshold,\nlimiting the leverage of firms. This leads to a rich phase diagram with, in\nparticular, a region where acute endogenous crises occur, during which the\nunemployment rate shoots up before the economy can recover. We also introduce\nsimple wage policies. This leads to inflation (in the 'good' phase) or\ndeflation (in the 'bad' phase), but leaves the overall phase diagram of the\nmodel essentially unchanged. We have also started exploring the effect of\nsimple monetary policies that attempt to contain rising unemployment and defang\ncrises. We end the paper with general comments on the usefulness of ABMs to\nmodel macroeconomic phenomena, in particular in view of the time needed to\nreach a steady state that raises the issue of ergodicity in these models.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.5319v4"
    },
    {
        "title": "Network Topologies of Financial Market During the Global Financial\n  Crisis",
        "authors": [
            "Ashadun Nobi",
            "Seong Eun Maeng",
            "Gyeong Gyun Ha",
            "Jae Woo Lee"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We consider the effects of the global financial crisis through a local Korean\nfinancial market around the 2008 crisis. We analyze 185 individual stock prices\nbelonging to the KOSPI (Korea Composite Stock Price Index), cosidering three\ntime periods: the time before, during, and after the crisis. The complex\nnetworks generate from the fully connected correlation network by using the\ncross-correlation coefficients among the stock price time series of the\ncompanies. We generate the threshold networks (TN), the minimal spanning tees\n(MST), and the hierarchical network (HN) from the fully connected\ncross-correlation networks. By assigning a threshold value of the\ncross-correlation coefficient, we obtain the threshold networks. We observe the\npower law of the degree distribution in the limited range of the threshold. The\ndegree distribution of the largest cluster in the threshold networks during the\ncrisis is fatter than other periods. The clustering coefficient of the\nthreshold networks follows the power law in the scaling range. We also generate\nthe minimal spanning trees from the fully connected correlation networks. The\nMST during the crisis period shrinks in comparison to the periods before and\nafter the crisis. The cophenetic correlation coefficient increases during the\ncrisis, indicating that the hierarchical structure increases during this\nperiod. When the crisis hit the market, the companies behave synchronously and\ntheir correlations become stronger than the normal period.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.6974v1"
    },
    {
        "title": "Physics and Financial Economics (1776-2014): Puzzles, Ising and\n  Agent-Based models",
        "authors": [
            "D. Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  This short review presents a selected history of the mutual fertilization\nbetween physics and economics, from Isaac Newton and Adam Smith to the present.\nThe fundamentally different perspectives embraced in theories developed in\nfinancial economics compared with physics are dissected with the examples of\nthe volatility smile and of the excess volatility puzzle. The role of the Ising\nmodel of phase transitions to model social and financial systems is reviewed,\nwith the concepts of random utilities and the logit model as the analog of the\nBoltzmann factor in statistic physics. Recent extensions in term of quantum\ndecision theory are also covered. A wealth of models are discussed briefly that\nbuild on the Ising model and generalize it to account for the many stylized\nfacts of financial markets. A summary of the relevance of the Ising model and\nits extensions is provided to account for financial bubbles and crashes. The\nreview would be incomplete if it would not cover the dynamical field of agent\nbased models (ABMs), also known as computational economic models, of which the\nIsing-type models are just special ABM implementations. We formulate the\n``Emerging Market Intelligence hypothesis'' to reconcile the pervasive presence\nof ``noise traders'' with the near efficiency of financial markets. Finally, we\nnote that evolutionary biology, more than physics, is now playing a growing\nrole to inspire models of financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.0243v1"
    },
    {
        "title": "Principal wind turbines for a conditional portfolio approach to wind\n  farms",
        "authors": [
            "Vitor V. Lopes",
            "Teresa Scholz",
            "Frank Raischel",
            "Pedro G. Lind"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We introduce a measure for estimating the best risk-return relation of power\nproduction in wind farms within a given time-lag, conditioned to the velocity\nfield. The velocity field is represented by a scalar that weighs the influence\nof the velocity at each wind turbine at present and previous time-steps for the\npresent \"state\" of the wind field. The scalar measure introduced is a linear\ncombination of the few turbines, that most influence the overall power\nproduction. This quantity is then used as the condition for computing a\nconditional expected return and corresponding risk associated to the future\ntotal power output.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.0375v1"
    },
    {
        "title": "Market Efficiency and Government Interventions in Prewar Japanese Rice\n  Futures Markets",
        "authors": [
            "Mikio Ito",
            "Kiyotaka Maeda",
            "Akihiko Noda"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  This study analyzes how colonial rice trade in prewar Japan affected its rice\nmarket, considering several government interventions in the two rice futures\nexchanges in Tokyo and Osaka. We explore the interventions in the futures\nmarkets using two procedures. First, we measure the joint degree of efficiency\nin the markets using a time-varying vector autoregression model. Second, we\nexamine many historical events that possibly affected the markets and focus on\none event at a time. The degree varies over time within our sample period\n(1881-1932). The observation, together with historical analysis, leads to the\nfollowing conclusions. (1) The two major markets in Tokyo and Osaka were nearly\nefficient. (2) Government interventions involving the delivery of imported rice\nfrom Taiwan and Korea often reduced futures market efficiency. Finally, (3)\nthis relationship continued as long as the quality difference between imported\nand domestic rice existed. The government interventions that promoted domestic\ndistributions of the colonial goods resulted in confusion in the commodity\nmarkets, and decreased efficiency of the markets in the metropole.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.1164v9"
    },
    {
        "title": "Directed Random Market: the equilibrium distribution",
        "authors": [
            "Guy Katriel"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We find the explicit expression for the equilibrium wealth distribution of\nthe Directed Random Market process, recently introduced by\nMart\\'inez-Mart\\'inez and L\\'opez-Ruiz, which turns out to be a Gamma\ndistribution with shape parameter $\\frac{1}{2}$. We also prove the convergence\nof the discrete-time process describing the evolution of the distribution of\nwealth to the equilibrium distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4068v1"
    },
    {
        "title": "Expected Cash Flow: A Novel Model Of Evaluating Financial Assets",
        "authors": [
            "Magomet Yandiev"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The present paper provides the basis for a novel financial asset pricing\nmodel that could avoid the shortcomings of, or even completely replace the\ntraditional DCF model. The model is based on Brownian motion logic and expected\nfuture cash flow values. It can be very useful for Islamic Finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4950v1"
    },
    {
        "title": "Predictable markets? A news-driven model of the stock market",
        "authors": [
            "Maxim Gusev",
            "Dimitri Kroujiline",
            "Boris Govorkov",
            "Sergey V. Sharov",
            "Dmitry Ushanov",
            "Maxim Zhilyaev"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We attempt to explain stock market dynamics in terms of the interaction among\nthree variables: market price, investor opinion and information flow. We\npropose a framework for such interaction and apply it to build a model of stock\nmarket dynamics which we study both empirically and theoretically. We\ndemonstrate that this model replicates observed market behavior on all relevant\ntimescales (from days to years) reasonably well. Using the model, we obtain and\ndiscuss a number of results that pose implications for current market theory\nand offer potential practical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7364v2"
    },
    {
        "title": "The gradual evolution of buyer--seller networks and their role in\n  aggregate fluctuations",
        "authors": [
            "Ryohei Hisano",
            "Tsutomu Watanabe",
            "Takayuki Mizuno",
            "Takaaki Ohnishi",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Buyer--seller relationships among firms can be regarded as a longitudinal\nnetwork in which the connectivity pattern evolves as each firm receives\nproductivity shocks. Based on a data set describing the evolution of\nbuyer--seller links among 55,608 firms over a decade and structural equation\nmodeling, we find some evidence that interfirm networks evolve reflecting a\nfirm's local decisions to mitigate adverse effects from neighbor firms through\ninterfirm linkage, while enjoying positive effects from them. As a result, link\nrenewal tends to have a positive impact on the growth rates of firms. We also\ninvestigate the role of networks in aggregate fluctuations.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.00236v3"
    },
    {
        "title": "Markets, herding and response to external information",
        "authors": [
            "Adrián Carro",
            "Raúl Toral",
            "Maxi San Miguel"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We focus on the influence of external sources of information upon financial\nmarkets. In particular, we develop a stochastic agent-based market model\ncharacterized by a certain herding behavior as well as allowing traders to be\ninfluenced by an external dynamic signal of information. This signal can be\ninterpreted as a time-varying advertising, public perception or rumor, in favor\nor against one of two possible trading behaviors, thus breaking the symmetry of\nthe system and acting as a continuously varying exogenous shock. As an\nillustration, we use a well-known German Indicator of Economic Sentiment as\ninformation input and compare our results with Germany's leading stock market\nindex, the DAX, in order to calibrate some of the model parameters. We study\nthe conditions for the ensemble of agents to more accurately follow the\ninformation input signal. The response of the system to the external\ninformation is maximal for an intermediate range of values of a market\nparameter, suggesting the existence of three different market regimes:\namplification, precise assimilation and undervaluation of incoming information.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03708v2"
    },
    {
        "title": "Universal Laws of Human Society's Income Distribution",
        "authors": [
            "Yong Tao"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  General equilibrium equations in economics play the same role with many-body\nNewtonian equations in physics. Accordingly, each solution of the general\nequilibrium equations can be regarded as a possible microstate of the economic\nsystem. Since Arrow's Impossibility Theorem and Rawls' principle of social\nfairness will provide a powerful support for the hypothesis of equal\nprobability, then the principle of maximum entropy is available in a just and\nequilibrium economy so that an income distribution will occur spontaneously\n(with the largest probability). Remarkably, some scholars have observed such an\nincome distribution in some democratic countries, e.g. USA. This result implies\nthat the hypothesis of equal probability may be only suitable for some \"fair\"\nsystems (economic or physical systems). From this meaning, the non-equilibrium\nsystems may be \"unfair\" so that the hypothesis of equal probability is\nunavailable.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.05418v1"
    },
    {
        "title": "The double role of GDP in shaping the structure of the International\n  Trade Network",
        "authors": [
            "Assaf Almog",
            "Tiziano Squartini",
            "Diego Garlaschelli"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The International Trade Network (ITN) is the network formed by trade\nrelationships between world countries. The complex structure of the ITN impacts\nimportant economic processes such as globalization, competitiveness, and the\npropagation of instabilities. Modeling the structure of the ITN in terms of\nsimple macroeconomic quantities is therefore of paramount importance. While\ntraditional macroeconomics has mainly used the Gravity Model to characterize\nthe magnitude of trade volumes, modern network theory has predominantly focused\non modeling the topology of the ITN. Combining these two complementary\napproaches is still an open problem. Here we review these approaches and\nemphasize the double role played by GDP in empirically determining both the\nexistence and the volume of trade linkages. Moreover, we discuss a unified\nmodel that exploits these patterns and uses only the GDP as the relevant\nmacroeconomic factor for reproducing both the topology and the link weights of\nthe ITN.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.02454v2"
    },
    {
        "title": "The structural constraints of income inequality in Latin America",
        "authors": [
            "Dominik Hartmann",
            "Cristian Jara-Figueroa",
            "Miguel Guevara",
            "Alex Simoes",
            "César A. Hidalgo"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Recent work has shown that a country's productive structure constrains its\nlevel of economic growth and income inequality. Here, we compare the productive\nstructure of countries in Latin America and the Caribbean (LAC) with that of\nChina and other High-Performing Asian Economies (HPAE) to expose the increasing\ngap in their productive capabilities. Moreover, we use the product space and\nthe Product Gini Index to reveal the structural constraints on income\ninequality. Our network maps reveal that HPAE have managed to diversify into\nproducts typically produced by countries with low levels of income inequality,\nwhile LAC economies have remained dependent on products related to high levels\nof income inequality. We also introduce the Xgini, a coefficient that captures\nthe constraints on income inequality imposed by the mix of products a country\nmakes. Finally, we argue that LAC countries need to emphasize a smart\ncombination of social and economic policies to overcome the structural\nconstraints for inclusive growth.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.03770v2"
    },
    {
        "title": "Revenue Forecasting for Enterprise Products",
        "authors": [
            "Amita Gajewar",
            "Gagan Bansal"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  For any business, planning is a continuous process, and typically\nbusiness-owners focus on making both long-term planning aligned with a\nparticular strategy as well as short-term planning that accommodates the\ndynamic market situations. An ability to perform an accurate financial forecast\nis crucial for effective planning. In this paper, we focus on providing an\nintelligent and efficient solution that will help in forecasting revenue using\nmachine learning algorithms. We experiment with three different revenue\nforecasting models, and here we provide detailed insights into the methodology\nand their relative performance measured on real finance data. As a real-world\napplication of our models, we partner with Microsoft's Finance organization\n(department that reports Microsoft's finances) to provide them a guidance on\nthe projected revenue for upcoming quarters.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06624v1"
    },
    {
        "title": "Supply based on demand dynamical model",
        "authors": [
            "Asaf Levi",
            "Juan Sabuco",
            "Miguel A. F. Sanjuan"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  We propose and analyze numerically a simple dynamical model that describes\nthe firm behaviors under uncertainty of demand forecast. Iterating this simple\nmodel and varying some parameters values we observe a wide variety of market\ndynamics such as equilibria, periodic and chaotic behaviors. Interestingly the\nmodel is also able to reproduce market collapses.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.07333v1"
    },
    {
        "title": "Sentiment-Driven Stochastic Volatility Model: A High-Frequency Textual\n  Tool for Economists",
        "authors": [
            "Jozef Barunik",
            "Cathy Yi-Hsuan Chen",
            "Jan Vecer"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We propose how to quantify high-frequency market sentiment using\nhigh-frequency news from NASDAQ news platform and support vector machine\nclassifiers. News arrive at markets randomly and the resulting news sentiment\nbehaves like a stochastic process. To characterize the joint evolution of\nsentiment, price, and volatility, we introduce a unified continuous-time\nsentiment-driven stochastic volatility model. We provide closed-form formulas\nfor moments of the volatility and news sentiment processes and study the news\nimpact. Further, we implement a simulation-based method to calibrate the\nparameters. Empirically, we document that news sentiment raises the threshold\nof volatility reversion, sustaining high market volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.00059v1"
    },
    {
        "title": "Trading in Complex Networks",
        "authors": [
            "Felipe M. Cardoso",
            "Carlos Gracia-Lazaro",
            "Frederic Moisan",
            "Sanjeev Goyal",
            "Angel Sanchez",
            "Yamir Moreno"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Global supply networks in agriculture, manufacturing, and services are a\ndefining feature of the modern world. The efficiency and the distribution of\nsurpluses across different parts of these networks depend on choices of\nintermediaries. This paper conducts price formation experiments with human\nsubjects located in large complex networks to develop a better understanding of\nthe principles governing behavior. Our first finding is that prices are larger\nand that trade is significantly less efficient in small-world networks as\ncompared to random networks. Our second finding is that location within a\nnetwork is not an important determinant of pricing. An examination of the price\ndynamics suggests that traders on cheapest -- and hence active -- paths raise\nprices while those off these paths lower them. We construct an agent-based\nmodel (ABM) that embodies this rule of thumb. Simulations of this ABM yield\nmacroscopic patterns consistent with the experimental findings. Finally, we\nextrapolate the ABM on to significantly larger random and small world networks\nand find that network topology remains a key determinant of pricing and\nefficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.01531v1"
    },
    {
        "title": "The temporal evolution of venture investment strategies in sector space",
        "authors": [
            "Theophile Carniel",
            "Clement Gastaud",
            "Jean-Michel Dalle"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We analyze the sectoral dynamics of startup venture financing. Based on a\ndataset of 52000 start-ups and 110000 funding rounds in the United States from\n2000 to 2017, and by applying both Principal Component Analysis (PCA) and\nTensor Component Analysis (TCA) in sector space, we visualize and measure the\nevolution of the investment strategies of different classes of investors across\nsectors and over time. During the past decade, we observe a coherent evolution\nof early stage investments towards a lower-tech area in sector space,\nassociated with a marked increase in the concentration of investments and with\nthe emergence of a newer class of investors called accelerators. We provide\nevidence for a more recent shift of start-up venture financing away from the\nprevious one.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.01980v1"
    },
    {
        "title": "Learning from Others in the Financial Market",
        "authors": [
            "Matthias Feiler",
            "Thibaut Ajdler"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Prediction problems in finance go beyond estimating the unknown parameters of\na model (e.g. of expected returns). This is because such a model would have to\ninclude parameters governing the market participants' propensity to change\ntheir opinions on the validity of that model. This leads to a well--known\ncircular situation characteristic of financial markets, where participants\ncollectively create the future they wish to estimate. In this paper, we\nintroduce a framework for organizing multiple expectation models and study the\nconditions under which they are adopted by a majority of market participants.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03201v2"
    },
    {
        "title": "Invoice Financing of Supply Chains with Blockchain technology and\n  Artificial Intelligence",
        "authors": [
            "Sandra Johnson",
            "Peter Robinson",
            "Kishore Atreya",
            "Claudio Lisco"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Supply chains lend themselves to blockchain technology, but certain\nchallenges remain, especially around invoice financing. For example, the\nfurther a supplier is removed from the final consumer product, the more\ndifficult it is to get their invoices financed. Moreover, for competitive\nreasons, retailers and manufacturers do not want to disclose their supply\nchains. However, upstream suppliers need to prove that they are part of a\n`stable' supply chain to get their invoices financed, which presents the\nupstream suppliers with huge, and often unsurmountable, obstacles to get the\nnecessary finance to fulfil the next order, or to expand their business. Using\na fictitious supply chain use case, which is based on a real world use case, we\ndemonstrate how these challenges have the potential to be solved by combining\nmore advanced and specialised blockchain technologies with other technologies\nsuch as Artificial Intelligence. We describe how atomic crosschain\nfunctionality can be utilised across private blockchains to retrieve the\ninformation required for an invoice financier to make informed decisions under\nuncertainty, and consider the effect this decision has on the overall stability\nof the supply chain.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03306v1"
    },
    {
        "title": "Learned Sectors: A fundamentals-driven sector reclassification project",
        "authors": [
            "Rukmal Weerawarana",
            "Yiyi Zhu",
            "Yuzhen He"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Market sectors play a key role in the efficient flow of capital through the\nmodern Global economy. We analyze existing sectorization heuristics, and\nobserve that the most popular - the GICS (which informs the S&P 500), and the\nNAICS (published by the U.S. Government) - are not entirely quantitatively\ndriven, but rather appear to be highly subjective and rooted in dogma. Building\non inferences from analysis of the capital structure irrelevance principle and\nthe Modigliani-Miller theoretic universe conditions, we postulate that\ncorporation fundamentals - particularly those components specific to the\nModigliani-Miller universe conditions - would be optimal descriptors of the\ntrue economic domain of operation of a company. We generate a set of potential\ncandidate learned sector universes by varying the linkage method of a\nhierarchical clustering algorithm, and the number of resulting sectors derived\nfrom the model (ranging from 5 to 19), resulting in a total of 60 candidate\nlearned sector universes. We then introduce reIndexer, a backtest-driven sector\nuniverse evaluation research tool, to rank the candidate sector universes\nproduced by our learned sector classification heuristic. This rank was utilized\nto identify the risk-adjusted return optimal learned sector universe as being\nthe universe generated under CLINK (i.e. complete linkage), with 17 sectors.\nThe optimal learned sector universe was tested against the benchmark GICS\nclassification universe with reIndexer, outperforming on both absolute\nportfolio value, and risk-adjusted return over the backtest period. We conclude\nthat our fundamentals-driven Learned Sector classification heuristic provides a\nsuperior risk-diversification profile than the status quo classification\nheuristic.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03935v1"
    },
    {
        "title": "The Coevolution of Banks and Corporate Securities Markets: The Financing\n  of Belgium's Industrial Take-Off in the 1830s",
        "authors": [
            "Stefano Ugolini"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Recent developments in the literature on financial architecture suggest that\nbanks and markets not only coexist, but also coevolve in ways that are\nnon-neutral from the viewpoint of optimality. This article aims to analyse the\nconcrete mechanisms of this coevolution by focusing on a very relevant case\nstudy: Belgium (the first Continental country to industrialize) at the time of\nthe very first emergence of a modern financial system (the 1830s). The article\nshows that intermediaries played a crucial role in developing secondary\nsecurities markets (as banks acted as securitizers), but market conditions also\nhad a strong feedback on banks' balance sheets and activities (as banks also\nacted as market-makers for the securities they had issued). The findings\nsuggest that not only structural, but also cyclical factors can be important\ndeterminants of changes in financial architecture.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11023v1"
    },
    {
        "title": "Deep Learning, Predictability, and Optimal Portfolio Returns",
        "authors": [
            "Mykola Babiak",
            "Jozef Barunik"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We study dynamic portfolio choice of a long-horizon investor who uses deep\nlearning methods to predict equity returns when forming optimal portfolios. Our\nresults show statistically and economically significant benefits from using\ndeep learning to form optimal portfolios through certainty equivalent returns\nand Sharpe ratios. We demonstrate that a long-short-term-memory recurrent\nneural network, which excels in learning complex time-series dependencies,\ngenerates a superior performance among a variety of networks considered. Return\npredictability via deep learning generates substantially improved portfolio\nperformance across different subsamples, particularly during recessionary\nperiods. These gains are robust to including transaction costs, short-selling\nand borrowing constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03394v3"
    },
    {
        "title": "Marxism, Logic and the Rate of Profit",
        "authors": [
            "Robin Hirsch"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  It is argued that Marxism, being based on contradictions, is an illogical\nmethod. More specifically, we present a rejection of Marx's thesis that the\nrate of profit has a long-term tendency to fall.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08269v6"
    },
    {
        "title": "International Trade Finance from the Origins to the Present: Market\n  Structures, Regulation and Governance",
        "authors": [
            "Olivier Accominotti",
            "Stefano Ugolini"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This chapter presents a history of international trade finance - the oldest\ndomain of international finance - from its emergence in the Middle Ages up to\ntoday. We describe how the structure and governance of the global trade finance\nmarket changed over time and how trade credit instruments evolved. Trade\nfinance products initially consisted of idiosyncratic assets issued by local\nmerchants and bankers. The financing of international trade then became\nincreasingly centralized and credit instruments were standardized through the\ndiffusion of the local standards of consecutive leading trading centres\n(Antwerp, Amsterdam, London). This process of market centralization/product\nstandardization culminated in the nineteenth century when London became the\nglobal centre for international trade finance and the sterling bill of exchange\nemerged as the most widely used trade finance instrument. The structure of the\ntrade finance market then evolved considerably following the First World War\nand disintegrated during the interwar de-globalization and Bretton Woods\nperiod. The reconstruction of global trade finance in the post-1970 period gave\nway to the decentralized market structure that prevails nowadays.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.08668v1"
    },
    {
        "title": "Ants, robots, humans: a self-organizing, complex systems modeling\n  approach",
        "authors": [
            "Martin Jaraiz"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Most of the grand challenges of humanity today involve complex agent-based\nsystems, such as epidemiology, economics or ecology. However, remains as a\npending task the challenge of identifying the general principles underlying\ntheir self-organizing capabilities. This article presents a novel modeling\napproach, capable to self-deploy both the system structure and the activities\nfor goal-driven agents that can take appropriate actions to achieve their\ngoals. Humans, robots, and animals are all endowed with this type of behavior.\nSelf-organization is shown to emerge from the decisions of a common rational\nactivity algorithm, based on the information of a system-specific goals\ndependency network. The unique self-deployment feature of this approach, that\ncan also be applied to non-goal-driven agents, can boost considerably the range\nand depth of application of agent-based modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.10823v2"
    },
    {
        "title": "Pareto's 80/20 Rule and the Gaussian Distribution",
        "authors": [
            "Katsuaki Tanabe"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The statistical state for the empirical Pareto's 80/20 rule has been found to\ncorrespond to a normal or Gaussian distribution with a standard deviation that\nis twice the mean. This finding represents large characteristic variations in\nour society and nature. In this distribution, the rule can be also referred to\nas, for example, the 25/5, 45/10, 60/15, or 90/25 rule. In addition, our result\nsuggests the existence of implicit negative contributors.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.14378v1"
    },
    {
        "title": "Regression to the Tail: Why the Olympics Blow Up",
        "authors": [
            "Bent Flyvbjerg",
            "Alexander Budzier",
            "Daniel Lunn"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The Olympic Games are the largest, highest-profile, and most expensive\nmegaevent hosted by cities and nations. Average sports-related costs of hosting\nare $12.0 billion. Non-sports-related costs are typically several times that.\nEvery Olympics since 1960 has run over budget, at an average of 172 percent in\nreal terms, the highest overrun on record for any type of megaproject. The\npaper tests theoretical statistical distributions against empirical data for\nthe costs of the Games, in order to explain the cost risks faced by host cities\nand nations. It is documented, for the first time, that cost and cost overrun\nfor the Games follow a power-law distribution. Olympic costs are subject to\ninfinite mean and variance, with dire consequences for predictability and\nplanning. We name this phenomenon \"regression to the tail\": it is only a matter\nof time until a new extreme event occurs, with an overrun larger than the\nlargest so far, and thus more disruptive and less plannable. The generative\nmechanism for the Olympic power law is identified as strong convexity prompted\nby six causal drivers: irreversibility, fixed deadlines, the Blank Check\nSyndrome, tight coupling, long planning horizons, and an Eternal Beginner\nSyndrome. The power law explains why the Games are so difficult to plan and\nmanage successfully, and why cities and nations should think twice before\nbidding to host. Based on the power law, two heuristics are identified for\nbetter decision making on hosting. Finally, the paper develops measures for\ngood practice in planning and managing the Games, including how to mitigate the\nextreme risks of the Olympic power law.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.14682v1"
    },
    {
        "title": "The Effect of Growth On Equality in Models of the Economy",
        "authors": [
            "Kang Liu",
            "N. Lubbers",
            "W. Klein",
            "J. Tobochnik",
            "B. Boghosian",
            "Harvey Gould"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We investigate the relation between economic growth and equality in a\nmodified version of the agent-based asset exchange model (AEM). The modified\nmodel is a driven system that for a range of parameter space is effectively\nergodic in the limit of an infinite system. We find that the belief that \"a\nrising tide lifts all boats\" does not always apply, but the effect of growth on\nthe wealth distribution depends on the nature of the growth. In particular, we\nfind that the rate of growth, the way the growth is distributed, and the\npercentage of wealth exchange determine the degree of equality. We find strong\nnumerical evidence that there is a phase transition in the modified model, and\nfor a part of parameter space the modified AEM acts like a geometric random\nwalk.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0794v1"
    },
    {
        "title": "Economics 2.0: The Natural Step towards A Self-Regulating, Participatory\n  Market Society",
        "authors": [
            "Dirk Helbing"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Despite all our great advances in science, technology and financial\ninnovations, many societies today are struggling with a financial, economic and\npublic spending crisis, over-regulation, and mass unemployment, as well as lack\nof sustainability and innovation. Can we still rely on conventional economic\nthinking or do we need a new approach?\n  I argue that, as the complexity of socio-economic systems increases,\nnetworked decision-making and bottom-up self-regulation will be more and more\nimportant features. It will be explained why, besides the \"homo economicus\"\nwith strictly self-regarding preferences, natural selection has also created a\n\"homo socialis\" with other-regarding preferences. While the \"homo economicus\"\noptimizes the own prospects in separation, the decisions of the \"homo socialis\"\nare self-determined, but interconnected, a fact that may be characterized by\nthe term \"networked minds\". Notably, the \"homo socialis\" manages to earn higher\npayoffs than the \"homo economicus\".\n  I show that the \"homo economicus\" and the \"homo socialis\" imply a different\nkind of dynamics and distinct aggregate outcomes. Therefore, next to the\ntraditional economics for the \"homo economicus\" (\"economics 1.0\"), a\ncomplementary theory must be developed for the \"homo socialis\". This economic\ntheory might be called \"economics 2.0\" or \"socionomics\". The names are\njustified, because the Web 2.0 is currently promoting a transition to a new\nmarket organization, which benefits from social media platforms and could be\ncharacterized as \"participatory market society\". To thrive, the \"homo socialis\"\nrequires suitable institutional settings such a particular kinds of reputation\nsystems, which will be sketched in this paper. I also propose a new kind of\nmoney, so-called \"qualified money\", which may overcome some of the problems of\nour current financial system.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.4078v2"
    },
    {
        "title": "Socio-economic inequality and prospects of institutional Econophysics",
        "authors": [
            "Arnab Chatterjee",
            "Asim Ghosh",
            "Bikas K Chakrabarti"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Socio-economic inequality is measured using various indices. The Gini ($g$)\nindex, giving the overall inequality is the most commonly used, while the\nrecently introduced Kolkata ($k$) index gives a measure of $1-k$ fraction of\npopulation who possess top $k$ fraction of wealth in the society. This article\nreviews the character of such inequalities, as seen from a variety of data\nsources, the apparent relationship between the two indices, and what toy models\ntell us. These socio-economic inequalities are also investigated in the context\nof man-made social conflicts or wars, as well as in natural disasters. Finally,\nwe forward a proposal for an international institution with sufficient fund for\nvisitors, where natural and social scientists from various institutions of the\nworld can come to discuss, debate and formulate further developments.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.00723v2"
    },
    {
        "title": "Analysis of Price and Income Elasticities of Energy Demand in Ecuador: A\n  Dynamic OLS Approach",
        "authors": [
            "Kathia Pinzón"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Energy consumption in Ecuador has increased significantly during the last\ndecades, affecting negatively the financial position of the country since large\nenergy consumption subsidies are provided in its internal market and Ecuador is\nmostly a crude oil exporter and oil derivatives importer country. This research\nseeks to state the long run price and income elasticities of energy demand in\nEcuador, by analyzing information spanning the period from 1970 to 2015. A\ncointegration analysis and an estimation by using a Dynamic Ordinary Least\nSquares approach considering structural breaks is carried out. Results obtained\nare robust and suggest that in the long run energy demand in Ecuador is highly\nincome elastic, has no relationship with its price and has an almost unitary\nbut inverse relationship with the industrial production level. Conclusions and\neconomic policy suggestions are also provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.05288v1"
    },
    {
        "title": "A Model of Synchronization for Self-Organized Crowding Behavior",
        "authors": [
            "Jake J. Xia"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  This paper proposes a general model for synchronized crowding behavior. An\norder parameter is introduced to quantify the level of synchronization which is\nshown a function of percentage of agents in reactive state. Further,\nsynchronization is shown to be driven by the most active agents with the\nhighest volatility. A tipping point is identified when crowd becomes\nself-amplifying and unstable. By applying this model, financial bubbles, market\nmomentum and volatility patterns are simulated.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.01132v1"
    },
    {
        "title": "Extreme prices in electricity balancing markets from an approach of\n  statistical physics",
        "authors": [
            "Mario Mureddu",
            "Hildegard Meyer-Ortmanns"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  An increase in energy production from renewable energy sources is viewed as a\ncrucial achievement in most industrialized countries.\n  The higher variability of power production via renewables leads to a rise in\nancillary service costs over the power system, in particular costs within the\nelectricity balancing markets, mainly due to an increased number of extreme\nprice spikes. This study focuses on forecasting the behavior of price and\nvolumes of the Italian balancing market in the presence of an increased share\nof renewable energy sources. Starting from configurations of load and power\nproduction, which guarantee a stable performance, we implement fluctuations in\nthe load and in renewables; in particular we artificially increase the\ncontribution of renewables as compared to conventional power sources to cover\nthe total load. We then forecast the amount of provided energy in the balancing\nmarket and its fluctuations, which are induced by production and consumption.\nWithin an approach of agent based modeling we estimate the resulting energy\nprices and costs. While their average values turn out to be only slightly\naffected by an increased contribution from renewables, the probability for\nextreme price events is shown to increase along with undesired peaks in the\ncosts.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.05525v1"
    },
    {
        "title": "The Blockchain: A Gentle Four Page Introduction",
        "authors": [
            "Jan Hendrik Witte"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Blockchain is a distributed database that keeps a chronologically-growing\nlist (chain) of records (blocks) secure from tampering and revision. While\ncomputerisation has changed the nature of a ledger from clay tables in the old\ndays to digital records in modern days, blockchain technology is the first true\ninnovation in record keeping that could potentially revolutionise the basic\nprinciples of information keeping. In this note, we provide a brief\nself-contained introduction to how the blockchain works.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.06244v1"
    },
    {
        "title": "Understanding the Impacts of Dark Pools on Price Discovery",
        "authors": [
            "Linlin Ye"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  This paper investigates the impact of dark pools on price discovery (the\nefficiency of prices on stock exchanges to aggregate information). Assets are\ntraded in either an exchange or a dark pool, with the dark pool offering better\nprices but lower execution rates. Informed traders receive noisy and\nheterogeneous signals about an asset's fundamental. We find that informed\ntraders use dark pools to mitigate their information risk and there is a\nsorting effect: in equilibrium, traders with strong signals trade in exchanges,\ntraders with moderate signals trade in dark pools, and traders with weak\nsignals do not trade. As a result, dark pools have an amplification effect on\nprice discovery. That is, when information precision is high (information risk\nis low), the majority of informed traders trade in the exchange hence adding a\ndark pool enhances price discovery, whereas when information precision is low\n(information risk is high), the majority of the informed traders trade in the\ndark pool hence adding a dark pool impairs price discovery. The paper\nreconciles the conflicting empirical evidence and produces novel empirical\npredictions. The paper also provides regulatory suggestions with dark pools on\ncurrent equity markets and in emerging markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08486v1"
    },
    {
        "title": "Is VIX still the investor fear gauge? Evidence for the US and BRIC\n  markets",
        "authors": [
            "Marco Neffelli",
            "Marina Resta"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We investigate the relationships of the VIX with US and BRIC markets. In\ndetail, we pick up the analysis from the point left off by (Sarwar, 2012), and\nwe focus on the period: Jan 2007 - Feb 2018, thus capturing the relations\nbefore, during and after the 2008 financial crisis. Results pinpoint frequent\nstructural breaks in the VIX and suggest an enhancement around 2008 of the fear\ntransmission in response to negative market moves; largely depending on\noverlaps in trading hours, this has become even stronger post-crisis for the\nUS, while for BRIC countries has gone back towards pre-crisis levels.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07556v2"
    },
    {
        "title": "The evolving networks of debtor-creditor relationships with addition and\n  deletion of nodes: a case of P2P lending",
        "authors": [
            "Lin Chen",
            "Ping Li",
            "Qiang Li"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  P2P lending activities have grown rapidly and have caused the huge and\ncomplex networks of debtor-creditor relationships. The aim of this study was to\nstudy the underlying structural characteristics of networks formed by\ndebtor-creditor relationships. According attributes of P2P lending, this paper\nmodel the networks of debtor-creditor relationships as an evolving networks\nwith addition and deletion of nodes. It was found that networks of\ndebtor-creditor relationships are scale-free networks. Moreover, the exponent\nof power-law was calculated by an empirical study. In addition, this paper\nstudy what factors impact on the exponent of power-law besides the number of\nnodes. It was found that the both interest rate and term have significantly\ninfluence on the exponent of power-law. Interest rate is negatively correlated\nwith the exponent of power-law and term is positively correlated with the\nexponent of power-law. Our results enriches the application of complex networks\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07829v1"
    },
    {
        "title": "Divestment may burst the carbon bubble if investors' beliefs tip to\n  anticipating strong future climate policy",
        "authors": [
            "Birte Ewers",
            "Jonathan F. Donges",
            "Jobst Heitzig",
            "Sonja Peterson"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  To achieve the ambitious aims of the Paris climate agreement, the majority of\nfossil-fuel reserves needs to remain underground. As current national\ngovernment commitments to mitigate greenhouse gas emissions are insufficient by\nfar, actors such as institutional and private investors and the social movement\non divestment from fossil fuels could play an important role in putting\npressure on national governments on the road to decarbonization. Using a\nstochastic agent-based model of co-evolving financial market and investors'\nbeliefs about future climate policy on an adaptive social network, here we find\nthat the dynamics of divestment from fossil fuels shows potential for social\ntipping away from a fossil-fuel based economy. Our results further suggest that\nsocially responsible investors have leverage: a small share of 10--20\\,\\% of\nsuch moral investors is sufficient to initiate the burst of the carbon bubble,\nconsistent with the Pareto Principle. These findings demonstrate that\ndivestment has potential for contributing to decarbonization alongside other\nsocial movements and policy instruments, particularly given the credible\nimminence of strong international climate policy. Our analysis also indicates\nthe possible existence of a carbon bubble with potentially destabilizing\neffects to the economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.07481v1"
    },
    {
        "title": "Quantum model for price forecasting in financial markets",
        "authors": [
            "J. L. Subias"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The present paper describes a practical example in which the probability\ndistribution of the prices of a stock market blue chip is calculated as the\nwave function of a quantum particle confined in a potential well. This model\nmay naturally explain the operation of several empirical rules used by\ntechnical analysts. Models based on the movement of a Brownian particle do not\naccount for fundamental aspects of financial markets. This is due to the fact\nthat the Brownian particle is a classical particle, while stock market prices\nbehave more like quantum particles. When a classical particle meets an obstacle\nor a potential barrier, it may either bounce or overcome the obstacle, yet not\nboth at a time. Only a quantum particle can simultaneously reflect and transmit\nitself on a potential barrier. This is precisely what prices in a stock market\nimitate when they find a resistance level: they partially bounce against and\npartially overcome it. This can only be explained by admitting that prices\nbehave as quantum rather than as classic particles. The proposed quantum model\nfinds natural justification not only for the aforementioned facts but also for\nother empirically well-known facts such as sudden changes in volatility,\nnon-Gaussian distribution in prices, among others.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10502v1"
    },
    {
        "title": "Three Different Ways Synchronization Can Cause Contagion in Financial\n  Markets",
        "authors": [
            "Naji Massad",
            "Jørgen Vitting Andersen"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We introduce tools to capture the dynamics of three different pathways, in\nwhich the synchronization of human decision-making could lead to turbulent\nperiods and contagion phenomena in financial markets. The first pathway is\ncaused when stock market indices, seen as a set of coupled integrate-and-fire\noscillators, synchronize in frequency. The integrate-and-fire dynamics happens\ndue to change blindness, a trait in human decision-making where people have the\ntendency to ignore small changes, but take action when a large change happens.\nThe second pathway happens due to feedback mechanisms between market\nperformance and the use of certain (decoupled) trading strategies. The third\npathway occurs through the effects of communication and its impact on human\ndecision-making. A model is introduced in which financial market performance\nhas an impact on decision-making through communication between people.\nConversely, the sentiment created via communication has an impact on financial\nmarket performance. The methodologies used are: agent based modeling, models of\nintegrate-and-fire oscillators, and communication models of human\ndecision-making\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10800v1"
    },
    {
        "title": "Dynamic model of firms competitive interaction on the market with\n  taxation",
        "authors": [
            "Oleg Malafeyev",
            "Eduard Abramyan",
            "Andrey Shulga"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In this article three models of firms interaction on the market are\ndescribed. One of these models is described by using a differential equation\nand by Lotka-Volterra model, where the equation has a different form. Also,\nthere are models of non-competing and competing firms. The article presents an\nalgorithm for solving the interaction of competing firms in taxation and the\ncalculation of a compromise point. Besides, the article presents a compromise\nbetween the interests of a state and an enterprise.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.06364v1"
    },
    {
        "title": "Monetary Stabilization in Cryptocurrencies - Design Approaches and Open\n  Questions",
        "authors": [
            "Ingolf G. A. Pernice",
            "Sebastian Henningsen",
            "Roman Proskalovich",
            "Martin Florian",
            "Hermann Elendner",
            "Björn Scheuermann"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The price volatility of cryptocurrencies is often cited as a major hindrance\nto their wide-scale adoption. Consequently, during the last two years, multiple\nso called stablecoins have surfaced---cryptocurrencies focused on maintaining\nstable exchange rates. In this paper, we systematically explore and analyze the\nstablecoin landscape. Based on a survey of 24 specific stablecoin projects, we\ngo beyond individual coins for extracting general concepts and approaches. We\ncombine our findings with learnings from classical monetary policy, resulting\nin a comprehensive taxonomy of cryptocurrency stabilization. We use our\ntaxonomy to highlight the current state of development from different\nperspectives and show blank spots. For instance, while over 91% of projects\npromote 1-to-1 stabilization targets to external assets, monetary policy\nliterature suggests that the smoothing of short term volatility is often a more\nsustainable alternative. Our taxonomy bridges computer science and economics,\nfostering the transfer of expertise. For example, we find that 38% of the\nreviewed projects use a combination of exchange rate targeting and specific\nstabilization techniques that can render them vulnerable to speculative\neconomic attacks - an avoidable design flaw.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.11905v1"
    },
    {
        "title": "The Network Effect in Credit Concentration Risk",
        "authors": [
            "Davide Cellai",
            "Trevor Fitzpatrick"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Measurement and management of credit concentration risk is critical for banks\nand relevant for micro-prudential requirements. While several methods exist for\nmeasuring credit concentration risk within institutions, the systemic effect of\ndifferent institutions' exposures to the same counterparties has been less\nexplored so far. In this paper, we propose a measure of the systemic credit\nconcentration risk that arises because of common exposures between different\ninstitutions within a financial system. This approach is based on a network\nmodel that describes the effect of overlapping portfolios. This network metric\nis applied to synthetic and real world data to illustrate that the effect of\ncommon exposures is not fully reflected in single portfolio concentration\nmeasures. It also allows to quantify several aspects of the interplay between\ninterconnectedness and credit risk. Using this network measure, we formulate an\nanalytical approximation for the additional capital requirement corresponding\nto the systemic risk arising from credit concentration interconnectedness. Our\nmethodology also avoids double counting between the granularity adjustment and\nthe common exposure adjustment. Although approximated, this common exposure\nadjustment is able to capture, with only two parameters, an aspect of systemic\nrisk that can extend single portfolios view to a system-wide one.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.13711v2"
    },
    {
        "title": "Stylized Facts and Agent-Based Modeling",
        "authors": [
            "Simon Cramer",
            "Torsten Trimborn"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The existence of stylized facts in financial data has been documented in many\nstudies. In the past decade the modeling of financial markets by agent-based\ncomputational economic market models has become a frequently used modeling\napproach. The main purpose of these models is to replicate stylized facts and\nto identify sufficient conditions for their creations. In this paper we\nintroduce the most prominent examples of stylized facts and especially present\nstylized facts of financial data. Furthermore, we given an introduction to\nagent-based modeling. Here, we not only provide an overview of this topic but\nintroduce the idea of universal building blocks for agent-based economic market\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.02684v1"
    },
    {
        "title": "A Dynamic MST- deltaCovar Model Of Systemic Risk In The European\n  Insurance Sector",
        "authors": [
            "Anna Denkowska",
            "Stanisław Wanat"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This work is an answer to the EIOPA 2017 report. It follows from the latter\nthat in order to assess the potential systemic risk we should take into account\nthe build-up of risk and in particular the risk that arises in time, as well as\nthe interlinkages in the financial sector and the whole economy. Our main tools\nused to analyse the systemic risk dynamics in the European insurance sector\nduring the years 2005-2019 are the topological indices of minimum spanning\ntrees (MST) and the deltaCoVaR measure. We address the following questions: 1)\nWhat is the contribution to systemic risk of each of the 28 largest European\ninsurance companies whose list includes also those appearing on the G-SIIs\nlist? 2) Does the analysis of the deltaCoVaR of those 28 insurance companies\nand the conclusions we draw agree with the our claims from our latest article\n[Wanat S., Denkowska A. 2019]. In clear: does the most important contribution\nto systemic risk come from the companies that have the highest betweenness\ncentrality or the highest degree in the MST obtained?\n",
        "pdf_link": "http://arxiv.org/pdf/1912.05641v1"
    },
    {
        "title": "Model uncertainty in financial forecasting",
        "authors": [
            "Matthias J. Feiler",
            "Thibaut Ajdler"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Models necessarily capture only parts of a reality. Prediction models aim at\ncapturing a future reality. In this paper we address the question of how the\nfuture is constructed (or: imagined) in an investment context where market\nparticipants form expectations on the returns of a risky investment. We observe\nthat the participants' model choices are subject to unforeseeable change. The\nobjective of the paper is to demonstrate that the resulting uncertainty may be\nreduced by incorporating relations among competing models in the estimation\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10813v1"
    },
    {
        "title": "Competitive equilibria between staking and on-chain lending",
        "authors": [
            "Tarun Chitra"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Proof of Stake (PoS) is a burgeoning Sybil resistance mechanism that aims to\nhave a digital asset (\"token\") serve as security collateral in crypto networks.\nHowever, PoS has so far eluded a comprehensive threat model that encompasses\nboth Byzantine attacks from distributed systems and financial attacks that\narise from the dual usage of the token as a means of payment and a Sybil\nresistance mechanism. In particular, the existence of derivatives markets makes\nmalicious coordination among validators easier to execute than in Proof of Work\nsystems. We demonstrate that it is also possible for on-chain lending smart\ncontracts to cannibalize network security in PoS systems. When the yield\nprovided by these contracts is more attractive than the inflation rate provided\nfrom staking, stakers will tend to remove their staked tokens and lend them\nout, thus reducing network security. In this paper, we provide a simple\nstochastic model that describes how rational validators with varying risk\npreferences react to changes in staking and lending returns. For a particular\nconfiguration of this model, we provide a formal proof of a phase transition\nbetween equilibria in which tokens are predominantly staked and those in which\nthey are predominantly lent. We further validate this emergent adversarial\nbehavior (e.g. reduced staked token supply) with agent-based simulations that\nsample transitions under more realistic conditions. Our results illustrate that\nrational, non-adversarial actors can dramatically reduce PoS network security\nif block rewards are not calibrated appropriately above the expected yields of\non-chain lending.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.00919v2"
    },
    {
        "title": "A growth adjusted price-earnings ratio",
        "authors": [
            "Graham Baird",
            "James Dodd",
            "Lawrence Middleton"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The purpose of this paper is to introduce a new growth adjusted\nprice-earnings measure (GA-P/E) and assess its efficacy as measure of value and\npredictor of future stock returns. Taking inspiration from the interpretation\nof the traditional price-earnings ratio as a period of time, the new measure\ncomputes the requisite payback period whilst accounting for earnings growth.\nHaving derived the measure, we outline a number of its properties before\nconducting an extensive empirical study utilising a sorted portfolio\nmethodology. We find that the returns of the low GA-P/E stocks exceed those of\nthe high GA-P/E stocks, both in an absolute sense and also on a risk-adjusted\nbasis. Furthermore, the returns from the low GA-P/E porfolio was found to\nexceed those of the value portfolio arising from a P/E sort on the same pool of\nstocks. Finally, the returns of our GA-P/E sorted porfolios were subjected to\nanalysis by conducting regressions against the standard Fama and French risk\nfactors.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.08240v1"
    },
    {
        "title": "The hyperbolic geometry of financial networks",
        "authors": [
            "Martin Keller-Ressel",
            "Stephanie Nargang"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Based on data from the European banking stress tests of 2014, 2016 and the\ntransparency exercise of 2018 we demonstrate for the first time that the latent\ngeometry of financial networks can be well-represented by geometry of negative\ncurvature, i.e., by hyperbolic geometry. This allows us to connect the network\nstructure to the popularity-vs-similarity model of Papdopoulos et al., which is\nbased on the Poincar\\'e disc model of hyperbolic geometry. We show that the\nlatent dimensions of `popularity' and `similarity' in this model are strongly\nassociated to systemic importance and to geographic subdivisions of the banking\nsystem. In a longitudinal analysis over the time span from 2014 to 2018 we find\nthat the systemic importance of individual banks has remained rather stable,\nwhile the peripheral community structure exhibits more (but still moderate)\nvariability.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.00399v2"
    },
    {
        "title": "Heuristics in experiments with infinitely large strategy spaces",
        "authors": [
            "Jørgen Vitting Andersen",
            "Philippe de Peretti"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We introduce a new methodology that enables detection of the onset of\nconvergence towards Nash equilibria in simple repeated games with infinitely\nlarge strategy spaces, thereby revealing the heuristics used in\ndecision-making. The method works by constraining on a special finite subset of\nstrategies, called decoupled strategies. We show how the technique can be\napplied to understand price formation in financial market experiments by\nintroducing a predictive measure {\\Delta}D: the different between positive\ndecoupled strategies (recommending to buy) and negative decoupled strategies\n(recommending to sell). Using {\\Delta}D we illustrate how the method can\npredict (at certain special times) participants' actions with a high success\nrate in a series of experiments\n",
        "pdf_link": "http://arxiv.org/pdf/2005.02337v1"
    },
    {
        "title": "Application of Facebook's Prophet Algorithm for Successful Sales\n  Forecasting Based on Real-world Data",
        "authors": [
            "Emir Zunic",
            "Kemal Korjenic",
            "Kerim Hodzic",
            "Dzenana Donko"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This paper presents a framework capable of accurately forecasting future\nsales in the retail industry and classifying the product portfolio according to\nthe expected level of forecasting reliability. The proposed framework, that\nwould be of great use for any company operating in the retail industry, is\nbased on Facebook's Prophet algorithm and backtesting strategy. Real-world\nsales forecasting benchmark data obtained experimentally in a production\nenvironment in one of the biggest retail companies in Bosnia and Herzegovina is\nused to evaluate the framework and demonstrate its capabilities in a real-world\nuse case scenario.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.07575v1"
    },
    {
        "title": "Detecting and explaining changes in various assets' relationships in\n  financial markets",
        "authors": [
            "Makoto Naraoka",
            "Teruaki Hayashi",
            "Takaaki Yoshino",
            "Toshiaki Sugie",
            "Kota Takano",
            "Yukio Ohsawa"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We study the method for detecting relationship changes in financial markets\nand providing human-interpretable network visualization to support the\ndecision-making of fund managers dealing with multi-assets. First, we construct\nco-occurrence networks with each asset as a node and a pair with a strong\nrelationship in price change as an edge at each time step. Second, we calculate\nGraph-Based Entropy to represent the variety of price changes based on the\nnetwork. Third, we apply the Differential Network to finance, which is\ntraditionally used in the field of bioinformatics. By the method described\nabove, we can visualize when and what kind of changes are occurring in the\nfinancial market, and which assets play a central role in changes in financial\nmarkets. Experiments with multi-asset time-series data showed results that were\nwell fit with actual events while maintaining high interpretability. It is\nsuggested that this approach is useful for fund managers to use as a new option\nfor decision making.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.10603v4"
    },
    {
        "title": "Green production as a factor of survival for innovative startups.\n  Evidence from Italy",
        "authors": [
            "Riccardo Gianluigi Serio",
            "Maria Michela Dickson",
            "Diego Giuliani",
            "Giuseppe Espa"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Many studies have analyzed empirically the determinants of survival for\ninnovative startup companies using data about the characteristics of\nentrepreneurs and management or focusing on firm- and industry-specific\nvariables. However, no attempts have been made so far to assess the role of the\nenvironmental sustainability of the production process. Based on data\ndescribing the characteristics of the Italian innovative startups in the period\n2009-2018, this article studies the differences in survival between green and\nnon-green companies. We show that, while controlling for other confounding\nfactors, startups characterized by a green production process tend to survive\nlonger than their counterparts. In particular, we estimate that a green\ninnovative startup is more than twice as likely to survive than a non-green\none. This evidence may support the idea that environment sustainability can\nhelp economic development.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.12102v1"
    },
    {
        "title": "Adaptation and Coevolution on an Emergent Global Competitive Landscape",
        "authors": [
            "Philip V. Fellman",
            "Jonathan Vos Post",
            "Roxana Wright",
            "Usha Dasari"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Notions of Darwinian selection have been implicit in economic theory for at\nleast sixty years. Richard Nelson and Sidney Winter have argued that while\nevolutionary thinking was prevalent in prewar economics, the postwar\nNeoclassical school became almost entirely preoccupied with equilibrium\nconditions and their mathematical conditions. One of the problems with the\neconomic interpretation of firm selection through competition has been a weak\ngrasp on an incomplete scientific paradigm. As I.F. Price notes, \"The\nbiological metaphor has long lurked in the background of management theory\nlargely because the message of 'survival of the fittest' (usually wrongly\nattributed to Charles Darwin rather than Herbert Spencer) provides a seemingly\nnatural model for market competition (e.g. Alchian 1950, Merrell 1984,\nHenderson 1989, Moore 1993), without seriously challenging the underlying\nparadigms of what an organisation is.\" In this paper we examine the application\nof dynamic fitness landscape models to economic theory, particularly the theory\nof technology substitution, drawing on recent work by Kauffman, Arthur,\nMcKelvey, Nelson and Winter, and Windrum and Birchenhall. In particular we use\nProfessor Post's early work with John Holland on the genetic algorithm to\nexplain some of the key differences between static and dynamic approaches to\neconomic modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.0854v1"
    },
    {
        "title": "Maximum Entropy, the Collective Welfare Principle and the Globalization\n  Process",
        "authors": [
            "Esteban Guevara Hidalgo"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Although both systems analyzed are described through two theories apparently\ndifferent (quantum mechanics and game theory) it is shown that both are\nanalogous and thus exactly equivalents. The quantum analogue of the replicator\ndynamics is the von Neumann equation. Quantum mechanics could be used to\nexplain more correctly biological and economical processes. It could even\nencloses theories like games and evolutionary dynamics. We can take some\nconcepts and definitions from quantum mechanics and physics for the best\nunderstanding of the behavior of economics and biology. Also, we could maybe\nunderstand nature like a game in where its players compete for a common welfare\nand the equilibrium of the system that they are members. All the members of our\nsystem will play a game in which its maximum payoff is the equilibrium of the\nsystem. They act as a whole besides individuals like they obey a rule in where\nthey prefer to work for the welfare of the collective besides the individual\nwelfare. A system where its members are in Nash Equilibrium (or ESS) is exactly\nequivalent to a system in a maximum entropy state. A system is stable only if\nit maximizes the welfare of the collective above the welfare of the individual.\nIf it is maximized the welfare of the individual above the welfare of the\ncollective the system gets unstable an eventually collapses. The results of\nthis work shows that the \"globalization\" process has a behavior exactly\nequivalent to a system that is tending to a maximum entropy state and predicts\nthe apparition of big common markets and strong common currencies that will\nfind its \"equilibrium\" by decreasing its number until they get a state\ncharacterized by only one common currency and only one common market around the\nworld.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.1897v2"
    },
    {
        "title": "Economic Amplifier - A New Econophysics Model",
        "authors": [
            "Ion Spanulescu",
            "Anca Gheorghiu"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Most of the econometric and econophysics models have been borrowed from the\nstatistical physics, and as a cosequence, a new interdisciplinary science\ncalled econophysics has emerged. In this paper we planned to extend the analogy\nbetween different economic processes or phenomena and processes and phenomena\nfrom different fields of physics, other than statistical physics. On the basis\nof the economic development process and amplification phenomenon analogy, a new\neconophysics model, named economic amplifier, on the electronic amplification\nprinciple from applied physics was proposed und largely analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.3703v1"
    },
    {
        "title": "The International Trade Network: weighted network analysis and modelling",
        "authors": [
            "K. Bhattacharya",
            "G. Mukherjee",
            "J. Saramaki",
            "K. Kaski",
            "S. S. Manna"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Tools of the theory of critical phenomena, namely the scaling analysis and\nuniversality, are argued to be applicable to large complex web-like network\nstructures. Using a detailed analysis of the real data of the International\nTrade Network we argue that the scaled link weight distribution has an\napproximate log-normal distribution which remains robust over a period of 53\nyears. Another universal feature is observed in the power-law growth of the\ntrade strength with gross domestic product, the exponent being similar for all\ncountries. Using the 'rich-club' coefficient measure of the weighted networks\nit has been shown that the size of the rich-club controlling half of the\nworld's trade is actually shrinking. While the gravity law is known to describe\nwell the social interactions in the static networks of population migration,\ninternational trade, etc, here for the first time we studied a non-conservative\ndynamical model based on the gravity law which excellently reproduced many\nempirical features of the ITN.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.4343v2"
    },
    {
        "title": "The International Trade Network",
        "authors": [
            "K. Bhattacharya",
            "G. Mukherjee",
            "S. S. Manna"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Bilateral trade relationships in the international level between pairs of\ncountries in the world give rise to the notion of the International Trade\nNetwork (ITN). This network has attracted the attention of network researchers\nas it serves as an excellent example of the weighted networks, the link weight\nbeing defined as a measure of the volume of trade between two countries. In\nthis paper we analyzed the international trade data for 53 years and studied in\ndetail the variations of different network related quantities associated with\nthe ITN. Our observation is that the ITN has also a scale invariant structure\nlike many other real-world networks.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.4347v1"
    },
    {
        "title": "Statistical ensembles for money and debt",
        "authors": [
            "Stefano Viaggiu",
            "Andrea Lionetto",
            "Leonardo Bargigli",
            "Michele Longo"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We build a statistical ensemble representation of two economic models\ndescribing respectively, in simplified terms, a payment system and a credit\nmarket. To this purpose we adopt the Boltzmann-Gibbs distribution where the\nrole of the Hamiltonian is taken by the total money supply (i.e. including\nmoney created from debt) of a set of interacting economic agents. As a result,\nwe can read the main thermodynamic quantities in terms of monetary ones. In\nparticular, we define for the credit market model a work term which is related\nto the impact of monetary policy on credit creation. Furthermore, with our\nformalism we recover and extend some results concerning the temperature of an\neconomic system, previously presented in the literature by considering only the\nmonetary base as conserved quantity. Finally, we study the statistical ensemble\nfor the Pareto distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0891v3"
    },
    {
        "title": "Optimal Capital Structure with Scale Effects under Spectrally Negative\n  Levy Models",
        "authors": [
            "Budhi Arta Surya",
            "Kazutoshi Yamazaki"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The optimal capital structure model with endogenous bankruptcy was first\nstudied by Leland (1994) and Leland and Toft (1996), and was later extended to\nthe spectrally negative Levy model by Hilberink and Rogers (2002) and Kyprianou\nand Surya (2007). This paper incorporates the scale effects by allowing the\nvalues of bankruptcy costs and tax benefits to be dependent on the firm's asset\nvalue. By using the fluctuation identities for the spectrally negative Levy\nprocess, we obtain a candidate bankruptcy level as well as a sufficient\ncondition for optimality. The optimality holds in particular when,\nmonotonically in the asset value, the value of tax benefits is increasing, the\nloss amount at bankruptcy is increasing, and its proportion relative to the\nasset value is decreasing. The solution admits a semi-explicit form in terms of\nthe scale function. A series of numerical studies are given to analyze the\nimpacts of scale effects on the default strategy and the optimal capital\nstructure.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0897v4"
    },
    {
        "title": "Collective behavior in financial market",
        "authors": [
            "Thomas Kauê Dal'Maso Peron",
            "Francisco Aparecido Rodrigues"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Financial market is an example of complex system, which is characterized by a\nhighly intricate organization and the emergence of collective behavior. In this\npaper, we quantify this emergent dynamics in the financial market by using\nconcepts of network synchronization. We consider networks constructed by the\ncorrelation matrix of asset returns and study the time evolution of the phase\ncoherence among stock prices. It is verified that during financial crisis a\nsynchronous state emerges in the system, defining the market's direction.\nFurthermore, the paper proposes a statistical regression model able to identify\nthe topological features that mostly influence such an emergence. The\ncoefficients of the proposed model indicate that the average shortest path\nlength is the measurement most related to network synchronization. Therefore,\nduring economic crisis, the stock prices present a similar evolution, which\ntends to shorten the distances between stocks, indication a collective\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.1167v1"
    },
    {
        "title": "Pricing stocks with yardsticks and sentiments",
        "authors": [
            "Sebastıan Martınez Bustos",
            "Jorgen Vitting Andersen",
            "Michel Miniconi",
            "Andrzej Nowak",
            "Magdalena Roszczynska-Kurasinska",
            "David Bree"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Human decision making by professionals trading daily in the stock market can\nbe a daunting task. It includes decisions on whether to keep on investing or to\nexit a market subject to huge price swings, and how to price in news or rumors\nattributed to a specific stock. The question then arises how professional\ntraders, who specialize in daily buying and selling large amounts of a given\nstock, know how to properly price a given stock on a given day? Here we\nintroduce the idea that people use heuristics, or \"rules of thumb\", in terms of\n\"yard sticks\" from the performance of the other stocks in a stock index. The\nunder- /over-performance with respect to such a yard stick then signifies a\ngeneral negative/positive sentiment of the market participants towards a given\nstock. Using empirical data of the Dow Jones Industrial Average, stocks are\nshown to have daily performances with a clear tendency to cluster around the\nmeasures introduced by the yard sticks. We illustrate how sentiments, most\nlikely due to insider information, can influence the performance of a given\nstock over period of months, and in one case years.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.6909v1"
    },
    {
        "title": "A generalized statistical model for the size distribution of wealth",
        "authors": [
            "F. Clementi",
            "M. Gallegati",
            "G. Kaniadakis"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In a recent paper in this journal [J. Stat. Mech. (2009) P02037] we proposed\na new, physically motivated, distribution function for modeling individual\nincomes having its roots in the framework of the k-generalized statistical\nmechanics. The performance of the k-generalized distribution was checked\nagainst real data on personal income for the United States in 2003. In this\npaper we extend our previous model so as to be able to account for the\ndistribution of wealth. Probabilistic functions and inequality measures of this\ngeneralized model for wealth distribution are obtained in closed form. In order\nto check the validity of the proposed model, we analyze the U.S. household\nwealth distributions from 1984 to 2009 and conclude an excellent agreement with\nthe data that is superior to any other model already known in the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.4787v2"
    },
    {
        "title": "Iterated Function Systems with Economic Applications",
        "authors": [
            "Shilei Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  This work's purpose is to understand the dynamics of some social systems\nwhose properties can be captured by certain iterated function systems. To\nachieve this intension, we start from the theory of iterated function systems,\nand then we study two specific economic models on random utility function and\noptimal stochastic growth.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.4849v1"
    },
    {
        "title": "A Dynamic Model of Functioning of a Bank",
        "authors": [
            "Oleg Malafeyev",
            "Achal Awasthi"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In this paper, we analyze dynamic programming as a novel approach to solve\nthe problem of maximizing the profits of a bank. The mathematical model of the\nproblem and the description of a bank's work is described in this paper. The\nproblem is then approached using the method of dynamic programming. Dynamic\nprogramming makes sure that the solutions obtained are globally optimal and\nnumerically stable. The optimization process is set up as a discrete\nmulti-stage decision process and solved with the help of dynamic programming.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.01529v1"
    },
    {
        "title": "Backbone of credit relationships in the Japanese credit market",
        "authors": [
            "Luca Marotta",
            "Salvatore Miccichè",
            "Yoshi Fujiwara",
            "Hiroshi Iyetomi",
            "Hideaki Aoyama",
            "Mauro Gallegati",
            "Rosario N. Mantegna"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We detect the backbone of the weighted bipartite network of the Japanese\ncredit market relationships. The backbone is detected by adapting a general\nmethod used in the investigation of weighted networks. With this approach we\ndetect a backbone that is statistically validated against a null hypothesis of\nuniform diversification of loans for banks and firms. Our investigation is done\nyear by year and it covers more than thirty years during the period from 1980\nto 2011. We relate some of our findings with economic events that have\ncharacterized the Japanese credit market during the last years. The study of\nthe time evolution of the backbone allows us to detect changes occurred in\nnetwork size, fraction of credit explained, and attributes characterizing the\nbanks and the firms present in the backbone.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06870v1"
    },
    {
        "title": "An Endogenous Mechanism of Business Cycles",
        "authors": [
            "Dimitri Kroujiline",
            "Maxim Gusev",
            "Dmitry Ushanov",
            "Sergey V. Sharov",
            "Boris Govorkov"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This paper suggests that business cycles may be a manifestation of coupled\nreal economy and stock market dynamics and describes a mechanism that can\ngenerate economic fluctuations consistent with observed business cycles. To\nthis end, we seek to incorporate into the macroeconomic framework a dynamic\nstock market model based on opinion interactions (Gusev et al., 2015). We\nderive this model from microfoundations, provide its empirical verification,\ndemonstrate that it contains the efficient market as a particular regime and\nestablish a link through which macroeconomic models can be attached for the\nstudy of real economy and stock market interaction. To examine key effects, we\nlink it with a simple macroeconomic model (Blanchard, 1981). The coupled system\ngenerates nontrivial endogenous dynamics, which exhibit deterministic and\nstochastic features, producing quasiperiodic fluctuations (business cycles). We\nalso inspect this system's behavior in the phase space. The real economy and\nthe stock market coevolve dynamically along the path governed by a\nstochastically-forced dynamical system with two stable equilibria, one where\nthe economy expands and the other where it contracts, resulting in business\ncycles identified as the coherence resonance phenomenon. Thus, the\nincorporation of stock market dynamics into the macroeconomic framework, as\npresented here, allows the derivation of realistic behaviors in a tractable\nsetting.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.05002v4"
    },
    {
        "title": "The Evolution of Security Prices Is Not Stochastic but Governed by a\n  Physicomathematical Law",
        "authors": [
            "Wally Tzara"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Since Bachelier's thesis in 1900 (laying the foundation of the stochastic\nprocess, or Brownian motion, as a model of stock price changes), attempts at\nunderstanding the nature of prices and at predicting them have failed.\nStatistical methods have only found minor regularities/anomalies, and other\nmathematical and physical approaches do not work. This leads researchers to\nconsider that the evolution of security prices is basically random, and, thus,\ninherently not predictable. We show that the evolution of security prices is\nnot a stochastic process but largely deterministic and governed by a physical\nlaw. The law takes the form of a physicomathematical theory centered around a\npurely mathematical function, unrelated to models and statistical methods. It\ncan be described as an \"isodense\" network of moving regression curves of an\norder greater than or equal to 1. The salient aspect of the function is that,\nwhen inputting a time series of any security into the function, new\nmathematical objects emerge spontaneously, and these objects exhibit the unique\nproperty of attracting and repelling the quantity. The graphical representation\nof the function is called a \"topological network\" due to the preeminence of\nshapes over metrics, and the emergent objects are called \"characteristic\nfigures\" (mainly \"cords\"). The attraction and repulsion of the price by the\ncords results in the price bouncing from cord to cord. Thus, the price has to\nbe considered as driven by the cords in a semi-deterministic manner (leaning\ntowards deterministic). With a function that describes the evolution of the\nprice, we now understand the reason behind each price movement and can also\npredict prices both qualitatively and quantitatively. The function is\nuniversal, does not rely on any fitting, and, due to its extreme sensitivity,\nreveals the hidden order in financial time series data that existing research\nnever uncovered.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.10114v3"
    },
    {
        "title": "Dynamic investment model of the life cycle of a company under the\n  influence of factors in a competitive environment",
        "authors": [
            "O. A. Malafeyev",
            "I. I. Pavlov"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Modelling all possible life cycles of a company in a highly competitive\neconomic environment gives a significant advantage to the owner in his business\ninvestment activities. This article proposes and analyses a dynamic model of a\ncompany's life cycle with known action costs and transition probabilities, that\ncan be affected by an outside influence. For this task, the Markov model was\nutilized. The proposed model is illustrated on a task of determining an\nadvertising policy for a car dealership, that would increase the stock equity\nof a company. The result demonstrates the usefulness of a model for use in\ndetermining future actions of a company. We also review multiple models of the\ninfluence of outside factors on a company's total capitalization.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.06298v2"
    },
    {
        "title": "The emergence of critical stocks in market crash",
        "authors": [
            "Shan Lu",
            "Jichang Zhao",
            "Huiwen Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In complex systems like financial market, risk tolerance of individuals is\ncrucial for system resilience.The single-security price limit, designed as risk\ntolerance to protect investors by avoiding sharp price fluctuation, is blamed\nfor feeding market panic in times of crash.The relationship between the\ncritical market confidence which stabilizes the whole system and the price\nlimit is therefore an important aspect of system resilience. Using a simplified\ndynamic model on networks of investors and stocks, an unexpected linear\nassociation between price limit and critical market confidence is theoretically\nderived and empirically verified in this paper. Our results highlight the\nimportance of relatively `small' but critical stocks that drive the system to\ncollapse by passing the failure from periphery to core. These small stocks,\nlargely originating from homogeneous investment strategies across the market,\nhas unintentionally suppressed system resilience with the exclusive increment\nof individual risk tolerance. Imposing random investment requirements to\nmitigate herding behavior can thus improve the market resilience.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.07244v1"
    },
    {
        "title": "Sabrina: Modeling and Visualization of Economy Data with Incremental\n  Domain Knowledge",
        "authors": [
            "Alessio Arleo",
            "Christos Tsigkanos",
            "Chao Jia",
            "Roger A. Leite",
            "Ilir Murturi",
            "Manfred Klaffenboeck",
            "Schahram Dustdar",
            "Michael Wimmer",
            "Silvia Miksch",
            "Johannes Sorger"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Investment planning requires knowledge of the financial landscape on a large\nscale, both in terms of geo-spatial and industry sector distribution. There is\nplenty of data available, but it is scattered across heterogeneous sources\n(newspapers, open data, etc.), which makes it difficult for financial analysts\nto understand the big picture. In this paper, we present Sabrina, a financial\ndata analysis and visualization approach that incorporates a pipeline for the\ngeneration of firm-to-firm financial transaction networks. The pipeline is\ncapable of fusing the ground truth on individual firms in a region with\n(incremental) domain knowledge on general macroscopic aspects of the economy.\nSabrina unites these heterogeneous data sources within a uniform visual\ninterface that enables the visual analysis process. In a user study with three\ndomain experts, we illustrate the usefulness of Sabrina, which eases their\nanalysis process.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.07479v2"
    },
    {
        "title": "A Statistical Field Approach to Capital Accumulation",
        "authors": [
            "Pierre Gosselin",
            "Aïleen Lotz",
            "Marc Wambst"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This paper presents a model of capital accumulation for a large number of\nheterogenous producer-consumers in an exchange space in which interactions\ndepend on agents' positions. Each agent is described by his production,\nconsumption, stock of capital, as well as the position he occupies in this\nabstract space. Each agent produces one differentiated good whose price is\nfixed by market clearing conditions. Production functions are Cobb-Douglas, and\ncapital stocks follow the standard capital accumulation dynamic equation.\nAgents consume all goods but have a preference for goods produced by their\nclosest neighbors. Agents in the exchange space are subject both to attractive\nand repulsive forces. Exchanges drive agents closer, but beyond a certain level\nof proximity, agents will tend to crowd out more distant agents. The present\nmodel uses a formalism based on statistical field theory developed earlier by\nthe authors. This approach allows the analytical treatment of economic models\nwith an arbitrary number of agents, while preserving the system's interactions\nand complexity at the individual level. Our results show that the dynamics of\ncapital accumulation and agents' position in the exchange space are correlated.\nInteractions in the exchange space induce several phases of the system. A first\nphase appears when attractive forces are limited. In this phase, an initial\ncentral position in the exchange space favors capital accumulation in average\nand leads to a higher level of capital, while agents far from the center will\nexperience a slower accumulation process. A high level of initial capital\ndrives agents towards a central position, i.e. improve the terms of their\nexchanges: they experience a higher demand and higher prices for their product.\nAs usual, high capital productivity favors capital accumulation, while higher\nrates of capital depreciation reduce capital stock. In a second phase,\nattractive forces are predominant. The previous results remain, but an\nadditional threshold effect appears. Even though no restriction was imposed\ninitially on the system, two types of agents emerge, depending on their initial\nstock of capital. One type of agents will remain above the capital threshold\nand occupy and benefit from a central position. The other type will remain\nbelow the threshold, will not be able to break it and will remain at the\nperiphery of the exchange space. In this phase, capital distribution is less\nhomogenous than in the first phase.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.11635v1"
    },
    {
        "title": "Econophysics deserves a revamping",
        "authors": [
            "Paolo Magrassi"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The paper argues that attracting more economists and adopting a more-precise\ndefinition of dynamic complexity might help econophysics acquire more attention\nin the economics community and bring new lymph to economic research. It may be\nnecessary to concentrate less on the applications than on the basics of\neconomic complexity, beginning with expansion and deepening of the study of\nsmall systems with few interacting components, while until thus far complexity\nhas been assumed to be a prerogative of complicated systems only. It is\npossible that without a thorough analysis at that level, the understanding of\nsystems that are at the same time complex and complicated will continue to\nelude economics and econophysics research altogether. To that purpose, the\npaper initiates and frames a definition of dynamic complexity grounded on the\nconcept of non-linear dynamical system.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.05814v2"
    },
    {
        "title": "Changes in Household Net Financial Assets After the Great Recession: Did\n  Financial Planners Make a Difference?",
        "authors": [
            "Joseph W. Goetz",
            "Lance Palmer",
            "Lini Zhang",
            "Swarn Chatterjee"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This study utilized the 2007-2009 Survey of Consumer Finances (SCF) panel\ndataset to examine the impact of financial planner use on household net\nfinancial asset level during the Great recession. Data included 3,862\nrespondents who completed the SCF survey and a follow up interview. The results\nindicated that starting to use a financial planner during the Great Recession\nhad a positive impact on preserving and increasing the value of households' net\nfinancial assets, while curtailing the use of a financial planner during this\ntime had a negative impact on preserving the value of households' financial\nassets. Thus, study findings indicated that the benefit of using a financial\nplanner maybe particularly high during a major financial downturn.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.00949v1"
    },
    {
        "title": "Dynamic Network Risk",
        "authors": [
            "Jozef Barunik",
            "Michael Ellington"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This paper examines the pricing of short-term and long-term dynamic network\nrisk in the cross-section of stock returns. Stocks with high sensitivities to\ndynamic network risk earn lower returns. We rationalize our finding with\neconomic theory that allows the stochastic discount factor to load on network\nrisk through the precautionary savings channel. A one-standard deviation\nincrease in long-term (short-term) network risk loadings associate with a 7.66%\n(6.71%) drop in annualized expected returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.04639v2"
    },
    {
        "title": "Bailout Stigma",
        "authors": [
            "Yeon-Koo Che",
            "Chongwoo Choe",
            "Keeyoung Rhee"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We develop a model of bailout stigma where accepting a bailout signals a\nfirm's balance-sheet weakness and worsens its funding prospect. To avoid\nstigma, high-quality firms either withdraw from subsequent financing after\nreceiving bailouts or refuse bailouts altogether to send a favorable signal.\nThe former leads to a short-lived stimulation with a subsequent market freeze\neven worse than if there were no bailouts. The latter revives the funding\nmarket, albeit with delay, to the level achievable without any stigma, and\nimplements a constrained optimal outcome. A menu of multiple bailout programs\nalso compounds bailout stigma and worsens market freeze.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.05640v4"
    },
    {
        "title": "Analysing the resilience of the European commodity production system\n  with PyResPro, the Python Production Resilience package",
        "authors": [
            "Matteo Zampieri",
            "Andrea Toreti",
            "Andrej Ceglar",
            "Pierluca De Palma",
            "Thomas Chatzopoulos"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This paper presents a Python object-oriented software and code to compute the\nannual production resilience indicator. The annual production resilience\nindicator can be applied to different anthropic and natural systems such as\nagricultural production, natural vegetation and water resources. Here, we show\nan example of resilience analysis of the economic values of the agricultural\nproduction in Europe. The analysis is conducted for individual time-series in\norder to estimate the resilience of a single commodity and to groups of\ntime-series in order to estimate the overall resilience of diversified\nproduction systems composed of different crops and/or different countries. The\nproposed software is powerful and easy to use with publicly available datasets\nsuch as the one used in this study.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.08976v2"
    },
    {
        "title": "Tri-criterion model for constructing low-carbon mutual fund portfolios:\n  a preference-based multi-objective genetic algorithm approach",
        "authors": [
            "A. Hilario-Caballero",
            "A. Garcia-Bernabeu",
            "J. V. Salcedo",
            "M. Vercher"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Sustainable finance, which integrates environmental, social and governance\n(ESG) criteria on financial decisions rests on the fact that money should be\nused for good purposes. Thus, the financial sector is also expected to play a\nmore important role to decarbonise the global economy. To align financial flows\nwith a pathway towards a low-carbon economy, investors should be able to\nintegrate in their financial decisions additional criteria beyond return and\nrisk to manage climate risk. We propose a tri-criterion portfolio selection\nmodel to extend the classical Markowitz mean-variance approach in order to\ninclude investors preferences on the portfolio carbon risk exposure as an\nadditional criterion. To approximate the 3D Pareto front we apply an efficient\nmulti-objective genetic algorithm called ev-MOGA which is based on the concept\nof e-dominance. Furthermore, we introduce an a posteriori approach to\nincorporate the investor's preferences into the solution process regarding\ntheir sustainability preferences measured by the carbon risk exposure and\nhis/her loss-adverse attitude. We test the performance of the proposed\nalgorithm in a cross section of European SRI open-end funds to assess the\nextent to which climate related risk could be embedded in the portfolio\naccording to the investor's preferences.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.11888v1"
    },
    {
        "title": "DeFi Protocols for Loanable Funds: Interest Rates, Liquidity and Market\n  Efficiency",
        "authors": [
            "Lewis Gudgeon",
            "Sam M. Werner",
            "Daniel Perez",
            "William J. Knottenbelt"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We coin the term *Protocols for Loanable Funds (PLFs)* to refer to protocols\nwhich establish distributed ledger-based markets for loanable funds. PLFs are\nemerging as one of the main applications within Decentralized Finance (DeFi),\nand use smart contract code to facilitate the intermediation of loanable funds.\nIn doing so, these protocols allow agents to borrow and save programmatically.\nWithin these protocols, interest rate mechanisms seek to equilibrate the supply\nand demand for funds. In this paper, we review the methodologies used to set\ninterest rates on three prominent DeFi PLFs, namely Compound, Aave and dYdX. We\nprovide an empirical examination of how these interest rate rules have behaved\nsince their inception in response to differing degrees of liquidity. We then\ninvestigate the market efficiency and inter-connectedness between multiple\nprotocols, examining first whether Uncovered Interest Parity holds within a\nparticular protocol and second whether the interest rates for a particular\ntoken market show dependence across protocols, developing a Vector Error\nCorrection Model for the dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.13922v3"
    },
    {
        "title": "The Nonuniversality of Wealth Distribution Tails Near Wealth\n  Condensation Criticality",
        "authors": [
            "Sam L. Polk",
            "Bruce M. Boghosian"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this work, we modify the affine wealth model of wealth distributions to\nexamine the effects of nonconstant redistribution on the very wealthy. Previous\nstudies of this model, restricted to flat redistribution schemes, have\ndemonstrated the presence of a phase transition to a partially wealth-condensed\nstate, or \"partial oligarchy,\" at the critical value of an order parameter.\nThese studies have also indicated the presence of an exponential tail in wealth\ndistribution precisely at criticality. Away from criticality, the tail was\nobserved to be Gaussian. In this work, we generalize the flat redistribution\nwithin the affine wealth model to allow for an essentially arbitrary\nredistribution policy. We show that the exponential tail observed near\ncriticality in prior work is, in fact, a special case of a much broader class\nof critical, slower-than-Gaussian decays that depend sensitively on the\ncorresponding asymptotic behavior of the progressive redistribution model used.\nWe thereby demonstrate that the functional form of the tail of the wealth\ndistribution in a near-critical society is not universal in nature but rather\nentirely determined by the specifics of public policy decisions. This is\nsignificant because most major economies today are observed to be\nnear-critical.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.15008v3"
    },
    {
        "title": "Wealth concentration in systems with unbiased binary exchanges",
        "authors": [
            "Ben-Hur Francisco Cardoso",
            "Sebastián Gonçalves",
            "José Roberto Iglesias"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Aiming to describe the wealth distribution evolution, several models consider\nan ensemble of interacting economic agents that exchange wealth in binary\nfashion. Intriguingly, models that consider an unbiased market, that gives to\neach agent the same chances to win in the game, are always out of equilibrium\nuntil the perfect inequality of the final state is attained. Here we present a\nrigorous analytical demonstration that any system driven by unbiased binary\nexchanges are doomed to drive the system to perfect inequality and zero\nmobility.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.00206v2"
    },
    {
        "title": "The Thermodynamic Approach to Whole-Life Insurance: A Method for\n  Evaluation of Surrender Risk",
        "authors": [
            "Jirô Akahori",
            "Yuuki Ida",
            "Maho Nishida",
            "Shuji Tamada"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We introduce a collective model for life insurance where the heterogeneity of\neach insured, including the health state, is modeled by a diffusion process.\nThis model is influenced by concepts in statistical mechanics. Using the\nproposed framework, one can describe the total pay-off as a functional of the\ndiffusion process, which can be used to derive a level premium that evaluates\nthe risk of lapses due tothe so-called adverse selection. Two numerically\ntractable models are presented to exemplify the flexibility of the proposed\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.09606v1"
    },
    {
        "title": "The VIX index under scrutiny of machine learning techniques and neural\n  networks",
        "authors": [
            "Ali Hirsa",
            "Joerg Osterrieder",
            "Branka Hadji Misheva",
            "Wenxin Cao",
            "Yiwen Fu",
            "Hanze Sun",
            "Kin Wai Wong"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  The CBOE Volatility Index, known by its ticker symbol VIX, is a popular\nmeasure of the market's expected volatility on the SP 500 Index, calculated and\npublished by the Chicago Board Options Exchange (CBOE). It is also often\nreferred to as the fear index or the fear gauge. The current VIX index value\nquotes the expected annualized change in the SP 500 index over the following 30\ndays, based on options-based theory and current options-market data. Despite\nits theoretical foundation in option price theory, CBOE's Volatility Index is\nprone to inadvertent and deliberate errors because it is weighted average of\nout-of-the-money calls and puts which could be illiquid. Many claims of market\nmanipulation have been brought up against VIX in recent years.\n  This paper discusses several approaches to replicate the VIX index as well as\nVIX futures by using a subset of relevant options as well as neural networks\nthat are trained to automatically learn the underlying formula. Using subset\nselection approaches on top of the original CBOE methodology, as well as\nbuilding machine learning and neural network models including Random Forests,\nSupport Vector Machines, feed-forward neural networks, and long short-term\nmemory (LSTM) models, we will show that a small number of options is sufficient\nto replicate the VIX index. Once we are able to actually replicate the VIX\nusing a small number of SP options we will be able to exploit potential\narbitrage opportunities between the VIX index and its underlying derivatives.\nThe results are supposed to help investors to better understand the options\nmarket, and more importantly, to give guidance to the US regulators and CBOE\nthat have been investigating those manipulation claims for several years.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.02119v1"
    },
    {
        "title": "Internal hydro- and wind portfolio optimisation in real-time market\n  operations",
        "authors": [
            "Hans Ole Riddervold",
            "Ellen Krohn Aasgård",
            "Lisa Haukaas",
            "Magnus Korpås"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  In this paper aspects related to handling of intraday imbalances for hydro\nand wind power are addressed. The definition of imbalance cost is established\nand used to describe the potential benefits of shifting from plant-specific\nschedules to a common load requirement for wind and hydropower units in the\nsame price area. The Nordpool intraday pay-as-bid market has been the basis for\nevaluation of imbalances, and some main characteristics for this market has\nbeen described. We consider how internal handling of complementary imbalances\nwithin the same river system with high inflow uncertainty and constrained\nreservoirs can reduce volatility in short-term marginal cost and risk compared\nto trading in the intraday market. We have also shown the the imbalance cost\nfor a power producer with both wind and hydropower assets can be reduced by\ninternal balancing in combination with sales and purchase in a pay-as-bid\nintraday market\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10098v1"
    },
    {
        "title": "Reducing the Volatility of Cryptocurrencies -- A Survey of Stablecoins",
        "authors": [
            "Ayten Kahya",
            "Bhaskar Krishnamachari",
            "Seokgu Yun"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  In the wake of financial crises, stablecoins are gaining adoption among\ndigital currencies. We discuss how stablecoins help reduce the volatility of\ncryptocurrencies by surveying different types of stablecoins and their\nstability mechanisms. We classify different approaches to stablecoins in three\nmain categories i) fiat or asset backed, ii) crypto-collateralized and iii)\nalgorithmic stablecoins, giving examples of concrete projects in each class. We\nassess the relative tradeoffs between the different approaches. We also discuss\nchallenges associated with the future of stablecoins and their adoption, their\nadoption and point out future research directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01340v1"
    },
    {
        "title": "The Origination and Distribution of Money Market Instruments: Sterling\n  Bills of Exchange during the First Globalization",
        "authors": [
            "Olivier Accominotti",
            "Delio Lucena-Piquero",
            "Stefano Ugolini"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This paper presents a detailed analysis of how liquid money market\ninstruments -- sterling bills of exchange -- were produced during the first\nglobalisation. We rely on a unique data set that reports systematic information\non all 23,493 bills re-discounted by the Bank of England in the year 1906.\nUsing descriptive statistics and network analysis, we reconstruct the complete\nnetwork of linkages between agents involved in the origination and distribution\nof these bills. Our analysis reveals the truly global dimension of the London\nbill market before the First World War and underscores the crucial role played\nby London intermediaries (acceptors and discounters) in overcoming information\nasymmetries between borrowers and lenders on this market. The complex\nindustrial organisation of the London money market ensured that risky private\ndebts could be transformed into extremely liquid and safe monetary instruments\ntraded throughout the global financial system.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01558v1"
    },
    {
        "title": "Indian Economy and Nighttime Lights",
        "authors": [
            "Jeet Agnihotri",
            "Subhankar Mishra"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Forecasting economic growth of India has been traditionally an uncertain\nexercise. The indicators and factors affecting economic structures and the\nvariables required to model that captures the situation correctly is point of\nconcern. Although the forecast should be specific to the country we are looking\nat however countries do have interlinkages among them. As the time series can\nbe more volatile and sometimes certain variables are unavailable it is harder\nto predict for the developing economies as compared to stable and developed\nnations. However it is very important to have accurate forecasts for economic\ngrowth for successful policy formations. One of the hypothesized indicators is\nthe nighttime lights. Here we aim to look for a relationship between GDP and\nNighttime lights. Specifically we look at the DMSP and VIIRS dataset. We are\nfinding relationship between various measures of economy.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03179v1"
    },
    {
        "title": "Radical Complexity",
        "authors": [
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This is an informal and sketchy review of six topical, somewhat unrelated\nsubjects in quantitative finance: rough volatility models; random covariance\nmatrix theory; copulas; crowded trades; high-frequency trading & market\nstability; and \"radical complexity\" & scenario based (macro)economics. Some\nopen questions and research directions are briefly discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09692v1"
    },
    {
        "title": "Quantum propensity in economics",
        "authors": [
            "David Orrell",
            "Monireh Houshmand"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This paper describes an approach to economics that is inspired by quantum\ncomputing, and is motivated by the need to develop a consistent quantum\nmathematical framework for economics. The traditional neoclassical approach\nassumes that rational utility-optimisers drive market prices to a stable\nequilibrium, subject to external perturbations. While this approach has been\nhighly influential, it has come under increasing criticism following the\nfinancial crisis of 2007/8. The quantum approach, in contrast, is inherently\nprobabilistic and dynamic. Decision-makers are described, not by a utility\nfunction, but by a propensity function which specifies the probability of\ntransacting. We show how a number of cognitive phenomena such as preference\nreversal and the disjunction effect can be modelled by using a simple quantum\ncircuit to generate an appropriate propensity function. Conversely, a general\npropensity function can be quantized to incorporate effects such as\ninterference and entanglement that characterise human decision-making.\nApplications to some common problems in economics and finance are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.10938v1"
    },
    {
        "title": "Deriving consensus rankings via multicriteria decision making\n  methodology",
        "authors": [
            "Amy Poh Ai Ling",
            "Mohamad Nasir Saludin",
            "Masao Mukaidono"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Purpose - This paper seeks to take a cautionary stance to the impact of the\nmarketing mix on customer satisfaction, via a case study deriving consensus\nrankings for benchmarking on selected retail stores in Malaysia.\n  Design/methodology/approach - The ELECTRE I model is used in deriving\nconsensus rankings via multicriteria decision making method for benchmarking\nbase on the marketing mix model 4P's. Descriptive analysis is used to analyze\nbest practice among the four marketing tactics.\n  Findings - Outranking methods in consequence constitute a strong base on\nwhich to found the entire structure of the behavioral theory of benchmarking\napplied to development of marketing strategy.\n  Research limitations/implications - This study looks only at a limited part\nof the puzzle of how consumer satisfaction translates into behavioral outcomes.\n  Practical implications - The study provides managers with guidance on how to\ngenerate a rough outline of potential marketing activities that can be used to\ntake advantage of capabilities and convert weaknesses and threats.\n  Originality/value - The paper interestingly portrays the effective usage of\nmulticriteria decision-making and ranking method to help marketing managers\npredict their marketing trends.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1604v1"
    },
    {
        "title": "Continuous Equilibrium in Affine and Information-Based Capital Asset\n  Pricing Models",
        "authors": [
            "Ulrich Horst",
            "Michael Kupper",
            "Andrea Macrina",
            "Christoph Mainberger"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We consider a class of generalized capital asset pricing models in continuous\ntime with a finite number of agents and tradable securities. The securities may\nnot be sufficient to span all sources of uncertainty. If the agents have\nexponential utility functions and the individual endowments are spanned by the\nsecurities, an equilibrium exists and the agents' optimal trading strategies\nare constant. Affine processes, and the theory of information-based asset\npricing are used to model the endogenous asset price dynamics and the terminal\npayoff. The derived semi-explicit pricing formulae are applied to numerically\nanalyze the impact of the agents' risk aversion on the implied volatility of\nsimultaneously-traded European-style options.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1840v2"
    },
    {
        "title": "Anti-correlation and subsector structure in financial systems",
        "authors": [
            "X. F. Jiang",
            "B. Zheng"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  With the random matrix theory, we study the spatial structure of the Chinese\nstock market, American stock market and global market indices. After taking\ninto account the signs of the components in the eigenvectors of the\ncross-correlation matrix, we detect the subsector structure of the financial\nsystems. The positive and negative subsectors are anti-correlated each other in\nthe corresponding eigenmode. The subsector structure is strong in the Chinese\nstock market, while somewhat weaker in the American stock market and global\nmarket indices. Characteristics of the subsector structures in different\nmarkets are revealed.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.6418v1"
    },
    {
        "title": "Mean field approximation for biased diffusion on Japanese inter-firm\n  trading network",
        "authors": [
            "Hayafumi Watanabe"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  By analysing the financial data of firms across Japan, a nonlinear power law\nwith an exponent of 1.3 was observed between the number of business partners\n(i.e. the degree of the inter-firm trading network) and sales. In a previous\nstudy using numerical simulations, we found that this scaling can be explained\nby both the money-transport model, where a firm (i.e. customer) distributes\nmoney to its out-edges (suppliers) in proportion to the in-degree of\ndestinations, and by the correlations among the Japanese inter-firm trading\nnetwork. However, in this previous study, we could not specifically identify\nwhat types of structure properties (or correlations) of the network determine\nthe 1.3 exponent. In the present study, we more clearly elucidate the\nrelationship between this nonlinear scaling and the network structure by\napplying mean-field approximation of the diffusion in a complex network to this\nmoney-transport model. Using theoretical analysis, we obtained the mean-field\nsolution of the model and found that, in the case of the Japanese firms, the\nscaling exponent of 1.3 can be determined from the power law of the average\ndegree of the nearest neighbours of the network with an exponent of -0.7.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0124v2"
    },
    {
        "title": "Four Points Beginner Risk Managers Should Learn from Jeff Holman's\n  Mistakes in the Discussion of Antifragile",
        "authors": [
            "Nassim Nicholas Taleb"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Using Jeff Holman's comments in Quantitative Finance to illustrate 4 critical\nerrors students should learn to avoid: 1) Mistaking tails (4th moment) for\nvolatility (2nd moment), 2) Missing Jensen's Inequality, 3) Analyzing the\nhedging wihout the underlying, 4) The necessity of a numeraire in finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.2524v1"
    },
    {
        "title": "Information theoretic approach for accounting classification",
        "authors": [
            "E. M. S. Ribeiro",
            "G. A. Prataviera"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  In this paper we consider an information theoretic approach for the\naccounting classification process. We propose a matrix formalism and an\nalgorithm for calculations of information theoretic measures associated to\naccounting classification. The formalism may be useful for further\ngeneralizations and computer-based implementation. Information theoretic\nmeasures, mutual information and symmetric uncertainty, were evaluated for\ndaily transactions recorded in the chart of accounts of a small company during\ntwo years. Variation in the information measures due the aggregation of data in\nthe process of accounting classification is observed. In particular, the\nsymmetric uncertainty seems to be a useful parameter for comparing companies\nover time or in different sectors or different accounting choices and\nstandards.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.2954v4"
    },
    {
        "title": "Hierarchicality of Trade Flow Networks Reveals Complexity of Products",
        "authors": [
            "Peiteng Shi",
            "Jiang Zhang",
            "Bo Yang",
            "Jingfei Luo"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  With globalization, countries are more connected than before by trading\nflows, which currently amount to at least 36 trillion dollars. Interestingly,\napproximately 30-60 percent of global exports consist of intermediate products.\nTherefore, the trade flow network of a particular product with high added\nvalues can be regarded as a value chain. The problem is weather we can\ndiscriminate between these products based on their unique flow network\nstructure. This paper applies the flow analysis method developed in ecology to\n638 trading flow networks of different products. We claim that the allometric\nscaling exponent $\\eta$ can be used to characterize the degree of\nhierarchicality of a flow network, i.e., whether the trading products flow on\nlong hierarchical chains. Then, the flow networks of products with higher added\nvalues and complexity, such as machinery&transport equipment with larger\nexponents, are highlighted. These higher values indicate that their trade flow\nnetworks are more hierarchical. As a result, without extra data such as global\ninput-output table, we can identify the product categories with higher\ncomplexity and the relative importance of a country in the global value chain\nsolely by the trading network.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.3103v1"
    },
    {
        "title": "Wealth distribution and collective knowledge. A Boltzmann approach",
        "authors": [
            "Lorenzo Pareschi",
            "Giuseppe Toscani"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We introduce and discuss a nonlinear kinetic equation of Boltzmann type which\ndescribes the influence of knowledge in the evolution of wealth in a system of\nagents which interact through the binary trades introduced in Cordier,\nPareschi, Toscani, J. Stat. Phys. 2005. The trades, which include both saving\npropensity and the risks of the market, are here modified in the risk and\nsaving parameters, which now are assumed to depend on the personal degree of\nknowledge. The numerical simulations show that the presence of knowledge has\nthe potential to produce a class of wealthy agents and to account for a larger\nproportion of wealth inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.4550v1"
    },
    {
        "title": "Propagation of Economic Shocks in Input-Output Networks: A Cross-Country\n  Analysis",
        "authors": [
            "Martha G. Alatriste Contreras",
            "Giorgio Fagiolo"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  This paper investigates how economic shocks propagate and amplify through the\ninput-output network connecting industrial sectors in developed economies. We\nstudy alternative models of diffusion on networks and we calibrate them using\ninput-output data on real-world inter-sectoral dependencies for several\nEuropean countries before the Great Depression. We show that the impact of\neconomic shocks strongly depends on the nature of the shock and country size.\nShocks that impact on final demand without changing production and the\ntechnological relationships between sectors have on average a large but very\nhomogeneous impact on the economy. Conversely, when shocks change also the\nmagnitudes of input-output across-sector interdependencies (and possibly sector\nproduction), the economy is subject to predominantly large but more\nheterogeneous avalanche sizes. In this case, we also find that: (i) the more a\nsector is globally central in the country network, the largest its impact; (ii)\nthe largest European countries, such as those constituting the core of the\nEuropean Union's economy, typically experience the largest avalanches,\nsignaling their intrinsic higher vulnerability to economic shocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.4704v2"
    },
    {
        "title": "Why free markets die: An evolutionary perspective",
        "authors": [
            "Eduardo Viegas",
            "Stuart P. Cockburn",
            "Henrik Jeldtoft Jensen",
            "Geoffrey B. West"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Company mergers and acquisitions are often perceived to act as catalysts for\ncorporate growth in free markets systems: it is conventional wisdom that those\nactivities lead to better and more efficient markets. However, the broad\nadoption of this perception into corporate strategy is prone to result in a\nless diverse and more unstable environment, dominated by either very large or\nvery small niche entities. We show here that ancestry, i.e. the cumulative\nhistory of mergers, is the key characteristic that encapsulates the diverse\nrange of drivers behind mergers and acquisitions, across a range of industries\nand geographies. A long-term growth analysis reveals that entities which have\nbeen party to fewer mergers tend to grow faster than more highly acquisitive\nbusinesses.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.5314v1"
    },
    {
        "title": "Release of the Kraken: A Novel Money Multiplier Equation's Debut in 21st\n  Century Banking",
        "authors": [
            "Brian P. Hanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Historically, the banking multiplier has been in a range of 4 to 100, with\n25% to 1% reserve ratios at most layers of the banking system encompassing the\nmajority of its range in recent centuries. Here it is shown that multipliers\nover 1 000 can occur from a new mechanism in banking. This new multiplier uses\na default insurance note to insure an outstanding loan in order to return the\nvalue of the insured amount into capital. The economic impact of this invention\nis calculably greater than the original invention of reserve banking. The\nconsequence of this lending invention is to render the existing money\nmultiplier equations of reserve banking obsolete where it occurs. The equations\ndescribing this new multiplier do not converge. Each set of parameters for\nreserve percentage, nesting depth, etc. creates a unique logarithmic curve\nrather than approaching a limit. Thus it is necessary to show the behavior of\nthis new equation by numerical methods. Understanding this new multiplier and\nassociated issues is necessary for economic analyses of the Global Financial\nCrisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.7344v1"
    },
    {
        "title": "Quantum Brownian motion model for the stock market",
        "authors": [
            "Xiangyi Meng",
            "Jian-Wei Zhang",
            "Hong Guo"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  It is believed by the majority today that the efficient market hypothesis is\nimperfect because of market irrationality. Using the physical concepts and\nmathematical structures of quantum mechanics, we construct an econophysics\nframework for the stock market, based on which we analogously map massive\nnumbers of single stocks into a reservoir consisting of many quantum harmonic\noscillators and their stock index into a typical quantum open system--a quantum\nBrownian particle. In particular, the irrationality of stock transactions is\nquantitatively considered as the Planck constant within Heisenberg's\nuncertainty relationship of quantum mechanics in an analogous manner. We\nanalyze real stock data of Shanghai Stock Exchange of China and investigate\nfat-tail phenomena and non-Markovian behaviors of the stock index with the\nassistance of the quantum Brownian motion model, thereby interpreting and\nstudying the limitations of the classical Brownian motion model for the\nefficient market hypothesis from a new perspective of quantum open system\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.3512v2"
    },
    {
        "title": "Quantum spatial-periodic harmonic model for daily price-limited stock\n  markets",
        "authors": [
            "Xiangyi Meng",
            "Jian-Wei Zhang",
            "Jingjing Xu",
            "Hong Guo"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We investigate the behavior of stocks in daily price-limited stock markets by\npurposing a quantum spatial-periodic harmonic model. The stock price is\npresumed to oscillate and damp in a quantum spatial-periodic harmonic\noscillator potential well. Complicated non-linear relations including\ninter-band positive correlation and intra-band negative correlation between the\nvolatility and the trading volume of stocks are derived by considering the\nenergy band structure of the model. The validity of price limitation is then\nexamined and abnormal phenomena of a price-limited stock market (Shanghai Stock\nExchange) of China are studied by applying our quantum model.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.4490v1"
    },
    {
        "title": "Micro and Macro Benefits of Random Investments in Financial Markets",
        "authors": [
            "Alessio Emanuele Biondo",
            "Alessandro Pluchino",
            "Andrea Rapisarda"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  In this paper, making use of recent statistical physics techniques and\nmodels, we address the specific role of randomness in financial markets, both\nat the micro and the macro level. In particular, we review some recent results\nobtained about the effectiveness of random strategies of investment, compared\nwith some of the most used trading strategies for forecasting the behavior of\nreal financial indexes. We also push forward our analysis by means of a\nSelf-Organized Criticality model, able to simulate financial avalanches in\ntrading communities with different network topologies, where a Pareto-like\npower law behavior of wealth spontaneously emerges. In this context, we present\nnew findings and suggestions for policies based on the effects that random\nstrategies can have in terms of reduction of dangerous financial extreme\nevents, i.e. bubbles and crashes.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.5805v1"
    },
    {
        "title": "Decision-theoretic approaches to non-knowledge in economics",
        "authors": [
            "Ekaterina Svetlova",
            "Henk van Elst"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We review two strands of conceptual approaches to the formal representation\nof a decision maker's non-knowledge at the initial stage of a static\none-person, one-shot decision problem in economic theory. One focuses on\nrepresentations of non-knowledge in terms of probability measures over sets of\nmutually exclusive and exhaustive consequence-relevant states of Nature, the\nother deals with unawareness of potentially important events by means of sets\nof states that are less complete than the full set of consequence-relevant\nstates of Nature. We supplement our review with a brief discussion of\nunresolved matters in both approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.0787v1"
    },
    {
        "title": "Impacts of Regional Trade Agreements(RTAs) on Food Security: A Case of\n  ASEAN Free Trade Agreement",
        "authors": [
            "H. M. S. P. Herath",
            "Cao Liang",
            "Chen Yongbing"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Discriminatory trade liberalization policies are becoming more popular among\nworld economies. Countries are motivated to enter for regional trade agreements\nto capture faster economic growth for alleviating poverty. In developing\neconomies like most of the member countries of the Association of South East\nAsian Nations (ASEAN), a sizeable portion of people are suffering from poverty\nby exposing them to food insecurity. Low level of income and low productivity\nof agricultural sector have augmented the severity of food insecurity of those\npeople. Discriminatory trade liberalization policies are expected to reduce\npoverty and strengthen the food security. The objective of this paper is to\nexamine the effect of ASEAN Free Trade Agreement (AFTA) on food security of its\nmember countries. The multiple regression analysis in panel data was employed\nto disentangle the impacts of trade liberalization on food securit y with use\nof regional trade agreement dummy variable. The finding of the study supports\nthat AFTA has influenced positively on food security of its member nations.\nAfter the formation of AFTA, the level of per-capita daily dietary energy\nsupply of the member countries has been increased moderately over time.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2677v1"
    },
    {
        "title": "Bank-firm credit network in Japan. An analysis of a bipartite network",
        "authors": [
            "Luca Marotta",
            "Salvatore Miccichè",
            "Yoshi Fujiwara",
            "Hiroshi Iyetomi",
            "Hideaki Aoyama",
            "Mauro Gallegati",
            "Rosario N. Mantegna"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We present an analysis of the credit market of Japan. The analysis is\nperformed by investigating the bipartite network of banks and firms which is\nobtained by setting a link between a bank and a firm when a credit relationship\nis present in a given time window. In our investigation we focus on a community\ndetection algorithm which is identifying communities composed by both banks and\nfirms. We show that the clusters obtained by directly working on the bipartite\nnetwork carry information about the networked nature of the Japanese credit\nmarket. Our analysis is performed for each calendar year during the time period\nfrom 1980 to 2011. Specifically, we obtain communities of banks and networks\nfor each of the 32 investigated years, and we introduce a method to track the\ntime evolution of these communities on a statistical basis. We then\ncharacterize communities by detecting the simultaneous over-expression of\nattributes of firms and banks. Specifically, we consider as attributes the\neconomic sector and the geographical location of firms and the type of banks.\nIn our 32 year long analysis we detect a persistence of the over-expression of\nattributes of clusters of banks and firms together with a slow dynamics of\nchanges from some specific attributes to new ones. Our empirical observations\nshow that the credit market in Japan is a networked market where the type of\nbanks, geographical location of firms and banks and economic sector of the firm\nplay a role in shaping the credit relationships between banks and firms.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.5429v1"
    },
    {
        "title": "Fokker-Planck Description of Wealth Dynamics and the Origin of Pareto's\n  Law",
        "authors": [
            "Bruce M. Boghosian"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The so-called \"Yard-Sale Model\" of wealth distribution posits that wealth is\ntransferred between economic agents as a result of transactions whose size is\nproportional to the wealth of the less wealthy agent. In recent work [B.M.\nBoghosian, \"Kinetics of Wealth and the Pareto Law,\" {\\it Phys. Rev. E} {\\bf 89}\n(2014) 042804], it was shown that this results in a Fokker-Planck equation\ngoverning the distribution of wealth. With the addition of a mechanism for\nwealth redistribution, it was further shown that this model results in\nstationary wealth distributions that are very similar in form to Pareto's well\nknown law. In this paper, a much simpler derivation of that Fokker-Planck\nequation is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.6851v1"
    },
    {
        "title": "Communication impacting financial markets",
        "authors": [
            "Jorgen Vitting Andersen",
            "Ioannis Vrontos",
            "Petros Dellaportas",
            "Serge Galam"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Behavioral finance has become an increasingly important subfield of finance.\nHowever the main parts of behavioral finance, prospect theory included,\nunderstand financial markets through individual investment behavior. Behavioral\nfinance thereby ignores any interaction between participants. We introduce a\nsocio-financial model that studies the impact of communication on the pricing\nin financial markets. Considering the simplest possible case where each market\nparticipant has either a positive (bullish) or negative (bearish) sentiment\nwith respect to the market, we model the evolution of the sentiment in the\npopulation due to communication in subgroups of different sizes. Nonlinear\nfeedback effects between the market performance and changes in sentiments are\ntaking into account by assuming that the market performance is dependent on\nchanges in sentiments (e.g. a large sudden positive change in bullishness would\nlead to more buying). The market performance in turn has an impact on the\nsentiment through the transition probabilities to change an opinion in a group\nof a given size. The idea is that if for example the market has observed a\nrecent downturn, it will be easier for even a bearish minority to convince a\nbullish majority to change opinion compared to the case where the meeting takes\nplace in a bullish upturn of the market. Within the framework of our proposed\nmodel, financial markets stylized facts such as volatility clustering and\nextreme events may be perceived as arising due to abrupt sentiment changes via\nongoing communication of the market participants. The model introduces a new\nvolatility measure which is apt of capturing volatility clustering and from\nmaximum likelihood analysis we are able to apply the model to real data and\ngive additional long term insight into where a market is heading.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2550v1"
    },
    {
        "title": "A study of Methods from Statistical Mechanics applied to income\n  distribution",
        "authors": [
            "Elvis Oltean",
            "Fedor Kusmartsev"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We applied Dirac distribution, Bose-Einstein distribution, and occasionally\nBoltzmann-Gibbs distribution in order to determine which is optimal for income\ndistribution on a large pool of countries. The best fit to the data was\nobserved in the case of Fermi-Dirac distribution, for which the coefficient of\ndetermination showed the best goodness of fit to the data. Using this\ndistribution for data (spun throughout more years), we obtained the underlying\ncritical parameters of annual income distribution such as chemical potential\nand temperature. The next step was to explore the evolution of income using\neconomic analogues to chemical potential and temperature. Using as background\nthe analogy made by Yakovenko between temperature from thermodynamic systems\nand nominal income from Economics, we found other analogies that would allow\nfurther analysis and explanation of income.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3128v2"
    },
    {
        "title": "An Econophysical dynamical approach of expenditure and income\n  distribution in the UK",
        "authors": [
            "Elvis Oltean",
            "Fedor Kusmartsev"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We extend the exploration regarding dynamical approach of macroeconomic\nvariables by tackling systematically expenditure using Statistical Physics\nmodels (for the first time to the best of our knowledge). Also, using\npolynomial distribution which characterizes the behavior of dynamical systems\nin certain situations, we extend also our analysis to mean income data from the\nUK that span for a time interval of 35 years. We find that most of the values\nfor coefficient of determination obtained from fitting the data from\nconsecutive years analysis to be above 80%. We used for our analysis first\ndegree polynomial, but higher degree polynomials and longer time intervals\nbetween the years considered can dramatically increase goodness of the fit. As\nthis methodology was applied successfully to income and wealth, we can conclude\nthat macroeconomic systems can be treated similarly to dynamic systems from\nPhysics. Subsequently, the analysis could be extended to other macroeconomic\nindicators.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3851v2"
    },
    {
        "title": "An econophysical approach of polynomial distribution applied to income\n  and expenditure",
        "authors": [
            "Elvis Oltean"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Polynomial distribution can be applied to dynamical systems in certain\nsituations. Macroeconomic systems characterized by economic variables such as\nincome and wealth can be modelled similarly using polynomials. We extend our\nprevious work to data regarding income from a more diversified pool of\ncountries, which contains developed countries with high income, developed\ncountries with middle income, developing and underdeveloped countries. Also,\nfor the first time we look at the applicability of polynomial distribution to\nexpenditure (consumption). Using cumulative distribution function, we found\nthat polynomials are applicable with a high degree of success to the\ndistribution of income to all countries considered without significant\ndifferences. Moreover, expenditure data can be fitted very well by this\npolynomial distribution. We considered a distribution to be robust if the\nvalues for coefficient of determination are higher than 90%. Using this\ncriterion, we decided the degree for the polynomials used in our analysis by\ntrying to minimize the number of coefficients, respectively first or second\ndegree. Lastly, we look at possible correlation between the values from\ncoefficient of determination and Gini coefficient for disposable income.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3860v2"
    },
    {
        "title": "A statistical physics analysis of expenditure in the UK",
        "authors": [
            "Elvis Oltean",
            "Fedor Kusmartsev"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Most papers which explored so far macroeconomic variables took into account\nincome and wealth. Equally important as the previous macroeconomic variables is\nthe expenditure or consumption, which shows the amount of goods and services\nthat a person or a household purchased. Using statistical distributions from\nPhysics, such as Fermi-Dirac and polynomial distributions, we try to fit the\ndata regarding the expenditure distribution divided in deciles of population\naccording to their income (gross and disposable expenditure are taken into\naccount). Using coefficient of determination as theoretical tool in order to\nassess the degree of success for these distributions, we find that both\ndistributions are really robust in describing the expenditure distribution,\nregardless the data set or the methodology used to calculate the expenditure\nvalues for the deciles of income. This is the first paper to our knowledge\nwhich tackles expenditure, especially using a method to describe expenditure\nsuch as lower limit on expenditure. This is also relevant since it allows the\napproach of macroeconomic systems using more variables characterizing their\nactivity, can help in the investigation of living standards and inequality, and\npoints to more theoretical explorations which can be very useful for the\nEconomics and business practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3865v2"
    },
    {
        "title": "Global Value Trees",
        "authors": [
            "Zhen Zhu",
            "Michelangelo Puliga",
            "Federica Cerina",
            "Alessandro Chessa",
            "Massimo Riccaboni"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The fragmentation of production across countries has become an important\nfeature of the globalization in recent decades and is often conceptualized by\nthe term, global value chains (GVCs). When empirically investigating the GVCs,\nprevious studies are mainly interested in knowing how global the GVCs are\nrather than how the GVCs look like. From a complex networks perspective, we use\nthe World Input-Output Database (WIOD) to study the global production system.\nWe find that the industry-level GVCs are indeed not chain-like but are better\ncharacterized by the tree topology. Hence, we compute the global value trees\n(GVTs) for all the industries available in the WIOD. Moreover, we compute an\nindustry importance measure based on the GVTs and compare it with other network\ncentrality measures. Finally, we discuss some future applications of the GVTs.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4694v2"
    },
    {
        "title": "A polynomial distribution applied to income and wealth distribution",
        "authors": [
            "Elvis Oltean",
            "Fedor Kusmartsev"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Income and wealth distribution affect stability of a society to a large\nextent and high inequality affects it negatively. Moreover, in the case of\ndeveloped countries, recently has been proven that inequality is closely\nrelated to all negative phenomena affecting society. So far, Econophysics\npapers tried to analyse income and wealth distribution by employing\ndistributions such as Fermi-Dirac, Bose-Einstein, Maxwell-Boltzmann, lognormal\n(Gibrat), and exponential. Generally, distributions describe mostly income and\nless wealth distribution for low and middle income segment of population, which\naccounts about 90% of the population. Our approach is based on a totally new\ndistribution, not used so far in the literature regarding income and wealth\ndistribution. Using cumulative distribution method, we find that polynomial\nfunctions, regardless of their degree (first, second, or higher), can describe\nwith very high accuracy both income and wealth distribution. Moreover, we find\nthat polynomial functions describe income and wealth distribution for entire\npopulation including upper income segment for which traditionally Pareto\ndistribution is used.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4866v2"
    },
    {
        "title": "The Precautionary Principle (with Application to the Genetic\n  Modification of Organisms)",
        "authors": [
            "Nassim Nicholas Taleb",
            "Rupert Read",
            "Raphael Douady",
            "Joseph Norman",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We present a non-naive version of the Precautionary (PP) that allows us to\navoid paranoia and paralysis by confining precaution to specific domains and\nproblems. PP is intended to deal with uncertainty and risk in cases where the\nabsence of evidence and the incompleteness of scientific knowledge carries\nprofound implications and in the presence of risks of \"black swans\", unforeseen\nand unforeseable events of extreme consequence. We formalize PP, placing it\nwithin the statistical and probabilistic structure of ruin problems, in which a\nsystem is at risk of total failure, and in place of risk we use a formal\nfragility based approach. We make a central distinction between 1) thin and fat\ntails, 2) Local and systemic risks and place PP in the joint Fat Tails and\nsystemic cases. We discuss the implications for GMOs (compared to Nuclear\nenergy) and show that GMOs represent a public risk of global harm (while harm\nfrom nuclear energy is comparatively limited and better characterized). PP\nshould be used to prescribe severe limits on GMOs.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.5787v1"
    },
    {
        "title": "Stock fluctuations are correlated and amplified across networks of\n  interlocking directorates",
        "authors": [
            "Serguei Saavedra",
            "Luis J. Gilarranz",
            "Rudolf P. Rohr",
            "Michael Schnabel",
            "Brian Uzzi",
            "Jordi Bascompte"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Traded corporations are required by law to have a majority of outside\ndirectors on their board. This requirement allows the existence of directors\nwho sit on the board of two or more corporations at the same time, generating\nwhat is commonly known as interlocking directorates. While research has shown\nthat networks of interlocking directorates facilitate the transmission of\ninformation between corporations, little is known about the extent to which\nsuch interlocking networks can explain the fluctuations of stock price returns.\nYet, this is a special concern since the risk of amplifying stock fluctuations\nis latent. To answer this question, here we analyze the board composition,\ntraders' perception, and stock performance of more than 1500 US traded\ncorporations from 2007-2011. First, we find that the fewer degrees of\nseparation between two corporations in the interlocking network, the stronger\nthe temporal correlation between their stock price returns. Second, we find\nthat the centrality of traded corporations in the interlocking network\ncorrelates with the frequency at which financial traders talk about such\ncorporations, and this frequency is in turn proportional to the corresponding\ntraded volume. Third, we show that the centrality of corporations was\nnegatively associated with their stock performance in 2008, the year of the big\nfinancial crash. These results suggest that the strategic decisions made by\ninterlocking directorates are strongly followed by stock analysts and have the\npotential to correlate and amplify the movement of stock prices during\nfinancial crashes. These results may have relevant implications for scholars,\ninvestors, and regulators.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.6646v1"
    },
    {
        "title": "Interactions between financial and environmental networks in OECD\n  countries",
        "authors": [
            "Franco Ruzzenenti",
            "Andreas Joseph",
            "Elisa Ticci",
            "Pietro Vozzella",
            "Giampaolo Gabbi"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  We analyse a multiplex of networks between OECD countries during the decade\n2002-2010, which consists of five financial layers, given by foreign direct\ninvestment, equity securities, short-term, long-term and total debt securities,\nand five environmental layers, given by emissions of N O x, P M 10 SO 2, CO 2\nequivalent and the water footprint associated with international trade. We\npresent a new measure of cross-layer correlations between flows in different\nlayers based on reciprocity. For the assessment of results, we implement a null\nmodel for this measure based on the exponential random graph theory. We find\nthat short-term financial flows are more correlated with environmental flows\nthan long-term investments. Moreover, the correlations between reverse\nfinancial and environmental flows (i.e. flows of different layers going in\nopposite directions) are generally stronger than correlations between synergic\nflows (flows going in the same direction). This suggests a trade-off between\nfinancial and environmental layers, where, more financialised countries display\nhigher correlations between outgoing financial flows and incoming environmental\nflows from lower financialised countries, which could have important policy\nimplications. Five countries are identified as hubs in this finance-environment\nmultiplex: The United States, France, Germany, Belgium-Luxembourg and the\nUnited Kingdom.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.04992v2"
    },
    {
        "title": "Cascades in multiplex financial networks with debts of different\n  seniority",
        "authors": [
            "Charles D. Brummitt",
            "Teruyoshi Kobayashi"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The seniority of debt, which determines the order in which a bankrupt\ninstitution repays its debts, is an important and sometimes contentious feature\nof financial crises, yet its impact on system-wide stability is not well\nunderstood. We capture seniority of debt in a multiplex network, a graph of\nnodes connected by multiple types of edges. Here, an edge between banks denotes\na debt contract of a certain level of seniority. Next we study cascading\ndefault. There exist multiple kinds of bankruptcy, indexed by the highest level\nof seniority at which a bank cannot repay all its debts. Self-interested banks\nwould prefer that all their loans be made at the most senior level. However,\nmixing debts of different seniority levels makes the system more stable, in\nthat it shrinks the set of network densities for which bankruptcies spread\nwidely. We compute the optimal ratio of senior to junior debts, which we call\nthe optimal seniority ratio, for two uncorrelated Erdos-Renyi networks. If\ninstitutions erode their buffer against insolvency, then this optimal seniority\nratio rises; in other words, if default thresholds fall, then more loans should\nbe senior. We generalize the analytical results to arbitrarily many levels of\nseniority and to heavy-tailed degree distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.05400v3"
    },
    {
        "title": "Information in stock prices and some consequences: A model-free approach",
        "authors": [
            "Yannis G. Yatracos"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  The price of a stock will rarely follow the assumed model and a curious\ninvestor or a Regulatory Authority may wish to obtain a probability model the\nprices support. A risk neutral probability ${\\cal P}^*$ for the stock's price\nat time $T$ is determined in closed form from the prices before $T$ without\nassuming a price model. The findings indicate that ${\\cal P}^*$ may be a\nmixture. Under mild conditions on the prices the necessary and sufficient\ncondition to obtain ${\\cal P}^*$ is the coincidence at $T$ of the stock price\nranges assumed by the stock's trader and buyer. This result clarifies the\nrelation between market's informational efficiency and the arbitrage-free\noption pricing methodology. It also shows that in an incomplete market there\nare risk neutral probabilities not supported by each stock and their use can be\nlimited. ${\\cal P}^*$-price $C$ for the stock's European call option expiring\nat $T$ is obtained. Among other results it is shown for \"calm\" prices, like the\nlog-normal, that i) $C$ is the Black-Scholes-Merton price thus confirming its\nvalidity for various stock prices, ii) the buyer's price carries an\nexponentially increasing volatility premium and its difference with $C$\nprovides a measure of the market risk premium.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.07473v2"
    },
    {
        "title": "Long-run evolution of the global economy - Part 2: Hindcasts of\n  innovation and growth",
        "authors": [
            "Timothy J. Garrett"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Long-range climate forecasts use integrated assessment models to link the\nglobal economy to greenhouse gas emissions. This paper evaluates an alternative\neconomic framework outlined in part 1 of this study (Garrett, 2014) that\napproaches the global economy using purely physical principles rather than\nexplicitly resolved societal dynamics. If this model is initialized with\neconomic data from the 1950s, it yields hindcasts for how fast global economic\nproduction and energy consumption grew between 2000 and 2010 with skill scores\n> 90 % relative to a model of persistence in trends. The model appears to\nattain high skill partly because there was a strong impulse of discovery of\nfossil fuel energy reserves in the mid-twentieth century that helped\ncivilization to grow rapidly as a deterministic physical response. Forecasting\nthe coming century may prove more of a challenge because the effect of the\nenergy impulse appears to have nearly run its course. Nonetheless, an\nunderstanding of the external forces that drive civilization may help\ndevelopment of constrained futures for the coupled evolution of civilization\nand climate during the Anthropocene.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00233v1"
    },
    {
        "title": "Econo- and socio- physics based remarks on the economical growth of the\n  World",
        "authors": [
            "Rzoska Agata Angelika"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  It has been shown that the long term evolution of the Gross Product of the\nWorld after World War II can be well portrayed by the exponential function with\nthe crossover at the year 1973, cinsiding with the Oil Crisis onset. For the\nthe Standard and Poor 500 index the single exponential behavior extends down to\nat least the mid of the nineteen century. It is notable that the detailed\nshort-term insight focused on the last quarter of century revealed the\nemergence of the power like dependence. However, such dependences can be\nintroduced only when taking into account the behavior at reference-baselines\nyears. The possible relationship to the growth/death evolution of\nmicroorganisms is also discussed. The report proposes the new discussion of the\npast and nowadays time of the global economy. It recalls econonophysics and\nsociophysics as disciplines within which the effective parameterization of\ntrends is possible. Finally, possible future trends are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.05081v1"
    },
    {
        "title": "Modeling and Estimation of the Risk When Choosing a Provider",
        "authors": [
            "Ekaterina Sorokina"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The paper provides an algorithm for the risk estimation when a company\nselects an outsourcing service provider for innovation product. Calculations\nare based on expert surveys conducted among customers and among providers of\noutsourcing. The surveys assessed the degree of materiality of species at risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.05294v1"
    },
    {
        "title": "Switching Economics for Physics and the Carbon Price Inflation: Problems\n  in Integrated Assessment Models and their Implications",
        "authors": [
            "Sgouris Sgouridis",
            "Abdulla Kaya",
            "Denes Csala"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Integrated Assessment Models (IAMs) are mainstay tools for assessing the\nlong-term interactions between climate and the economy and for deriving optimal\npolicy responses in the form of carbon prices. IAMs have been criticized for\ncontroversial discount rate assumptions, arbitrary climate damage functions,\nand the inadequate handling of potentially catastrophic climate outcomes. We\nreview these external shortcomings for prominent IAMs before turning our focus\non an internal modeling fallacy: the widespread misapplication of the Constant\nElasticity of Substitution (CES) function for the technology transitions\nmodeled by IAMs. Applying CES, an economic modeling approach, on technical\nfactor inputs over long periods where an entire factor (the greenhouse gas\nemitting fossil fuel inputs) must be substituted creates artifacts that fail to\nmatch the S-curve patterns observed historically. A policy critical result, the\nmonotonically increasing cost of carbon, a universal feature of IAMs, is called\ninto question by showing that it is unrealistic as it is an artifact of the\nmodeling approach and not representative of the technical substitutability\npotential nor of the expected cost of the technologies. We demonstrate this\nfirst through a simple but representative example of CES application on the\nenergy system and with a sectoral discussion of the actual fossil substitution\ncosts. We propose a methodological modification using dynamically varying\nelasticity of substitution as a plausible alternative to model the energy\ntransition in line with the historical observations and technical realities\nwithin the existing modeling systems. Nevertheless, a fundamentally different\napproach based on physical energy principles would be more appropriate.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06196v1"
    },
    {
        "title": "Why have asset price properties changed so little in 200 years",
        "authors": [
            "Jean-Philippe Bouchaud",
            "Damien Challet"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We first review empirical evidence that asset prices have had episodes of\nlarge fluctuations and been inefficient for at least 200 years. We briefly\nreview recent theoretical results as well as the neurological basis of trend\nfollowing and finally argue that these asset price properties can be attributed\nto two fundamental mechanisms that have not changed for many centuries: an\ninnate preference for trend following and the collective tendency to exploit as\nmuch as possible detectable price arbitrage, which leads to destabilizing\nfeedback loops.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.00634v1"
    },
    {
        "title": "Is it \"natural\" to expect Economics to become a part of the Natural\n  Sciences?",
        "authors": [
            "Arnab Chatterjee"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We are in the middle of a complex debate as to whether Economics is really a\nproper natural science. The 'Discussion & Debate' issue of this Euro. Phys. J.\nSpecial Topic volume is: 'Can economics be a Physical Science?' I discuss some\naspects here.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.01920v2"
    },
    {
        "title": "The wage transition in developed countries and its implications for\n  China",
        "authors": [
            "Belal Baaquie",
            "Bertrand M. Roehner",
            "Qinghai Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The expression \"wage transition\" refers to the fact that over the past two or\nthree decades in all developed economies wage increases have levelled off.\nThere has been a widening divergence and decoupling between wages on the one\nhand and GDP per capita on the other hand. Yet, in China wages and GDP per\ncapita climbed in sync (at least up to now). In the first part of the paper we\npresent comparative statistical evidence which measures the extent of the wage\ntransition effect. In a second part we consider the reasons of this phenomenon,\nin particular we explain how the transfers of labor from low productivity\nsectors (such as agriculture) to high productivity sectors (such as\nmanufacturing) are the driver of productivity growth, particularly through\ntheir synergetic effects. Although rural flight represents only one of these\neffects, it is certainly the most visible because of the geographical\nrelocation that it implies; it is also the most well-defined statistically.\nMoreover, it will be seen that it is a good indicator of the overall\nproductivity and attractivity of the non-agricultural sector. Because this\nmodel accounts fairly well for the observed evolution in industrialized\ncountries, we use it to predict the rate of Chinese economic growth in the\ncoming decades. Our forecast for the average annual growth of real wages ranges\nfrom 4% to 6% depending on how well China will control the development of its\nhealthcare industry.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.01949v1"
    },
    {
        "title": "The Accounting Network: how financial institutions react to systemic\n  crisis",
        "authors": [
            "Andrea Flori",
            "Giuseppe Pappalardo",
            "Michelangelo Puliga",
            "Alessandro Chessa",
            "Fabio Pammolli"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The role of Network Theory in the study of the financial crisis has been\nwidely spotted in the latest years. It has been shown how the network topology\nand the dynamics running on top of it can trigger the outbreak of large\nsystemic crisis. Following this methodological perspective we introduce here\nthe Accounting Network, i.e. the network we can extract through vector\nsimilarities techniques from companies' financial statements. We build the\nAccounting Network on a large database of worldwide banks in the period\n2001-2013, covering the onset of the global financial crisis of mid-2007. After\na careful data cleaning, we apply a quality check in the construction of the\nnetwork, introducing a parameter (the Quality Ratio) capable of trading off the\nsize of the sample (coverage) and the representativeness of the financial\nstatements (accuracy). We compute several basic network statistics and check,\nwith the Louvain community detection algorithm, for emerging communities of\nbanks. Remarkably enough sensible regional aggregations show up with the\nJapanese and the US clusters dominating the community structure, although the\npresence of a geographically mixed community points to a gradual convergence of\nbanks into similar supranational practices. Finally, a Principal Component\nAnalysis procedure reveals the main economic components that influence\ncommunities' heterogeneity. Even using the most basic vector similarity\nhypotheses on the composition of the financial statements, the signature of the\nfinancial crisis clearly arises across the years around 2008. We finally\ndiscuss how the Accounting Networks can be improved to reflect the best\npractices in the financial statement analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.01976v1"
    },
    {
        "title": "Global Gauge Symmetries, Risk-Free Portfolios, and the Risk-Free Rate",
        "authors": [
            "Martin Gremm"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We define risk-free portfolios using three gauge invariant differential\noperators that require such portfolios to be insensitive to price changes, to\nbe self-financing, and to produce a zero real return so there are no risk-free\nprofits. This definition identifies the risk-free rate as the return of an\ninfinitely diversified portfolio rather than as an arbitrary external\nparameter. The risk-free rate measures the rate of global price rescaling,\nwhich is a gauge symmetry of economies. We explore the properties of risk-free\nrates, rederive the Black Scholes equation with a new interpretation of the\nrisk-free rate parameter as a that background gauge field, and discuss gauge\ninvariant discounting of cash flows.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.03551v1"
    },
    {
        "title": "A Possibilistic and Probabilistic Approach to Precautionary Saving",
        "authors": [
            "Irina Georgescu",
            "Adolfo Cristóbal Campoamor",
            "Ana Maria Lucia Casademunt"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  This paper proposes two mixed models to study a consumer's optimal saving in\nthe presence of two types of risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07760v1"
    },
    {
        "title": "The role of industry, occupation, and location specific knowledge in the\n  survival of new firms",
        "authors": [
            "C. Jara-Figueroa",
            "Bogang Jun",
            "Edward Glaeser",
            "Cesar Hidalgo"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  How do regions acquire the knowledge they need to diversify their economic\nactivities? How does the migration of workers among firms and industries\ncontribute to the diffusion of that knowledge? Here we measure the industry,\noccupation, and location-specific knowledge carried by workers from one\nestablishment to the next using a dataset summarizing the individual work\nhistory for an entire country. We study pioneer firms--firms operating in an\nindustry that was not present in a region--because the success of pioneers is\nthe basic unit of regional economic diversification. We find that the growth\nand survival of pioneers increase significantly when their first hires are\nworkers with experience in a related industry, and with work experience in the\nsame location, but not with past experience in a related occupation. We compare\nthese results with new firms that are not pioneers and find that\nindustry-specific knowledge is significantly more important for pioneer than\nnon-pioneer firms. To address endogeneity we use Bartik instruments, which\nleverage national fluctuations in the demand for an activity as shocks for\nlocal labor supply. The instrumental variable estimates support the finding\nthat industry-related knowledge is a predictor of the survival and growth of\npioneer firms. These findings expand our understanding of the micro-mechanisms\nunderlying regional economic diversification events.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.01237v1"
    },
    {
        "title": "Resource and Competence (Internal) View vs. Environment and Market\n  (External) View when defining a Business",
        "authors": [
            "Yngve Dahle",
            "Martin Steinert",
            "Anh Nguyen Duc",
            "Roman Chizhevskiy"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Startups is a popular phenomenon that has a significant impact on global\neconomy growth, innovation and society development. However, there is still\ninsufficient understanding about startups, particularly, how to start a new\nbusiness in the relation to consequent performance. Toward this knowledge, we\nhave performed an empirical study regarding the differences between a Resource\nand Competence View (Internal) vs Environment and Market View (External) when\ndefining a Business. 701 entrepreneurs have reflected on their startups on nine\nclasses of Resources (values, vision, personal objectives, employees and\npartners, buildings and rental contracts, cash and credit, patents, IPR's and\nbrands, products and services and finally revenues and grants) and three\nelements of the Business Mission (\"KeyContribution\", \"KeyMarket\" and\n\"Distinction\"). It seems to be a tendency to favour the Internal View over the\nExternal View. This tendency is clearer in Stable Economies (Europe) than in\nEmerging Economies (South Africa). There seems to be a co-variation between the\ntendency to favour the Internal View and the tendency to focus on adding\nResources. Finally, we found that an order-based analysis seems to explain the\ndifferences between the two views better than a number-based method.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01487v1"
    },
    {
        "title": "A Game of Tax Evasion: evidences from an agent-based model",
        "authors": [
            "L. S. Di Mauro",
            "A. Pluchino",
            "A. E. Biondo"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This paper presents a simple agent-based model of an economic system,\npopulated by agents playing different games according to their different view\nabout social cohesion and tax payment. After a first set of simulations,\ncorrectly replicating results of existing literature, a wider analysis is\npresented in order to study the effects of a dynamic-adaptation rule, in which\ncitizens may possibly decide to modify their individual tax compliance\naccording to individual criteria, such as, the strength of their ethical\ncommitment, the satisfaction gained by consumption of the public good and the\nperceived opinion of neighbors. Results show the presence of thresholds levels\nin the composition of society - between taxpayers and evaders - which explain\nthe extent of damages deriving from tax evasion.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08146v1"
    },
    {
        "title": "The Affordable Care Act and the IRS Iterative Fixed Point Procedure",
        "authors": [
            "Samuel J. Ferguson"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We model the quantities appearing in Internal Revenue Service (IRS) tax\nguidance for calculating the health insurance premium tax credit created by the\nPatient Protection and Affordable Care Act, also called Obamacare. We ask the\nquestion of whether there is a procedure, computable by hand, which can\ncalculate the appropriate premium tax credit for any household with\nself-employment income. We give an example showing that IRS tax guidance, which\nhas had self-employed taxpayers use an iterative fixed point procedure to\ncalculate their premium tax credits since 2014, can lead to a divergent\nsequence of iterates. As a consequence, IRS guidance does not calculate\nappropriate premium tax credits for tax returns in certain income intervals,\nadversely affecting eligible beneficiaries. A bisection procedure for\ncalculating premium tax credits is proposed. We prove that this procedure\ncalculates appropriate premium tax credits for a model of simple tax returns;\nand apparently, this procedure has already been used to prepare accepted tax\nreturns. We outline the problem of finding a procedure which calculates\nappropriate premium tax credits for models of general tax returns. While the\nbisection procedure will work with the tax code in its current configuration,\nit could fail, in states which have not expanded Medicaid, if a certain\ndeduction were to revert to an earlier form. Future policy objectives might\nalso lead to further problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05424v1"
    },
    {
        "title": "Analysis of the problem of intervention control in the economy on the\n  basis of solving the problem of tuning",
        "authors": [
            "Peter Shnurkov",
            "Daniil Novikov"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  The paper proposes a new stochastic intervention control model conducted in\nvarious commodity and stock markets. The essence of the phenomenon of\nintervention is described in accordance with current economic theory. A review\nof papers on intervention research has been made. A general construction of the\nstochastic intervention model was developed as a Markov process with discrete\ntime, controlled at the time it hits the boundary of a given subset of a set of\nstates. Thus, the problem of optimal control of interventions is reduced to a\ntheoretical problem of control by the specified process or the problem of\ntuning. A general solution of the tuning problem for a model with discrete time\nis obtained. It is proved that the optimal control in such a problem is\ndeterministic and is determined by the global maximum point of the function of\ntwo discrete variables, for which an explicit analytical representation is\nobtained. It is noted that the solution of the stochastic tuning problem can be\nused as a basis for solving control problems of various technical systems in\nwhich there is a need to maintain some main parameter in a given set of its\nvalues.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10993v1"
    },
    {
        "title": "Quantum Brownian oscillator for the stock market",
        "authors": [
            "Jasmina Jeknić-Dugić",
            "Sonja Radi\\' c",
            "Igor Petrović",
            "Momir Arsenijević",
            "Miroljub Dugić"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  We pursue the quantum-mechanical challenge to the efficient market hypothesis\nfor the stock market by employing the quantum Brownian motion model. We utilize\nthe quantum Caldeira-Leggett master equation as a possible phenomenological\nmodel for the stock-market-prices fluctuations while introducing the external\nharmonic field for the Brownian particle. Two quantum regimes are of particular\ninterest: the exact regime as well as the approximate regime of the pure\ndecoherence (\"recoilless\") limit of the Caldeira-Leggett equation. By\ncalculating the standard deviation and the kurtosis for the particle's position\nobservable, we can detect deviations of the quantum-mechanical behavior from\nthe classical counterpart, which bases the efficient market hypothesis. By\nvarying the damping factor, temperature as well as the oscillator's frequency,\nwe are able to provide interpretation of different economic scenarios and\npossible situations that are not normally recognized by the efficient market\nhypothesis. Hence we recognize the quantum Brownian oscillator as a possibly\nuseful model for the realistic behavior of stock prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.10544v1"
    },
    {
        "title": "Stochastic Estimated Risk for Storage Capacity",
        "authors": [
            "Revathi Anil Kumar",
            "Mark Chamness"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Managing data storage growth is of crucial importance to businesses. Poor\npractices can lead to large data and financial losses. Access to storage\ninformation along with timely action, or capacity forecasting, are essential to\navoid these losses. In addition, ensuring high accuracy of capacity forecast\nestimates along with ease of interpretability plays an important role for any\ncustomer facing tool. In this paper, we introduce Stochastic Estimated Risk\n(SER), a tool developed at Nutanix that has been in production. SER shifts the\nfocus from forecasting a single estimate for date of attaining full capacity to\npredicting the risk associated with running out of storage capacity. Using a\nBrownian motion with drift model, SER estimates the probability that a system\nwill run out of capacity within a specific time frame. Our results showed that\na probabilistic approach is more accurate and credible, for systems with\nnon-linear patterns, compared to a regression or ensemble forecasting models.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.10552v1"
    },
    {
        "title": "Possibilistic investment models with background risk",
        "authors": [
            "Irina Georgescu"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  In the study of investment problem, aside from the investment risk the\nbackground risk appears. Both the investment risk and the background risk are\nprobabilistically described by random variables. This paper starts from the\nhypothesis that the two types of risk can be represented both probabilistically\n(by random variables) and possibilistically (by fuzzy numbers). We will study\nthree models in which the investment risk and the background risk can be: fuzzy\nnumbers, a random variabl-a fuzzy number and a fuzzy number-a random variable.\nA portfolio problem is formulated for each model and an approximate calculation\nformula of the optimal solution is proved.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.10556v1"
    },
    {
        "title": "Top performing stocks recommendation strategy for portfolio",
        "authors": [
            "Kartikay Gupta",
            "Niladri Chatterjee"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Stock return forecasting is of utmost importance in the business world. This\nhas been the favourite topic of research for many academicians since decades.\nRecently, regularization techniques have reported to tremendously increase the\nforecast accuracy of the simple regression model. Still, this model cannot\nincorporate the effect of things like a major natural disaster, large foreign\ninfluence, etc. in its prediction. Such things affect the whole stock market\nand are very unpredictable. Thus, it is more important to recommend top stocks\nrather than predicting exact stock returns. The present paper modifies the\nregression task to output value for each stock which is more suitable for\nranking the stocks by expected returns. Two large datasets consisting of\naltogether 1205 companies listed at Indian exchanges were used for\nexperimentation. Five different metrics were used for evaluating the different\nmodels. Results were also analysed subjectively through plots. The results\nshowed the superiority of the proposed techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.11013v3"
    },
    {
        "title": "The evolving liaisons between the transaction networks of Bitcoin and\n  its price dynamics",
        "authors": [
            "Alexandre Bovet",
            "Carlo Campajola",
            "Francesco Mottes",
            "Valerio Restocchi",
            "Nicolò Vallarano",
            "Tiziano Squartini",
            "Claudio J. Tessone"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Cryptocurrencies are distributed systems that allow exchanges of native\ntokens among participants, or the exchange of such tokens for fiat currencies\nin markets external to these public ledgers. The availability of their complete\nhistorical bookkeeping opens up the possibility of understanding the\nrelationship between aggregated users' behaviour and the cryptocurrency pricing\nin exchange markets. This paper analyses the properties of the transaction\nnetwork of Bitcoin. We consider four different representations of it, over a\nperiod of nine years since the Bitcoin creation and involving 16 million users\nand 283 million transactions. By analysing these networks, we show the\nexistence of causal relationships between Bitcoin price movements and changes\nof its transaction network topology. Our results reveal the interplay between\nstructural quantities, indicative of the collective behaviour of Bitcoin users,\nand price movements, showing that, during price drops, the system is\ncharacterised by a larger heterogeneity of nodes activity.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.03577v1"
    },
    {
        "title": "Nonlinear price dynamics of S&P 100 stocks",
        "authors": [
            "Gunduz Caginalp",
            "Mark DeSantis"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The methodology presented provides a quantitative way to characterize\ninvestor behavior and price dynamics within a particular asset class and time\nperiod. The methodology is applied to a data set consisting of over 250,000\ndata points of the S&P 100 stocks during 2004-2018. Using a two-way\nfixed-effects model, we uncover trader motivations including evidence of both\nunder- and overreaction within a unified setting. A nonlinear relationship is\nfound between return and trend suggesting a small, positive trend increases the\nreturn, while a larger one tends to decrease it. The shape parameters of the\nnonlinearity quantify trader motivation to buy into trends or wait for\nbargains. The methodology allows the testing of any behavioral finance bias or\ntechnical analysis concept.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04422v1"
    },
    {
        "title": "A Classification Framework for Stablecoin Designs",
        "authors": [
            "Amani Moin",
            "Emin Gün Sirer",
            "Kevin Sekniqi"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Stablecoins promise to bridge fiat currencies with the world of\ncryptocurrencies. They provide a way for users to take advantage of the\nbenefits of digital currencies, such as ability to transfer assets over the\ninternet, provide assurance on minting schedules and scarcity, and enable new\nasset classes, while also partially mitigating their volatility risks. In this\npaper, we systematically discuss general design, decompose existing stablecoins\ninto various component design elements, explore their strengths and drawbacks,\nand identify future directions.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.10098v1"
    },
    {
        "title": "Time-dependent lead-lag relationships between the VIX and VIX futures\n  markets",
        "authors": [
            "Yan-Hong Yang",
            "Ying-Hui Shao"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We utilize the symmetric thermal optimal path (TOPS) method to examine the\ndynamic interaction patterns between the VIX and VIX futures markets. We\ndocument that the VIX dominates the VIX futures more in the first few years,\nespecially before the introduction of VIX options. We further observe that the\nTOPS paths show an alternate lead-lag relationship instead of a dominance\nbetween the VIX and VIX futures in most of the time periods. Meanwhile, we find\nthat the VIX futures have been increasingly more important in the price\ndiscovery since the launch of several VIX ETPs.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.13729v1"
    },
    {
        "title": "Michael Milken: The Junk Dealer",
        "authors": [
            "Ravi Kashyap"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We take a closer look at the life and legacy of Micheal Milken. We discuss\nwhy Michael Milken, also know as the Junk Bond King, was not just any other\nKing or run-of-the-mill Junk Dealer, but \"The Junk Dealer\". We find parallels\nbetween the three parts to any magic act and what Micheal Milken did, showing\nthat his accomplishments were nothing short of a miracle. His compensation at\nthat time captures to a certain extent the magnitude of the changes he brought\nabout, the eco-system he created for businesses to flourish, the impact he had\non the wider economy and also on the future growth and development of American\nIndustry. We emphasize two of his contributions to the financial industry that\nhave grown in importance over the years. One was the impetus given to the\nPrivate Equity industry and the use of LBOs. The second was the realization\nthat thorough research was the key to success, financial and otherwise. Perhaps\nan unintended consequence of the growth in junk bonds and tailored financing\nwas the growth of Silicon valley and technology powerhouses in the California\nbay area. Investors witnessed that there was a possibility for significant\nreturns and that financial success could be had due to the risk mitigation that\nMilken demonstrated by investing in portfolios of so called high risk and low\nprofile companies. We point out the current trend in many regions of the world,\nwhich is the birth of financial and technology firms and we suggest that\nfinding innovative ways of financing could be the key to the sustained growth\nof these eco-systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.13882v1"
    },
    {
        "title": "Disturbing the Peace: Anatomy of the Hostile Takeover of China Vanke Co",
        "authors": [
            "Taurai Muvunza",
            "Terrill Frantz"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Wang Shi, a business mogul who created his empire of wealth from scratch,\nrelished in his fame and basked in the glory of his affluent business. Nothing\nlasts forever! After mastering the turbulent business of real estate\ndevelopment in his country and therefore enjoying a rising and robust stock\nprice, China Vanke Co. Ltd (\"Vanke\") founder and Chairman of the Board of\nDirectors, Wang Shi was suddenly presented with a scathing notice from the Hong\nKong Stock Exchange: rival Baoneng Group (\"Baoneng\") filed the regulatory\ndocumentation indicating that it had nicodemously acquired 5% of his company\nand was looking to buy more. Vanke case became brutal and sparked national\ncontroversy over corporate governance and the role of Chinese government in\ncapital markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.06019v2"
    },
    {
        "title": "Where do we stand in cryptocurrencies economic research? A survey based\n  on hybrid analysis",
        "authors": [
            "Aurelio F. Bariviera",
            "Ignasi Merediz-Solà"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This survey develops a dual analysis, consisting, first, in a bibliometric\nexamination and, second, in a close literature review of all the scientific\nproduction around cryptocurrencies conducted in economics so far. The aim of\nthis paper is twofold. On the one hand, proposes a methodological hybrid\napproach to perform comprehensive literature reviews. On the other hand, we\nprovide an updated state of the art in cryptocurrency economic literature. Our\nmethodology emerges as relevant when the topic comprises a large number of\npapers, that make unrealistic to perform a detailed reading of all the papers.\nThis dual perspective offers a full landscape of cryptocurrency economic\nresearch. Firstly, by means of the distant reading provided by machine learning\nbibliometric techniques, we are able to identify main topics, journals, key\nauthors, and other macro aggregates. Secondly, based on the information\nprovided by the previous stage, the traditional literature review provides a\ncloser look at methodologies, data sources and other details of the papers. In\nthis way, we offer a classification and analysis of the mounting research\nproduced in a relative short time span.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.09723v1"
    },
    {
        "title": "Physics and Derivatives -- Interview Questions and Answers",
        "authors": [
            "Alexander Lipton"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Answers to interview questions sent to a selected group of former physicists\nworking in finance. The interview will be published as part of a Special Issue\non Physics and Derivatives by The Journal of Derivatives in the second half of\n2020.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11471v1"
    },
    {
        "title": "Forecasting Models for Daily Natural Gas Consumption Considering\n  Periodic Variations and Demand Segregation",
        "authors": [
            "Ergun Yukseltan",
            "Ahmet Yucekaya",
            "Ayse Humeyra Bilge",
            "Esra Agca Aktunc"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Due to expensive infrastructure and the difficulties in storage, supply\nconditions of natural gas are different from those of other traditional energy\nsources like petroleum or coal. To overcome these challenges, supplier\ncountries require take-or-pay agreements for requested natural gas quantities.\nThese contracts have many pre-clauses; if they are not met due to low/high\nconsumption or other external factors, buyers must completely fulfill them. A\nsimilar contract is then imposed on distributors and wholesale consumers. It is\nthus important for all parties to forecast their daily, monthly, and annual\nnatural gas demand to minimize their risk. In this paper, a model consisting of\na modulated expansion in Fourier series, supplemented by deviations from\ncomfortable temperatures as a regressor is proposed for the forecast of monthly\nand weekly consumption over a one-year horizon. This model is supplemented by a\nday-ahead feedback mechanism for the forecast of daily consumption. The method\nis applied to the study of natural gas consumption for major residential areas\nin Turkey, on a yearly, monthly, weekly, and daily basis. It is shown that\nresidential heating dominates winter consumption and masks all other\nvariations. On the other hand, weekend and holiday effects are visible in\nsummer consumption and provide an estimate for residential and industrial use.\nThe advantage of the proposed method is the capability of long term projections\nand to outperform time series methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13385v1"
    },
    {
        "title": "Risk Management and Return Prediction",
        "authors": [
            "Qingyin Ge",
            "Yunuo Ma",
            "Yuezhi Liao",
            "Rongyu Li",
            "Tianle Zhu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  With the good development in the financial industry, the market starts to\ncatch people's eyes, not only by the diversified investing choices ranging from\nbonds and stocks to futures and options but also by the general \"high-risk,\nhigh-reward\" mindset prompting people to put money in the financial market.\nPeople are interested in reducing risk at a given level of return since there\nis no way of having both high returns and low risk. Many researchers have been\nstudying this issue, and the most pioneering one is Harry Markowitz's Modern\nPortfolio Theory developed in 1952, which is the cornerstone of investment\nportfolio management and aims at \"maximum the return at the given risk\". In\ncontrast to that, fifty years later, E. Robert Fernholz's Stochastic Portfolio\nTheory, as opposed to the normative assumption served as the basis of earlier\nmodern portfolio theory, is consistent with the observable characteristics of\nactual portfolios and markets. In this paper, after introducing some basic\ntheories of Markowitz's MPT and Fernholz's SPT, then we step across to the\napplication side, trying to figure out under four basic models based on\nMarkowitz Efficient Frontier, including Markowitz Model, Constant Correlation\nModel, Single Index Model, and Multi-Factor Model, which portfolios will be\nselected and how do these portfolios perform in the real world. Here we also\ninvolve universal Portfolio Algorithmby Thomas M. Cover to select portfolios as\na comparison. Besides, each portfolio value at Risk, Expected Shortfall, and\ncorresponding bootstrap confidence interval for risk management will be\nevaluated. Finally, by utilizing factor analysis and time series models, we\ncould predict the future performance of our four models.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.01194v1"
    },
    {
        "title": "Portfolio Optimization of 40 Stocks Using the DWave Quantum Annealer",
        "authors": [
            "Jeffrey Cohen",
            "Alex Khan",
            "Clark Alexander"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We investigate the use of quantum computers for building a portfolio out of a\nuniverse of U.S. listed, liquid equities that contains an optimal set of\nstocks. Starting from historical market data, we look at various problem\nformulations on the D-Wave Systems Inc. D-Wave 2000Q(TM) System (hereafter\ncalled DWave) to find the optimal risk vs return portfolio; an optimized\nportfolio based on the Markowitz formulation and the Sharpe ratio, a simplified\nChicago Quantum Ratio (CQR), then a new Chicago Quantum Net Score (CQNS). We\napproach this first classically, then by our new method on DWave. Our results\nshow that practitioners can use a DWave to select attractive portfolios out of\n40 U.S. liquid equities.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.01430v1"
    },
    {
        "title": "Big Data links from Climate to Commodity Production Forecasts and Risk\n  Management",
        "authors": [
            "Paulina Concha Larrauri",
            "Upmanu Lall"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Frozen concentrated orange juice (FCOJ) is a commodity traded in the\nInternational Commodity Exchange. The FCOJ future price volatility is high\nbecause the world's orange production is concentrated in a few places, which\nresults in extreme sensitivity to weather and disease. Most of the oranges\nproduced in the United States are from Florida. The United States Department of\nAgriculture (USDA) issues orange production forecasts on the second week of\neach month from October to July. The October forecast in particular seems to\naffect FCOJ price volatility. We assess how a prediction of the directionality\nand magnitude of the error of the USDA October forecast could affect the\ndecision making process of multiple FCOJ market participants, and if the\n\"production uncertainty\" of the forecast could be reduced by incorporating\nother climate variables. The models developed open up the opportunity to assess\nthe application of the resulting probabilistic forecasts of the USDA production\nforecast error on the trading decisions of the different FCOJ stakeholders, and\nto perhaps consider the inclusion of climate predictors in the USDA forecast.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.03015v1"
    },
    {
        "title": "Government intervention modeling in microeconomic company market\n  evolution",
        "authors": [
            "Michał Chorowski",
            "Ryszard Kutner"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Modern technology and innovations are becoming more crucial than ever for the\nsurvival of companies in the market. Therefore, it is significant both from\ntheoretical and practical points of view to understand how governments can\ninfluence technology growth and innovation diffusion (TGID) processes. We\npropose a simple but essential extension of Ausloos-Clippe-P\\c{e}kalski and\nrelated Cichy numerical models of the TGID in the market. Both models are\ninspired by the nonlinear non-equilibrium statistical physics. Our extension\ninvolves a parameter describing the probability of government intervention in\nthe TGID process in the company market. We show, using Monte Carlo simulations,\nthe effects interventionism can have on the companies' market, depending on the\nsegment of firms that are supported. The high intervention probability can\nresult, paradoxically, in the destabilization of the market development. It\nlowers the market's technology level in the long-time limit compared to markets\nwith a lower intervention parameter. We found that the intervention in the\ntechnologically weak and strong segments of the company market does not\nsubstantially influence the market dynamics, compared to the intervention\nhelping the middle-level companies. However, this is still a simple model which\ncan be extended further and made more realistic by including other factors.\nNamely, the cost and risk of innovation or limited government resources and\ncapabilities to support companies.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.06451v1"
    },
    {
        "title": "Symmetry and financial Markets",
        "authors": [
            "Jørgen Vitting Andersen",
            "Andrzej Nowak"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  It is hard to overstate the importance that the concept of symmetry has had\nin every field of physics, a fact alluded to by the Nobel Prize winner P.W.\nAnderson, who once wrote that physics is the study of symmetry. Whereas the\nidea of symmetry is widely used in science in general, very few (if not almost\nno) applications has found its way into the field of finance. Still, the\nphenomenon appears relevant in terms of for example the symmetry of strategies\nthat can happen in the decision making to buy or sell financial shares. Game\ntheory is therefore one obvious avenue where to look for symmetry, but as will\nbe shown, also technical analysis and long term economic growth could be\nphenomena which show the hallmark of a symmetry\n",
        "pdf_link": "http://arxiv.org/pdf/2007.08475v1"
    },
    {
        "title": "Deep neural network for optimal retirement consumption in defined\n  contribution pension system",
        "authors": [
            "Wen Chen",
            "Nicolas Langrené"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this paper, we develop a deep neural network approach to solve a lifetime\nexpected mortality-weighted utility-based model for optimal consumption in the\ndecumulation phase of a defined contribution pension system. We formulate this\nproblem as a multi-period finite-horizon stochastic control problem and train a\ndeep neural network policy representing consumption decisions. The optimal\nconsumption policy is determined by personal information about the retiree such\nas age, wealth, risk aversion and bequest motive, as well as a series of\neconomic and financial variables including inflation rates and asset returns\njointly simulated from a proposed seven-factor economic scenario generator\ncalibrated from market data. We use the Australian pension system as an\nexample, with consideration of the government-funded means-tested Age Pension\nand other practical aspects such as fund management fees. The key findings from\nour numerical tests are as follows. First, our deep neural network optimal\nconsumption policy, which adapts to changes in market conditions, outperforms\ndeterministic drawdown rules proposed in the literature. Moreover, the\nout-of-sample outperformance ratios increase as the number of training\niterations increases, eventually reaching outperformance on all testing\nscenarios after less than 10 minutes of training. Second, a sensitivity\nanalysis is performed to reveal how risk aversion and bequest motives change\nthe consumption over a retiree's lifetime under this utility framework. Third,\nwe provide the optimal consumption rate with different starting wealth\nbalances. We observe that optimal consumption rates are not proportional to\ninitial wealth due to the Age Pension payment. Forth, with the same initial\nwealth balance and utility parameter settings, the optimal consumption level is\ndifferent between males and females due to gender differences in mortality.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.09911v2"
    },
    {
        "title": "Towards a Sustainable Agricultural Credit Guarantee Scheme",
        "authors": [
            "Reason Lesego Machete"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Since 1986, Government of Botswana has been running an Agricultural Credit\nGuarantee Scheme for dry-land arable farming. The scheme purports to assist\ndry-land crop farmers who have taken loans with participating banks or lending\ninstitutions to help them meet their debt obligations in case of crop failure\ndue to drought, floods, frost or hailstorm. Nonetheless, to date, the scheme\nhas focused solely on drought. The scheme has placed an unsustainable financial\nburden on Government because it is not based on sound actuarial principles.\nThis paper argues that the level of Government subsidies should take into\naccount the gains made by farmers during non-drought years. It is an attempt to\ncircumvent the challenges of correlated climate risks and recommends a quasi\nself-financing mechanism, assuming that the major driver of crop yield failure\nis drought. Moreover, it provides a novel subsidy and premium rate setting\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11618v4"
    },
    {
        "title": "Data science and AI in FinTech: An overview",
        "authors": [
            "Longbing Cao",
            "Qiang Yang",
            "Philip S. Yu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Financial technology (FinTech) has been playing an increasingly critical role\nin driving modern economies, society, technology, and many other areas. Smart\nFinTech is the new-generation FinTech, largely inspired and empowered by data\nscience and new-generation AI and (DSAI) techniques. Smart FinTech synthesizes\nbroad DSAI and transforms finance and economies to drive intelligent,\nautomated, whole-of-business and personalized economic and financial\nbusinesses, services and systems. The research on data science and AI in\nFinTech involves many latest progress made in smart FinTech for BankingTech,\nTradeTech, LendTech, InsurTech, WealthTech, PayTech, RiskTech,\ncryptocurrencies, and blockchain, and the DSAI techniques including complex\nsystem methods, quantitative methods, intelligent interactions, recognition and\nresponses, data analytics, deep learning, federated learning,\nprivacy-preserving processing, augmentation, optimization, and system\nintelligence enhancement. Here, we present a highly dense research overview of\nsmart financial businesses and their challenges, the smart FinTech ecosystem,\nthe DSAI techniques to enable smart FinTech, and some research directions of\nsmart FinTech futures to the DSAI communities.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12681v2"
    },
    {
        "title": "Money flow network among firms' accounts in a regional bank of Japan",
        "authors": [
            "Yoshi Fujiwara",
            "Hiroyasu Inoue",
            "Takayuki Yamaguchi",
            "Hideaki Aoyama",
            "Takuma Tanaka"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this study, we investigate the flow of money among bank accounts possessed\nby firms in a region by employing an exhaustive list of all the bank transfers\nin a regional bank in Japan, to clarify how the network of money flow is\nrelated to the economic activities of the firms. The network statistics and\nstructures are examined and shown to be similar to those of a nationwide\nproduction network. Specifically, the bowtie analysis indicates what we refer\nto as a \"walnut\" structure with core and upstream/downstream components. To\nquantify the location of an individual account in the network, we used the\nHodge decomposition method and found that the Hodge potential of the account\nhas a significant correlation to its position in the bowtie structure as well\nas to its net flow of incoming and outgoing money and links, namely the net\ndemand/supply of individual accounts. In addition, we used non-negative matrix\nfactorization to identify important factors underlying the entire flow of\nmoney; it can be interpreted that these factors are associated with regional\neconomic activities.One factor has a feature whereby the remittance source is\nlocalized to the largest city in the region, while the destination is\nscattered. The other factors correspond to the economic activities specific to\ndifferent local places.This study serves as a basis for further investigation\non the relationship between money flow and economic activities of firms.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.14630v2"
    },
    {
        "title": "Long term dynamics of poverty transitions in India",
        "authors": [
            "Anand Sahasranaman"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We model the dynamics of poverty using a stochastic model of Geometric\nBrownian Motion with reallocation (RGBM) and explore both transient and\npersistent poverty over 1952-2006. We find that annual transitions in and out\nof poverty are common and show a rising trend, with the rise largely being\ndriven by transitions out of poverty. Despite this promising trend, even toward\nthe end of the time frame, there is a non-trivial proportion of individuals\nstill transitioning annually into poverty, indicative of the economic fragility\nof those near the poverty line. We also find that there is still a marked\npersistence of poverty over time, though the probability of poverty persistence\nis slowly declining. Particularly concerning in this context are the poverty\ntrajectories of those at the very bottom of the income distribution. The choice\nof poverty line appears to impact the dynamics, with higher poverty lines\ncorresponding to lower transitions and higher persistence probabilities. The\ndistinct nature of emergent transient and persistence dynamics suggests that\nthe approaches to counter these phenomena need to be different, possibly\nincorporating both missing financial markets and state action.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.06954v1"
    },
    {
        "title": "Reel Stock Analysis for an Integrated Paper Packaging Company",
        "authors": [
            "Constantine Goulimis",
            "Gaston Simone"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The production of corrugated paper boxes accounts for roughly one third of\nthe world's total paper production and, as a result of both COVID-19 and the\nrise of e-commerce, is a growing market. We provide a fresh approach to\ndetermining near-optimal stock policies for integrated paper companies. The new\napproach shows that existing policies can be improved by a significant margin.\nIn a case study we saw a reduction in total waste by 9%, with a simultaneous\ndecrease in logistics costs.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.05858v2"
    },
    {
        "title": "Bankruptcy prediction using disclosure text features",
        "authors": [
            "Sridhar Ravula"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  A public firm's bankruptcy prediction is an important financial research\nproblem because of the security price downside risks. Traditional methods rely\non accounting metrics that suffer from shortcomings like window dressing and\nretrospective focus. While disclosure text-based metrics overcome some of these\nissues, current methods excessively focus on disclosure tone and sentiment.\nThere is a requirement to relate meaningful signals in the disclosure text to\nfinancial outcomes and quantify the disclosure text data. This work proposes a\nnew distress dictionary based on the sentences used by managers in explaining\nfinancial status. It demonstrates the significant differences in linguistic\nfeatures between bankrupt and non-bankrupt firms. Further, using a large sample\nof 500 bankrupt firms, it builds predictive models and compares the performance\nagainst two dictionaries used in financial text analysis. This research shows\nthat the proposed stress dictionary captures unique information from\ndisclosures and the predictive models based on its features have the highest\naccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.00719v1"
    },
    {
        "title": "Design and analysis of momentum trading strategies",
        "authors": [
            "Richard J. Martin"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We give a complete description of the third-moment (skewness) characteristics\nof both linear and nonlinear momentum trading strategies, the latter being\nunderstood as transformations of a normalised moving-average filter (EMA). We\nexplain in detail why the skewness is generally positive and has a term\nstructure.\n  This paper is a synthesis of two papers published by the author in RISK in\n2012, with some updates and comments.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.01006v2"
    },
    {
        "title": "Quantum credit loans",
        "authors": [
            "Ardenghi Juan Sebastian"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Quantum models based on the mathematics of quantum mechanics (QM) have been\ndeveloped in cognitive sciences, game theory and econophysics. In this work a\ngeneralization of credit loans is introduced by using the vector space\nformalism of QM. Operators for the debt, amortization, interest and periodic\ninstallments are defined and its mean values in an arbitrary orthonormal basis\nof the vectorial space give the corresponding values at each period of the\nloan. Endowing the vector space of dimension M, where M is the loan duration,\nwith a SO(M) symmetry, it is possible to rotate the eigenbasis to obtain better\nschedule periodic payments for the borrower, by using the rotation angles of\nthe SO(M) transformation. Given that a rotation preserves the length of the\nvectors, the total amortization, debt and periodic installments are not\nchanged. For a general description of the formalism introduced, the loan\noperator relations are given in terms of a generalized Heisenberg algebra,\nwhere finite dimensional representations are considered and commutative\noperators are defined for the specific loan types. The results obtained are an\nimprovement of the usual financial instrument of credit because introduce\nseveral degrees of freedom through the rotation angles, which allows to select\nsuperposition states of the corresponding commutative operators that enables\nthe borrower to tune the periodic installments in order to obtain better\nbenefits without changing what the lender earns.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03231v1"
    },
    {
        "title": "Pricing the COVID-19 Vaccine: A Mathematical Approach",
        "authors": [
            "Susan Martonosi",
            "Banafsheh Behzad",
            "Kayla Cummings"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  According to the World Health Organization, development of the COVID-19\nvaccine is occurring in record time. Administration of the vaccine has started\nthe same year as the declaration of the COVID-19 pandemic. The United Nations\nemphasized the importance of providing COVID-19 vaccines as \"a global public\ngood\", which is accessible and affordable world-wide. Pricing the COVID-19\nvaccines is a controversial topic. We use optimization and game theoretic\napproaches to model the COVID-19 U.S. vaccine market as a duopoly with two\nmanufacturers Pfizer-BioNTech and Moderna. The results suggest that even in the\ncontext of very high production and distribution costs, the government can\nnegotiate prices with the manufacturers to keep public sector prices as low as\npossible while meeting demand and ensuring each manufacturer earns a target\nprofit. Furthermore, these prices are consistent with those currently predicted\nin the media.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03234v1"
    },
    {
        "title": "Currency Network Risk",
        "authors": [
            "Mykola Babiak",
            "Jozef Barunik"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This paper identifies new currency risk stemming from a network of\nidiosyncratic option-based currency volatilities and shows how such network\nrisk is priced in the cross-section of currency returns. A portfolio that buys\nnet-receivers and sells net-transmitters of short-term linkages between\ncurrency volatilities generates a significant Sharpe ratio. The network\nstrategy formed on causal connections is uncorrelated with popular benchmarks\nand generates a significant alpha, while network returns formed on aggregate\nconnections, which are driven by a strong correlation component, are partially\nsubsumed by standard factors. Long-term linkages are priced less, indicating a\ndownward-sloping term structure of network risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.09738v2"
    },
    {
        "title": "Risks of heterogeneously persistent higher moments",
        "authors": [
            "Jozef Barunik",
            "Josef Kurka"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Using intraday data for the cross-section of individual stocks, we show that\nboth transitory and persistent fluctuations in realized market and average\nidiosyncratic volatility, skewness and kurtosis are differentially priced in\nthe cross-section of asset returns, implying a heterogeneous persistence\nstructure of different sources of higher moment risks. Specifically, we find\nthat idiosyncratic transitory shocks to volatility as well as idiosyncratic\npersistent shocks to skewness contain strong commonalities that are relevant to\ninvestors.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.04264v2"
    },
    {
        "title": "Trends in the E-commerce and in the Traditional Retail Sectors During\n  the Covid-19 Pandemic: an Evolutionary Game Approach",
        "authors": [
            "André Barreira da Silva Rocha",
            "Matheus Oliveira Meirim",
            "Lara Corrêa Nogueira"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  An evolutionary game model is developed to study the interplay between\nconsumers and producers when trade takes place on an e-commerce marketplace.\nThe type of delivery service available and consumers' taste are particularly\nimportant regarding both game payoffs and players' strategies. The game payoff\nmatrix is then adapted to analyse the different trading patterns that were\ndeveloped during the COVID-19 pandemic in both the traditional retail and\ne-commerce sectors. In contrast to the former, investment in logistics and\nwarehouses in the e-commerce sector allowed for the emergence of a trend in\nwhich fast delivery and eager consumers are becoming the norm.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.06833v1"
    },
    {
        "title": "An Empirical Study of DeFi Liquidations: Incentives, Risks, and\n  Instabilities",
        "authors": [
            "Kaihua Qin",
            "Liyi Zhou",
            "Pablo Gamito",
            "Philipp Jovanovic",
            "Arthur Gervais"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Financial speculators often seek to increase their potential gains with\nleverage. Debt is a popular form of leverage, and with over 39.88B USD of total\nvalue locked (TVL), the Decentralized Finance (DeFi) lending markets are\nthriving. Debts, however, entail the risks of liquidation, the process of\nselling the debt collateral at a discount to liquidators. Nevertheless, few\nquantitative insights are known about the existing liquidation mechanisms.\n  In this paper, to the best of our knowledge, we are the first to study the\nbreadth of the borrowing and lending markets of the Ethereum DeFi ecosystem. We\nfocus on Aave, Compound, MakerDAO, and dYdX, which collectively represent over\n85% of the lending market on Ethereum. Given extensive liquidation data\nmeasurements and insights, we systematize the prevalent liquidation mechanisms\nand are the first to provide a methodology to compare them objectively. We find\nthat the existing liquidation designs well incentivize liquidators but sell\nexcessive amounts of discounted collateral at the borrowers' expenses. We\nmeasure various risks that liquidation participants are exposed to and quantify\nthe instabilities of existing lending protocols. Moreover, we propose an\noptimal strategy that allows liquidators to increase their liquidation profit,\nwhich may aggravate the loss of borrowers.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.06389v2"
    },
    {
        "title": "CeFi vs. DeFi -- Comparing Centralized to Decentralized Finance",
        "authors": [
            "Kaihua Qin",
            "Liyi Zhou",
            "Yaroslav Afonin",
            "Ludovico Lazzaretti",
            "Arthur Gervais"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  To non-experts, the traditional Centralized Finance (CeFi) ecosystem may seem\nobscure, because users are typically not aware of the underlying rules or\nagreements of financial assets and products. Decentralized Finance (DeFi),\nhowever, is making its debut as an ecosystem claiming to offer transparency and\ncontrol, which are partially attributable to the underlying integrity-protected\nblockchain, as well as currently higher financial asset yields than CeFi. Yet,\nthe boundaries between CeFi and DeFi may not be always so clear cut.\n  In this work, we systematically analyze the differences between CeFi and\nDeFi, covering legal, economic, security, privacy and market manipulation. We\nprovide a structured methodology to differentiate between a CeFi and a DeFi\nservice. Our findings show that certain DeFi assets (such as USDC or USDT\nstablecoins) do not necessarily classify as DeFi assets, and may endanger the\neconomic security of intertwined DeFi protocols. We conclude this work with the\nexploration of possible synergies between CeFi and DeFi.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.08157v2"
    },
    {
        "title": "Bitcoin's Crypto Flow Network",
        "authors": [
            "Yoshi Fujiwara",
            "Rubaiyat Islam"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  How crypto flows among Bitcoin users is an important question for\nunderstanding the structure and dynamics of the cryptoasset at a global scale.\nWe compiled all the blockchain data of Bitcoin from its genesis to the year\n2020, identified users from anonymous addresses of wallets, and constructed\nmonthly snapshots of networks by focusing on regular users as big players. We\napply the methods of bow-tie structure and Hodge decomposition in order to\nlocate the users in the upstream, downstream, and core of the entire crypto\nflow. Additionally, we reveal principal components hidden in the flow by using\nnon-negative matrix factorization, which we interpret as a probabilistic model.\nWe show that the model is equivalent to a probabilistic latent semantic\nanalysis in natural language processing, enabling us to estimate the number of\nsuch hidden components. Moreover, we find that the bow-tie structure and the\nprincipal components are quite stable among those big players. This study can\nbe a solid basis on which one can further investigate the temporal change of\ncrypto flow, entry and exit of big players, and so forth.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.11446v2"
    },
    {
        "title": "The macroeconomic cost of climate volatility",
        "authors": [
            "Piergiorgio Alessandri",
            "Haroon Mumtaz"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  We study the impact of climate volatility on economic growth exploiting data\non 133 countries between 1960 and 2019. We show that the conditional (ex ante)\nvolatility of annual temperatures increased steadily over time, rendering\nclimate conditions less predictable across countries, with important\nimplications for growth. Controlling for concomitant changes in temperatures, a\n+1 degree C increase in temperature volatility causes on average a 0.3 percent\ndecline in GDP growth and a 0.7 percent increase in the volatility of GDP.\nUnlike changes in average temperatures, changes in temperature volatility\naffect both rich and poor countries.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.01617v2"
    },
    {
        "title": "Two-Stage Sector Rotation Methodology Using Machine Learning and Deep\n  Learning Techniques",
        "authors": [
            "Tugce Karatas",
            "Ali Hirsa"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Market indicators such as CPI and GDP have been widely used over decades to\nidentify the stage of business cycles and also investment attractiveness of\nsectors given market conditions. In this paper, we propose a two-stage\nmethodology that consists of predicting ETF prices for each sector using market\nindicators and ranking sectors based on their predicted rate of returns. We\ninitially start with choosing sector specific macroeconomic indicators and\nimplement Recursive Feature Elimination algorithm to select the most important\nfeatures for each sector. Using our prediction tool, we implement different\nRecurrent Neural Networks models to predict the future ETF prices for each\nsector. We then rank the sectors based on their predicted rate of returns. We\nselect the best performing model by evaluating the annualized return,\nannualized Sharpe ratio, and Calmar ratio of the portfolios that includes the\ntop four ranked sectors chosen by the model. We also test the robustness of the\nmodel performance with respect to lookback windows and look ahead windows. Our\nempirical results show that our methodology beats the equally weighted\nportfolio performance even in the long run. We also find that Echo State\nNetworks exhibits an outstanding performance compared to other models yet it is\nfaster to implement compared to other RNN models.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.02838v1"
    },
    {
        "title": "BDG inequalities and their applications for model-free continuous price\n  paths with instant enforcement",
        "authors": [
            "Rafał M. Łochowski"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Shafer and Vovk introduce in their book \\cite{ShaferVovk:2018} the notion of\n\\emph{instant enforcement} and \\emph{instantly blockable} properties. However,\nthey do not associate these notions with any outer measure, unlike what Vovk\ndid in the case of sets of ''typical'' price paths. In this paper we introduce\nan outer measure on the space $[0, +\\ns) \\times \\Omega$ which assigns zero\nvalue exactly to those sets (properties) of pairs of time $t$ and an elementary\nevent $\\omega$ which are instantly blockable. Next, for a slightly modified\nmeasure, we prove It\\^o's isometry and BDG inequalities, and then use them to\ndefine an It\\^o-type integral. Additionally, we prove few properties for the\nquadratic variation of model-free, continuous martingales, which hold with\ninstant enforcement.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.07928v4"
    },
    {
        "title": "On the nature of monetary and price inflation and hyperinflation",
        "authors": [
            "Laurence Francis Lacey"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Monetary inflation is a sustained increase in the money supply than can\nresult in price inflation, which is a rise in the general level of prices of\ngoods and services. The objectives of this paper were to develop economic\nmodels to (1) predict the annual rate of growth in the US consumer price index\n(CPI), based on the annual growth in the US broad money supply (BMS), the\nannual growth in US real GDP, and the annual growth in US savings, over the\ntime period 2001 to 2019; (2) investigate the means by which monetary and price\ninflation can develop into monetary and price hyperinflation. The hypothesis\nthat the annual rate of growth in the US CPI is a function of the annual growth\nin the US BMS minus the annual growth in US real GDP minus the annual growth in\nUS savings, over the time period investigated, has been shown to be the case.\nHowever, an exact relationship required the use of a non-zero residual term. A\nmathematical statistical formulation of a hyperinflationary process has been\nprovided and used to quantify the period of hyperinflation in the Weimar\nRepublic, from July 1922 until the end of November 1923.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.12980v1"
    },
    {
        "title": "Predicting Status of Pre and Post M&A Deals Using Machine Learning and\n  Deep Learning Techniques",
        "authors": [
            "Tugce Karatas",
            "Ali Hirsa"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Risk arbitrage or merger arbitrage is a well-known investment strategy that\nspeculates on the success of M&A deals. Prediction of the deal status in\nadvance is of great importance for risk arbitrageurs. If a deal is mistakenly\nclassified as a completed deal, then enormous cost can be incurred as a result\nof investing in target company shares. On the contrary, risk arbitrageurs may\nlose the opportunity of making profit. In this paper, we present an ML and DL\nbased methodology for takeover success prediction problem. We initially apply\nvarious ML techniques for data preprocessing such as kNN for data imputation,\nPCA for lower dimensional representation of numerical variables, MCA for\ncategorical variables, and LSTM autoencoder for sentiment scores. We experiment\nwith different cost functions, different evaluation metrics, and oversampling\ntechniques to address class imbalance in our dataset. We then implement\nfeedforward neural networks to predict the success of the deal status. Our\npreliminary results indicate that our methodology outperforms the benchmark\nmodels such as logit and weighted logit models. We also integrate sentiment\nscores into our methodology using different model architectures, but our\npreliminary results show that the performance is not changing much compared to\nthe simple FFNN framework. We will explore different architectures and employ a\nthorough hyperparameter tuning for sentiment scores as a future work.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.09315v1"
    },
    {
        "title": "Role of Variable Renewable Energy Penetration on Electricity Price and\n  its Volatility Across Independent System Operators in the United States",
        "authors": [
            "Olukunle O. Owolabi",
            "Toryn L. J. Schafer",
            "Georgia E. Smits",
            "Sanhita Sengupta",
            "Sean E. Ryan",
            "Lan Wang",
            "David S. Matteson",
            "Mila Getmansky Sherman",
            "Deborah A. Sunter"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  The U.S. electrical grid has undergone substantial transformation with\nincreased penetration of wind and solar -- forms of variable renewable energy\n(VRE). Despite the benefits of VRE for decarbonization, it has garnered some\ncontroversy for inducing unwanted effects in regional electricity markets. In\nthis study, the role of VRE penetration is examined on the system electricity\nprice and price volatility based on hourly, real-time, historical data from six\nIndependent System Operators (ISOs) in the U.S. using quantile and skew\nt-distribution regressions. After correcting for temporal effects, we found an\nincrease in VRE penetration is associated with decrease in system electricity\nprice in all ISOs studied. The increase in VRE penetration is associated with\ndecrease in temporal price volatility in five out of six ISOs studied. The\nrelationships are non-linear. These results are consistent with the modern\nportfolio theory where diverse volatile assets may lead to more stable and less\nrisky portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.11338v3"
    },
    {
        "title": "Stationary GE-Process and its Application in Analyzing Gold Price Data",
        "authors": [
            "Debasis Kundu"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  In this paper we introduce a new discrete time and continuous state space\nstationary process $\\{X_n; n = 1, 2, \\ldots \\}$, such that $X_n$ follows a\ntwo-parameter generalized exponential (GE) distribution. Joint distribution\nfunctions, characterization and some dependency properties of this new process\nhave been investigated. The GE-process has three unknown parameters, two shape\nparameters and one scale parameter, and due to this reason it is more flexible\nthan the existing exponential process. In presence of the scale parameter, if\nthe two shape parameters are equal, then the maximum likelihood estimators of\nthe unknown parameters can be obtained by solving one non-linear equation and\nif the two shape parameters are arbitrary, then the maximum likelihood\nestimators can be obtained by solving a two dimensional optimization problem.\nTwo {\\color{black} synthetic} data sets, and one real gold-price data set have\nbeen analyzed to see the performance of the proposed model in practice. Finally\nsome generalizations have been indicated.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.02568v1"
    },
    {
        "title": "Decomposing LIBOR in Transition: Evidence from the Futures Markets",
        "authors": [
            "David Skovmand",
            "Jacob Bjerre Skov"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Applying historical data from the USD LIBOR transition period, we estimate a\njoint model for SOFR, Fed Funds, and Eurodollar futures rates as well as spot\nUSD LIBOR and term repo rates. The framework endogenously models basis spreads\nbetween each of the benchmark rates and allows for the decomposition of\nspreads. Modelling the LIBOR-OIS spread as credit and funding-liquidity\nroll-over risk, we find that the spike in the LIBOR-OIS spread during the onset\nof COVID-19 was mainly due to credit risk, while on average credit and\nfunding-liquidity risk contribute equally to the spread.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06930v2"
    },
    {
        "title": "Applicability of Large Corporate Credit Models to Small Business Risk\n  Assessment",
        "authors": [
            "Khalid El-Awady"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  There is a massive underserved market for small business lending in the US\nwith the Federal Reserve estimating over \\$650B in unmet annual financing\nneeds. Assessing the credit risk of a small business is key to making good\ndecisions whether to lend and at what terms. Large corporations have a\nwell-established credit assessment ecosystem, but small businesses suffer from\nlimited publicly available data and few (if any) credit analysts who cover them\nclosely. We explore the applicability of (DL-based) large corporate credit risk\nmodels to small business credit rating.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.08276v1"
    },
    {
        "title": "FiNCAT: Financial Numeral Claim Analysis Tool",
        "authors": [
            "Sohom Ghosh",
            "Sudip Kumar Naskar"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  While making investment decisions by reading financial documents, investors\nneed to differentiate between in-claim and outof-claim numerals. In this paper,\nwe present a tool which does it automatically. It extracts context embeddings\nof the numerals using one of the transformer based pre-trained language model\ncalled BERT. After this, it uses a Logistic Regression based model to detect\nwhether the numerals is in-claim or out-of-claim. We use FinNum-3 (English)\ndataset to train our model. After conducting rigorous experiments we achieve a\nMacro F1 score of 0.8223 on the validation set. We have open-sourced this tool\nand it can be accessed from\nhttps://github.com/sohomghosh/FiNCAT_Financial_Numeral_Claim_Analysis_Tool\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00631v1"
    },
    {
        "title": "Equilibrium Defaultable Corporate Debt and Investment",
        "authors": [
            "Hong Chen",
            "Murray Zed Frank"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  In dynamic capital structure models with an investor break-even condition,\nthe firm's Bellman equation may not generate a contraction mapping, so the\nstandard existence and uniqueness conditions do not apply. First, we provide an\nexample showing the problem in a classical trade-off model. The firm can issue\none-period defaultable debt, invest in capital and pay a dividend. If the firm\ncannot meet the required debt payment, it is liquidated. Second, we show how to\nuse a dual to the original problem and a change of measure, such that existence\nand uniqueness can be proved. In the unique Markov-perfect equilibrium, firm\ndecisions reflect state-dependent capital and debt targets. Our approach may be\nuseful for other dynamic firm models that have an investor break-even\ncondition.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05885v1"
    },
    {
        "title": "Exploring Classic Quantitative Strategies",
        "authors": [
            "Jun Lu"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The goal of this paper is to debunk and dispel the magic behind the black-box\nquantitative strategies. It aims to build a solid foundation on how and why the\ntechniques work. This manuscript crystallizes this knowledge by deriving from\nsimple intuitions, the mathematics behind the strategies. This tutorial doesn't\nshy away from addressing both the formal and informal aspects of quantitative\nstrategies. By doing so, it hopes to provide readers with a deeper\nunderstanding of these techniques as well as the when, the how and the why of\napplying these techniques. The strategies are presented in terms of both\nS\\&P500 and SH510300 data sets. However, the results from the tests are just\nexamples of how the methods work; no claim is made on the suggestion of real\nmarket positions.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.11309v1"
    },
    {
        "title": "Solar Term Anomaly in China Stock Market: Evidence from Shanghai Index",
        "authors": [
            "Zhou Tianbao",
            "Li Xinghao",
            "Zhao Junguang"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  This paper investigates the solar term effect in China stock market as a\nsupplementary to the existing literature of calender effect. Based on a\nregression framework, this paper verifies the existence of solar term effect in\nShanghai Index in multiple dimensions: inter-solar-term analysis, full sample\nanalysis at mean level and risk level as well as the turn of solar term effect.\nSeveral solar terms have been found to cause significant positive and negative\nvalue to the return such as solar term 1,3 and 4. and bring high volatility\nsuch as solar term 8, 11 and 14. The result is reliable and robust under the\nExtreme Bound Analysis and various assumptions of errors distribution in IGARCH\nmodel. These findings give readers a new perspective to view calender effect\nunder the influence of traditional Chinese culture that solar terms affect the\nmarket through affecting investors mood, expectation, enthusiasm, etc. which is\na good evidence to the Culture bonus hypothesis proposed by Chen and Chien and\nthe possible influence by the Chinese culture in other Asian markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12603v2"
    },
    {
        "title": "Journey of Cryptocurrency in India In View of Financial Budget 2022-23",
        "authors": [
            "Varun Shukla",
            "Manoj Kumar Misra",
            "Atul Chaturvedi"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Recently, Indian Finance minister Nirmala Sitharaman announced in Union\nbudget 2022-23 that Indian government will put 30% tax (the highest tax slab in\nIndia) on income generated from cryptocurrencies. Big financial institutions,\nexperts and academicians have different opinions in this regard. They claim\nthat it would be the end of cryptocurrency market in India or it would be\npossible that RBI (Reserve Bank of India) may launch its own crypto or digital\ncurrency. So in this context, in this article, the journey and future aspects\nof cryptocurrency in India are discussed and we hope that it will be a\nreference for further research and discussion in this area.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12606v1"
    },
    {
        "title": "Transactional Interpretation for the Principle of Minimum Fisher\n  Information",
        "authors": [
            "Marcin Makowski",
            "Edward W. Piotrowski",
            "Piotr Frąckiewicz",
            "Marek Szopa"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The principle of minimum Fisher information states that in the set of\nacceptable probability distributions characterizing the given system, it is\nbest done by the one that minimizes the corresponding Fisher information. This\nprinciple can be applied to transaction processes, the dynamics of which can be\ninterpreted as the market tendency to minimize the information revealed about\nitself. More information involves higher costs (information is physical). The\nstarting point for our considerations is a description of the market derived\nfrom the assumption of minimum Fisher information for a strategy with a fixed\nfinancial risk. Strategies of this type that minimize Fisher information\noverlap with the well-known eigenstates of a the quantum harmonic oscillator.\nThe analytical extension of this field of strategy to the complex vector space\n(traditional for quantum mechanics) suggests the study of the interference of\nthe oscillator eigenstates in terms of their minimization of Fisher\ninformation. It is revealed that the minimum value of Fisher information of the\nsuperposition of the two strategies being the ground state and the second\nexcited state of the oscillator, has Fisher information less than the ground\nstate of the oscillator. Similarly, less information is obtained for the system\nof strategies (the oscillator eigenstates) randomized by the Gibbs\ndistribution. We distinguish two different views on the description of Fisher\ninformation. One of them, the classical, is based on the value of Fisher\ninformation. The second, we call it transactional, expresses Fisher information\nfrom the perspective of the constant risk of market strategies. The orders of\nthe market strategies derived from these two descriptions are different. From a\nmarket standpoint, minimizing Fisher information is equivalent to minimizing\nrisk.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12607v1"
    },
    {
        "title": "Two-dimensional forward and backward transition rates",
        "authors": [
            "Theis Bathke",
            "Marcus Christiansen"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Forward transition rates were originally introduced with the aim to evaluate\nlife insurance liabilities market-consistently. While this idea turned out to\nhave its limitations, recent literature repurposes forward transition rates as\na tool for avoiding Markov assumptions in the calculation of life insurance\nreserves. While life insurance reserves are some form of conditional\nfirst-order moments, the calculation of conditional second-order moments needs\nan extension of the forward transition rate concept from one dimension to two\ndimensions. Two-dimensional forward transition rates are also needed for the\ncalculation of path-dependent life insurance cash-flows as they occur upon\ncontract modifications. Forward transition rates are designed for doing\nprospective calculations, and by a time-symmetric definition of so-called\nbackward transition rates one can do retrospective calculations.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12766v1"
    },
    {
        "title": "Investigating the concentration of High Yield Investment Programs in the\n  United Kingdom",
        "authors": [
            "Sharad Agarwal",
            "Marie Vasek"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Ponzi schemes that offer absurdly high rates of return by relying on more and\nmore people paying into the scheme have been documented since at least the\nmid-1800s. Ponzi schemes have shifted online in the Internet age, and some are\nre-branded as HYIPs or High Yield Investment Programs. This paper focuses on\nunderstanding HYIPs' continuous presence and presents various possible reasons\nbehind their existence in today's world. A look into the countries where these\nschemes purport to exist, we find that 62.89% of all collected HYIPs claim to\nbe in the United Kingdom (UK), and a further 55.56% are officially registered\nin the UK as a 'limited company' with a registration number provided by the UK\nCompanies House, a UK agency that registers companies. We investigate other\nfactors influencing these schemes, including the HYIPs' social media platforms\nand payment processors. The lifetime of the HYIPs helps to understand the\nsuccess/failure of the investment schemes and helps indicate the schemes that\ncould attract more investors. Using Cox proportional regression analysis, we\nfind that having a valid UK address significantly affects the lifetime of an\nHYIP.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.08569v1"
    },
    {
        "title": "Static Replication of Impermanent Loss for Concentrated Liquidity\n  Provision in Decentralised Markets",
        "authors": [
            "Jun Deng",
            "Hua Zong",
            "Yun Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  This article analytically characterizes the impermanent loss of concentrated\nliquidity provision for automatic market makers in decentralised markets such\nas Uniswap. We propose two static replication formulas for the impermanent loss\nby a combination of European calls or puts with strike prices supported on the\nliquidity provision price interval. It facilitates liquidity providers to hedge\npermanent loss by trading crypto options in more liquid centralised exchanges\nsuch as Deribit. Numerical examples illustrate the astonishing accuracy of the\nstatic replication.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.12043v2"
    },
    {
        "title": "The fractional volatility model and rough volatility",
        "authors": [
            "R. Vilela Mendes"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The question of the volatility roughness is interpreted in the framework of a\ndata-reconstructed fractional volatility model, where volatility is driven by\nfractional noise. Some examples are worked out and also, using Malliavin\ncalculus for fractional processes, an option pricing equation and its solution\nare obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.02205v1"
    },
    {
        "title": "Corporate Environmental Management Accounting Practicing and Reporting\n  in Bangladesh",
        "authors": [
            "Nazrul Islam",
            "Syed Khaled Rahman"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  In the management of environment the Environmental Management Accounting\n(EMA) is essential for corporate or companies because corporate sectors are the\nmain parties of environmental humiliation as they are existed in the\nenvironment and for protecting environment a branch of accounting is emerged\nwhich is called environmental management accounting. The objective of the study\nis to develop a compliance framework for EMA and appraise the ER practices in\nselected industries in Bangladesh. In conducting the study, 50 environmental\nsensitive industries were selected from DSE. A compliance checklist was\ndeveloped on 75 aspects of EMA and ER under 13 groups. In developing the\ncompliance index binary method is used i.e. 1= if ER practices; 0= if not\npractices. Further the level of EMR/ER practices have been evaluated in terms\nof selected independent variables of the company viz. total assets, total\nsales, return on equity and size of board. The study found that the\nenvironmental management accounting in the manufacturing companies is in poor\nlevel. The maximum compliance is 67% and the lowest is 20%. The TA, TS BS and\nSP have been considered to find out the explanatory variables. In most of the\ncases board size does not play significant role in the practice of EMA in the\nsampled firms.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.12541v1"
    },
    {
        "title": "Common Idiosyncratic Quantile Risk",
        "authors": [
            "Jozef Barunik",
            "Matej Nevrla"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  We identify a new type of risk that is characterised by commonalities in the\nquantiles of the cross-sectional distribution of asset returns. Our newly\nproposed quantile risk factors are associated with a quantile-specific risk\npremia and provide new insights into how upside and downside risks are priced\nby investors. In contrast to the previous literature, we recover the common\nstructure in cross-sectional quantiles without making confounding assumptions\nor aggregating potentially non-linear information. We discuss how the new\nquantile-based risk factors differ from popular volatility and downside risk\nfactors, and we identify heterogeneous implications of quantile-dependent risks\nfor asset prices. Quantile factors also have predictive power for aggregate\nmarket returns. We explore potential mechanisms that give rise to these asset\npricing facts.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.14267v3"
    },
    {
        "title": "Shift in house price estimates during COVID-19 reveals effect of crisis\n  on collective speculation",
        "authors": [
            "Alexander M. Petersen"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  We exploit a city-level panel comprised of individual house price estimates\nto estimate the impact of COVID-19 on both small and big real-estate markets in\nCalifornia USA. Descriptive analysis of spot house price estimates, including\ncontemporaneous price uncertainty and 30-day price change for individual\nproperties listed on the online real-estate platform Zillow.com, together\nfacilitate quantifying both the excess valuation and valuation confidence\nattributable to this global socio-economic shock. Our quasi-experimental\npre-/post-COVID-19 design spans several years around 2020 and leverages\ncontemporaneous price estimates of rental properties - i.e., real estate\nentering the habitation market, just not for purchase (off-market) and hence\nfree of speculation - as an appropriate counterfactual to properties listed for\nsale, which are subject to on-market speculation. Combining unit-level matching\nand multivariate difference-in-difference regression approaches, we obtain\nconsistent estimates regarding the sign and magnitude of excess price growth\nobserved after the pandemic onset. Specifically, our results indicate that\nproperties listed for sale appreciated an additional 1% per month above what\nwould be expected in the absence of the pandemic. This corresponds to an excess\nannual price growth of roughly 12.7 percentage points, which accounts for more\nthan half of the actual annual price growth in 2021 observed across the studied\nregions. Simultaneously, uncertainty in price estimates decreased, signaling\nthe irrational confidence characteristic of prior asset bubbles. We explore how\nthese two trends are related to market size, local market supply and borrowing\ncosts, which altogether lend support for the counterintuitive roles of\nuncertainty and interruptions in decision-making.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.03936v2"
    },
    {
        "title": "Seller-buyer networks in NFT art are driven by preferential ties",
        "authors": [
            "Giovanni Colavizza"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Non-Fungible Tokens (NFTs) have recently surged to mainstream attention by\nallowing the exchange of digital assets via blockchains. NFTs have also been\nadopted by artists to sell digital art. One of the promises of NFTs is\nbroadening participation to the arts market, a traditionally closed and opaque\nsystem, to sustain a wider and more diverse set of artists and collectors. A\nkey sign of this effect would be the disappearance or at least reduction in\nimportance of seller-buyer preferential ties, whereby the success of an artist\nis strongly dependent on the patronage of a single collector. We investigate\nNFT art seller-buyer networks considering several galleries and a large set of\nnearly 40,000 sales for over 230M USD in total volume. We find that NFT art is\na highly concentrated market driven by few successful sellers and even fewer\nsystematic buyers. High concentration is present in both the number of sales\nand, even more strongly, in their priced volume. Furthermore, we show that,\nwhile a broader-participation market was present in the early phase of NFT art\nadoption, preferential ties have dominated during market growth, peak and\nrecent decline. We consistently find that the top buyer accounts on average for\nover 80% of buys for a given seller. Similar trends apply to buyers and their\ntop seller. We conclude that NFT art constitutes, at the present, a highly\nconcentrated market driven by preferential seller-buyer ties.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.04339v2"
    },
    {
        "title": "ETF construction on CRIX",
        "authors": [
            "Konstantin Häusler"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Investments in cryptocurrencies (CCs) remain risky due to high volatility.\nExchange Traded Funds (ETFs) are a suitable tool to diversify risk and to\nbenefit from the growth of the whole CC sector. We construct an ETF on the\nCRIX, the CRyptocurrency IndeX that maps the non-stationary CC dynamics closely\nby adapting its constituents weights dynamically. The scenario analysis\nconsiders the fee schedules of regulated CC exchanges, spreads obtained from\nhigh-frequency order book data, and models capital deposits to the ETF\nstochastically. The analysis yields valuable insights into the mechanisms,\ncosts and risks of this new financial product: i) although the composition of\nthe CRIX ETF changes frequently (from 5 to 30 constituents), it remains robust\nin its core, as the weights of Bitcoin (BTC) and Ethereum (ETH) are robust over\ntime, ii) on average, a portion of 5.2% needed to be rebalanced at the\nrebalancing dates, iii) trading costs are low compared to traditional assets,\niv) the liquidity of the CC sector has increased significantly during the\nanalysis period, spreads occur especially for altcoins and increase by the size\nof the transactions. But since BTC and ETH are most affected by rebalancing,\nthe cost of spreads remains limited.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.15260v2"
    },
    {
        "title": "Business-cycles and Cash-on-Market: Pre-money Startup Valuation in the\n  Macroeconomic Environment",
        "authors": [
            "Max Berre",
            "Benjamin Le Pendeven"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  How do business-cycles impact startup-valuations? While several studies\nexplore VC startupecosystems and pre-money valuations, relatively-few delve\ndeeper into the role of macro-level economic factors in influencing those\nstartup deals valuations. Using a dataset of 1,089 venturecapital investments\nin European Union and European Economic Area markets, this article examines\nmacroeconomic, cyclical and macro-sectoral influences on VC startups pre-money\nVC valuations. Our findings show that business-cycles impact startup-valuation\nboth directly and indirectly. Beyond DCF factors, startup-valuations are\nimpacted via by business-cycles directly, and via local venture-capital\nmarket-size. By using a Structural Equation Model approach, our findings\ncontribute to entrepreneurship and financial-intermediary literature by\nexploring indirect and endogenous relationship possibilities finding that most\ndeterminants are transmission-channels rather than independent drivers. Our\nfindings effectively tie-together startup-valuations, intermediary markets, and\nmacroeconomic determinants.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.16151v1"
    },
    {
        "title": "Why do investors buy shares of actively managed equity mutual funds?\n  Considering the Correct Reference Portfolio from an Uninformed Investor's\n  Perspective 1, 2",
        "authors": [
            "Radu Burlacu",
            "Patrice Fontaine",
            "Sonia Jimenez-Garcès"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  We use the Grossman \\& Stiglitz (1980) framework to build a reference\nportfolio for uninformed investors and employ this portfolio to assess the\nperformance of actively managed equity mutual funds. We propose an empirical\nmethodology to construct this reference portfolio using the information on\nprices and supply. We show that mutual funds provide, on average, an\ninsignificant alpha of 23 basis points per year when considering this portfolio\nas a reference. With the stock market index as a proxy for the market\nportfolio, the average fund alpha is negative and highly significant, --128\nbasis points per year. The results are robust when considering various subsets\nof funds based on their characteristics and their degree of selectivity. In\nline with rational expectations equilibrium models considering asymmetrically\ninformed investors and partially revealing equilibrium prices, our study\nsupports that active management adds value for uniformed investors.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.02307v1"
    },
    {
        "title": "Long bet will lose: demystifying seemingly fair gambling via two-armed\n  Futurity bandit",
        "authors": [
            "Zengjing Chen",
            "Huaijin Liang",
            "Wei Wang",
            "Xiaodong Yan"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  No matter how much some gamblers occasionally win, as long as they continue\nto gamble, sooner or later they will lose more to the casino, which is the\nso-called long bet will lose. Our results demonstrate the counter-intuitive\nphenomenon, that gamblers involved in long bets will lose but casinos always\nadvertise their unprofitable circumstances. Here we expose the law of\ninevitability behind long bet will loss by theoretically and experimentally\ndemystifying the profitable mystery behind casinos under two-armed antique\nMills Futurity slot machine. The main results straightforwardly elucidate that\nall casino projects are seemingly a fair gamble but essentially unfair, i.e.,\nthe casino's win rate is greater than 50%. We anticipate our assay to be a\nstarting point for studying the fairness of more sophisticated multi-armed\nFuturity bandits based on the mathematical tool. In application, a fairness\nstudy of the Futurity bandits not only exposes the fraud of casinos for\ngamblers but also discloses discount marketing, bundled sales, or other induced\nconsumption tactics.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.11766v1"
    },
    {
        "title": "A study on Non-Performing Assets Cases and Cryptocurrency in Japan",
        "authors": [
            "Burina Fujiwara"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  The economic bubble bursting resulted in a large number of non-performing\nloans in Japanese financial institutions, which weakened their functions and\nprevented them from extending credit for normal economic activities. However,\ncryptocurrency operations are thriving in Japan. In this way, this paper\nfocuses on non-performing assets and cryptocurrencies. The goal is to use\nliterature analysis methods to summarise the development process, types of\nissuance, mechanisms, evaluation models, application scenarios, and trends in\nhow cryptocurrencies are supervised.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.07619v1"
    },
    {
        "title": "Invoice discounting using kelly criterion by automated market\n  makers-like implementations",
        "authors": [
            "Peplluis R. Esteva",
            "Alberto Ballesteros Rodríguez"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  There is a persistent lack of funding, especially for SMEs, that cyclically\nworsens. The factoring and invoice discounting market appears to address delays\nin paying commercial invoices: sellers bring still-to-be-paid invoices to\nfinancial organizations, intermediaries, typically banks that provide an\nadvance payment. This article contains research on novel decentralized\napproaches to said lending services without intermediaries by using liquidity\npools and its associated heuristics, creating an Automated Market Maker. In our\napproach, the contributed collateral and the invoice trades with risk is\nmeasured with a formula: The Kelly criterion is used to calculate the optimal\npremium to be contributed to a liquidity pool in the funding of the said\ninvoices. The behavior of the algorithm is studied in several scenarios of\nstreams of invoices with representative amounts, collaterals, payment delays,\nand nonpayments rates or mora. We completed the study with hack scenarios with\nbogus, nonpayable invoices. As a result, we have created a resilient solution\nthat performs the best with partially collateralized invoices. The outcome is\ndecentralized market developed with the Kelly criterion that is reasonably\nresilient to a wide variety of the invoicing cases that provides sound profit\nto liquidity providers, and several premium distribution policies were checked\nthat contributed with extra resilience to the performance of the algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.09009v1"
    },
    {
        "title": "FTX's downfall and Binance's consolidation: The fragility of centralised\n  digital finance",
        "authors": [
            "David Vidal-Tomás",
            "Antonio Briola",
            "Tomaso Aste"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This paper investigates the causes of the FTX digital currency exchange's\nfailure in November 2022. We identify the collapse of the Terra-Luna ecosystem\nas the pivotal event that triggered a significant decrease in the exchange's\nliquidity. Analysing on-chain data, we report that FTX heavily relied on\nleveraging and misusing its native token, FTT, and we show how this behaviour\nexacerbated the company's fragile financial situation. To gain further insights\ninto the downfall, we study evolutionary dependency structures of 199\ncryptocurrencies on an hourly basis, and we investigate public trades at the\ntime of the events. Results suggest that the collapse was actively accelerated\nby Binance tweets causing a systemic reaction in the cryptocurrency market.\nFinally, identifying the actors who mostly benefited from the FTX's collapse\nand highlighting a generalised trend toward centralisation in the crypto space,\nwe emphasise the importance of genuinely decentralised finance for a\ntransparent, future digital economy.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11371v3"
    },
    {
        "title": "The impact of the AI revolution on asset management",
        "authors": [
            "Michael Kopp"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Recent progress in deep learning, a special form of machine learning, has led\nto remarkable capabilities machines can now be endowed with: they can read and\nunderstand free flowing text, reason and bargain with human counterparts,\ntranslate texts between languages, learn how to take decisions to maximize\ncertain outcomes, etc. Today, machines have revolutionized the detection of\ncancer, the prediction of protein structures, the design of drugs, the control\nof nuclear fusion reactors etc. Although these capabilities are still in their\ninfancy, it seems clear that their continued refinement and application will\nresult in a technological impact on nearly all social and economic areas of\nhuman activity, the likes of which we have not seen before. In this article, I\nwill share my view as to how AI will likely impact asset management in general\nand I will provide a mental framework that will equip readers with a simple\ncriterion to assess whether and to what degree a given fund really exploits\ndeep learning and whether a large disruption risk from deep learning exist.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10212v2"
    },
    {
        "title": "Multi-Modal Deep Learning for Credit Rating Prediction Using Text and\n  Numerical Data Streams",
        "authors": [
            "Mahsa Tavakoli",
            "Rohitash Chandra",
            "Fengrui Tian",
            "Cristián Bravo"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Knowing which factors are significant in credit rating assignment leads to\nbetter decision-making. However, the focus of the literature thus far has been\nmostly on structured data, and fewer studies have addressed unstructured or\nmulti-modal datasets. In this paper, we present an analysis of the most\neffective architectures for the fusion of deep learning models for the\nprediction of company credit rating classes, by using structured and\nunstructured datasets of different types. In these models, we tested different\ncombinations of fusion strategies with different deep learning models,\nincluding CNN, LSTM, GRU, and BERT. We studied data fusion strategies in terms\nof level (including early and intermediate fusion) and techniques (including\nconcatenation and cross-attention). Our results show that a CNN-based\nmulti-modal model with two fusion strategies outperformed other multi-modal\ntechniques. In addition, by comparing simple architectures with more complex\nones, we found that more sophisticated deep learning models do not necessarily\nproduce the highest performance; however, if attention-based models are\nproducing the best results, cross-attention is necessary as a fusion strategy.\nFinally, our comparison of rating agencies on short-, medium-, and long-term\nperformance shows that Moody's credit ratings outperform those of other\nagencies like Standard & Poor's and Fitch Ratings.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10740v3"
    },
    {
        "title": "NFT Wash Trading Detection",
        "authors": [
            "Derek Liu",
            "Francesco Piccoli",
            "Katie Chen",
            "Adrina Tang",
            "Victor Fang"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Wash trading is a form of market manipulation where the same entity sells an\nasset to themselves to drive up market prices, launder money under the cover of\na legitimate transaction, or claim a tax loss without losing ownership of an\nasset. Although the practice is illegal with traditional assets, lack of\nsupervision in the non-fungible token market enables criminals to wash trade\nand scam unsuspecting buyers while operating under regulators radar. AnChain.AI\ndesigned an algorithm that flags transactions within an NFT collection history\nas wash trades when a wallet repurchases a token within 30 days of previously\nselling it. The algorithm also identifies intermediate transactions within a\nwash trade cycle. Testing on 7 popular NFT collections reveals that on average,\n0.14% of transactions, 0.11% of wallets, and 0.16% of tokens in each collection\nare involved in wash trading. These wash trades generate an overall total price\nmanipulation, sales, and repurchase profit of \\$900K, \\$1.1M, and negative\n\\$1.6M respectively. The results draw attention to the prevalent market\nmanipulation taking place and inform unsuspecting buyers which tokens and\nsellers may be involved in criminal activity.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.01543v1"
    },
    {
        "title": "The Influence of ChatGPT on Artificial Intelligence Related Crypto\n  Assets: Evidence from a Synthetic Control Analysis",
        "authors": [
            "Aman Saggu",
            "Lennart Ante"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  The introduction of OpenAI's large language model, ChatGPT, catalyzed\ninvestor attention towards artificial intelligence (AI) technologies, including\nAI-related crypto assets not directly related to ChatGPT. Utilizing the\nsynthetic difference-in-difference methodology, we identify significant\n'ChatGPT effects' with returns of AI-related crypto assets experiencing average\nreturns ranging between 10.7% and 15.6% (35.5% to 41.3%) in the one-month\n(two-month) period after the ChatGPT launch. Furthermore, Google search\nvolumes, a proxy for attention to AI, emerged as critical pricing indicators\nfor AI-related crypto post-launch. We conclude that investors perceived\nAI-assets as possessing heightened potential or value after the launch,\nresulting in higher market valuations.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.12739v1"
    },
    {
        "title": "Optimizing Credit Limit Adjustments Under Adversarial Goals Using\n  Reinforcement Learning",
        "authors": [
            "Sherly Alfonso-Sánchez",
            "Jesús Solano",
            "Alejandro Correa-Bahnsen",
            "Kristina P. Sendova",
            "Cristián Bravo"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Reinforcement learning has been explored for many problems, from video games\nwith deterministic environments to portfolio and operations management in which\nscenarios are stochastic; however, there have been few attempts to test these\nmethods in banking problems. In this study, we sought to find and automatize an\noptimal credit card limit adjustment policy by employing reinforcement learning\ntechniques. Because of the historical data available, we considered two\npossible actions per customer, namely increasing or maintaining an individual's\ncurrent credit limit. To find this policy, we first formulated this\ndecision-making question as an optimization problem in which the expected\nprofit was maximized; therefore, we balanced two adversarial goals: maximizing\nthe portfolio's revenue and minimizing the portfolio's provisions. Second,\ngiven the particularities of our problem, we used an offline learning strategy\nto simulate the impact of the action based on historical data from a super-app\nin Latin America to train our reinforcement learning agent. Our results, based\non the proposed methodology involving synthetic experimentation, show that a\nDouble Q-learning agent with optimized hyperparameters can outperform other\nstrategies and generate a non-trivial optimal policy not only reflecting the\ncomplex nature of this decision but offering an incentive to explore\nreinforcement learning in real-world banking scenarios. Our research\nestablishes a conceptual structure for applying reinforcement learning\nframework to credit limit adjustment, presenting an objective technique to make\nthese decisions primarily based on data-driven methods rather than relying only\non expert-driven systems. We also study the use of alternative data for the\nproblem of balance prediction, as the latter is a requirement of our proposed\nmodel. We find the use of such data does not always bring prediction gains.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.15585v2"
    },
    {
        "title": "A Classical Model of Speculative Asset Price Dynamics",
        "authors": [
            "Sabiou Inoua",
            "Vernon Smith"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  In retrospect, the experimental findings on competitive market behavior\ncalled for a revival of the old, classical, view of competition as a collective\nhiggling and bargaining process (as opposed to price-taking behaviors) founded\non reservation prices (in place of the utility function). In this paper, we\nspecialize the classical methodology to deal with speculation, an important\nimpediment to price stability. The model involves typical features of a field\nor lab asset market setup and lends itself to an experimental test of its\nspecific predictions; here we use the model to explain three general stylized\nfacts, well established both empirically and experimentally: the excess,\nfat-tailed, and clustered volatility of speculative asset prices. The fat tails\nemerge in the model from the amplifying nature of speculation, leading to a\nrandom-coefficient autoregressive return process (and power-law tails); the\nvolatility clustering is due to the traders' long memory of news; bubbles are a\npersistent phenomenon in the model, and, assuming the standard lab present\nvalue pattern, the bubble size increases with the proportion of speculators and\ndecreases with the trading horizon.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00410v1"
    },
    {
        "title": "dYdX: Liquidity Providers' Incentive Programme Review",
        "authors": [
            "Colin Chan"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Liquidity providers are currently incentivised to provide liquidity through\nthe LP Incentives Programme on dYdX. Based on the various parameters -\nmakerVolume, depths and spreads, they are rewarded accordingly based on their\nactivities. Given the maturity of the BTC and ETH markets, alongside other\naltcoins which enjoy a consistent amount of liquidity, this paper aims to\nupdate the formula to encourage more active and efficient liquidity, improving\nthe overall trading experience. In this research, I begin by providing a basic\nunderstanding of spread management, before introducing the methodology with the\nvarious metrics and conditions. This includes gathering orderbooks on a minute\ninterval and reconstructing the depths based on historical trades to establish\nan upper bound. I end off by providing recommendations to update the maxSpread\nparameter and alternative mechanisms/solutions to improve the existing market\nstructures.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.03935v1"
    },
    {
        "title": "Exploring Implied Certainty Equivalent Rates in Financial Markets:\n  Empirical Analysis and Application to the Electric Vehicle Industry",
        "authors": [
            "Yifan He",
            "Svetlozar Rachev"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  In this paper, we mainly study the impact of the implied certainty equivalent\nrate on investment in financial markets. First, we derived the mathematical\nexpression of the implied certainty equivalent rate by using put-call parity,\nand then we selected some company stocks and options; we considered the\nbest-performing and worst-performing company stocks and options from the\nbeginning of 2023 to the present for empirical research. By visualizing the\nrelationship between the time to maturity, moneyness, and implied certainty\nequivalent rate of these options, we have obtained a universal conclusion -- a\npositive implied certainty equivalent rate is more suitable for investment than\na negative implied certainty equivalent rate, but for a positive implied\ncertainty equivalent rate, a larger value also means a higher investment risk.\nNext, we applied these results to the electric vehicle industry, and by\ncomparing several well-known US electric vehicle production companies, we\nfurther strengthened our conclusions. Finally, we give a warning concerning\nrisk, that is, investment in the financial market should not focus solely on\nthe implied certainty equivalent rate, because investment is not an easy task,\nand many factors need to be considered, including some factors that are\ndifficult to predict with models.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08675v2"
    },
    {
        "title": "The Impacts of Registration Regime Implementation on IPO Pricing\n  Efficiency",
        "authors": [
            "Qi Deng",
            "Linhong Zheng",
            "Jiaqi Peng",
            "Xu Li",
            "Zhong-guo Zhou",
            "Monica Hussein",
            "Dingyi Chen",
            "Mick Swartz"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We study the impacts of regime changes and related rule implementations on\nIPOs initial return for China entrepreneurial boards (ChiNext and STAR). We\npropose that an initial return contains the issuer fair value and an investors\noverreaction and examine their magnitudes and determinants. Our findings reveal\nan evolution of IPO pricing in response to the progression of regulation\nchanges along four dimensions: 1) governing regulation regime, 2) listing day\ntrading restrictions, 3) listing rules for issuers, and 4) participation\nrequirements for investors. We find that the most efficient regulation regime\nin Chinese IPO pricing has four characteristics: 1) registration system, 2) no\nhard return caps nor trading curbs that restrict the initial return; 3) more\nspecific listing rules for issuers, and 4) more stringent participation\nrequirements for investors. In all contexts, we show that the registration\nregime governing the STAR IPOs offers the most efficient pricing.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.09669v1"
    },
    {
        "title": "Changes in Risk Appreciation, and Short Memory of House Buyers When the\n  Market is Hot, a Case Study of Christchurch, New Zealand",
        "authors": [
            "Emil Mendoza",
            "Fabian Dunker",
            "Marco Reale"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  In this paper house prices in Christchurch are analyzed over three distinct\nperiods of time: post-2011 earthquake, pre-COVID-19 lockdown, and post-COVID-19\nlockdown using the well-established hedonic price model. Results show that\nbuyers, in periods that are temporally distant from the 2011 Christchurch\nearthquake, value the risk of potential earthquake damage to a property\ndifferently from buyers soon after the earthquake. We find that there are\nobservable shifts in hedonic prices across the different time periods,\nspecifically for section size pre and post COVID-19 lockdown.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.13232v1"
    },
    {
        "title": "Capital Structure Theories and its Practice, A study with reference to\n  select NSE listed public sectors banks, India",
        "authors": [
            "Kurada T S S Satyanarayana",
            "Addada Narasimha Rao"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Among the various factors affecting the firms positioning and performance in\nmodern day markets, capital structure of the firm has its own way of expressing\nitself as a crucial one. With the rapid changes in technology, firms are being\npushed onto a paradigm that is burdening the capital management process. Hence\nthe study of capital structure changes gives the investors an insight into\nfirm's behavior and intrinsic goals. These changes will vary for firms in\ndifferent sectors. This work considers the banking sector, which has a unique\ncapital structure for the given regulations of its operations in India. The\ncapital structure behavioral changes in a few public sector banks are studied\nin this paper. A theoretical framework has been developed from the popular\ncapital structure theories and hypotheses are derived from them accordingly.\nThe main idea is to validate different theories with real time performance of\nthe select banks from 2011 to 2022. Using statistical techniques like\nregression and correlation, tested hypotheses have resulted in establishing the\nrelation between debt component and financial performance variables of the\nselect banks which are helping in understanding the theories in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14049v1"
    },
    {
        "title": "A new probabilistic analysis of the yard-sale model",
        "authors": [
            "Christoph Börgers",
            "Claude Greengard"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  In Chakraborti's yard-sale model of an economy, identical agents engage in\ntrades that result in wealth exchanges, but conserve the combined wealth of all\nagents and each agent's expected wealth. In this model, wealth condensation,\nthat is, convergence to a state in which one agent owns everything and the\nothers own nothing, occurs almost surely. We give a proof of this fact that is\nmuch shorter than existing ones and extends to a modified model in which there\nis a wealth-acquired advantage, i.e., the wealthier of two trading partners is\nmore likely to benefit from the trade.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.01485v1"
    },
    {
        "title": "Quantifying MEV On Layer 2 Networks",
        "authors": [
            "Arthur Bagourd",
            "Luca Georges Francois"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This paper addresses the lack of research on quantifying Maximal Extractable\nValue (MEV) on Ethereum Layer 2 networks (L2s). Our findings reveal a\nsubstantial amount of MEV to be extracted on L2s, particularly on Polygon, with\na lower bound of $213 million surpassing previous estimates. We observe that\nthe majority of detected MEV on L2s consists of arbitrage opportunities, as\nliquidations are rare. These results emphasize the need for continuous\nmonitoring and analysis of MEV on L2s, promoting informed decision-making for\nnetwork selection and highlighting the associated risks.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.00629v1"
    },
    {
        "title": "Perishable Goods versus Re-tradable Assets: A Theoretical Reappraisal of\n  a Fundamental Dichotomy",
        "authors": [
            "Sabiou Inoua",
            "Vernon Smith"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Experimental results on market behavior establish a lower stability and\nefficiency of markets for durable re-tradable assets compared to markets for\nnon-durable, or perishable, goods. In this chapter, we revisit this known but\nunderappreciated dichotomy of goods in the light of our theory of competitive\nmarket price formation, and we emphasize the fundamental nature of the concept\nof asset re-tradability in neoclassical finance through a simple reformulation\nof the famous no-trade and no-arbitrage theorems.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.03432v1"
    },
    {
        "title": "News-driven Expectations and Volatility Clustering",
        "authors": [
            "Sabiou Inoua"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Financial volatility obeys two fascinating empirical regularities that apply\nto various assets, on various markets, and on various time scales: it is\nfat-tailed (more precisely power-law distributed) and it tends to be clustered\nin time. Many interesting models have been proposed to account for these\nregularities, notably agent-based models, which mimic the two empirical laws\nthrough a complex mix of nonlinear mechanisms such as traders' switching\nbetween trading strategies in highly nonlinear way. This paper explains the two\nregularities simply in terms of traders' attitudes towards news, an explanation\nthat follows almost by definition of the traditional dichotomy of financial\nmarket participants, investors versus speculators, whose behaviors are reduced\nto their simplest forms. Long-run investors' valuations of an asset are assumed\nto follow a news-driven random walk, thus capturing the investors' persistent,\nlong memory of fundamental news. Short-term speculators' anticipated returns,\non the other hand, are assumed to follow a news-driven autoregressive process,\ncapturing their shorter memory of fundamental news, and, by the same token, the\nfeedback intrinsic to the short-sighted, trend-following (or herding) mindset\nof speculators. These simple, linear, models of traders' expectations, it is\nshown, explain the two financial regularities in a generic and robust way.\nRational expectations, the dominant model of traders' expectations, is not\nassumed here, owing to the famous no-speculation, no-trade results\n",
        "pdf_link": "http://arxiv.org/pdf/2309.04876v1"
    },
    {
        "title": "New News is Bad News",
        "authors": [
            "Paul Glasserman",
            "Harry Mamaysky",
            "Jimmy Qin"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  An increase in the novelty of news predicts negative stock market returns and\nnegative macroeconomic outcomes over the next year. We quantify news novelty -\nchanges in the distribution of news text - through an entropy measure,\ncalculated using a recurrent neural network applied to a large news corpus.\nEntropy is a better out-of-sample predictor of market returns than a collection\nof standard measures. Cross-sectional entropy exposure carries a negative risk\npremium, suggesting that assets that positively covary with entropy hedge the\naggregate risk associated with shifting news language. Entropy risk cannot be\nexplained by existing long-short factors.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.05560v1"
    },
    {
        "title": "A New Framework to Estimate Return on Investment for Player Salaries in\n  the National Basketball Association",
        "authors": [
            "Jackson P. Lautier"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  The National Basketball Association (NBA) imposes a player salary cap. It is\ntherefore useful to develop tools to measure the relative realized return of a\nplayer's salary given their on court performance. Very few such studies exist,\nhowever. We thus present the first known framework to estimate a return on\ninvestment (ROI) for NBA player contracts. The framework operates in five\nparts: (1) decide on a measurement time horizon, such as the standard 82-game\nNBA regular season; (2) calculate the novel game contribution percentage (GCP)\nmeasure we propose, which is a single game summary statistic that sums to unity\nfor each competing team and is comprised of traditional, playtype, hustle, box\nouts, defensive, tracking, and rebounding per game NBA statistics; (3) estimate\nthe single game value (SGV) of each regular season NBA game using a standard\ncurrency conversion calculation; (4) multiply the SGV by the vector of realized\nGCPs to obtain a series of realized per-player single season cash flows; and\n(5) use the player salary as an initial investment to perform the traditional\nROI calculation. We illustrate our framework by compiling a novel, sharable\ndataset of per game GCP statistics and salaries for the 2022-2023 NBA regular\nseason. A scatter plot of ROI by salary for all players is presented, including\nthe top and bottom 50 performers. Notably, missed games are treated as defaults\nbecause GCP is a per game metric. This allows for break-even calculations\nbetween high-performing players with frequent missed games and average\nperformers with few missed games, which we demonstrate with a comparison of the\n2023 NBA regular seasons of Anthony Davis and Brook Lopez. We conclude by\nsuggesting uses of our framework, discussing its flexibility through\ncustomization, and outlining potential future improvements.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.05783v1"
    },
    {
        "title": "Assessing the Solvency of Virtual Asset Service Providers: Are Current\n  Standards Sufficient?",
        "authors": [
            "Pietro Saggese",
            "Esther Segalla",
            "Michael Sigmund",
            "Burkhard Raunig",
            "Felix Zangerl",
            "Bernhard Haslhofer"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Entities like centralized cryptocurrency exchanges fall under the business\ncategory of virtual asset service providers (VASPs). As any other enterprise,\nthey can become insolvent. VASPs enable the exchange, custody, and transfer of\ncryptoassets organized in wallets across distributed ledger technologies\n(DLTs). Despite the public availability of DLT transactions, the cryptoasset\nholdings of VASPs are not yet subject to systematic auditing procedures. In\nthis paper, we propose an approach to assess the solvency of a VASP by\ncross-referencing data from three distinct sources: cryptoasset wallets,\nbalance sheets from the commercial register, and data from supervisory\nentities. We investigate 24 VASPs registered with the Financial Market\nAuthority in Austria and provide regulatory data insights such as who are the\ncustomers and where do they come from. Their yearly incoming and outgoing\ntransaction volume amount to 2 billion EUR for around 1.8 million users. We\ndescribe what financial services they provide and find that they are most\nsimilar to traditional intermediaries such as brokers, money exchanges, and\nfunds, rather than banks. Next, we empirically measure DLT transaction flows of\nfour VASPs and compare their cryptoasset holdings to balance sheet entries.\nData are consistent for two VASPs only. This enables us to identify gaps in the\ndata collection and propose strategies to address them. We remark that any\nentity in charge of auditing requires proof that a VASP actually controls the\nfunds associated with its on-chain wallets. It is also important to report fiat\nand cryptoasset and liability positions broken down by asset types at a\nreasonable frequency.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.16408v2"
    },
    {
        "title": "Assessing Look-Ahead Bias in Stock Return Predictions Generated By GPT\n  Sentiment Analysis",
        "authors": [
            "Paul Glasserman",
            "Caden Lin"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Large language models (LLMs), including ChatGPT, can extract profitable\ntrading signals from the sentiment in news text. However, backtesting such\nstrategies poses a challenge because LLMs are trained on many years of data,\nand backtesting produces biased results if the training and backtesting periods\noverlap. This bias can take two forms: a look-ahead bias, in which the LLM may\nhave specific knowledge of the stock returns that followed a news article, and\na distraction effect, in which general knowledge of the companies named\ninterferes with the measurement of a text's sentiment. We investigate these\nsources of bias through trading strategies driven by the sentiment of financial\nnews headlines. We compare trading performance based on the original headlines\nwith de-biased strategies in which we remove the relevant company's identifiers\nfrom the text. In-sample (within the LLM training window), we find,\nsurprisingly, that the anonymized headlines outperform, indicating that the\ndistraction effect has a greater impact than look-ahead bias. This tendency is\nparticularly strong for larger companies--companies about which we expect an\nLLM to have greater general knowledge. Out-of-sample, look-ahead bias is not a\nconcern but distraction remains possible. Our proposed anonymization procedure\nis therefore potentially useful in out-of-sample implementation, as well as for\nde-biased backtesting.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.17322v1"
    },
    {
        "title": "AI for Investment: A Platform Disruption",
        "authors": [
            "Mohammad Rasouli",
            "Ravi Chiruvolu",
            "Ali Risheh"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  With the investment landscape becoming more competitive, efficiently scaling\ndeal sourcing and improving deal insights have become a dominant strategy for\nfunds. While funds are already spending significant efforts on these two tasks,\nthey cannot be scaled with traditional approaches; hence, there is a surge in\nautomating them. Many third party software providers have emerged recently to\naddress this need with productivity solutions, but they fail due to a lack of\npersonalization for the fund, privacy constraints, and natural limits of\nsoftware use cases. Therefore, most major funds and many smaller funds have\nstarted developing their in-house AI platforms: a game changer for the\nindustry. These platforms grow smarter by direct interactions with the fund and\ncan be used to provide personalized use cases. Recent developments in large\nlanguage models, e.g. ChatGPT, have provided an opportunity for other funds to\nalso develop their own AI platforms. While not having an AI platform now is not\na competitive disadvantage, it will be in two years. Funds require a practical\nplan and corresponding risk assessments for such AI platforms.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06251v1"
    },
    {
        "title": "A Hypothesis on Good Practices for AI-based Systems for Financial Time\n  Series Forecasting: Towards Domain-Driven XAI Methods",
        "authors": [
            "Branka Hadji Misheva",
            "Joerg Osterrieder"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Machine learning and deep learning have become increasingly prevalent in\nfinancial prediction and forecasting tasks, offering advantages such as\nenhanced customer experience, democratising financial services, improving\nconsumer protection, and enhancing risk management. However, these complex\nmodels often lack transparency and interpretability, making them challenging to\nuse in sensitive domains like finance. This has led to the rise of eXplainable\nArtificial Intelligence (XAI) methods aimed at creating models that are easily\nunderstood by humans. Classical XAI methods, such as LIME and SHAP, have been\ndeveloped to provide explanations for complex models. While these methods have\nmade significant contributions, they also have limitations, including\ncomputational complexity, inherent model bias, sensitivity to data sampling,\nand challenges in dealing with feature dependence. In this context, this paper\nexplores good practices for deploying explainability in AI-based systems for\nfinance, emphasising the importance of data quality, audience-specific methods,\nconsideration of data properties, and the stability of explanations. These\npractices aim to address the unique challenges and requirements of the\nfinancial industry and guide the development of effective XAI tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.07513v1"
    },
    {
        "title": "Cryptocurrency in the Aftermath: Unveiling the Impact of the SVB\n  Collapse",
        "authors": [
            "Qin Wang",
            "Guangsheng Yu",
            "Shiping Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  In this paper, we explore the aftermath of the Silicon Valley Bank (SVB)\ncollapse, with a particular focus on its impact on crypto markets. We conduct a\nmulti-dimensional investigation, which includes a factual summary, analysis of\nuser sentiment, and examination of market performance. Based on such efforts,\nwe uncover a somewhat counterintuitive finding: the SVB collapse did not lead\nto the destruction of cryptocurrencies; instead, they displayed resilience.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.10720v1"
    },
    {
        "title": "Predicting Failure of P2P Lending Platforms through Machine Learning:\n  The Case in China",
        "authors": [
            "Jen-Yin Yeh",
            "Hsin-Yu Chiu",
            "Jhih-Huei Huang"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This study employs machine learning models to predict the failure of\nPeer-to-Peer (P2P) lending platforms, specifically in China. By employing the\nfilter method and wrapper method with forward selection and backward\nelimination, we establish a rigorous and practical procedure that ensures the\nrobustness and importance of variables in predicting platform failures. The\nresearch identifies a set of robust variables that consistently appear in the\nfeature subsets across different selection methods and models, suggesting their\nreliability and relevance in predicting platform failures. The study highlights\nthat reducing the number of variables in the feature subset leads to an\nincrease in the false acceptance rate while the performance metrics remain\nstable, with an AUC value of approximately 0.96 and an F1 score of around 0.88.\nThe findings of this research provide significant practical implications for\nregulatory authorities and investors operating in the Chinese P2P lending\nindustry.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14577v1"
    },
    {
        "title": "The Paradox Of Just-in-Time Liquidity in Decentralized Exchanges: More\n  Providers Can Sometimes Mean Less Liquidity",
        "authors": [
            "Agostino Capponi",
            "Ruizhe Jia",
            "Brian Zhu"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We study Just-in-time (JIT) liquidity provision in blockchain-based\ndecentralized exchanges. A JIT liquidity provider (LP) monitors pending swap\norders in public mempools of blockchains to sandwich orders of their choice\nwith liquidity, depositing right before and withdrawing right after the order.\nOur game-theoretic model with asymmetrically informed agents reveals that a JIT\nLP's presence does not always enhance liquidity pool depth, as one might\nexpect. While passive LPs face adverse selection by informed arbitrageurs, a\nJIT LP's ability to detect pending orders for toxic order flow prior to\nliquidity provision lets them avoid being adversely selected. JIT LPs thus only\nprovide liquidity to uninformed orders and crowd out passive LPs when order\nvolume is not sufficiently elastic to pool depth, possibly reducing overall\nmarket liquidity. We show that using a two-tiered fee structure which transfers\na part of a JIT LP's fee revenue to passive LPs or allowing for JIT LPs to\ncompete \\`{a} la Cournot are potential solutions to mitigate the negative\neffects of JIT liquidity.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.18164v2"
    },
    {
        "title": "A Statistical Field Perspective on Capital Allocation and Accumulation:\n  Individual dynamics",
        "authors": [
            "Pierre Gosselin",
            "Aïleen Lotz"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We have shown, in a series of articles, that a classical description of a\nlarge number of economic agents can be replaced by a statistical fields\nformalism. To better understand the accumulation and allocation of capital\namong different sectors, the present paper applies this statistical fields\ndescription to a large number of heterogeneous agents divided into two groups.\nThe first group is composed of a large number of firms in different sectors\nthat collectively own the entire physical capital. The second group, investors,\nholds the entire financial capital and allocates it between firms across\nsectors according to investment preferences, expected returns, and stock prices\nvariations on financial markets. In return, firms pay dividends to their\ninvestors. Financial capital is thus a function of dividends and stock\nvaluations, whereas physical capital is a function of the total capital\nallocated by the financial sector. Whereas our previous work focused on the\nbackground fields that describe potential long-term equilibria, here we compute\nthe transition functions of individual agents and study their probabilistic\ndynamics in the background field, as a function of their initial state. We show\nthat capital accumulation depends on various factors. The probability\nassociated with each firm's trajectories is the result of several contradictory\neffects: the firm tends to shift towards sectors with the greatest long-term\nreturn, but must take into account the impact of its shift on its\nattractiveness for investors throughout its trajectory. Since this trajectory\ndepends largely on the average capital of transition sectors, a firm's\nattractiveness during its relocation depends on the relative level of capital\nin those sectors. Thus, an under-capitalized firm reaching a high-capital\nsector will experience a loss of attractiveness, and subsequently, in\ninvestors. Moreover, the firm must also consider the effects of competition in\nthe intermediate sectors. An under-capitalized firm will tend to be ousted out\ntowards sectors with lower average capital, while an over-capitalized firm will\ntend to shift towards higher averagecapital sectors. For investors, capital\nallocation depends on their short and long-term returns. These returns are not\nindependent: in the short-term, returns are composed of both the firm's\ndividends and the increase in its stock prices. In the long-term, returns are\nbased on the firm's growth expectations, but also, indirectly, on expectations\nof higher stock prices. Investors' capital allocation directly depends on the\nvolatility of stock prices and {\\ldots}rms'dividends. Investors will tend to\nreallocate their capital to maximize their short and long-term returns. The\nhigher their level of capital, the stronger the reallocation will be.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.06142v1"
    },
    {
        "title": "Multimodal Gen-AI for Fundamental Investment Research",
        "authors": [
            "Lezhi Li",
            "Ting-Yu Chang",
            "Hai Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This report outlines a transformative initiative in the financial investment\nindustry, where the conventional decision-making process, laden with\nlabor-intensive tasks such as sifting through voluminous documents, is being\nreimagined. Leveraging language models, our experiments aim to automate\ninformation summarization and investment idea generation. We seek to evaluate\nthe effectiveness of fine-tuning methods on a base model (Llama2) to achieve\nspecific application-level goals, including providing insights into the impact\nof events on companies and sectors, understanding market condition\nrelationships, generating investor-aligned investment ideas, and formatting\nresults with stock recommendations and detailed explanations. Through\nstate-of-the-art generative modeling techniques, the ultimate objective is to\ndevelop an AI agent prototype, liberating human investors from repetitive tasks\nand allowing a focus on high-level strategic thinking. The project encompasses\na diverse corpus dataset, including research reports, investment memos, market\nnews, and extensive time-series market data. We conducted three experiments\napplying unsupervised and supervised LoRA fine-tuning on the llama2_7b_hf_chat\nas the base model, as well as instruction fine-tuning on the GPT3.5 model.\nStatistical and human evaluations both show that the fine-tuned versions\nperform better in solving text modeling, summarization, reasoning, and finance\ndomain questions, demonstrating a pivotal step towards enhancing\ndecision-making processes in the financial domain. Code implementation for the\nproject can be found on GitHub: https://github.com/Firenze11/finance_lm.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.06164v1"
    },
    {
        "title": "Forking paths in financial economics",
        "authors": [
            "Guillaume Coqueret"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We argue that spanning large numbers of degrees of freedom in empirical\nanalysis allows better characterizations of effects and thus improves the\ntrustworthiness of conclusions. Our ideas are illustrated in three studies:\nequity premium prediction, asset pricing anomalies and risk premia estimation.\nIn the first, we find that each additional degree of freedom in the protocol\nexpands the average range of $t$-statistics by at least 30%. In the second, we\nshow that resorting to forking paths instead of bootstrapping in multiple\ntesting raises the bar of significance for anomalies: at the 5% confidence\nlevel, the threshold for bootstrapped statistics is 4.5, whereas with paths, it\nis at least 8.2, a bar much higher than those currently used in the literature.\nIn our third application, we reveal the importance of particular steps in the\nestimation of premia. In addition, we use paths to corroborate prior findings\nin the three topics. We document heterogeneity in our ability to replicate\nprior studies: some conclusions seem robust, others do not align with the paths\nwe were able to generate.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.08606v1"
    },
    {
        "title": "Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities\n  and Risks",
        "authors": [
            "Xihan Xiong",
            "Zhipeng Wang",
            "Xi Chen",
            "William Knottenbelt",
            "Michael Huth"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  In the Proof of Stake (PoS) Ethereum ecosystem, users can stake ETH on Lido\nto receive stETH, a Liquid Staking Derivative (LSD) that represents staked ETH\nand accrues staking rewards. LSDs improve the liquidity of staked assets by\nfacilitating their use in secondary markets, such as for collateralized\nborrowing on Aave or asset exchanges on Curve. The composability of Lido, Aave,\nand Curve enables an emerging strategy known as leverage staking, an iterative\nprocess that enhances financial returns while introducing potential risks. This\npaper establishes a formal framework for leverage staking with stETH and\nidentifies 442 such positions on Ethereum over 963 days. These positions\nrepresent a total volume of 537,123 ETH (877m USD). Our data reveal that 81.7%\nof leverage staking positions achieved an Annual Percentage Rate (APR) higher\nthan conventional staking on Lido. Despite the high returns, we also recognize\nthe potential risks. For example, the Terra crash incident demonstrated that\ntoken devaluation can impact the market. Therefore, we conduct stress tests\nunder extreme conditions of significant stETH devaluation to evaluate the\nassociated risks. Our simulations reveal that leverage staking amplifies the\nrisk of cascading liquidations by triggering intensified selling pressure\nthrough liquidation and deleveraging processes. Furthermore, this dynamic not\nonly accelerates the decline of stETH prices but also propagates a contagion\neffect, endangering the stability of both leveraged and ordinary positions.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.08610v4"
    },
    {
        "title": "Attention-based Dynamic Multilayer Graph Neural Networks for Loan\n  Default Prediction",
        "authors": [
            "Sahab Zandi",
            "Kamesh Korangi",
            "María Óskarsdóttir",
            "Christophe Mues",
            "Cristián Bravo"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Whereas traditional credit scoring tends to employ only individual borrower-\nor loan-level predictors, it has been acknowledged for some time that\nconnections between borrowers may result in default risk propagating over a\nnetwork. In this paper, we present a model for credit risk assessment\nleveraging a dynamic multilayer network built from a Graph Neural Network and a\nRecurrent Neural Network, each layer reflecting a different source of network\nconnection. We test our methodology in a behavioural credit scoring context\nusing a dataset provided by U.S. mortgage financier Freddie Mac, in which\ndifferent types of connections arise from the geographical location of the\nborrower and their choice of mortgage provider. The proposed model considers\nboth types of connections and the evolution of these connections over time. We\nenhance the model by using a custom attention mechanism that weights the\ndifferent time snapshots according to their importance. After testing multiple\nconfigurations, a model with GAT, LSTM, and the attention mechanism provides\nthe best results. Empirical results demonstrate that, when it comes to\npredicting probability of default for the borrowers, our proposed model brings\nboth better results and novel insights for the analysis of the importance of\nconnections and timestamps, compared to traditional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00299v2"
    },
    {
        "title": "Predicting the volatility of major energy commodity prices: the dynamic\n  persistence model",
        "authors": [
            "Jozef Barunik",
            "Lukas Vacha"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Time variation and persistence are crucial properties of volatility that are\noften studied separately in energy volatility forecasting models. Here, we\npropose a novel approach that allows shocks with heterogeneous persistence to\nvary smoothly over time, and thus model the two together. We argue that this is\nimportant because such dynamics arise naturally from the dynamic nature of\nshocks in energy commodities. We identify such dynamics from the data using\nlocalised regressions and build a model that significantly improves volatility\nforecasts. Such forecasting models, based on a rich persistence structure that\nvaries smoothly over time, outperform state-of-the-art benchmark models and are\nparticularly useful for forecasting over longer horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.01354v2"
    },
    {
        "title": "Volatility-based strategy on Chinese equity index ETF options",
        "authors": [
            "Peng Yifeng"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This study examines the performance of a volatility-based strategy using\nChinese equity index ETF options. Initially successful, the strategy's\neffectiveness waned post-2018. By integrating GARCH models for volatility\nforecasting, the strategy's positions and exposures are dynamically adjusted.\nThe results indicate that such an approach can enhance returns in volatile\nmarkets, suggesting potential for refined trading strategies in China's\nevolving derivatives landscape. The research underscores the importance of\nadaptive strategies in capturing market opportunities amidst changing trading\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00474v2"
    },
    {
        "title": "Tax Policy Handbook for Crypto Assets",
        "authors": [
            "Arindam Misra"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The Financial system has witnessed rapid technological changes. The rise of\nBitcoin and other crypto assets based on Distributed Ledger Technology mark a\nfundamental change in the way people transact and transmit value over a\ndecentralized network, spread across geographies. This has created regulatory\nand tax policy blind spots, as governments and tax administrations take time to\nunderstand and provide policy responses to this innovative, revolutionary, and\nfast-paced technology. Due to the breakneck speed of innovation in blockchain\ntechnology and advent of Decentralized Finance, Decentralized Autonomous\nOrganizations and the Metaverse, it is unlikely that the policy interventions\nand guidance by regulatory authorities or tax administrations would be ahead or\nin sync with the pace of innovation. This paper tries to explain the principles\non which crypto assets function, their underlying technology and relates them\nto the tax issues and taxable events which arise within this ecosystem. It also\nprovides instances of tax and regulatory policy responses already in effect in\nvarious jurisdictions, including the recent changes in reporting standards by\nthe FATF and the OECD. This paper tries to explain the rationale behind\nexisting laws and policies and the challenges in their implementation. It also\nattempts to present a ballpark estimate of tax potential of this asset class\nand suggests creation of global public digital infrastructure that can address\nissues related to pseudonymity and extra-territoriality. The paper analyses\nboth direct and indirect taxation issues related to crypto assets and discusses\nmore recent aspects like proof-of-stake and maximal extractable value in\ngreater detail.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15074v3"
    },
    {
        "title": "One Factor to Bind the Cross-Section of Returns",
        "authors": [
            "Nicola Borri",
            "Denis Chetverikov",
            "Yukun Liu",
            "Aleh Tsyvinski"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  We propose a new non-linear single-factor asset pricing model\n$r_{it}=h(f_{t}\\lambda_{i})+\\epsilon_{it}$. Despite its parsimony, this model\nrepresents exactly any non-linear model with an arbitrary number of factors and\nloadings -- a consequence of the Kolmogorov-Arnold representation theorem. It\nfeatures only one pricing component $h(f_{t}\\lambda_{I})$, comprising a\nnonparametric link function of the time-dependent factor and factor loading\nthat we jointly estimate with sieve-based estimators. Using 171 assets across\nmajor classes, our model delivers superior cross-sectional performance with a\nlow-dimensional approximation of the link function. Most known finance and\nmacro factors become insignificant controlling for our single-factor.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.08129v1"
    },
    {
        "title": "Riding Wavelets: A Method to Discover New Classes of Price Jumps",
        "authors": [
            "Cecilia Aubrun",
            "Rudy Morel",
            "Michael Benzaquen",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Cascades of events and extreme occurrences have garnered significant\nattention across diverse domains such as financial markets, seismology, and\nsocial physics. Such events can stem either from the internal dynamics inherent\nto the system (endogenous), or from external shocks (exogenous). The\npossibility of separating these two classes of events has critical implications\nfor professionals in those fields. We introduce an unsupervised framework\nleveraging a representation of jump time-series based on wavelet coefficients\nand apply it to stock price jumps. In line with previous work, we recover the\nfact that the time-asymmetry of volatility is a major feature. Mean-reversion\nand trend are found to be two additional key features, allowing us to identify\nnew classes of jumps. Furthermore, thanks to our wavelet-based representation,\nwe investigate the reflexive properties of co-jumps, which occur when multiple\nstocks experience price jumps within the same minute. We argue that a\nsignificant fraction of co-jumps results from an endogenous contagion\nmechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.16467v1"
    },
    {
        "title": "Predicting the distributions of stock returns around the globe in the\n  era of big data and learning",
        "authors": [
            "Jozef Barunik",
            "Martin Hronec",
            "Ondrej Tobek"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This paper presents a method for accurately predicting the full distribution\nof stock returns, given a comprehensive set of 194 stock characteristics and\nmarket variables. Such distributions, learned from rich data using a machine\nlearning algorithm, are not constrained by restrictive model assumptions and\nallow the exploration of non-Gaussian, heavy-tailed data and their non-linear\ninteractions. The method uses a two-stage quantile neural network combined with\nspline interpolation. The results show that the proposed approach outperforms\nalternative models in terms of out-of-sample losses. Furthermore, we show that\nthe moments derived from such distributions can be useful as alternative\nempirical estimates in many cases, including mean estimation and forecasting.\nFinally, we examine the relationship between cross-sectional returns and\nseveral distributional characteristics. The results are robust to a wide range\nof US and international data.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07497v1"
    },
    {
        "title": "TIMeSynC: Temporal Intent Modelling with Synchronized Context Encodings\n  for Financial Service Applications",
        "authors": [
            "Dwipam Katariya",
            "Juan Manuel Origgi",
            "Yage Wang",
            "Thomas Caputo"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Users engage with financial services companies through multiple channels,\noften interacting with mobile applications, web platforms, call centers, and\nphysical locations to service their accounts. The resulting interactions are\nrecorded at heterogeneous temporal resolutions across these domains. This\nmulti-channel data can be combined and encoded to create a comprehensive\nrepresentation of the customer's journey for accurate intent prediction. This\ndemands sequential learning solutions. NMT transformers achieve\nstate-of-the-art sequential representation learning by encoding context and\ndecoding for the next best action to represent long-range dependencies.\nHowever, three major challenges exist while combining multi-domain sequences\nwithin an encode-decoder transformers architecture for intent prediction\napplications: a) aligning sequences with different sampling rates b) learning\ntemporal dynamics across multi-variate, multi-domain sequences c) combining\ndynamic and static sequences. We propose an encoder-decoder transformer model\nto address these challenges for contextual and sequential intent prediction in\nfinancial servicing applications. Our experiments show significant improvement\nover the existing tabular method.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12825v2"
    },
    {
        "title": "Financial Performance and Economic Implications of COFCO's Strategic\n  Acquisition of Mengniu",
        "authors": [
            "Jessica Ji",
            "David Yu"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This paper examines the merger and acquisition (M&A) process between COFCO\nand Mengniu Dairy, exploring the motivations behind this strategic move and\nidentifying its key aspects. By analyzing both the financial and non-financial\ncontributions of Mengniu Dairy to COFCO, this study provides valuable insights\nand references for future corporate M&A activities. The theoretical\nsignificance of this research lies in its focus on the relatively underexplored\narea of M&A within the dairy industry, particularly in terms of M&A\ncontributions. Using the COFCO-Mengniu case as a model, the study broadens\ncurrent research perspectives by assessing the impact of M&A from financial and\nnon-financial standpoints, enriching the body of literature on dairy industry\nM&As.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16299v1"
    },
    {
        "title": "A FinTech Clustering Framework: Technology, Model, and Stakeholder\n  Perspectives",
        "authors": [
            "Pak-Lok Poon",
            "Santoso Wibowo",
            "Sau-Fun Tang"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Nowadays, the global booming of FinTech can be seen everywhere. FinTech has\ncreated innovative disruptions to traditional, long-established financial\ninstitutions (e.g., banks and insurance companies) in financial services\nmarkets. Despite of its popularity, there are many different definitions of\nFinTech. This problem occurs because many existing studies only focus on a\nparticular aspect of FinTech without a comprehensive and in-depth analysis.\nThis problem will hinder further development and industrial application of\nFinTech. In view of this problem, we perform a narrative review involving over\n100 relevant studies or reports, with a view to developing a FinTech clustering\nframework for providing a more comprehensive and holistic view of FinTech.\nFurthermore, we use an Indian FinTech firm to illustrate how to apply our\nclustering framework for analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.05285v1"
    },
    {
        "title": "Analysis of the real estate market in Las Vegas: Bubble, seasonal\n  patterns, and prediction of the CSW indexes",
        "authors": [
            "Wei-Xing Zhou",
            "Didier Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We analyze 27 house price indexes of Las Vegas from Jun. 1983 to Mar. 2005,\ncorresponding to 27 different zip codes. These analyses confirm the existence\nof a real-estate bubble, defined as a price acceleration faster than\nexponential, which is found however to be confined to a rather limited time\ninterval in the recent past from approximately 2003 to mid-2004 and has\nprogressively transformed into a more normal growth rate comparable to\npre-bubble levels in 2005. There has been no bubble till 2002 except for a\nmedium-sized surge in 1990. In addition, we have identified a strong yearly\nperiodicity which provides a good potential for fine-tuned prediction from\nmonth to month. A monthly monitoring using a model that we have developed could\nconfirm, by testing the intra-year structure, if indeed the market has returned\nto ``normal'' or if more turbulence is expected ahead. We predict the evolution\nof the indexes one year ahead, which is validated with new data up to Sep.\n2006. The present analysis demonstrates the existence of very significant\nvariations at the local scale, in the sense that the bubble in Las Vegas seems\nto have preceded the more global USA bubble and has ended approximately two\nyears earlier (mid 2004 for Las Vegas compared with mid-2006 for the whole of\nthe USA).\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0589v1"
    },
    {
        "title": "Patterns of dominant flows in the world trade web",
        "authors": [
            "M. Angeles Serrano",
            "Marian Boguna",
            "Alessandro Vespignani"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  The large-scale organization of the world economies is exhibiting\nincreasingly levels of local heterogeneity and global interdependency.\nUnderstanding the relation between local and global features calls for\nanalytical tools able to uncover the global emerging organization of the\ninternational trade network. Here we analyze the world network of bilateral\ntrade imbalances and characterize its overall flux organization, unraveling\nlocal and global high-flux pathways that define the backbone of the trade\nsystem. We develop a general procedure capable to progressively filter out in a\nconsistent and quantitative way the dominant trade channels. This procedure is\ncompletely general and can be applied to any weighted network to detect the\nunderlying structure of transport flows. The trade fluxes properties of the\nworld trade web determines a ranking of trade partnerships that highlights\nglobal interdependencies, providing information not accessible by simple local\nanalysis. The present work provides new quantitative tools for a dynamical\napproach to the propagation of economic crises.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.1225v1"
    },
    {
        "title": "Quantitative relations between corruption and economic factors",
        "authors": [
            "Jia Shao",
            "Plamen Ch. Ivanov",
            "Boris Podobnik",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We report quantitative relations between corruption level and economic\nfactors, such as country wealth and foreign investment per capita, which are\ncharacterized by a power law spanning multiple scales of wealth and investments\nper capita. These relations hold for diverse countries, and also remain stable\nover different time periods. We also observe a negative correlation between\nlevel of corruption and long-term economic growth. We find similar results for\ntwo independent indices of corruption, suggesting that the relation between\ncorruption and wealth does not depend on the specific measure of corruption.\nThe functional relations we report have implications when assessing the\nrelative level of corruption for two countries with comparable wealth, and for\nquantifying the impact of corruption on economic growth and foreign\ninvestments.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.0161v1"
    },
    {
        "title": "The log-normal distribution from Non-Gibrat's law in the middle scale\n  region of profits",
        "authors": [
            "Atushi Ishikawa"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Employing profits data of Japanese firms in 2003--2005, we kinematically\nexhibit the static log-normal distribution in the middle scale region. In the\nderivation, a Non-Gibrat's law under the detailed balance is adopted together\nwith following two approximations. Firstly, the probability density function of\nprofits growth rate is described as a tent-shaped exponential function.\nSecondly, the value of the origin of the growth rate distribution divided into\nbins is constant. The derivation is confirmed in the database consistently.\n  This static procedure is applied to a quasi-static system. We dynamically\ndescribe a quasi-static log-normal distribution in the middle scale region. In\nthe derivation, a Non-Gibrat's law under the detailed quasi-balance is adopted\ntogether with two approximations confirmed in the static system. The resultant\ndistribution is power-law with varying Pareto index in the large scale region\nand the quasi-static log-normal distribution in the middle scale region. In the\ndistribution, not only the change of Pareto index but also the change of the\nvariance of the log-normal distribution depends on the parameter of the\ndetailed quasi-balance. As a result, Pareto index and the variance of the\nlog-normal distribution are related to each other.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.1056v1"
    },
    {
        "title": "The Macro Model of the Inequality Process and The Surging Relative\n  Frequency of Large Wage Incomes",
        "authors": [
            "John Angle"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  This paper presents a model of the dynamics of the wage income distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.3430v1"
    },
    {
        "title": "Utility Maximization with a Stochastic Clock and an Unbounded Random\n  Endowment",
        "authors": [
            "Gordan Zitkovic"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We introduce a linear space of finitely additive measures to treat the\nproblem of optimal expected utility from consumption under a stochastic clock\nand an unbounded random endowment process. In this way we establish existence\nand uniqueness for a large class of utility maximization problems including the\nclassical ones of terminal wealth or consumption, as well as the problems\ndepending on a random time-horizon or multiple consumption instances. As an\nexample we treat explicitly the problem of maximizing the logarithmic utility\nof a consumption stream, where the local time of an Ornstein-Uhlenbeck process\nacts as a stochastic clock.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.4487v1"
    },
    {
        "title": "The public goods game on homogeneous and heterogeneous networks:\n  investment strategy according to the pool size",
        "authors": [
            "Zi-Gang Huang",
            "Zhi-Xi Wu",
            "Jian-Yue Guan",
            "An-Cai Wu",
            "Ying-Hai Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We propose an extended public goods interaction model to study the evolution\nof cooperation in heterogeneous population. The investors are arranged on the\nwell known scale-free type network, the Barab\\'{a}si-Albert model. Each\ninvestor is supposed to preferentially distribute capital to pools in its\nportfolio based on the knowledge of pool sizes. The extent that investors\nprefer larger pools is determined by investment strategy denoted by a tunable\nparameter $\\alpha$, with larger $\\alpha$ corresponding to more preference to\nlarger pools. As comparison, we also study this interaction model on square\nlattice, and find that the heterogeneity contacts favors cooperation.\nAdditionally, the influence of local topology to the game dynamics under\ndifferent $\\alpha$ strategies are discussed. It is found that the system with\nsmaller $\\alpha$ strategy can perform comparatively better than the larger\n$\\alpha$ ones.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.2805v1"
    },
    {
        "title": "Kinetic Exchange Models for Income and Wealth Distributions",
        "authors": [
            "Arnab Chatterjee",
            "Bikas K. Chakrabarti"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Increasingly, a huge amount of statistics have been gathered which clearly\nindicates that income and wealth distributions in various countries or\nsocieties follow a robust pattern, close to the Gibbs distribution of energy in\nan ideal gas in equilibrium. However, it also deviates in the low income and\nmore significantly for the high income ranges. Application of physics models\nprovides illuminating ideas and understanding, complementing the observations.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.1543v2"
    },
    {
        "title": "Statistical properties of agent-based market area model",
        "authors": [
            "Zoltan Kuscsik",
            "Denis Horvath"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  One dimensional stylized model taking into account spatial activity of firms\nwith uniformly distributed customers is proposed. The spatial selling area of\neach firm is defined by a short interval cut out from selling space (large\ninterval). In this representation, the firm size is directly associated with\nthe size of its selling interval.\n  The recursive synchronous dynamics of economic evolution is discussed where\nthe growth rate is proportional to the firm size incremented by the term\nincluding the overlap of the selling area with areas of competing firms. Other\nwords, the overlap of selling areas inherently generate a negative feedback\noriginated from the pattern of demand. Numerical simulations focused on the\nobtaining of the firm size distributions uncovered that the range of free\nparameters where the Pareto's law holds corresponds to the range for which the\npair correlation between the nearest neighbor firms attains its minimum.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.0459v1"
    },
    {
        "title": "Wealth distribution in a System with Wealth-limited Interactions",
        "authors": [
            "Marisciel L. Palima",
            "Eduardo J. David"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We model a closed economic system with interactions that generates the\nfeatures of empirical wealth distribution across all wealth brackets, namely a\nGibbsian trend in the lower and middle wealth range and a Pareto trend in the\nhigher range, by simply limiting the an agents' interaction to only agents with\nnearly the same wealth. To do this, we introduce a parameter BETA that limits\nthe range on the wealth of a partner with which an agent is allowed to\ninteract. We show that this wealth-limited interaction is enough to distribute\nwealth in a purely power law trend. If the interaction is not wealth limited,\nthe wealth distribution is expectedly Gibbsian. The value of BETA where the\ntransition from a purely Gibbsian law to a purely power law distribution\nhappens depends on whether the choice of interaction partner is mutual nor not.\nFor a non-mutual choice, where the richer agent gets to decide, the transition\nhappens at BETA=1.0. For a mutual choice, the transition is at BETA= 0.60. In\norder to generate a mixed Gibbs-Pareto distribution, we apply another\nwealth-based rule that depends on the parameter w_limit. An agent whose wealth\nis below w_limit can choose any partner to interact with, while an agent whose\nwealth is above w_limit is subject to the wealth-limited range in his choice of\npartner. A Gibbs-Pareto distribution appears if both these wealth-based rules\nare applied.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.1014v1"
    },
    {
        "title": "The Grounds For Time Dependent Market Potentials From Dealers' Dynamics",
        "authors": [
            "Kenta Yamada",
            "Hideki Takayasu",
            "Misako Takayasu"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We apply the potential force estimation method to artificial time series of\nmarket price produced by a deterministic dealer model. We find that dealers'\nfeedback of linear prediction of market price based on the latest mean price\nchanges plays the central role in the market's potential force. When markets\nare dominated by dealers with positive feedback the resulting potential force\nis repulsive, while the effect of negative feedback enhances the attractive\npotential force.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.1729v1"
    },
    {
        "title": "Influence of corruption on economic growth rate and foreign investments",
        "authors": [
            "Boris Podobnik",
            "Jia Shao",
            "Djuro Njavro",
            "Plamen Ch. Ivanov",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  In order to investigate whether government regulations against corruption can\naffect the economic growth of a country, we analyze the dependence between\nGross Domestic Product (GDP) per capita growth rates and changes in the\nCorruption Perceptions Index (CPI). For the period 1999-2004 on average for all\ncountries in the world, we find that an increase of CPI by one unit leads to an\nincrease of the annual GDP per capita by 1.7 %. By regressing only European\ntransition countries, we find that $\\Delta$CPI = 1 generates increase of the\nannual GDP per capita by 2.4 %. We also analyze the relation between foreign\ndirect investments received by different countries and CPI, and we find a\nstatistically significant power-law functional dependence between foreign\ndirect investment per capita and the country corruption level measured by the\nCPI. We introduce a new measure to quantify the relative corruption between\ncountries based on their respective wealth as measured by GDP per capita.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.1995v1"
    },
    {
        "title": "Effects of network topology on wealth distributions",
        "authors": [
            "Diego Garlaschelli",
            "Maria I. Loffredo"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We focus on the problem of how wealth is distributed among the units of a\nnetworked economic system. We first review the empirical results documenting\nthat in many economies the wealth distribution is described by a combination of\nlog--normal and power--law behaviours. We then focus on the Bouchaud--M\\'ezard\nmodel of wealth exchange, describing an economy of interacting agents connected\nthrough an exchange network. We report analytical and numerical results showing\nthat the system self--organises towards a stationary state whose associated\nwealth distribution depends crucially on the underlying interaction network. In\nparticular we show that if the network displays a homogeneous density of links,\nthe wealth distribution displays either the log--normal or the power--law form.\nThis means that the first--order topological properties alone (such as the\nscale--free property) are not enough to explain the emergence of the\nempirically observed \\emph{mixed} form of the wealth distribution. In order to\nreproduce this nontrivial pattern, the network has to be heterogeneously\ndivided into regions with variable density of links. We show new results\ndetailing how this effect is related to the higher--order correlation\nproperties of the underlying network. In particular, we analyse assortativity\nby degree and the pairwise wealth correlations, and discuss the effects that\nthese properties have on each other.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.4710v2"
    },
    {
        "title": "An Economic Model of Coupled Exponential Maps",
        "authors": [
            "R. Lopez-Ruiz",
            "J. Gonzalez-Estevez",
            "M. G. Cosenza",
            "J. R. Sanchez"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  In this work, an ensemble of economic interacting agents is considered. The\nagents are arranged in a linear array where only local couplings are allowed.\nThe deterministic dynamics of each agent is given by a map. This map is\nexpressed by two factors. The first one is a linear term that models the\nexpansion of the agent's economy and that is controlled by the {\\it growth\ncapacity parameter}. The second one is an inhibition exponential term that is\nregulated by the {\\it local environmental pressure}. Depending on the parameter\nsetting, the system can display Pareto or Boltzmann-Gibbs behavior in the\nasymptotic dynamical regime. The regions of parameter space where the system\nexhibits one of these two statistical behaviors are delimited. Other properties\nof the system, such as the mean wealth, the standard deviation and the Gini\ncoefficient, are also calculated.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.2684v1"
    },
    {
        "title": "Econophysics: historical perspectives",
        "authors": [
            "G. Daniel",
            "D. Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Econophysics embodies the recent upsurge of interest by physicists into\nfinancial economics, driven by the availability of large amount of data, job\nshortage in physics and the possibility of applying many-body techniques\ndeveloped in statistical and theoretical physics to the understanding of the\nself-organizing economy. This brief historical survey emphasizes that\nEconophysics has many historical precursors, and is in fact rooted in a\ncontinuous cross-fertilization between economics and physics that has been\nactive in the last centuries.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.1416v1"
    },
    {
        "title": "Microscopic Study Reveals the Singular Origins of Growth",
        "authors": [
            "Gur Yaari",
            "Andrzej Nowak",
            "Kamil Rakocy",
            "Sorin Solomon"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  P.W. Anderson proposed the concept of complexity in order to describe the\nemergence and growth of macroscopic collective patterns out of the simple\ninteractions of many microscopic agents. In the physical sciences this paradigm\nwas implemented systematically and confirmed repeatedly by successful\nconfrontation with reality. In the social sciences however, the possibilities\nto stage experiments to validate it are limited. During the 90's a series of\ndramatic political and economic events have provided the opportunity to do so.\nWe exploit the resulting empirical evidence to validate a simple agent based\nalternative to the classical logistic dynamics. The post-liberalization\nempirical data from Poland confirm the theoretical prediction that the dynamics\nis dominated by singular rare events which insure the resilience and\nadaptability of the system. We have shown that growth is led by few singular\n\"growth centers\" (Figure 1), that initially developed at a tremendous rate\n(Figure3), followed by a diffusion process to the rest of the country and\nleading to a positive growth rate uniform across the counties. In addition to\nthe interdisciplinary unifying potential of our generic formal approach, the\npresent work reveals the strong causal ties between the \"softer\" social\nconditions and their \"hard\" economic consequences.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2201v1"
    },
    {
        "title": "Quantitative analysis of privatization",
        "authors": [
            "M. Vahabi",
            "G. R. Jafari"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  In recent years, the economic policy of privatization, which is defined as\nthe transfer of property or responsibility from public sector to private\nsector, is one of the global phenomenon that increases use of markets to\nallocate resources. One important motivation for privatization is to help\ndevelop factor and product markets, as well as security markets. Progress in\nprivatization is correlated with improvements in perceived political and\ninvestment risk. Many emerging countries have gradually reduced their political\nrisks during the course of sustained privatization. In fact, most risk\nresolution seems to take place as privatization proceeds to its later stage.\nAlternative benefits of privatization are improved risk sharing and increased\nliquidity and activity of the market. One of the main methods to develop\nprivatization is entering a new stock to the markets for arising competition.\nHowever, attention to the capability of the markets to accept a new stock is\nsubstantial. Without considering the above statement, it is possible to reduce\nthe market's efficiency. In other words, introduction of a new stock to the\nmarket usually decreases the stage of development and activity and increases\nthe risk. Based on complexity theory, we quantify how the following factors:\nstage of development, activity, risk and investment horizons play roles in the\nprivatization.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2388v1"
    },
    {
        "title": "Modeling wealth distribution in growing markets",
        "authors": [
            "Urna Basu",
            "P. K. Mohanty"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We introduce an auto-regressive model which captures the growing nature of\nrealistic markets. In our model agents do not trade with other agents, they\ninteract indirectly only through a market. Change of their wealth depends,\nlinearly on how much they invest, and stochastically on how much they gain from\nthe noisy market. The average wealth of the market could be fixed or growing.\nWe show that in a market where investment capacity of agents differ, average\nwealth of agents generically follow the Pareto-law. In few cases, the\nindividual distribution of wealth of every agent could also be obtained\nexactly. We also show that the underlying dynamics of other well studied\nkinetic models of markets can be mapped to the dynamics of our auto-regressive\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.3902v2"
    },
    {
        "title": "Mathematical analysis of long tail economy using stochastic ranking\n  processes",
        "authors": [
            "Kumiko Hattori",
            "Tetsuya Hattori"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We present a new method of estimating the distribution of sales rates of,\ne.g., book titles at an online bookstore, from the time evolution of ranking\ndata found at websites of the store. The method is based on new mathematical\nresults on an infinite particle limit of the stochastic ranking process, and is\nsuitable for quantitative studies of the long tail structure of online retails.\nWe give an example of a fit to the actual data obtained from Amazon.co.jp,\nwhich gives the Pareto slope parameter of the distribution of sales rates of\nthe book titles in the store.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.1837v1"
    },
    {
        "title": "The Problem of Modelling of Economic Dynamics in Differential Form",
        "authors": [
            "S. I. Chernyshov",
            "V. S. Ponomarenko",
            "A. V. Voronin"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Traditional models of macroeconomic dynamics are fundamentally incorrect. The\nreason lies in a misunderstanding of peculiarities of the analysis of\ninfinitesimal quantities. However, even those types of solutions that are\nenvisaged by the above-mentioned models are nonrepresentative in the sense of\nthe reflection of realities. It became obvious that the techniques of the\ntheory of linear differential equations were insufficient here. Accordingly,\nthe scientists' attention switched to the theory of nonlinear differential\nequations. At the same time, balance and, accordingly, the model with matrix\nproperties are objectively inherent in the economic system. For the reduction\nof this model to a differential form, there exist rather elementary means that\nproved to be unclaimed. Macroeconomic rhetoric - the power of the accelerator,\na lag on the part of demand, etc. - accompanied by the use of a lot of abstract\ncoefficients prevailed. However, there is no organic interrelation between\nmatrix and nonlinear differential equations. On the contrary, it can be said\nthat linear theory of integral equations originated in matrix analysis. The\nFredholm linear integral equation of the second kind with a parameter-dependent\nkernel proves to be rather representative with regard to the class of possible\nsolutions. It seems that it can be used for the description of any zigzags of\nthe economy. The price one has to pay for this is the nontriviality of existing\ntheory.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.3658v2"
    },
    {
        "title": "Convergence and cluster structures in EU area according to fluctuations\n  in macroeconomic indices",
        "authors": [
            "Mircea Gligor",
            "Marcel Ausloos"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  The cluster analysis methods are used in order to perform a comparative study\nof 15 EU countries in relation with the fluctuations of some basic\nmacroeconomic indicators. The statistical distances between countries are\ncalculated for various moving time windows, and the time variation of the mean\nstatistical distance is investigated. The decreasing of the mean statistical\ndistance between EU countries is reflected in the correlated fluctuations of\nthe basic ME indicators: GDP, GDP/capita, Consumption and Investments. This\nempirical evidence can be seen as an economic aspect of globalization. The\nMoving Average Minimal Length Path (MAMLP) algorithm allows to search for a\ncluster-like structures derived both from the hierarchical organization of\ncountries and from their relative movement inside the hierarchy. It is found\nthat the strongly correlated countries with respect to GDP fluctuations can be\npartitioned into stable clusters. Some of the highly correlated countries, with\nrespect to GDP fluctuations, display strong correlations also in the Final\nConsumption Expenditure, while others are strongly correlated in Gross Capital\nFormation. On the other hand, one notices the similitude of the classifications\nregarding GDP and Net Exports fluctuations as concerns the squared sum of the\ncorrelation coefficients (so called country sensitivity). The final structure\nproves to be robust against the constant size time window moving over the\nscanned time interval. The policy implications of the above empirical results\nconcern the economic clusters arising in the presence of Marshallian\nexternalities and the relationships between trade barriers, R&D incentives and\ngrowth that must be accounted in elaborating a cluster-promotion policy.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.3071v1"
    },
    {
        "title": "The 2006-2008 Oil Bubble and Beyond",
        "authors": [
            "D. Sornette",
            "R. Woodard",
            "W. -X. Zhou"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We present an analysis of oil prices in US$ and in other major currencies\nthat diagnoses unsustainable faster-than-exponential behavior. This supports\nthe hypothesis that the recent oil price run-up has been amplified by\nspeculative behavior of the type found during a bubble-like expansion. We also\nattempt to unravel the information hidden in the oil supply-demand data\nreported by two leading agencies, the US Energy Information Administration\n(EIA) and the International Energy Agency (IEA). We suggest that the found\nincreasing discrepancy between the EIA and IEA figures provides a measure of\nthe estimation errors. Rather than a clear transition to a supply restricted\nregime, we interpret the discrepancy between the IEA and EIA as a signature of\nuncertainty, and there is no better fuel than uncertainty to promote\nspeculation!\n",
        "pdf_link": "http://arxiv.org/pdf/0806.1170v3"
    },
    {
        "title": "Anomalous Returns in a Neural Network Equity-Ranking Predictor",
        "authors": [
            "J. B. Satinover",
            "D. Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Using an artificial neural network (ANN), a fixed universe of approximately\n1500 equities from the Value Line index are rank-ordered by their predicted\nprice changes over the next quarter. Inputs to the network consist only of the\nten prior quarterly percentage changes in price and in earnings for each equity\n(by quarter, not accumulated), converted to a relative rank scaled around zero.\nThirty simulated portfolios are constructed respectively of the 10, 20,..., and\n100 top ranking equities (long portfolios), the 10, 20,..., 100 bottom ranking\nequities (short portfolios) and their hedged sets (long-short portfolios). In a\n29-quarter simulation from the end of the third quarter of 1994 through the\nfourth quarter of 2001 that duplicates real-world trading of the same method\nemployed during 2002, all portfolios are held fixed for one quarter. Results\nare compared to the S&P 500, the Value Line universe itself, trading the\nuniverse of equities using the proprietary ``Value Line Ranking System'' (to\nwhich this method is in some ways similar), and to a Martingale method of\nranking the same equities. The cumulative returns generated by the network\npredictor significantly exceed those generated by the S&P 500, the overall\nuniverse, the Martingale and Value Line prediction methods and are not eroded\nby trading costs. The ANN shows significantly positive Jensen's alpha, i.e.,\nanomalous risk-adjusted expected return. A time series of its global\nperformance shows a clear antipersistence. However, its performance is\nsignificantly better than a simple one-step Martingale predictor, than the\nValue Line system itself and than a simple buy and hold strategy, even when\ntransaction costs are accounted for.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.2606v1"
    },
    {
        "title": "Consequences of increased longevity for wealth, fertility, and\n  population growth",
        "authors": [
            "Aleksandar Bogojevic",
            "Antun Balaz",
            "Rasa Karapandza"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We present, solve and numerically simulate a simple model that describes the\nconsequences of increased longevity on fertility rates, population growth and\nthe distribution of wealth in developed societies. We look at the consequences\nof the repeated use of life extension techniques and show that they represent a\nnovel commodity whose introduction will profoundly influence key aspects of\neconomy and society in general. In particular, we uncover two phases within our\nsimplified model, labeled as 'mortal' and 'immortal'. Within the life extension\nscenario it is possible to have sustainable economic growth in a population of\nstable size, as a result of dynamical equilibrium between the two phases.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.2964v1"
    },
    {
        "title": "The Question of Relaxation in the Wealth Exchange Models",
        "authors": [
            "Abhijit KarGupta"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We look at the meaning of 'relaxation' in the wealth exchange models that are\nrecently proposed in Econophysics to interpret the wealth distributions. To\nquantify and characterise the process of relaxation, we define an appropriate\nquantity and evaluate that numerically for the systems of many agents. Also,\nthe numerical results have been supported heuristically by constructing a\nsimple differential equation.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.3813v1"
    },
    {
        "title": "Cooperation Evolution in Random Multiplicative Environments",
        "authors": [
            "Gur Yaari",
            "Sorin Solomon"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Most real life systems have a random component: the multitude of endogenous\nand exogenous factors influencing them result in stochastic fluctuations of the\nparameters determining their dynamics. These empirical systems are in many\ncases subject to noise of multiplicative nature. The special properties of\nmultiplicative noise as opposed to additive noise have been noticed for a long\nwhile. Even though apparently and formally the difference between free additive\nvs. multiplicative random walks consists in just a move from normal to\nlog-normal distributions, in practice the implications are much more far\nreaching. While in an additive context the emergence and survival of\ncooperation requires special conditions (especially some level of reward,\npunishment, reciprocity), we find that in the multiplicative random context the\nemergence of cooperation is much more natural and effective. We study the\nvarious implications of this observation and its applications in various\ncontexts.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1823v3"
    },
    {
        "title": "Theory of Zipf's Law and of General Power Law Distributions with\n  Gibrat's law of Proportional Growth",
        "authors": [
            "A. Saichev",
            "Y. Malevergne",
            "D. Sornette"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We summarize a book under publication with his title written by the three\npresent authors, on the theory of Zipf's law, and more generally of power laws,\ndriven by the mechanism of proportional growth. The preprint is available upon\nrequest from the authors.\n  For clarity, consistence of language and conciseness, we discuss the origin\nand conditions of the validity of Zipf's law using the terminology of firms'\nasset values. We use firms at the entities whose size distributions are to be\nexplained. It should be noted, however, that most of the relations discussed in\nthis book, especially the intimate connection between Zipf's and Gilbrat's\nlaws, underlie Zipf's law in diverse scientific areas. The same models and\nvariations thereof can be straightforwardly applied to any of the other domains\nof application.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.1828v1"
    },
    {
        "title": "Outflow Dynamics in Modeling Oligopoly Markets: The Case of the Mobile\n  Telecommunications Market in Poland",
        "authors": [
            "Katarzyna Sznajd-Weron",
            "Rafał Weron",
            "Maja Włoszczowska"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  In this paper we introduce two models of opinion dynamics in oligopoly\nmarkets and apply them to a situation, where a new entrant challenges two\nincumbents of the same size. The models differ in the way the two forces\ninfluencing consumer choice -- (local) social interactions and (global)\nadvertising -- interact. We study the general behavior of the models using the\nMean Field Approach and Monte Carlo simulations and calibrate the models to\ndata from the Polish telecommunications market. For one of the models\ncriticality is observed -- below a certain critical level of advertising the\nmarket approaches a lock-in situation, where one market leader dominates the\nmarket and all other brands disappear. Interestingly, for both models the best\nfits to real data are obtained for conformity level $p \\in (0.3,0.4)$. This\nagrees very well with the conformity level found by Solomon Asch in his famous\nsocial experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.1534v1"
    },
    {
        "title": "Labour Productivity Superstatistics",
        "authors": [
            "Hideaki Aoyama",
            "Hiroshi Yoshikawa",
            "Hiroshi Iyetomi",
            "Yoshi Fujiwara"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We discuss superstatistics theory of labour productivity. Productivity\ndistribution across workers, firms and industrial sectors are studied\nempirically and found to obey power-distributions, in sharp contrast to the\nequilibrium theories of mainstream economics. The Pareto index is found to\ndecrease with the level of aggregation, {\\it i.e.}, from workers to firms and\nto industrial sectors. In order to explain these phenomenological laws, we\npropose a superstatistics framework, where the role of the fluctuating\ntemperature is played by the fluctuating demand.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.3541v1"
    },
    {
        "title": "Transition from Pareto to Boltzmann-Gibbs behavior in a deterministic\n  economic model",
        "authors": [
            "J. Gonzalez-Estevez",
            "M. G. Cosenza",
            "O. Alvarez-Llamoza",
            "R. Lopez-Ruiz"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  The one-dimensional deterministic economic model recently studied by\nGonzalez-Estevez et al. [Physica A 387, 4367 (2008)] is considered on a\ntwo-dimensional square lattice with periodic boundary conditions. In this\nmodel, the evolution of each agent is described by a map coupled with its\nnearest neighbors. The map has two factors: a linear term that accounts for the\nagent's own tendency to grow and an exponential term that saturates this growth\nthrough the control effect of the environment. The regions in the parameter\nspace where the system displays Pareto and Boltzmann-Gibbs statistics are\ncalculated for the cases of von Neumann and of Moore's neighborhoods. It is\nfound that, even when the parameters in the system are kept fixed, a transition\nfrom Pareto to Boltzmann-Gibbs behavior can occur when the number of neighbors\nof each agent increases.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.1064v1"
    },
    {
        "title": "International Comparison of Labor Productivity Distribution for\n  Manufacturing and Non-Manufacturing Firms",
        "authors": [
            "Yuichi Ikeda",
            "Wataru Souma"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  Labor productivity was studied at the microscopic level in terms of\ndistributions based on individual firm financial data from Japan and the US. A\npower-law distribution in terms of firms and sector productivity was found in\nboth countries' data. The labor productivities were not equal for nation and\nsectors, in contrast to the prevailing view in the field of economics. It was\nfound that the low productivity of the Japanese non-manufacturing sector\nreported in macro-economic studies was due to the low productivity of small\nfirms.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.0208v4"
    },
    {
        "title": "Superstatistics of Labour Productivity in Manufacturing and\n  Nonmanufacturing Sectors",
        "authors": [
            "Hideaki Aoyama",
            "Yoshi Fujiwara",
            "Yuichi Ikeda",
            "Hiroshi Iyetomi",
            "Wataru Souma"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  Labour productivity distribution (dispersion) is studied both theoretically\nand empirically. Superstatistics is presented as a natural theoretical\nframework for productivity. The demand index $\\kappa$ is proposed within this\nframework as a new business index. Japanese productivity data covering\nsmall-to-medium to large firms from 1996 to 2006 is analyzed and the power-law\nfor both firms and workers is established. The demand index $\\kappa$ is\nevaluated in the manufacturing sector. A new discovery is reported for the\nnonmanufacturing (service) sector, which calls for expansion of the\nsuperstatistics framework to negative temperature range.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.1500v1"
    },
    {
        "title": "Agent-Based Model Approach to Complex Phenomena in Real Economy",
        "authors": [
            "Hiroshi Iyetomi",
            "Hideaki Aoyama",
            "Yoshi Fujiwara",
            "Yuichi Ikeda",
            "Wataru Souma"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  An agent-based model for firms' dynamics is developed. The model consists of\nfirm agents with identical characteristic parameters and a bank agent. Dynamics\nof those agents is described by their balance sheets. Each firm tries to\nmaximize its expected profit with possible risks in market. Infinite growth of\na firm directed by the \"profit maximization\" principle is suppressed by a\nconcept of \"going concern\". Possibility of bankruptcy of firms is also\nintroduced by incorporating a retardation effect of information on firms'\ndecision. The firms, mutually interacting through the monopolistic bank, become\nheterogeneous in the course of temporal evolution. Statistical properties of\nfirms' dynamics obtained by simulations based on the model are discussed in\nlight of observations in the real economy.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.1794v1"
    },
    {
        "title": "Information geometries and Microeconomic Theories",
        "authors": [
            "Richard Nock",
            "Brice Magdalou",
            "Nicolas Sanz",
            "Eric Briys",
            "Fred Celimene",
            "Frank Nielsen"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  More than thirty years ago, Charnes, Cooper and Schinnar (1976) established\nan enlightening contact between economic production functions (EPFs) -- a\ncornerstone of neoclassical economics -- and information theory, showing how a\ngeneralization of the Cobb-Douglas production function encodes homogeneous\nfunctions.\n  As expected by Charnes \\textit{et al.}, the contact turns out to be much\nbroader: we show how information geometry as pioneered by Amari and others\nunderpins static and dynamic descriptions of microeconomic cornerstones.\n  We show that the most popular EPFs are fundamentally grounded in a very weak\naxiomatization of economic transition costs between inputs. The strength of\nthis characterization is surprising, as it geometrically bonds altogether a\nwealth of collateral economic notions\n  -- advocating for applications in various economic fields --: among all, it\ncharacterizes (i) Marshallian and Hicksian demands and their geometric duality,\n(ii) Slutsky-type properties for the transformation paths, (iii) Roy-type\nproperties for their elementary variations.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.2586v1"
    },
    {
        "title": "Doves and hawks in economics revisited. An evolutionary quantum game\n  theory-based analysis of financial crises",
        "authors": [
            "Matthias Hanauske",
            "Jennifer Kunz",
            "Steffen Bernius",
            "Wolfgang König"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  The last financial and economic crisis demonstrated the dysfunctional\nlong-term effects of aggressive behaviour in financial markets. Yet,\nevolutionary game theory predicts that under the condition of strategic\ndependence a certain degree of aggressive behaviour remains within a given\npopulation of agents. However, as the consequences of the financial crisis\nexhibit, it would be desirable to change the 'rules of the game' in a way that\nprevents the occurrence of any aggressive behaviour and thereby also the danger\nof market crashes. The paper picks up this aspect. Through the extension of the\nin literature well-known Hawk-Dove game by a quantum approach, we can show that\ndependent on entanglement, also evolutionary stable strategies can emerge,\nwhich are not predicted by classical evolutionary game theory and where the\ntotal economic population uses a non aggressive quantum strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.2113v1"
    },
    {
        "title": "Money Distributions in Chaotic Economies",
        "authors": [
            "Carmen Pellicer-Lostao",
            "Ricardo Lopez-Ruiz"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This paper considers the ideal gas-like model of trading markets, where each\nindividual is identified as a gas molecule that interacts with others trading\nin elastic or money-conservative collisions. Traditionally this model\nintroduces different rules of random selection and exchange between pair\nagents. Real economic transactions are complex but obviously non-random.\nConsequently, unlike this traditional model, this work implements chaotic\nelements in the evolution of an economic system. In particular, we use a\nchaotic signal that breaks the natural pairing symmetry\n$(i,j)\\Leftrightarrow(j,i)$ of a random gas-like model. As a result of that, it\nis found that a chaotic market like this can reproduce the referenced wealth\ndistributions observed in real economies (the Gamma, Exponential and Pareto\ndistributions).\n",
        "pdf_link": "http://arxiv.org/pdf/0906.1899v1"
    },
    {
        "title": "The Structure and Growth of Weighted Networks",
        "authors": [
            "Massimo Riccaboni",
            "Stefano Schiavo"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We develop a simple theoretical framework for the evolution of weighted\nnetworks that is consistent with a number of stylized features of real-world\ndata. In our framework, the Barabasi-Albert model of network evolution is\nextended by assuming that link weights evolve according to a geometric Brownian\nmotion. Our model is verified by means of simulations and real world trade\ndata. We show that the model correctly predicts the intensity and growth\ndistribution of links, the size-variance relationships of the growth of link\nweights, the relationship between the degree and strength of nodes, as well as\nthe scale-free structure of the network.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.0348v3"
    },
    {
        "title": "Multinetwork of international trade: A commodity-specific analysis",
        "authors": [
            "Matteo Barigozzi",
            "Giorgio Fagiolo",
            "Diego Garlaschelli"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We study the topological properties of the multinetwork of commodity-specific\ntrade relations among world countries over the 1992-2003 period, comparing them\nwith those of the aggregate-trade network, known in the literature as the\ninternational-trade network (ITN). We show that link-weight distributions of\ncommodity-specific networks are extremely heterogeneous and (quasi) log\nnormality of aggregate link-weight distribution is generated as a sheer outcome\nof aggregation. Commodity-specific networks also display average connectivity,\nclustering, and centrality levels very different from their aggregate\ncounterpart. We also find that ITN complete connectivity is mainly achieved\nthrough the presence of many weak links that keep commodity-specific networks\ntogether and that the correlation structure existing between topological\nstatistics within each single network is fairly robust and mimics that of the\naggregate network. Finally, we employ cross-commodity correlations between link\nweights to build hierarchies of commodities. Our results suggest that on the\ntop of a relatively time-invariant ``intrinsic\" taxonomy (based on inherent\nbetween-commodity similarities), the roles played by different commodities in\nthe ITN have become more and more dissimilar, possibly as the result of an\nincreased trade specialization. Our approach is general and can be used to\ncharacterize any multinetwork emerging as a nontrivial aggregation of several\ninterdependent layers.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.1879v4"
    },
    {
        "title": "The International-Trade Network: Gravity Equations and Topological\n  Properties",
        "authors": [
            "Giorgio Fagiolo"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This paper begins to explore the determinants of the topological properties\nof the international - trade network (ITN). We fit bilateral-trade flows using\na standard gravity equation to build a \"residual\" ITN where trade-link weights\nare depurated from geographical distance, size, border effects, trade\nagreements, and so on. We then compare the topological properties of the\noriginal and residual ITNs. We find that the residual ITN displays, unlike the\noriginal one, marked signatures of a complex system, and is characterized by a\nvery different topological architecture. Whereas the original ITN is\ngeographically clustered and organized around a few large-sized hubs, the\nresidual ITN displays many small-sized but trade-oriented countries that,\nindependently of their geographical position, either play the role of local\nhubs or attract large and rich countries in relatively complex\ntrade-interaction patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.2086v1"
    },
    {
        "title": "Econophysics: Empirical facts and agent-based models",
        "authors": [
            "Anirban Chakraborti",
            "Ioane Muni Toke",
            "Marco Patriarca",
            "Frederic Abergel"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This article aims at reviewing recent empirical and theoretical developments\nusually grouped under the term Econophysics. Since its name was coined in 1995\nby merging the words Economics and Physics, this new interdisciplinary field\nhas grown in various directions: theoretical macroeconomics (wealth\ndistributions), microstructure of financial markets (order book modelling),\neconometrics of financial bubbles and crashes, etc. In the first part of the\nreview, we discuss on the emergence of Econophysics. Then we present empirical\nstudies revealing statistical properties of financial time series. We begin the\npresentation with the widely acknowledged stylized facts which describe the\nreturns of financial assets- fat tails, volatility clustering, autocorrelation,\netc.- and recall that some of these properties are directly linked to the way\ntime is taken into account. We continue with the statistical properties\nobserved on order books in financial markets. For the sake of illustrating this\nreview, (nearly) all the stated facts are reproduced using our own\nhigh-frequency financial database. Finally, contributions to the study of\ncorrelations of assets such as random matrix theory and graph theory are\npresented. In the second part of the review, we deal with models in\nEconophysics through the point of view of agent-based modelling. Amongst a\nlarge number of multi-agent-based models, we have identified three\nrepresentative areas. First, using previous work originally presented in the\nfields of behavioural finance and market microstructure theory, econophysicists\nhave developed agent-based models of order-driven markets that are extensively\npresented here. Second, kinetic theory models designed to explain some\nempirical facts on wealth distribution are reviewed. Third, we briefly\nsummarize game theory models by reviewing the now classic minority game and\nrelated problems.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.1974v2"
    },
    {
        "title": "Tremor price dynamics in the world's network of stock exchanges",
        "authors": [
            "Jorgen Vitting Andersen",
            "Andrzej Nowak",
            "Giulia Rotundo",
            "Lael Parrott"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  We use insight from a model of earth tectonic plate movement to obtain a new\nunderstanding of the build up and release of stress in the price dynamics of\nthe worlds stock exchanges. Nonlinearity enters the model due to a behavioral\nattribute of humans reacting disproportionately to big changes. This nonlinear\nresponse allows us to classify price movements of a given stock index as either\nbeing generated due to specific economic news for the country in question, or\nby the ensemble of the worlds stock exchanges reacting together like a complex\nsystem. Similar in structure to the Capital Asset Pricing Model in Finance, the\nmodel predicts how an individual stock exchange should be priced in terms of\nthe performance of the global market of exchanges, but with human behavioral\ncharacteristics included in the pricing. A number of the models assumptions are\nvalidated against empirical data for 24 of the worlds leading stock exchanges.\nWe show how treshold effects can lead to synchronization in the global network\nof stock exchanges.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.3771v1"
    },
    {
        "title": "Consumer Expenditure Distribution in India, 1983-2007: Evidence of a\n  Long Pareto Tail",
        "authors": [
            "Abhik Ghosh",
            "Kausik Gangopadhyay",
            "B. Basu"
        ],
        "category": "q-fin.GN",
        "published_year": "2009",
        "summary": "  This work presents an empirical study of the evolution of the consumer\nexpenditure distribution in India during 1982-2007. We have used the National\nSample Survey Organization data and analysed the expenditure distribution for\nthe urban and rural sectors. It is found that this distribution is a mixture of\ntwo distributions, more particularly, it follows a lognormal in the lower tail\nand a Pareto distribution in the higher end. The Pareto tail consists of a\nremarkable 30-40% of the population in the upper end and the lower end is\nsuitably modeled by the lognormal one. The goodness-of-fit tests endorse the\nproposed distribution. Moreover, the Pareto tail is widening over time for the\nrural sector. The Gini coefficient, a prominent measure for inequality, for the\nexpenditure distribution is found to be stable for the entire time span.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.5420v3"
    },
    {
        "title": "A Spectral Analysis of Business Cycle Patterns in UK Sectoral Output",
        "authors": [
            "Peijie Wang",
            "Trefor Jones"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This paper studies business cycle patterns in UK sectoral output. It analyzes\nthe distinction between white noise processes and their non-white noise\ncounterparts in the frequency domain and further examines the associated\nfeatures and patterns for the process where white noise conditions are\nviolated. The characteristics of these sectors, arising from their\ninstitutional features that may influence business cycles behavior and\npatterns, are discussed. The study then investigates the output of UK GDP\nsectors empirically, revealing their similarities and differences in their\nbusiness cycle patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4762v1"
    },
    {
        "title": "The Hazards of Propping Up: Bubbles and Chaos",
        "authors": [
            "Philip Maymin"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  In the current environment of financial distress, many governments are likely\nto soon become major holders of financial assets, but the policy debate focuses\nonly on the likelihood and extent of short-term market stabilization. This\npaper shows that government intervention and propping up are likely to lead to\nlong-term bubbles and even wildly chaotic behavior. The discontinuities occur\nwhen the committed capital reaches a critical amount that depends on just two\nparameters: the market impact of trading and the target exposure percentage.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2282v1"
    },
    {
        "title": "Scale invariant properties of public debt growth",
        "authors": [
            "Alexander M. Petersen",
            "Boris Podobnik",
            "Davor Horvatic",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Public debt is one of the important economic variables that quantitatively\ndescribes a nation's economy. Because bankruptcy is a risk faced even by\ninstitutions as large as governments (e.g. Iceland), national debt should be\nstrictly controlled with respect to national wealth. Also, the problem of\neliminating extreme poverty in the world is closely connected to the study of\nextremely poor debtor nations. We analyze the time evolution of national public\ndebt and find \"convergence\": initially less-indebted countries increase their\ndebt more quickly than initially more-indebted countries. We also analyze the\npublic debt-to-GDP ratio R, a proxy for default risk, and approximate the\nprobability density function P(R) with a Gamma distribution, which can be used\nto establish thresholds for sustainable debt. We also observe \"convergence\" in\nR: countries with initially small R increase their R more quickly than\ncountries with initially large R. The scaling relationships for debt and R have\npractical applications, e.g. the Maastricht Treaty requires members of the\nEuropean Monetary Union to maintain R < 0.6.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2491v1"
    },
    {
        "title": "Explicit equilibria in a kinetic model of gambling",
        "authors": [
            "Federico Bassetti",
            "Giuseppe Toscani"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We introduce and discuss a nonlinear kinetic equation of Boltzmann type which\ndescribes the evolution of wealth in a pure gambling process, where the entire\nsum of wealths of two agents is up for gambling, and randomly shared between\nthe agents. For this equation the analytical form of the steady states is found\nfor various realizations of the random fraction of the sum which is shared to\nthe agents. Among others, Gibbs distribution appears as steady state in case of\na uniformly distributed random fraction, while Gamma distribution appears for a\nrandom fraction which is Beta distributed. The case in which the gambling game\nis only conservative-in-the-mean is shown to lead to an explicit heavy tailed\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.3689v1"
    },
    {
        "title": "Is It Real, or Is It Randomized?: A Financial Turing Test",
        "authors": [
            "Jasmina Hasanhodzic",
            "Andrew W. Lo",
            "Emanuele Viola"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We construct a financial \"Turing test\" to determine whether human subjects\ncan differentiate between actual vs. randomized financial returns. The\nexperiment consists of an online video-game (http://arora.ccs.neu.edu) where\nplayers are challenged to distinguish actual financial market returns from\nrandom temporal permutations of those returns. We find overwhelming statistical\nevidence (p-values no greater than 0.5%) that subjects can consistently\ndistinguish between the two types of time series, thereby refuting the\nwidespread belief that financial markets \"look random.\" A key feature of the\nexperiment is that subjects are given immediate feedback regarding the validity\nof their choices, allowing them to learn and adapt. We suggest that such novel\ninterfaces can harness human capabilities to process and extract information\nfrom financial data in ways that computers cannot.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.4592v1"
    },
    {
        "title": "The individual income distribution in Argentina in the period 2000-2009.\n  A unique source of non stationary data",
        "authors": [
            "Juan C. Ferrero"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The economic crisis in Argentina around year 2002 provides a unique\nopportunity for Econophysics studies. The available data on individual income\nare analyzed to show that they correspond to non stationary states. However,\nthe rather restricted size of the data survey imposes difficulties that must be\novercome through a careful analysis, for a reliable use. A new method of data\ntreatment is presented that could be helpful in theoretical studies.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.2057v1"
    },
    {
        "title": "Fairness Is an Emergent Self-Organized Property of the Free Market for\n  Labor",
        "authors": [
            "Venkat Venkatasubramanian"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  The excessive compensation packages of CEOs of U.S. corporations in recent\nyears have brought to the foreground the issue of fairness in economics. The\nconventional wisdom is that the free market for labor, which determines the pay\npackages, cares only about efficiency and not fairness. We present an\nalternative theory that shows that an ideal free market environment also\npromotes fairness, as an emergent property resulting from the self-organizing\nmarket dynamics. Even though an individual employee may care only about his or\nher salary and no one else's, the collective actions of all the employees,\ncombined with the profit maximizing actions of all the companies, in a free\nmarket environment under budgetary constraints, lead towards a more fair\nallocation of wages, guided by Adam Smith's invisible hand of\nself-organization. By exploring deep connections with statistical\nthermodynamics, we show that entropy is the appropriate measure of fairness in\na free market environment which is maximized at equilibrium to yield the\nlognormal distribution of salaries as the fairest inequality of pay in an\norganization under ideal conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.4382v1"
    },
    {
        "title": "Modeling total expenditure on warranty claims",
        "authors": [
            "Abhimanyu Mitra",
            "Sidney I. Resnick"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We approximate the distribution of total expenditure of a retail company over\nwarranty claims incurred in a fixed period [0, T], say the following quarter.\nWe consider two kinds of warranty policies, namely, the non-renewing free\nreplacement warranty policy and the non-renewing pro-rata warranty policy. Our\napproximation holds under modest assumptions on the distribution of the sales\nprocess of the warranted item and the nature of arrivals of warranty claims. We\npropose a method of using historical data to statistically estimate the\nparameters of the approximate distribution. Our methodology is applied to the\nwarranty claims data from a large car manufacturer for a single car model and\nmodel year.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.1032v1"
    },
    {
        "title": "Exponential wealth distribution in different discrete economic models",
        "authors": [
            "Ricardo Lopez-Ruiz"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Exponential distribution is ubiquitous in the framework of multi-agent\nsystems. An alternative approach with an economic motivation to derive the\nexponential distribution in the framework of iterations in the space of\ndistributions is disclosed.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.3550v1"
    },
    {
        "title": "Complex Networks and Symmetry II: Reciprocity and Evolution of World\n  Trade",
        "authors": [
            "Franco Ruzzenenti",
            "Diego Garlaschelli",
            "Riccardo Basosi"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We exploit the symmetry concepts developed in the companion review of this\narticle to introduce a stochastic version of link reversal symmetry, which\nleads to an improved understanding of the reciprocity of directed networks. We\napply our formalism to the international trade network and show that a strong\nembedding in economic space determines particular symmetries of the network,\nwhile the observed evolution of reciprocity is consistent with a symmetry\nbreaking taking place in production space. Our results show that networks can\nbe strongly affected by symmetry-breaking phenomena occurring in embedding\nspaces, and that stochastic network symmetries can successfully suggest, or\nrule out, possible underlying mechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.4489v1"
    },
    {
        "title": "Self-organized criticality in a network of economic agents with finite\n  consumption",
        "authors": [
            "João P. da Cruz",
            "Pedro G. Lind"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We introduce a simple model for addressing the controversy in the study of\nfinancial systems, sometimes taken as brownian-like processes and other as\ncritical systems with fluctuations of arbitrary magnitude. The model considers\na collection of economical agents which establish trade connections among them\naccording to basic economical principles properly translated into physical\nproperties and interaction. With our model we are able to reproduce the\nevolution of macroscopic quantities (indices) and to correctly retrieve the\ncommon exponent value characterizing several indices in financial markets,\nrelating it to the underlying topology of connections.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.5830v2"
    },
    {
        "title": "How sensitive are equilibrium pricing models to real-world distortions?",
        "authors": [
            "Harbir Lamba"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  In both finance and economics, quantitative models are usually studied as\nisolated mathematical objects --- most often defined by very strong simplifying\nassumptions concerning rationality, efficiency and the existence of\ndisequilibrium adjustment mechanisms. This raises the important question of how\nsensitive such models might be to real-world effects that violate the\nassumptions. We show how the consequences of rational behavior caused by\nperverse incentives, as well as various irrational tendencies identified by\nbehavioral economists, can be systematically and consistently introduced into\nan agent-based model for a financial asset. This generates a class of models\nwhich, in the special case where such effects are absent, reduces to geometric\nBrownian motion --- the usual equilibrium pricing model. Thus we are able to\nnumerically perturb a widely-used equilibrium pricing model market and\ninvestigate its stability. The magnitude of such perturbations in real markets\ncan be estimated and the simulations imply that this is far outside the\nstability region of the equilibrium solution, which is no longer observed.\nIndeed the price fluctuations generated by endogenous dynamics, are in good\ngeneral agreement with the excess kurtosis and heteroskedasticity of actual\nasset prices. The methodology is presented within the context of a financial\nmarket. However, there are close links to concepts and theories from both\nmicro- and macro-economics including rational expectations, Soros' theory of\nreflexivity, and Minsky's theory of financial instability.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.0027v1"
    },
    {
        "title": "Equilibrium distributions and relaxation times in gas-like economic\n  models: an analytical derivation",
        "authors": [
            "Xavier Calbet",
            "Jose-Luis Lopez",
            "Ricardo Lopez-Ruiz"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  A step by step procedure to derive analytically the exact dynamical evolution\nequations of the probability density functions (PDF) of well known kinetic\nwealth exchange economic models is shown. This technique gives a dynamical\ninsight into the evolution of the PDF, e.g., allowing the calculation of its\nrelaxation times. Their equilibrium PDFs can also be calculated by finding its\nstationary solutions. This gives as a result an integro-differential equation,\nwhich can be solved analytically in some cases and numerically in others. This\nshould provide some guidance into the type of probability density functions\nthat can be derived from particular economic agent exchange rules, or for that\nmatter, any other kinetic model of gases with particular collision physics.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.0208v2"
    },
    {
        "title": "Structure and Response in the World Trade Network",
        "authors": [
            "Jiankui He",
            "Michael W. Deem"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  We examine how the structure of the world trade network has been shaped by\nglobalization and recessions over the last 40 years. We show that by treating\nthe world trade network as an evolving system, theory predicts the trade\nnetwork is more sensitive to evolutionary shocks and recovers more slowly from\nthem now than it did 40 years ago, due to structural changes in the world trade\nnetwork induced by globalization. We also show that recession-induced change to\nthe world trade network leads to an \\emph{increased} hierarchical structure of\nthe global trade network for a few years after the recession.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.0410v1"
    },
    {
        "title": "The Gompertz-Pareto Income Distribution",
        "authors": [
            "F. Chami Figueira",
            "N. J. Moura Jr",
            "Marcelo B. Ribeiro"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  This work analyzes the Gompertz-Pareto distribution (GPD) of personal income,\nformed by the combination of the Gompertz curve, representing the overwhelming\nmajority of the economically less favorable part of the population of a\ncountry, and the Pareto power law, which describes its tiny richest part.\nEquations for the Lorenz curve, Gini coefficient and the percentage share of\nthe Gompertzian part relative to the total income are all written in this\ndistribution. We show that only three parameters, determined by linear data\nfitting, are required for its complete characterization. Consistency checks are\ncarried out using income data of Brazil from 1981 to 2007 and they lead to the\nconclusion that the GPD is consistent and provides a coherent and simple\nanalytical tool to describe personal income distribution data.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.1994v1"
    },
    {
        "title": "The additive property of the inconsistency degree in intertemporal\n  decision making through the generalization of psychophysical laws",
        "authors": [
            "Natalia Destefano",
            "Alexandre Souto Martinez"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  Intertemporal decision making involves choices among options whose effects\noccur at different moments. These choices are influenced not only by the effect\nof rewards value perception at different moments, but also by the time\nperception effect. One of the main difficulties that affect standard\nexperiments involving intertemporal choices is the simultaneity of both effects\non time discounting. In this paper, we unify the psycophysical laws and\ndiscount value functions using the one-parameter exponential and logaritmic\nfunctions from nonextensive statistical mechanics. Also, we propose to measure\nthe degree of inconsistency. This quantity allow us to discriminate both\neffects of time and value perception on discounting process and, by\nintegration, obtain other main quantities like impulsivity and discount\nfunctions.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.5648v2"
    },
    {
        "title": "A Mispricing Model of Stocks Under Asymmetric Information",
        "authors": [
            "Winston Buckley",
            "Garfield Brown",
            "Mario Marshall"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We extend the theory of asymmetric information in mispricing models for\nstocks following geometric Brownian motion to constant relative risk averse\ninvestors. Mispricing follows a continuous mean--reverting Ornstein--Uhlenbeck\nprocess. Optimal portfolios and maximum expected log--linear utilities from\nterminal wealth for informed and uninformed investors are derived. We obtain\nanalogous but more general results which nests those of Guasoni (2006) as a\nspecial case of the relative risk aversion approaching one.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.1148v1"
    },
    {
        "title": "Adelic theory of stock market",
        "authors": [
            "V. Zharkov"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The p-adic theory of the stock market is presented. It is shown that the\nprice dynamics is very naturally described by the adelic function. The\nprocedure of derivation of the functional integral formulation of adelic type\nis derived from microscopic models using generalized supercoherent states.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.2515v1"
    },
    {
        "title": "Google matrix of the world trade network",
        "authors": [
            "Leonardo Ermann",
            "Dima L. Shepelyansky"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Using the United Nations Commodity Trade Statistics Database\n[http://comtrade.un.org/db/] we construct the Google matrix of the world trade\nnetwork and analyze its properties for various trade commodities for all\ncountries and all available years from 1962 to 2009. The trade flows on this\nnetwork are classified with the help of PageRank and CheiRank algorithms\ndeveloped for the World Wide Web and other large scale directed networks. For\nthe world trade this ranking treats all countries on equal democratic grounds\nindependent of country richness. Still this method puts at the top a group of\nindustrially developed countries for trade in {\\it all commodities}. Our study\nestablishes the existence of two solid state like domains of rich and poor\ncountries which remain stable in time, while the majority of countries are\nshown to be in a gas like phase with strong rank fluctuations. A simple random\nmatrix model provides a good description of statistical distribution of\ncountries in two-dimensional rank plane. The comparison with usual ranking by\nexport and import highlights new features and possibilities of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5027v1"
    },
    {
        "title": "Exponential wealth distribution in a random market. A rigorous\n  explanation",
        "authors": [
            "Jose-Luis Lopez",
            "Ricardo Lopez-Ruiz",
            "Xavier Calbet"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  In simulations of some economic gas-like models, the asymptotic regime shows\nan exponential wealth distribution, independently of the initial wealth\ndistribution given to the system. The appearance of this statistical\nequilibrium for this type of gas-like models is explained in a rigorous\nanalytical way.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5703v3"
    },
    {
        "title": "A Generalized Continuous Model for Random Markets",
        "authors": [
            "R. Lopez-Ruiz",
            "E. Shivanian",
            "S. Abbasbandy",
            "J. L. Lopez"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  A generalized continuous economic model is proposed for random markets. In\nthis model, agents interact by pairs and exchange their money in a random way.\nA parameter controls the effectiveness of the transactions between the agents.\nWe show in a rigorous way that this type of markets reach their asymptotic\nequilibrium on the exponential wealth distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2187v3"
    },
    {
        "title": "Statistical mechanics of the international trade network",
        "authors": [
            "Agata Fronczak",
            "Piotr Fronczak"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Analyzing real data on international trade covering the time interval\n1950-2000, we show that in each year over the analyzed period the network is a\ntypical representative of the ensemble of maximally random weighted networks,\nwhose directed connections (bilateral trade volumes) are only characterized by\nthe product of the trading countries' GDPs. It means that time evolution of\nthis network may be considered as a continuous sequence of equilibrium states,\ni.e. quasi-static process. This, in turn, allows one to apply the linear\nresponse theory to make (and also verify) simple predictions about the network.\nIn particular, we show that bilateral trade fulfills fluctuation-response\ntheorem, which states that the average relative change in import (export)\nbetween two countries is a sum of relative changes in their GDPs. Yearly\nchanges in trade volumes prove that the theorem is valid.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2606v2"
    },
    {
        "title": "Robustness and Contagion in the International Financial Network",
        "authors": [
            "Tilman Dette",
            "Scott Pauls",
            "Daniel N. Rockmore"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The recent financial crisis of 2008 and the 2011 indebtedness of Greece\nhighlight the importance of understanding the structure of the global financial\nnetwork. In this paper we set out to analyze and characterize this network, as\ncaptured by the IMF Coordinated Portfolio Investment Survey (CPIS), in two\nways. First, through an adaptation of the \"error and attack\" methodology [1],\nwe show that the network is of the \"robust-yet-fragile\" type, a topology found\nin a wide variety of evolved networks. We compare these results against four\ncommon null-models, generated only from first-order statistics of the empirical\ndata. In addition, we suggest a fifth, log-normal model, which generates\nnetworks that seem to match the empirical one more closely. Still, this model\ndoes not account for several higher order network statistics, which reenforces\nthe added value of the higher-order analysis. Second, using loss-given-default\ndynamics [2], we model financial interdependence and potential cascading of\nfinancial distress through the network. Preliminary simulations indicate that\ndefault by a single relatively small country like Greece can be absorbed by the\nnetwork, but that default in combination with defaults of other PIGS countries\n(Portugal, Ireland, and Spain) could lead to a massive extinction cascade in\nthe global economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.4249v2"
    },
    {
        "title": "Hierarchical structure in phonographic market",
        "authors": [
            "Andrzej Buda"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  I find a topological arrangement of assets traded in a phonographic market\nwhich has associated a meaningful economic taxonomy. I continue using the\nMinimal Spanning Tree and the Life-time Of Correlations between assets, but now\noutside the stock markets. This is the first attempt to use these methods on\nphonographic market where we have artists instead of stocks. The value of an\nartist is defined by record sales. The graph is obtained starting from the\nmatrix of correlations coefficient computed between the world's most popular 30\nartists by considering the synchronous time evolution of the difference of the\nlogarithm of weekly record sales. This method provides the hierarchical\nstructure of phonographic market and information on which music genre is\nmeaningful according to customers.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.6265v1"
    },
    {
        "title": "Spontaneous symmetry breaking of arbitrage",
        "authors": [
            "Jaehyung Choi"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We introduce the concept of spontaneous symmetry breaking to arbitrage\nmodeling. In the model, the arbitrage strategy is considered as being in the\nsymmetry breaking phase and the phase transition between arbitrage mode and\nno-arbitrage mode is triggered by a control parameter. We estimate the control\nparameter for momentum strategy with real historical data. The momentum\nstrategy aided by symmetry breaking shows stronger performance and has a better\nrisk measure than the naive momentum strategy in U.S. and South Korean markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.5122v3"
    },
    {
        "title": "The network of global corporate control",
        "authors": [
            "Stefania Vitali",
            "James B. Glattfelder",
            "Stefano Battiston"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The structure of the control network of transnational corporations affects\nglobal market competition and financial stability. So far, only small national\nsamples were studied and there was no appropriate methodology to assess control\nglobally. We present the first investigation of the architecture of the\ninternational ownership network, along with the computation of the control held\nby each global player. We find that transnational corporations form a giant\nbow-tie structure and that a large portion of control flows to a small\ntightly-knit core of financial institutions. This core can be seen as an\neconomic \"super-entity\" that raises new important issues both for researchers\nand policy makers.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.5728v2"
    },
    {
        "title": "Multiplicative Asset Exchange with Arbitrary Return Distributions",
        "authors": [
            "Cristian F. Moukarzel"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  The conservative wealth-exchange process derived from trade interactions is\nmodeled as a multiplicative stochastic transference of value, where each\ninteraction multiplies the wealth of the poorest of the two intervening agents\nby a random gain eta=(1+kappa), with kappa a random return. Analyzing the\nkinetic equation for the wealth distribution P(w,t), general properties are\nderived for arbitrary return distributions pi(kappa). If the geometrical\naverage of the gain is larger than one, i.e. if <ln eta> >0, in the long time\nlimit a nontrivial equilibrium wealth distribution P(w) is attained. Whenever\n<ln eta> <0, on the other hand, Wealth Condensation occurs, meaning that a\nsingle agent gets the whole wealth in the long run. This concentration\nphenomenon happens even if the average return <kappa> of the poor agent is\npositive. In the stable phase, P(w) behaves as w^{(T-1)} for w -> 0, and we\nfind T exactly. This exponent is nonzero in the stable phase but goes to zero\non approach to the condensation interface. The exact wealth distribution can be\nobtained analytically for the particular case of Kelly betting, and it turns\nout to be exponential. We show, however, that our model is never reversible, no\nmatter what pi(kappa) is. In the condensing phase, the wealth of an agent with\nrelative rank x is found to be w(x,t) \\sim e^{x t <ln eta>} for finite times t.\nThe wealth distribution is consequently P(w) \\sim 1/w for finite times, while\nall wealth ends up in the hands of a single agent for large times. Numerical\nsimulations are carried out, and found to satisfactorily compare with the above\nmentioned analytic results.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.0386v1"
    },
    {
        "title": "Permit Allocation in Emissions Trading using the Boltzmann Distribution",
        "authors": [
            "Ji-Won Park",
            "Chae Un Kim",
            "Walter Isard"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  In emissions trading, the initial allocation of permits is an intractable\nissue because it needs to be essentially fair to the participating countries.\nThere are many ways to distribute a given total amount of emissions permits\namong countries, but the existing distribution methods, such as auctioning and\ngrandfathering, have been debated. In this paper we describe a new method for\nallocating permits in emissions trading using the Boltzmann distribution. We\nintroduce the Boltzmann distribution to permit allocation by combining it with\nconcepts in emissions trading. We then demonstrate through empirical data\nanalysis how emissions permits can be allocated in practice among participating\ncountries. The new allocation method using the Boltzmann distribution describes\nthe most probable, natural, and unbiased distribution of emissions permits\namong multiple countries. Simple and versatile, this new method holds potential\nfor many economic and environmental applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.2305v2"
    },
    {
        "title": "Biased diffusion on Japanese inter-firm trading network: Estimation of\n  sales from network structure",
        "authors": [
            "Hayafumi Watanabe",
            "Hideki Takayasu",
            "Misako Takayasu"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  To investigate the actual phenomena of transport on a complex network, we\nanalysed empirical data for an inter-firm trading network, which consists of\nabout one million Japanese firms and the sales of these firms (a sale\ncorresponds to the total in-flow into a node). First, we analysed the\nrelationships between sales and sales of nearest neighbourhoods from which we\nobtain a simple linear relationship between sales and the weighted sum of sales\nof nearest neighbourhoods (i.e., customers). In addition, we introduce a simple\nmoney transport model that is coherent with this empirical observation. In this\nmodel, a firm (i.e., customer) distributes money to its out-edges (suppliers)\nproportionally to the in-degree of destinations. From intensive numerical\nsimulations, we find that the steady flows derived from these models can\napproximately reproduce the distribution of sales of actual firms. The sales of\nindividual firms deduced from the money-transport model are shown to be\nproportional, on an average, to the real sales.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.4852v1"
    },
    {
        "title": "Modeling the International-Trade Network: A Gravity Approach",
        "authors": [
            "Marco Duenas",
            "Giorgio Fagiolo"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This paper investigates whether the gravity model (GM) can explain the\nstatistical properties of the International Trade Network (ITN). We fit data on\ninternational-trade flows with a GM specification using alternative fitting\ntechniques and we employ GM estimates to build a weighted predicted ITN, whose\ntopological properties are compared to observed ones. Furthermore, we propose\nan estimation strategy to predict the binary ITN with a GM. We find that the GM\nsuccessfully replicates the weighted-network structure of the ITN, only if one\nfixes its binary architecture equal to the observed one. Conversely, the GM\nperforms very badly when asked to predict the presence of a link, or the level\nof the trade flow it carries, whenever the binary structure must be\nsimultaneously estimated.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2867v1"
    },
    {
        "title": "Evidence of market manipulation in the financial crisis",
        "authors": [
            "Vedant Misra",
            "Marco Lagi",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We provide direct evidence of market manipulation at the beginning of the\nfinancial crisis in November 2007. The type of manipulation, a \"bear raid,\"\nwould have been prevented by a regulation that was repealed by the Securities\nand Exchange Commission in July 2007. The regulation, the uptick rule, was\ndesigned to prevent manipulation and promote stability and was in force from\n1938 as a key part of the government response to the 1929 market crash and its\naftermath. On November 1, 2007, Citigroup experienced an unusual increase in\ntrading volume and decrease in price. Our analysis of financial industry data\nshows that this decline coincided with an anomalous increase in borrowed\nshares, the selling of which would be a large fraction of the total trading\nvolume. The selling of borrowed shares cannot be explained by news events as\nthere is no corresponding increase in selling by share owners. A similar number\nof shares were returned on a single day six days later. The magnitude and\ncoincidence of borrowing and returning of shares is evidence of a concerted\neffort to drive down Citigroup's stock price and achieve a profit, i.e., a bear\nraid. Interpretations and analyses of financial markets should consider the\npossibility that the intentional actions of individual actors or coordinated\ngroups can impact market behavior. Markets are not sufficiently transparent to\nreveal even major market manipulation events. Our results point to the need for\nregulations that prevent intentional actions that cause markets to deviate from\nequilibrium and contribute to crashes. Enforcement actions cannot reverse\nsevere damage to the economic system. The current \"alternative\" uptick rule\nwhich is only in effect for stocks dropping by over 10% in a single day is\ninsufficient. Prevention may be achieved through improved availability of\nmarket data and the original uptick rule or other transaction limitations.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.3095v3"
    },
    {
        "title": "Periodic Sequences of Arbitrage: A Tale of Four Currencies",
        "authors": [
            "Rod Cross",
            "Victor Kozyakin",
            "Brian O'Callaghan",
            "Alexei Pokrovskii",
            "Alexey Pokrovskiy"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  This paper investigates arbitrage chains involving four currencies and four\nforeign exchange trader-arbitrageurs. In contrast with the three-currency case,\nwe find that arbitrage operations when four currencies are present may appear\nperiodic in nature, and not involve smooth convergence to a \"balanced\" ensemble\nof exchange rates in which the law of one price holds. The goal of this article\nis to understand some interesting features of sequences of arbitrage\noperations, features which might well be relevant in other contexts in finance\nand economics.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.5850v1"
    },
    {
        "title": "Quantum Financial Economics of Games of Strategy and Financial Decisions",
        "authors": [
            "Carlos Pedro Gonçalves"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  A quantum financial approach to finite games of strategy is addressed, with\nan extension of Nash's theorem to the quantum financial setting, allowing for\nan entanglement of games of strategy with two-period financial allocation\nproblems that are expressed in terms of: the consumption plans' optimization\nproblem in pure exchange economies and the finite-state securities market\noptimization problem, thus addressing, within the financial setting, the\ninterplay between companies' business games and financial agents' behavior.\n  A complete set of quantum Arrow-Debreu prices, resulting from the game of\nstrategy's quantum Nash equilibrium, is shown to hold, even in the absence of\nsecurities' market completeness, such that Pareto optimal results are obtained\nwithout having to assume the completeness condition that the rank of the\nsecurities' payoff matrix is equal to the number of alternative lottery states.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.2080v1"
    },
    {
        "title": "The evolvability of business and the role of antitrust",
        "authors": [
            "Ian Wilkinson"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In this paper, based on theories of complex adaptive systems, I argue that\nthe main case for antitrust policy should be extended to include the criteria\nof \"evolvability.\" To date, the main case focuses on economizing, including\nmarket power as a key filter for identifying suspect cases. Both production and\ntransaction costs are considered as part of economizing and other factors are\nuse to consider the benefits of different industry structures. CAS analysis\nfocuses attention on dynamics, evolution and networks. As I will show, the\ncriteria of evolvability requires us to consider various types of direct and\nindirect network impacts in business that go beyond the traditional focus on\nproduction and transaction costs. These network impacts stem from the\nconnections between transactions and relations over time and place, including\nhow business arrangements at one time, limit or enable arrangements in the\nfuture. An assessment of the impacts, I argue, can and should be included in\nthe rules of antitrust and in the processes of antitrust case analysis and\ndecision making.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.1311v1"
    },
    {
        "title": "Dynamical fluctuations in a simple housing market model",
        "authors": [
            "Rémi Lemoy",
            "Eric Bertin"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We consider a simple stochastic model of a urban rental housing market, in\nwhich the interaction of tenants and landlords induces rent fluctuations. We\nsimulate the model numerically and measure the equilibrium rent distribution,\nwhich is found to be close to a lognormal law. We also study the influence of\nthe density of agents (or equivalently, the vacancy rate) on the rent\ndistribution. A simplified version of the model, amenable to analytical\ntreatment, is studied and leads to a lognormal distribution of rents. The\npredicted equilibrium value agrees quantitatively with numerical simulations,\nwhile a qualitative agreement is obtained for the standard deviation. The\nconnection with non-equilibrium statistical physics models like ratchets is\nalso emphasized.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.5298v2"
    },
    {
        "title": "From Nuclear Reactions to High-Frequency Trading: an R-function Approach",
        "authors": [
            "Frank W. K. Firk"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The R-function theory of Thomas is used to model neutron inelastic scattering\nand the fine, intermediate, and gross structure observed in the Dow Jones\nIndustrial Average on a typical trading day.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.6021v1"
    },
    {
        "title": "Double Exponential Instability of Triangular Arbitrage Systems",
        "authors": [
            "Rod Cross",
            "Victor Kozyakin"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  If financial markets displayed the informational efficiency postulated in the\nefficient markets hypothesis (EMH), arbitrage operations would be\nself-extinguishing. The present paper considers arbitrage sequences in foreign\nexchange (FX) markets, in which trading platforms and information are\nfragmented. In Kozyakin et al. (2010) and Cross et al. (2012) it was shown that\nsequences of triangular arbitrage operations in FX markets containing 4\ncurrencies and trader-arbitrageurs tend to display periodicity or grow\nexponentially rather than being self-extinguishing. This paper extends the\nanalysis to 5 or higher-order currency worlds. The key findings are that in a\n5-currency world arbitrage sequences may also follow an exponential law as well\nas display periodicity, but that in higher-order currency worlds a double\nexponential law may additionally apply. There is an \"inheritance of\ninstability\" in the higher-order currency worlds. Profitable arbitrage\noperations are thus endemic rather that displaying the self-extinguishing\nproperties implied by the EMH.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.3422v3"
    },
    {
        "title": "Toward A Normative Theory of Normative Marketing Theory",
        "authors": [
            "Ian Wilkinson",
            "Louise Young"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We show how different approaches to developing marketing strategies depending\non the type of environment a firm faces, where environments are distinguished\nin terms of their systems properties not their context. Particular emphasis is\ngiven to turbulent environments in which outcomes are not a priori predictable\nand are not traceable to individual firm actions and we show that, in these\nconditions, the relevant unit of competitive response and understanding is no\nlonger the individual firm but the network of relations comprising\ninterdependent, interacting firms. Networks of relations are complex adaptive\nsystems that are more 'intelligent' than the individual firms that comprise\nthem and are capable of comprehending and responding to more complex and\nturbulent environments. Yet they are co-produced by the patterns of actions and\ninteractions of the firms involved. The creation and accessing of such\ndistributed intelligence cannot be centrally directed, as this necessarily\nlimits it. Instead managers and firms are involved in a kind of participatory\nplanning and adaptation process through which the network self-organises and\nadapts. Drawing on research in systems theory, complexity, biology and\ncognitive science, extensions to the resource-based theory of the firm are\nproposed that include how resources are linked across relations and network in\na dynamic and evolutionary way. The concept of an extended firm and soft\nassembled strategies are introduced to describe the nature of the strategy\ndevelopment process. This results in a more theoretically grounded basis for\nunderstanding the nature and role of relationship and network strategies in\nmarketing and management. We finish by considering the research implications of\nour analysis and the role of agent based models as a means of sensitising and\ninforming management action.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.5821v1"
    },
    {
        "title": "A quantum mechanical model for the relationship between stock price and\n  stock ownership",
        "authors": [
            "Liviu-Adrian Cotfas"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The trade of a fixed stock can be regarded as the basic process that measures\nits momentary price. The stock price is exactly known only at the time of sale\nwhen the stock is between traders, that is, only in the case when the owner is\nunknown. We show that the stock price can be better described by a function\nindicating at any moment of time the probabilities for the possible values of\nprice if a transaction takes place. This more general description contains\npartial information on the stock price, but it also contains partial\ninformation on the stock owner. By following the analogy with quantum\nmechanics, we assume that the time evolution of the function describing the\nstock price can be described by a Schrodinger type equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.3412v2"
    },
    {
        "title": "Physical approach to price momentum and its application to momentum\n  strategy",
        "authors": [
            "Jaehyung Choi"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We introduce various quantitative and mathematical definitions for price\nmomentum of financial instruments. The price momentum is quantified with\nvelocity and mass concepts originated from the momentum in physics. By using\nthe physical momentum of price as a selection criterion, the weekly contrarian\nstrategies are implemented in South Korea KOSPI 200 and US S&P 500 universes.\nThe alternative strategies constructed by the physical momentum achieve the\nbetter expected returns and reward-risk measures than those of the traditional\ncontrarian strategy in weekly scale. The portfolio performance is not\nunderstood by the Fama-French three-factor model.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.2775v5"
    },
    {
        "title": "Finite quantum mechanical model for the stock market",
        "authors": [
            "Liviu-Adrian Cotfas"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  The price of a given stock is exactly known only at the time of sale when the\nstock is between the traders. If we know the price (owner) then we have no\ninformation on the owner (price). A more general description including cases\nwhen we have partial information on both price and ownership is obtained by\nusing the quantum mechanics methods. The relation price-ownership is similar to\nthe relation position-momentum. Our approach is based on the mathematical\nformalism used in the case of quantum systems with finite-dimensional Hilbert\nspace. The linear operator corresponding to the ownership is obtained from the\nlinear operator corresponding to the price by using the finite Fourier\ntransform. In our idealized model, the Schrodinger type equation describing the\ntime evolution of the stock price is solved numerically.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.6146v2"
    },
    {
        "title": "Kinetic models for the trading of goods",
        "authors": [
            "G. Toscani",
            "C. Brugna",
            "S. Demichelis"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In this paper we introduce kinetic equations for the evolution of the\nprobability distribution of two goods among a huge population of agents. The\nleading idea is to describe the trading of these goods by means of some\nfundamental rules in price theory, in particular by using Cobb-Douglas utility\nfunctions for the binary exchange, and the Edgeworth box for the description of\nthe common exchange area in which utility is increasing for both agents. This\nleads to a Boltzmann-type equation in which the post-interaction variables\ndepend in a nonlinear way from the pre-interaction ones. Other models will be\nderived, by suitably linearizing this Boltzmann equation. In presence of\nuncertainty in the exchanges, it is shown that the solution to some of the\nlinearized kinetic equations develop Pareto tails, where the Pareto index\ndepends on the ratio between the gain and the variance of the uncertainty. In\nparticular, the result holds true for the solution of a drift-diffusion\nequation of Fokker-Planck type, obtained from the linear Boltzmann equation as\nthe limit of quasi-invariant trades.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.6305v1"
    },
    {
        "title": "Econoinformatics meets Data-Centric Social Sciences",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Our society has been computerised and globalised due to emergence and spread\nof information and communication technology (ICT). This enables us to\ninvestigate our own socio-economic systems based on large amounts of data on\nhuman activities. In this article, methods of treating complexity arising from\na vast amount of data, and linking data from different sources, are discussed.\nFurthermore, several examples are given of studies into the applications of\neconoinformatics for the Japanese stock exchange, foreign exchange markets,\ndomestic hotel booking data and international flight booking data are shown. It\nis the main message that spatio-temporal information is a key element to\nsynthesise data from different data sources.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.4643v2"
    },
    {
        "title": "Cascading Failures in Bi-partite Graphs: Model for Systemic Risk\n  Propagation",
        "authors": [
            "Xuqing Huang",
            "Irena Vodenska",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  As economic entities become increasingly interconnected, a shock in a\nfinancial network can provoke significant cascading failures throughout the\nsystem. To study the systemic risk of financial systems, we create a bi-partite\nbanking network model composed of banks and bank assets and propose a cascading\nfailure model to describe the risk propagation process during crises. We\nempirically test the model with 2007 US commercial banks balance sheet data and\ncompare the model prediction of the failed banks with the real failed banks\nafter 2007. We find that our model efficiently identifies a significant portion\nof the actual failed banks reported by Federal Deposit Insurance Corporation.\nThe results suggest that this model could be useful for systemic risk stress\ntesting for financial systems. The model also identifies that commercial rather\nthan residential real estate assets are major culprits for the failure of over\n350 US commercial banks during 2008-2011.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.4973v3"
    },
    {
        "title": "Ethics and Finance: the role of mathematics",
        "authors": [
            "Timothy C. Johnson"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  This paper presents the contemporary Fundamental Theorem of Asset Pricing as\nbeing equivalent to approaches to pricing that emerged before 1700 in the\ncontext of Virtue Ethics. This is done by considering the history of science\nand mathematics in the thirteenth and seventeenth century. An explanation as to\nwhy these approaches to pricing were forgotten between 1700 and 2000 is given,\nalong with some of the implications on economics of viewing the Fundamental\nTheorem as a product of Virtue Ethics.\n  The Fundamental Theorem was developed in mathematics to establish a `theory'\nthat underpinned the Black-Scholes-Merton approach to pricing derivatives. In\ndoing this, the Fundamental Theorem unified a number of different approaches in\nfinancial economics, this strengthened the status of neo-classical economics\nbased on Consequentialist Ethics. We present an alternative to this narrative.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.5390v1"
    },
    {
        "title": "Stability analysis of financial contagion due to overlapping portfolios",
        "authors": [
            "Fabio Caccioli",
            "Munik Shrestha",
            "Cristopher Moore",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Common asset holdings are widely believed to have been the primary vector of\ncontagion in the recent financial crisis. We develop a network approach to the\namplification of financial contagion due to the combination of overlapping\nportfolios and leverage, and we show how it can be understood in terms of a\ngeneralized branching process. By studying a stylized model we estimate the\ncircumstances under which systemic instabilities are likely to occur as a\nfunction of parameters such as leverage, market crowding, diversification, and\nmarket impact. Although diversification may be good for individual\ninstitutions, it can create dangerous systemic effects, and as a result\nfinancial contagion gets worse with too much diversification. Under our model\nthere is a critical threshold for leverage; below it financial networks are\nalways stable, and above it the unstable region grows as leverage increases.\nThe financial system exhibits \"robust yet fragile\" behavior, with regions of\nthe parameter space where contagion is rare but catastrophic whenever it\noccurs. Our model and methods of analysis can be calibrated to real data and\nprovide simple yet powerful tools for macroprudential stress testing.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.5987v1"
    },
    {
        "title": "A quantum mechanical model for the rate of return",
        "authors": [
            "Liviu-Adrian Cotfas"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In their activity, the traders approximate the rate of return by integer\nmultiples of a minimal one. Therefore, it can be regarded as a quantized\nvariable. On the other hand, there is the impossibility of observing the rate\nof return and its instantaneous forward time derivative, even if we consider it\nas a continuous variable. We present a quantum model for the rate of return\nbased on the mathematical formalism used in the case of quantum systems with\nfinite-dimensional Hilbert space. The rate of return is described by a discrete\nwave function and its time evolution by a Schodinger type equation.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.1938v1"
    },
    {
        "title": "How Non-Arbitrage, Viability and Numéraire Portfolio are Related",
        "authors": [
            "Tahir Choulli",
            "Jun Deng",
            "Junfeng Ma"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  This paper proposes two approaches that quantify the exact relationship among\nthe viability, the absence of arbitrage, and/or the existence of the\nnum\\'eraire portfolio under minimal assumptions and for general continuous-time\nmarket models. Precisely, our first and principal contribution proves the\nequivalence among the No-Unbounded-Profit-with-Bounded-Risk condition (NUPBR\nhereafter), the existence of the num\\'eraire portfolio, and the existence of\nthe optimal portfolio under an equivalent probability measure for any \"nice\"\nutility and positive initial capital. Herein, a 'nice\" utility is any smooth\nvon Neumann-Morgenstern utility satisfying Inada's conditions and the\nelasticity assumptions of Kramkov and Schachermayer. Furthermore, the\nequivalent probability measure ---under which the utility maximization problems\nhave solutions--- can be chosen as close to the real-world probability measure\nas we want (but might not be equal). Without changing the underlying\nprobability measure and under mild assumptions, our second contribution proves\nthat the NUPBR is equivalent to the \"{\\it local}\" existence of the optimal\nportfolio. This constitutes an alternative to the first contribution, if one\ninsists on working under the real-world probability. These two contributions\nlead naturally to new types of viability that we call weak and local\nviabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4598v3"
    },
    {
        "title": "The Illusion of the Perpetual Money Machine",
        "authors": [
            "D. Sornette",
            "P. Cauwels"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We argue that the present crisis and stalling economy continuing since 2007\nare rooted in the delusionary belief in policies based on a \"perpetual money\nmachine\" type of thinking. We document strong evidence that, since the early\n1980s, consumption has been increasingly funded by smaller savings, booming\nfinancial profits, wealth extracted from house price appreciation and explosive\ndebt. This is in stark contrast with the productivity-fueled growth that was\nseen in the 1950s and 1960s. This transition, starting in the early 1980s, was\nfurther supported by a climate of deregulation and a massive growth in\nfinancial derivatives designed to spread and diversify the risks globally. The\nresult has been a succession of bubbles and crashes, including the worldwide\nstock market bubble and great crash of October 1987, the savings and loans\ncrisis of the 1980s, the burst in 1991 of the enormous Japanese real estate and\nstock market bubbles, the emerging markets bubbles and crashes in 1994 and\n1997, the LTCM crisis of 1998, the dotcom bubble bursting in 2000, the recent\nhouse price bubbles, the financialization bubble via special investment\nvehicles, the stock market bubble, the commodity and oil bubbles and the debt\nbubbles, all developing jointly and feeding on each other. Rather than still\nhoping that real wealth will come out of money creation, we need fundamentally\nnew ways of thinking. In uncertain times, it is essential, more than ever, to\nthink in scenarios: what can happen in the future, and, what would be the\neffect on your wealth and capital? How can you protect against adverse\nscenarios? We thus end by examining the question \"what can we do?\" from the\nmacro level, discussing the fundamental issue of incentives and of constructing\nand predicting scenarios as well as developing investment insights.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.2833v1"
    },
    {
        "title": "Modeling of income distribution in the European Union with the\n  Fokker-Planck equation",
        "authors": [
            "Maciej Jagielski",
            "Ryszard Kutner"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Herein, we applied statistical physics to study incomes of three (low-,\nmedium- and high-income) society classes instead of the two (low- and\nmedium-income)classes studied so far. In the frame of the threshold nonlinear\nLangevin dynamics and its threshold Fokker-Planck counterpart, we derived a\nunified formula for description of income of all society classes, by way of\nexample, of those of the European Union in year 2006 and 2008. Hence, the\nformula is more general than the well known that of Yakovenko et al. That is,\nour formula well describes not only two regions but simultaneously the third\nregion in the plot of the complementary cumulative distribution function vs. an\nannual household income. Furthermore, the known stylised facts concerning this\nincome are well described by our formula. Namely, the formula provides the\nBoltzmann-Gibbs income distribution function for the low-income society class\nand the weak Pareto law for the medium-income society class, as expected.\nImportantly, it predicts (to satisfactory approximation) the Zipf law for the\nhigh-income society class. Moreover, the region of medium-income society class\nis now distinctly reduced because the bottom of high-income society class is\ndistinctly lowered. This reduction made, in fact, the medium-income society\nclass an intermediate-income society class.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2076v1"
    },
    {
        "title": "Polish and Silesian Non-Profit Organizations Liquidity Strategies",
        "authors": [
            "Grzegorz Michalski",
            "Aleksander Mercik"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The kind of realized mission inflows the sensitivity to risk. Among other\nfactors, the risk results from decision about liquid assets investment level\nand liquid assets financing. The higher the risk exposure, the higher the level\nof liquid assets. If the specific risk exposure is smaller, the more aggressive\ncould be the net liquid assets strategy. The organization choosing between\nvarious solutions in liquid assets needs to decide what level of risk is\nacceptable for her owners (or donors) and / or capital suppliers. The paper\nshows how, in authors opinion, decisions, about liquid assets management\nstrategy inflow the risk of the organizations and its economical results during\nrealization of main mission. Comparison of theoretical model with empirical\ndata for over 450 Silesian nonprofit organization results suggests that\nnonprofit organization managing teams choose more risky aggressive liquid\nassets solutions than for-profit firms.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.3825v1"
    },
    {
        "title": "On the optimal allocation of assets in investment portfolio with\n  application of modern portfolio and nonlinear dynamic chaos theories in\n  investment, commercial and central banks",
        "authors": [
            "Dimitri O. Ledenyov",
            "Viktor O. Ledenyov"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The investment economy is a main characteristic of prosperous society. The\ninvestment portfolio management is a main financial problem, which has to be\nsolved by the investment, commercial and central banks with the application of\nmodern portfolio theory in the investment economy. We use the learning\nanalytics together with the integrative creative imperative intelligent\nconceptual co-lateral adaptive thinking with the purpose to advance our\nscientific knowledge on the diversified investment portfolio management in the\nnonlinear dynamic financial system. We apply the econophysics principles and\nthe econometrics methods with the aim to find the solution to the problem of\nthe optimal allocation of assets in the investment portfolio, using the\nadvanced risk management techniques with the efficient frontier modeling in\nagreement with the modern portfolio theory and using the stability management\ntechniques with the dynamic regimes modeling on the bifurcation diagram in\nagreement with the dynamic chaos theory. We show that the bifurcation diagram,\ncreated with the use of the logistic function in Matlab, can provide some\nvaluable information on the stability of combining risky investments in the\ninvestment portfolio, solving the problem of optimization of assets allocation\nin the investment portfolio. We propose the Ledenyov investment portfolio\ntheorem, based on the Lyapunov stability criteria, with the aim to create the\noptimized investment portfolio with the uncorrelated diversified assets, which\ncan deliver the increased expected returns to the institutional and private\ninvestors in the nonlinear dynamic financial system in the frames of investment\neconomy.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.4881v2"
    },
    {
        "title": "Evolutionary Model of a Anonymous Consumer Durable Market",
        "authors": [
            "Joachim Kaldasch"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  An analytic model is presented that considers the evolution of a market of\ndurable goods. The model suggests that after introduction goods spread always\naccording to a Bass diffusion. However, this phase will be followed by a\ndiffusion process for durable consumer goods governed by a\nvariation-selection-reproduction mechanism and the growth dynamics can be\ndescribed by a replicator equation. Describing the aggregate sales as the sum\nof first, multiple and replacement purchase the product life cycle can be\nderived. Replacement purchase causes periodic variations of the sales\ndetermined by the finite lifetime of the good (Juglar cycles). The model\nsuggests that both, Bass- and Gompertz diffusion may contribute to the product\nlife cycle of a consumer durable. The theory contains the standard equilibrium\nview of a market as a special case. It depends on the time scale, whether an\nequilibrium or evolutionary description is more appropriate. The evolutionary\nframework is used to derive also the size, growth rate and price distribution\nof manufacturing business units. It predicts that the size distribution of the\nbusiness units (products) is lognormal, while the growth rates exhibit a\nLaplace distribution. Large price deviations from the mean price are also\ngoverned by a Laplace distribution (fat tails). These results are in agreement\nwith empirical findings. The explicit comparison of the time evolution of\nconsumer durables with empirical investigations confirms the close relationship\nbetween price decline and Gompertz diffusion, while the product life cycle can\nbe described qualitatively for a long time period.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3395v1"
    },
    {
        "title": "Social Discounting and the Long Rate of Interest",
        "authors": [
            "Dorje C. Brody",
            "Lane P. Hughston"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The well-known theorem of Dybvig, Ingersoll and Ross shows that the long\nzero-coupon rate can never fall. This result, which, although undoubtedly\ncorrect, has been regarded by many as surprising, stems from the implicit\nassumption that the long-term discount function has an exponential tail. We\nrevisit the problem in the setting of modern interest rate theory, and show\nthat if the long \"simple\" interest rate (or Libor rate) is finite, then this\nrate (unlike the zero-coupon rate) acts viably as a state variable, the value\nof which can fluctuate randomly in line with other economic indicators. New\ninterest rate models are constructed, under this hypothesis and certain\ngeneralizations thereof, that illustrate explicitly the good asymptotic\nbehaviour of the resulting discount bond systems. The conditions necessary for\nthe existence of such \"hyperbolic\" and \"generalized hyperbolic\" long rates are\nthose of so-called social discounting, which allow for long-term cash flows to\nbe treated as broadly \"just as important\" as those of the short or medium term.\nAs a consequence, we are able to provide a consistent arbitrage-free valuation\nframework for the cost-benefit analysis and risk management of long-term social\nprojects, such as those associated with sustainable energy, resource\nconservation, and climate change.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.5145v4"
    },
    {
        "title": "Time-reversal asymmetry in financial systems",
        "authors": [
            "X. F. Jiang",
            "T. T. Chen",
            "B. Zheng"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We investigate the large-fluctuation dynamics in financial markets, based on\nthe minute-to-minute and daily data of the Chinese Indices and German DAX. The\ndynamic relaxation both before and after the large fluctuations is\ncharacterized by a power law, and the exponents $p_\\pm$ usually vary with the\nstrength of the large fluctuations. The large-fluctuation dynamics is\ntime-reversal symmetric at the time scale in minutes, while asymmetric at the\ndaily time scale. Careful analysis reveals that the time-reversal asymmetry is\nmainly induced by external forces. It is also the external forces which drive\nthe financial system to a non-stationary state. Different characteristics of\nthe Chinese and German stock markets are uncovered.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0669v1"
    },
    {
        "title": "New models of income distribution, graduation as the explanation of Gini\n  coefficient",
        "authors": [
            "Dmitry Schmerling"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  The paper covers the new model of wage distribution in typical group of\npeople. The model provides the opportunity to reparameterize applicable income\ndistribution model: Pareto, logarithmically normal, logarithmically logistic,\nDagum etc. The model ensures the graduation of Gini index values by polynomial\ndegree of wage distribution as well as different types of income distribution.\nThe given approach clarifies the nature of income inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.3771v1"
    },
    {
        "title": "Modelling the income distribution in the European Union: An application\n  for the initial analysis of the recent worldwide financial crisis",
        "authors": [
            "Maciej Jagielski",
            "Ryszard Kutner"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  By using methods of statistical physics, we focus on the quantitative\nanalysis of the economic income data descending from different databases. To\nexplain our approach, we introduce the necessary theoretical background, the\nextended Yakovenko et al. (EY) model. This model gives an analytical\ndescription of the annual household incomes of all society classes in the\nEuropean Union (i.e., the low-, medium-, and high-income ones) by a single\nunified formula based on unified formalism. We show that the EY model is very\nuseful for the analyses of various income datasets, in particular, in the case\nof a smooth matching of two different datasets. The completed database which we\nhave constructed using this matching emphasises the significance of the\nhigh-income society class in the analysis of all household incomes. For\ninstance, the Pareto exponent, which characterises this class, defines the Zipf\nlaw having an exponent much lower than the one characterising the medium-income\nsociety class. This result makes it possible to clearly distinguish between\nmedium- and high-income society classes. By using our approach, we found that\nthe high-income society class almost disappeared in 2009, which defines this\nyear as the most difficult for the EU. To our surprise, this is a contrast with\n2008, considered the first year of a worldwide financial crisis, when the\nstatus of the high-income society class was similar to that of 2010. This,\nperhaps, emphasises that the crisis in the EU was postponed by about one year\nin comparison with the United States.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2362v1"
    },
    {
        "title": "On idiosyncratic stochasticity of financial leverage effects",
        "authors": [
            "Carles Bretó"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We model leverage as stochastic but independent of return shocks and of\nvolatility and perform likelihood-based inference via the recently developed\niterated filtering algorithm using S&P500 data, contributing new evidence to\nthe still slim empirical support for random leverage variation.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.5496v1"
    },
    {
        "title": "Netconomics: Novel Forecasting Techniques from the Combination of Big\n  Data, Network Science and Economics",
        "authors": [
            "Andreas Joseph",
            "Irena Vodenska",
            "Eugene Stanley",
            "Guanrong Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The combination of the network theoretic approach with recently available\nabundant economic data leads to the development of novel analytic and\ncomputational tools for modelling and forecasting key economic indicators. The\nmain idea is to introduce a topological component into the analysis, taking\ninto account consistently all higher-order interactions. We present three basic\nmethodologies to demonstrate different approaches to harness the resulting\nnetwork gain. First, a multiple linear regression optimisation algorithm is\nused to generate a relational network between individual components of national\nbalance of payment accounts. This model describes annual statistics with a high\naccuracy and delivers good forecasts for the majority of indicators. Second, an\nearly-warning mechanism for global financial crises is presented, which\ncombines network measures with standard economic indicators. From the analysis\nof the cross-border portfolio investment network of long-term debt securities,\nthe proliferation of a wide range of over-the-counter-traded financial\nderivative products, such as credit default swaps, can be described in terms of\ngross-market values and notional outstanding amounts, which are associated with\nincreased levels of market interdependence and systemic risk. Third,\nconsidering the flow-network of goods traded between G-20 economies, network\nstatistics provide better proxies for key economic measures than conventional\nindicators. For example, it is shown that a country's gate-keeping potential,\nas a measure for local power, projects its annual change of GDP generally far\nbetter than the volume of its imports or exports.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.0848v1"
    },
    {
        "title": "Maximum drawdown, recovery, and momentum",
        "authors": [
            "Jaehyung Choi"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We empirically test predictability on asset price by using stock selection\nrules based on maximum drawdown and its consecutive recovery. In various equity\nmarkets, monthly momentum- and weekly contrarian-style portfolios constructed\nfrom these alternative selection criteria are superior not only in forecasting\ndirections of asset prices but also in capturing cross-sectional return\ndifferentials. In monthly periods, the alternative portfolios ranked by maximum\ndrawdown measures exhibit outperformance over other alternative momentum\nportfolios including traditional cumulative return-based momentum portfolios.\nIn weekly time scales, recovery-related stock selection rules are the best\nranking criteria for detecting mean-reversion. For the alternative portfolios\nand their ranking baskets, improved risk profiles in various reward-risk\nmeasures also imply more consistent prediction on the direction of assets in\nfuture. In the Carhart four-factor analysis, higher factor-neutral intercepts\nfor the alternative strategies are another evidence for the robust prediction\nby the alternative stock selection rules.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.8125v4"
    },
    {
        "title": "Structure of local interactions in complex financial dynamics",
        "authors": [
            "X. F. Jiang",
            "T. T. Chen",
            "B. Zheng"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  With the network methods and random matrix theory, we investigate the\ninteraction structure of communities in financial markets. In particular, based\non the random matrix decomposition, we clarify that the local interactions\nbetween the business sectors (subsectors) are mainly contained in the sector\nmode. In the sector mode, the average correlation inside the sectors is\npositive, while that between the sectors is negative. Further, we explore the\ntime evolution of the interaction structure of the business sectors, and\nobserve that the local interaction structure changes dramatically during a\nfinancial bubble or crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0070v1"
    },
    {
        "title": "Hierarchical representation of socio-economic complex systems according\n  to minimal sapnning trees",
        "authors": [
            "Andrzej Jarynowski",
            "Andrzej Buda"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  We investigate hierarchical structure in various complex systems according to\nMinimum Spanning Tree methods. Firstly, we investigate stock markets where the\ngraphis obtained from the matrix of correlations coefficient computed between\nall pairs of assets by considering the synchronous time evolution of the\ndifference of the logarithm of daily stock price. The hierarchical tree\nprovides information useful to investigate the number and nature of economic\nfactors that have associated a meaningful economic taxonomy. We continue to use\nthis method in social systems (sport, political parties and pharmacy) to\ninvestigate collective effects and detect how single element of the system\ninfluences on the other ones. The level of correlations and Minimum Spanning\nTrees in various complex systems is also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.3064v1"
    },
    {
        "title": "Developing Knowledge States: Technology and the Enhancement of National\n  Statistical Capacity",
        "authors": [
            "Derrick M. Anderson",
            "Andrew B. Whitford"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  National statistical systems are the enterprises tasked with collecting,\nvalidating and reporting societal attributes. These data serve many purposes -\nthey allow governments to improve services, economic actors to traverse\nmarkets, and academics to assess social theories. National statistical systems\nvary in quality, especially in developing countries. This study examines\ndeterminants of national statistical capacity in developing countries, focusing\non the impact of general purpose technologies (GPTs). Just as technological\nprogress helps to explain differences in economic growth, states with markets\nwith greater technological attainment (specifically, general purpose\ntechnologies) arguably have greater capacity for gathering and processing\nquality data. Analysis using panel methods shows a strong, statistically\nsignificant positive linear relationship between GPTs and national statistical\ncapacity. There is no evidence to support a non-linear function in this\nrelationship. Which is to say, there does not appear to be a marginal\ndepreciating National Statistical Capacity benefit associated with increases in\nGPTs.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.07625v1"
    },
    {
        "title": "The Intrafirm Complexity of Systemically Important Financial\n  Institutions",
        "authors": [
            "Robin L. Lumsdaine",
            "Daniel N. Rockmore",
            "Nicholas Foti",
            "Gregory Leibon",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  In November, 2011, the Financial Stability Board, in collaboration with the\nInternational Monetary Fund, published a list of 29 \"systemically important\nfinancial institutions\" (SIFIs). This designation reflects a concern that the\nfailure of any one of them could have dramatic negative consequences for the\nglobal economy and is based on \"their size, complexity, and systemic\ninterconnectedness\". While the characteristics of \"size\" and \"systemic\ninterconnectedness\" have been the subject of a good deal of quantitative\nanalysis, less attention has been paid to measures of a firm's \"complexity.\" In\nthis paper we take on the challenges of measuring the complexity of a financial\ninstitution and to that end explore the use of the structure of an individual\nfirm's control hierarchy as a proxy for institutional complexity. The control\nhierarchy is a network representation of the institution and its subsidiaries.\nWe show that this mathematical representation (and various associated metrics)\nprovides a consistent way to compare the complexity of firms with often very\ndisparate business models and as such may provide the foundation for\ndetermining a SIFI designation. By quantifying the level of complexity of a\nfirm, our approach also may prove useful should firms need to reduce their\nlevel of complexity either in response to business or regulatory needs. Using a\ndata set containing the control hierarchies of many of the designated SIFIs, we\nfind that in the past two years, these firms have decreased their level of\ncomplexity, perhaps in response to regulatory requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.02305v1"
    },
    {
        "title": "Model Risk Analysis via Investment Structuring",
        "authors": [
            "Andrei N. Soklakov"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  \"What are the origins of risks?\" and \"How material are they?\" -- these are\nthe two most fundamental questions of any risk analysis. Quantitative\nStructuring -- a technology for building financial products -- provides\neconomically meaningful answers for both of these questions. It does so by\nconsidering risk as an investment opportunity. The structure of the investment\nreveals the precise sources of risk and its expected performance measures\nmateriality. We demonstrate these capabilities of Quantitative Structuring\nusing a concrete practical example -- model risk in options on vol-targeted\nindices.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.07216v2"
    },
    {
        "title": "Approximating the Sum of Correlated Lognormals: An Implementation",
        "authors": [
            "Christopher J. Rook",
            "Mitchell Kerman"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Lognormal random variables appear naturally in many engineering disciplines,\nincluding wireless communications, reliability theory, and finance. So, too,\ndoes the sum of (correlated) lognormal random variables. Unfortunately, no\nclosed form probability distribution exists for such a sum, and it requires\napproximation. Some approximation methods date back over 80 years and most take\none of two approaches, either: 1) an approximate probability distribution is\nderived mathematically, or 2) the sum is approximated by a single lognormal\nrandom variable. In this research, we take the latter approach and review a\nfairly recent approximation procedure proposed by Mehta, Wu, Molisch, and Zhang\n(2007), then implement it using C++. The result is applied to a discrete time\nmodel commonly encountered within the field of financial economics.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.07582v1"
    },
    {
        "title": "Wealth distribution across communities of adaptive financial agents",
        "authors": [
            "Pietro DeLellis",
            "Franco Garofalo",
            "Francesco Lo Iudice",
            "Elena Napoletano"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  This paper studies the trading volumes and wealth distribution of a novel\nagent-based model of an artificial financial market. In this model,\nheterogeneous agents, behaving according to the Von Neumann and Morgenstern\nutility theory, may mutually interact. A Tobin-like tax (TT) on successful\ninvestments and a flat tax are compared to assess the effects on the agents'\nwealth distribution. We carry out extensive numerical simulations in two\nalternative scenarios: i) a reference scenario, where the agents keep their\nutility function fixed, and ii) a focal scenario, where the agents are adaptive\nand self-organize in communities, emulating their neighbours by updating their\nown utility function. Specifically, the interactions among the agents are\nmodelled through a directed scale-free network to account for the presence of\ncommunity leaders, and the herding-like effect is tested against the reference\nscenario. We observe that our model is capable of replicating the benefits and\ndrawbacks of the two taxation systems and that the interactions among the\nagents strongly affect the wealth distribution across the communities.\nRemarkably, the communities benefit from the presence of leaders with\nsuccessful trading strategies, and are more likely to increase their average\nwealth. Moreover, this emulation mechanism mitigates the decrease in trading\nvolumes, which is a typical drawback of TTs.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01217v2"
    },
    {
        "title": "Is the public sector of your country a diffusion borrower? Empirical\n  evidence from Brazil",
        "authors": [
            "Leno S. Rocha",
            "Frederico S. A. Rocha",
            "Thársis T. P. Souza"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We propose a diffusion process to describe the global dynamic evolution of\ncredit operations at a national level given observed operations at a\nsubnational level in a sovereign country. Empirical analysis with a unique\ndataset from Brazilian federate constituents supports the conclusions. Despite\nthe heterogeneity observed in credit operations at a subnational level, the\naggregated dynamics at a national level were accurately described with the\nproposed model. Results may guide management of public finances, particularly\ndebt manager authorities in charge of reaching surplus targets.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07782v1"
    },
    {
        "title": "Kinetic and mean field description of Gibrat's law",
        "authors": [
            "Giuseppe Toscani"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We introduce and analyze a linear kinetic model that describes the evolution\nof the probability density of the number of firms in a society, in which the\nmicroscopic rate of change obeys to the so-called law of proportional effect\nproposed by Gibrat. Despite its apparent simplicity, the possible mean field\nlimits of the kinetic model are varied. In some cases, the asymptotic limit can\nbe described by a first-order partial differential equation. In other cases,\nthe mean field equation is a linear diffusion with a non constant diffusion\ncoefficient that models also the geometric Brownian motion and can be studied\nanalytically. In this case, it is shown that the large-time behavior of the\nsolution is represented, for a large class of initial data, by a lognormal\ndistribution with constant mean value and variance increasing exponentially in\ntime at a precise rate. The relationship between the kinetic and the diffusion\nmodels allow to introduce an easy-to- implement expression for computing the\nFourier transform of the lognormal distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.04796v1"
    },
    {
        "title": "Stock Market Market Crash of 2008: an empirical study of the deviation\n  of share prices from company fundamentals",
        "authors": [
            "Taisei Kaizoji",
            "Michiko Miyano"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  The aim of this study is to investigate quantitatively whether share prices\ndeviated from company fundamentals in the stock market crash of 2008. For this\npurpose, we use a large database containing the balance sheets and share prices\nof 7,796 worldwide companies for the period 2004 through 2013. We develop a\npanel regression model using three financial indicators--dividends per share,\ncash flow per share, and book value per share--as explanatory variables for\nshare price. We then estimate individual company fundamentals for each year by\nremoving the time fixed effects from the two-way fixed effects model, which we\nidentified as the best of the panel regression models. One merit of our model\nis that we are able to extract unobservable factors of company fundamentals by\nusing the individual fixed effects.\n  Based on these results, we analyze the market anomaly quantitatively using\nthe divergence rate--the rate of the deviation of share price from a company's\nfundamentals. We find that share prices on average were overvalued in the\nperiod from 2005 to 2007, and were undervalued significantly in 2008, when the\nglobal financial crisis occurred. Share prices were equivalent to the\nfundamentals on average in the subsequent period. Our empirical results clearly\ndemonstrate that the worldwide stock market fluctuated excessively in the time\nperiod before and just after the global financial crisis of 2008.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.03205v1"
    },
    {
        "title": "New economic windows on income and wealth: The k-generalized family of\n  distributions",
        "authors": [
            "F. Clementi",
            "M. Gallegati"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Over the last decades, the distribution of income and wealth has been\ndeteriorating in many countries, leading to increased inequalities within and\nbetween societies. This tendency has revived the interest in the subject\ngreatly, yet it still receives very little attention within the realm of\nmainstream economic thinking. One reason for this is that the basic paradigm of\n\"standard economics\", the representative-agent General Equilibrium framework,\nis badly equipped to cope with distributional issues. Here we argue that when\nthe economy is treated as a complex system composed of many heterogeneous\ninteracting agents who give rise to emergent phenomena, to address the main\nstylized facts of income/wealth distribution requires leaving the toolbox of\nmainstream economics in favour of alternative approaches. The \"k-generalized\"\nfamily of income/wealth distributions, building on the categories of\ncomplexity, is an example of how advances in the field can be achieved within\nnew interdisciplinary research contexts.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.06076v1"
    },
    {
        "title": "Institutionalization in Efficient Markets: The Case of Price Bubbles",
        "authors": [
            "Sheen S. Levine",
            "Edward J. Zajac"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  We seek to deepen understanding of the micro-foundations of\ninstitutionalization while contributing to a sociological theory of markets by\ninvestigating the puzzle of price bubbles in financial markets. We find that\nsuch markets, despite textbook conditions of high efficiency -- perfect\ninformation, atomistic agents, no uncertainty -- quickly develop patterns\nconsistent with institutionalization processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04629v1"
    },
    {
        "title": "Leontief Meets Shannon - Measuring the Complexity of the Economic System",
        "authors": [
            "Dave Zachariah",
            "Paul Cockshott"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  We develop a complexity measure for large-scale economic systems based on\nShannon's concept of entropy. By adopting Leontief's perspective of the\nproduction process as a circular flow, we formulate the process as a Markov\nchain. Then we derive a measure of economic complexity as the average number of\nbits required to encode the flow of goods and services in the production\nprocess. We illustrate this measure using data from seven national economies,\nspanning several decades.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.02154v2"
    },
    {
        "title": "Herding boosts too-connected-to-fail risk in stock market of China",
        "authors": [
            "Shan Lu",
            "Jichang Zhao",
            "Huiwen Wang",
            "Ruoen Ren"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  The crowd panic and its contagion play non-negligible roles at the time of\nthe stock crash, especially for China where inexperienced investors dominate\nthe market. However, existing models rarely consider investors in networking\nstocks and accordingly miss the exact knowledge of how panic contagion leads to\nabrupt crash. In this paper, by networking stocks of sharing common mutual\nfunds, a new methodology of investigating the market crash is presented. It is\nsurprisingly revealed that the herding, which origins in the mimic of seeking\nfor high diversity across investment strategies to lower individual risk, will\nproduce too-connected-to-fail stocks and reluctantly boosts the systemic risk\nof the entire market. Though too-connected stocks might be relatively stable\nduring the crisis, they are so influential that a small downward fluctuation\nwill cascade to trigger severe drops of massive successor stocks, implying that\ntheir falls might be unexpectedly amplified by the collective panic and result\nin the market crash. Our findings suggest that the whole picture of portfolio\nstrategy has to be carefully supervised to reshape the stock network.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08240v3"
    },
    {
        "title": "The Impact of Digital Financial Services on Firm's Performance: a\n  Literature Review",
        "authors": [
            "Tariq Abbasi",
            "Hans Weigand"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Digital Financial Services continue to expand and replace the delivery of\ntraditional banking services to the customers through innovative technologies\nto meet the growing complex needs and globalization challenges. These\ndiversified digital products help the organizations (service providers) to\nimprove their firm performance and to remain competitive in the market. It also\nassists in increasing market share to grow their profitability and improve\nfinancial position. There is a growing literature on Digital Financial Services\nand firm performance. At this point of the development, this paper systemically\nreviews the existing (within last one decade) amount of literature\ninvestigating the impact of DFS on firm performance, analyzes and identifies\nthe research gaps. We identify 39 works that have appeared in a wide range of\npeer-reviewed scientific journals. We classify the methodologies and approaches\nthat researchers have used to predict the effect of such services on the\nfinancial growth and profitability. We observe that despite rapid technological\nadvancement in DFS during the last ten years, Digital Financial Services being\nthe factor affecting firm performance did not get the reasonable attention in\nacademic literature. One of the reason is that almost all the authors limit\ntheir research to banking sector while ignoring others particularly mobile\nnetwork operators (providing branchless banking) and new non-banking entrants.\nWe also notice that newer researchers often ignore past research and\ninvestigate the same issues. This study also makes several recommendations and\nsuggest directions for future research in this still emerging field.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.10294v1"
    },
    {
        "title": "Stock-flow consistent macroeconomic model with nonuniform distributional\n  constraint",
        "authors": [
            "Aurélien Hazan"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  We report on results concerning a partially aggregated Stock Flow Consistent\n(SFC) macroeconomic model in the stationary state where the sectors of banks\nand firms are aggregated, the sector of households is dis-aggregated, and the\nprobability density function (pdf) of the wealth of households is exogenous,\nconstrained by econometric data. It is shown that the equality part of the\nconstraint can be reduced to a single constant-sum equation, which relates this\nproblem to the study of continuous mass transport problems, and to the sum of\niid random variables. Existing results can thus be applied, and provide\nmarginal probabilities, and the location of the critical point before\ncondensation occurs. Various numerical experiments are performed using Monte\nCarlo sampling of the hit-and-run type, using wealth and income data for\nFrance.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.00645v1"
    },
    {
        "title": "qBitcoin: A Peer-to-Peer Quantum Cash System",
        "authors": [
            "Kazuki Ikeda"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  A decentralized online quantum cash system, called qBitcoin, is given. We\ndesign the system which has great benefits of quantization in the following\nsense. Firstly, quantum teleportation technology is used for coin transaction,\nwhich prevents from the owner of the coin keeping the original coin data even\nafter sending the coin to another. This was a main problem in a classical\ncircuit and a blockchain was introduced to solve this issue. In qBitcoin, the\ndouble-spending problem never happens and its security is guaranteed\ntheoretically by virtue of quantum information theory. Making a block is time\nconsuming and the system of qBitcoin is based on a quantum chain, instead of\nblocks. Therefore a payment can be completed much faster than Bitcoin. Moreover\nwe employ quantum digital signature so that it naturally inherits properties of\npeer-to-peer (P2P) cash system as originally proposed in Bitcoin.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04955v2"
    },
    {
        "title": "Volatility and Economic Growth in the Twentieth Century",
        "authors": [
            "Mercedes Campi",
            "Marco Dueñas"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  The twentieth century was a period of outstanding economic growth together\nwith an unequal income distribution. This paper analyses the international\ndistribution of growth rates and its dynamics during the twentieth century. We\nshow that the whole century is characterized by a high heterogeneity in the\ndistribution of GDP per capita growth rates, which is reflected in different\nshapes and a persistent asymmetry of the distributions at the regional level\nand for countries of different development levels. We find that in the context\nof the global conflicts that characterized the first half of the twentieth\ncentury and involved mainly large economies, the well-known negative scale\nrelation between volatility and size of countries is not significant. After the\nyear 1956, a redistribution of volatility leads to a significant negative\nscale-relation, which has been recently considered as a robust feature of the\nevolution of economic organizations. Our results contribute with more empirical\nfacts that call the attention to traditional macroeconomic theories to better\nexplain the underlying complexity of the growth process and sheds light on its\nhistorical evolution.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06792v1"
    },
    {
        "title": "Multivariate Density Modeling for Retirement Finance",
        "authors": [
            "Christopher J. Rook"
        ],
        "category": "q-fin.GN",
        "published_year": "2017",
        "summary": "  Prior to the financial crisis mortgage securitization models increased in\nsophistication as did products built to insure against losses. Layers of\ncomplexity formed upon a foundation that could not support it and as the\nfoundation crumbled the housing market followed. That foundation was the\nGaussian copula which failed to correctly model failure-time correlations of\nderivative securities in duress. In retirement, surveys suggest the greatest\nfear is running out of money and as retirement decumulation models become\nincreasingly sophisticated, large financial firms and robo-advisors may\nguarantee their success. Similar to an investment bank failure the event of\nretirement ruin is driven by outliers and correlations in times of stress. It\nwould be desirable to have a foundation able to support the increased\ncomplexity before it forms however the industry currently relies upon similar\nGaussian (or lognormal) dependence structures. We propose a multivariate\ndensity model having fixed marginals that is tractable and fits data which are\nskewed, heavy-tailed, multimodal, i.e., of arbitrary complexity allowing for a\nrich correlation structure. It is also ideal for stress-testing a retirement\nplan by fitting historical data seeded with black swan events. A preliminary\nsection reviews all concepts before they are used and fully documented C/C++\nsource code is attached making the research self-contained. Lastly, we take the\nopportunity to challenge existing retirement finance dogma and also review some\nrecent criticisms of retirement ruin probabilities and their suggested\nreplacement metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.04070v1"
    },
    {
        "title": "Characterization of catastrophic instabilities: Market crashes as\n  paradigm",
        "authors": [
            "Anirban Chakraborti",
            "Kiran Sharma",
            "Hirdesh K. Pharasi",
            "Sourish Das",
            "Rakesh Chatterjee",
            "Thomas H. Seligman"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Catastrophic events, though rare, do occur and when they occur, they have\ndevastating effects. It is, therefore, of utmost importance to understand the\ncomplexity of the underlying dynamics and signatures of catastrophic events,\nsuch as market crashes. For deeper understanding, we choose the US and Japanese\nmarkets from 1985 onward, and study the evolution of the cross-correlation\nstructures of stock return matrices and their eigenspectra over different short\ntime-intervals or \"epochs\". A slight non-linear distortion is applied to the\ncorrelation matrix computed for any epoch, leading to the emerging spectrum of\neigenvalues. The statistical properties of the emerging spectrum display: (i)\nthe shape of the emerging spectrum reflects the market instability, (ii) the\nsmallest eigenvalue may be able to statistically distinguish the nature of a\nmarket turbulence or crisis -- internal instability or external shock, and\n(iii) the time-lagged smallest eigenvalue has a statistically significant\ncorrelation with the mean market cross-correlation. The smallest eigenvalue\nseems to indicate that the financial market has become more turbulent in a\nsimilar way as the mean does. Yet we show features of the smallest eigenvalue\nof the emerging spectrum that distinguish different types of market\ninstabilities related to internal or external causes. Based on the paradigmatic\ncharacter of financial time series for other complex systems, the capacity of\nthe emerging spectrum to understand the nature of instability may be a new\nfeature, which can be broadly applied.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.07213v1"
    },
    {
        "title": "Private Information, Credit Risk and Graph Structure in P2P Lending\n  Networks",
        "authors": [
            "J. Christopher Westland",
            "Tuan Q. Phan",
            "Tianhui Tan"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  This research investigated the potential for improving Peer-to-Peer (P2P)\ncredit scoring by using \"private information\" about communications and travels\nof borrowers. We found that P2P borrowers' ego networks exhibit scale-free\nbehavior driven by underlying preferential attachment mechanisms that connect\nborrowers in a fashion that can be used to predict loan profitability. The\nprojection of these private networks onto networks of mobile phone\ncommunication and geographical locations from mobile phone GPS potentially give\nloan providers access to private information through graph and location metrics\nwhich we used to predict loan profitability. Graph topology was found to be an\nimportant predictor of loan profitability, explaining over 5.5% of variability.\nNetworks of borrower location information explain an additional 19% of the\nprofitability. Machine learning algorithms were applied to the data set\npreviously analyzed to develop the predictive model and resulted in a 4%\nreduction in mean squared error.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.10000v1"
    },
    {
        "title": "Hierarchical financial structures with money cascade",
        "authors": [
            "Mahendra K. Verma"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In this paper we show similarities between turbulence and financial systems.\nMotivated by similarities between the two systems, we construct a multiscale\nmodel for hierarchical financial structures that exhibits a constant cascade of\nwealth from large financial entities to small financial entities. According to\nour model, large and intermediate scale financial institutions have a power law\ndistribution. However, the wealth distribution is Maxwellian at individual\nscales.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.00313v1"
    },
    {
        "title": "Using Deep Learning Neural Networks and Candlestick Chart Representation\n  to Predict Stock Market",
        "authors": [
            "Rosdyana Mangir Irawan Kusuma",
            "Trang-Thi Ho",
            "Wei-Chun Kao",
            "Yu-Yen Ou",
            "Kai-Lung Hua"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Stock market prediction is still a challenging problem because there are many\nfactors effect to the stock market price such as company news and performance,\nindustry performance, investor sentiment, social media sentiment and economic\nfactors. This work explores the predictability in the stock market using Deep\nConvolutional Network and candlestick charts. The outcome is utilized to design\na decision support framework that can be used by traders to provide suggested\nindications of future stock price direction. We perform this work using various\ntypes of neural networks like convolutional neural network, residual network\nand visual geometry group network. From stock market historical data, we\nconverted it to candlestick charts. Finally, these candlestick charts will be\nfeed as input for training a Convolutional Neural Network model. This\nConvolutional Neural Network model will help us to analyze the patterns inside\nthe candlestick chart and predict the future movements of stock market. The\neffectiveness of our method is evaluated in stock market prediction with a\npromising results 92.2% and 92.1% accuracy for Taiwan and Indonesian stock\nmarket dataset respectively. The constructed model have been implemented as a\nweb-based system freely available at http://140.138.155.216/deepcandle/ for\npredicting stock market using candlestick chart and deep learning neural\nnetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.12258v1"
    },
    {
        "title": "The Inverted Parabola World of Classical Quantitative Finance:\n  Non-Equilibrium and Non-Perturbative Finance Perspective",
        "authors": [
            "Igor Halperin"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Classical quantitative finance models such as the Geometric Brownian Motion\nor its later extensions such as local or stochastic volatility models do not\nmake sense when seen from a physics-based perspective, as they are all\nequivalent to a negative mass oscillator with a noise. This paper presents an\nalternative formulation based on insights from physics.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.03623v1"
    },
    {
        "title": "Supervised Machine Learning Techniques: An Overview with Applications to\n  Banking",
        "authors": [
            "Linwei Hu",
            "Jie Chen",
            "Joel Vaughan",
            "Hanyu Yang",
            "Kelly Wang",
            "Agus Sudjianto",
            "Vijayan N. Nair"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This article provides an overview of Supervised Machine Learning (SML) with a\nfocus on applications to banking. The SML techniques covered include Bagging\n(Random Forest or RF), Boosting (Gradient Boosting Machine or GBM) and Neural\nNetworks (NNs). We begin with an introduction to ML tasks and techniques. This\nis followed by a description of: i) tree-based ensemble algorithms including\nBagging with RF and Boosting with GBMs, ii) Feedforward NNs, iii) a discussion\nof hyper-parameter optimization techniques, and iv) machine learning\ninterpretability. The paper concludes with a comparison of the features of\ndifferent ML algorithms. Examples taken from credit risk modeling in banking\nare used throughout the paper to illustrate the techniques and interpret the\nresults of the algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04059v1"
    },
    {
        "title": "Coexisting Hidden and self-excited attractors in an economic system of\n  integer or fractional order",
        "authors": [
            "Marius-F. Danca"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this paper the dynamics of an economic system with foreign financing, of\ninteger or fractional order, are analyzed. The symmetry of the system\ndetermines the existence of two pairs of coexisting attractors. The\ninteger-order version of the system proves to have several combinations of\ncoexisting hidden attractors with self-excited attractors. Because one of the\nsystem variables represents the foreign capital in ow, the presence of hidden\nattractors could be of a real interest in economic models. The fractional-order\nvariant presents another interesting coexistence of attractors in the\nfractional order space.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.12108v2"
    },
    {
        "title": "ESG investments: Filtering versus machine learning approaches",
        "authors": [
            "Carmine de Franco",
            "Christophe Geissler",
            "Vincent Margot",
            "Bruno Monnier"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We designed a machine learning algorithm that identifies patterns between ESG\nprofiles and financial performances for companies in a large investment\nuniverse. The algorithm consists of regularly updated sets of rules that map\nregions into the high-dimensional space of ESG features to excess return\npredictions. The final aggregated predictions are transformed into scores which\nallow us to design simple strategies that screen the investment universe for\nstocks with positive scores. By linking the ESG features with financial\nperformances in a non-linear way, our strategy based upon our machine learning\nalgorithm turns out to be an efficient stock picking tool, which outperforms\nclassic strategies that screen stocks according to their ESG ratings, as the\npopular best-in-class approach. Our paper brings new ideas in the growing field\nof financial literature that investigates the links between ESG behavior and\nthe economy. We show indeed that there is clearly some form of alpha in the ESG\nprofile of a company, but that this alpha can be accessed only with powerful,\nnon-linear techniques such as machine learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07477v2"
    },
    {
        "title": "Short-time behaviour of demand and price viewed through an exactly\n  solvable model for heterogeneous interacting market agents",
        "authors": [
            "Gunter M. Schütz",
            "Fernando Pigeard de Almeida Prado",
            "Rosemary J. Harris",
            "Vladimir Belitsky"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We introduce a stochastic heterogeneous interacting-agent model for the\nshort-time non-equilibrium evolution of excess demand and price in a stylized\nasset market. We consider a combination of social interaction within peer\ngroups and individually heterogeneous fundamentalist trading decisions which\ntake into account the market price and the perceived fundamental value of the\nasset. The resulting excess demand is coupled to the market price. Rigorous\nanalysis reveals that this feedback may lead to price oscillations, a single\nbounce, or monotonic price behaviour. The model is a rare example of an\nanalytically tractable interacting-agent model which allows us to deduce in\ndetail the origin of these different collective patterns. For a natural choice\nof initial distribution the results are independent of the graph structure that\nmodels the peer network of agents whose decisions influence each other.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.0003v2"
    },
    {
        "title": "Explicit Description of HARA Forward Utilities and Their Optimal\n  Portfolios",
        "authors": [
            "Tahir Choulli",
            "Junfeng Ma"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  This paper deals with forward performances of HARA type. Precisely, for a\nmarket model in which stock price processes are modeled by a locally bounded\n$d$-dimensional semimartingale, we elaborate a complete and explicit\ncharacterization for this type of forward utilities. Furthermore, the optimal\nportfolios for each of these forward utilities are explicitly described. Our\napproach is based on the minimal Hellinger martingale densities that are\nobtained from the important statistical concept of Hellinger process. These\nmartingale densities were introduced recently, and appeared herein tailor-made\nfor these forward utilities. After outlining our parametrization method for the\nHARA forward, we provide illustrations on discrete-time market models. Finally,\nwe conclude our paper by pointing out a number of related open questions.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0785v1"
    },
    {
        "title": "Optimal Equity Glidepaths in Retirement",
        "authors": [
            "Christopher J. Rook"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Dynamic retirement glidepaths evolve over time based on some measure such as\nthe retiree's funded status or current market valuations. Conversely, static\nglidepaths are fixed at a starting point and selected under the assumption that\nthey will not change. In practice, new static glidepaths may be derived\nperiodically making them more flexible. The optimal static retirement glidepath\nwould be the one that performs better than all others with respect to some\nmetric. When systematic withdrawals are made from a retirement portfolio,\nglidepaths are often assessed via the probability of ruin (or success). Our\ngoal here is to derive the optimal static glidepath with respect to this\nmetric. It is a result new to the literature and the shape will be of special\ninterest to retirees, financial advisors, retirement researchers, and\ntarget-date fund providers.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08400v1"
    },
    {
        "title": "Analyses of Aggregate Fluctuations of Firm Network Based on the\n  Self-Organized Criticality Model",
        "authors": [
            "Hiroyasu Inoue"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  This study examine the difference in the size of avalanches among industries\ntriggered by demand shocks, which can be rephrased by control of the economy or\nfiscal policy, and by using the production-inventory model and observed data.\nWe obtain the following results. (1) The size of avalanches follows power law.\n(2) The mean sizes of avalanches for industries are diverse but their standard\ndeviations highly overlap. (3) We compare the simulation with an input-output\ntable and with the actual policies. They are compatible.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.05066v2"
    },
    {
        "title": "The varying importance of extrinsic factors in the success of startup\n  fundraising: competition at early-stage and networks at growth-stage",
        "authors": [
            "Clement Gastaud",
            "Theophile Carniel",
            "Jean-Michel Dalle"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We address the issue of the factors driving startup success in raising funds.\nUsing the popular and public startup database Crunchbase, we explicitly take\ninto account two extrinsic characteristics of startups: the competition that\nthe companies face, using similarity measures derived from the Word2Vec\nalgorithm, as well as the position of investors in the investment network,\npioneering the use of Graph Neural Networks (GNN), a recent deep learning\ntechnique that enables the handling of graphs as such and as a whole. We show\nthat the different stages of fundraising, early- and growth-stage, are\nassociated with different success factors. Our results suggest a marked\nrelevance of startup competition for early stage while growth-stage fundraising\nis influenced by network features. Both of these factors tend to average out in\nglobal models, which could lead to the false impression that startup success in\nfundraising would mostly if not only be influenced by its intrinsic\ncharacteristics, notably those of their founders.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03210v1"
    },
    {
        "title": "Contingent Convertible Bonds in Financial Networks",
        "authors": [
            "Giovanni Calice",
            "Carlo Sala",
            "Daniele Tantari"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We study the role of contingent convertible bonds (CoCos) in a complex\nnetwork of interconnected banks. By studying the system's phase transitions, we\nreveal that the structure of the interbank network is of fundamental importance\nfor the effectiveness of CoCos as a financial stability enhancing mechanism.\nOur results show that, under some network structures, the presence of CoCos can\nincrease (and not reduce) financial fragility, because of the occurring of\nunneeded triggers and consequential suboptimal conversions that damage CoCos\ninvestors. We also demonstrate that, in the presence of a moderate financial\nshock, lightly interconnected financial networks are more robust than highly\ninterconnected networks. This makes them a potentially optimal choice for both\nCoCos issuers and buyers.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00062v2"
    },
    {
        "title": "Qlib: An AI-oriented Quantitative Investment Platform",
        "authors": [
            "Xiao Yang",
            "Weiqing Liu",
            "Dong Zhou",
            "Jiang Bian",
            "Tie-Yan Liu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Quantitative investment aims to maximize the return and minimize the risk in\na sequential trading period over a set of financial instruments. Recently,\ninspired by rapid development and great potential of AI technologies in\ngenerating remarkable innovation in quantitative investment, there has been\nincreasing adoption of AI-driven workflow for quantitative research and\npractical investment. In the meantime of enriching the quantitative investment\nmethodology, AI technologies have raised new challenges to the quantitative\ninvestment system. Particularly, the new learning paradigms for quantitative\ninvestment call for an infrastructure upgrade to accommodate the renovated\nworkflow; moreover, the data-driven nature of AI technologies indeed indicates\na requirement of the infrastructure with more powerful performance;\nadditionally, there exist some unique challenges for applying AI technologies\nto solve different tasks in the financial scenarios. To address these\nchallenges and bridge the gap between AI technologies and quantitative\ninvestment, we design and develop Qlib that aims to realize the potential,\nempower the research, and create the value of AI technologies in quantitative\ninvestment.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.11189v1"
    },
    {
        "title": "Arbitrage and duality in nondominated discrete-time models",
        "authors": [
            "Bruno Bouchard",
            "Marcel Nutz"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  We consider a nondominated model of a discrete-time financial market where\nstocks are traded dynamically, and options are available for static hedging. In\na general measure-theoretic setting, we show that absence of arbitrage in a\nquasi-sure sense is equivalent to the existence of a suitable family of\nmartingale measures. In the arbitrage-free case, we show that optimal\nsuperhedging strategies exist for general contingent claims, and that the\nminimal superhedging price is given by the supremum over the martingale\nmeasures. Moreover, we obtain a nondominated version of the Optional\nDecomposition Theorem.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.6008v3"
    },
    {
        "title": "Exploring the Uncharted Export: an Analysis of Tourism-Related Foreign\n  Expenditure with International Spend Data",
        "authors": [
            "Michele Coscia",
            "Ricardo Hausmann",
            "Frank Neffke"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  Tourism is one of the most important economic activities in the world: for\nmany countries it represents the single largest product in their export basket.\nHowever, it is a product difficult to chart: \"exporters\" of tourism do not ship\nit abroad, but they welcome importers inside the country. Current research uses\nsocial accounting matrices and general equilibrium models, but the standard\nindustry classifications they use make it hard to identify which domestic\nindustries cater to foreign visitors. In this paper, we make use of open source\ndata and of anonymized and aggregated transaction data giving us insights about\nthe spend behavior of foreigners inside two countries, Colombia and the\nNetherlands, to inform our research. With this data, we are able to describe\nwhat constitutes the tourism sector, and to map the most attractive\ndestinations for visitors. In particular, we find that countries might observe\ndifferent geographical tourists' patterns -- concentration versus\ndecentralization --; we show the importance of distance, a country's reported\nwealth and cultural affinity in informing tourism; and we show the potential of\ncombining open source data and anonymized and aggregated transaction data on\nforeign spend patterns in gaining insight as to the evolution of tourism from\none year to another.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.09893v1"
    },
    {
        "title": "Market Manipulation of Bitcoin: Evidence from Mining the Mt. Gox\n  Transaction Network",
        "authors": [
            "Weili Chen",
            "Jun Wu",
            "Zibin Zheng",
            "Chuan Chen",
            "Yuren Zhou"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The cryptocurrency market is a very huge market without effective\nsupervision. It is of great importance for investors and regulators to\nrecognize whether there are market manipulation and its manipulation patterns.\nThis paper proposes an approach to mine the transaction networks of exchanges\nfor answering this question.By taking the leaked transaction history of Mt. Gox\nBitcoin exchange as a sample,we first divide the accounts into three categories\naccording to its characteristic and then construct the transaction history into\nthree graphs. Many observations and findings are obtained via analyzing the\nconstructed graphs. To evaluate the influence of the accounts' transaction\nbehavior on the Bitcoin exchange price,the graphs are reconstructed into series\nand reshaped as matrices. By using singular value decomposition (SVD) on the\nmatrices, we identify many base networks which have a great correlation with\nthe price fluctuation. When further analyzing the most important accounts in\nthe base networks, plenty of market manipulation patterns are found. According\nto these findings, we conclude that there was serious market manipulation in\nMt. Gox exchange and the cryptocurrency market must strengthen the supervision.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01941v1"
    },
    {
        "title": "Global Stock Market Prediction Based on Stock Chart Images Using Deep\n  Q-Network",
        "authors": [
            "Jinho Lee",
            "Raehyun Kim",
            "Yookyung Koh",
            "Jaewoo Kang"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We applied Deep Q-Network with a Convolutional Neural Network function\napproximator, which takes stock chart images as input, for making global stock\nmarket predictions. Our model not only yields profit in the stock market of the\ncountry where it was trained but generally yields profit in global stock\nmarkets. We trained our model only in the US market and tested it in 31\ndifferent countries over 12 years. The portfolios constructed based on our\nmodel's output generally yield about 0.1 to 1.0 percent return per transaction\nprior to transaction costs in 31 countries. The results show that there are\nsome patterns on stock chart image, that tend to predict the same future stock\nprice movements across global stock markets. Moreover, the results show that\nfuture stock prices can be predicted even if the training and testing\nprocedures are done in different countries. Training procedure could be done in\nrelatively large and liquid markets (e.g., USA) and tested in small markets.\nThis result demonstrates that artificial intelligence based stock price\nforecasting models can be used in relatively small markets (emerging countries)\neven though they do not have a sufficient amount of data for training.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10948v1"
    },
    {
        "title": "Graph-based era segmentation of international financial integration",
        "authors": [
            "Cécile Bastidon",
            "Antoine Parent",
            "Pablo Jensen",
            "Patrice Abry",
            "Pierre Borgnat"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Assessing world-wide financial integration constitutes a recurrent challenge\nin macroeconometrics, often addressed by visual inspections searching for data\npatterns. Econophysics literature enables us to build complementary,\ndata-driven measures of financial integration using graphs. The present\ncontribution investigates the potential and interests of a novel 3-step\napproach that combines several state-of-the-art procedures to i) compute\ngraph-based representations of the multivariate dependence structure of asset\nprices time series representing the financial states of 32 countries world-wide\n(1955-2015); ii) compute time series of 5 graph-based indices that characterize\nthe time evolution of the topologies of the graph; iii) segment these time\nevolutions in piece-wise constant eras, using an optimization framework\nconstructed on a multivariate multi-norm total variation penalized functional.\nThe method shows first that it is possible to find endogenous stable eras of\nworld-wide financial integration. Then, our results suggest that the most\nrelevant globalization eras would be based on the historical patterns of global\ncapital flows, while the major regulatory events of the 1970s would only appear\nas a cause of sub-segmentation.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.11842v1"
    },
    {
        "title": "151 Estrategias de Trading (151 Trading Strategies)",
        "authors": [
            "Zura Kakushadze",
            "Juan Andrés Serur"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  This book, which is in Spanish, provides detailed descriptions, including\nover 550 mathematical formulas, for over 150 trading strategies across a host\nof asset classes (and trading styles). This includes stocks, options, fixed\nincome, futures, ETFs, indexes, commodities, foreign exchange, convertibles,\nstructured assets, volatility (as an asset class), real estate, distressed\nassets, cash, cryptocurrencies, miscellany (such as weather, energy,\ninflation), global macro, infrastructure, and tax arbitrage. Some strategies\nare based on machine learning algorithms (such as artificial neural networks,\nBayes, k-nearest neighbors). We also give: source code for illustrating\nout-of-sample backtesting with explanatory notes; around 2,000 bibliographic\nreferences; and over 900 glossary, acronym and math definitions. The\npresentation is intended to be descriptive and pedagogical.\n  -----\n  Este libro proporciona descripciones detalladas, que incluyen m\\'as de 550\nf\\'ormulas matem\\'aticas, para m\\'as de 150 estrategias de trading para una\ngran cantidad de clases de activos y estilos de trading. Esto incluye acciones,\nopciones, bonos (renta fija), futuros, ETFs, \\'indices, commodities, divisas,\nbonos convertibles, activos estructurados, volatilidad (como clase de activos),\nbienes inmuebles, activos en distress, efectivo, criptomonedas, miscel\\'aneos\n(como clima, energ\\'ia, inflaci\\'on), macro global, infraestructura y arbitraje\nimpositivo. Algunas estrategias se basan en algoritmos de aprendizaje\nautom\\'atico (como redes neuronales artificiales, Bayes, k vecinos m\\'as\ncercanos). El libro tambi\\'en incluye c\\'odigo para backtesting fuera de la\nmuestra con notas explicativas; cerca de 2,000 referencias bibliogr\\'aficas;\nm\\'as de 900 t\\'erminos que comprenden el glosario, acr\\'onimos y definiciones\nmatem\\'aticas. La presentaci\\'on pretende ser descriptiva y pedag\\'ogica.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04492v1"
    },
    {
        "title": "Dynamics of the Price Behavior in Stock Market: A Statistical Physics\n  Approach",
        "authors": [
            "Hung T. Diep",
            "Gabriel Desgranges"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We study in this paper the time evolution of stock markets using a\nstatistical physics approach. Each agent is represented by a spin having a\nnumber of discrete states $q$ or continuous states, describing the tendency of\nthe agent for buying or selling. The market ambiance is represented by a\nparameter $T$ which plays the role of the temperature in physics. We show that\nthere is a critical value of $T$, say $T_c$, where strong fluctuations between\nindividual states lead to a disordered situation in which there is no majority:\nthe numbers of sellers and buyers are equal, namely the market clearing. We\nhave considered three models: $q=3$ ( sell, buy, wait), $q=5$ (5 states between\nabsolutely buy and absolutely sell), and $q=\\infty$. The specific measure, by\nthe government or by economic organisms, is parameterized by $H$ applied on the\nmarket at the time $t_1$ and removed at the time $t_2$. We have used Monte\nCarlo simulations to study the time evolution of the price as functions of\nthose parameters. Many striking results are obtained. In particular we show\nthat the price strongly fluctuates near $T_c$ and there exists a critical value\n$H_c$ above which the boosting effect remains after $H$ is removed. This\nhappens only if $H$ is applied in the critical region. Otherwise, the effect of\n$H$ lasts only during the time of the application of $H$. The second party of\nthe paper deals with the price variation using a time-dependent mean-field\ntheory. By supposing that the sellers and the buyers belong to two distinct\ncommunities with their characteristics different in both intra-group and\ninter-group interactions, we find the price oscillation with time.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11665v2"
    },
    {
        "title": "Estimation of the yield curve for Costa Rica using combinatorial\n  optimization metaheuristics applied to nonlinear regression",
        "authors": [
            "Andres Quiros-Granados",
            "JAvier Trejos-Zelaya"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  The term structure of interest rates or yield curve is a function relating\nthe interest rate with its own term. Nonlinear regression models of\nNelson-Siegel and Svensson were used to estimate the yield curve using a sample\nof historical data supplied by the National Stock Exchange of Costa Rica. The\noptimization problem involved in the estimation process of model parameters is\naddressed by the use of four well known combinatorial optimization\nmetaheuristics: Ant colony optimization, Genetic algorithm, Particle swarm\noptimization and Simulated annealing. The aim of the study is to improve the\nlocal minima obtained by a classical quasi-Newton optimization method using a\ndescent direction. Good results with at least two metaheuristics are achieved,\nParticle swarm optimization and Simulated annealing. Keywords: Yield curve,\nnonlinear regression, Nelson-\n",
        "pdf_link": "http://arxiv.org/pdf/2001.00920v1"
    },
    {
        "title": "Nonparametric sign prediction of high-dimensional correlation matrix\n  coefficients",
        "authors": [
            "Christian Bongiorno",
            "Damien Challet"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We introduce a method to predict which correlation matrix coefficients are\nlikely to change their signs in the future in the high-dimensional regime, i.e.\nwhen the number of features is larger than the number of samples per feature.\nThe stability of correlation signs, two-by-two relationships, is found to\ndepend on three-by-three relationships inspired by Heider social cohesion\ntheory in this regime. We apply our method to US and Hong Kong equities\nhistorical data to illustrate how the structure of correlation matrices\ninfluences the stability of the sign of its coefficients.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.11214v1"
    },
    {
        "title": "Inequality, a scourge of the XXI century",
        "authors": [
            "José Roberto Iglesias",
            "Ben-Hur Francisco Cardoso",
            "Sebastián Gonçalves"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Social and economic inequality is a plague of the XXI Century. It is\ncontinuously widening, as the wealth of a relatively small group increases and,\ntherefore, the rest of the world shares a shrinking fraction of resources. This\nsituation has been predicted and denounced by economists and econophysicists.\nThe latter ones have widely used models of market dynamics which consider that\nwealth distribution is the result of wealth exchanges among economic agents. A\nsimple analogy relates the wealth in a society with the kinetic energy of the\nmolecules in a gas, and the trade between agents to the energy exchange between\nthe molecules during collisions. However, while in physical systems, thanks to\nthe equipartition of energy, the gas eventually arrives at an equilibrium\nstate, in many exchange models the economic system never equilibrates. Instead,\nit moves toward a \"condensed\" state, where one or a few agents concentrate all\nthe wealth of the society and the rest of agents shares zero or a very small\nfraction of the total wealth. Here we discuss two ways of avoiding the\n\"condensed\" state. On one hand, we consider a regulatory policy that favors the\npoorest agent in the exchanges, thus increasing the probability that the wealth\ngoes from the richest to the poorest agent. On the other hand, we study a tax\nsystem and its effects on wealth distribution. We compare the redistribution\nprocesses and conclude that complete control of the inequalities can be\nattained with simple regulations or interventions.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.06106v1"
    },
    {
        "title": "Egalitarian and Just Digital Currency Networks",
        "authors": [
            "Gal Shahaf",
            "Ehud Shapiro",
            "Nimrod Talmon"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Cryptocurrencies are a digital medium of exchange with decentralized control\nthat renders the community operating the cryptocurrency its sovereign. Leading\ncryptocurrencies use proof-of-work or proof-of-stake to reach consensus, thus\nare inherently plutocratic. This plutocracy is reflected not only in control\nover execution, but also in the distribution of new wealth, giving rise to\n``rich get richer'' phenomena. Here, we explore the possibility of an\nalternative digital currency that is egalitarian in control and just in the\ndistribution of created wealth. Such currencies can form and grow in grassroots\nand sybil-resilient way. A single currency community can achieve distributive\njustice by egalitarian coin minting, whereby each member mints one coin at\nevery time step. Egalitarian minting results, in the limit, in the dilution of\nany inherited assets and in each member having an equal share of the minted\ncurrency, adjusted by the relative productivity of the members. Our main\ntheorem shows that a currency network, where agents can be members of more than\none currency community, can achieve distributive justice globally across the\nnetwork by joint egalitarian minting, whereby each agent mints one coin in only\none community at each timestep. Specifically, we show that a sufficiently large\nintersection between two communities -- relative to the gap in their\nproductivity -- will cause the exchange rates between their currencies to\nconverge to 1:1, resulting in global distributive justice.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.14631v3"
    },
    {
        "title": "Super-App Behavioral Patterns in Credit Risk Models: Financial,\n  Statistical and Regulatory Implications",
        "authors": [
            "Luisa Roa",
            "Alejandro Correa-Bahnsen",
            "Gabriel Suarez",
            "Fernando Cortés-Tejada",
            "María A. Luque",
            "Cristián Bravo"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In this paper we present the impact of alternative data that originates from\nan app-based marketplace, in contrast to traditional bureau data, upon credit\nscoring models. These alternative data sources have shown themselves to be\nimmensely powerful in predicting borrower behavior in segments traditionally\nunderserved by banks and financial institutions. Our results, validated across\ntwo countries, show that these new sources of data are particularly useful for\npredicting financial behavior in low-wealth and young individuals, who are also\nthe most likely to engage with alternative lenders. Furthermore, using the\nTreeSHAP method for Stochastic Gradient Boosting interpretation, our results\nalso revealed interesting non-linear trends in the variables originating from\nthe app, which would not normally be available to traditional banks. Our\nresults represent an opportunity for technology companies to disrupt\ntraditional banking by correctly identifying alternative data sources and\nhandling this new information properly. At the same time alternative data must\nbe carefully validated to overcome regulatory hurdles across diverse\njurisdictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.14658v2"
    },
    {
        "title": "A Cultural Market Model",
        "authors": [
            "Amac Herdagdelen",
            "Haluk Bingol"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  Social interactions and personal tastes shape our consumption behavior of\ncultural products. In this study, we present a computational model of a\ncultural market and we aim to analyze the behavior of the consumer population\nas an emergent phenomena. Our results suggest that the final market shares of\ncultural products dramatically depend on consumer heterogeneity and social\ninteraction pressure. Furthermore, the relation between the resulting market\nshares and social interaction is robust with respect to a wide range of\nvariation in the parameter values and the type of topology.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.2341v2"
    },
    {
        "title": "The Food Crises: A quantitative model of food prices including\n  speculators and ethanol conversion",
        "authors": [
            "Marco Lagi",
            "Yavni Bar-Yam",
            "Karla Z. Bertrand",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  Recent increases in basic food prices are severely impacting vulnerable\npopulations worldwide. Proposed causes such as shortages of grain due to\nadverse weather, increasing meat consumption in China and India, conversion of\ncorn to ethanol in the US, and investor speculation on commodity markets lead\nto widely differing implications for policy. A lack of clarity about which\nfactors are responsible reinforces policy inaction. Here, for the first time,\nwe construct a dynamic model that quantitatively agrees with food prices. The\nresults show that the dominant causes of price increases are investor\nspeculation and ethanol conversion. Models that just treat supply and demand\nare not consistent with the actual price dynamics. The two sharp peaks in\n2007/2008 and 2010/2011 are specifically due to investor speculation, while an\nunderlying upward trend is due to increasing demand from ethanol conversion.\nThe model includes investor trend following as well as shifting between\ncommodities, equities and bonds to take advantage of increased expected\nreturns. Claims that speculators cannot influence grain prices are shown to be\ninvalid by direct analysis of price setting practices of granaries. Both causes\nof price increase, speculative investment and ethanol conversion, are promoted\nby recent regulatory changes---deregulation of the commodity markets, and\npolicies promoting the conversion of corn to ethanol. Rapid action is needed to\nreduce the impacts of the price increases on global hunger.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4859v1"
    },
    {
        "title": "On the global economic potentials and marginal costs of non-renewable\n  resources and the price of energy commodities",
        "authors": [
            "Jean-Francois Mercure",
            "Pablo Salas"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  A model is presented in this work for simulating endogenously the evolution\nof the marginal costs of production of energy carriers from non-renewable\nresources, their consumption, depletion pathways and timescales. Such marginal\ncosts can be used to simulate the long term average price formation of energy\ncommodities. Drawing on previous work where a global database of energy\nresource economic potentials was constructed, this work uses cost distributions\nof non-renewable resources in order to evaluate global flows of energy\ncommodities. A mathematical framework is given to calculate endogenous flows of\nenergy resources given an exogenous commodity price path. This framework can be\nused in reverse in order to calculate an exogenous marginal cost of production\nof energy carriers given an exogenous carrier demand. Using rigid price\ninelastic assumptions independent of the economy, these two approaches generate\nlimiting scenarios that depict extreme use of natural resources. This is useful\nto characterise the current state and possible uses of remaining non-renewable\nresources such as fossil fuels and natural uranium. The theory is however\ndesigned for use within economic or technology models that allow technology\nsubstitutions. In this work, it is implemented in the global power sector model\nFTT:Power. Policy implications are given.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0708v3"
    },
    {
        "title": "Entanglement between Demand and Supply in Markets with Bandwagon Goods",
        "authors": [
            "Mirta B. Gordon",
            "Jean-Pierre Nadal",
            "Denis Phan",
            "Viktoriya Semeshenko"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Whenever customers' choices (e.g. to buy or not a given good) depend on\nothers choices (cases coined 'positive externalities' or 'bandwagon effect' in\nthe economic literature), the demand may be multiply valued: for a same posted\nprice, there is either a small number of buyers, or a large one -- in which\ncase one says that the customers coordinate. This leads to a dilemma for the\nseller: should he sell at a high price, targeting a small number of buyers, or\nat low price targeting a large number of buyers? In this paper we show that the\ninteraction between demand and supply is even more complex than expected,\nleading to what we call the curse of coordination: the pricing strategy for the\nseller which aimed at maximizing his profit corresponds to posting a price\nwhich, not only assumes that the customers will coordinate, but also lies very\nnear the critical price value at which such high demand no more exists. This is\nobtained by the detailed mathematical analysis of a particular model formally\nrelated to the Random Field Ising Model and to a model introduced in social\nsciences by T C Schelling in the 70's.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.1321v2"
    },
    {
        "title": "How is non-knowledge represented in economic theory?",
        "authors": [
            "Ekaterina Svetlova",
            "Henk van Elst"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  In this article, we address the question of how non-knowledge about future\nevents that influence economic agents' decisions in choice settings has been\nformally represented in economic theory up to date. To position our discussion\nwithin the ongoing debate on uncertainty, we provide a brief review of\nhistorical developments in economic theory and decision theory on the\ndescription of economic agents' choice behaviour under conditions of\nuncertainty, understood as either (i) ambiguity, or (ii) unawareness.\nAccordingly, we identify and discuss two approaches to the formalisation of\nnon-knowledge: one based on decision-making in the context of a state space\nrepresenting the exogenous world, as in Savage's axiomatisation and some\nsuccessor concepts (ambiguity as situations with unknown probabilities), and\none based on decision-making over a set of menus of potential future\nopportunities, providing the possibility of derivation of agents' subjective\nstate spaces (unawareness as situation with imperfect subjective knowledge of\nall future events possible). We also discuss impeding challenges of the\nformalisation of non-knowledge.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.2204v1"
    },
    {
        "title": "The European debt crisis: Defaults and market equilibrium",
        "authors": [
            "Marco Lagi",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  During the last two years, Europe has been facing a debt crisis, and Greece\nhas been at its center. In response to the crisis, drastic actions have been\ntaken, including the halving of Greek debt. Policy makers acted because\ninterest rates for sovereign debt increased dramatically. High interest rates\nimply that default is likely due to economic conditions. High interest rates\nalso increase the cost of borrowing and thus cause default to be likely. If\nthere is a departure from equilibrium, increasing interest rates may contribute\nto---rather than be caused by---default risk. Here we build a quantitative\nequilibrium model of sovereign default risk that, for the first time, is able\nto determine if markets are consistently set by economic conditions. We show\nthat over the period 2001-2012, the annually-averaged long-term interest rates\nof Greek debt are quantitatively related to the ratio of debt to GDP. The\nrelationship shows that the market consistently expects default to occur if the\nGreek debt reaches twice the GDP. Our analysis does not preclude\nnon-equilibrium increases in interest rates over shorter timeframes. We find\nevidence of such non-equilibrium fluctuations in a separate analysis. According\nto the equilibrium model, the date by which a half-default must occur is March\n2013, almost one year after the actual debt write-down. Any acceleration of\ndefault by non-equilibrium fluctuations is significant for national and\ninternational interventions. The need for austerity or bailout costs would be\nreduced if market regulations were implemented to increase market stability to\nprevent short term interest rate increases. We similarly evaluate the timing of\nprojected defaults without interventions for Portugal, Ireland, Spain and Italy\nto be March 2013, April 2014, May 2014, and July 2016, respectively. All\ndefaults are mitigated by planned interventions.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.6369v1"
    },
    {
        "title": "Corruption Risk in Contracting Markets: A Network Science Perspective",
        "authors": [
            "Johannes Wachs",
            "Mihály Fazekas",
            "János Kertész"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We use methods from network science to analyze corruption risk in a large\nadministrative dataset of over 4 million public procurement contracts from\nEuropean Union member states covering the years 2008-2016. By mapping\nprocurement markets as bipartite networks of issuers and winners of contracts\nwe can visualize and describe the distribution of corruption risk. We study the\nstructure of these networks in each member state, identify their cores and find\nthat highly centralized markets tend to have higher corruption risk. In all EU\ncountries we analyze, corruption risk is significantly clustered. However,\nthese risks are sometimes more prevalent in the core and sometimes in the\nperiphery of the market, depending on the country. This suggests that the same\nlevel of corruption risk may have entirely different distributions. Our\nframework is both diagnostic and prescriptive: it roots out where corruption is\nlikely to be prevalent in different markets and suggests that different\nanti-corruption policies are needed in different countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.08664v1"
    },
    {
        "title": "Scaling in Income Inequalities and its Dynamical Origin",
        "authors": [
            "Zoltan Neda",
            "Istvan Gere",
            "Tamas S. Biro",
            "Geza Toth",
            "Noemi Derzsy"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We provide an analytically treatable model that describes in a unified manner\nincome distribution for all income categories. The approach is based on a\nmaster equation with growth and reset terms. The model assumptions on the\ngrowth and reset rates are tested on an exhaustive database with incomes on\nindividual level spanning a nine year period in the Cluj county (Romania). In\nagreement with our theoretical predictions we find that income distributions\ncomputed for several years collapse on a master-curve when a properly\nnormalised income is considered. The Beta Prime distribution is appropriate to\nfit the collapsed data and it is shown that distributions derived for other\ncountries are following similar trends with different fit parameters. The\nnon-universal feature of the fit parameters suggests that for a more realistic\nmodelling the model parameters have to be linked with specific socio-economic\nregulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.02449v2"
    },
    {
        "title": "Earnings Prediction with Deep Learning",
        "authors": [
            "Lars Elend",
            "Sebastian A. Tideman",
            "Kerstin Lopatta",
            "Oliver Kramer"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In the financial sector, a reliable forecast the future financial performance\nof a company is of great importance for investors' investment decisions. In\nthis paper we compare long-term short-term memory (LSTM) networks to temporal\nconvolution network (TCNs) in the prediction of future earnings per share\n(EPS). The experimental analysis is based on quarterly financial reporting data\nand daily stock market returns. For a broad sample of US firms, we find that\nboth LSTMs outperform the naive persistent model with up to 30.0% more accurate\npredictions, while TCNs achieve and an improvement of 30.8%. Both types of\nnetworks are at least as accurate as analysts and exceed them by up to 12.2%\n(LSTM) and 13.2% (TCN).\n",
        "pdf_link": "http://arxiv.org/pdf/2006.03132v2"
    },
    {
        "title": "Why Stake When You Can Borrow?",
        "authors": [
            "Tarun Chitra",
            "Alex Evans"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  As smart contract platforms autonomously manage billions of dollars of\ncapital, quantifying the portfolio risk that investors engender in these\nsystems is increasingly important. Recent work illustrates that Proof of Stake\n(PoS) is vulnerable to financial attacks arising from on-chain lending and has\nworse capital efficiency than Proof of Work (PoW) \\cite{fanti_pos_econ}.\nNumerous methods for improving capital efficiency have been proposed that allow\nstakers to create fungible derivative claims on their staked assets. In this\npaper, we construct a unifying model for studying the security risks of these\nproposals. This model combines birth-death P\\'olya processes and risk models\nadapted from the credit derivatives literature to assess token inequality and\nreturn profiles. We find that there is a sharp transition between 'safe' and\n'unsafe' derivative usage. Surprisingly, we find that contrary to\n\\cite{fanti2019compounding} there exist conditions where derivatives can\n\\emph{reduce} concentration of wealth in these networks. This model also\napplies to Decentralized Finance (DeFi) protocols where staked assets are used\nas insurance. Our theoretical results are validated using agent-based\nsimulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.11156v1"
    },
    {
        "title": "Determining Secondary Attributes for Credit Evaluation in P2P Lending",
        "authors": [
            "Revathi Bhuvaneswari",
            "Antonio Segalini"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  There has been an increased need for secondary means of credit evaluation by\nboth traditional banking organizations as well as peer-to-peer lending\nentities. This is especially important in the present technological era where\nsticking with strict primary credit histories doesn't help distinguish between\na 'good' and a 'bad' borrower, and ends up hurting both the individual borrower\nas well as the investor as a whole. We utilized machine learning classification\nand clustering algorithms to accurately predict a borrower's creditworthiness\nwhile identifying specific secondary attributes that contribute to this score.\nWhile extensive research has been done in predicting when a loan would be fully\npaid, the area of feature selection for lending is relatively new. We achieved\n65% F1 and 73% AUC on the LendingClub data while identifying key secondary\nattributes.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.13921v1"
    },
    {
        "title": "Applications of Mean Field Games in Financial Engineering and Economic\n  Theory",
        "authors": [
            "Rene Carmona"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This is an expanded version of the lecture given at the AMS Short Course on\nMean Field Games, on January 13, 2020 in Denver CO. The assignment was to\ndiscuss applications of Mean Field Games in finance and economics. I need to\nadmit upfront that several of the examples reviewed in this chapter were\nalready discussed in book form. Still, they are here accompanied with\ndiscussions of, and references to, works which appeared over the last three\nyears. Moreover, several completely new sections are added to show how recent\ndevelopments in financial engineering and economics can benefit from being\nviewed through the lens of the Mean Field Game paradigm. The new financial\nengineering applications deal with bitcoin mining and the energy markets, while\nthe new economic applications concern models offering a smooth transition\nbetween macro-economics and finance, and contract theory.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.05237v1"
    },
    {
        "title": "Complex decision-making strategies in a stock market experiment\n  explained as the combination of few simple strategies",
        "authors": [
            "Gael Poux-Medard",
            "Sergio Cobo-Lopez",
            "Jordi Duch",
            "Roger Guimera",
            "Marta Sales-Pardo"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Many studies have shown that there are regularities in the way human beings\nmake decisions. However, our ability to obtain models that capture such\nregularities and can accurately predict unobserved decisions is still limited.\nWe tackle this problem in the context of individuals who are given information\nrelative to the evolution of market prices and asked to guess the direction of\nthe market. We use a networks inference approach with stochastic block models\n(SBM) to find the model and network representation that is most predictive of\nunobserved decisions. Our results suggest that users mostly use recent\ninformation (about the market and about their previous decisions) to guess.\nFurthermore, the analysis of SBM groups reveals a set of strategies used by\nplayers to process information and make decisions that is analogous to\nbehaviors observed in other contexts. Our study provides and example on how to\nquantitatively explore human behavior strategies by representing decisions as\nnetworks and using rigorous inference and model-selection approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.06121v1"
    },
    {
        "title": "Ecological analysis of world trade",
        "authors": [
            "Leonardo Ermann",
            "Dima L. Shepelyansky"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  Ecological systems have a high level of complexity combined with stability\nand rich biodiversity. Recently, the analysis of their properties and evolution\nhas been pushed forward on a basis of concept of mutualistic networks that\nprovides a detailed understanding of their features being linked to a high\nnestedness of these networks. It was shown that the nestedness architecture of\nmutualistic networks of plants and their pollinators minimizes competition and\nincreases biodiversity. Here, using the United Nations COMTRADE database for\nyears 1962 - 2009, we show that a similar ecological analysis gives a valuable\ndescription of the world trade. In fact the countries and trade products are\nanalogous to plants and pollinators, and the whole trade network is\ncharacterized by a low nestedness temperature which is typical for the\necological networks. This approach provides new mutualistic features of the\nworld trade highlighting new significance of countries and trade products for\nthe world trade.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3584v1"
    },
    {
        "title": "No need for conspiracy: Self-organized cartel formation in a modified\n  trust game",
        "authors": [
            "Tiago P. Peixoto",
            "Stefan Bornholdt"
        ],
        "category": "q-fin.GN",
        "published_year": "2012",
        "summary": "  We investigate the dynamics of a trust game on a mixed population where\nindividuals with the role of buyers are forced to play against a predetermined\nnumber of sellers, whom they choose dynamically. Agents with the role of\nsellers are also allowed to adapt the level of value for money of their\nproducts, based on payoff. The dynamics undergoes a transition at a specific\nvalue of the strategy update rate, above which an emergent cartel organization\nis observed, where sellers have similar values of below optimal value for\nmoney. This cartel organization is not due to an explicit collusion among\nagents; instead it arises spontaneously from the maximization of the individual\npayoffs. This dynamics is marked by large fluctuations and a high degree of\nunpredictability for most of the parameter space, and serves as a plausible\nqualitative explanation for observed elevated levels and fluctuations of\ncertain commodity prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3798v3"
    },
    {
        "title": "Informational Efficiency under Short Sale Constraints",
        "authors": [
            "Robert A. Jarrow",
            "Martin Larsson"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  A constrained informationally efficient market is defined to be one whose\nprice process arises as the outcome of some equilibrium where agents face\nrestrictions on trade. This paper investigates the case of short sale\nconstraints, a setting which despite its simplicity, generates new insights. In\nparticular, it is shown that short sale constrained informationally efficient\nmarkets always admit equivalent supermartingale measures and local martingale\ndeflators, but not necessarily local martingale measures. And if in addition\nsome local martingale deflator turns the price process into a true martingale,\nthen the market is constrained informationally efficient. Examples are given to\nillustrate the subtle phenomena that can arise in the presence of short sale\nconstraints, with particular attention to representative agent equilibria and\nthe different notions of no arbitrage.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.1851v1"
    },
    {
        "title": "Bartering integer commodities with exogenous prices",
        "authors": [
            "Stefano Nasini",
            "Jordi Castro",
            "Pau Fonseca i Casas"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  The analysis of markets with indivisible goods and fixed exogenous prices has\nplayed an important role in economic models, especially in relation to wage\nrigidity and unemployment. This research report provides a mathematical and\ncomputational details associated to the mathematical programming based\napproaches proposed by Nasini et al. (accepted 2014) to study pure exchange\neconomies where discrete amounts of commodities are exchanged at fixed prices.\nBarter processes, consisting in sequences of elementary reallocations of couple\nof commodities among couples of agents, are formalized as local searches\nconverging to equilibrium allocations. A direct application of the analyzed\nprocesses in the context of computational economics is provided, along with a\nJava implementation of the approaches described in this research report.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.3145v3"
    },
    {
        "title": "A Multiple Network Approach to Corporate Governance",
        "authors": [
            "Fausto Bonacina",
            "Marco D'Errico",
            "Enrico Moretto",
            "Silvana Stefani",
            "Anna Torriero"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  In this work, we consider Corporate Governance (CG) ties among companies from\na multiple network perspective. Such a structure naturally arises from the\nclose interrelation between the Shareholding Network (SH) and the Board of\nDirectors network (BD). In order to capture the simultaneous effects of both\nnetworks on CG, we propose to model the CG multiple network structure via\ntensor analysis. In particular, we consider the TOPHITS model, based on the\nPARAFAC tensor decomposition, to show that tensor techniques can be\nsuccessfully applied in this context. By providing some empirical results from\nthe Italian financial market in the univariate case, we then show that a\ntensor--based multiple network approach can reveal important information.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.4387v2"
    },
    {
        "title": "Microeconomic Structure determines Macroeconomic Dynamics. Aoki defeats\n  the Representative Agent",
        "authors": [
            "Sorin Solomon",
            "Natasa Golo"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  Masanao Aoki developed a new methodology for a basic problem of economics:\ndeducing rigorously the macroeconomic dynamics as emerging from the\ninteractions of many individual agents. This includes deduction of the fractal\n/ intermittent fluctuations of macroeconomic quantities from the granularity of\nthe mezo-economic collective objects (large individual wealth, highly\nproductive geographical locations, emergent technologies, emergent economic\nsectors) in which the micro-economic agents self-organize.\n  In particular, we present some theoretical predictions, which also met\nextensive validation from empirical data in a wide range of systems: - The\nfractal Levy exponent of the stock market index fluctuations equals the Pareto\nexponent of the investors wealth distribution. The origin of the macroeconomic\ndynamics is therefore found in the granularity induced by the wealth / capital\nof the wealthiest investors. - Economic cycles consist of a Schumpeter\n'creative destruction' pattern whereby the maxima are cusp-shaped while the\nminima are smooth. In between the cusps, the cycle consists of the sum of 2\n'crossing exponentials': one decaying and the other increasing.\n  This unification within the same theoretical framework of short term market\nfluctuations and long term economic cycles offers the perspective of a genuine\nconceptual synthesis between micro- and macroeconomics. Joining another giant\nof contemporary science - Phil Anderson - Aoki emphasized the role of rare,\nlarge fluctuations in the emergence of macroeconomic phenomena out of\nmicroscopic interactions and in particular their non self-averaging, in the\nlanguage of statistical physics. In this light, we present a simple stochastic\nmulti-sector growth model.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.7496v1"
    },
    {
        "title": "Propagation of Systemic Risk in Interbank Networks",
        "authors": [
            "Vanessa Hoffmann de Quadros",
            "Juan Carlos González-Avella",
            "José Roberto Iglesias"
        ],
        "category": "q-fin.GN",
        "published_year": "2014",
        "summary": "  This work explores the characteristics of financial contagion in networks\nwhose links distributions approaches a power law, using a model that defines\nbanks balance sheets from information of network connectivity. By varying the\nparameters for the creation of the network, several interbank networks are\nbuilt, in which the concentrations of debts and credits are obtained from links\ndistributions during the creation networks process. Three main types of\ninterbank network are analyzed for their resilience to contagion: i)\nconcentration of debts is greater than concentration of credits, ii)\nconcentration of credits is greater than concentration of debts and iii)\nconcentrations of debts and credits are similar. We also tested the effect of a\nvariation in connectivity in conjunction with variation in concentration of\nlinks. The results suggest that more connected networks with high concentration\nof credits (featuring nodes that are large creditors of the system) present\ngreater resilience to contagion when compared with the others networks\nanalyzed. Evaluating some topological indices of systemic risk suggested by the\nliterature we have verified the ability of these indices to explain the impact\non the system caused by the failure of a node. There is a clear positive\ncorrelation between the topological indices and the magnitude of losses in the\ncase of networks with high concentration of debts. This correlation is smaller\nfor more resilient networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2549v1"
    },
    {
        "title": "Minimizing the Probability of Ruin in Retirement",
        "authors": [
            "Christopher J. Rook"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  Retirees who exhaust their savings while still alive are said to experience\nfinancial ruin. These savings are typically grown during the accumulation phase\nthen spent during the retirement decumulation phase. Extensive research into\ninvest-and-harvest decumulation strategies has been conducted, but\nrecommendations differ markedly. This has likely been a source of concern and\nconfusion for the retiree. Our goal is to find what has heretofore been\nelusive, namely an optimal decumulation strategy. Optimality implies that no\nalternate strategy exists or can be constructed that delivers a lower\nprobability of ruin, given a fixed inflation-adjusted withdrawal rate.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.00419v1"
    },
    {
        "title": "Modelling income, wealth, and expenditure data by use of Econophysics",
        "authors": [
            "Elvis Oltean"
        ],
        "category": "q-fin.GN",
        "published_year": "2016",
        "summary": "  In the present paper, we identify several distributions from Physics and\nstudy their applicability to phenomena such as distribution of income, wealth,\nand expenditure. Firstly, we apply logistic distribution to these data and we\nfind that it fits very well the annual data for the entire income interval\nincluding for upper income segment of population. Secondly, we apply\nFermi-Dirac distribution to these data. We seek to explain possible\ncorrelations and analogies between economic systems and statistical\nthermodynamics systems. We try to explain their behavior and properties when we\ncorrelate physical variables with macroeconomic aggregates and indicators. Then\nwe draw some analogies between parameters of the Fermi-Dirac distribution and\nmacroeconomic variables. Thirdly, as complex systems are modeled using\npolynomial distributions, we apply polynomials to the annual sets of data and\nwe find that it fits very well also the entire income interval. Fourthly, we\ndevelop a new methodology to approach dynamically the income, wealth, and\nexpenditure distribution similarly with dynamical complex systems. This\nmethodology was applied to different time intervals consisting of consecutive\nyears up to 35 years. Finally, we develop a mathematical model based on a\nHamiltonian that maximizes utility function applied to Ramsey model using\nFermi-Dirac and polynomial utility functions. We find some theoretical\nconnections with time preference theory. We apply these distributions to a\nlarge pool of data from countries with different levels of development, using\ndifferent methods for calculation of income, wealth, and expenditure.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.08383v1"
    },
    {
        "title": "Lost in Diversification",
        "authors": [
            "Marco Bardoscia",
            "Daniele d'Arienzo",
            "Matteo Marsili",
            "Valerio Volpati"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  As financial instruments grow in complexity more and more information is\nneglected by risk optimization practices. This brings down a curtain of opacity\non the origination of risk, that has been one of the main culprits in the\n2007-2008 global financial crisis. We discuss how the loss of transparency may\nbe quantified in bits, using information theoretic concepts. We find that {\\em\ni)} financial transformations imply large information losses, {\\em ii)}\nportfolios are more information sensitive than individual stocks only if\nfundamental analysis is sufficiently informative on the co-movement of assets,\nthat {\\em iii)} securitisation, in the relevant range of parameters, yields\nassets that are less information sensitive than the original stocks and that\n{\\em iv)} when diversification (or securitisation) is at its best (i.e. when\nassets are uncorrelated) information losses are maximal. We also address the\nissue of whether pricing schemes can be introduced to deal with information\nlosses. This is relevant for the transmission of incentives to gather\ninformation on the risk origination side. Within a simple mean variance scheme,\nwe find that market incentives are not generally sufficient to make information\nharvesting sustainable.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.09795v1"
    },
    {
        "title": "Confidence Collapse in a Multi-Household, Self-Reflexive DSGE Model",
        "authors": [
            "Federico Guglielmo Morelli",
            "Michael Benzaquen",
            "Marco Tarzia",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We investigate a multi-household DSGE model in which past aggregate\nconsumption impacts the confidence, and therefore consumption propensity, of\nindividual households. We find that such a minimal setup is extremely rich, and\nleads to a variety of realistic output dynamics: high output with no crises;\nhigh output with increased volatility and deep, short lived recessions;\nalternation of high and low output states where relatively mild drop in\neconomic conditions can lead to a temporary confidence collapse and steep\ndecline in economic activity. The crisis probability depends exponentially on\nthe parameters of the model, which means that markets cannot efficiently price\nthe associated risk premium. We conclude by stressing that within our\nframework, {\\it narratives} become an important monetary policy tool, that can\nhelp steering the economy back on track.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.07425v1"
    },
    {
        "title": "Special Drawing Rights in a New Decentralized Century",
        "authors": [
            "Andreas Veneris",
            "Andreas Park"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Unfulfilled expectations from macro-economic initiatives during the Great\nRecession and the massive shift into globalization echo today with political\nupheaval, anti-establishment propaganda, and looming trade/currency wars that\nthreaten domestic and international value chains. Once stable entities like the\nEU now look fragile and political instability in the US presents unprecedented\nchallenges to an International Monetary System (IMS) that predominantly relies\non the USD and EUR as reserve currencies. In this environment, it is critical\nfor an international organization mandated to ensure stability to plan and act\nahead. This paper argues that Decentralized Ledger-based technology (DLT) is\nkey for the International Monetary Fund (IMF) to mitigate some of those risks,\npromote stability and safeguard world prosperity. Over the last two years, DLT\nhas made headline news globally and created a worldwide excitement not seen\nsince the internet entered the mainstream. The rapid adoption and open-to-all\nphilosophy of DLT has already redefined global socioeconomics, promises to\nshake up the world of commerce/finance and challenges the workings of central\ngovernments/regulators. This paper examines DLT core premises and proposes a\ntwo-step approach for the IMF to expand Special Drawing Rights (SDR) into that\nsphere so as to become the originally envisioned numeraire and reserve currency\nfor cross-border transactions in this new decentralized century.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11057v1"
    },
    {
        "title": "On the Statistical Differences between Binary Forecasts and Real World\n  Payoffs",
        "authors": [
            "Nassim Nicholas Taleb"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  What do binary (or probabilistic) forecasting abilities have to do with\noverall performance? We map the difference between (univariate) binary\npredictions, bets and \"beliefs\" (expressed as a specific \"event\" will\nhappen/will not happen) and real-world continuous payoffs (numerical benefits\nor harm from an event) and show the effect of their conflation and\nmischaracterization in the decision-science literature. We also examine the\ndifferences under thin and fat tails. The effects are:\n  A- Spuriousness of many psychological results particularly those documenting\nthat humans overestimate tail probabilities and rare events, or that they\noverreact to fears of market crashes, ecological calamities, etc. Many\nperceived \"biases\" are just mischaracterizations by psychologists. There is\nalso a misuse of Hayekian arguments in promoting prediction markets.\n  We quantify such conflations with a metric for \"pseudo-overestimation\".\n  B- Being a \"good forecaster\" in binary space doesn't lead to having a good\nactual performance}, and vice versa, especially under nonlinearities. A binary\nforecasting record is likely to be a reverse indicator under some classes of\ndistributions. Deeper uncertainty or more complicated and realistic probability\ndistribution worsen the conflation .\n  C- Machine Learning: Some nonlinear payoff functions, while not lending\nthemselves to verbalistic expressions and \"forecasts\", are well captured by ML\nor expressed in option contracts.\n  D- Fattailedness: The difference is exacerbated in the power law classes of\nprobability distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11162v3"
    },
    {
        "title": "Recommendation Engine for Lower Interest Borrowing on Peer to Peer\n  Lending (P2PL) Platform",
        "authors": [
            "Ke Ren",
            "Avinash Malik"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  Online Peer to Peer Lending (P2PL) systems connect lenders and borrowers\ndirectly, thereby making it convenient to borrow and lend money without\nintermediaries such as banks. Many recommendation systems have been developed\nfor lenders to achieve higher interest rates and avoid defaulting loans.\nHowever, there has not been much research in developing recommendation systems\nto help borrowers make wise decisions. On P2PL platforms, borrowers can either\napply for bidding loans, where the interest rate is determined by lenders\nbidding on a loan or traditional loans where the P2PL platform determines the\ninterest rate. Different borrower grades -- determining the credit worthiness\nof borrowers get different interest rates via these two mechanisms. Hence, it\nis essential to determine which type of loans borrowers should apply for. In\nthis paper, we build a recommendation system that recommends to any new\nborrower the type of loan they should apply for. Using our recommendation\nsystem, any borrower can achieve lowered interest rates with a higher\nlikelihood of getting funded.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11634v1"
    },
    {
        "title": "A tale of two sentiment scales: Disentangling short-run and long-run\n  components in multivariate sentiment dynamics",
        "authors": [
            "Danilo Vassallo",
            "Giacomo Bormetti",
            "Fabrizio Lillo"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  We propose a novel approach to sentiment data filtering for a portfolio of\nassets. In our framework, a dynamic factor model drives the evolution of the\nobserved sentiment and allows to identify two distinct components: a long-term\ncomponent, modeled as a random walk, and a short-term component driven by a\nstationary VAR(1) process. Our model encompasses alternative approaches\navailable in literature and can be readily estimated by means of Kalman\nfiltering and expectation maximization. This feature makes it convenient when\nthe cross-sectional dimension of the portfolio increases. By applying the model\nto a portfolio of Dow Jones stocks, we find that the long term component\nco-integrates with the market principal factor, while the short term one\ncaptures transient swings of the market associated with the idiosyncratic\ncomponents and captures the correlation structure of returns. Using quantile\nregressions, we assess the significance of the contemporaneous and lagged\nexplanatory power of sentiment on returns finding strong statistical evidence\nwhen extreme returns, especially negative ones, are considered. Finally, the\nlagged relation is exploited in a portfolio allocation exercise.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01407v4"
    },
    {
        "title": "Optimal implementation delay of taxation with trade-off for Lévy\n  risk Processes",
        "authors": [
            "Wenyuan Wang",
            "Xueyuan Wu",
            "Cheng Chi"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In this paper we consider two problems on optimal implementation delay of\ntaxation with trade-off for spectrally negative L\\'{e}vy insurance risk\nprocesses. In the first case, we assume that an insurance company starts to pay\ntax when its surplus reaches a certain level $b$ and at the termination time of\nthe business there is a terminal value incurred to the company. The total\nexpected discounted value of tax payments plus the terminal value is maximized\nto obtain the optimal implementation level $b^*$. In the second case, the\ncompany still pays tax subject to an implementation level $a$ but with capital\ninjections to prevent bankruptcy. The total expected discounted value of tax\npayments minus the capital injection costs is maximized to obtain the optimal\nimplementation level $a^*$. Numerical examples are also given to illustrate the\nmain results in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08158v1"
    },
    {
        "title": "A multilevel analysis to systemic exposure: insights from local and\n  system-wide information",
        "authors": [
            "Yérali Gandica",
            "Sophie Béreau",
            "Jean-Yves Gnabo"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In the aftermath of the financial crisis, the growing literature on financial\nnetworks has widely documented the predictive power of topological\ncharacteristics (e.g. degree centrality measures) to explain the systemic\nimpact or systemic vulnerability of financial institutions. In this work, we\nshow that considering alternative topological measures based on local\nsub-network environment improves our ability to identify systemic institutions.\nTo provide empirical evidence, we apply a two-step procedure. First, we recover\nnetwork communities (i.e. close-peer environment) on a spillover network of\nfinancial institutions. Second, we regress alternative measures of\nvulnerability on three levels of topological measures: the global level (i.e.\nfirm topological characteristics computed over the whole system), local level\n(i.e. firm topological characteristics computed over the community) and\naggregated level by averaging individual characteristics over the community.\nThe sample includes $46$ financial institutions (banks, broker-dealers,\ninsurance and real-estate companies) listed in the Standard \\& Poor's 500\nindex. Our results confirm the informational content of topological metrics\nbased on close-peer environment. Such information is different from the one\nembeds in traditional system wide topological metrics and is proved to be\npredictor of distress for financial institutions in time of crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08611v1"
    },
    {
        "title": "Ascertaining price formation in cryptocurrency markets with DeepLearning",
        "authors": [
            "Fan Fang",
            "Waichung Chung",
            "Carmine Ventre",
            "Michail Basios",
            "Leslie Kanthan",
            "Lingbo Li",
            "Fan Wu"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The cryptocurrency market is amongst the fastest-growing of all the financial\nmarkets in the world. Unlike traditional markets, such as equities, foreign\nexchange and commodities, cryptocurrency market is considered to have larger\nvolatility and illiquidity. This paper is inspired by the recent success of\nusing deep learning for stock market prediction. In this work, we analyze and\npresent the characteristics of the cryptocurrency market in a high-frequency\nsetting. In particular, we applied a deep learning approach to predict the\ndirection of the mid-price changes on the upcoming tick. We monitored live\ntick-level data from $8$ cryptocurrency pairs and applied both statistical and\nmachine learning techniques to provide a live prediction. We reveal that\npromising results are possible for cryptocurrencies, and in particular, we\nachieve a consistent $78\\%$ accuracy on the prediction of the mid-price\nmovement on live exchange rate of Bitcoins vs US dollars.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.00803v1"
    },
    {
        "title": "Using News Articles and Financial Data to predict the likelihood of\n  bankruptcy",
        "authors": [
            "Michael Filletti",
            "Aaron Grech"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Over the past decade, millions of companies have filed for bankruptcy. This\nhas been caused by a plethora of reasons, namely, high interest rates, heavy\ndebts and government regulations. The effect of a company going bankrupt can be\ndevastating, hurting not only workers and shareholders, but also clients,\nsuppliers and any related external companies. One of the aims of this paper is\nto provide a framework for company bankruptcy to be predicted by making use of\nfinancial figures, provided by our external dataset, in conjunction with the\nsentiment of news articles about certain sectors. News articles are used to\nattempt to quantify the sentiment on a company and its sector from an external\nperspective, rather than simply using internal figures. This work builds on\nprevious studies carried out by multiple researchers, to bring us closer to\nlessening the impact of such events.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13414v1"
    },
    {
        "title": "Data Science in Economics",
        "authors": [
            "Saeed Nosratabadi",
            "Amir Mosavi",
            "Puhong Duan",
            "Pedram Ghamisi"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This paper provides the state of the art of data science in economics.\nThrough a novel taxonomy of applications and methods advances in data science\nare investigated. The data science advances are investigated in three\nindividual classes of deep learning models, ensemble models, and hybrid models.\nApplication domains include stock market, marketing, E-commerce, corporate\nbanking, and cryptocurrency. Prisma method, a systematic literature review\nmethodology is used to ensure the quality of the survey. The findings revealed\nthat the trends are on advancement of hybrid models as more than 51% of the\nreviewed articles applied hybrid model. On the other hand, it is found that\nbased on the RMSE accuracy metric, hybrid models had higher prediction accuracy\nthan other algorithms. While it is expected the trends go toward the\nadvancements of deep learning models.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13422v1"
    },
    {
        "title": "Potential in the Schrodinger equation: estimation from empirical data",
        "authors": [
            "J. L. Subias"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  A recent model for the stock market calculates future price distributions of\na stock as a wave function of a quantum particle confined in an infinite\npotential well. In such a model the question arose as to how to estimate the\nclassical potential needed for solving the Schrodinger equation. In the present\narticle the method used in that work for evaluating the potential is described,\nin the simplest version to implement, and more sophisticated implementations\nare suggested later.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06626v1"
    },
    {
        "title": "Quantifying the Economic Impact of Extreme Shocks on Businesses using\n  Human Mobility Data: a Bayesian Causal Inference Approach",
        "authors": [
            "Takahiro Yabe",
            "Yunchang Zhang",
            "Satish Ukkusuri"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In recent years, extreme shocks, such as natural disasters, are increasing in\nboth frequency and intensity, causing significant economic loss to many cities\naround the world. Quantifying the economic cost of local businesses after\nextreme shocks is important for post-disaster assessment and pre-disaster\nplanning. Conventionally, surveys have been the primary source of data used to\nquantify damages inflicted on businesses by disasters. However, surveys often\nsuffer from high cost and long time for implementation, spatio-temporal\nsparsity in observations, and limitations in scalability. Recently, large scale\nhuman mobility data (e.g. mobile phone GPS) have been used to observe and\nanalyze human mobility patterns in an unprecedented spatio-temporal granularity\nand scale. In this work, we use location data collected from mobile phones to\nestimate and analyze the causal impact of hurricanes on business performance.\nTo quantify the causal impact of the disaster, we use a Bayesian structural\ntime series model to predict the counterfactual performances of affected\nbusinesses (what if the disaster did not occur?), which may use performances of\nother businesses outside the disaster areas as covariates. The method is tested\nto quantify the resilience of 635 businesses across 9 categories in Puerto Rico\nafter Hurricane Maria. Furthermore, hierarchical Bayesian models are used to\nreveal the effect of business characteristics such as location and category on\nthe long-term resilience of businesses. The study presents a novel and more\nefficient method to quantify business resilience, which could assist policy\nmakers in disaster preparation and relief processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11121v1"
    },
    {
        "title": "Applying Dynamic Training-Subset Selection Methods Using Genetic\n  Programming for Forecasting Implied Volatility",
        "authors": [
            "Sana Ben Hamida",
            "Wafa Abdelmalek",
            "Fathi Abid"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Volatility is a key variable in option pricing, trading and hedging\nstrategies. The purpose of this paper is to improve the accuracy of forecasting\nimplied volatility using an extension of genetic programming (GP) by means of\ndynamic training-subset selection methods. These methods manipulate the\ntraining data in order to improve the out of sample patterns fitting. When\napplied with the static subset selection method using a single training data\nsample, GP could generate forecasting models which are not adapted to some out\nof sample fitness cases. In order to improve the predictive accuracy of\ngenerated GP patterns, dynamic subset selection methods are introduced to the\nGP algorithm allowing a regular change of the training sample during evolution.\nFour dynamic training-subset selection methods are proposed based on random,\nsequential or adaptive subset selection. The latest approach uses an adaptive\nsubset weight measuring the sample difficulty according to the fitness cases\nerrors. Using real data from SP500 index options, these techniques are compared\nto the static subset selection method. Based on MSE total and percentage of non\nfitted observations, results show that the dynamic approach improves the\nforecasting performance of the generated GP models, specially those obtained\nfrom the adaptive random training subset selection method applied to the whole\nset of training samples.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.07207v1"
    },
    {
        "title": "How Macroeconomists Lost Control of Stabilization Policy: Towards Dark\n  Ages",
        "authors": [
            "Jean Bernard Chatelain",
            "Kirsten Ralf"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  This paper is a study of the history of the transplant of mathematical tools\nusing negative feedback for macroeconomic stabilization policy from 1948 to\n1975 and the subsequent break of the use of control for stabilization policy\nwhich occurred from 1975 to 1993. New-classical macroeconomists selected a\nsubset of the tools of control that favored their support of rules against\ndiscretionary stabilization policy. The Lucas critique and Kydland and\nPrescott's time-inconsistency were over-statements that led to the \"dark ages\"\nof the prevalence of the stabilization-policy-ineffectiveness idea. These\nover-statements were later revised following the success of the Taylor rule.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.00212v1"
    },
    {
        "title": "Evolutionary dynamics in financial markets with heterogeneities in\n  strategies and risk tolerance",
        "authors": [
            "Wen-Juan Xu",
            "Chen-Yang Zhong",
            "Fei Ren",
            "Tian Qiu",
            "Rong-Da Chen",
            "Yun-Xin He",
            "Li-Xin Zhong"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  In nature and human societies, the effects of homogeneous and heterogeneous\ncharacteristics on the evolution of collective behaviors are quite different\nfrom each other. It is of great importance to understand the underlying\nmechanisms of the occurrence of such differences. By incorporating pair pattern\nstrategies and reference point strategies into an agent-based model, we have\ninvestigated the coupled effects of heterogeneous investment strategies and\nheterogeneous risk tolerance on price fluctuations. In the market flooded with\nthe investors with homogeneous investment strategies or homogeneous risk\ntolerance, large price fluctuations are easy to occur. In the market flooded\nwith the investors with heterogeneous investment strategies or heterogeneous\nrisk tolerance, the price fluctuations are suppressed. For a heterogeneous\npopulation, the coexistence of investors with pair pattern strategies and\nreference point strategies causes the price to have a slow fluctuation around a\ntypical equilibrium point and both a large price fluctuation and a no-trading\nstate are avoided, in which the pair pattern strategies push the system far\naway from the equilibrium while the reference point strategies pull the system\nback to the equilibrium. A theoretical analysis indicates that the evolutionary\ndynamics in the present model is governed by the competition between different\nstrategies. The strategy that causes large price fluctuations loses more while\nthe strategy that pulls the system back to the equilibrium gains more.\nOverfrequent trading does harm to one's pursuit for more wealth.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08962v1"
    },
    {
        "title": "Balancing the Payment System",
        "authors": [
            "Tomaž Fleischman",
            "Paolo Dini"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  The increasingly complex economic and financial environment in which we live\nmakes the management of liquidity in payment systems and the economy in general\na persistent challenge. New technologies are making it possible to address this\nchallenge through alternative solutions that complement and strengthen existing\npayment systems. For example, the interbank balancing method can also be\napplied to private payment systems, complementary currencies, and trade credit\nclearing systems to provide better liquidity and risk management. In this paper\nwe introduce the concept of a balanced payment system and demonstrate the\neffects of balancing on a small example. We show how to construct a balanced\npayment subsystem that can be settled in full and, therefore, that can be\nremoved from the payment system to achieve liquidity-saving and payments\ngridlock resolution. We also briefly introduce a generalization of a payment\nsystem and of the method to balance it in the form of a specific application\n(Tetris Core Technologies), whose wider adoption could contribute to the\nfinancial stability of and better management of liquidity and risk for the\nwhole economy.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.03517v1"
    },
    {
        "title": "An Equilibrium Model for the Cross-Section of Liquidity Premia",
        "authors": [
            "Johannes Muhle-Karbe",
            "Xiaofei Shi",
            "Chen Yang"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  We study a risk-sharing economy where an arbitrary number of heterogenous\nagents trades an arbitrary number of risky assets subject to quadratic\ntransaction costs. For linear state dynamics, the forward-backward stochastic\ndifferential equations characterizing equilibrium asset prices and trading\nstrategies in this context reduce to a system of matrix-valued Riccati\nequations. We prove the existence of a unique global solution and provide\nexplicit asymptotic expansions that allow us to approximate the corresponding\nequilibrium for small transaction costs. These tractable approximation formulas\nmake it feasible to calibrate the model to time series of prices and trading\nvolume, and to study the cross-section of liquidity premia earned by assets\nwith higher and lower trading costs. This is illustrated by an empirical case\nstudy.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.13625v1"
    },
    {
        "title": "A note on the CAPM with endogenously consistent market returns",
        "authors": [
            "Andreas Krause"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  I demonstrate that with the market return determined by the equilibrium\nreturns of the CAPM, expected returns of an asset are affected by the risks of\nall assets jointly. Another implication is that the range of feasible market\nreturns will be limited and dependent on the distribution of weights in the\nmarket portfolio. A large and well diversified market with no dominating asset\nwill only return zero while a market dominated by a small number of assets will\nonly return the risk-free rate. In the limiting case of atomistic assets, we\nrecover the properties of the standard CAPM.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.10252v1"
    },
    {
        "title": "Co-optimization of Energy and Reserve with Incentives to Wind\n  Generation: Case Study",
        "authors": [
            "Yves Smeers",
            "Sebastian Martin",
            "Jose A. Aguado"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  This case study presents an analysis and quantification of the impact of the\nlack of co-optimization of energy and reserve in the presence of high\npenetration of wind energy. The methodology is developed in a companion paper,\nPart I. Two models, with and without co-optimization are confronted. The\nmodeling of reserve and the incentive to renewable as well as the calibration\nof the model are inspired by the Spanish market. A sensitivity analysis is\nperformed on configurations that differ by generation capacity, ramping\ncapability, and market parameters (available wind, Feed in Premium to wind,\ngenerators risk aversion, and reserve requirement). The models and the case\nstudy are purely illustrative but the methodology is general.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.09636v1"
    },
    {
        "title": "Supervised Neural Networks for Illiquid Alternative Asset Cash Flow\n  Forecasting",
        "authors": [
            "Tugce Karatas",
            "Federico Klinkert",
            "Ali Hirsa"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Institutional investors have been increasing the allocation of the illiquid\nalternative assets such as private equity funds in their portfolios, yet there\nexists a very limited literature on cash flow forecasting of illiquid\nalternative assets. The net cash flow of private equity funds typically follow\na J-curve pattern, however the timing and the size of the contributions and\ndistributions depend on the investment opportunities. In this paper, we develop\na benchmark model and present two novel approaches (direct vs. indirect) to\npredict the cash flows of private equity funds. We introduce a sliding window\napproach to apply on our cash flow data because different vintage year funds\ncontain different lengths of cash flow information. We then pass the data to an\nLSTM/ GRU model to predict the future cash flows either directly or indirectly\n(based on the benchmark model). We further integrate macroeconomic indicators\ninto our data, which allows us to consider the impact of market environment on\ncash flows and to apply stress testing. Our results indicate that the direct\nmodel is easier to implement compared to the benchmark model and the indirect\nmodel, but still the predicted cash flows align better with the actual cash\nflows. We also show that macroeconomic variables improve the performance of the\ndirect model whereas the impact is not obvious on the indirect model.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.02853v1"
    },
    {
        "title": "Trading styles and long-run variance of asset prices",
        "authors": [
            "Lawrence Middleton",
            "James Dodd",
            "Simone Rijavec"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  Trading styles can be classified into either trend-following or\nmean-reverting. If the net trading style is trend-following the traded asset is\nmore likely to move in the same direction it moved previously (the opposite is\ntrue if the net style is mean-reverting). The result of this is to introduce\npositive (or negative) correlations into the time series. We here explore the\neffect of these correlations on the long-run variance of the series through\nprobabilistic models designed to explicitly capture the direction of trading.\nOur theoretical insights suggests that relative to random walk models of asset\nprices the long-run variance is increased under trend-following strategies and\ncan actually be reduced under mean-reversal conditions. We apply these models\nto some of the largest US stocks by market capitalisation as well as\nhigh-frequency EUR/USD data and show that in both these settings, the ability\nto predict the asset price is generally increased relative to a random walk.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.08242v1"
    },
    {
        "title": "A transformer-based model for default prediction in mid-cap corporate\n  markets",
        "authors": [
            "Kamesh Korangi",
            "Christophe Mues",
            "Cristián Bravo"
        ],
        "category": "q-fin.GN",
        "published_year": "2021",
        "summary": "  In this paper, we study mid-cap companies, i.e. publicly traded companies\nwith less than US $10 billion in market capitalisation. Using a large dataset\nof US mid-cap companies observed over 30 years, we look to predict the default\nprobability term structure over the medium term and understand which data\nsources (i.e. fundamental, market or pricing data) contribute most to the\ndefault risk. Whereas existing methods typically require that data from\ndifferent time periods are first aggregated and turned into cross-sectional\nfeatures, we frame the problem as a multi-label time-series classification\nproblem. We adapt transformer models, a state-of-the-art deep learning model\nemanating from the natural language processing domain, to the credit risk\nmodelling setting. We also interpret the predictions of these models using\nattention heat maps. To optimise the model further, we present a custom loss\nfunction for multi-label classification and a novel multi-channel architecture\nwith differential training that gives the model the ability to use all input\ndata efficiently. Our results show the proposed deep learning architecture's\nsuperior performance, resulting in a 13% improvement in AUC (Area Under the\nreceiver operating characteristic Curve) over traditional models. We also\ndemonstrate how to produce an importance ranking for the different data sources\nand the temporal relationships using a Shapley approach specific to these\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.09902v4"
    },
    {
        "title": "Strategic Storage Investment in Electricity Markets",
        "authors": [
            "Dongwei Zhao",
            "Mehdi Jafari",
            "Audun Botterud",
            "Apurba Sakti"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Arbitrage is one important revenue source for energy storage in electricity\nmarkets. However, a large amount of storage in the market will impact the\nenergy price and reduce potential revenues. This can lead to strategic\nbehaviors of profit-seeking storage investors. To study the investors'\nstrategic storage investments, we formulate a non-cooperative game between\ncompeting investors. Each investor decides the storage investment over a long\ninvestment horizon, and operates the storage for arbitrage revenues in the\ndaily electricity market. Different investors can deploy storage with different\ncharacteristics. Their decisions are coupled due to the market price that is\ndetermined by all the investors' decisions. We use market data from California\nISO to characterize the storage impact on the market price, based on which we\nestablish a centralized optimization problem to compute the market equilibrium.\nWe show that an increasing number of investors will increase the market\ncompetition, which reduces investors' profits but increases the total invested\nstorage capacity. Furthermore, we find that a slight increase in the storage\nefficiency (e.g., increased charge and discharge efficiency) can significantly\nimprove an investor's profit share in the market.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.02290v2"
    },
    {
        "title": "Characteristics-driven returns in equilibrium",
        "authors": [
            "Guillaume Coqueret"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  We reverse-engineer the equilibrium construction process of asset prices in\norder to obtain returns which depend on firm characteristics, possibly in a\nlinear fashion. One key requirement is that agents must have demands that rely\nseparately on firm characteristics and on the log-price of assets. Market\nclearing via exogenous (non-factor driven) supply, combined with linear demands\nin characteristics, yields the sought form. The coefficients in the resulting\nlinear expressions are scaled net aggregate demands for characteristics, as\nwell as their variations, and both can be jointly estimated via panel\nregressions. Conditions underpinning asset pricing anomalies are derived and\nunderline the theoretical importance of the links between characteristics.\nEmpirically, when the number of characteristics is small, the value and\nmomentum anomalies are mostly driven by firm-specific fixed-effects, i.e.,\nlatent demands, which highlights the shortcomings of low-dimensional models.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.07865v1"
    },
    {
        "title": "Standing Forest Coin (SFC)",
        "authors": [
            "Marcelo de A. Borges",
            "Guido L. de S. Filho",
            "Cicero Inacio da Silva",
            "Anderson M. P. Barros",
            "Raul V. B. J. Britto",
            "Nivaldo M. de C. Junior",
            "Daniel F. L. de Souza"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  This article describes a proposal to create a digital currency that allows\nthe decentralized collection of resources directed to initiatives and\nactivities that aim to protect the Brazilian Amazon ecosystem by using\nblockchain and digital contracts. In addition to the digital currency, the goal\nis to design a smart contract based in oracles to ensure credibility and\nsecurity for investors and donors of financial resources invested in projects\nwithin the Standing Forest Coin (SFC - standingforest.org).\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12600v1"
    },
    {
        "title": "Do t-Statistic Hurdles Need to be Raised?",
        "authors": [
            "Andrew Y. Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Many scholars have called for raising statistical hurdles to guard against\nfalse discoveries in academic publications. I show these calls may be difficult\nto justify empirically. Published data exhibit bias: results that fail to meet\nexisting hurdles are often unobserved. These unobserved results must be\nextrapolated, which can lead to weak identification of revised hurdles. In\ncontrast, statistics that can target only published findings (e.g. empirical\nBayes shrinkage and the FDR) can be strongly identified, as data on published\nfindings is plentiful. I demonstrate these results theoretically and in an\nempirical analysis of the cross-sectional return predictability literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10275v4"
    },
    {
        "title": "Shai-am: A Machine Learning Platform for Investment Strategies",
        "authors": [
            "Jonghun Kwak",
            "Jungyu Ahn",
            "Jinho Lee",
            "Sungwoo Park"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The finance industry has adopted machine learning (ML) as a form of\nquantitative research to support better investment decisions, yet there are\nseveral challenges often overlooked in practice. (1) ML code tends to be\nunstructured and ad hoc, which hinders cooperation with others. (2) Resource\nrequirements and dependencies vary depending on which algorithm is used, so a\nflexible and scalable system is needed. (3) It is difficult for domain experts\nin traditional finance to apply their experience and knowledge in ML-based\nstrategies unless they acquire expertise in recent technologies. This paper\npresents Shai-am, an ML platform integrated with our own Python framework. The\nplatform leverages existing modern open-source technologies, managing\ncontainerized pipelines for ML-based strategies with unified interfaces to\nsolve the aforementioned issues. Each strategy implements the interface defined\nin the core framework. The framework is designed to enhance reusability and\nreadability, facilitating collaborative work in quantitative research. Shai-am\naims to be a pure AI asset manager for solving various tasks in financial\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00436v1"
    },
    {
        "title": "Deep Reinforcement Learning for Market Making Under a Hawkes\n  Process-Based Limit Order Book Model",
        "authors": [
            "Bruno Gašperov",
            "Zvonko Kostanjčar"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  The stochastic control problem of optimal market making is among the central\nproblems in quantitative finance. In this paper, a deep reinforcement\nlearning-based controller is trained on a weakly consistent, multivariate\nHawkes process-based limit order book simulator to obtain market making\ncontrols. The proposed approach leverages the advantages of Monte Carlo\nbacktesting and contributes to the line of research on market making under\nweakly consistent limit order book models. The ensuing deep reinforcement\nlearning controller is compared to multiple market making benchmarks, with the\nresults indicating its superior performance with respect to various risk-reward\nmetrics, even under significant transaction costs.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.09951v1"
    },
    {
        "title": "A Learning and Control Perspective for Microfinance",
        "authors": [
            "Christian Kurniawan",
            "Xiyu Deng",
            "Adhiraj Chakraborty",
            "Assane Gueye",
            "Niangjun Chen",
            "Yorie Nakahira"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Microfinance, despite its significant potential for poverty reduction, is\nfacing sustainability hardships due to high default rates. Although many\nmethods in regular finance can estimate credit scores and default\nprobabilities, these methods are not directly applicable to microfinance due to\nthe following unique characteristics: a) under-explored (developing) areas such\nas rural Africa do not have sufficient prior loan data for microfinance\ninstitutions (MFIs) to establish a credit scoring system; b) microfinance\napplicants may have difficulty providing sufficient information for MFIs to\naccurately predict default probabilities; and c) many MFIs use group liability\n(instead of collateral) to secure repayment. Here, we present a novel\ncontrol-theoretic model of microfinance that accounts for these\ncharacteristics. We construct an algorithm to learn microfinance decision\npolicies that achieve financial inclusion, fairness, social welfare, and\nsustainability. We characterize the convergence conditions to Pareto-optimum\nand the convergence speeds. We demonstrate, in numerous real and synthetic\ndatasets, that the proposed method accounts for the complexities induced by\ngroup liability to produce robust decisions before sufficient loans are given\nto establish credit scoring systems and for applicants whose default\nprobability cannot be accurately estimated due to missing information. To the\nbest of our knowledge, this paper is the first to connect microfinance and\ncontrol theory. We envision that the connection will enable safe learning and\ncontrol techniques to help modernize microfinance and alleviate poverty.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.12631v2"
    },
    {
        "title": "Anatomy of a Stablecoin's failure: the Terra-Luna case",
        "authors": [
            "Antonio Briola",
            "David Vidal-Tomás",
            "Yuanrong Wang",
            "Tomaso Aste"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  We quantitatively describe the main events that led to the Terra project's\nfailure in May 2022. We first review, in a systematic way, news from\nheterogeneous social media sources; we discuss the fragility of the Terra\nproject and its vicious dependence on the Anchor protocol. We hence identify\nthe crash's trigger events, analysing hourly and transaction data for Bitcoin,\nLuna, and TerraUSD. Finally, using state-of-the-art techniques from network\nscience, we study the evolution of dependency structures for 61 highly\ncapitalised cryptocurrencies during the down-market and we also highlight the\nabsence of herding behaviour analysing cross-sectional absolute deviation of\nreturns.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.13914v3"
    },
    {
        "title": "150 Years of Return Predictability Around the World: A Holistic View",
        "authors": [
            "Yang Bai"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Using new annual data of 16 developed countries across bond, equity, and\nhousing markets, I study the return predictability using the payout-price\nratios, i.e., coupon price, dividend price, and rent price. None of the 48\ncountry-asset combinations shows consistent in-sample and out-of-sample\nperformance with positive utility gain for the mean-variance investor. Only 3\n(4/2) countries show positive economic gains in their equity (housing/bond)\nmarkets. The return predictability for the representative agents' risky asset\nportfolios and wealth portfolios is even weaker, suggesting that timing the\ninvestment return of a country using payout-price ratios will not make the\ninvestors better off. The predictive regressions based on the VAR analysis by\nCochrane (2008, 2011) suggest that 14 (5) countries have predictable payout\ngrowth in the equity (housing) markets, ex., the dividend price predicts the\ndividend growth in the US. The VAR simulation using data from all the countries\ndoes not reject the null that the dividend growth is predictable. This paper\npresents firm evidence against the return predictability based on payout\nratios.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.00121v1"
    },
    {
        "title": "Systematization of Knowledge: Synthetic Assets, Derivatives, and\n  On-Chain Portfolio Management",
        "authors": [
            "Abrar Rahman",
            "Victor Shi",
            "Matthew Ding",
            "Elliot Choi"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Synthetic assets are decentralized finance (DeFi) analogues of derivatives in\nthe traditional finance (TradFi) world - financial arrangements which derive\nvalue from and are directly pegged to fluctuations in the value of an\nunderlying asset (ex: futures and options). Synthetic assets occupy a unique\nniche, serving to facilitate currency exchange, giving traders a means to\nspeculate on the value of crypto assets without directly holding them, and\npowering more complex financial tools such as yield optimizers and portfolio\nmanagement suites. Unfortunately, the academic literature on this topic is\nhighly disparate and struggles to keep up with rapid changes in the space. We\npresent the first Systematization of Knowledge (SoK) in this area, focusing on\npresenting the key mechanisms, protocols, and issues in an accessible fashion\nto highlight risks for participants as well as areas of research interest. This\npaper takes a broad perspective in establishing a general framework for\nsynthetic assets, from the ideological origins of crypto to legal barriers for\nfirms in this space, encapsulating the basic mechanisms underpinning\nderivatives markets as well as presenting data-driven analyses of major\nprotocols.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.09958v1"
    },
    {
        "title": "A Survey: Credit Sentiment Score Prediction",
        "authors": [
            "A. N. M. Sajedul Alam",
            "Junaid Bin Kibria",
            "Arnob Kumar Dey",
            "Zawad Alam",
            "Shifat Zaman",
            "Motahar Mahtab",
            "Mohammed Julfikar Ali Mahbub",
            "Annajiat Alim Rasel"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Manual approvals are still used by banks and other NGOs to approve loans. It\ntakes time and is prone to mistakes because it is controlled by a bank\nemployee. Several fields of machine learning mining technologies have been\nutilized to enhance various areas of credit rating forecast. A major goal of\nthis research is to look at current sentiment analysis techniques that are\nbeing used to generate creditworthiness.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.15293v1"
    },
    {
        "title": "Holder Recommendations using Graph Representation Learning & Link\n  Prediction",
        "authors": [
            "Rachna Saxena",
            "Abhijeet Kumar",
            "Mridul Mishra"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Lead recommendations for financial products such as funds or ETF is\npotentially challenging in investment space due to changing market scenarios,\nand difficulty in capturing financial holder's mindset and their philosophy.\nCurrent methods surface leads based on certain product categorization and\nattributes like returns, fees, category etc. to suggest similar product to\ninvestors which may not capture the holder's investment behavior holistically.\nOther reported works does subjective analysis of institutional holder's\nideology. This paper proposes a comprehensive data driven framework for\ndeveloping a lead recommendations system in holder's space for financial\nproducts like funds by using transactional history, asset flows and product\nspecific attributes. The system assumes holder's interest implicitly by\nconsidering all investment transactions made and collects possible meta\ninformation to detect holder's investment profile/persona like investment\nanticipation and investment behavior. This paper focusses on holder\nrecommendation component of framework which employs a bi-partite graph\nrepresentation of financial holders and funds using variety of attributes and\nfurther employs GraphSage model for learning representations followed by link\nprediction model for ranking recommendation for future period. The performance\nof the proposed approach is compared with baseline model i.e., content-based\nfiltering approach on metric hits at Top-k (50, 100, 200) recommendations. We\nfound that the proposed graph ML solution outperform baseline by absolute 42%,\n22% and 14% with a look ahead bias and by absolute 18%, 19% and 18% on\ncompletely unseen holders in terms of hit rate for top-k recommendations: 50,\n100 and 200 respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.09624v1"
    },
    {
        "title": "Predicting Companies' ESG Ratings from News Articles Using Multivariate\n  Timeseries Analysis",
        "authors": [
            "Tanja Aue",
            "Adam Jatowt",
            "Michael Färber"
        ],
        "category": "q-fin.GN",
        "published_year": "2022",
        "summary": "  Environmental, social and governance (ESG) engagement of companies moved into\nthe focus of public attention over recent years. With the requirements of\ncompulsory reporting being implemented and investors incorporating\nsustainability in their investment decisions, the demand for transparent and\nreliable ESG ratings is increasing. However, automatic approaches for\nforecasting ESG ratings have been quite scarce despite the increasing\nimportance of the topic. In this paper, we build a model to predict ESG ratings\nfrom news articles using the combination of multivariate timeseries\nconstruction and deep learning techniques. A news dataset for about 3,000 US\ncompanies together with their ratings is also created and released for\ntraining. Through the experimental evaluation we find out that our approach\nprovides accurate results outperforming the state-of-the-art, and can be used\nin practice to support a manual determination or analysis of ESG ratings.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.11765v1"
    },
    {
        "title": "Leveraging Machine Learning for Multichain DeFi Fraud Detection",
        "authors": [
            "Georgios Palaiokrassas",
            "Sandro Scherrers",
            "Iason Ofeidis",
            "Leandros Tassiulas"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Since the inception of permissionless blockchains with Bitcoin in 2008, it\nbecame apparent that their most well-suited use case is related to making the\nfinancial system and its advantages available to everyone seamlessly without\ndepending on any trusted intermediaries. Smart contracts across chains provide\nan ecosystem of decentralized finance (DeFi), where users can interact with\nlending pools, Automated Market Maker (AMM) exchanges, stablecoins,\nderivatives, etc. with a cumulative locked value which had exceeded 160B USD.\nWhile DeFi comes with high rewards, it also carries plenty of risks. Many\nfinancial crimes have occurred over the years making the early detection of\nmalicious activity an issue of high priority. The proposed framework introduces\nan effective method for extracting a set of features from different chains,\nincluding the largest one, Ethereum and it is evaluated over an extensive\ndataset we gathered with the transactions of the most widely used DeFi\nprotocols (23 in total, including Aave, Compound, Curve, Lido, and Yearn) based\non a novel dataset in collaboration with Covalent. Different Machine Learning\nmethods were employed, such as XGBoost and a Neural Network for identifying\nfraud accounts detection interacting with DeFi and we demonstrate that the\nintroduction of novel DeFi-related features, significantly improves the\nevaluation results, where Accuracy, Precision, Recall, F1-score and F2-score\nwhere utilized.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.07972v1"
    },
    {
        "title": "A Theory of Complex Adaptive Learning and a Non-Localized Wave Equation\n  in Quantum Mechanics",
        "authors": [
            "Leilei Shi",
            "Xinshuai Guo",
            "Jiuchang Wei",
            "Wei Zhang",
            "Guocheng Wang",
            "Bing-Hong Wang"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Complex adaptive learning is intelligent. It is adaptive, learns in feedback\nloops, and generates hidden patterns as many individuals, elements or particles\ninteract in complex adaptive systems (CAS). CAS highlights adaptation in life\nand lifeless complex systems cutting across all traditional natural and social\nsciences disciplines. However, discovering a universal law in CAS and\nunderstanding the underlying mechanism of distribution formation, such as a\nnon-Gauss distribution in complex quantum entanglement, remains highly\nchallenging. Quantifying the uncertainty of CAS by probability wave functions,\nthe authors explore the inherent logical relationship between Schr\\\"odinger's\nwave equation in quantum mechanics and Shi's trading volume-price wave equation\nin finance. Subsequently, the authors propose a non-localized wave equation in\nquantum mechanics if cumulative observable in a time interval represents\nmomentum or momentum force in Skinner-Shi (reinforcement-frequency-interaction)\ncoordinates. It reveals that the invariance of interaction as a universal law\nexists in quantum mechanics and finance. The theory shows that quantum\nentanglement is an interactively coherent state instead of a consequence of the\nsuperposition of coherent states. As a resource, quantum entanglement is\nnon-separable, steerable, and energy-consumed. The entanglement state has\nopposite states subject to interaction conservation between the momentum and\nreversal forces. Keywords: complex adaptive systems, complex adaptive learning,\ncomplex quantum systems, non-localized wave equation, interaction conservation,\ninteractively coherent entanglement PACS: 89.75.-k (Complex Systems); 89.65.Gh\n(Economics, Econophysics, Financial Markets, Business and Management); 03.65.Ud\n(Entanglement and Quantum Nonlocality)\n",
        "pdf_link": "http://arxiv.org/pdf/2306.15554v12"
    },
    {
        "title": "Exploring the Dynamics of the Specialty Insurance Market Using a Novel\n  Discrete Event Simulation Framework: a Lloyd's of London Case Study",
        "authors": [
            "Sedar Olmez",
            "Akhil Ahmed",
            "Keith Kam",
            "Zhe Feng",
            "Alan Tua"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  This research presents a novel Discrete Event Simulation (DES) of the Lloyd's\nof London specialty insurance market, exploring complex market dynamics that\nhave not been previously studied quantitatively. The proof-of-concept model\nallows for the simulation of various scenarios that capture important market\nphenomena such as the underwriting cycle, the impact of risk syndication, and\nthe importance of appropriate exposure management. Despite minimal calibration,\nour model has shown that it is a valuable tool for understanding and analysing\nthe Lloyd's of London specialty insurance market, particularly in terms of\nidentifying areas for further investigation for regulators and participants of\nthe market alike. The results generate the expected behaviours that, syndicates\n(insurers) are less likely to go insolvent if they adopt sophisticated exposure\nmanagement practices, catastrophe events lead to more defined patterns of\ncyclicality and cause syndicates to substantially increase their premiums\noffered. Lastly, syndication enhances the accuracy of actuarial price estimates\nand narrows the divergence among syndicates. Overall, this research offers a\nnew perspective on the Lloyd's of London market and demonstrates the potential\nof individual-based modelling (IBM) for understanding complex financial\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.05581v1"
    },
    {
        "title": "InvestLM: A Large Language Model for Investment using Financial Domain\n  Instruction Tuning",
        "authors": [
            "Yi Yang",
            "Yixuan Tang",
            "Kar Yan Tam"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We present a new financial domain large language model, InvestLM, tuned on\nLLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset\nrelated to financial investment. Inspired by less-is-more-for-alignment (Zhou\net al., 2023), we manually curate a small yet diverse instruction dataset,\ncovering a wide range of financial related topics, from Chartered Financial\nAnalyst (CFA) exam questions to SEC filings to Stackexchange quantitative\nfinance discussions. InvestLM shows strong capabilities in understanding\nfinancial text and provides helpful responses to investment related questions.\nFinancial experts, including hedge fund managers and research analysts, rate\nInvestLM's response as comparable to those of state-of-the-art commercial\nmodels (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of\nfinancial NLP benchmarks demonstrates strong generalizability. From a research\nperspective, this work suggests that a high-quality domain specific LLM can be\ntuned using a small set of carefully curated instructions on a well-trained\nfoundation model, which is consistent with the Superficial Alignment Hypothesis\n(Zhou et al., 2023). From a practical perspective, this work develops a\nstate-of-the-art financial domain LLM with superior capability in understanding\nfinancial texts and providing helpful investment advice, potentially enhancing\nthe work efficiency of financial professionals. We release the model parameters\nto the research community.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.13064v1"
    },
    {
        "title": "High-Throughput Asset Pricing",
        "authors": [
            "Andrew Y. Chen",
            "Chukwuma Dim"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  We use empirical Bayes (EB) to mine data on 140,000 long-short strategies\nconstructed from accounting ratios, past returns, and ticker symbols. This\n\"high-throughput asset pricing\" produces out-of-sample performance comparable\nto strategies in top finance journals. But unlike the published strategies, the\ndata-mined strategies are free of look-ahead bias. EB predicts that high\nreturns are concentrated in accounting strategies, small stocks, and pre-2004\nsamples, consistent with limited attention theories. The intuition is seen in\nthe cross-sectional distribution of t-stats, which is far from the null for\nequal-weighted accounting strategies. High-throughput methods provide a\nrigorous, unbiased method for documenting asset pricing facts.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.10685v2"
    },
    {
        "title": "Large Language Models in Finance: A Survey",
        "authors": [
            "Yinheng Li",
            "Shaofei Wang",
            "Han Ding",
            "Hang Chen"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Recent advances in large language models (LLMs) have opened new possibilities\nfor artificial intelligence applications in finance. In this paper, we provide\na practical survey focused on two key aspects of utilizing LLMs for financial\ntasks: existing solutions and guidance for adoption.\n  First, we review current approaches employing LLMs in finance, including\nleveraging pretrained models via zero-shot or few-shot learning, fine-tuning on\ndomain-specific data, and training custom LLMs from scratch. We summarize key\nmodels and evaluate their performance improvements on financial natural\nlanguage processing tasks.\n  Second, we propose a decision framework to guide financial professionals in\nselecting the appropriate LLM solution based on their use case constraints\naround data, compute, and performance needs. The framework provides a pathway\nfrom lightweight experimentation to heavy investment in customized LLMs.\n  Lastly, we discuss limitations and challenges around leveraging LLMs in\nfinancial applications. Overall, this survey aims to synthesize the\nstate-of-the-art and provide a roadmap for responsibly applying LLMs to advance\nfinancial AI.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.10723v2"
    },
    {
        "title": "Interplay between Cryptocurrency Transactions and Online Financial\n  Forums",
        "authors": [
            "Ana Fernández Vilas",
            "Rebeca P. Díaz Redondo",
            "Daniel Couto Cancela",
            "Alejandro Torrado Pazos"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Cryptocurrencies are a type of digital money meant to provide security and\nanonymity while using cryptography techniques. Although cryptocurrencies\nrepresent a breakthrough and provide some important benefits, their usage poses\nsome risks that are a result of the lack of supervising institutions and\ntransparency. Because disinformation and volatility is discouraging for\npersonal investors, cryptocurrencies emerged hand-in-hand with the\nproliferation of online users' communities and forums as places to share\ninformation that can alleviate users' mistrust. This research focuses on the\nstudy of the interplay between these cryptocurrency forums and fluctuations in\ncryptocurrency values. In particular, the most popular cryptocurrency Bitcoin\n(BTC) and a related active discussion community, Bitcointalk, are analyzed.\nThis study shows that the activity of Bitcointalk forum keeps a direct\nrelationship with the trend in the values of BTC, therefore analysis of this\ninteraction would be a perfect base to support personal investments in a\nnon-regulated market and, to confirm whether cryptocurrency forums show\nevidences to detect abnormal behaviors in BTC values as well as to predict or\nestimate these values. The experiment highlights that forum data can explain\nspecific events in the financial field. It also underlines the relevance of\nquotes (regular mechanism to response a post) at periods: (1) when there is a\nhigh concentration of posts around certain topics; (2) when peaks in the BTC\nprice are observed; and, (3) when the BTC price gradually shifts downwards and\nusers intend to sell.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10238v1"
    },
    {
        "title": "Assessing the Efficacy of Heuristic-Based Address Clustering for Bitcoin",
        "authors": [
            "Hugo Schnoering",
            "Pierre Porthaux",
            "Michalis Vazirgiannis"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Exploring transactions within the Bitcoin blockchain entails examining the\ntransfer of bitcoins among several hundred million entities. However, it is\noften impractical and resource-consuming to study such a vast number of\nentities. Consequently, entity clustering serves as an initial step in most\nanalytical studies. This process often employs heuristics grounded in the\npractices and behaviors of these entities. In this research, we delve into the\nexamination of two widely used heuristics, alongside the introduction of four\nnovel ones. Our contribution includes the introduction of the\n\\textit{clustering ratio}, a metric designed to quantify the reduction in the\nnumber of entities achieved by a given heuristic. The assessment of this\nreduction ratio plays an important role in justifying the selection of a\nspecific heuristic for analytical purposes. Given the dynamic nature of the\nBitcoin system, characterized by a continuous increase in the number of\nentities on the blockchain, and the evolving behaviors of these entities, we\nextend our study to explore the temporal evolution of the clustering ratio for\neach heuristic. This temporal analysis enhances our understanding of the\neffectiveness of these heuristics over time.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00523v1"
    },
    {
        "title": "Anticipatory Gains and Event-Driven Losses in Blockchain-Based Fan\n  Tokens: Evidence from the FIFA World Cup",
        "authors": [
            "Aman Saggu",
            "Lennart Ante",
            "Ender Demir"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  National football teams increasingly issue tradeable blockchain-based fan\ntokens to strategically enhance fan engagement. This study investigates the\nimpact of 2022 World Cup matches on the dynamic performance of each team's fan\ntoken. The event study uncovers fan token returns surged six months before the\nWorld Cup, driven by positive anticipation effects. However, intraday analysis\nreveals a reversal of fan token returns consistently declining and trading\nvolumes rising as matches unfold. To explain findings, we uncover asymmetries\nwhereby defeats in high-stake matches caused a plunge in fan token returns,\ncompared to low-stake matches, intensifying in magnitude for knockout matches.\nContrarily, victories enhance trading volumes, reflecting increased market\nactivity without a corresponding positive effect on returns. We align findings\nwith the classic market adage \"buy the rumor, sell the news,\" unveiling\ncognitive biases and nuances in investor sentiment, cautioning the dichotomy of\npre-event optimism and subsequent performance declines.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15810v1"
    },
    {
        "title": "Artificial Intelligence-based Analysis of Change in Public Finance\n  between US and International Markets",
        "authors": [
            "Kapil Panda"
        ],
        "category": "q-fin.GN",
        "published_year": "2023",
        "summary": "  Public finances are one of the fundamental mechanisms of economic governance\nthat refer to the financial activities and decisions made by government\nentities to fund public services, projects, and operations through assets. In\ntoday's globalized landscape, even subtle shifts in one nation's public debt\nlandscape can have significant impacts on that of international finances,\nnecessitating a nuanced understanding of the correlations between international\nand national markets to help investors make informed investment decisions.\nTherefore, by leveraging the capabilities of artificial intelligence, this\nstudy utilizes neural networks to depict the correlations between US and\nInternational Public Finances and predict the changes in international public\nfinances based on the changes in US public finances. With the neural network\nmodel achieving a commendable Mean Squared Error (MSE) value of 2.79, it is\nable to affirm a discernible correlation and also plot the effect of US market\nvolatility on international markets. To further test the accuracy and\nsignificance of the model, an economic analysis was conducted that aimed to\ncorrelate the changes seen by the results of the model with historical stock\nmarket changes. This model demonstrates significant potential for investors to\npredict changes in international public finances based on signals from US\nmarkets, marking a significant stride in comprehending the intricacies of\nglobal public finances and the role of artificial intelligence in decoding its\nmultifaceted patterns for practical forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.18823v1"
    },
    {
        "title": "Data-generating process and time-series asset pricing",
        "authors": [
            "Shuxin Guo",
            "Qiang Liu"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  We study the data-generating processes for factors expressed in return\ndifferences, which the literature on time-series asset pricing seems to have\noverlooked. For the factors' data-generating processes or long-short zero-cost\nportfolios, a meaningful definition of returns is impossible; further, the\ncompounded market factor (MF) significantly underestimates the return\ndifference between the market and the risk-free rate compounded separately.\nSurprisingly, if MF were treated coercively as periodic-rebalancing long-short\n(i.e., the same as size and value), Fama-French three-factor (FF3) would be\neconomically unattractive for lacking compounding and irrelevant for suffering\nfrom the small \"size of an effect.\" Otherwise, FF3 might be misspecified if MF\nwere buy-and-hold long-short. Finally, we show that OLS with net returns for\nsingle-index models leads to inflated alphas, exaggerated t-values, and\noverestimated Sharpe ratios (SR); worse, net returns may lead to pathological\nalphas and SRs. We propose defining factors (and SRs) with non-difference\ncompound returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10920v1"
    },
    {
        "title": "Evaluating Investment Risks in LATAM AI Startups: Ranking of Investment\n  Potential and Framework for Valuation",
        "authors": [
            "Abraham Ramos-Torres",
            "Laura N. Montoya"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  The growth of the tech startup ecosystem in Latin America (LATAM) is driven\nby innovative entrepreneurs addressing market needs across various sectors.\nHowever, these startups encounter unique challenges and risks that require\nspecific management approaches. This paper explores a case study with the Total\nAddressable Market (TAM), Serviceable Available Market (SAM), and Serviceable\nObtainable Market (SOM) metrics within the context of the online food delivery\nindustry in LATAM, serving as a model for valuing startups using the Discounted\nCash Flow (DCF) method. By analyzing key emerging powers such as Argentina,\nColombia, Uruguay, Costa Rica, Panama, and Ecuador, the study highlights the\npotential and profitability of AI-driven startups in the region through the\ndevelopment of a ranking of emerging powers in Latin America for tech startup\ninvestment. The paper also examines the political, economic, and competitive\nrisks faced by startups and offers strategic insights on mitigating these risks\nto maximize investment returns. Furthermore, the research underscores the value\nof diversifying investment portfolios with startups in emerging markets,\nemphasizing the opportunities for substantial growth and returns despite\ninherent risks.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.03552v1"
    },
    {
        "title": "Tourism destination events classifier based on artificial intelligence\n  techniques",
        "authors": [
            "Miguel Camacho-Ruiz",
            "Ramón Alberto Carrasco",
            "Gema Fernández-Avilés",
            "Antonio LaTorre"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  Identifying client needs to provide optimal services is crucial in tourist\ndestination management. The events held in tourist destinations may help to\nmeet those needs and thus contribute to tourist satisfaction. As with product\nmanagement, the creation of hierarchical catalogs to classify those events can\naid event management. The events that can be found on the internet are listed\nin dispersed, heterogeneous sources, which makes direct classification a\ndifficult, time-consuming task. The main aim of this work is to create a novel\nprocess for automatically classifying an eclectic variety of tourist events\nusing a hierarchical taxonomy, which can be applied to support tourist\ndestination management. Leveraging data science methods such as CRISP-DM,\nsupervised machine learning, and natural language processing techniques, the\nautomatic classification process proposed here allows the creation of a\nnormalized catalog across very different geographical regions. Therefore, we\ncan build catalogs with consistent filters, allowing users to find events\nregardless of the event categories assigned at source, if any. This is very\nvaluable for companies that offer this kind of information across multiple\nregions, such as airlines, travel agencies or hotel chains. Ultimately, this\ntool has the potential to revolutionize the way companies and end users\ninteract with tourist events information.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19741v1"
    },
    {
        "title": "Uncertain Regulations, Definite Impacts: The Impact of the US Securities\n  and Exchange Commission's Regulatory Interventions on Crypto Assets",
        "authors": [
            "Aman Saggu",
            "Lennart Ante",
            "Kaja Kopiec"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  This study employs an event study methodology to investigate the market\nimpact of the U.S. Securities and Exchange Commission's (SEC) classification of\ncrypto assets as securities. It explores how SEC interventions influence asset\nreturns and trading volumes, focusing on explicitly named crypto assets. The\nempirical analysis highlights significant adverse market reactions, notably\nreturns plummeting 12% over one week post-announcement, persisting for a month.\nWe demonstrate that the severity of market reaction depends on sentiment and\nasset characteristics such as market size, age, volatility, and illiquidity.\nFurther, we identify significant ex-ante trading volume effects indicative of\npre-announcement informed trading.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.02452v1"
    },
    {
        "title": "Log-Ergodic Dynamics in Stochastic Monetary Velocity: Theoretical\n  Insights and Economic Implications",
        "authors": [
            "Kiarash Firouzi",
            "Mohammad Jelodari Mamaghani"
        ],
        "category": "q-fin.GN",
        "published_year": "2024",
        "summary": "  We suggest employing log-ergodic processes to simulate the velocity of money\nin an ergodic manner. Our approach sheds light on economic behavior, policy\nimplications, and financial dynamics by maintaining long-term stability. By\nbridging theory and practice, the partially ergodic model helps analysts and\npolicymakers comprehend and forecast velocity of money. The empirical analysis,\nusing historical U.S. GDP and money supply data, demonstrates the model's\neffectiveness in capturing the long-term stability of the velocity of money.\nKey findings indicate that the log-ergodic model offers superior predictive\npower compared to traditional models, making it a valuable tool for\npolicymakers to control economic factors in vital situations.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08657v1"
    },
    {
        "title": "The Intraday Bitcoin Response to Tether Minting and Burning Events:\n  Asymmetry, Investor Sentiment, And \"Whale Alerts\" On Twitter",
        "authors": [
            "Aman Saggu"
        ],
        "category": "q-fin.GN",
        "published_year": "2025",
        "summary": "  Tether Limited has the sole authority to create (mint) and destroy (burn)\nTether stablecoins (USDT). This paper investigates Bitcoin's response to USDT\nsupply change events between 2014 and 2021 and identifies an interesting\nasymmetry between Bitcoin's responses to USDT minting and burning events.\nBitcoin responds positively to USDT minting events over 5- to 30-minute event\nwindows, but this response begins declining after 60 minutes. State-dependence\nis also demonstrated, with Bitcoin prices exhibiting a greater increase when\nthe corresponding USDT minting event coincides with positive investor sentiment\nand is announced to the public by data service provider, Whale Alert, on\nTwitter.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.05232v1"
    },
    {
        "title": "Time-Varying Bidirectional Causal Relationships Between Transaction Fees\n  and Economic Activity of Subsystems Utilizing the Ethereum Blockchain Network",
        "authors": [
            "Lennart Ante",
            "Aman Saggu"
        ],
        "category": "q-fin.GN",
        "published_year": "2025",
        "summary": "  The Ethereum blockchain network enables transaction processing and\nsmart-contract execution through levies of transaction fees, commonly known as\ngas fees. This framework mediates economic participation via a market-based\nmechanism for gas fees, permitting users to offer higher gas fees to expedite\npro-cessing. Historically, the ensuing gas fee volatility led to critical\ndisequilibria between supply and demand for block space, presenting stakeholder\nchallenges. This study examines the dynamic causal interplay between\ntransaction fees and economic subsystems leveraging the network. By utilizing\ndata related to unique active wallets and transaction volume of each subsystem\nand applying time-varying Granger causality analysis, we reveal temporal\nheterogeneity in causal relationships between economic activity and transaction\nfees across all subsystems. This includes (a) a bidirectional causal feedback\nloop between cross-blockchain bridge user activity and transaction fees, which\ndiminishes over time, potentially signaling user migration; (b) a bidirectional\nrelationship between centralized cryptocurrency exchange deposit and withdrawal\ntransaction volume and fees, indicative of increased competition for block\nspace; (c) decentralized exchange volumes causally influence fees, while fees\ncausally influence user activity, although this relationship is weakening,\npotentially due to the diminished significance of decentralized finance; (d)\nintermittent causal relationships with maximal extractable value bots; (e) fees\ncausally in-fluence non-fungible token transaction volumes; and (f) a highly\nsignificant and growing causal influence of transaction fees on stablecoin\nactivity and transaction volumes highlight its prominence.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.05299v1"
    },
    {
        "title": "Persistence in a Random Bond Ising Model of Socio-Econo Dynamics",
        "authors": [
            "S. Jain",
            "T. Yamano"
        ],
        "category": "q-fin.GN",
        "published_year": "2007",
        "summary": "  We study the persistence phenomenon in a socio-econo dynamics model using\ncomputer simulations at a finite temperature on hypercubic lattices in\ndimensions up to 5. The model includes a ` social\\rq local field which contains\nthe magnetization at time $t$. The nearest neighbour quenched interactions are\ndrawn from a binary distribution which is a function of the bond concentration,\n$p$. The decay of the persistence probability in the model depends on both the\nspatial dimension and $p$. We find no evidence of ` blocking\\rq in this model.\nWe also discuss the implications of our results for possible applications in\nthe social and economic fields. It is suggested that the absence, or otherwise,\nof blocking could be used as a criterion to decide on the validity of a given\nmodel in different scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.1092v2"
    },
    {
        "title": "Productivity Dispersion: Facts, Theory, and Implications",
        "authors": [
            "Hideaki Aoyama",
            "Hiroshi Yoshikawa",
            "Hiroshi Iyetomi",
            "Yoshi Fujiwara"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  We study productivity dispersions across workers, firms and industrial\nsectors. Empirical study of the Japanese data shows that they all obey the\nPareto law, and also that the Pareto index decreases with the level of\naggregation. In order to explain these two stylized facts, we propose a\ntheoretical framework built upon the basic principle of statistical physics. In\nthis framework, we employ the concept of superstatistics which accommodates\nfluctuations of aggregate demand.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.2792v1"
    },
    {
        "title": "Evidence for the Gompertz Curve in the Income Distribution of Brazil\n  1978-2005",
        "authors": [
            "Newton J. Moura Jr.",
            "Marcelo B. Ribeiro"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  This work presents an empirical study of the evolution of the personal income\ndistribution in Brazil. Yearly samples available from 1978 to 2005 were studied\nand evidence was found that the complementary cumulative distribution of\npersonal income for 99% of the economically less favorable population is well\nrepresented by a Gompertz curve of the form $G(x)=\\exp [\\exp (A-Bx)]$, where\n$x$ is the normalized individual income. The complementary cumulative\ndistribution of the remaining 1% richest part of the population is well\nrepresented by a Pareto power law distribution $P(x)= \\beta x^{-\\alpha}$. This\nresult means that similarly to other countries, Brazil's income distribution is\ncharacterized by a well defined two class system. The parameters $A$, $B$,\n$\\alpha$, $\\beta$ were determined by a mixture of boundary conditions,\nnormalization and fitting methods for every year in the time span of this\nstudy. Since the Gompertz curve is characteristic of growth models, its\npresence here suggests that these patterns in income distribution could be a\nconsequence of the growth dynamics of the underlying economic system. In\naddition, we found out that the percentage share of both the Gompertzian and\nParetian components relative to the total income shows an approximate cycling\npattern with periods of about 4 years and whose maximum and minimum peaks in\neach component alternate at about every 2 years. This finding suggests that the\ngrowth dynamics of Brazil's economic system might possibly follow a\nGoodwin-type class model dynamics based on the application of the\nLotka-Volterra equation to economic growth and cycle.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2664v1"
    },
    {
        "title": "Jan Tinbergen's legacy for economic networks: from the gravity model to\n  quantum statistics",
        "authors": [
            "Tiziano Squartini",
            "Diego Garlaschelli"
        ],
        "category": "q-fin.GN",
        "published_year": "2013",
        "summary": "  Jan Tinbergen, the first recipient of the Nobel Memorial Prize in Economics\nin 1969, obtained his PhD in physics at the University of Leiden under the\nsupervision of Paul Ehrenfest in 1929. Among many achievements as an economist\nafter his training as a physicist, Tinbergen proposed the so-called Gravity\nModel of international trade. The model predicts that the intensity of trade\nbetween two countries is described by a formula similar to Newton's law of\ngravitation, where mass is replaced by Gross Domestic Product. Since\nTinbergen's proposal, the Gravity Model has become the standard model of\nnon-zero trade flows in macroeconomics. However, its intrinsic limitation is\nthe prediction of a completely connected network, which fails to explain the\nobserved intricate topology of international trade. Recent network models\novercome this limitation by describing the real network as a member of a\nmaximum-entropy statistical ensemble. The resulting expressions are formally\nanalogous to quantum statistics: the international trade network is found to\nclosely follow the Fermi-Dirac statistics in its purely binary topology, and\nthe recently proposed mixed Bose-Fermi statistics in its full (binary plus\nweighted) structure. This seemingly esoteric result is actually a simple effect\nof the heterogeneity of world countries, that imposes strong structural\nconstraints on the network. Our discussion highlights similarities and\ndifferences between macroeconomics and statistical-physics approaches to\neconomic networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3252v1"
    },
    {
        "title": "Crowd, Lending, Machine, and Bias",
        "authors": [
            "Runshan Fu",
            "Yan Huang",
            "Param Vir Singh"
        ],
        "category": "q-fin.GN",
        "published_year": "2020",
        "summary": "  Big data and machine learning (ML) algorithms are key drivers of many fintech\ninnovations. While it may be obvious that replacing humans with machine would\nincrease efficiency, it is not clear whether and where machines can make better\ndecisions than humans. We answer this question in the context of crowd lending,\nwhere decisions are traditionally made by a crowd of investors. Using data from\nProsper.com, we show that a reasonably sophisticated ML algorithm predicts\nlisting default probability more accurately than crowd investors. The dominance\nof the machine over the crowd is more pronounced for highly risky listings. We\nthen use the machine to make investment decisions, and find that the machine\nbenefits not only the lenders but also the borrowers. When machine prediction\nis used to select loans, it leads to a higher rate of return for investors and\nmore funding opportunities for borrowers with few alternative funding options.\nWe also find suggestive evidence that the machine is biased in gender and race\neven when it does not use gender and race information as input. We propose a\ngeneral and effective \"debasing\" method that can be applied to any prediction\nfocused ML applications, and demonstrate its use in our context. We show that\nthe debiased ML algorithm, which suffers from lower prediction accuracy, still\nleads to better investment decisions compared with the crowd. These results\nindicate that ML can help crowd lending platforms better fulfill the promise of\nproviding access to financial resources to otherwise underserved individuals\nand ensure fairness in the allocation of these resources.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04068v1"
    },
    {
        "title": "Exploring Multi-Banking Customer-to-Customer Relations in AML Context\n  with Poincaré Embeddings",
        "authors": [
            "Lucia Larise Stavarache",
            "Donatas Narbutis",
            "Toyotaro Suzumura",
            "Ray Harishankar",
            "Augustas Žaltauskas"
        ],
        "category": "q-fin.GN",
        "published_year": "2019",
        "summary": "  In the recent years money laundering schemes have grown in complexity and\nspeed of realization, affecting financial institutions and millions of\ncustomers globally. Strengthened privacy policies, along with in-country\nregulations, make it hard for banks to inner- and cross-share, and report\nsuspicious activities for the AML (Anti-Money Laundering) measures. Existing\ntopologies and models for AML analysis and information sharing are subject to\nmajor limitations, such as compliance with regulatory constraints, extended\ninfrastructure to run high-computation algorithms, data quality and span,\nproving cumbersome and costly to execute, federate, and interpret. This paper\nproposes a new topology for exploring multi-banking customer social relations\nin AML context -- customer-to-customer, customer-to-transaction, and\ntransaction-to-transaction -- using a 3D modeling topological algebra\nformulated through Poincar\\'e embeddings.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.07701v2"
    },
    {
        "title": "Disentangling bipartite and core-periphery structure in financial\n  networks",
        "authors": [
            "Paolo Barucca",
            "Fabrizio Lillo"
        ],
        "category": "q-fin.GN",
        "published_year": "2015",
        "summary": "  A growing number of systems are represented as networks whose architecture\nconveys significant information and determines many of their properties.\nExamples of network architecture include modular, bipartite, and core-periphery\nstructures. However inferring the network structure is a non trivial task and\ncan depend sometimes on the chosen null model. Here we propose a method for\nclassifying network structures and ranking its nodes in a statistically\nwell-grounded fashion. The method is based on the use of Belief Propagation for\nlearning through Entropy Maximization on both the Stochastic Block Model (SBM)\nand the degree-corrected Stochastic Block Model (dcSBM). As a specific\napplication we show how the combined use of the two ensembles -SBM and dcSBM-\nallows to disentangle the bipartite and the core-periphery structure in the\ncase of the e-MID interbank network. Specifically we find that, taking into\naccount the degree, this interbank network is better described by a bipartite\nstructure, while using the SBM the core-periphery structure emerges only when\ndata are aggregated for more than a week.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08830v1"
    },
    {
        "title": "Quantifying A Firm's AI Engagement: Constructing Objective, Data-Driven,\n  AI Stock Indices Using 10-K Filings",
        "authors": [
            "Lennart Ante",
            "Aman Saggu"
        ],
        "category": "q-fin.GN",
        "published_year": "2025",
        "summary": "  Following an analysis of existing AI-related exchange-traded funds (ETFs), we\nreveal the selection criteria for determining which stocks qualify as\nAI-related are often opaque and rely on vague phrases and subjective judgments.\nThis paper proposes a new, objective, data-driven approach using natural\nlanguage processing (NLP) techniques to classify AI stocks by analyzing annual\n10-K filings from 3,395 NASDAQ-listed firms between 2011 and 2023. This\nanalysis quantifies each company's engagement with AI through binary indicators\nand weighted AI scores based on the frequency and context of AI-related terms.\nUsing these metrics, we construct four AI stock indices-the Equally Weighted AI\nIndex (AII), the Size-Weighted AI Index (SAII), and two Time-Discounted AI\nIndices (TAII05 and TAII5X)-offering different perspectives on AI investment.\nWe validate our methodology through an event study on the launch of OpenAI's\nChatGPT, demonstrating that companies with higher AI engagement saw\nsignificantly greater positive abnormal returns, with analyses supporting the\npredictive power of our AI measures. Our indices perform on par with or surpass\n14 existing AI-themed ETFs and the Nasdaq Composite Index in risk-return\nprofiles, market responsiveness, and overall performance, achieving higher\naverage daily returns and risk-adjusted metrics without increased volatility.\nThese results suggest our NLP-based approach offers a reliable,\nmarket-responsive, and cost-effective alternative to existing AI-related ETF\nproducts. Our innovative methodology can also guide investors, asset managers,\nand policymakers in using corporate data to construct other thematic\nportfolios, contributing to a more transparent, data-driven, and competitive\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.01763v1"
    },
    {
        "title": "Complex Networks and Symmetry I: A Review",
        "authors": [
            "Diego Garlaschelli",
            "Franco Ruzzenenti",
            "Riccardo Basosi"
        ],
        "category": "q-fin.GN",
        "published_year": "2010",
        "summary": "  In this review we establish various connections between complex networks and\nsymmetry. While special types of symmetries (e.g., automorphisms) are studied\nin detail within discrete mathematics for particular classes of deterministic\ngraphs, the analysis of more general symmetries in real complex networks is far\nless developed. We argue that real networks, as any entity characterized by\nimperfections or errors, necessarily require a stochastic notion of invariance.\nWe therefore propose a definition of stochastic symmetry based on graph\nensembles and use it to review the main results of network theory from an\nunusual perspective. The results discussed here and in a companion paper show\nthat stochastic symmetry highlights the most informative topological properties\nof real networks, even in noisy situations unaccessible to exact techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.3923v2"
    },
    {
        "title": "Proportionate vs disproportionate distribution of wealth of two\n  individuals in a tempered Paretian ensemble",
        "authors": [
            "G. Oshanin",
            "Yu. Holovatch",
            "G. Schehr"
        ],
        "category": "q-fin.GN",
        "published_year": "2011",
        "summary": "  We study the distribution P(\\omega) of the random variable \\omega = x_1/(x_1\n+ x_2), where x_1 and x_2 are the wealths of two individuals selected at random\nfrom the same tempered Paretian ensemble characterized by the distribution\n\\Psi(x) \\sim \\phi(x)/x^{1 + \\alpha}, where \\alpha > 0 is the Pareto index and\n$\\phi(x)$ is the cut-off function. We consider two forms of \\phi(x): a bounded\nfunction \\phi(x) = 1 for L \\leq x \\leq H, and zero otherwise, and a smooth\nexponential function \\phi(x) = \\exp(-L/x - x/H). In both cases \\Psi(x) has\nmoments of arbitrary order.\n  We show that, for \\alpha > 1, P(\\omega) always has a unimodal form and is\npeaked at \\omega = 1/2, so that most probably x_1 \\approx x_2. For 0 < \\alpha <\n1 we observe a more complicated behavior which depends on the value of \\delta =\nL/H. In particular, for \\delta < \\delta_c - a certain threshold value -\nP(\\omega) has a three-modal (for a bounded \\phi(x)) and a bimodal M-shape (for\nan exponential \\phi(x)) form which signifies that in such ensembles the wealths\nx_1 and x_2 are disproportionately different.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.4710v1"
    },
    {
        "title": "Pareto and Boltzmann-Gibbs behaviors in a deterministic multi-agent\n  system",
        "authors": [
            "J. Gonzalez-Estevez",
            "M. G. Cosenza",
            "R. Lopez-Ruiz",
            "J. R. Sanchez"
        ],
        "category": "q-fin.GN",
        "published_year": "2008",
        "summary": "  A deterministic system of interacting agents is considered as a model for\neconomic dynamics. The dynamics of the system is described by a coupled map\nlattice with near neighbor interactions. The evolution of each agent results\nfrom the competition between two factors: the agent's own tendency to grow and\nthe environmental influence that moderates this growth. Depending on the values\nof the parameters that control these factors, the system can display Pareto or\nBoltzmann-Gibbs statistical behaviors in its asymptotic dynamical regime. The\nregions where these behaviors appear are calculated on the space of parameters\nof the system. Other statistical properties, such as the mean wealth, the\nstandard deviation, and the Gini coefficient characterizing the degree of\nequity in the wealth distribution are also calculated on the space of\nparameters of the system.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.0969v1"
    },
    {
        "title": "Economics of disagreement -- financial intuition for the Rényi\n  divergence",
        "authors": [
            "Andrei N. Soklakov"
        ],
        "category": "q-fin.GN",
        "published_year": "2018",
        "summary": "  Disagreement is an essential element of science and life in general. The\nlanguage of probabilities and statistics is often used to describe\ndisagreements quantitatively. In practice, however, we want much more than\nthat. We want disagreements to be resolved. This leaves us with a substantial\nknowledge gap which is often perceived as a lack of practical intuition\nregarding probabilistic and statistical concepts.\n  Take for instance the R\\'enyi divergence which is a well-known statistical\nquantity specifically designed as a measure of disagreement between\nprobabilistic models. Despite its widespread use in science and engineering,\nthe R\\'enyi divergence remains a highly abstract axiomatically-motivated\nmeasure. Certainly, it offers no practical insight as to how disagreements can\nbe resolved.\n  Here we propose to address disagreements using the methods of financial\neconomics. In particular, we show how a large class of disagreements can be\ntransformed into investment opportunities. The expected financial performance\nof such investments quantifies the amount of disagreement in a tangible way.\nThis provides intuition for statistical concepts such as the R\\'enyi divergence\nwhich becomes connected to the financial performance of optimized investments.\nInvestment optimization takes into account individual opinions as well as\nattitudes towards risk. The result is a market-like social mechanism by which\nfunds flow naturally to support a more accurate view. Such social mechanisms\ncan help us with difficult disagreements (e.g., financial arguments concerning\nthe future climate).\n  In terms of scientific validation, we used the findings of independent\nneurophysiological experiments as well as our own research on the equity\npremium.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.08308v7"
    }
]