[
    {
        "title": "A Stochastic Processes Toolkit for Risk Management",
        "authors": [
            "Damiano Brigo",
            "Antonio Dalessandro",
            "Matthias Neugebauer",
            "Fares Triki"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  In risk management it is desirable to grasp the essential statistical\nfeatures of a time series representing a risk factor. This tutorial aims to\nintroduce a number of different stochastic processes that can help in grasping\nthe essential features of risk factors describing different asset classes or\nbehaviors. This paper does not aim at being exhaustive, but gives examples and\na feeling for practically implementable models allowing for stylised features\nin the data. The reader may also use these models as building blocks to build\nmore complex models, although for a number of risk management applications the\nmodels developed here suffice for the first step in the quantitative analysis.\nThe broad qualitative features addressed here are {fat tails} and {mean\nreversion}. We give some orientation on the initial choice of a suitable\nstochastic process and then explain how the process parameters can be estimated\nbased on historical data. Once the process has been calibrated, typically\nthrough maximum likelihood estimation, one may simulate the risk factor and\nbuild future scenarios for the risky portfolio. On the terminal simulated\ndistribution of the portfolio one may then single out several risk measures,\nalthough here we focus on the stochastic processes estimation preceding the\nsimulation of the risk factors Finally, this first survey report focuses on\nsingle time series. Correlation or more generally dependence across risk\nfactors, leading to multivariate processes modeling, will be addressed in\nfuture work.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.4210v1"
    },
    {
        "title": "Partial Equilibria with Convex Capital Requirements: Existence,\n  Uniqueness and Stability",
        "authors": [
            "Michail Anthropelos",
            "Gordan Zitkovic"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  In an incomplete semimartingale model of a financial market, we consider\nseveral risk-averse financial agents who negotiate the price of a bundle of\ncontingent claims. Assuming that the agents' risk preferences are modelled by\nconvex capital requirements, we define and analyze their demand functions and\npropose a notion of a partial equilibrium price. In addition to sufficient\nconditions for the existence and uniqueness, we also show that the equilibrium\nprices are stable with respect to misspecifications of agents' risk\npreferences.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.3318v1"
    },
    {
        "title": "Monitoring dates of maximal risk",
        "authors": [
            "Erick Trevino Aguilar"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  Monitoring means to observe a system for any changes which may occur over\ntime, using a monitor or measuring device of some sort. In this paper we\nformulate a problem of monitoring dates of maximal risk of a financial\nposition. Thus, the systems we are going to observe arise from situations in\nfinance. The measuring device we are going to use is a time-consistent measure\nof risk.\n  In the first part of the paper we discuss the numerical representation of\nconditional convex risk measures which are defined in a space Lp(F,R) and take\nvalues in L1(G,R). This will allow us to consider time-consistent convex risk\nmeasures in L1(R).\n  In the second part of the paper we use a time-consistent convex risk measure\nin order to define an abstract problem of monitoring stopping times of maximal\nrisk. The penalty function involved in the robust representation changes\nqualitatively the time when maximal risk is for the first time identified. A\nphenomenon which we discuss from the point of view of robust statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.2756v1"
    },
    {
        "title": "The Structural Modelling of Operational Risk via Bayesian inference:\n  Combining Loss Data with Expert Opinions",
        "authors": [
            "P. V. Shevchenko",
            "M. V. Wüthrich"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  To meet the Basel II regulatory requirements for the Advanced Measurement\nApproaches, the bank's internal model must include the use of internal data,\nrelevant external data, scenario analysis and factors reflecting the business\nenvironment and internal control systems. Quantification of operational risk\ncannot be based only on historical data but should involve scenario analysis.\nHistorical internal operational risk loss data have limited ability to predict\nfuture behaviour moreover, banks do not have enough internal data to estimate\nlow frequency high impact events adequately. Historical external data are\ndifficult to use due to different volumes and other factors. In addition,\ninternal and external data have a survival bias, since typically one does not\nhave data of all collapsed companies. The idea of scenario analysis is to\nestimate frequency and severity of risk events via expert opinions taking into\naccount bank environment factors with reference to events that have occurred\n(or may have occurred) in other banks. Scenario analysis is forward looking and\ncan reflect changes in the banking environment. It is important to not only\nquantify the operational risk capital but also provide incentives to business\nunits to improve their risk management policies, which can be accomplished\nthrough scenario analysis. By itself, scenario analysis is very subjective but\ncombined with loss data it is a powerful tool to estimate operational risk\nlosses. Bayesian inference is a statistical technique well suited for combining\nexpert opinions and historical data. In this paper, we present examples of the\nBayesian inference methods for operational risk quantification.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1067v1"
    },
    {
        "title": "Estimation of Operational Risk Capital Charge under Parameter\n  Uncertainty",
        "authors": [
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  Many banks adopt the Loss Distribution Approach to quantify the operational\nrisk capital charge under Basel II requirements. It is common practice to\nestimate the capital charge using the 0.999 quantile of the annual loss\ndistribution, calculated using point estimators of the frequency and severity\ndistribution parameters. The uncertainty of the parameter estimates is\ntypically ignored. One of the unpleasant consequences for the banks accounting\nfor parameter uncertainty is an increase in the capital requirement. This paper\ndemonstrates how the parameter uncertainty can be taken into account using a\nBayesian framework that also allows for incorporation of expert opinions and\nexternal data into the estimation procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1771v1"
    },
    {
        "title": "A \"Toy\" Model for Operational Risk Quantification using Credibility\n  Theory",
        "authors": [
            "Hans Bühlmann",
            "Pavel V. Shevchenko",
            "Mario V. Wüthrich"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  To meet the Basel II regulatory requirements for the Advanced Measurement\nApproaches in operational risk, the bank's internal model should make use of\nthe internal data, relevant external data, scenario analysis and factors\nreflecting the business environment and internal control systems. One of the\nunresolved challenges in operational risk is combining of these data sources\nappropriately. In this paper we focus on quantification of the low frequency\nhigh impact losses exceeding some high threshold. We suggest a full credibility\ntheory approach to estimate frequency and severity distributions of these\nlosses by taking into account bank internal data, expert opinions and industry\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1772v1"
    },
    {
        "title": "Implementing Loss Distribution Approach for Operational Risk",
        "authors": [
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  To quantify the operational risk capital charge under the current regulatory\nframework for banking supervision, referred to as Basel II, many banks adopt\nthe Loss Distribution Approach. There are many modeling issues that should be\nresolved to use the approach in practice. In this paper we review the\nquantitative methods suggested in literature for implementation of the\napproach. In particular, the use of the Bayesian inference method that allows\nto take expert judgement and parameter uncertainty into account, modeling\ndependence and inclusion of insurance are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1805v2"
    },
    {
        "title": "Collective firm bankruptcies and phase transition in rating dynamics",
        "authors": [
            "Paweł Sieczka",
            "Janusz A. Hołyst"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  We present a simple model of firm rating evolution. We consider two sources\nof defaults: individual dynamics of economic development and Potts-like\ninteractions between firms. We show that such a defined model leads to phase\ntransition, which results in collective defaults. The existence of the\ncollective phase depends on the mean interaction strength. For small\ninteraction strength parameters, there are many independent bankruptcies of\nindividual companies. For large parameters, there are giant collective defaults\nof firm clusters. In the case when the individual firm dynamics favors dumping\nof rating changes, there is an optimal strength of the firm's interactions from\nthe systemic risk point of view.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.4430v2"
    },
    {
        "title": "Conditional Value-at-Risk Constraint and Loss Aversion Utility Functions",
        "authors": [
            "Laetitia Andrieu",
            "Michel De Lara",
            "Babacar Seck"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  We provide an economic interpretation of the practice consisting in\nincorporating risk measures as constraints in a classic expected return\nmaximization problem. For what we call the infimum of expectations class of\nrisk measures, we show that if the decision maker (DM) maximizes the\nexpectation of a random return under constraint that the risk measure is\nbounded above, he then behaves as a ``generalized expected utility maximizer''\nin the following sense. The DM exhibits ambiguity with respect to a family of\nutility functions defined on a larger set of decisions than the original one;\nhe adopts pessimism and performs first a minimization of expected utility over\nthis family, then performs a maximization over a new decisions set. This\neconomic behaviour is called ``Maxmin under risk'' and studied by Maccheroni\n(2002). This economic interpretation allows us to exhibit a loss aversion\nfactor when the risk measure is the Conditional Value-at-Risk.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.3425v1"
    },
    {
        "title": "A Bayesian Networks Approach to Operational Risk",
        "authors": [
            "V. Aquaro",
            "M. Bardoscia",
            "R. Bellotti",
            "A. Consiglio",
            "F. De Carlo",
            "G. Ferri"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  A system for Operational Risk management based on the computational paradigm\nof Bayesian Networks is presented. The algorithm allows the construction of a\nBayesian Network targeted for each bank using only internal loss data, and\ntakes into account in a simple and realistic way the correlations among\ndifferent processes of the bank. The internal losses are averaged over a\nvariable time horizon, so that the correlations at different times are removed,\nwhile the correlations at the same time are kept: the averaged losses are thus\nsuitable to perform the learning of the network topology and parameters. The\nalgorithm has been validated on synthetic time series. It should be stressed\nthat the practical implementation of the proposed algorithm has a small impact\non the organizational structure of a bank and requires an investment in human\nresources limited to the computational area.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.3968v2"
    },
    {
        "title": "Preferences Yielding the \"Precautionary Effect\"",
        "authors": [
            "Michel De Lara"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  Consider an agent taking two successive decisions to maximize his expected\nutility under uncertainty. After his first decision, a signal is revealed that\nprovides information about the state of nature. The observation of the signal\nallows the decision-maker to revise his prior and the second decision is taken\naccordingly. Assuming that the first decision is a scalar representing\nconsumption, the \\emph{precautionary effect} holds when initial consumption is\nless in the prospect of future information than without (no signal).\n\\citeauthor{Epstein1980:decision} in \\citep*{Epstein1980:decision} has provided\nthe most operative tool to exhibit the precautionary effect. Epstein's Theorem\nholds true when the difference of two convex functions is either convex or\nconcave, which is not a straightforward property, and which is difficult to\nconnect to the primitives of the economic model. Our main contribution consists\nin giving a geometric characterization of when the difference of two convex\nfunctions is convex, then in relating this to the primitive utility model. With\nthis tool, we are able to study and unite a large body of the literature on the\nprecautionary effect.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.4093v1"
    },
    {
        "title": "Les Générateurs de Scénarios Économiques : quelle utilisation en\n  assurance?",
        "authors": [
            "Alaeddine Faleh",
            "Frédéric Planchet",
            "Didier Rullière"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  In this paper, we present the principal components of an economic scenario\ngenerator (ESG), both for the theoretical design and for practical\nimplementation. The choice of these components should be linked to the ultimate\nvocation of the economic scenario generator, which can be either a tool for\npricing financial products or a tool for projection and risk management. We\nthen develop a study on some performance measure indicators of the ESG as an\ninput for the decision-making process, namely the indicators of stability and\nbias absence. Finally, a numerical application illustrates the main ideas of\nthe paper.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.3472v1"
    },
    {
        "title": "Allocation d'actifs selon le critère de maximisation des fonds propres\n  économiques en assurance non-vie",
        "authors": [
            "Frédéric Planchet",
            "Pierre-Emanuel Thérond"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  The economic equities maximization criterion (MFPE) leads to the choice of\nfinancial portfolio, which maximizes the ratio of the expected value of the\ninsurance company on the capital. This criterion is presented in the framework\nof a non-life insurance company and is applied within the framework of the\nFrench legislation and in a lawful context inspired of the works in progress\nabout the European project Solvency 2. In the French regulation case, the\nrequired solvency margin does not depend of the asset allocation. It is quite\ndifferent in the Solvency 2 framework because the target capital has to control\nthe global risk of the company. And the financial risk takes part of this\nglobal risk. Thus the economic equities maximization criterion leads to search\na couple asset allocation / equities which solves a stochastic program. A\nnumerical illustration makes it possible to analyze the consequences of the\nintroduction of a Solvency 2 framework on the technical reserves and the\nequities of a non-life insurance company and on the optimal allocation due to\nthe economic equities maximization criterion. Finally, the impact of a\nmisspecification of the risky asset model on the optimal allocation is\nillustrated.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1867v1"
    },
    {
        "title": "Mesure des risques de marché et de souscription vie en situation\n  d'information incomplète pour un portefeuille de prévoyance",
        "authors": [
            "Jean-Paul Félix",
            "Frédéric Planchet"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  In the framework of Embedded Value new standards, namely the MCEV norms, the\nlatest principles published in June 2008 address the issue of market and\nunderwriting risks measurement by using stochastic models of projection and\nvalorization. Knowing that stochastic models particularly data-consuming, the\nquestion which can arise is the treatment of insurance portfolios only\navailable in aggregate data or portfolios in situation of incomplete\ninformation. The aim of this article is to propose a pragmatic modeling of\nthese risks tied up with death covers of individual protection products in\nthese situations.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1908v1"
    },
    {
        "title": "Optimal Reversible Annuities to Minimize the Probability of Lifetime\n  Ruin",
        "authors": [
            "Ting Wang",
            "Virginia R. Young"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We find the minimum probability of lifetime ruin of an investor who can\ninvest in a market with a risky and a riskless asset and who can purchase a\nreversible life annuity. The surrender charge of a life annuity is a proportion\nof its value. Ruin occurs when the total of the value of the risky and riskless\nassets and the surrender value of the life annuity reaches zero. We find the\noptimal investment strategy and optimal annuity purchase and surrender\nstrategies in two situations: (i) the value of the risky and riskless assets is\nallowed to be negative, with the imputed surrender value of the life annuity\nkeeping the total positive; or (ii) the value of the risky and riskless assets\nis required to be non-negative. In the first case, although the individual has\nthe flexiblity to buy or sell at any time, we find that the individual will not\nbuy a life annuity unless she can cover all her consumption via the annuity and\nshe will never sell her annuity. In the second case, the individual surrenders\njust enough annuity income to keep her total assets positive. However, in this\nsecond case, the individual's annuity purchasing strategy depends on the size\nof the proportional surrender charge. When the charge is large enough, the\nindividual will not buy a life annuity unless she can cover all her\nconsumption, the so-called safe level. When the charge is small enough, the\nindividual will buy a life annuity at a wealth lower than this safe level.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4270v1"
    },
    {
        "title": "The two defaults scenario for stressing credit portfolio loss\n  distributions",
        "authors": [
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  The impact of a stress scenario of default events on the loss distribution of\na credit portfolio can be assessed by determining the loss distribution\nconditional on these events. While it is conceptually easy to estimate loss\ndistributions conditional on default events by means of Monte Carlo simulation,\nit becomes impractical for two or more simultaneous defaults as then the\nconditioning event is extremely rare. We provide an analytical approach to the\ncalculation of the conditional loss distribution for the CreditRisk+ portfolio\nmodel with independent random loss given default distributions. The analytical\nsolution for this case can be used to check the accuracy of an approximation to\nthe conditional loss distribution whereby the unconditional model is run with\nstressed input probabilities of default (PDs). It turns out that this\napproximation is unbiased. Numerical examples, however, suggest that the\napproximation may be seriously inaccurate but that the inaccuracy leads to\noverestimation of tail losses and hence the approach errs on the conservative\nside.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2604v2"
    },
    {
        "title": "Tracking errors from discrete hedging in exponential Lévy models",
        "authors": [
            "Mats Brodén",
            "Peter Tankov"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We analyze the errors arising from discrete readjustment of the hedging\nportfolio when hedging options in exponential Levy models, and establish the\nrate at which the expected squared error goes to zero when the readjustment\nfrequency increases. We compare the quadratic hedging strategy with the common\nmarket practice of delta hedging, and show that for discontinuous option\npay-offs the latter strategy may suffer from very large discretization errors.\nFor options with discontinuous pay-offs, the convergence rate depends on the\nunderlying Levy process, and we give an explicit relation between the rate and\nthe Blumenthal-Getoor index of the process.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.0709v1"
    },
    {
        "title": "Multivariate heavy-tailed models for Value-at-Risk estimation",
        "authors": [
            "Carlo Marinelli",
            "Stefano d'Addona",
            "Svetlozar T. Rachev"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  For purposes of Value-at-Risk estimation, we consider several multivariate\nfamilies of heavy-tailed distributions, which can be seen as multidimensional\nversions of Paretian stable and Student's t distributions allowing different\nmarginals to have different tail thickness. After a discussion of relevant\nestimation and simulation issues, we conduct a backtesting study on a set of\nportfolios containing derivative instruments, using historical US stock price\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.2862v3"
    },
    {
        "title": "Recent progress in random metric theory and its applications to\n  conditional risk measures",
        "authors": [
            "Tiexin Guo"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  The purpose of this paper is to give a selective survey on recent progress in\nrandom metric theory and its applications to conditional risk measures. This\npaper includes eight sections. Section 1 is a longer introduction, which gives\na brief introduction to random metric theory, risk measures and conditional\nrisk measures. Section 2 gives the central framework in random metric theory,\ntopological structures, important examples, the notions of a random conjugate\nspace and the Hahn-Banach theorems for random linear functionals. Section 3\ngives several important representation theorems for random conjugate spaces.\nSection 4 gives characterizations for a complete random normed module to be\nrandom reflexive. Section 5 gives hyperplane separation theorems currently\navailable in random locally convex modules. Section 6 gives the theory of\nrandom duality with respect to the locally $L^{0}-$convex topology and in\nparticular a characterization for a locally $L^{0}-$convex module to be\n$L^{0}-$pre$-$barreled. Section 7 gives some basic results on $L^{0}-$convex\nanalysis together with some applications to conditional risk measures. Finally,\nSection 8 is devoted to extensions of conditional convex risk measures, which\nshows that every representable $L^{\\infty}-$type of conditional convex risk\nmeasure and every continuous $L^{p}-$type of convex conditional risk measure\n($1\\leq p<+\\infty$) can be extended to an $L^{\\infty}_{\\cal F}({\\cal E})-$type\nof $\\sigma_{\\epsilon,\\lambda}(L^{\\infty}_{\\cal F}({\\cal E}), L^{1}_{\\cal\nF}({\\cal E}))-$lower semicontinuous conditional convex risk measure and an\n$L^{p}_{\\cal F}({\\cal E})-$type of ${\\cal T}_{\\epsilon,\\lambda}-$continuous\nconditional convex risk measure ($1\\leq p<+\\infty$), respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.0697v17"
    },
    {
        "title": "A Loan Portfolio Model Subject to Random Liabilities and Systemic Jump\n  Risk",
        "authors": [
            "Luis H. R. Alvarez",
            "Jani Sainio"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We extend the Vasi\\v{c}ek loan portfolio model to a setting where liabilities\nfluctuate randomly and asset values may be subject to systemic jump risk. We\nderive the probability distribution of the percentage loss of a uniform\nportfolio and analyze its properties. We find that the impact of liability risk\nis ambiguous and depends on the correlation between the continuous aggregate\nfactor and the asset-liability ratio as well as on the default intensity. We\nalso find that systemic jump risk has a significant impact on the upper\npercentiles of the loss distribution and, therefore, on both the VaR-measure as\nwell as on the expected shortfall.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.0863v1"
    },
    {
        "title": "Alarm System for Insurance Companies: A Strategy for Capital Allocation",
        "authors": [
            "Shubhabrata Das",
            "Marie Kratz"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  One possible way of risk management for an insurance company is to develop an\nearly and appropriate alarm system before the possible ruin. The ruin is\ndefined through the status of the aggregate risk process, which in turn is\ndetermined by premium accumulation as well as claim settlement outgo for the\ninsurance company. The main purpose of this work is to design an effective\nalarm system, i.e. to define alarm times and to recommend augmentation of\ncapital of suitable magnitude at those points to prevent or reduce the chance\nof ruin. To draw a fair measure of effectiveness of alarm system, comparison is\ndrawn between an alarm system, with capital being added at the sound of every\nalarm, and the corresponding system without any alarm, but an equivalently\nhigher initial capital. Analytical results are obtained in general setup and\nthis is backed up by simulated performances with various types of loss severity\ndistributions. This provides a strategy for suitably spreading out the capital\nand yet addressing survivability concerns at satisfactory level.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.5473v1"
    },
    {
        "title": "A Dynamical Model for Forecasting Operational Losses",
        "authors": [
            "Marco Bardoscia",
            "Roberto Bellotti"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  A novel dynamical model for the study of operational risk in banks and\nsuitable for the calculation of the Value at Risk (VaR) is proposed. The\nequation of motion takes into account the interactions among different bank's\nprocesses, the spontaneous generation of losses via a noise term and the\nefforts made by the bank to avoid their occurrence. Since the model is very\ngeneral, it can be tailored on the internal organizational structure of a\nspecific bank by estimating some of its parameters from historical operational\nlosses. The model is exactly solved in the case in which there are no causal\nloops in the matrix of couplings and it is shown how the solution can be\nexploited to estimate also the parameters of the noise. The forecasting power\nof the model is investigated by using a fraction $f$ of simulated data to\nestimate the parameters, showing that for $f = 0.75$ the VaR can be forecast\nwith an error $\\simeq 10^{-3}$.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.0026v6"
    },
    {
        "title": "Target market risk evaluation",
        "authors": [
            "Anda Gheorghiu",
            "Anca Gheorghiu",
            "Ion Spanulescu"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  After the shocking series of bankruptcies started in 2008, the public does\nnot trust anymore the classical methods of assessing business risks. The global\neconomic severe downturn caused demand for both developed and emerging\neconomies' exports to drop and the crisis became truly global. However, this\ncurrent crisis offers opportunities for those companies able to play well their\ncards. Entering new markets has always been a hazardous entrepreneurial\nattempt, but also a rewarding one, in the case of success. The paper presents a\nnew indicator meant for assessing the prospective of success or failure for a\ncompany trying to enter a new market by using an associative strategy. In order\nto take the right decision concerning the optimal market entry strategy,\nmarketers may use a software application, \"AnBilant\", created by a research\nteam from Hyperion University.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.1908v1"
    },
    {
        "title": "Liquidity-adjusted Market Risk Measures with Stochastic Holding Period",
        "authors": [
            "Damiano Brigo",
            "Claudio Nordio"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  Within the context of risk integration, we introduce in risk measurement\nstochastic holding period (SHP) models. This is done in order to obtain a\n`liquidity-adjusted risk measure' characterized by the absence of a fixed time\nhorizon. The underlying assumption is that - due to changes on market liquidity\nconditions - one operates along an `operational time' to which the P&L process\nof liquidating a market portfolio is referred. This framework leads to a\nmixture of distributions for the portfolio returns, potentially allowing for\nskewness, heavy tails and extreme scenarios. We analyze the impact of possible\ndistributional choices for the SHP. In a multivariate setting, we hint at the\npossible introduction of dependent SHP processes, which potentially lead to non\nlinear dependence among the P&L processes and therefore to tail dependence\nacross assets in the portfolio, although this may require drastic choices on\nthe SHP distributions. We also find that increasing dependence as measured by\nKendall's tau through common SHP's appears to be unfeasible. We finally discuss\npotential developments following future availability of market data.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.3760v2"
    },
    {
        "title": "Capital allocation for credit portfolios under normal and stressed\n  market conditions",
        "authors": [
            "Norbert Jobst",
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  If the probability of default parameters (PDs) fed as input into a credit\nportfolio model are estimated as through-the-cycle (TTC) PDs stressed market\nconditions have little impact on the results of the capital calculations\nconducted with the model. At first glance, this is totally different if the PDs\nare estimated as point-in-time (PIT) PDs. However, it can be argued that the\nreflection of stressed market conditions in input PDs should correspond to the\nuse of reduced correlation parameters or even the removal of correlations in\nthe model. Additionally, the confidence levels applied for the capital\ncalculations might be made reflective of the changing market conditions. We\ninvestigate the interplay of PIT PDs, correlations, and confidence levels in a\ncredit portfolio model in more detail and analyse possible designs of\ncapital-levelling policies. Our findings may of interest to banks that want to\ncombine their approaches to capital measurement and allocation with active\nportfolio management that, by its nature, needs to be reflective of current\nmarket conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.5401v2"
    },
    {
        "title": "Quantile hedging for basket derivatives",
        "authors": [
            "Michał Barski"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  The problem of quantile hedging for basket derivatives in the Black-Scholes\nmodel with correlation is considered. Explicit formulas for the probability\nmaximizing function and the cost reduction function are derived. Applicability\nof the results for the widely traded derivatives as digital, quantos,\noutperformance and spread options is shown.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.5810v3"
    },
    {
        "title": "Markov chain Monte Carlo estimation of default and recovery: dependent\n  via the latent systematic factor",
        "authors": [
            "Xiaolin Luo",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  It is a well known fact that recovery rates tend to go down when the number\nof defaults goes up in economic downturns. We demonstrate how the loss given\ndefault model with the default and recovery dependent via the latent systematic\nrisk factor can be estimated using Bayesian inference methodology and Markov\nchain Monte Carlo method. This approach is very convenient for joint estimation\nof all model parameters and latent systematic factors. Moreover, all relevant\nuncertainties are easily quantified. Typically available data are annual\naverages of defaults and recoveries and thus the datasets are small and\nparameter uncertainty is significant. In this case Bayesian approach is\nsuperior to the maximum likelihood method that relies on a large sample limit\nGaussian approximation for the parameter uncertainty. As an example, we\nconsider a homogeneous portfolio with one latent factor. However, the approach\ncan be easily extended to deal with non-homogenous portfolios and several\nlatent factors.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.2827v5"
    },
    {
        "title": "Set-valued risk measures for conical market models",
        "authors": [
            "Andreas H. Hamel",
            "Frank Heyde",
            "Birgit Rudloff"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  Set-valued risk measures on $L^p_d$ with $0 \\leq p \\leq \\infty$ for conical\nmarket models are defined, primal and dual representation results are given.\nThe collection of initial endowments which allow to super-hedge a multivariate\nclaim are shown to form the values of a set-valued sublinear (coherent) risk\nmeasure. Scalar risk measures with multiple eligible assets also turn out to be\na special case within the set-valued framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.5986v1"
    },
    {
        "title": "An Active Margin System and its Application in Chinese Margin Lending\n  Market",
        "authors": [
            "Guanghui Huang",
            "Jianping Wan",
            "Cheng Chen"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  In order to protect brokers from customer defaults in a volatile market, an\nactive margin system is proposed for the transactions of margin lending in\nChina. The probability of negative return under the condition that collaterals\nare liquidated in a falling market is used to measure the risk associated with\nmargin loans, and a recursive algorithm is proposed to calculate this\nprobability under a Markov chain model. The optimal maintenance margin ratio\ncan be given under the constraint of the proposed risk measurement for a\nspecified amount of initial margin. An example of such a margin system is\nconstructed and applied to $26,800$ margin loans of 134 stocks traded on the\nShanghai Stock Exchange. The empirical results indicate that the proposed\nmethod is an operational method for brokers to set margin system with a clearly\nspecified target of risk control.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.3974v1"
    },
    {
        "title": "Dependence of defaults and recoveries in structural credit risk models",
        "authors": [
            "Rudi Schäfer",
            "Alexander F. R. Koivusalo"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  The current research on credit risk is primarily focused on modeling default\nprobabilities. Recovery rates are often treated as an afterthought; they are\nmodeled independently, in many cases they are even assumed constant. This is\ndespite of their pronounced effect on the tail of the loss distribution. Here,\nwe take a step back, historically, and start again from the Merton model, where\ndefaults and recoveries are both determined by an underlying process. Hence,\nthey are intrinsically connected. For the diffusion process, we can derive the\nfunctional relation between expected recovery rate and default probability.\nThis relation depends on a single parameter only. In Monte Carlo simulations we\nfind that the same functional dependence also holds for jump-diffusion and\nGARCH processes. We discuss how to incorporate this structural recovery rate\ninto reduced form models, in order to restore essential structural information\nwhich is usually neglected in the reduced-form approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3150v2"
    },
    {
        "title": "A Random Matrix Approach to Credit Risk",
        "authors": [
            "Michael C. Münnix",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  We estimate generic statistical properties of a structural credit risk model\nby considering an ensemble of correlation matrices. This ensemble is set up by\nRandom Matrix Theory. We demonstrate analytically that the presence of\ncorrelations severely limits the effect of diversification in a credit\nportfolio if the correlations are not identically zero. The existence of\ncorrelations alters the tails of the loss distribution considerably, even if\ntheir average is zero. Under the assumption of randomly fluctuating\ncorrelations, a lower bound for the estimation of the loss distribution is\nprovided.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3900v7"
    },
    {
        "title": "Portfolio Insurance under a risk-measure constraint",
        "authors": [
            "Carmine De Franco",
            "Peter Tankov"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  We study the problem of portfolio insurance from the point of view of a fund\nmanager, who guarantees to the investor that the portfolio value at maturity\nwill be above a fixed threshold. If, at maturity, the portfolio value is below\nthe guaranteed level, a third party will refund the investor up to the\nguarantee. In exchange for this protection, the third party imposes a limit on\nthe risk exposure of the fund manager, in the form of a convex monetary risk\nmeasure. The fund manager therefore tries to maximize the investor's utility\nfunction subject to the risk measure constraint.We give a full solution to this\nnonconvex optimization problem in the complete market setting and show in\nparticular that the choice of the risk measure is crucial for the optimal\nportfolio to exist. Explicit results are provided for the entropic risk measure\n(for which the optimal portfolio always exists) and for the class of spectral\nrisk measures (for which the optimal portfolio may fail to exist in some\ncases).\n",
        "pdf_link": "http://arxiv.org/pdf/1102.4489v1"
    },
    {
        "title": "Calibration of structural and reduced-form recovery models",
        "authors": [
            "Alexander F. R. Koivusalo",
            "Rudi Schäfer"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  In recent years research on credit risk modelling has mainly focused on\ndefault probabilities. Recovery rates are usually modelled independently, quite\noften they are even assumed constant. Then, however, the structural connection\nbetween recovery rates and default probabilities is lost and the tails of the\nloss distribution can be underestimated considerably. The problem of\nunderestimating tail losses becomes even more severe, when calibration issues\nare taken into account. To demonstrate this we choose a Merton-type structural\nmodel as our reference system. Diffusion and jump-diffusion are considered as\nunderlying processes. We run Monte Carlo simulations of this model and\ncalibrate different recovery models to the simulation data. For simplicity, we\ntake the default probabilities directly from the simulation data. We compare a\nreduced-form model for recoveries with a constant recovery approach. In\naddition, we consider a functional dependence between recovery rates and\ndefault probabilities. This dependence can be derived analytically for the\ndiffusion case. We find that the constant recovery approach drastically and\nsystematically underestimates the tail of the loss distribution. The\nreduced-form recovery model shows better results, when all simulation data is\nused for calibration. However, if we restrict the simulation data used for\ncalibration, the results for the reduced-form model deteriorate. We find the\nmost reliable and stable results, when we make use of the functional dependence\nbetween recovery rates and default probabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.4864v1"
    },
    {
        "title": "The dynamics of financial stability in complex networks",
        "authors": [
            "João P. da Cruz",
            "Pedro G. Lind"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  We address the problem of banking system resilience by applying\noff-equilibrium statistical physics to a system of particles, representing the\neconomic agents, modelled according to the theoretical foundation of the\ncurrent banking regulation, the so called Merton-Vasicek model. Economic agents\nare attracted to each other to exchange `economic energy', forming a network of\ntrades. When the capital level of one economic agent drops below a minimum, the\neconomic agent becomes insolvent. The insolvency of one single economic agent\naffects the economic energy of all its neighbours which thus become susceptible\nto insolvency, being able to trigger a chain of insolvencies (avalanche). We\nshow that the distribution of avalanche sizes follows a power-law whose\nexponent depends on the minimum capital level. Furthermore, we present evidence\nthat under an increase in the minimum capital level, large crashes will be\navoided only if one assumes that agents will accept a drop in business levels,\nwhile keeping their trading attitudes and policies unchanged. The alternative\nassumption, that agents will try to restore their business levels, may lead to\nthe unexpected consequence that large crises occur with higher probability.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.0717v2"
    },
    {
        "title": "Spectral Risk Measures: Properties and Limitations",
        "authors": [
            "Kevin Dowd",
            "John Cotter",
            "Ghulam Sorwar"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Spectral risk measures (SRMs) are risk measures that take account of user\nriskaversion, but to date there has been little guidance on the choice of\nutility function underlying them. This paper addresses this issue by examining\nalternative approaches based on exponential and power utility functions. A\nnumber of problems are identified with both types of spectral risk measure. The\ngeneral lesson is that users of spectral risk measures must be careful to\nselect utility functions that fit the features of the particular problems they\nare dealing with, and should be especially careful when using power SRMs.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5674v1"
    },
    {
        "title": "Extreme Measures of Agricultural Financial Risk",
        "authors": [
            "John Cotter",
            "Kevin Dowd",
            "Wyn Morgan"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Risk is an inherent feature of agricultural production and marketing and\naccurate measurement of it helps inform more efficient use of resources. This\npaper examines three tail quantile-based risk measures applied to the\nestimation of extreme agricultural financial risk for corn and soybean\nproduction in the US: Value at Risk (VaR), Expected Shortfall (ES) and Spectral\nRisk Measures (SRMs). We use Extreme Value Theory (EVT) to model the tail\nreturns and present results for these three different risk measures using\nagricultural futures market data. We compare the estimated risk measures in\nterms of their size and precision, and find that they are all considerably\nhigher than normal estimates; they are also quite uncertain, and become more\nuncertain as the risks involved become more extreme.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5962v1"
    },
    {
        "title": "Concave Distortion Semigroups",
        "authors": [
            "Alexander Cherny",
            "Damir Filipović"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  The problem behind this paper is the proper measurement of the degree of\nquality/acceptability/distance to arbitrage of trades. We are narrowing the\nclass of coherent acceptability indices introduced by Cherny and Madan (2007)\nby imposing an additional mathematical property. For this, we introduce the\nnotion of a concave distortion semigroup as a family $(\\Psi_t)_{t\\ge0}$ of\nconcave increasing functions $[0,1]\\to[0,1]$ satisfying the semigroup property\n$$ \\Psi_s\\circ\\Psi_t=\\Psi_{s+t},\\quad s,t\\ge0. $$ The goal of the paper is the\ninvestigation of these semigroups with regard to the following aspects:\nrepresentation of distortion semigroups; properties of distortion semigroups\ndesirable from the economical or mathematical perspective; determining which\nconcave distortions belong to some distortion semigroup.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.0508v1"
    },
    {
        "title": "Banking retail consumer finance data generator - credit scoring data\n  repository",
        "authors": [
            "Karol Przanowski"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This paper presents two cases of random banking data generators based on\nmigration matrices and scoring rules. The banking data generator is a new hope\nin researches of finding the proving method of comparisons of various credit\nscoring techniques. There is analyzed the influence of one cyclic\nmacro--economic variable on stability in the time account and client\ncharacteristics. Data are very useful for various analyses to understand in the\nbetter way the complexity of the banking processes and also for students and\ntheir researches. There are presented very interesting conclusions for crisis\nbehavior, namely that if a crisis is impacted by many factors, both customer\ncharacteristics: application and behavioral; then there is very difficult to\nindicate these factors in the typical scoring analysis and the crisis is\neverywhere, in every kind of risk reports.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.2968v1"
    },
    {
        "title": "A Stochastic Model for the Analysis of Demographic Risk in Pay-As-You-Go\n  Pension Funds",
        "authors": [
            "Alessandro Fiori Maccioni"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This research presents an analysis of the demographic risk related to future\nmembership patterns in pension funds with restricted entrance, financed under a\npay-as-you-go scheme. The paper, therefore, proposes a stochastic model for\ninvestigating the behaviour of the demographic variable \"new entrants\" and the\ninfluence it exerts on the financial dynamics of such funds. Further\ninformation on pension funds of Italian professional categories and an\napplication to the Cassa Nazionale di Previdenza e Assistenza dei Dottori\nCommercialisti (CNPADC) are then provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.5081v1"
    },
    {
        "title": "One-year reserve risk including a tail factor: closed formula and\n  bootstrap approaches",
        "authors": [
            "Alexandre Boumezoued",
            "Yoboua Angoua",
            "Laurent Devineau",
            "Jean-Philippe Boisseau"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  In this paper, we detail the main simulation methods used in practice to\nmeasure one-year reserve risk, and describe the bootstrap method providing an\nempirical distribution of the Claims Development Result (CDR) whose variance is\nidentical to the closed-form expression of the prediction error proposed by\nW\\\"uthrich et al. (2008). In particular, we integrate the stochastic modeling\nof a tail factor in the bootstrap procedure. We demonstrate the equivalence\nwith existing analytical results and develop closed-form expressions for the\nerror of prediction including a tail factor. A numerical example is given at\nthe end of this study.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0164v2"
    },
    {
        "title": "Quantifying mortality risk in small defined-benefit pension schemes",
        "authors": [
            "Catherine Donnelly"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  A risk of small defined-benefit pension schemes is that there are too few\nmembers to eliminate idiosyncratic mortality risk, that is there are too few\nmembers to effectively pool mortality risk. This means that when there are few\nmembers in the scheme, there is an increased risk of the liability value\ndeviating significantly from the expected liability value, as compared to a\nlarge scheme.\n  We quantify this risk through examining the coefficient of variation of a\nscheme's liability value relative to its expected value. We examine how the\ncoefficient of variation varies with the number of members and find that, even\nwith a few hundred members in the scheme, idiosyncratic mortality risk may\nstill be significant. Using a stochastic mortality model reduces the\nidiosyncratic mortality risk but at the cost of increasing the overall\nmortality risk in the scheme.\n  Next we quantify the amount of the mortality risk concentrated in the\nexecutive section of the scheme, where the executives receive a benefit that is\nhigher than the non-executive benefit. We use the Euler capital allocation\nprinciple to allocate the total standard deviation of the liability value\nbetween the executive and non-executive sections. We find that the proportion\nof the standard deviation allocated to the executive section is higher than is\nsuggested by an allocation based on the members' benefit amounts. While the\nresults are sensitive to the choice of mortality model, they do suggest that\nthe mortality risk of the scheme should be monitored and managed within the\nsections of a scheme, and not only on a scheme-wide basis.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.1380v4"
    },
    {
        "title": "Hedging strategies with a put option and their failure rates",
        "authors": [
            "Guanghui Huang",
            "Jing Xu",
            "Wenting Xing"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  The problem of stock hedging is reconsidered in this paper, where a put\noption is chosen from a set of available put options to hedge the market risk\nof a stock. A formula is proposed to determine the probability that the\npotential loss exceeds a predetermined level of Value-at-Risk, which is used to\nfind the optimal strike price and optimal hedge ratio. The assumptions that the\nchosen put option finishes in-the-money and the constraint of hedging budget is\nbinding are relaxed in this paper. A hypothesis test is proposed to determine\nwhether the failure rate of hedging strategy is greater than the predetermined\nlevel of risk. The performances of the proposed method and the method with\nthose two assumptions are compared through simulations. The results of\nsimulated investigations indicate that the proposed method is much more prudent\nthan the method with those two assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.0159v1"
    },
    {
        "title": "Menger 1934 revisited",
        "authors": [
            "Ole Peters"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Karl Menger's 1934 paper on the St. Petersburg paradox contains mathematical\nerrors that invalidate his conclusion that unbounded utility functions,\nspecifically Bernoulli's logarithmic utility, fail to resolve modified versions\nof the St. Petersburg paradox.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.1578v1"
    },
    {
        "title": "Historical risk measures on stock market indices and energy markets",
        "authors": [
            "Wayne Tarrant"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  In this paper we look at the efficacy of different risk measures on energy\nmarkets and across several different stock market indices. We use both the\nValue at Risk and the Tail Conditional Expectation on each of these data sets.\nWe also consider several different durations and levels for historical risk\nmeasures. Through our results we make some recommendations for a robust risk\nmanagement strategy that involves historical risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.4421v1"
    },
    {
        "title": "A Mathematical Method for Deriving the Relative Effect of Serviceability\n  on Default Risk",
        "authors": [
            "Graham Andersen",
            "David Chisholm"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  The writers propose a mathematical Method for deriving risk weights which\ndescribe how a borrower's income, relative to their debt service obligations\n(serviceability) affects the probability of default of the loan.\n  The Method considers the borrower's income not simply as a known quantity at\nthe time the loan is made, but as an uncertain quantity following a statistical\ndistribution at some later point in the life of the loan. This allows a\nprobability to be associated with an income level leading to default, so that\nthe relative risk associated with different serviceability levels can be\nquantified. In a sense, the Method can be thought of as an extension of the\nMerton Model to quantities that fail to satisfy Merton's 'critical' assumptions\nrelating to the efficient markets hypothesis.\n  A set of numerical examples of risk weights derived using the Method suggest\nthat serviceability may be under-represented as a risk factor in many mortgage\ncredit risk models.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.5397v1"
    },
    {
        "title": "Restructuring Counterparty Credit Risk",
        "authors": [
            "Claudio Albanese",
            "Damiano Brigo",
            "Frank Oertel"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  We introduce an innovative theoretical framework to model derivative\ntransactions between defaultable entities based on the principle of arbitrage\nfreedom. Our framework extends the traditional formulations based on Credit and\nDebit Valuation Adjustments (CVA and DVA). Depending on how the default\ncontingency is accounted for, we list a total of ten different structuring\nstyles. These include bipartite structures between a bank and a counterparty,\ntri-partite structures with one margin lender in addition, quadri-partite\nstructures with two margin lenders and, most importantly, configurations where\nall derivative transactions are cleared through a Central Counterparty (CCP).\nWe compare the various structuring styles under a number of criteria including\nconsistency from an accounting standpoint, counterparty risk hedgeability,\nnumerical complexity, transaction portability upon default, induced behaviour\nand macro-economic impact of the implied wealth allocation.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.1607v2"
    },
    {
        "title": "Bayesian estimation of probabilities of default for low default\n  portfolios",
        "authors": [
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  The estimation of probabilities of default (PDs) for low default portfolios\nby means of upper confidence bounds is a well established procedure in many\nfinancial institutions. However, there are often discussions within the\ninstitutions or between institutions and supervisors about which confidence\nlevel to use for the estimation. The Bayesian estimator for the PD based on the\nuninformed, uniform prior distribution is an obvious alternative that avoids\nthe choice of a confidence level. In this paper, we demonstrate that in the\ncase of independent default events the upper confidence bounds can be\nrepresented as quantiles of a Bayesian posterior distribution based on a prior\nthat is slightly more conservative than the uninformed prior. We then describe\nhow to implement the uninformed and conservative Bayesian estimators in the\ndependent one- and multi-period default data cases and compare their estimates\nto the upper confidence bound estimates. The comparison leads us to suggest a\nconstrained version of the uninformed (neutral) Bayesian estimator as an\nalternative to the upper confidence bound estimators.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.5550v5"
    },
    {
        "title": "A Dynamical Approach to Operational Risk Measurement",
        "authors": [
            "Marco Bardoscia",
            "Roberto Bellotti"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We propose a dynamical model for the estimation of Operational Risk in\nbanking institutions. Operational Risk is the risk that a financial loss occurs\nas the result of failed processes. Examples of operational losses are the ones\ngenerated by internal frauds, human errors or failed transactions. In order to\nencompass the most heterogeneous set of processes, in our approach the losses\nof each process are generated by the interplay among random noise, interactions\nwith other processes and the efforts the bank makes to avoid losses. We show\nhow some relevant parameters of the model can be estimated from a database of\nhistorical operational losses, validate the estimation procedure and test the\nforecasting power of the model. Some advantages of our approach over the\ntraditional statistical techniques are that it allows to follow the whole time\nevolution of the losses and to take into account different-time correlations\namong the processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.2532v1"
    },
    {
        "title": "Derivatives and Credit Contagion in Interconnected Networks",
        "authors": [
            "Sebastian Heise",
            "Reimer Kuehn"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  The importance of adequately modeling credit risk has once again been\nhighlighted in the recent financial crisis. Defaults tend to cluster around\ntimes of economic stress due to poor macro-economic conditions, {\\em but also}\nby directly triggering each other through contagion. Although credit default\nswaps have radically altered the dynamics of contagion for more than a decade,\nmodels quantifying their impact on systemic risk are still missing. Here, we\nexamine contagion through credit default swaps in a stylized economic network\nof corporates and financial institutions. We analyse such a system using a\nstochastic setting, which allows us to exploit limit theorems to exactly solve\nthe contagion dynamics for the entire system. Our analysis shows that, by\ncreating additional contagion channels, CDS can actually lead to greater\ninstability of the entire network in times of economic stress. This is\nparticularly pronounced when CDS are used by banks to expand their loan books\n(arguing that CDS would offload the additional risks from their balance\nsheets). Thus, even with complete hedging through CDS, a significant loan book\nexpansion can lead to considerably enhanced probabilities for the occurrence of\nvery large losses and very high default rates in the system. Our approach adds\na new dimension to research on credit contagion, and could feed into a rational\nunderpinning of an improved regulatory framework for credit derivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.3025v1"
    },
    {
        "title": "Active margin system for margin loans and its application in Chinese\n  market: using cash and randomly selected stock as collateral",
        "authors": [
            "Guanghui Huang",
            "Wenting Xin",
            "Weiqing Gu"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  An active margin system for margin loans is proposed for Chinese margin\nlending market, which uses cash and randomly selected stock as collateral. The\nconditional probability of negative return(CPNR) after a forced sale of\nsecurities from under-margined account in a falling market is used to measure\nthe risk faced by the brokers, and the margin system is chosen under the\nconstraint of the risk measure. In order to calculate CPNR, a recursive\nalgorithm is proposed under a Markov chain model, which is constructed by\nsample learning method. The resulted margin system is an active system, which\nis able to adjust actively with respect to the changes of stock prices and the\nchanges of different collateral. The resulted margin system is applied to\n30,000 margin loans of 150 stocks listed on Shanghai Stock Exchange. The\nempirical results show the number of margin calls and the average costs of the\nloans under the proposed margin system are less than their counterparts under\nthe system required by SSE and SZSE.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4913v2"
    },
    {
        "title": "Active margin system for margin loans using cash and stock as collateral\n  and its application in Chinese market",
        "authors": [
            "Guanghui Huang",
            "Weiqing Gu",
            "Wenting Xing",
            "Hongyu Li"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  Margin system for margin loans using cash and stock as collateral is\nconsidered in this paper, which is the line of defence for brokers against risk\nassociated with margin trading. The conditional probability of negative return\nis used as risk measure, and a recursive algorithm is proposed to realize this\nmeasure under a Markov chain model. Optimal margin system is chosen from those\nsystems which satisfy the constraint of the risk measure. The resulted margin\nsystem is able to adjust actively with respect to the changes of stock prices.\nThe margin system required by the Shanghai Stock Exchange is compared with the\nproposed system, where 25,200 margin loans of 126 stocks listed on the SSE are\ninvestigated. It is found that the number of margin calls under the proposed\nmargin system is significantly less than its counterpart under the required\nsystem for the same level of risk, and the average costs of the loans are\nsimilar under the two types of margin systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.5180v1"
    },
    {
        "title": "Using Decision Tree Learner to Classify Solvency Position for Thai\n  Non-life Insurance Companies",
        "authors": [
            "Phaiboon Jhongpita",
            "Sukree Sinthupinyo",
            "Thitivadee Chaiyawat"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This paper introduces a Decision Tree Learner as an early warning system for\nclassification of the non-life insurance companies according to their financial\nsolid as strong, moderate, weak, or insolvency. In this study, we ran several\nexperiments to show that the proposed model can achieve a good result using\nstandard 10 fold crossvalidation, split train and test data set, and separated\ntest set. The results show that the method is effective and can accurately\nclassify the solvency position.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.3031v1"
    },
    {
        "title": "Empirical Evidence for the Structural Recovery Model",
        "authors": [
            "Alexander Becker",
            "Alexander F. R. Koivusalo",
            "Rudi Schäfer"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  While defaults are rare events, losses can be substantial even for credit\nportfolios with a large number of contracts. Therefore, not only a good\nevaluation of the probability of default is crucial, but also the severity of\nlosses needs to be estimated. The recovery rate is often modeled independently\nwith regard to the default probability, whereas the Merton model yields a\nfunctional dependence of both variables. We use Moody's Default and Recovery\nDatabase in order to investigate the relationship of default probability and\nrecovery rate for senior secured bonds. The assumptions in the Merton model do\nnot seem justified by the empirical situation. Yet the empirical dependence of\ndefault probability and recovery rate is well described by the functional\ndependence found in the Merton model.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.3188v1"
    },
    {
        "title": "Ordinal Classification Method for the Evaluation Of Thai Non-life\n  Insurance Companies",
        "authors": [
            "Phaiboon Jhonpita",
            "Sukree Sinthupinyo",
            "Thitivadee Chaiyawat"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This paper proposes a use of an ordinal classifier to evaluate the financial\nsolidity of non-life insurance companies as strong, moderate, weak, and\ninsolvency. This study constructed an efficient classification model that can\nbe used by regulators to evaluate the financial solidity and to determine the\npriority of further examination as an early warning system. The proposed model\nis beneficial to policy-makers to create guidelines for the solvency\nregulations and roles of the government in protecting the public against\ninsolvency.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.6424v2"
    },
    {
        "title": "Systemic losses in banking networks: indirect interaction of nodes via\n  asset prices",
        "authors": [
            "Igor Tsatskis"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  A simple banking network model is proposed which features multiple waves of\nbank defaults and is analytically solvable in the limiting case of an\ninfinitely large homogeneous network. The model is a collection of nodes\nrepresenting individual banks; associated with each node is a balance sheet\nconsisting of assets and liabilities. Initial node failures are triggered by\nexternal correlated shocks applied to the asset sides of the balance sheets.\nThese defaults lead to further reductions in asset values of all nodes which in\nturn produce additional failures, and so on. This mechanism induces indirect\ninteractions between the nodes and leads to a cascade of defaults. There are no\ninterbank links, and therefore no direct interactions, between the nodes. The\nresulting probability distribution for the total (direct plus systemic) network\nloss can be viewed as a modification of the well-known Vasicek distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.6778v1"
    },
    {
        "title": "From Risk Measures to Research Measures",
        "authors": [
            "Marco Frittelli",
            "Ilaria Peri"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  In order to evaluate the quality of the scientific research, we introduce a\nnew family of scientific performance measures, called Scientific Research\nMeasures (SRM). Our proposal originates from the more recent developments in\nthe theory of risk measures and is an attempt to resolve the many problems of\nthe existing bibliometric indices. The SRM that we introduce are based on the\nwhole scientist's citation record and are: coherent, as they share the same\nstructural properties; flexible to fit peculiarities of different areas and\nseniorities; granular, as they allow a more precise comparison between\nscientists, and inclusive, as they comprehend several popular indices. Another\nkey feature of our SRM is that they are planned to be calibrated to the\nparticular scientific community. We also propose a dual formulation of this\nproblem and explain its relevance in this context.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1012v1"
    },
    {
        "title": "Optimal retirement consumption with a stochastic force of mortality",
        "authors": [
            "Huaxiong Huang",
            "Moshe A. Milevsky",
            "Thomas S. Salisbury"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We extend the lifecycle model (LCM) of consumption over a random horizon\n(a.k.a. the Yaari model) to a world in which (i.) the force of mortality obeys\na diffusion process as opposed to being deterministic, and (ii.) a consumer can\nadapt their consumption strategy to new information about their mortality rate\n(a.k.a. health status) as it becomes available. In particular, we derive the\noptimal consumption rate and focus on the impact of mortality rate uncertainty\nvs. simple lifetime uncertainty -- assuming the actuarial survival curves are\ninitially identical -- in the retirement phase where this risk plays a greater\nrole.\n  In addition to deriving and numerically solving the PDE for the optimal\nconsumption rate, our main general result is that when utility preferences are\nlogarithmic the initial consumption rates are identical. But, in a CRRA\nframework in which the coefficient of relative risk aversion is greater\n(smaller) than one, the consumption rate is higher (lower) and a stochastic\nforce of mortality does make a difference.\n  That said, numerical experiments indicate that even for non-logarithmic\npreferences, the stochastic mortality effect is relatively minor from the\nindividual's perspective. Our results should be relevant to researchers\ninterested in calibrating the lifecycle model as well as those who provide\nnormative guidance (a.k.a. financial advice) to retirees.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2295v1"
    },
    {
        "title": "A different perspective on retirement income sustainability: the\n  blueprint for a ruin contingent life annuity (RCLA)",
        "authors": [
            "Huaxiong Huang",
            "Moshe A. Milevsky",
            "Thomas S. Salisbury"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  The purpose of this article is twofold. First, we motivate the need for a new\ntype of stand-alone retirement income insurance product that would help\nindividuals protect against personal longevity risk and possible \"retirement\nruin\" in an economically efficient manner. We label this product a\nruin-contingent life annuity (RCLA), which we elaborate-on and explain with\nvarious numerical examples and a basic pricing model. Second, we argue that\nwith the proper perspective a similar product actually exists, albeit not\navailable on a stand-alone basis. Namely, they are fused and embedded within\nmodern variable annuity (VA) policies with guaranteed living income benefit\n(GLiB) riders. Indeed, the popularity of GLiB riders on VA policies point\ntowards the potential commercial success of such a stand-alone vehicle.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2513v1"
    },
    {
        "title": "Beyond cash-additive risk measures: when changing the numéraire\n  fails",
        "authors": [
            "Walter Farkas",
            "Pablo Koch-Medina",
            "Cosimo Munari"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We discuss risk measures representing the minimum amount of capital a\nfinancial institution needs to raise and invest in a pre-specified eligible\nasset to ensure it is adequately capitalized. Most of the literature has\nfocused on cash-additive risk measures, for which the eligible asset is a\nrisk-free bond, on the grounds that the general case can be reduced to the\ncash-additive case by a change of numeraire. However, discounting does not work\nin all financially relevant situations, typically when the eligible asset is a\ndefaultable bond. In this paper we fill this gap allowing for general eligible\nassets. We provide a variety of finiteness and continuity results for the\ncorresponding risk measures and apply them to risk measures based on\nValue-at-Risk and Tail Value-at-Risk on $L^p$ spaces, as well as to shortfall\nrisk measures on Orlicz spaces. We pay special attention to the property of\ncash subadditivity, which has been recently proposed as an alternative to cash\nadditivity to deal with defaultable bonds. For important examples, we provide\ncharacterizations of cash subadditivity and show that, when the eligible asset\nis a defaultable bond, cash subadditivity is the exception rather than the\nrule. Finally, we consider the situation where the eligible asset is not\nliquidly traded and the pricing rule is no longer linear. We establish when the\nresulting risk measures are quasiconvex and show that cash subadditivity is\nonly compatible with continuous pricing rules.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.0478v8"
    },
    {
        "title": "Forecasting Value-at-Risk with Time-Varying Variance, Skewness and\n  Kurtosis in an Exponential Weighted Moving Average Framework",
        "authors": [
            "A. Gabrielsen",
            "P. Zagaglia",
            "A. Kirchner",
            "Z. Liu"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This paper provides an insight to the time-varying dynamics of the shape of\nthe distribution of financial return series by proposing an exponential\nweighted moving average model that jointly estimates volatility, skewness and\nkurtosis over time using a modified form of the Gram-Charlier density in which\nskewness and kurtosis appear directly in the functional form of this density.\nIn this setting VaR can be described as a function of the time-varying higher\nmoments by applying the Cornish-Fisher expansion series of the first four\nmoments. An evaluation of the predictive performance of the proposed model in\nthe estimation of 1-day and 10-day VaR forecasts is performed in comparison\nwith the historical simulation, filtered historical simulation and GARCH model.\nThe adequacy of the VaR forecasts is evaluated under the unconditional,\nindependence and conditional likelihood ratio tests as well as Basel II\nregulatory tests. The results presented have significant implications for risk\nmanagement, trading and hedging activities as well as in the pricing of equity\nderivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.1380v1"
    },
    {
        "title": "Interest Rate Risk of Bond Prices on Macedonian Stock Exchange -\n  Empirical Test of the Duration, Modified Duration and Convexity and Bonds\n  Valuation",
        "authors": [
            "Zoran Ivanovski",
            "Toni Draganov Stojanovski",
            "Nadica Ivanovska"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This article presents valuation of Treasury Bonds (T-Bonds) on Macedonian\nStock Exchange (MSE) and empirical test of duration, modified duration and\nconvexity of the T-bonds at MSE in order to determine sensitivity of bonds\nprices on interest rate changes. The main goal of this study is to determine\nhow standard valuation models fit in case of T- Bonds that are traded on MSE\nand to verify whether they offer reliable results compared with average bonds\nprices on MSE. We test the sensitivity of T- Bonds on MSE on interest rate\nchanges and determine that convexity is more accurate measure as approximation\nof bond prices changes than duration. Final conclusion is that T-Bonds traded\nat MSE are not sensitive on interest rate changes due to institutional\ninvestors' permanent higher demand and at the same time market limited offer of\nrisk-free instruments.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.6998v1"
    },
    {
        "title": "A Dynamical Model for Operational Risk in Banks",
        "authors": [
            "Marco Bardoscia"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  Operational risk is the risk relative to monetary losses caused by failures\nof bank internal processes due to heterogeneous causes. A dynamical model\nincluding both spontaneous generation of losses and generation via interactions\nbetween different processes is presented; the efforts made by the bank to avoid\nthe occurrence of losses is also taken into account. Under certain hypotheses,\nthe model can be exactly solved and, in principle, the solution can be\nexploited to estimate most of the model parameters from real data. The\nforecasting power of the model is also investigated and proved to be\nsurprisingly remarkable.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.6186v1"
    },
    {
        "title": "Mathematical Definition, Mapping, and Detection of (Anti)Fragility",
        "authors": [
            "Nassim N. Taleb",
            "Raphael Douady"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We provide a mathematical definition of fragility and antifragility as\nnegative or positive sensitivity to a semi-measure of dispersion and volatility\n(a variant of negative or positive \"vega\") and examine the link to nonlinear\neffects. We integrate model error (and biases) into the fragile or antifragile\ncontext. Unlike risk, which is linked to psychological notions such as\nsubjective preferences (hence cannot apply to a coffee cup) we offer a measure\nthat is universal and concerns any object that has a probability distribution\n(whether such distribution is known or, critically, unknown). We propose a\ndetection of fragility, robustness, and antifragility using a single\n\"fast-and-frugal\", model-free, probability free heuristic that also picks up\nexposure to model error. The heuristic lends itself to immediate\nimplementation, and uncovers hidden risks related to company size, forecasting\nproblems, and bank tail exposures (it explains the forecasting biases). While\nsimple to implement, it outperforms stress testing and other such methods such\nas Value-at-Risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.1189v1"
    },
    {
        "title": "Hedging Swing contract on gas markets",
        "authors": [
            "Xavier Warin"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  Swing options on the gas market are american style option where daily\nquantities exercices are constrained and global quantities exerciced each year\nconstrained too. The option holder has to decide each day how much he consumes\nof the quantities satisfying the constraints and tries to use a strategy in\norder to maximize its expected profit. The pay off fonction is a spread between\nthe spot gas market and the value of an index composed of the past average of\nsome commodities spot or future prices. We study the valorization and the\neffectiveness of the dynamic hedging of such a contract.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.5303v1"
    },
    {
        "title": "Fostering Project Scheduling and Controlling Risk Management",
        "authors": [
            "Abdul Razaque",
            "Christian Bach",
            "Nyembo salama",
            "Aziz Alotaibi"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  Deployment of emerging technologies and rapid change in industries has\ncreated a lot of risk for initiating the new projects. Many techniques and\nsuggestions have been introduced but still lack the gap from various\nprospective. This paper proposes a reliable project scheduling approach. The\nobjectives of project scheduling approach are to focus on critical chain\nschedule and risk management. Several risks and reservations exist in projects.\nThese critical reservations may not only foil the projects to be finished\nwithin time limit and budget, but also degrades the quality, and operational\nprocess. In the proposed approach, the potential risks of project are\ncritically analyzed. To overcome these potential risks, fuzzy failure mode and\neffect analysis (FMEA) is introduced. In addition, several affects of each risk\nagainst each activity are evaluated. We use Monte Carlo simulation that helps\nto calculate the total time of project. Our approach helps to control risk\nmitigation that is determined using event tree analysis and fault tree\nanalysis. We also implement distribute critical chain schedule for reliable\nscheduling that makes the project to be implemented within defined plan and\nschedule. Finally, adaptive procedure with density (APD) is deployed to get\nreasonable feeding buffer time and project buffer time.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.2021v1"
    },
    {
        "title": "Russian interbank networks: main characteristics and stability with\n  respect to contagion",
        "authors": [
            "A. V. Leonidov",
            "E. L. Rumyantsev"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  Systemic risks characterizing the Russian overnight interbank market from the\nnetwork point of view are analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.3814v1"
    },
    {
        "title": "Measuring and Analysing Marginal Systemic Risk Contribution using CoVaR:\n  A Copula Approach",
        "authors": [
            "Brice Hakwa",
            "Manfred Jäger-Ambrożewicz",
            "Barbara Rüdiger"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This paper is devoted to the quantification and analysis of marginal risk\ncontribution of a given single financial institution i to the risk of a\nfinancial system s. Our work expands on the CoVaR concept proposed by Adrian\nand Brunnermeier as a tool for the measurement of marginal systemic risk\ncontribution. We first give a mathematical definition of\nCoVaR_{\\alpha}^{s|L^i=l}. Our definition improves the CoVaR concept by\nexpressing CoVaR_{\\alpha}^{s|L^i=l} as a function of a state l and of a given\nprobability level \\alpha relative to i and s respectively. Based on Copula\ntheory we connect CoVaR_{\\alpha}^{s|L^i=l} to the partial derivatives of Copula\nthrough their probabilistic interpretation and definitions (Conditional\nProbability). Using this we provide a closed formula for the calculation of\nCoVaR_{\\alpha}^{s|L^i=l} for a large class of (marginal) distributions and\ndependence structures (linear and non-linear). Our formula allows a better\nanalysis of systemic risk using CoVaR in the sense that it allows to define\nCoVaR_{\\alpha}^{s|L^i=l} depending on the marginal distributions of the losses\nof i and s respectively and the copula between L^i and L^s. We discuss the\nimplications of this in the context of the quantification and analysis of\nsystemic risk contributions. %some mathematical This makes possible the For\nexample we will analyse the marginal effects of L^i, L^s and C of the risk\ncontribution of i.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.4713v2"
    },
    {
        "title": "Solvency assessment within the ORSA framework: issues and quantitative\n  methodologies",
        "authors": [
            "Julien Vedani",
            "Laurent Devineau"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  The implementation of the Own Risk and Solvency Assessment is a critical\nissue raised by Pillar II of Solvency II framework. In particular the Overall\nSolvency Needs calculation left the Insurance companies to define an optimal\nentity-specific solvency constraint on a multi-year time horizon. In a life\ninsurance society framework, the intuitive approaches to answer this problem\ncan sometimes lead to new implementation issues linked to the highly stochastic\nnature of the methodologies used to project a company Net Asset Value over\nseveral years. One alternative approach can be the use of polynomial proxies to\nreplicate the outcomes of this variable throughout the time horizon. Polynomial\nfunctions are already considered as efficient replication methodologies for the\nNet Asset Value over 1 year. The Curve Fitting and Least Squares Monte-Carlo\nprocedures are the best-known examples of such procedures. In this article we\nintroduce a possibility of adaptation for these methodologies to be used on a\nmulti-year time horizon, in order to assess the Overall Solvency Needs.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.6000v2"
    },
    {
        "title": "The role of the Model Validation function to manage and mitigate model\n  risk",
        "authors": [
            "Alberto Elices"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This paper describes the current taxonomy of model risk, ways for its\nmitigation and management and the importance of the model validation function\nin collaboration with other departments to design and implement them.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.0225v1"
    },
    {
        "title": "Optimal portfolio for a robust financial system",
        "authors": [
            "Yoshiharu Maeno",
            "Kenji Nishiguchi",
            "Satoshi Morinaga",
            "Hirokazu Matsushima"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This study presents an ANWSER model (asset network systemic risk model) to\nquantify the risk of financial contagion which manifests itself in a financial\ncrisis. The transmission of financial distress is governed by a heterogeneous\nbank credit network and an investment portfolio of banks. Bankruptcy\nreproductive ratio of a financial system is computed as a function of the\ndiversity and risk exposure of an investment portfolio of banks, and the\ndenseness and concentration of a heterogeneous bank credit network. An analytic\nsolution of the bankruptcy reproductive ratio for a small financial system is\nderived and a numerical solution for a large financial system is obtained. For\na large financial system, Large diversity among banks in the investment\nportfolio makes financial contagion more damaging on the average. But large\ndiversity is essentially effective in eliminating the risk of financial\ncontagion in the worst case of financial crisis scenarios. A bank-unique\nspecialization portfolio is more suitable than a uniform diversification\nportfolio and a system-wide specialization portfolio in strengthening the\nrobustness of a financial system.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5235v2"
    },
    {
        "title": "Optimal portfolio model based on WVAR",
        "authors": [
            "Tianyu Hao"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This article is focused on using a new measurement of risk-- Weighted Value\nat Risk to develop a new method of constructing initiate from the TVAR solving\nproblem, based on MATLAB software, using the historical simulation method\n(avoiding income distribution will be assumed to be normal), the results of\nprevious studies also based on, study the U.S. Nasdaq composite index,\ncombining the Simpson formula for the solution of TVAR and its deeply study;\nthen, through the representation of WVAR formula discussed and indispensable\nanalysis, also using the Simpson formula and the numerical calculations, we\nhave done the empirical analysis and review test. this paper is based on WVAR\nwhich possesses better properties, taking the idea of portfolio into the\nmulti-index comprehensive evaluation, to build innovative WVAR based portfolio\nselection under the framework of a theoretical model; in this framework, a\ndescription of risks is designed by WVAR, its advantage is no influence by\nincome distribution, meanwhile various optimization problems have a unique\nsolution; then take AHP weights to different indicators deal on this basis,\nafter that we put a nonlinear satisfaction portfolio selected model forward and\nconduct tests of empirical analysis, finally we use weighted linear approach to\nconvert the portfolio model into a single-objective problem, which is easier to\nsolve, then we use the data of two ETFs to construct portfolio, and compare the\nperformance of portfolio constructed by Mean-Weighted V@R and by Mean-Variance.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5628v1"
    },
    {
        "title": "Parameter estimation of a Levy copula of a discretely observed bivariate\n  compound Poisson process with an application to operational risk modelling",
        "authors": [
            "J. L. van Velsen"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  A method is developed to estimate the parameters of a Levy copula of a\ndiscretely observed bivariate compound Poisson process without knowledge of\ncommon shocks. The method is tested in a small sample simulation study. Also,\nthe method is applied to a real data set and a goodness of fit test is\ndeveloped. With the methodology of this work, the Levy copula becomes a\nrealistic tool of the advanced measurement approach of operational risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.0092v1"
    },
    {
        "title": "Risk Measures in a Regime Switching Model Capturing Stylized Facts",
        "authors": [
            "Rainer Haidinger",
            "Richard Warnung"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We pick up the regime switching model for asset returns introduced by Rogers\nand Zhang. The calibration involves various markets including implied\nvolatility in order to gain additional predictive power. We focus on the\ncalculation of risk measures by Fourier methods that have successfully been\napplied to option pricing and analyze the accuracy of the results.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.4126v1"
    },
    {
        "title": "Multiportfolio time consistency for set-valued convex and coherent risk\n  measures",
        "authors": [
            "Zachary Feinstein",
            "Birgit Rudloff"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  Equivalent characterizations of multiportfolio time consistency are deduced\nfor closed convex and coherent set-valued risk measures on $L^p(\\Omega,\\mathcal\nF, P; R^d)$ with image space in the power set of $L^p(\\Omega,\\mathcal\nF_t,P;R^d)$. In the convex case, multiportfolio time consistency is equivalent\nto a cocycle condition on the sum of minimal penalty functions. In the coherent\ncase, multiportfolio time consistency is equivalent to a generalized version of\nstability of the dual variables. As examples, the set-valued entropic risk\nmeasure with constant risk aversion coefficient is shown to satisfy the cocycle\ncondition for its minimal penalty functions, the set of superhedging portfolios\nin markets with proportional transaction costs is shown to have the stability\nproperty and in markets with convex transaction costs is shown to satisfy the\ncomposed cocycle condition, and a multiportfolio time consistent version of the\nset-valued average value at risk, the composed AV@R, is given and its dual\nrepresentation deduced.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.5563v5"
    },
    {
        "title": "The Foster-Hart Measure of Riskiness for General Gambles",
        "authors": [
            "Frank Riedel",
            "Tobias Hellmann"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Foster and Hart proposed an operational measure of riskiness for discrete\nrandom variables. We show that their defining equation has no solution for many\ncommon continuous distributions including many uniform distributions, e.g. We\nshow how to extend consistently the definition of riskiness to continuous\nrandom variables. For many continuous random variables, the risk measure is\nequal to the worst--case risk measure, i.e. the maximal possible loss incurred\nby that gamble. We also extend the Foster--Hart risk measure to dynamic\nenvironments for general distributions and probability spaces, and we show that\nthe extended measure avoids bankruptcy in infinitely repeated gambles.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1471v1"
    },
    {
        "title": "A new approach for an unitary risk theory",
        "authors": [
            "Nicolae Popoviciu",
            "Floarea Baicu"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The work deals with the risk assessment theory. An unitary risk algorithm is\nelaborated. The algorithm is based on parallel curves. The basic curve of risk\nis a hyperbolic curve, obtained as a multiplication between the probability of\noccurrence of certain event and its impact. Section 1 contains the problem\nformulation. Section 2 contains some specific notations and the mathematical\nbackground of risk algorithm. A numerical application based on risk algorithm\nis the content of section 3. Section 4 contains several conclusions.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.1672v1"
    },
    {
        "title": "Quantifying the Impact of Leveraging and Diversification on Systemic\n  Risk",
        "authors": [
            "Paolo Tasca",
            "Pavlin Mavrodiev",
            "Frank Schweitzer"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Excessive leverage, i.e. the abuse of debt financing, is considered one of\nthe primary factors in the default of financial institutions. Systemic risk\nresults from correlations between individual default probabilities that cannot\nbe considered independent. Based on the structural framework by Merton (1974),\nwe discuss a model in which these correlations arise from overlaps in banks'\nportfolios. Portfolio diversification is used as a strategy to mitigate losses\nfrom investments in risky projects. We calculate an optimal level of\ndiversification that has to be reached for a given level of excessive leverage\nto still mitigate an increase in systemic risk. In our model, this optimal\ndiversification further depends on the market size and the market conditions\n(e.g. volatility). It allows to distinguish between a safe regime, in which\nexcessive leverage does not result in an increase of systemic risk, and a risky\nregime, in which excessive leverage cannot be mitigated leading to an increased\nsystemic risk. Our results are of relevance for financial regulators.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.5552v1"
    },
    {
        "title": "Premiums And Reserves, Adjusted By Distortions",
        "authors": [
            "Alois Pichler"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The net-premium principle is considered to be the most genuine and fair\npremium principle in actuarial applications. However, an insurance company,\napplying the net-premium principle, goes bankrupt with probability one in the\nlong run, even if the company covers its entire costs by collecting the\nrespective fees from its customers. It is therefore an intrinsic necessity for\nthe insurance industry to apply premium principles, which guarantee at least\nfurther existence of the company itself; otherwise, the company naturally could\nnot insure its clients to cover their potential, future claims. Beside this\nintriguing fact the underlying loss distribution typically is not known\nprecisely. Hence alternative premium principles have been developed. A simple\nprinciple, ensuring risk-adjusted credibility premiums, is the distorted\npremium principle. This principle is convenient in insurance companies, as the\nactuary does not have to change his or her tools to compute the premiums or\nreserves. This paper addresses the distorted premium principle from various\nangles. First, dual characterizations are developed. Next, distorted premiums\nare typically computed by under-weighting or ignoring low, but over-weighting\nhigh losses. It is demonstrated here that there is an alternative, opposite\npoint of view, which consists in leaving the probability measure unchanged, but\nincreasing the outcomes instead. It turns out that this new point of view is\nnatural in actuarial practice, as it can be used for premium calculations, as\nwell as to determine the reserves of subsequent years in a time consistent way.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.0490v1"
    },
    {
        "title": "Measuring the default risk of sovereign debt from the perspective of\n  network",
        "authors": [
            "Hongwei Chuang",
            "Hwai-Chung Ho"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Recently, there has been a growing interest in network research, especially\nin these fields of biology, computer science, and sociology. It is natural to\naddress complex financial issues such as the European sovereign debt crisis\nfrom the perspective of network. In this article, we construct a network model\naccording to the debt--credit relations instead of using the conventional\nmethodology to measure the default risk. Based on the model, a risk index is\nexamined using the quarterly report of consolidated foreign claims from the\nBank for International Settlements (BIS) and debt/GDP ratios among these\nreporting countries. The empirical results show that this index can help the\nregulators and practitioners not only to determine the status of\ninterconnectivity but also to point out the degree of the sovereign debt\ndefault risk. Our approach sheds new light on the investigation of quantifying\nthe systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3814v1"
    },
    {
        "title": "Central Clearing of OTC Derivatives: bilateral vs multilateral netting",
        "authors": [
            "Rama Cont",
            "Thomas Kokholm"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We study the impact of central clearing of over-the-counter (OTC)\ntransactions on counterparty exposures in a market with OTC transactions across\nseveral asset classes with heterogeneous characteristics. The impact of\nintroducing a central counterparty (CCP) on expected interdealer exposure is\ndetermined by the tradeoff between multilateral netting across dealers on one\nhand and bilateral netting across asset classes on the other hand. We find this\ntradeoff to be sensitive to assumptions on heterogeneity of asset classes in\nterms of `riskyness' of the asset class as well as correlation of exposures\nacross asset classes. In particular, while an analysis assuming independent,\nhomogeneous exposures suggests that central clearing is efficient only if one\nhas an unrealistically high number of participants, the opposite conclusion is\nreached if differences in riskyness and correlation across asset classes are\nrealistically taken into account. We argue that empirically plausible\nspecifications of model parameters lead to the conclusion that central clearing\ndoes reduce interdealer exposures: the gain from multilateral netting in a CCP\noverweighs the loss of netting across asset classes in bilateral netting\nagreements. When a CCP exists for interest rate derivatives, adding a CCP for\ncredit derivatives is shown to decrease overall exposures. These findings are\nshown to be robust to the statistical assumptions of the model as well as the\nchoice of risk measure used to quantify exposures.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.5065v1"
    },
    {
        "title": "Mean-Variance Asset-Liability Management with State-Dependent Risk\n  Aversion",
        "authors": [
            "Qian Zhao",
            "Jiaqin Wei",
            "Rongming Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  In this paper, we consider the asset-liability management under the\nmean-variance criterion. The financial market consists of a risk-free bond and\na stock whose price process is modeled by a geometric Brownian motion. The\nliability of the investor is uncontrollable and is modeled by another geometric\nBrownian motion. We consider a specific state-dependent risk aversion which\ndepends on a power function of the liability. By solving a flow of FBSDEs with\nbivariate state process, we obtain the equilibrium strategy among all the\nopen-loop controls for this time-inconsistent control problem. It shows that\nthe equilibrium strategy is a feedback control of the liability.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7882v1"
    },
    {
        "title": "Consistent iterated simulation of multi-variate default times: a\n  Markovian indicators characterization",
        "authors": [
            "Damiano Brigo",
            "Jan-Frederik Mai",
            "Matthias Scherer"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We investigate under which conditions a single simulation of joint default\ntimes at a final time horizon can be decomposed into a set of simulations of\njoint defaults on subsequent adjacent sub-periods leading to that final\nhorizon. Besides the theoretical interest, this is also a practical problem as\npart of the industry has been working under the misleading assumption that the\ntwo approaches are equivalent for practical purposes. As a reasonable trade-off\nbetween realistic stylized facts, practical demands, and mathematical\ntractability, we propose models leading to a Markovian multi-variate\nsurvival--indicator process, and we investigate two instances of static models\nfor the vector of default times from the statistical literature that fall into\nthis class. On the one hand, the \"looping default\" case is known to be equipped\nwith this property, and we point out that it coincides with the classical\n\"Freund distribution\" in the bivariate case. On the other hand, if all\nsub-vectors of the survival indicator process are Markovian, this constitutes a\nnew characterization of the Marshall--Olkin distribution, and hence of\nmulti-variate lack-of-memory. A paramount property of the resulting model is\nstability of the type of multi-variate distribution with respect to elimination\nor insertion of a new marginal component with marginal distribution from the\nsame family. The practical implications of this \"nested margining\" property are\nenormous. To implement this distribution we present an efficient and unbiased\nsimulation algorithm based on the L\\'evy-frailty construction. We highlight\ndifferent pitfalls in the simulation of dependent default times and examine,\nwithin a numerical case study, the effect of inadequate simulation practices.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0887v3"
    },
    {
        "title": "A Financial Risk Analysis: Does the 2008 Financial Crisis Give Impact on\n  Weekends Returns of the U.S. Movie Box Office?",
        "authors": [
            "Novriana Sumarti",
            "Rafki Hidayat"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The Financial Crisis of 2008 is a worldwide financial crisis causing a\nworldwide economic decline that is the most severe since the 1930s. According\nto the International Monetary Fund (IMF), the global financial crisis gave\nimpact on USD 3.4 trillion losses from financial institutions around the world\nbetween 2007 and 2010. Does the crisis give impact on the returns of the U.S.\nmovie Box Office? It will be answered by doing an analysis on the financial\nrisk model based on Extreme Value Theory (EVT) and calculations of Value at\nRisk (VaR) and Expected Shortfall (ES). The values of VaR and ES from 2\nperiods, 1982 to 1995 and 1996 to 2010, are compared. Results show that the\npossibility of loss for an investment in the movie industry is relatively lower\nthan the possibility of gain for both periods of time. The values of VaR and ES\nfor the second period are higher than the first period. We are able to conclude\nthat the 2008 financial crisis gave no significant effect on these measurement\nvalues in the second period. This result describes the high potential\nopportunity in the investment of the U.S. movie makers.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0966v1"
    },
    {
        "title": "Ruin probability of a discrete-time risk process with proportional\n  reinsurance and investment for exponential and Pareto distributions",
        "authors": [
            "Helena Jasiulewicz",
            "Wojciech Kordecki"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  In this paper a quantitative analysis of the ruin probability in finite time\nof discrete risk process with proportional reinsurance and investment of\nfinance surplus is focused on. It is assumed that the total loss on a unit\ninterval has a light-tailed distribution -- exponential distribution and a\nheavy-tailed distribution -- Pareto distribution. The ruin probability for\nfinite-horizon 5 and 10 was determined from recurrence equations. Moreover for\nexponential distribution the upper bound of ruin probability by Lundberg\nadjustment coefficient is given. For Pareto distribution the adjustment\ncoefficient does not exist, hence an asymptotic approximation of the ruin\nprobability if an initial capital tends to infinity is given. Obtained\nnumerical results are given as tables and they are illustrated as graphs.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3479v2"
    },
    {
        "title": "Computational Dynamic Market Risk Measures in Discrete Time Setting",
        "authors": [
            "Babacar Seck",
            "Robert J. Elliott",
            "Jean-Pierre Gueyie"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Different approaches to defining dynamic market risk measures are available\nin the literature. Most are focused or derived from probability theory,\neconomic behavior or dynamic programming. Here, we propose an approach to\ndefine and implement dynamic market risk measures based on recursion and state\neconomy representation. The proposed approach is to be implementable and to\ninherit properties from static market risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.5705v1"
    },
    {
        "title": "The Meaning of Probability of Default for Asset-backed Loans",
        "authors": [
            "David Chisholm",
            "Graham Andersen"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The authors examine the concept of probability of default for asset-backed\nloans. In contrast to unsecured loans it is shown that probability of default\ncan be defined as either a measure of the likelihood of the borrower failing to\nmake required payments, or as the likelihood of an insufficiency of collateral\nvalue on foreclosure. Assuming expected loss is identical under either\ndefinition, this implies a corresponding pair of definitions for loss given\ndefault. Industry treatment of probability of default for asset-backed loans\nappears to inconsistently blend the two types of definition.\n  The authors develop a mathematical treatment of asset-backed loans which\nconsistently applies each type of definition in a framework to produce the same\nexpected loss and allows translation between the two frameworks.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.6715v1"
    },
    {
        "title": "Efficient immunization strategies to prevent financial contagion",
        "authors": [
            "Teruyoshi Kobayashi",
            "Kohei Hasui"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Many immunization strategies have been proposed to prevent infectious viruses\nfrom spreading through a network. In this study, we propose efficient\nimmunization strategies to prevent a default contagion that might occur in a\nfinancial network. An essential difference from the previous studies on\nimmunization strategy is that we take into account the possibility of serious\nside effects. Uniform immunization refers to a situation in which banks are\n\"vaccinated\" with a common low-risk asset. The riskiness of immunized banks\nwill decrease significantly, but the level of systemic risk may increase due to\nthe de-diversification effect. To overcome this side effect, we propose another\nimmunization strategy, counteractive immunization, which prevents pairs of\nbanks from failing simultaneously. We find that counteractive immunization can\nefficiently reduce systemic risk without altering the riskiness of individual\nbanks.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0652v3"
    },
    {
        "title": "Network versus portfolio structure in financial systems",
        "authors": [
            "Teruyoshi Kobayashi"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The question of how to stabilize financial systems has attracted considerable\nattention since the global financial crisis of 2007-2009. Recently, Beale et\nal. (\"Individual versus systemic risk and the regulator's dilemma\", Proc Natl\nAcad Sci USA 108: 12647-12652, 2011) demonstrated that higher portfolio\ndiversity among banks would reduce systemic risk by decreasing the risk of\nsimultaneous defaults at the expense of a higher likelihood of individual\ndefaults. In practice, however, a bank default has an externality in that it\nundermines other banks' balance sheets. This paper explores how each of these\ndifferent sources of risk, simultaneity risk and externality, contributes to\nsystemic risk. The results show that the allocation of external assets that\nminimizes systemic risk varies with the topology of the financial network as\nlong as asset returns have negative correlations. In the model, a well-known\ncentrality measure, PageRank, reflects an appropriately defined \"infectiveness\"\nof a bank. An important result is that the most infective bank need not always\nbe the safest bank. Under certain circumstances, the most infective node should\nact as a firewall to prevent large-scale collective defaults. The introduction\nof a counteractive portfolio structure will significantly reduce systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0773v1"
    },
    {
        "title": "Measuring risk with multiple eligible assets",
        "authors": [
            "Walter Farkas",
            "Pablo Koch-Medina",
            "Cosimo Munari"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The risk of financial positions is measured by the minimum amount of capital\nto raise and invest in eligible portfolios of traded assets in order to meet a\nprescribed acceptability constraint. We investigate nondegeneracy, finiteness\nand continuity properties of these risk measures with respect to multiple\neligible assets. Our finiteness and continuity results highlight the interplay\nbetween the acceptance set and the class of eligible portfolios. We present a\nsimple, alternative approach to the dual representation of convex risk measures\nby directly applying to the acceptance set the external characterization of\nclosed, convex sets. We prove that risk measures are nondegenerate if and only\nif the pricing functional admits a positive extension which is a supporting\nfunctional for the underlying acceptance set, and provide a characterization of\nwhen such extensions exist. Finally, we discuss applications to set-valued risk\nmeasures, superhedging with shortfall risk, and optimal risk sharing.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.3331v2"
    },
    {
        "title": "Analytical models of operational risk and new results on the correlation\n  problem",
        "authors": [
            "Vivien Brunel"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We propose a portfolio approach for operational risk quantification based on\na class of analytical models from which we derive new results on the\ncorrelation problem. In particular, we show that uniform correlation is a\nrobust assumption for measuring capital charges in these models.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5064v2"
    },
    {
        "title": "Credit Risk and the Instability of the Financial System: an Ensemble\n  Approach",
        "authors": [
            "Thilo A. Schmitt",
            "Desislava Chetalova",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The instability of the financial system as experienced in recent years and in\nprevious periods is often linked to credit defaults, i.e., to the failure of\nobligors to make promised payments. Given the large number of credit contracts,\nthis problem is amenable to be treated with approaches developed in statistical\nphysics. We introduce the idea of ensemble averaging and thereby uncover\ngeneric features of credit risk. We then show that the often advertised concept\nof diversification, i.e., reducing the risk by distributing it, is deeply\nflawed when it comes to credit risk. The risk of extreme losses remain due to\nthe ever present correlations, implying a substantial and persistent intrinsic\ndanger to the financial system.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.5245v2"
    },
    {
        "title": "Continuous compliance: a proxy-based monitoring framework",
        "authors": [
            "Julien Vedani",
            "Fabien Ramaharobandro"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Within the Own Risk and Solvency Assessment framework, the Solvency II\ndirective introduces the need for insurance undertakings to have efficient\ntools enabling the companies to assess the continuous compliance with\nregulatory solvency requirements. Because of the great operational complexity\nresulting from each complete evaluation of the Solvency Ratio, this monitoring\nis often complicated to implement in practice. This issue is particularly\nimportant for life insurance companies due to the high complexity to project\nlife insurance liabilities. It appears relevant in such a context to use\nparametric tools, such as Curve Fitting and Least Squares Monte Carlo in order\nto estimate, on a regular basis, the impact on the economic own funds and on\nthe regulatory capital of the company of any change over time of its underlying\nrisk factors. In this article, we first outline the principles of the\ncontinuous compliance requirement then we propose and implement a possible\nmonitoring tool enabling to approximate the eligible elements and the\nregulatory capital over time. In a final section we compare the use of the\nCurve Fitting and the Least Squares Monte Carlo methodologies in a standard\nempirical finite sample framework, and stress adapted advices for future\nproxies users.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.7222v2"
    },
    {
        "title": "Restructuring the \"one-way CSA\" counterparty risk in a CDO",
        "authors": [
            "Lorenzo Giada",
            "Claudio Nordio"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We show how to restructure the counterparty risk faced by the originator of a\nsecuritization or covered bond arising from an interest rate hedging swap\nassisted by a \"one-way\" collateral agreement. This risk emerges when the swap\nis negotiated between the special purpose vehicle and a third party that covers\nitself through a back-to-back swap with the originator. We show that the\ncounterparty risk of the originator may be removed by adding a chain of\nback-to-back credit derivatives between the three parties (originator,\ncounterparty and vehicle).\n",
        "pdf_link": "http://arxiv.org/pdf/1310.7128v1"
    },
    {
        "title": "On the Capital Allocation Problem for a New Coherent Risk Measure in\n  Collective Risk Theory",
        "authors": [
            "Assa Hirbod",
            "Morales Manuel",
            "Omidi Firouzi Hassan"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  In this paper we introduce a new coherent cumulative risk measure on\n$\\mathcal{R}_L^p$, the space of c\\`adl\\`ag processes having Laplace transform.\nThis new coherent risk measure turns out to be tractable enough within a class\nof models where the aggregate claims is driven by a spectrally positive L\\'evy\nprocess. Moreover, we study the problem of capital allocation in an insurance\ncontext and we show that the capital allocation problem for this risk measure\nhas a unique solution determined by the Euler allocation method. Some examples\nare provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.0354v1"
    },
    {
        "title": "Prévision du risque de crédit : Une étude comparative entre\n  l'Analyse Discriminante et l'Approche Neuronale",
        "authors": [
            "Younes Boujelbène",
            "Sihem Khemakhem"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Banks are interested in evaluating the risk of the financial distress before\ngiving out a loan. Many researchers proposed the use of models based on the\nNeural Networks in order to help the banker better make a decision. The\nobjective of this paper is to explore a new practical way based on the Neural\nNetworks that would help improve the capacity of the banker to predict the risk\nclass of the companies asking for a loan. This work is motivated by the\ninsufficiency of traditional prevision models. The sample consists of 86\nTunisian firms and 15 financial ratios are calculated, over the period from\n2005 to 2007. The results are compared with those of discriminant analysis.\nThey show that the neural networks technique is the best in term of\npredictability.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4266v1"
    },
    {
        "title": "What is the best risk measure in practice? A comparison of standard\n  measures",
        "authors": [
            "Susanne Emmer",
            "Marie Kratz",
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Expected Shortfall (ES) has been widely accepted as a risk measure that is\nconceptually superior to Value-at-Risk (VaR). At the same time, however, it has\nbeen criticised for issues relating to backtesting. In particular, ES has been\nfound not to be elicitable which means that backtesting for ES is less\nstraightforward than, e.g., backtesting for VaR. Expectiles have been suggested\nas potentially better alternatives to both ES and VaR. In this paper, we\nrevisit commonly accepted desirable properties of risk measures like coherence,\ncomonotonic additivity, robustness and elicitability. We check VaR, ES and\nExpectiles with regard to whether or not they enjoy these properties, with\nparticular emphasis on Expectiles. We also consider their impact on capital\nallocation, an important issue in risk management. We find that, despite the\ncaveats that apply to the estimation and backtesting of ES, it can be\nconsidered a good risk measure. As a consequence, there is no sufficient\nevidence to justify an all-inclusive replacement of ES by Expectiles in\napplications. For backtesting ES, we propose an empirical approach that\nconsists in replacing ES by a set of four quantiles, which should allow to make\nuse of backtesting methods for VaR.\n  Keywords: Backtesting; capital allocation; coherence; diversification;\nelicitability; expected shortfall; expectile; forecasts; probability integral\ntransform (PIT); risk measure; risk management; robustness; value-at-risk\n",
        "pdf_link": "http://arxiv.org/pdf/1312.1645v4"
    },
    {
        "title": "A model of financial contagion with variable asset returns may be\n  replaced with a simple threshold model of cascades",
        "authors": [
            "Teruyoshi Kobayashi"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  I show the equivalence between a model of financial contagion and the\nthreshold model of global cascades proposed by Watts (2002). The model\nfinancial network comprises banks that hold risky external assets as well as\ninterbank assets. It is shown that a simple threshold model can replicate the\nsize and the frequency of financial contagion without using information about\nindividual balance sheets. Keywords: financial network, cascades, financial\ncontagion, systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.6804v2"
    },
    {
        "title": "Estimation Error of Expected Shortfall",
        "authors": [
            "Imre Kondor"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  The problem of estimation error of Expected Shortfall is analyzed, with a\nview of its introduction as a global regulatory risk measure.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.5534v1"
    },
    {
        "title": "Structure conditions under progressively added information",
        "authors": [
            "Tahir Choulli",
            "Jun Deng"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  It has been understood that the \"local\" existence of the Markowitz' optimal\nportfolio or the solution to the local-risk minimization problem is guaranteed\nby some specific mathematical structures on the underlying assets price\nprocesses known in the literature as \"{\\it Structure Conditions}\". In this\npaper, we consider a semi-martingale market model, and an arbitrary random time\nthat is not adapted to the information flow of the market model. This random\ntime may model the default time of a firm, the death time of an insured, or any\nthe occurrence time of an event that might impact the market model somehow. By\nadding additional uncertainty to the market model, via this random time, the\n{\\it structures conditions} may fail and hence the Markowitz's optimal\nportfolio and other quadratic-optimal portfolios might fail to exist. Our aim\nis to investigate the impact of this random time on the structures conditions\nfrom different perspectives. Our analysis allows us to conclude that under some\nmild assumptions on the market model and the random time, these structures\nconditions will remain valid on the one hand. Furthermore, we provide two\nexamples illustrating the importance of these assumptions. On the other hand,\nwe describe the random time models for which these structure conditions are\npreserved for any market model. These results are elaborated separately for the\ntwo contexts of stopping with the random time and incorporating totally a\nspecific class of random times respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.3459v2"
    },
    {
        "title": "Investment under Duality Risk Measure",
        "authors": [
            "Zuo Quan Xu"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  One index satisfies the duality axiom if one agent, who is uniformly more\nrisk-averse than another, accepts a gamble, the latter accepts any less risky\ngamble under the index. Aumann and Serrano (2008) show that only one index\ndefined for so-called gambles satisfies the duality and positive homogeneity\naxioms. We call it a duality index. This paper extends the definition of\nduality index to all outcomes including all gambles, and considers a portfolio\nselection problem in a complete market, in which the agent's target is to\nminimize the index of the utility of the relative investment outcome. By\nlinking this problem to a series of Merton's optimum consumption-like problems,\nthe optimal solution is explicitly derived. It is shown that if the prior\nbenchmark level is too high (which can be verified), then the investment risk\nwill be beyond any agent's risk tolerance. If the benchmark level is\nreasonable, then the optimal solution will be the same as that of one of the\nMerton's series problems, but with a particular value of absolute risk\naversion, which is given by an explicit algebraic equation as a part of the\noptimal solution. According to our result, it is riskier to achieve the same\nsurplus profit in a stable market than in a less-stable market, which is\nconsistent with the common financial intuition.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.4222v1"
    },
    {
        "title": "Systemic risk through contagion in a core-periphery structured banking\n  network",
        "authors": [
            "Oliver Kley",
            "Claudia Klüppelberg",
            "Lukas Reichel"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We contribute to the understanding of how systemic risk arises in a network\nof credit-interlinked agents. Motivated by empirical studies we formulate a\nnetwork model which, despite its simplicity, depicts the nature of interbank\nmarkets better than a homogeneous model. The components of a vector\nOrnstein-Uhlenbeck process living on the vertices of the network describe the\nfinancial robustnesses of the agents. For this system, we prove a LLN for\ngrowing network size leading to a propagation of chaos result. We state\nproperties, which arise from such a structure, and examine the effect of\ninhomogeneity on several risk management issues and the possibility of\ncontagion.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.6575v1"
    },
    {
        "title": "Value-at-Risk time scaling for long-term risk estimation",
        "authors": [
            "Luca Spadafora",
            "Marco Dubrovich",
            "Marcello Terraneo"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In this paper we discuss a general methodology to compute the market risk\nmeasure over long time horizons and at extreme percentiles, which are the\ntypical conditions needed for estimating Economic Capital. The proposed\napproach extends the usual market-risk measure, ie, Value-at-Risk (VaR) at a\nshort-term horizon and 99% confidence level, by properly applying a scaling on\nthe short-term Profit-and-Loss (P&L) distribution. Besides the standard\nsquare-root-of-time scaling, based on normality assumptions, we consider two\nleptokurtic probability density function classes for fitting empirical P&L\ndatasets and derive accurately their scaling behaviour in light of the Central\nLimit Theorem, interpreting time scaling as a convolution problem. Our analyses\nresult in a range of possible VaR-scaling approaches depending on the\ndistribution providing the best fit to empirical data, the desired percentile\nlevel and the time horizon of the Economic Capital calculation. After assessing\nthe different approaches on a test equity trading portfolio, it emerges that\nthe choice of the VaR-scaling approach can affect substantially the Economic\nCapital calculation. In particular, the use of a convolution-based approach\ncould lead to significantly larger risk measures (by up to a factor of four)\nthan those calculated using Normal assumptions on the P&L distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.2462v1"
    },
    {
        "title": "Downturn LGD: A More Conservative Approach for Economic Decline Periods",
        "authors": [
            "Mauro R. Oliveira",
            "Armando Chinelatto Neto"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  The purpose of this paper is to identify a relevant statistical correlation\nbetween rate of default, RD, and loss given default, LGD, in a major Brazilian\nfinancial institution Retail Home Equity exposure rated using the IRB approach,\nso that we may find a causal relationship between the two risk parameters.\nTherefore, according to Central Bank of Brazil requirements, a methodology is\napplied to add conservatism to the estimation of the Loss Given Default\nparameter at times of economic decline, reflected as increased rates of\ndefault.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.3086v1"
    },
    {
        "title": "Risk measures with the CxLS property",
        "authors": [
            "Freddy Delbaen",
            "Fabio Bellini",
            "Valeria Bignozzi",
            "Johanna F. Ziegel"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In the present contribution we characterize law determined convex risk\nmeasures that have convex level sets at the level of distributions. By relaxing\nthe assumptions in Weber (2006), we show that these risk measures can be\nidentified with a class of generalized shortfall risk measures. As a direct\nconsequence, we are able to extend the results in Ziegel (2014) and Bellini and\nBignozzi (2014) on convex elicitable risk measures and confirm that expectiles\nare the only elicitable coherent risk measures. Further, we provide a simple\ncharacterization of robustness for convex risk measures in terms of a weak\nnotion of mixture continuity.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.0426v1"
    },
    {
        "title": "Multi-curve HJM modelling for risk management",
        "authors": [
            "Chiara Sabelli",
            "Michele Pioppi",
            "Luca Sitzia",
            "Giacomo Bormetti"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We present a HJM approach to the projection of multiple yield curves\ndeveloped to capture the volatility content of historical term structures for\nrisk management purposes. Since we observe the empirical data at daily\nfrequency and only for a finite number of time-to-maturity buckets, we propose\na modelling framework which is inherently discrete. In particular, we show how\nto approximate the HJM continuous time description of the multi-curve dynamics\nby a Vector Autoregressive process of order one. The resulting dynamics lends\nitself to a feasible estimation of the model volatility-correlation structure\nand market risk-premia. Then, resorting to the Principal Component Analysis we\nfurther simplify the dynamics reducing the number of covariance components.\nApplying the constant volatility version of our model on a sample of curves\nfrom the Euro area, we demonstrate its forecasting ability through an\nout-of-sample test.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.3977v3"
    },
    {
        "title": "Methodological thoughts on expected loss estimates for IFRS 9\n  impairment: hidden reserves, cyclical loss predictions and LGD backtesting",
        "authors": [
            "Wolfgang Reitgruber"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  After the release of the final accounting standards for impairment in July\n2014 by the IASB, banks will face the next significant methodological challenge\nafter Basel 2. In this paper, first methodological thoughts are presented, and\nways how to approach underlying questions are proposed. It starts with a\ndetailed discussion of the structural conservatism in the final standard. The\nexposure value iACV(c) (idealized Amortized Cost Value), as originally\nintroduced in the Exposure Draft 2009 (ED 2009), will be interpreted as\neconomic value under amortized cost accounting and provides the valuation\nbenchmark under IFRS 9. Consequently, iACV(c) can be used to quantify\nconservatism (ie potential hidden reserves) in the actual implementation of the\nfinal standard and to separate operational side-effects caused by the local\nimplementation from actual credit risk impacts. The second part continues with\na quantification of expected credit losses based on Impact of Risk(c) instead\nof traditional cost of risk measures. An objective framework is suggested which\nallows for improved testing of forward looking credit risk estimates during\ncredit cycles. This framework will prove useful to mitigate overly pro-cyclical\nprovisioning and to reduce earnings volatility. Finally, an LGD monitoring and\nbacktesting approach, applicable under regulatory requirements and accounting\nstandards as well, is proposed. On basis of the NPL Dashboard, part of the\nImpact of Risk(c) framework, specific key risk indicators are introduced that\nallow for a detailed assessment of collections performance versus LGD in in NPL\nportfolio (bucket 3).\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4265v3"
    },
    {
        "title": "Assessing the Basel II Internal Ratings-Based Approach: Empirical\n  Evidence from Australia",
        "authors": [
            "Silvio Tarca",
            "Marek Rutkowski"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  The Basel II internal ratings-based (IRB) approach to capital adequacy for\ncredit risk implements an asymptotic single risk factor (ASRF) model.\nMeasurements from the ASRF model of the prevailing state of Australia's economy\nand the level of capitalisation of its banking sector find general agreement\nwith macroeconomic indicators, financial statistics and external credit\nratings. However, given the range of economic conditions, from mild contraction\nto moderate expansion, experienced in Australia since the implementation of\nBasel II, we cannot attest to the validity of the model specification of the\nIRB approach for its intended purpose of solvency assessment. With the\nimplementation of Basel II preceding the time when the effect of the financial\ncrisis of 2007-09 was most acutely felt, our empirical findings offer a\nfundamental assessment of the impact of the crisis on the Australian banking\nsector. Access to internal bank data collected by the prudential regulator\ndistinguishes our research from other empirical studies on the IRB approach and\nrecent crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.0064v2"
    },
    {
        "title": "Regulatory Capital Modelling for Credit Risk",
        "authors": [
            "Marek Rutkowski",
            "Silvio Tarca"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  The Basel II internal ratings-based (IRB) approach to capital adequacy for\ncredit risk plays an important role in protecting the Australian banking sector\nagainst insolvency. We outline the mathematical foundations of regulatory\ncapital for credit risk, and extend the model specification of the IRB approach\nto a more general setting than the usual Gaussian case. It rests on the\nproposition that quantiles of the distribution of conditional expectation of\nportfolio percentage loss may be substituted for quantiles of the portfolio\nloss distribution. We present a more economical proof of this proposition under\nweaker assumptions. Then, constructing a portfolio that is representative of\ncredit exposures of the Australian banking sector, we measure the rate of\nconvergence, in terms of number of obligors, of empirical loss distributions to\nthe asymptotic (infinitely fine-grained) portfolio loss distribution. Moreover,\nwe evaluate the sensitivity of credit risk capital to dependence structure as\nmodelled by asset correlations and elliptical copulas. Access to internal bank\ndata collected by the prudential regulator distinguishes our research from\nother empirical studies on the IRB approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.1183v2"
    },
    {
        "title": "Tail Risk Constraints and Maximum Entropy",
        "authors": [
            "Donald Geman",
            "Hélyette Geman",
            "Nassim Nicholas Taleb"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In the world of modern financial theory, portfolio construction has\ntraditionally operated under at least one of two central assumptions: the\nconstraints are derived from a utility function and/or the multivariate\nprobability distribution of the underlying asset returns is fully known. In\npractice, both the performance criteria and the informational structure are\nmarkedly different: risk-taking agents are mandated to build portfolios by\nprimarily constraining the tails of the portfolio return to satisfy VaR, stress\ntesting, or expected shortfall (CVaR) conditions, and are largely ignorant\nabout the remaining properties of the probability distributions. As an\nalternative, we derive the shape of portfolio distributions which have maximum\nentropy subject to real-world left-tail constraints and other expectations. Two\nconsequences are (i) the left-tail constraints are sufficiently powerful to\noveride other considerations in the conventional theory, rendering individual\nportfolio components of limited relevance; and (ii) the \"barbell\" payoff\n(maximal certainty/low risk on one side, maximum uncertainty on the other)\nemerges naturally from this construction.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7647v1"
    },
    {
        "title": "Measures of Systemic Risk",
        "authors": [
            "Zachary Feinstein",
            "Birgit Rudloff",
            "Stefan Weber"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Systemic risk refers to the risk that the financial system is susceptible to\nfailures due to the characteristics of the system itself. The tremendous cost\nof systemic risk requires the design and implementation of tools for the\nefficient macroprudential regulation of financial institutions. The current\npaper proposes a novel approach to measuring systemic risk.\n  Key to our construction is a rigorous derivation of systemic risk measures\nfrom the structure of the underlying system and the objectives of a financial\nregulator. The suggested systemic risk measures express systemic risk in terms\nof capital endowments of the financial firms. Their definition requires two\ningredients: a cash flow or value model that assigns to the capital allocations\nof the entities in the system a relevant stochastic outcome; and an\nacceptability criterion, i.e. a set of random outcomes that are acceptable to a\nregulatory authority. Systemic risk is measured by the set of allocations of\nadditional capital that lead to acceptable outcomes. We explain the conceptual\nframework and the definition of systemic risk measures, provide an algorithm\nfor their computation, and illustrate their application in numerical case\nstudies.\n  Many systemic risk measures in the literature can be viewed as the minimal\namount of capital that is needed to make the system acceptable after\naggregating individual risks, hence quantify the costs of a bail-out. In\ncontrast, our approach emphasizes operational systemic risk measures that\ninclude both ex post bailout costs as well as ex ante capital requirements and\nmay be used to prevent systemic crises.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.07961v5"
    },
    {
        "title": "Negative Dependence Concept in Copulas and the Marginal Free Herd\n  Behavior Index",
        "authors": [
            "Jae Youn Ahn"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  We provide a set of copulas that can be interpreted as having the negative\nextreme dependence. This set of copulas is interesting because it coincides\nwith countermonotonic copula for a bivariate case, and more importantly, is\nshown to be minimal in concordance ordering in the sense that no copula exists\nwhich is strictly smaller than the given copula outside the proposed copula\nset. Admitting the absence of the minimum copula in multivariate dimensions\ngreater than 2, the study of the set of minimal copulas can be important in the\ninvestigation of various optimization problems. To demonstrate the importance\nof the proposed copula set, we provide the variance minimization problem of the\naggregated sum with arbitrarily given uniform marginals. As a\nfinancial/actuarial application of these copulas, we define a new herd behavior\nindex using weighted Spearman's rho, and determine the sharp lower bound of the\nindex using the proposed set of copulas.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.03180v1"
    },
    {
        "title": "Bounds for randomly shared risk of heavy-tailed loss factors",
        "authors": [
            "Oliver Kley",
            "Claudia Kluppelberg"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  For a risk vector $V$, whose components are shared among agents by some\nrandom mechanism, we obtain asymptotic lower and upper bounds for the\nindividual agents' exposure risk and the aggregated risk in the market. Risk is\nmeasured by Value-at-Risk or Conditional Tail Expectation. We assume Pareto\ntails for the components of $V$ and arbitrary dependence structure in a\nmultivariate regular variation setting. Upper and lower bounds are given by\nasymptotically independent and fully dependent components of $V$ with respect\nto the tail index $\\alpha$ being smaller or larger than 1. Counterexamples,\nwhere for non-linear aggregation functions no bounds are available, complete\nthe picture.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.03726v3"
    },
    {
        "title": "Optimal risk allocation in a market with non-convex preferences",
        "authors": [
            "Hirbod Assa"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  The aims of this study are twofold. First, we consider an optimal risk\nallocation problem with non-convex preferences. By establishing an infimal\nrepresentation for distortion risk measures, we give some necessary and\nsufficient conditions for the existence of optimal and asymptotic optimal\nallocations. We will show that, similar to a market with convex preferences, in\na non-convex framework with distortion risk measures the boundedness of the\noptimal risk allocation problem depends only on the preferences. Second, we\nconsider the same optimal allocation problem by adding a further assumption\nthat allocations are co-monotone. We characterize the co-monotone optimal risk\nallocations within which we prove the \"marginal risk allocations\" take only the\nvalues zero or one. Remarkably, we can separate the role of the market\npreferences and the total risk in our representation.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.04460v1"
    },
    {
        "title": "ON Integrated Chance Constraints in ALM for Pension Funds",
        "authors": [
            "Youssouf A. F. Toukourou",
            "François Dufresne"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  We discuss the role of integrated chance constraints (ICC) as quantitative\nrisk constraints in asset and liability management (ALM) for pension funds. We\ndefine two types of ICC: the one period integrated chance constraint (OICC) and\nthe multiperiod integrated chance constraint (MICC). As their names suggest,\nthe OICC covers only one period whereas several periods are taken into account\nwith the MICC. A multistage stochastic linear programming model is therefore\ndeveloped for this purpose and a special mention is paid to the modeling of the\nMICC.\n  Based on a numerical example, we firstly analyse the effects of the OICC and\nthe MICC on the optimal decisions (asset allocation and contribution rate) of a\npension fund. By definition, the MICC is more restrictive and safer compared to\nthe OICC. Secondly, we quantify this MICC safety increase. The results show\nthat although the optimal decisions from the OICC and the MICC differ, the\ntotal costs are very close, showing that the MICC is definitely a better\napproach since it is more prudent.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.05343v1"
    },
    {
        "title": "Option Pricing Beyond Black-Scholes Based on Double-Fractional Diffusion",
        "authors": [
            "Hagen Kleinert",
            "Jan Korbel"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  We show how the prices of options can be determined with the help of\ndouble-fractional differential equation in such a way that their inclusion in a\nportfolio of stocks provides a more reliable hedge against dramatic price drops\nthat the use of options whose prices were fixed by the Black-Scholes formula.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.05655v2"
    },
    {
        "title": "Measuring Systemic Risk: Robust Ranking Techniques Approach",
        "authors": [
            "Amirhossein Sadoghi"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In this research, we introduce a robust metric to identify Systemically\nImportant Financial Institution (SIFI) in a financial network by taking into\naccount both common idiosyncratic shocks and contagion through counterparty\nexposures. We develop an efficient algorithm to rank financial institutions by\nformulating a fixed point problem and reducing it to a non-smooth convex\noptimization problem. We then study the underlying distribution of the proposed\nmetric and analyze the performance of the algorithm by using different\nfinancial network structures. Overall, our findings suggest that the level of\ninterconnection and position of institutions in the financial network are\nimportant elements to measure systemic risk and identify SIFIs. Results show\nthat increasing the levels of out- and in-degree connections of an institution\ncan have a diverse impact on its systemic ranking. Additionally, on the\nempirical side, we investigate the factors which lead to the identification of\nGlobal Systemic Important Banks (G-SIB) by using a panel dataset of the largest\nbanks in each country. Our empirical results supports the main findings of the\ntheoretical model.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.06317v2"
    },
    {
        "title": "New class of distortion risk measures and their tail asymptotics with\n  emphasis on VaR",
        "authors": [
            "Chuancun Yin",
            "Dan Zhu"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Distortion risk measures are extensively used in finance and insurance\napplications because of their appealing properties. We present three methods to\nconstruct new class of distortion functions and measures. The approach involves\nthe composting methods, the mixing methods and the approach that based on the\ntheory of copula. Subadditivity is an important property when aggregating risks\nin order to preserve the benefits of diversification. However, Value at risk\n(VaR), as the most well-known example of distortion risk measure is not always\nglobally subadditive, except of elliptically distributed risks. In this paper,\ninstead of study subadditivity we investigate the tail subadditivity for VaR\nand other distortion risk measures. In particular, we demonstrate that VaR is\ntail subadditive for the case where the support of risk is bounded. Various\nexamples are also presented to illustrate the results.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08586v2"
    },
    {
        "title": "DebtRank: A microscopic foundation for shock propagation",
        "authors": [
            "Marco Bardoscia",
            "Stefano Battiston",
            "Fabio Caccioli",
            "Guido Caldarelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  The DebtRank algorithm has been increasingly investigated as a method to\nestimate the impact of shocks in financial networks, as it overcomes the\nlimitations of the traditional default-cascade approaches. Here we formulate a\ndynamical \"microscopic\" theory of instability for financial networks by\niterating balance sheet identities of individual banks and by assuming a simple\nrule for the transfer of shocks from borrowers to lenders. By doing so, we\ngeneralise the DebtRank formulation, both providing an interpretation of the\neffective dynamics in terms of basic accounting principles and preventing the\nunderestimation of losses on certain network topologies. Depending on the\nstructure of the interbank leverage matrix the dynamics is either stable, in\nwhich case the asymptotic state can be computed analytically, or unstable,\nmeaning that at least one bank will default. We apply this framework to a\ndataset of the top listed European banks in the period 2008 - 2013. We find\nthat network effects can generate an amplification of exogenous shocks of a\nfactor ranging between three (in normal periods) and six (during the crisis)\nwhen we stress the system with a 0.5% shock on external (i.e. non-interbank)\nassets for all banks.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.01857v2"
    },
    {
        "title": "Operational risk modeled analytically II: the consequences of\n  classification invariance",
        "authors": [
            "Vivien Brunel"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Most of the banks' operational risk internal models are based on loss pooling\nin risk and business line categories. The parameters and outputs of operational\nrisk models are sensitive to the pooling of the data and the choice of the risk\nclassification. In a simple model, we establish the link between the number of\nrisk cells and the model parameters by requiring invariance of the bank's loss\ndistribution upon a change in classification. We provide details on the impact\nof this requirement on the domain of attraction of the loss distribution, on\ndiversification effects and on cell risk correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.07805v2"
    },
    {
        "title": "New copulas based on general partitions-of-unity and their applications\n  to risk management",
        "authors": [
            "Dietmar Pfeifer",
            "Hervé Awoumlac Tsatedem",
            "Andreas Mändle",
            "Côme Girschig"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  We construct new multivariate copulas on the basis of a generalized infinite\npartition-of-unity approach. This approach allows - in contrast to finite\npartition-of-unity copulas - for tail-dependence as well as for asymmetry. A\npossibility of fitting such copulas to real data from quantitative risk\nmanagement is also pointed out.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.00288v3"
    },
    {
        "title": "Improved Algorithms for Computing Worst Value-at-Risk: Numerical\n  Challenges and the Adaptive Rearrangement Algorithm",
        "authors": [
            "Marius Hofert",
            "Amir Memartoluie",
            "David Saunders",
            "Tony Wirjanto"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Numerical challenges inherent in algorithms for computing worst Value-at-Risk\nin homogeneous portfolios are identified and solutions as well as words of\nwarning concerning their implementation are provided. Furthermore, both\nconceptual and computational improvements to the Rearrangement Algorithm for\napproximating worst Value-at-Risk for portfolios with arbitrary marginal loss\ndistributions are given. In particular, a novel Adaptive Rearrangement\nAlgorithm is introduced and investigated. These algorithms are implemented\nusing the R package qrmtools.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.02281v2"
    },
    {
        "title": "Wrong-Way Bounds in Counterparty Credit Risk Management",
        "authors": [
            "Amir Memartoluie",
            "David Saunders",
            "Tony Wirjanto"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  We study the problem of finding the worst-case joint distribution of a set of\nrisk factors given prescribed multivariate marginals and a nonlinear loss\nfunction. We show that when the risk measure is CVaR, and the distributions are\ndiscretized, the problem can be conveniently solved using linear programming\ntechnique. The method has applications to any situation where marginals are\nprovided, and bounds need to be determined on total portfolio risk. This arises\nin many financial contexts, including pricing and risk management of exotic\noptions, analysis of structured finance instruments, and aggregation of\nportfolio risk across risk types. Applications to counterparty credit risk are\nemphasized, and they include assessing wrong-way risk in the credit valuation\nadjustment, and counterparty credit risk measurement. Lastly a detailed\napplication of the algorithm for counterparty risk measurement to a real\nportfolio case is also presented in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.02292v1"
    },
    {
        "title": "The multi-layer network nature of systemic risk and its implications for\n  the costs of financial crises",
        "authors": [
            "Sebastian Poledna",
            "José Luis Molina-Borboa",
            "Serafín Martínez-Jaramillo",
            "Marco van der Leij",
            "Stefan Thurner"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  The inability to see and quantify systemic financial risk comes at an immense\nsocial cost. Systemic risk in the financial system arises to a large extent as\na consequence of the interconnectedness of its institutions, which are linked\nthrough networks of different types of financial contracts, such as credit,\nderivatives, foreign exchange and securities. The interplay of the various\nexposure networks can be represented as layers in a financial multi-layer\nnetwork. In this work we quantify the daily contributions to systemic risk from\nfour layers of the Mexican banking system from 2007-2013. We show that focusing\non a single layer underestimates the total systemic risk by up to 90%. By\nassigning systemic risk levels to individual banks we study the systemic risk\nprofile of the Mexican banking system on all market layers. This profile can be\nused to quantify systemic risk on a national level in terms of nation-wide\nexpected systemic losses. We show that market-based systemic risk indicators\nsystematically underestimate expected systemic losses. We find that expected\nsystemic losses are up to a factor four higher now than before the financial\ncrisis of 2007-2008. We find that systemic risk contributions of individual\ntransactions can be up to a factor of thousand higher than the corresponding\ncredit risk, which creates huge risks for the public. We find an intriguing\nnon-linear effect whereby the sum of systemic risk of all layers underestimates\nthe total risk. The method presented here is the first objective data driven\nquantification of systemic risk on national scales that reveal its true levels.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04276v1"
    },
    {
        "title": "The efficiency of Anderson-Darling test with limited sample size: an\n  application to Backtesting Counterparty Credit Risk internal model",
        "authors": [
            "M. Formenti",
            "L. Spadafora",
            "M. Terraneo",
            "F. Ramponi"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  This work presents a theoretical and empirical evaluation of Anderson-Darling\ntest when the sample size is limited. The test can be applied in order to\nbacktest the risk factors dynamics in the context of Counterparty Credit Risk\nmodelling. We show the limits of such test when backtesting the distributions\nof an interest rate model over long time horizons and we propose a modified\nversion of the test that is able to detect more efficiently an underestimation\nof the model's volatility. Finally we provide an empirical application.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04593v1"
    },
    {
        "title": "Actuarial Applications and Estimation of Extended~CreditRisk$^+$",
        "authors": [
            "Jonas Hirz",
            "Uwe Schmock",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  We introduce an additive stochastic mortality model which allows joint\nmodelling and forecasting of underlying death causes. Parameter families for\nmortality trends can be chosen freely. As model settings become high\ndimensional, Markov chain Monte Carlo (MCMC) is used for parameter estimation.\nWe then link our proposed model to an extended version of the credit risk model\nCreditRisk$^+$. This allows exact risk aggregation via an efficient numerically\nstable Panjer recursion algorithm and provides numerous applications in credit,\nlife insurance and annuity portfolios to derive P\\&L distributions.\nFurthermore, the model allows exact (without Monte Carlo simulation error)\ncalculation of risk measures and their sensitivities with respect to model\nparameters for P\\&L distributions such as value-at-risk and expected shortfall.\nNumerous examples, including an application to partial internal models under\nSolvency II, using Austrian and Australian data are shown.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04757v7"
    },
    {
        "title": "Market Fragility, Systemic Risk, and Ricci Curvature",
        "authors": [
            "Romeil Sandhu",
            "Tryphon Georgiou",
            "Allen Tannenbaum"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Measuring systemic risk or fragility of financial systems is a ubiquitous\ntask of fundamental importance in analyzing market efficiency, portfolio\nallocation, and containment of financial contagions. Recent attempts have shown\nthat representing such systems as a weighted graph characterizing the complex\nweb of interacting agents over some information flow (e.g., debt, stock\nreturns, shareholder ownership) may provide certain keen insights. Here, we\nshow that fragility, or the ability of system to be prone to failures in the\nface of random perturbations, is negatively correlated with geometric notion of\nRicci curvature. The key ingredient relating fragility and curvature is\nentropy. As a proof of concept, we examine returns from a set of stocks\ncomprising the S\\&P 500 over a 15 year span to show that financial crashes are\nmore robust compared to normal \"business as usual\" fragile market behavior -\ni.e., Ricci curvature is a \"crash hallmark.\" Perhaps more importantly, this\nwork lays the foundation of understanding of how to design systems and policy\nregulations in a manner that can combat financial instabilities exposed during\nthe 2007-2008 crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.05182v1"
    },
    {
        "title": "The Effects of Leverage Requirements and Fire Sales on Financial\n  Contagion via Asset Liquidation Strategies in Financial Networks",
        "authors": [
            "Zachary Feinstein",
            "Fatena El-Masri"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  This paper provides a framework for modeling the financial system with\nmultiple illiquid assets when liquidation of illiquid assets is caused by\nfailure to meet a leverage requirement. This extends the network model of\nCifuentes, Shin & Ferrucci (2005) which incorporates a single asset with fire\nsales and capital adequacy ratio. This also extends the network model of\nFeinstein (2015) which incorporates multiple illiquid assets with fire sales\nand no leverage ratios. We prove existence of equilibrium clearing payments and\nliquidation prices for a known liquidation strategy when leverage requirements\nare required. We also prove sufficient conditions for the existence of an\nequilibrium liquidation strategy with corresponding clearing payments and\nliquidation prices. Finally we calibrate network models to asset and liability\ndata for 50 banks in the United States from 2007-2014 in order to draw\nconclusions on systemic risk as a function of leverage requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.01847v2"
    },
    {
        "title": "Semi-parametric time series modelling with autocopulas",
        "authors": [
            "Antony Ware",
            "Ilnaz Asadzadeh"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In this paper we present an application of the use of autocopulas for\nmodelling financial time series showing serial dependencies that are not\nnecessarily linear. The approach presented here is semi-parametric in that it\nis characterized by a non-parametric autocopula and parametric marginals. One\nadvantage of using autocopulas is that they provide a general representation of\nthe auto-dependency of the time series, in particular making it possible to\nstudy the interdependence of values of the series at different extremes\nseparately. The specific time series that is studied here comes from daily cash\nflows involving the product of daily natural gas price and daily temperature\ndeviations from normal levels. Seasonality is captured by using a time\ndependent normal inverse Gaussian (NIG) distribution fitted to the raw values.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04767v1"
    },
    {
        "title": "Assessing systemic risk due to fire sales spillover through maximum\n  entropy network reconstruction",
        "authors": [
            "Domenico Di Gangi",
            "Fabrizio Lillo",
            "Davide Pirino"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Assessing systemic risk in financial markets is of great importance but it\noften requires data that are unavailable or available at a very low frequency.\nFor this reason, systemic risk assessment with partial information is\npotentially very useful for regulators and other stakeholders. In this paper we\nconsider systemic risk due to fire sales spillover and portfolio rebalancing by\nusing the risk metrics defined by Greenwood et al. (2015). By using the Maximum\nEntropy principle we propose a method to assess aggregated and single bank's\nsystemicness and vulnerability and to statistically test for a change in these\nvariables when only the information on the size of each bank and the\ncapitalization of the investment assets are available. We prove the\neffectiveness of our method on 2001-2013 quarterly data of US banks for which\nportfolio composition is available.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.00607v2"
    },
    {
        "title": "Conditional risk measures in a bipartite market structure",
        "authors": [
            "Oliver Kley",
            "Claudia Klüppelberg",
            "Gesine Reinert"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In this paper we study the effect of network structure between agents and\nobjects on measures for systemic risk. We model the influence of sharing large\nexogeneous losses to the financial or (re)insuance market by a bipartite graph.\nUsing Pareto-tailed losses and multivariate regular variation we obtain\nasymptotic results for systemic conditional risk measures based on the\nValue-at-Risk and the Conditional Tail Expectation. These results allow us to\nassess the influence of an individual institution on the systemic or market\nrisk and vice versa through a collection of conditional systemic risk measures.\nFor large markets Poisson approximations of the relevant constants are provided\nin the example of an insurance market. The example of an underlying homogeneous\nrandom graph is analysed in detail, and the results are illustrated through\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.00616v1"
    },
    {
        "title": "Efficient Randomized Quasi-Monte Carlo Methods For Portfolio Market Risk",
        "authors": [
            "Halis Sak",
            "İsmail Başoğlu"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  We consider the problem of simulating loss probabilities and conditional\nexcesses for linear asset portfolios under the t-copula model. Although in the\nliterature on market risk management there are papers proposing efficient\nvariance reduction methods for Monte Carlo simulation of portfolio market risk,\nthere is no paper discussing combining the randomized quasi-Monte Carlo method\nwith variance reduction techniques. In this paper, we combine the randomized\nquasi-Monte Carlo method with importance sampling and stratified importance\nsampling. Numerical results for realistic portfolio examples suggest that\nreplacing pseudorandom numbers (Monte Carlo) with quasi-random sequences\n(quasi-Monte Carlo) in the simulations increases the robustness of the\nestimates once we reduce the effective dimension and the non-smoothness of the\nintegrands.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.01593v1"
    },
    {
        "title": "A State-Dependent Dual Risk Model",
        "authors": [
            "Lingjiong Zhu"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In a dual risk model, the premiums are considered as the costs and the claims\nare regarded as the profits. The surplus can be interpreted as the wealth of a\nventure capital, whose profits depend on research and development. In most of\nthe existing literature of dual risk models, the profits follow the compound\nPoisson model and the cost is constant. In this paper, we develop a\nstate-dependent dual risk model, in which the arrival rate of the profits and\nthe costs depend on the current state of the wealth process. Ruin probabilities\nare obtained in closed-forms. Further properties and results will also be\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.03920v2"
    },
    {
        "title": "A Supermartingale Relation for Multivariate Risk Measures",
        "authors": [
            "Zachary Feinstein",
            "Birgit Rudloff"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  The equivalence between multiportfolio time consistency of a dynamic\nmultivariate risk measure and a supermartingale property is proven.\nFurthermore, the dual variables under which this set-valued supermartingale is\na martingale are characterized as the worst-case dual variables in the dual\nrepresentation of the risk measure. Examples of multivariate risk measures\nsatisfying the supermartingale property are given. Crucial for obtaining the\nresults are dual representations of scalarizations of set-valued dynamic risk\nmeasures, which are of independent interest in the fast growing literature on\nmultivariate risks.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.05561v4"
    },
    {
        "title": "Basel III capital surcharges for G-SIBs fail to control systemic risk\n  and can cause pro-cyclical side effects",
        "authors": [
            "Sebastian Poledna",
            "Olaf Bochmann",
            "Stefan Thurner"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  In addition to constraining bilateral exposures of financial institutions,\nthere are essentially two options for future financial regulation of systemic\nrisk (SR): First, financial regulation could attempt to reduce the financial\nfragility of global or domestic systemically important financial institutions\n(G-SIBs or D-SIBs), as for instance proposed in Basel III. Second, future\nfinancial regulation could attempt strengthening the financial system as a\nwhole. This can be achieved by re-shaping the topology of financial networks.\nWe use an agent-based model (ABM) of a financial system and the real economy to\nstudy and compare the consequences of these two options. By conducting three\n\"computer experiments\" with the ABM we find that re-shaping financial networks\nis more effective and efficient than reducing leverage. Capital surcharges for\nG-SIBs can reduce SR, but must be larger than those specified in Basel III in\norder to have a measurable impact. This can cause a loss of efficiency. Basel\nIII capital surcharges for G-SIBs can have pro-cyclical side effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.03505v1"
    },
    {
        "title": "Entropy and credit risk in highly correlated markets",
        "authors": [
            "Sylvia Gottschalk"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We compare two models of corporate default by calculating the\nJeffreys-Kullback-Leibler divergence between their predicted default\nprobabilities when asset correlations are either high or low. Our main results\nshow that the divergence between the two models increases in highly correlated,\nvolatile, and large markets, but that it is closer to zero in small markets,\nwhen asset correlations are low and firms are highly leveraged. These findings\nsuggest that during periods of financial instability the single-and\nmulti-factor models of corporate default will generate increasingly\ninconsistent predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07042v1"
    },
    {
        "title": "Comments on the BCBS proposal for a New Standardized Approach for\n  Operational Risk",
        "authors": [
            "Giulio Mignola",
            "Roberto Ugoccioni",
            "Eric Cope"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  On March 4th 2016 the Basel Committee on Banking Supervision published a\nconsultative document where a new methodology, called the Standardized\nMeasurement Approach (SMA), is introduced for computing Operational Risk\nregulatory capital for banks. In this note, the behavior of the SMA is studied\nunder a variety of hypothetical and realistic conditions, showing that the\nsimplicity of the new approach is very costly on other aspects: we find that\nthe SMA does not respond appropriately to changes in the risk profile of a\nbank, nor is it capable of differentiating among the range of possible risk\nprofiles across banks; that SMA capital results generally appear to be more\nvariable across banks than the previous AMA option of fitting the loss data;\nthat the SMA can result in banks over- or under-insuring against operational\nrisks relative to previous AMA standards. Finally, we argue that the SMA is not\nonly retrograde in terms of its capability to measure risk, but perhaps more\nimportantly, it fails to create any link between management actions and capital\nrequirement.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.00756v1"
    },
    {
        "title": "Should the advanced measurement approach be replaced with the\n  standardized measurement approach for operational risk?",
        "authors": [
            "Gareth W. Peters",
            "Pavel V. Shevchenko",
            "Bertrand Hassani",
            "Ariane Chapelle"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Recently, Basel Committee for Banking Supervision proposed to replace all\napproaches, including Advanced Measurement Approach (AMA), for operational risk\ncapital with a simple formula referred to as the Standardised Measurement\nApproach (SMA). This paper discusses and studies the weaknesses and pitfalls of\nSMA such as instability, risk insensitivity, super-additivity and the implicit\nrelationship between SMA capital model and systemic risk in the banking sector.\nWe also discuss the issues with closely related operational risk\nCapital-at-Risk (OpCar) Basel Committee proposed model which is the precursor\nto the SMA. In conclusion, we advocate to maintain the AMA internal model\nframework and suggest as an alternative a number of standardization\nrecommendations that could be considered to unify internal modelling of\noperational risk. The findings and views presented in this paper have been\ndiscussed with and supported by many OpRisk practitioners and academics in\nAustralia, Europe, UK and USA, and recently at OpRisk Europe 2016 conference in\nLondon.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02319v2"
    },
    {
        "title": "Dual representations for systemic risk measures",
        "authors": [
            "Çağın Ararat",
            "Birgit Rudloff"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  The financial crisis showed the importance of measuring, allocating and\nregulating systemic risk. Recently, the systemic risk measures that can be\ndecomposed into an aggregation function and a scalar measure of risk, received\na lot of attention. In this framework, capital allocations are added after\naggregation and can represent bailout costs. More recently, a framework has\nbeen introduced, where institutions are supplied with capital allocations\nbefore aggregation. This yields an interpretation that is particularly useful\nfor regulatory purposes. In each framework, the set of all feasible capital\nallocations leads to a multivariate risk measure. In this paper, we present\ndual representations for scalar systemic risk measures as well as for the\ncorresponding multivariate risk measures concerning capital allocations. Our\nresults cover both frameworks: aggregating after allocating and allocating\nafter aggregation. As examples, we consider the aggregation mechanisms of the\nEisenberg-Noe model as well as those of the resource allocation and network\nflow models.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.03430v3"
    },
    {
        "title": "Insurance valuation: a computable multi-period cost-of-capital approach",
        "authors": [
            "Hampus Engsner",
            "Mathias Lindholm",
            "Filip Lindskog"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We present an approach to market-consistent multi-period valuation of\ninsurance liability cash flows based on a two-stage valuation procedure. First,\na portfolio of traded financial instrument aimed at replicating the liability\ncash flow is fixed. Then the residual cash flow is managed by repeated\none-period replication using only cash funds. The latter part takes capital\nrequirements and costs into account, as well as limited liability and risk\naverseness of capital providers. The cost-of-capital margin is the value of the\nresidual cash flow. We set up a general framework for the cost-of-capital\nmargin and relate it to dynamic risk measurement. Moreover, we present explicit\nformulas and properties of the cost-of-capital margin under further assumptions\non the model for the liability cash flow and on the conditional risk measures\nand utility functions. Finally, we highlight computational aspects of the\ncost-of-capital margin, and related quantities, in terms of an example from\nlife insurance.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04100v1"
    },
    {
        "title": "A form of multivariate Pareto distribution with applications to\n  financial risk measurement",
        "authors": [
            "Jianxi Su",
            "Edward Furman"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  A new multivariate distribution possessing arbitrarily parametrized and\npositively dependent univariate Pareto margins is introduced. Unlike the\nprobability law of Asimit et al. (2010) [Asimit, V., Furman, E. and Vernic, R.\n(2010) On a multivariate Pareto distribution. Insurance: Mathematics and\nEconomics 46(2), 308-316], the structure in this paper is absolutely continuous\nwith respect to the corresponding Lebesgue measure. The distribution is of\nimportance to actuaries through its connections to the popular frailty models,\nas well as because of the capacity to describe dependent heavy-tailed risks.\nThe genesis of the new distribution is linked to a number of existing\nprobability models, and useful characteristic results are proved. Expressions\nfor, e.g., the decumulative distribution and probability density functions,\n(joint) moments and regressions are developed. The distributions of minima and\nmaxima, as well as, some weighted risk measures are employed to exemplify\npossible applications of the distribution in insurance.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04737v1"
    },
    {
        "title": "Multiple risk factor dependence structures: Distributional properties",
        "authors": [
            "Jianxi Su",
            "Edward Furman"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We introduce a class of dependence structures, that we call the Multiple Risk\nFactor (MRF) dependence structures. On the one hand, the new constructions\nextend the popular CreditRisk+ approach, and as such they formally describe\ndefault risk portfolios exposed to an arbitrary number of fatal risk factors\nwith conditionally exponential and dependent hitting (or occurrence) times. On\nthe other hand, the MRF structures can be seen as an encompassing family of\nmultivariate probability distributions with univariate margins distributed\nPareto of the 2nd kind, and in this role they can be used to model insurance\nrisk portfolios of dependent and heavy tailed risk components.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04739v1"
    },
    {
        "title": "Rethinking Financial Contagion",
        "authors": [
            "Gabriele Visentin",
            "Stefano Battiston",
            "Marco D'Errico"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  How, and to what extent, does an interconnected financial system endogenously\namplify external shocks? This paper attempts to reconcile some apparently\ndifferent views emerged after the 2008 crisis regarding the nature and the\nrelevance of contagion in financial networks. We develop a common framework\nencompassing several network contagion models and show that, regardless of the\nshock distribution and the network topology, precise ordering relationships on\nthe level of aggregate systemic losses hold among models. We argue that the\nextent of contagion crucially depends on the amount of information that each\nmodel assumes to be available to market players. Under no uncertainty about the\nnetwork structure and values of external assets, the well-known Eisenberg and\nNoe (2001) model applies, which delivers the lowest level of contagion. This is\ndue to a property of loss conservation: aggregate losses after contagion are\nequal to the losses incurred by those institutions initially hit by a shock.\nThis property implies that many contagion analyses rule out by construction any\nloss amplification, treating de facto an interconnected system as a single\naggregate entity, where losses are simply mutualised. Under higher levels of\nuncertainty, as captured for instance by the DebtRank model, losses become\nnon-conservative and get compounded through the network. This has important\npolicy implications: by reducing the levels of uncertainty in times of distress\n(e.g. by obtaining specific data on the network) policymakers would be able to\nmove towards more conservative scenarios. Empirically, we compare the magnitude\nof contagion across models on a sample of the largest European banks during the\nyears 2006-2016. In particular, we analyse contagion effects as a function of\nthe size of the shock and the type of external assets shocked.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.07831v1"
    },
    {
        "title": "Risk measures and Margining control",
        "authors": [
            "Giuseppe Carlo Calafiore",
            "Leonardo Massai"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  This document constitutes the final report of the contractual activity\nbetween Directa SIM and Dipartimento di Automatica e Informatica, Politecnico\ndi Torino, on the research topic titled \"quantificazione del rischio di un\nportafoglio di strumenti finanziari per trading online su device fissi e\nmobili.\"\n",
        "pdf_link": "http://arxiv.org/pdf/1608.08283v1"
    },
    {
        "title": "On Jensen's inequality for generalized Choquet integral with an\n  application to risk aversion",
        "authors": [
            "Wioletta Szeligowska",
            "Marek Kaluszka"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  In the paper we give necessary and sufficient conditions for the Jensen\ninequality to hold for the generalized Choquet integral with respect to a pair\nof capacities. Next, we apply obtained result to the theory of risk aversion by\nproviding the assumptions on utility function and capacities under which an\nagent is risk averse. Moreover, we show that the Arrow-Pratt theorem can be\ngeneralized to cover the case, where the expectation is replaced by the\ngeneralized Choquet integral.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.00554v1"
    },
    {
        "title": "Value at risk and the diversification dogma",
        "authors": [
            "Arturo Erdely"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  The so-called risk diversification principle is analyzed, showing that its\nconvenience depends on individual characteristics of the risks involved and the\ndependence relationship among them.\n  -----\n  Se analiza el principio de diversificaci\\'on de riesgos y se demuestra que no\nsiempre resulta mejor que no diversificar, pues esto depende de\ncaracter\\'isticas individuales de los riesgos involucrados, as\\'i como de la\nrelaci\\'on de dependencia entre los mismos.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.02774v1"
    },
    {
        "title": "Crises and Physical Phases of a Bipartite Market Model",
        "authors": [
            "Nima Dehmamy",
            "Sergey Buldyrev",
            "Shlomo Havlin",
            "Harry Eugene Stanley",
            "Irena Vodenska"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We analyze the linear response of a market network to shocks based on the\nbipartite market model we introduced in an earlier paper, which we claimed to\nbe able to identify the time-line of the 2009-2011 Eurozone crisis correctly.\nWe show that this model has three distinct phases that can broadly be\ncategorized as \"stable\" and \"unstable\". Based on the interpretation of our\nbehavioral parameters, the stable phase describes periods where investors and\ntraders have confidence in the market (e.g. predict that the market rebounds\nfrom a loss). We show that the unstable phase happens when there is a lack of\nconfidence and seems to describe \"boom-bust\" periods in which changes in prices\nare exponential. We analytically derive these phases and where the phase\ntransition happens using a mean field approximation of the model. We show that\nthe condition for stability is $\\alpha \\beta <1$ with $\\alpha$ being the\ninverse of the \"price elasticity\" and $\\beta$ the \"income elasticity of\ndemand\", which measures how rash the investors make decisions. We also show\nthat in the mean-field limit this model reduces to the Langevin model by\nBouchaud et al. for price returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05939v2"
    },
    {
        "title": "Risk-Consistent Conditional Systemic Risk Measures",
        "authors": [
            "Hannes Hoffmann",
            "Thilo Meyer-Brandis",
            "Gregor Svindland"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We axiomatically introduce risk-consistent conditional systemic risk measures\ndefined on multidimensional risks. This class consists of those conditional\nsystemic risk measures which can be decomposed into a state-wise conditional\naggregation and a univariate conditional risk measure. Our studies extend known\nresults for unconditional risk measures on finite state spaces. We argue in\nfavor of a conditional framework on general probability spaces for assessing\nsystemic risk. Mathematically, the problem reduces to selecting a realization\nof a random field with suitable properties. Moreover, our approach covers many\nprominent examples of systemic risk measures from the literature and used in\npractice.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.07897v1"
    },
    {
        "title": "Multiple risk factor dependence structures: Copulas and related\n  properties",
        "authors": [
            "Jianxi Su",
            "Edward Furman"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Copulas have become an important tool in the modern best practice Enterprise\nRisk Management, often supplanting other approaches to modelling stochastic\ndependence. However, choosing the `right' copula is not an easy task, and the\ntemptation to prefer a tractable rather than a meaningful candidate from the\nencompassing copulas toolbox is strong. The ubiquitous applications of the\nGaussian copula is just one illuminating example.\n  Speaking generally, a `good' copula should conform to the problem at hand,\nallow for asymmetry in the domain of definition and exhibit some extent of tail\ndependence. In this paper we introduce and study a new class of Multiple Risk\nFactor (MRF) copula functions, which we show are exactly such. Namely, the MRF\ncopulas (1) arise from a number of meaningful default risk specification with\nstochastic default barriers, (2) are in general non-exchangeable and (3)\npossess a variety of tail dependences. That being said, the MRF copulas turn\nout to be surprisingly tractable analytically.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02126v1"
    },
    {
        "title": "Fast, Accurate, Straightforward Extreme Quantiles of Compound Loss\n  Distributions",
        "authors": [
            "J. D. Opdyke"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We present an easily implemented, fast, and accurate method for approximating\nextreme quantiles of compound loss distributions (frequency+severity) as are\ncommonly used in insurance and operational risk capital models. The\nInterpolated Single Loss Approximation (ISLA) of Opdyke (2014) is based on the\nwidely used Single Loss Approximation (SLA) of Degen (2010) and maintains two\nimportant advantages over its competitors: first, ISLA correctly accounts for a\ndiscontinuity in SLA that otherwise can systematically and notably bias the\nquantile (capital) approximation under conditions of both finite and infinite\nmean. Secondly, because it is based on a closed-form approximation, ISLA\nmaintains the notable speed advantages of SLA over other methods requiring\nalgorithmic looping (e.g. fast Fourier transform or Panjer recursion). Speed is\nimportant when simulating many quantile (capital) estimates, as is so often\nrequired in practice, and essential when simulations of simulations are needed\n(e.g. some power studies). The modified ISLA (MISLA) presented herein increases\nthe range of application across the severity distributions most commonly used\nin these settings, and it is tested against extensive Monte Carlo simulation\n(one billion years' worth of losses) and the best competing method (the\nperturbative expansion (PE2) of Hernandez et al., 2014) using twelve\nheavy-tailed severity distributions, some of which are truncated. MISLA is\nshown to be comparable to PE2 in terms of both speed and accuracy, and it is\narguably more straightforward to implement for the majority of Advanced\nMeasurement Approaches (AMA) banks that are already using SLA (and failing to\ntake into account its biasing discontinuity).\n",
        "pdf_link": "http://arxiv.org/pdf/1610.03718v7"
    },
    {
        "title": "Managing Default Contagion in Inhomogeneous Financial Networks",
        "authors": [
            "Nils Detering",
            "Thilo Meyer-Brandis",
            "Konstantinos Panagiotou",
            "Daniel Ritter"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  The aim of this paper is to quantify and manage systemic risk caused by\ndefault contagion in the interbank market. We model the market as a random\ndirected network, where the vertices represent financial institutions and the\nweighted edges monetary exposures between them. Our model captures the strong\ndegree of heterogeneity observed in empirical data and the parameters can\neasily be fitted to real data sets. One of our main results allows us to\ndetermine the impact of local shocks, where initially some banks default, to\nthe entire system and the wider economy. Here the impact is measured by some\nindex of total systemic importance of all eventually defaulted institutions. As\na central application, we characterize resilient and non-resilient cases. In\nparticular, for the prominent case where the network has a degree sequence\nwithout second moment, we show that a small number of initially defaulted banks\ncan trigger a substantial default cascade. Our results complement and extend\nsignificantly earlier findings derived in the configuration model where the\nexistence of a second moment of the degree distribution is assumed. As a second\nmain contribution, paralleling regulatory discussions, we determine minimal\ncapital requirements for financial institutions sufficient to make the network\nresilient to small shocks. An appealing feature of these capital requirements\nis that they can be determined locally by each institution without knowing the\ncomplete network structure as they basically only depend on the institution's\nexposures to its counterparties.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09542v7"
    },
    {
        "title": "Optimality of Excess-Loss Reinsurance under a Mean-Variance Criterion",
        "authors": [
            "Danping Li",
            "Dongchen Li",
            "Virginia R. Young"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  In this paper, we study an insurer's reinsurance-investment problem under a\nmean-variance criterion. We show that excess-loss is the unique equilibrium\nreinsurance strategy under a spectrally negative L\\'{e}vy insurance model when\nthe reinsurance premium is computed according to the expected value premium\nprinciple. Furthermore, we obtain the explicit equilibrium\nreinsurance-investment strategy by solving the extended Hamilton-Jacobi-Bellman\nequation.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.01984v2"
    },
    {
        "title": "Systemic Risk, Maximum Entropy and Interbank Contagion",
        "authors": [
            "M. Andrecut"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We discuss the systemic risk implied by the interbank exposures reconstructed\nwith the maximum entropy method. The maximum entropy method severely\nunderestimates the risk of interbank contagion by assuming a fully connected\nnetwork, while in reality the structure of the interbank network is sparsely\nconnected. Here, we formulate an algorithm for sparse network reconstruction,\nand we show numerically that it provides a more reliable estimation of the\nsystemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.04549v1"
    },
    {
        "title": "Data driven partition-of-unity copulas with applications to risk\n  management",
        "authors": [
            "Dietmar Pfeifer",
            "Andreas Mändle",
            "Olena Ragulina"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We present a constructive and self-contained approach to data driven general\npartition-of-unity copulas that were recently introduced in the literature. In\nparticular, we consider Bernstein-, negative binomial and Poisson copulas and\npresent a solution to the problem of fitting such copulas to highly asymmetric\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.05047v4"
    },
    {
        "title": "Multivariate Geometric Expectiles",
        "authors": [
            "Klaus Herrmann",
            "Marius Hofert",
            "Melina Mailhot"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  A generalization of expectiles for d-dimensional multivariate distribution\nfunctions is introduced. The resulting geometric expectiles are unique\nsolutions to a convex risk minimization problem and are given by d-dimensional\nvectors. They are well behaved under common data transformations and the\ncorresponding sample version is shown to be a consistent estimator. We\nexemplify their usage as risk measures in a number of multivariate settings,\nhighlighting the influence of varying margins and dependence structures.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01503v2"
    },
    {
        "title": "Parameter uncertainty for integrated risk capital calculations based on\n  normally distributed subrisks",
        "authors": [
            "Andreas Fröhlich",
            "Annegret Weng"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  In this contribution we consider the overall risk given as the sum of random\nsubrisks $\\mathbf{X}_j$ in the context of value-at-risk (VaR) based risk\ncalculations. If we assume that the undertaking knows the parametric\ndistribution family subrisk $\\mathbf{X}_j=\\mathbf{X}_j(\\theta_j)$, but does not\nknow the true parameter vectors $\\theta_j$, the undertaking faces parameter\nuncertainty. To assess the appropriateness of methods to model parameter\nuncertainty for risk capital calculation we consider a criterion introduced in\nthe recent literature. According to this criterion, we demonstrate that, in\ngeneral, appropriateness of a risk capital model for each subrisk does not\nimply appropriateness of the model on the aggregate level of the overall\nrisk.\\\\ For the case where the overall risk is given by the sum of normally\ndistributed subrisks we prove a theoretical result leading to an appropriate\nintegrated risk capital model taking parameter uncertainty into account. Based\non the theorem we develop a method improving the approximation of the required\nconfidence level simultaneously for both - on the level of each subrisk as well\nas for the overall risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01608v1"
    },
    {
        "title": "The case of 'Less is more': Modelling risk-preference with Expected\n  Downside Risk",
        "authors": [
            "Mihaly Ormos",
            "Dusan Timotity"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  This paper discusses an alternative explanation for the empirical findings\ncontradicting the positive relationship between risk (variance) and reward\n(expected return). We show that these contradicting results might be due to the\nfalse definition of risk-perception, which we correct by introducing Expected\nDownside Risk (EDR). The EDR parameter, similar to the Expected Shortfall or\nConditional Value-at-Risk, measures the tail risk, however, fits and better\nexplains the utility perception of investors. Our results indicate that when\nusing the EDR as risk measure, both the positive and negative relationship\nbetween expected return and risk can be derived under standard conditions (e.g.\nexpected utility theory and positive risk-aversion). Therefore, no alternative\npsychological explanation or additional boundary condition on utility theory is\nrequired to explain the phenomenon. Furthermore, we show empirically that it is\na more precise linear predictor of expected return than volatility, both for\nindividual assets and portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05332v1"
    },
    {
        "title": "Value-at-Risk Diversification of $α$-stable Risks: The\n  Tail-Dependence Puzzle",
        "authors": [
            "Umberto Cherubini",
            "Paolo Neri"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We consider the problem of risk diversification of $\\alpha$-stable heavy\ntailed risks. We study the behaviour of the aggregated Value-at-Risk, with\nparticular reference to the impact of different tail dependence structures on\nthe limits to diversification. We confirm the large evidence of sub-additivity\nviolations, particularly for risks with low tail index values and positive\ndependence. So, reinsurance strategies are not allowed to exploit\ndiversification gains, or only a very limited amount of them. Concerning the\nimpact of tail dependence, we find the peculiar results that for high tail\ndependence levels the limits to diversification are uniformly lower for all the\nlevels of dependence, and for all levels of $\\alpha<2$. The result is confirmed\nas we move towards extreme points in the tail: in this case, we show that at\nsome point in the tail the aggregated VaR becomes additive above some level of\ndependence, but this critical dependence level is lower for copulas with lower\ntail dependence.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.07235v1"
    },
    {
        "title": "Economic Neutral Position: How to best replicate not fully replicable\n  liabilities",
        "authors": [
            "Andreas Kunz",
            "Markus Popp"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Financial undertakings often have to deal with liabilities of the form\n'non-hedgeable claim size times value of a tradeable asset', e.g. foreign\nproperty insurance claims times fx rates. Which strategy to invest in the\ntradeable asset is risk minimal? We generalize the Gram-Charlier series for the\nsum of two dependent random variable, which allows us to expand the capital\nrequirements based on value-at-risk and expected shortfall. We derive a stable\nand fairly model independent approximation of the risk minimal asset allocation\nin terms of the claim size distribution and the moments of asset return. The\nresults enable a correct and easy-to-implement modularization of capital\nrequirements into a market risk and a non-hedgeable risk component.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.08523v3"
    },
    {
        "title": "A Novel Approach to Quantification of Model Risk for Practitioners",
        "authors": [
            "Zuzana Krajcovicova",
            "Pedro Pablo Perez-Velasco",
            "Carlos Vazquez"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Models continue to increase their already broad use across industry as well\nas their sophistication. Worldwide regulation oblige financial institutions to\nmanage and address model risk with the same severity as any other type of risk,\nwhich besides defines model risk as the potential for adverse consequences from\ndecisions based on incorrect and misused model outputs and reports. Model risk\nquantification is essential not only in meeting these requirements but for\ninstitution's basic internal operative. It is however a complex task as any\ncomprehensive quantification methodology should at least consider the data used\nfor building the model, its mathematical foundations, the IT infrastructure,\noverall performance and (most importantly) usage. Besides, the current amount\nof models and different mathematical modelling techniques is overwhelming.\n  Our proposal is to define quantification of model risk as a calculation of\nthe norm of some appropriate function that belongs to a Banach space, defined\nover a weighted Riemannian manifold endowed with the Fisher--Rao metric. The\naim of the present contribution is twofold: Introduce a sufficiently general\nand sound mathematical framework to cover the aforementioned points and\nillustrate how a practitioner may identify the relevant abstract concepts and\nput them to work.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.05572v1"
    },
    {
        "title": "Sparse Structural Approach for Rating Transitions",
        "authors": [
            "Volodymyr Perederiy"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  In banking practice, rating transition matrices have become the standard\napproach of deriving multi-year probabilities of default (PDs) from one-year\nPDs, the latter normally being available from Basel ratings. Rating transition\nmatrices have gained in importance with the newly adopted IFRS 9 accounting\nstandard. Here, the multi-year PDs can be used to calculate the so-called\nexpected credit losses (ECL) over the entire lifetime of relevant credit\nassets. A typical approach for estimating the rating transition matrices relies\non calculating empirical rating migration counts and frequencies from rating\nhistory data. For small portfolios, however, this approach often leads to zero\ncounts and high count volatility, which makes the estimations unreliable and\nunstable, and can also produce counter-intuitive prediction patterns such as\nnon-parallel/crossing forward PD patterns. This paper proposes a structural\nmodel which overcomes these problems. We make a plausible assumption of an\nunderlying autoregressive mean-reverting ability-to-pay process. With only\nthree parameters, this sparse process can well describe an entire typical\nrating transition matrix, provided the one-year PDs of the rating classes are\nspecified. The transition probabilities produced by the structural approach are\nwell-behaved by design. The approach significantly reduces the statistical\ndegrees of freedom of the estimated transition probabilities, which makes the\nrating transition matrix more reliable for small portfolios. The approach can\nbe applied to data with as few as 50 observed rating transitions. Moreover, the\napproach can be efficiently applied to data consisting of continuous PDs (prior\nto rating discretization). In the IFRS 9 context, the approach offers an\nadditional merit: it can easily account for the macroeconomic adjustments,\nwhich are required by the IFRS 9 accounting standard.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.00062v3"
    },
    {
        "title": "Spectral backtests of forecast distributions with application to risk\n  management",
        "authors": [
            "Michael B. Gordy",
            "Alexander J. McNeil"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We study a class of backtests for forecast distributions in which the test\nstatistic depends on a spectral transformation that weights exceedance events\nby a function of the modeled probability level. The weighting scheme is\nspecified by a kernel measure which makes explicit the user's priorities for\nmodel performance. The class of spectral backtests includes tests of\nunconditional coverage and tests of conditional coverage. We show how the class\nembeds a wide variety of backtests in the existing literature, and further\npropose novel variants which are easily implemented, well-sized and have good\npower. In an empirical application, we backtest forecast distributions for the\novernight P&L of ten bank trading portfolios. For some portfolios, test results\ndepend materially on the choice of kernel.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01489v6"
    },
    {
        "title": "Financial option insurance",
        "authors": [
            "Qi-Wen Wang",
            "Jian-Jun Shu"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  The option is a financial derivative, which is regularly employed in reducing\nthe risk of its underlying securities. However, investing in option is still\nrisky. Such risk becomes much severer for speculators who utilize option as a\nmeans of leverage to increase their potential returns. In order to mitigate\nrisk on their positions, the rudimentary concept of financial option insurance\nis introduced into practice. Two starkly-dissimilar concepts of insurance and\nfinancial option are integrated into the formation of financial option\ninsurance. The proposed financial product insures investors option premiums\nwhen misfortune befalls on them. As a trade-off, they are likely to sacrifice a\nlimited portion of their potential profits. The loopholes of prevailing\nfinancial market are addressed and the void is filled by introducing a stable\nthree-entity framework. Moreover, a specifically designed mathematical model is\nproposed. It consists of two portions: the business strategy of matching and a\nverification-and-modification process. The proposed model enables the option\ninvestors with calls and puts of different moneyness to be protected by the\nissued option insurance. Meanwhile, it minimizes the exposure of option\ninsurers position to any potential losses.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02180v1"
    },
    {
        "title": "VIX-linked fees for GMWBs via Explicit Solution Simulation Methods",
        "authors": [
            "Michael A. Kouritzin",
            "Anne MacKay"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  In a market with stochastic volatility and jumps, we consider a VIX-linked\nfee structure for variable annuity contracts with guaranteed minimum withdrawal\nbenefits (GMWB). Our goal is to assess the effectiveness of the VIX-linked fee\nstructure in decreasing the sensitivity of the insurer's liability to\nvolatility risk. Since the GMWB payoff is highly path-dependent, it is\nparticularly sensitive to volatility risk, and can also be challenging to\nprice, especially in the presence of the VIX-linked fee. In this paper, we\npresent an explicit weak solution for the value of the VA account and use it in\nMonte Carlo simulations to value the GMWB guarantee. Numerical examples are\nprovided to analyze the impact of the VIX-linked fee on the sensitivity of the\nliability to changes in market volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06886v2"
    },
    {
        "title": "Value-at-Risk and Expected Shortfall for the major digital currencies",
        "authors": [
            "Stavros Stavroyiannis"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Digital currencies and cryptocurrencies have hesitantly started to penetrate\nthe investors, and the next step will be the regulatory risk management\nframework. We examine the Value-at-Risk and Expected Shortfall properties for\nthe major digital currencies, Bitcoin, Ethereum, Litecoin, and Ripple. The\nmethodology used is GARCH modelling followed by Filtered Historical Simulation.\nWe find that digital currencies are subject to a higher risk, therefore, to\nhigher sufficient buffer and risk capital to cover potential losses.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.09343v1"
    },
    {
        "title": "Extending Yagil exchange ratio determination model to the case of\n  stochastic dividends",
        "authors": [
            "Alessandra Mainini",
            "Enrico Moretto"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  This article extends, in a stochastic environment, the Yagil (1987) model\nwhich establishes, in a deterministic dividend discount model, a range for the\nexchange ratio in a stock-for-stock merger agreement. Here, we generalize\nYagil's work letting both pre- and post-merger dividends grow randomly over\ntime. If Yagil focuses only on changes in stock prices before and after the\nmerger, our stochastic environment allows to keep in account both shares'\nexpected values and variance, letting us to identify a more complex bargaining\nregion whose shape depends on mean and standard deviation of the dividends'\ngrowth rate.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.09810v1"
    },
    {
        "title": "Risk-Minimizing Hedging of Counterparty Risk",
        "authors": [
            "Lijun Bo",
            "Agostino Capponi",
            "Claudia Ceci"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We study dynamic hedging of counterparty risk for a portfolio of credit\nderivatives. Our empirically driven credit model consists of interacting\ndefault intensities which ramp up and then decay after the occurrence of credit\nevents. Using the Galtchouk-Kunita-Watanabe decomposition of the counterparty\nrisk price payment stream, we recover a closed-form representation for the risk\nminimizing strategy in terms of classical solutions to nonlinear recursive\nsystems of Cauchy problems. We discuss applications of our framework to the\nmost prominent class of credit derivatives, including credit swap and risky\nbond portfolios, as well as first-to-default claims.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.01115v1"
    },
    {
        "title": "New copulas based on general partitions-of-unity and their applications\n  to risk management (part II)",
        "authors": [
            "Dietmar Pfeifer",
            "Andreas Mändle",
            "Olena Ragulina"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We present a constructive and self-contained approach to data driven infinite\npartition-of-unity copulas that were recently introduced in the literature. In\nparticular, we consider negative binomial and Poisson copulas and present a\nsolution to the problem of fitting such copulas to highly asymmetric data in\narbitrary dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.07682v2"
    },
    {
        "title": "Optimizing S-shaped utility and implications for risk management",
        "authors": [
            "John Armstrong",
            "Damiano Brigo"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We consider market players with tail-risk-seeking behaviour as exemplified by\nthe S-shaped utility introduced by Kahneman and Tversky. We argue that risk\nmeasures such as value at risk (VaR) and expected shortfall (ES) are\nineffective in constraining such players. We show that, in many standard market\nmodels, product design aimed at utility maximization is not constrained at all\nby VaR or ES bounds: the maximized utility corresponding to the optimal payoff\nis the same with or without ES constraints. By contrast we show that, in\nreasonable markets, risk management constraints based on a second more\nconventional concave utility function can reduce the maximum S-shaped utility\nthat can be achieved by the investor, even if the constraining utility function\nis only rather modestly concave. It follows that product designs leading to\nunbounded S-shaped utilities will lead to unbounded negative expected\nconstraining utilities when measured with such conventional utility functions.\nTo prove these latter results we solve a general problem of optimizing an\ninvestor expected utility under risk management constraints where both investor\nand risk manager have conventional concave utility functions, but the investor\nhas limited liability. We illustrate our results throughout with the example of\nthe Black--Scholes option market. These results are particularly important\ngiven the historical role of VaR and that ES was endorsed by the Basel\ncommittee in 2012--2013.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.00443v3"
    },
    {
        "title": "Asymptotic Analysis for Spectral Risk Measures Parameterized by\n  Confidence Level",
        "authors": [
            "Takashi Kato"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We study the asymptotic behavior of the difference $\\Delta \\rho ^{X,\nY}_\\alpha := \\rho _\\alpha (X + Y) - \\rho _\\alpha (X)$ as $\\alpha \\rightarrow\n1$, where $\\rho_\\alpha $ is a risk measure equipped with a confidence level\nparameter $0 < \\alpha < 1$, and where $X$ and $Y$ are non-negative random\nvariables whose tail probability functions are regularly varying. The case\nwhere $\\rho _\\alpha $ is the value-at-risk (VaR) at $\\alpha $, is treated in\nKato (2017). This paper investigates the case where $\\rho _\\alpha $ is a\nspectral risk measure that converges to the worst-case risk measure as $\\alpha\n\\rightarrow 1$. We give the asymptotic behavior of the difference between the\nmarginal risk contribution and the Euler contribution of $Y$ to the portfolio\n$X + Y$. Similarly to Kato (2017), our results depend primarily on the relative\nmagnitudes of the thicknesses of the tails of $X$ and $Y$. We also conducted a\nnumerical experiment, finding that when the tail of $X$ is sufficiently thicker\nthan that of $Y$, $\\Delta \\rho ^{X, Y}_\\alpha $ does not increase monotonically\nwith $\\alpha$ and takes a maximum at a confidence level strictly less than $1$.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07335v1"
    },
    {
        "title": "Risk Apportionment: The Dual Story",
        "authors": [
            "Louis R. Eeckhoudt",
            "Roger J. A. Laeven",
            "Harris Schlesinger"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  By specifying model free preferences towards simple nested classes of lottery\npairs, we develop the dual story to stand on equal footing with that of\n(primal) risk apportionment. The dual story provides an intuitive\ninterpretation, and full characterization, of dual counterparts of such\nconcepts as prudence and temperance. The direction of preference between these\nnested classes of lottery pairs is equivalent to signing the successive\nderivatives of the probability weighting function within Yaari's (1987) dual\ntheory. We explore implications of our results for optimal portfolio choice and\nshow that the sign of the third derivative of the probability weighting\nfunction may be naturally linked to a self-protection problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.02182v1"
    },
    {
        "title": "Modeling the number of hidden events subject to observation delay",
        "authors": [
            "Jonas Crevecoeur",
            "Katrien Antonio",
            "Roel Verbelen"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  This paper considers the problem of predicting the number of events that have\noccurred in the past, but which are not yet observed due to a delay. Such\ndelayed events are relevant in predicting the future cost of warranties,\npricing maintenance contracts, determining the number of unreported claims in\ninsurance and in modeling the outbreak of diseases. Disregarding these\nunobserved events results in a systematic underestimation of the event\noccurrence process. Our approach puts emphasis on modeling the time between the\noccurrence and observation of the event, the so-called observation delay. We\npropose a granular model for the heterogeneity in this observation delay based\non the occurrence day of the event and on calendar day effects in the\nobservation process, such as weekday and holiday effects. We illustrate this\napproach on a European general liability insurance data set where the\noccurrence of an accident is reported to the insurer with delay.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02935v2"
    },
    {
        "title": "Is there a housing bubble in China",
        "authors": [
            "Tianhao Zhi",
            "Zhongfei Li",
            "Zhiqiang Jiang",
            "Lijian Wei",
            "Didier Sornette"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  There is a growing concern in recent years over the potential formation of\nbubbles in the Chinese real estate market. This paper aims to conduct a series\nof bubble diagnostic analysis over nine representative Chinese cities from two\naspects. First, we investigate whether the prices had been significantly\ndeviating from economic fundamentals by applying a standard Engle-Granger\ncointegration test. Second, we apply the Log-Periodic-Power-Law-Singularity\n(LPPLS) model to detect whether there is any evidence of unsustainable,\nself-reinforcing speculative behaviours amongst the price series. We propose\nthat, given the heterogeneity that exists amongst cities with different types\nof bubble signatures, it is vital to conduct bubble diagnostic tests and\nimplement relevant policies toward specific bubble characteristics, rather than\nenforcing one-that-fits-for-all type policy that does not take into account\nsuch heterogeneity.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.03678v1"
    },
    {
        "title": "Regression Based Expected Shortfall Backtesting",
        "authors": [
            "Sebastian Bayer",
            "Timo Dimitriadis"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  This paper introduces novel backtests for the risk measure Expected Shortfall\n(ES) following the testing idea of Mincer and Zarnowitz (1969). Estimating a\nregression framework for the ES stand-alone is infeasible, and thus, our tests\nare based on a joint regression for the Value at Risk and the ES, which allows\nfor different test specifications. These ES backtests are the first which\nsolely backtest the ES in the sense that they only require ES forecasts as\ninput parameters. As the tests are potentially subject to model\nmisspecification, we provide asymptotic theory under misspecification for the\nunderlying joint regression. We find that employing a misspecification robust\ncovariance estimator substantially improves the tests' performance. We compare\nour backtests to existing approaches and find that our tests outperform the\ncompetitors throughout all considered simulations. In an empirical\nillustration, we apply our backtests to ES forecasts for 200 stocks of the S&P\n500 index.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04112v2"
    },
    {
        "title": "The Influence of Seed Selection on the Solvency II Ratio",
        "authors": [
            "Quinn Culver",
            "Dennis Heitmann",
            "Christian Weiß"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  This article contains the first published example of a real economic balance\nsheet where the Solvency II ratio substantially depends on the seed selected\nfor the random number generator (RNG) used. The theoretical background and the\nmain quality criteria for RNGs are explained in detail. To serve as a gauge for\nRNGs, a definition of true randomness is given. Quality tests that RNGs should\npass in order to generate stable results when used in risk management under\nSolvency II are described.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05409v2"
    },
    {
        "title": "Modelo de maturidade em gerenciamento de riscos em projetos (Project\n  Risk Management Model Maturity)",
        "authors": [
            "Ricardo Antunes",
            "Daniel Birchal",
            "João Márcio Abijaodi",
            "Paulo Abreu",
            "Rogério Peixoto"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  The globalization feeded by the technology explosion that begans in the end\nof the last century, started the world to change faster every day. The only\ntoday's certain is the tomorrow's uncertain. Risk is defined as uncertain where\none or many causes composed of ocurrence probality can generate an impact or\nconsequence (threat if negative and oportunity if positive, to a determinated\ngoal). The Risk Management is composed of culture, procedure and process of an\norganization or individual care of uncertain, aiming to minimize threats e\nmaximizing the oportunities, to reach a desired goal. The \"Risk maturity model\nin projects\" proposed on this document, wants to measure the organizations\ncapacity and skills to manage the riks involved in projects when adopting a\ngeneric risk management methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06595v1"
    },
    {
        "title": "On a capital allocation principle coherent with the Solvency 2 standard\n  formula",
        "authors": [
            "Fabio Baione",
            "Paolo De Angelis",
            "Ivan Granito"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Solvency II Directive 2009/138/EC requires an insurance and reinsurance\nundertakings assessment of a Solvency Capital Requirement by means of the\nso-called \"Standard Formula\" or by means of partial or full internal models.\nFocusing on the first approach, the bottom-up aggregation formula proposed by\nthe regulator permits a capital reduction due to diversification effect,\naccording to the typical subadditivity property of risk measures. However, once\nthe overall capital has been assessed no specific allocation formula is\nprovided or required in order to evaluate the contribution of each risk source\non the overall SCR. The aim of this paper is to provide a closed formula for\ncapital allocation fully coherent with the Solvency II Capital Requirement\nassessed by means of Standard Formula. The solution proposed permits a top-down\napproach to assess the allocated SCR among the risks considered in the\nmultilevel aggregation scheme established by Solvency II. Besides, we\ndemonstrate that the allocation formula here proposed is consistent with the\nEuler's allocation principle\n",
        "pdf_link": "http://arxiv.org/pdf/1801.09004v1"
    },
    {
        "title": "Systemic-risk-efficient asset allocation: Minimization of systemic risk\n  as a network optimization problem",
        "authors": [
            "Anton Pichler",
            "Sebastian Poledna",
            "Stefan Thurner"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Systemic risk arises as a multi-layer network phenomenon. Layers represent\ndirect financial exposures of various types, including interbank liabilities,\nderivative- or foreign exchange exposures. Another network layer of systemic\nrisk emerges through common asset holdings of financial institutions. Strongly\noverlapping portfolios lead to similar exposures that are caused by price\nmovements of the underlying financial assets. Based on the knowledge of\nportfolio holdings of financial agents we quantify systemic risk of overlapping\nportfolios. We present an optimization procedure, where we minimize the\nsystemic risk in a given financial market by optimally rearranging overlapping\nportfolio networks, under the constraints that the expected returns and risks\nof the individual portfolios are unchanged. We explicitly demonstrate the power\nof the method on the overlapping portfolio network of sovereign exposure\nbetween major European banks by using data from the European Banking Authority\nstress test of 2016. We show that systemic-risk-efficient allocations are\naccessible by the optimization. In the case of sovereign exposure, systemic\nrisk can be reduced by more than a factor of two, with- out any detrimental\neffects for the individual banks. These results are confirmed by a simple\nsimulation of fire sales in the government bond market. In particular we show\nthat the contagion probability is reduced dramatically in the optimized\nnetwork.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.10515v2"
    },
    {
        "title": "Quantification of systemic risk from overlapping portfolios in the\n  financial system",
        "authors": [
            "Sebastian Poledna",
            "Serafín Martínez-Jaramillo",
            "Fabio Caccioli",
            "Stefan Thurner"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Financial markets are exposed to systemic risk, the risk that a substantial\nfraction of the system ceases to function and collapses. Systemic risk can\npropagate through different mechanisms and channels of contagion. One important\nform of financial contagion arises from indirect interconnections between\nfinancial institutions mediated by financial markets. This indirect\ninterconnection occurs when financial institutions invest in common assets and\nis referred to as overlapping portfolios. In this work we quantify systemic\nrisk from indirect interconnections between financial institutions. Having\ncomplete information of security holdings of major Mexican financial\nintermediaries and the ability to uniquely identify securities in their\nportfolios, allows us to represent the Mexican financial system as a bipartite\nnetwork of securities and financial institutions. This makes it possible to\nquantify systemic risk arising from overlapping portfolios. We show that\nfocusing only on direct exposures underestimates total systemic risk levels by\nup to 50%. By representing the financial system as a multi-layer network of\ndirect exposures (default contagion) and indirect exposures (overlapping\nportfolios) we estimate the mutual influence of different channels of\ncontagion. The method presented here is the first objective data-driven\nquantification of systemic risk on national scales that includes overlapping\nportfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.00311v1"
    },
    {
        "title": "Analytical Validation Formulas for Best Estimate Calculation in\n  Traditional Life Insurance",
        "authors": [
            "Simon Hochgerner",
            "Florian Gach"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Within the context of traditional life insurance, a model-independent\nrelationship about how the market value of assets is attributed to the best\nestimate, the value of in-force business and tax is established. This\nrelationship holds true for any portfolio under run-off assumptions and can be\nused for the validation of models set up for Solvency~II best estimate\ncalculation. Furthermore, we derive a lower bound for the value of future\ndiscretionary benefits. This lower bound formula is applied to publicly\navailable insurance data to show how it can be used for practical validation\npurposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.07009v3"
    },
    {
        "title": "RACORN-K: Risk-Aversion Pattern Matching-based Portfolio Selection",
        "authors": [
            "Yang Wang",
            "Dong Wang",
            "Yaodong Wang",
            "You Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Portfolio selection is the central task for assets management, but it turns\nout to be very challenging. Methods based on pattern matching, particularly the\nCORN-K algorithm, have achieved promising performance on several stock markets.\nA key shortage of the existing pattern matching methods, however, is that the\nrisk is largely ignored when optimizing portfolios, which may lead to\nunreliable profits, particularly in volatile markets. We present a\nrisk-aversion CORN-K algorithm, RACORN-K, that penalizes risk when searching\nfor optimal portfolios. Experiments on four datasets (DJIA, MSCI, SP500(N),\nHSI) demonstrate that the new algorithm can deliver notable and reliable\nimprovements in terms of return, Sharp ratio and maximum drawdown, especially\non volatile markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.10244v1"
    },
    {
        "title": "The determinants of bank loan recovery rates in good times and bad - new\n  evidence",
        "authors": [
            "Hong Wang",
            "Catherine S. Forbes",
            "Jean-Pierre Fenech",
            "John Vaz"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We find that factors explaining bank loan recovery rates vary depending on\nthe state of the economic cycle. Our modeling approach incorporates a two-state\nMarkov switching mechanism as a proxy for the latent credit cycle, helping to\nexplain differences in observed recovery rates over time. We are able to\ndemonstrate how the probability of default and certain loan-specific and other\nvariables hold different explanatory power with respect to recovery rates over\n`good' and `bad' times in the credit cycle. That is, the relationship between\nrecovery rates and certain loan characteristics, firm characteristics and the\nprobability of default differs depending on underlying credit market\nconditions. This holds important implications for modelling capital retention,\nparticularly in terms of countercyclicality.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07022v1"
    },
    {
        "title": "On the complexity of solving a decision problem with flow-depending\n  costs: the case of the IJsselmeer dikes",
        "authors": [
            "Aida Abiad",
            "Sander Gribling",
            "Domenico Lahaye",
            "Matthias Mnich",
            "Guus Regts",
            "Lluis Vena",
            "Gerard Verweij",
            "Peter Zwaneveld"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We consider a fundamental integer programming (IP) model for cost-benefit\nanalysis flood protection through dike building in the Netherlands, due to\nVerweij and Zwaneveld.\n  Experimental analysis with data for the Ijsselmeer lead to integral optimal\nsolution of the linear programming relaxation of the IP model.\n  This naturally led to the question of integrality of the polytope associated\nwith the IP model.\n  In this paper we first give a negative answer to this question by\nestablishing non-integrality of the polytope.\n  Second, we establish natural conditions that guarantee the linear programming\nrelaxation of the IP model to be integral.\n  We then test the most recent data on flood probabilities, damage and\ninvestment costs of the IJsselmeer for these conditions.\n  Third, we show that the IP model can be solved in polynomial time when the\nnumber of dike segments, or the number of feasible barrier heights, are\nconstant.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.09752v1"
    },
    {
        "title": "Chebyshev Methods for Ultra-efficient Risk Calculations",
        "authors": [
            "Mariano Zeron Medina Laris",
            "Ignacio Ruiz"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Financial institutions now face the important challenge of having to do\nmultiple portfolio revaluations for their risk computation. The list is almost\nendless: from XVAs to FRTB, stress testing programs, etc. These computations\nrequire from several hundred up to a few million revaluations. The cost of\nimplementing these calculations via a \"brute-force\" full revaluation is\nenormous. There is now a strong demand in the industry for algorithmic\nsolutions to the challenge. In this paper we show a solution based on Chebyshev\ninterpolation techniques. It is based on the demonstrated fact that those\ninterpolants show exponential convergence for the vast majority of pricing\nfunctions that an institution has. In this paper we elaborate on the theory\nbehind it and extend those techniques to any dimensionality. We then approach\nthe problem from a practical standpoint, illustrating how it can be applied to\nmany of the challenges the industry is currently facing. We show that the\ncomputational effort of many current risk calculations can be decreased orders\nof magnitude with the proposed techniques, without compromising accuracy.\nIllustrative examples include XVAs and IMM on exotics, XVA sensitivities,\nInitial Margin Simulations, IMA-FRTB and AAD.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.00898v1"
    },
    {
        "title": "Improving Value-at-Risk prediction under model uncertainty",
        "authors": [
            "Shige Peng",
            "Shuzhen Yang",
            "Jianfeng Yao"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Several well-established benchmark predictors exist for Value-at-Risk (VaR),\na major instrument for financial risk management. Hybrid methods combining\nAR-GARCH filtering with skewed-$t$ residuals and the extreme value theory-based\napproach are particularly recommended. This study introduces yet another VaR\npredictor, G-VaR, which follows a novel methodology. Inspired by the recent\nmathematical theory of sublinear expectation, G-VaR is built upon the concept\nof model uncertainty, which in the present case signifies that the inherent\nvolatility of financial returns cannot be characterized by a single\ndistribution but rather by infinitely many statistical distributions. By\nconsidering the worst scenario among these potential distributions, the G-VaR\npredictor is precisely identified. Extensive experiments on both the NASDAQ\nComposite Index and S\\&P500 Index demonstrate the excellent performance of the\nG-VaR predictor, which is superior to most existing benchmark VaR predictors.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.03890v4"
    },
    {
        "title": "Bitcoin Risk Modeling with Blockchain Graphs",
        "authors": [
            "Cuneyt Akcora",
            "Matthew Dixon",
            "Yulia Gel",
            "Murat Kantarcioglu"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  A key challenge for Bitcoin cryptocurrency holders, such as startups using\nICOs to raise funding, is managing their FX risk. Specifically, a misinformed\ndecision to convert Bitcoin to fiat currency could, by itself, cost USD\nmillions.\n  In contrast to financial exchanges, Blockchain based crypto-currencies expose\nthe entire transaction history to the public. By processing all transactions,\nwe model the network with a high fidelity graph so that it is possible to\ncharacterize how the flow of information in the network evolves over time. We\ndemonstrate how this data representation permits a new form of microstructure\nmodeling - with the emphasis on the topological network structures to study the\nrole of users, entities and their interactions in formation and dynamics of\ncrypto-currency investment risk. In particular, we identify certain sub-graphs\n('chainlets') that exhibit predictive influence on Bitcoin price and\nvolatility, and characterize the types of chainlets that signify extreme\nlosses.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04698v1"
    },
    {
        "title": "The strong Fatou property of risk measures",
        "authors": [
            "Shengzhong Chen",
            "Niushan Gao",
            "Foivos Xanthos"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this paper, we explore several Fatou-type properties of risk measures. The\npaper continues to reveal that the strong Fatou property, which was introduced\nin [17], seems to be most suitable to ensure nice dual representations of risk\nmeasures. Our main result asserts that every quasiconvex law-invariant\nfunctional on a rearrangement invariant space $\\mathcal{X}$ with the strong\nFatou property is $\\sigma(\\mathcal{X},L^\\infty)$ lower semicontinuous and that\nthe converse is true on a wide range of rearrangement invariant spaces. We also\nstudy inf-convolutions of law-invariant or surplus-invariant risk measures that\npreserve the (strong) Fatou property.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.05259v1"
    },
    {
        "title": "Semi-parametric Dynamic Asymmetric Laplace Models for Tail Risk\n  Forecasting, Incorporating Realized Measures",
        "authors": [
            "Richard Gerlach",
            "Chao Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  The joint Value at Risk (VaR) and expected shortfall (ES) quantile regression\nmodel of Taylor (2017) is extended via incorporating a realized measure, to\ndrive the tail risk dynamics, as a potentially more efficient driver than daily\nreturns. Both a maximum likelihood and an adaptive Bayesian Markov Chain Monte\nCarlo method are employed for estimation, whose properties are assessed and\ncompared via a simulation study; results favour the Bayesian approach, which is\nsubsequently employed in a forecasting study of seven market indices and two\nindividual assets. The proposed models are compared to a range of parametric,\nnon-parametric and semi-parametric models, including GARCH, Realized-GARCH and\nthe joint VaR and ES quantile regression models in Taylor (2017). The\ncomparison is in terms of accuracy of one-day-ahead Value-at-Risk and Expected\nShortfall forecasts, over a long forecast sample period that includes the\nglobal financial crisis in 2007-2008. The results favor the proposed models\nincorporating a realized measure, especially when employing the sub-sampled\nRealized Variance and the sub-sampled Realized Range.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.08653v1"
    },
    {
        "title": "Cascading Losses in Reinsurance Networks",
        "authors": [
            "Ariah Klages-Mundt",
            "Andreea Minca"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We develop a model for contagion in reinsurance networks by which primary\ninsurers' losses are spread through the network. Our model handles general\nreinsurance contracts, such as typical excess of loss contracts. We show that\nsimpler models existing in the literature--namely proportional\nreinsurance--greatly underestimate contagion risk. We characterize the fixed\npoints of our model and develop efficient algorithms to compute contagion with\nguarantees on convergence and speed under conditions on network structure. We\ncharacterize exotic cases of problematic graph structure and nonlinearities,\nwhich cause network effects to dominate the overall payments in the system. We\nlastly apply our model to data on real world reinsurance networks. Our\nsimulations demonstrate the following: (1) Reinsurance networks face extreme\nsensitivity to parameters. A firm can be wildly uncertain about its losses even\nunder small network uncertainty. (2) Our sensitivity results reveal a new\nincentive for firms to cooperate to prevent fraud, as even small cases of fraud\ncan have outsized effect on the losses across the network. (3) Nonlinearities\nfrom excess of loss contracts obfuscate risks and can cause excess costs in a\nreal world system.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.12222v5"
    },
    {
        "title": "Time consistency for scalar multivariate risk measures",
        "authors": [
            "Zachary Feinstein",
            "Birgit Rudloff"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this paper we present results on dynamic multivariate scalar risk\nmeasures, which arise in markets with transaction costs and systemic risk. Dual\nrepresentations of such risk measures are presented. These are then used to\nobtain the main results of this paper on time consistency; namely, an\nequivalent recursive formulation of multivariate scalar risk measures to\nmultiportfolio time consistency. We are motivated to study time consistency of\nmultivariate scalar risk measures as the superhedging risk measure in markets\nwith transaction costs (with a single eligible asset) (Jouini and Kallal\n(1995), Roux and Zastawniak (2016), Loehne and Rudloff (2014)) does not satisfy\nthe usual scalar concept of time consistency. In fact, as demonstrated in\n(Feinstein and Rudloff (2021)), scalar risk measures with the same\nscalarization weight at all times would not be time consistent in general. The\ndeduced recursive relation for the scalarizations of multiportfolio time\nconsistent set-valued risk measures provided in this paper requires\nconsideration of the entire family of scalarizations. In this way we develop a\ndirect notion of a \"moving scalarization\" for scalar time consistency that\ncorroborates recent research on scalarizations of dynamic multi-objective\nproblems (Karnam, Ma, and Zhang (2017), Kovacova and Rudloff (2021)).\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04978v5"
    },
    {
        "title": "Systemic Greeks: Measuring risk in financial networks",
        "authors": [
            "Nils Bertschinger",
            "Julian Stobbe"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Since the latest financial crisis, the idea of systemic risk has received\nconsiderable interest. In particular, contagion effects arising from\ncross-holdings between interconnected financial firms have been studied\nextensively. Drawing inspiration from the field of complex networks, these\nattempts are largely unaware of models and theories for credit risk of\nindividual firms. Here, we note that recent network valuation models extend the\nseminal structural risk model of Merton (1974). Furthermore, we formally\ncompute sensitivities to various risk factors -- commonly known as Greeks -- in\na network context. In particular, we propose the network $\\Delta$ as a\nquantitative measure of systemic risk and illustrate our findings on some\nnumerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11849v1"
    },
    {
        "title": "Modelling China's Credit System with Complex Network Theory for\n  Systematic Credit Risk Control",
        "authors": [
            "Xuan Lu",
            "Li Huang",
            "Kangjuan Lyu"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  The insufficient understanding of the credit network structure was recognized\nas a key factor for regulators' underestimation of the destructive systematic\nrisk during the financial crisis that started in 2007. The existing credit\nnetwork research either took a macro perspective to clarify the topological\nproperties of financial systems at a descriptive level or analyzed the risk\ntransmission path and characteristics of individual entities with much\npre-assumptions of the network. Here, we used the theory of complex network to\nmodel China's credit system from 2000 to 2014 based on actual financial data. A\nbipartite financial institution-firm network and its projected sub-networks\nwere constructed for an integrated analysis from both macro and micro\nperspectives, and the relationship between typological properties and\nsystematic credit risk control was also explored. The typological analysis of\nthe networks suggested that the financial institutions and firms were highly\nbut asymmetrically connected, and the credit network structure made local\nidiosyncratic shocks possible to proliferate through the whole economy. In\naddition, the Chinese credit market was still dominated by state-owned\nfinancial institutions with firms competing fiercely for financial resources in\nthe past fifteen years. Furthermore, the credit risk score (CRS) was introduced\nby simulation to identify the systematically important vertices in terms of\nsystematic risk control. The results indicated that the vertices with more\naccess to the credit market or less likelihood to be a bridge in the network\nwere the ones with higher systematically importance. The empirical results from\nthis study would provide specific policy suggestions to financial regulators on\nsupervisory approaches and optimizing the allocation of regulatory resources to\nenhance the robustness of credit systems in China and in other countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.01341v1"
    },
    {
        "title": "Monetary Measures of Risk",
        "authors": [
            "Andreas H Hamel"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  This survey gives an introduction to monetary measures of risk as monotone\nand cash additive functions on spaces of univariate random variables. Primal\nand dual representation results as well as several examples are discussed.\nPrincipal ways to construct risk measures are given and extensions to more\ngeneral situations indicated.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.04354v1"
    },
    {
        "title": "Weak comonotonicity",
        "authors": [
            "Ruodu Wang",
            "Ricardas Zitikis"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  The classical notion of comonotonicity has played a pivotal role when solving\ndiverse problems in economics, finance, and insurance. In various practical\nproblems, however, this notion of extreme positive dependence structure is\noverly restrictive and sometimes unrealistic. In the present paper, we put\nforward a notion of weak comonotonicity, which contains the classical notion of\ncomonotonicity as a special case, and gives rise to necessary and sufficient\nconditions for a number of optimization problems, such as those arising in\nportfolio diversification, risk aggregation, and premium calculation. In\nparticular, we show that a combination of weak comonotonicity and weak\nantimonotonicity with respect to some choices of measures is sufficient for the\nmaximization of Value-at-Risk aggregation, and weak comonotonicity is necessary\nand sufficient for the Expected Shortfall aggregation. Finally, with the help\nof weak comonotonicity acting as an intermediate notion of dependence between\nthe extreme cases of no dependence and strong comonotonicity, we give a natural\nsolution to a risk-sharing problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.04827v2"
    },
    {
        "title": "Systemic risk measures with markets volatility",
        "authors": [
            "Fei Sun",
            "Yijun Hu"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  As systemic risk has become a hot topic in the financial markets, how to\nmeasure, allocate and regulate the systemic risk are becoming especially\nimportant. However, the financial markets are becoming more and more\ncomplicate, which makes the usual study of systemic risk to be restricted. In\nthis paper, we will study the systemic risk measures on a special space\n$L^{p(\\cdot)}$ where the variable exponent $p(\\cdot)$ is no longer a given real\nnumber like the space $L^{p}$, but a random variable, which reflects the\npossible volatility of the financial markets. Finally, the dual representation\nfor this new systemic risk measures will be studied. Our results show that\nevery this new systemic risk measure can be decomposed into a convex certain\nfunction and a simple-systemic risk measure, which provides a new ideas for\ndealing with the systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06185v3"
    },
    {
        "title": "Systemic risk governance in a dynamical model of a banking system",
        "authors": [
            "Lorella Fatone",
            "Francesca Mariani"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We consider the problem of governing systemic risk in a banking system model.\nThe banking system model consists in an initial value problem for a system of\nstochastic differential equations whose dependent variables are the\nlog-monetary reserves of the banks as functions of time. The banking system\nmodel considered generalizes previous models studied in [5], [4], [7] and\ndescribes an homogeneous population of banks. Two distinct mechanisms are used\nto model the cooperation among banks and the cooperation between banks and\nmonetary authority. These mechanisms are regulated respectively by the\nparameters $\\alpha$ and $\\gamma$. A bank fails when its log-monetary reserves\ngo below an assigned default level. We call systemic risk or systemic event in\na bounded time interval the fact that in that time interval at least a given\nfraction of the banks fails. The probability of systemic risk in a bounded time\ninterval is evaluated using statistical simulation. A method to govern the\nprobability of systemic risk in a bounded time interval is presented. The goal\nof the governance is to keep the probability of systemic risk in a bounded time\ninterval between two given thresholds. The governance is based on the choice of\nthe log-monetary reserves of a kind of \"ideal bank\" as a function of time and\non the solution of an optimal control problem for the mean field approximation\nof the banking system model. The solution of the optimal control problem\ndetermines the parameters $\\alpha$ and $\\gamma$ as functions of time, that is\ndefines the rules of the borrowing and lending activity among banks and between\nbanks and monetary authority. Some numerical examples are discussed. The\nsystemic risk governance is tested in absence and in presence of positive and\nnegative shocks acting on the banking system.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06973v1"
    },
    {
        "title": "An Enhanced Initial Margin Methodology to Manage Warehoused Credit Risk",
        "authors": [
            "Lucia Cipolina-Kun",
            "Ignacio Ruiz",
            "Mariano Zero-Medina Laris"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  The use of CVA to cover credit risk is widely spread, but has its\nlimitations. Namely, dealers face the problem of the illiquidity of instruments\nused for hedging it, hence forced to warehouse credit risk. As a result,\ndealers tend to offer a limited OTC derivatives market to highly risky\ncounterparties. Consequently, those highly risky entities rarely have access to\nhedging services precisely when they need them most. In this paper we propose a\nmethod to overcome this limitation. We propose to extend the CVA risk-neutral\nframework to compute an initial margin (IM) specific to each counterparty,\nwhich depends on the credit quality of the entity at stake, transforming the\neffective credit rating of a given netting set to AAA, regardless of the credit\nrating of the counterparty. By transforming CVA requirement into IM ones, as\nproposed in this paper, an institution could rely on the existing mechanisms\nfor posting and calling of IM, hence ensuring the operational viability of this\nnew form of managing warehoused risk. The main difference with the currently\nstandard framework is the creation of a Specific Initial Margin, that depends\nin the credit rating of the counterparty and the characteristics of the netting\nset in question. In this paper we propose a methodology for such transformation\nin a sound manner, and hence this method overcomes some of the limitations of\nthe CVA framework.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09407v1"
    },
    {
        "title": "Pro-Cyclicality of Traditional Risk Measurements: Quantifying and\n  Highlighting Factors at its Source",
        "authors": [
            "Marcel Bräutigam",
            "Michel Dacorogna",
            "Marie Kratz"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Since the introduction of risk-based solvency regulation, pro-cyclicality has\nbeen a subject of concerns from all market participants. Here, we lay down a\nmethodology to evaluate the amount of pro-cyclicality in the way finnancial\ninstitutions measure risk, and identify factors explaining this pro-cyclical\nbehavior. We introduce a new indicator based on the Sample Quantile Process\n(SQP, a dynamic generalization of Value-at-Risk), conditioned on realized\nvolatility to quantify the pro-cyclicality, and evaluate its amount in the\nmarkets, considering 11 stock indices as realizations of the SQP. Then we\ndetermine two main factors explaining the pro-cyclicality: the clustering and\nreturn-to-the-mean of volatility, as it could have been anticipated but not\nquantified before, and, more surprisingly, the very way risk is measured,\nindependently of this return-to-the-mean effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.03969v2"
    },
    {
        "title": "Optimal FX Hedge Tenor with Liquidity Risk",
        "authors": [
            "Rongju Zhang",
            "Mark Aarons",
            "Gregoire Loeper"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We develop an optimal currency hedging strategy for fund managers who own\nforeign assets to choose the hedge tenors that maximize their FX carry returns\nwithin a liquidity risk constraint. The strategy assumes that the offshore\nassets are fully hedged with FX forwards. The chosen liquidity risk metric is\nCash Flow at Risk (CFaR). The strategy involves time-dispersing the total\nnominal hedge value into future time buckets to maximize (minimize) the\nexpected FX carry benefit (cost), given the constraint that the CFaRs in all\nthe future time buckets do not breach a predetermined liquidity budget. We\ndemonstrate the methodology via an illustrative example where shorter-dated\nforwards are assumed to deliver higher carry trade returns (motivated by the\nhistorical experience where AUD is the domestic currency and USD is the foreign\ncurrency). We also introduce a tenor-ranking method which is useful when this\nassumption fails. We show by Monte Carlo simulation and by backtesting that our\nhedging strategy successfully operates within the liquidity budget. We provide\npractical insights on when and why fund managers should choose short-dated or\nlong-dated tenors.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.06346v1"
    },
    {
        "title": "Generalized Autoregressive Score asymmetric Laplace Distribution and\n  Extreme Downward Risk Prediction",
        "authors": [
            "Hong Shaopeng"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Due to the skessed distribution, high peak and thick tail and asymmetry of\nfinancial return data, it is difficult to describe the traditional\ndistribution. In recent years, generalized autoregressive score (GAS) has been\nused in many fields and achieved good results. In this paper, under the\nframework of generalized autoregressive score (GAS), the asymmetric Laplace\ndistribution (ALD) is improved, and the GAS-ALD model is proposed, which has\nthe characteristics of time-varying parameters, can describe the peak thick\ntail, biased and asymmetric distribution. The model is used to study the\nShanghai index, Shenzhen index and SME board index. It is found that: 1) the\ndistribution parameters and moments of the three indexes have obvious\ntime-varying characteristics and aggregation characteristics. 2) Compared with\nthe commonly used models for calculating VaR and ES, the GAS-ALD model has a\nhigh prediction effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.01277v3"
    },
    {
        "title": "Evidence of Predicting Early Signs of Corporate Bankruptcy Using\n  Financial Ratios in the Indian Landscape",
        "authors": [
            "Adit Chopra",
            "Abhi Bansal",
            "Aryaman Wadhwa"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Corporate bankruptcy impacts the functioning of the economy as it impacts its\nvarious stakeholders: Shareholders, financial and operational lenders, and the\ngovernment. This paper aims to study the impact of a wide array of\nprofitability, leverage and efficiency ratios to predict early signs of\nbankruptcy in public listed companies in India using a logistic regression\nconsidering impacts at two levels: one year and two years before the filing of\nbankruptcy with the NCLT during the year 2019. The study proves that the\naccuracies of the classification model are 81.4% and 85.1% respectively for one\nyear and two years before the bankruptcy.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04782v1"
    },
    {
        "title": "Variance Contracts",
        "authors": [
            "Yichun Chi",
            "Xun Yu Zhou",
            "Sheng Chao Zhuang"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We study the design of an optimal insurance contract in which the insured\nmaximizes her expected utility and the insurer limits the variance of his risk\nexposure while maintaining the principle of indemnity and charging the premium\naccording to the expected value principle. We derive the optimal policy\nsemi-analytically, which is coinsurance above a deductible when the variance\nbound is binding. This policy automatically satisfies the incentive-compatible\ncondition, which is crucial to rule out ex post moral hazard. We also find that\nthe deductible is absent if and only if the contract pricing is actuarially\nfair. Focusing on the actuarially fair case, we carry out comparative statics\non the effects of the insured's initial wealth and the variance bound on\ninsurance demand. Our results indicate that the expected coverage is always\nlarger for a wealthier insured, implying that the underlying insurance is a\nnormal good, which supports certain recent empirical findings. Moreover, as the\nvariance constraint tightens, the insured who is prudent cedes less losses,\nwhile the insurer is exposed to less tail risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.07103v1"
    },
    {
        "title": "Pricing and Capital Allocation for Multiline Insurance Firms With Finite\n  Assets in an Imperfect Market",
        "authors": [
            "John A. Major",
            "Stephen J. Mildenhall"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We analyze multiline pricing and capital allocation in equilibrium\nno-arbitrage markets. Existing theories often assume a perfect complete market,\nbut when pricing is linear, there is no diversification benefit from risk\npooling and therefore no role for insurance companies. Instead of a perfect\nmarket, we assume a non-additive distortion pricing functional and the\nprinciple of equal priority of payments in default. Under these assumptions, we\nderive a canonical allocation of premium and margin, with properties that merit\nthe name the natural allocation. The natural allocation gives non-negative\nmargins to all independent lines for default-free insurance but can exhibit\nnegative margins for low-risk lines under limited liability. We introduce novel\nconditional expectation measures of relative risk within a portfolio and use\nthem to derive simple, intuitively appealing expressions for risk margins and\ncapital allocations. We give a unique capital allocation consistent with our\nlaw invariant pricing functional. Such allocations produce returns that vary by\nline, in contrast to many other approaches. Our model provides a bridge between\nthe theoretical perspective that there should be no compensation for bearing\ndiversifiable risk and the empirical observation that more risky lines fetch\nhigher margins relative to subjective expected values.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.12427v1"
    },
    {
        "title": "The effect of the number of states on the validity of credit ratings",
        "authors": [
            "P. Lencastre",
            "F. Raischel",
            "P. G. Lind"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We explicitly test if the reliability of credit ratings depends on the total\nnumber of admissible states. We analyse open access credit rating data and show\nthat the effect of the number of states in the dynamical properties of ratings\nchange with time, thus giving supportive evidence that the ideal number of\nadmissible states changes with time. We use matrix estimation methods that\nexplicitly assume the hypothesis needed for the process to be a valid rating\nprocess. By comparing with the likelihood maximization method of matrix\nestimation, we quantify the \"likelihood-loss\" of assuming that the process is a\nwell grounded rating process.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2661v1"
    },
    {
        "title": "The partial damage loss cover ratemaking of the automobile insurance\n  using generalized linear models",
        "authors": [
            "William Guevara-Alarcón",
            "Luz Mery González",
            "Armando Antonio Zarruk"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  It is illustrated a methodology to compute the pure premium for the\nautomobile insurance (claim frequency and severity) using generalized linear\nmodels. It is obtained the pure premium for the partial damage loss cover (PPD)\nusing a set of automobile insurance policies with an exposition of a year. It\nis found that the most influential variables in the claim frequency are the car\nproduction year, the insured's age, and the region's subscription policy and\nthe most influential variables in the claim severity are the car's value, type\nand make and the insured's gender.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.03391v1"
    },
    {
        "title": "Portfolio Risk Assessment using Copula Models",
        "authors": [
            "Mikhail Semenov",
            "Daulet Smagulov"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  In the paper, we use and investigate copulas models to represent multivariate\ndependence in financial time series. We propose the algorithm of risk measure\ncomputation using copula models. Using the optimal mean-$CVaR$ portfolio we\ncompute portfolio's Profit and Loss series and corresponded risk measures\ncurves. Value-at-risk and Conditional-Value-at-risk curves were simulated by\nthree copula models: full Gaussian, Student's $t$ and regular vine copula.\nThese risk curves are lower than historical values of the risk measures curve.\nAll three models have superior prediction ability than a usual empirical\nmethod. Further directions of research are described.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.03516v1"
    },
    {
        "title": "Modeling Financial System with Interbank Flows, Borrowing, and Investing",
        "authors": [
            "Aditya Maheshwari",
            "Andrey Sarantsev"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  In our model, private actors with interbank cash flows similar to, but nore\ngeneral than (Carmona, Fouque, Sun, 2013) borrow from the outside economy at a\ncertain interest rate, controlled by the central bank, and invest in risky\nassets. Each private actor aims to maximize its expected terminal logarithmic\nutility. The central bank, in turn, aims to control the overall economy by\nmeans of an exponential utility function. We solve all stochastic optimal\ncontrol problems explicitly. We are able to recreate occasions such as\nliquidity trap. We study distribution of the number of defaults (net worth of a\nprivate actor going below a certain threshold).\n",
        "pdf_link": "http://arxiv.org/pdf/1707.03542v4"
    },
    {
        "title": "Bayesian Realized-GARCH Models for Financial Tail Risk Forecasting\n  Incorporating Two-sided Weibull Distribution",
        "authors": [
            "Chao Wang",
            "Qian Chen",
            "Richard Gerlach"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  The realized GARCH framework is extended to incorporate the two-sided Weibull\ndistribution, for the purpose of volatility and tail risk forecasting in a\nfinancial time series. Further, the realized range, as a competitor for\nrealized variance or daily returns, is employed in the realized GARCH\nframework. Further, sub-sampling and scaling methods are applied to both the\nrealized range and realized variance, to help deal with inherent\nmicro-structure noise and inefficiency. An adaptive Bayesian Markov Chain Monte\nCarlo method is developed and employed for estimation and forecasting, whose\nproperties are assessed and compared with maximum likelihood, via a simulation\nstudy. Compared to a range of well-known parametric GARCH, GARCH with two-sided\nWeibull distribution and realized GARCH models, tail risk forecasting results\nacross 7 market index return series and 2 individual assets clearly favor the\nrealized GARCH models incorporating two-sided Weibull distribution, especially\nmodels employing the sub-sampled realized variance and sub-sampled realized\nrange, over a six year period that includes the global financial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.03715v1"
    },
    {
        "title": "Machine learning application in online lending risk prediction",
        "authors": [
            "Xiaojiao Yu"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Online leading has disrupted the traditional consumer banking sector with\nmore effective loan processing. Risk prediction and monitoring is critical for\nthe success of the business model. Traditional credit score models fall short\nin applying big data technology in building risk model. In this manuscript,\ndata with various format and size were collected from public website,\nthird-parties and assembled with client's loan application information data.\nEnsemble machine learning models, random forest model and XGBoost model, were\nbuilt and trained with the historical transaction data and subsequently tested\nwith separate data. XGBoost model shows higher K-S value, suggesting better\nclassification capability in this task. Top 10 important features from the two\nmodels suggest external data such as zhimaScore, multi-platform stacking loans\ninformation, and social network information are important factors in predicting\nloan default probability.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.04831v1"
    },
    {
        "title": "On a robust risk measurement approach for capital determination errors\n  minimization",
        "authors": [
            "Marcelo Brutti Righi",
            "Fernanda Maria Müller",
            "Marlon Ruoso Moresco"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We propose a robust risk measurement approach that minimizes the expectation\nof overestimation plus underestimation costs. We consider uncertainty by taking\nthe supremum over a collection of probability measures, relating our approach\nto dual sets in the representation of coherent risk measures. We provide\nresults that guarantee the existence of a solution and explore the properties\nof minimizer and minimum as risk and deviation measures, respectively. An\nempirical illustration is carried out to demonstrate the use of our approach in\ncapital determination.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09829v4"
    },
    {
        "title": "An internal fraud model for operational losses in retail banking",
        "authors": [
            "Rocío Paredes",
            "Marco Vega"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  This paper develops a dynamic internal fraud model for operational losses in\nretail banking. It considers public operational losses arising from internal\nfraud in retail banking within a group of international banks. Additionally,\nthe model takes into account internal factors such as the ethical quality of\nworkers and the risk controls set by bank managers. The model is validated by\nmeasuring the impact of macroeconomic indicators such as GDP growth and the\ncorruption perception upon the severity and frequency of losses implied by the\nmodel. In general,results show that internal fraud losses are pro-cyclical, and\nthat country specific corruption perceptions positively affects internal fraud\nlosses. Namely, when a country is perceived to be more corrupt, retail banking\nin that country will feature more severe internal fraud losses.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.03235v1"
    },
    {
        "title": "Mathematical Foundations of Regression Methods for the approximation of\n  the Forward Initial Margin",
        "authors": [
            "Lucia Cipolina Kun",
            "Simone Caenazzo",
            "Ksenia Ponomareva"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Abundant literature has been published on approximation methods for the\nforward initial margin. The most popular ones being the family of regression\nmethods. This paper describes the mathematical foundations on which these\nregression approximation methods lie. We introduce mathematical rigor to show\nthat in essence, all the methods propose variations of approximations for the\nconditional expectation function, which is interpreted as an orthogonal\nprojection on Hilbert spaces. We show that each method is simply choosing a\ndifferent functional form to numerically estimate the conditional expectation.\nWe cover in particular the most popular methods in the literature so far,\nPolynomial approximation, Kernel regressions and Neural Networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04563v3"
    },
    {
        "title": "Sharing of longevity basis risk in pension schemes with income-drawdown\n  guarantees",
        "authors": [
            "Ankush Agarwal",
            "Christian-Oliver Ewald",
            "Yongjie Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  This work studies a stochastic optimal control problem for a pension scheme\nwhich provides an income-drawdown policy to its members after their retirement.\nTo manage the scheme efficiently, the manager and members agree to share the\ninvestment risk based on a pre-decided risk-sharing rule. The objective is to\nmaximise both sides' utilities by controlling the manager's investment in risky\nassets and members' benefit withdrawals. We use stochastic affine class models\nto describe the force of mortality of the members' population and consider a\nlongevity bond whose coupon payment is linked to a survival index. In our\nframework, we also investigate the longevity basis risk, which arises when the\nmembers' and the longevity bond's reference populations show different\nmortality behaviours. By applying the dynamic programming principle to solve\nthe corresponding HJB equations, we derive optimal solutions for the single-\nand sub-population cases. Our numerical results show that by sharing the risk,\nboth manager and members increase their utility. Moreover, even in the presence\nof longevity basis risk, we demonstrate that the longevity bond acts as an\neffective hedging instrument.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05232v1"
    },
    {
        "title": "Sector connectedness in the Chinese stock markets",
        "authors": [
            "Ying-Ying Shen",
            "Zhi-Qiang Jiang",
            "Jun-Chao Ma",
            "Gang-Jin Wang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Uncovering the risk transmitting path within economic sectors in China is\ncrucial for understanding the stability of the Chinese economic system,\nespecially under the current situation of the China-US trade conflicts. In this\npaper, we try to uncover the risk spreading channels by means of volatility\nspillovers within the Chinese sectors using stock market data. By applying the\ngeneralized variance decomposition framework based on the VAR model and the\nrolling window approach, a set of connectedness matrices is obtained to reveal\nthe overall and dynamic spillovers within sectors. It is found that 17 sectors\n(mechanical equipment, electrical equipment, utilities, and so on) are risk\ntransmitters and 11 sectors (national defence, bank, non-bank finance, and so\non) are risk takers during the whole period. During the periods with the\nextreme risk events (the global financial crisis, the Chinese interbank\nliquidity crisis, the Chinese stock market plunge, and the China-US trade war),\nwe observe that the connectedness measures significantly increase and the\nfinancial sectors play a buffer role in stabilizing the economic system. The\nrobust tests suggest that our results are not sensitive to the changes of model\nparameters. Our results not only uncover the spillover effects within the\nChinese sectors, but also highlight the deep understanding of the risk\ncontagion patterns in the Chinese stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09097v1"
    },
    {
        "title": "Assessing Financial Model Risk",
        "authors": [
            "Pauline Barrieu",
            "Giacomo Scandolo"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Model risk has a huge impact on any risk measurement procedure and its\nquantification is therefore a crucial step. In this paper, we introduce three\nquantitative measures of model risk when choosing a particular reference model\nwithin a given class: the absolute measure of model risk, the relative measure\nof model risk and the local measure of model risk. Each of the measures has a\nspecific purpose and so allows for flexibility. We illustrate the various\nnotions by studying some relevant examples, so as to emphasize the\npracticability and tractability of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.0684v2"
    },
    {
        "title": "Contraction or steady state? An analysis of credit risk management in\n  Italy in the period 2008-2012",
        "authors": [
            "Stefano Olgiati",
            "Alessandro Danovi"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Credit risk management in Italy is characterized, in the period June 2008 to\nJune 2012, by frequent (frequency=0.5 cycles per year) and intense (peak\namplitude: mean=39.2 billion Euros, s.e.=2.83 billion Euros) quarterly\ncontractions and expansions around the mean (915.4 billion Euros, s.e.=3.59\nbillion Euros) of the nominal total credit used by non-financial corporations.\nSuch frequent and intense fluctuations are frequently ascribed to exogenous\nBasel II procyclical effects on credit flow into the economy and, consequently,\nBasel III output based point in time Credit to GDP countercyclical buffering\nadvocated. We have tested the opposite null hypotheses that such variation is\nsignificantly correlated to actual default rates, and that such correlation is\nexplained by fluctuations of credit supply around a steady state. We have found\nthat, in the period June 2008 to June 2012 (n=17), linear regression of credit\ngrowth rates on default rates reveals a negative correlation of r=minus 0.6903\nwith R squared=0.4765, and that credit supply fluctuates steadily around the\ndefault rate with an Internal Steady State Parameter SSP=0.00245 with chi\nsquared=37.47 (v=16, P<.005). We conclude that fluctuations of the total credit\nused by non-financial corporations are exhaustively explained by variation of\nthe independent variable default rate, and that credit variation fluctuates\naround a steady state. We conclude that credit risk management in Italy has\nbeen effective in parameterizing credit supply variation to default rates\nwithin the Basel II operating framework. Basel III prospective countercyclical\npoint in time output buffers based on filtered Credit to GDP ratios and dynamic\nprovisioning proposals should take into account this underlying steady state\nstatistical pattern.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2465v1"
    },
    {
        "title": "Financial Contagion and Asset Liquidation Strategies",
        "authors": [
            "Zachary Feinstein"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  This paper provides a framework for modeling the financial system with\nmultiple illiquid assets during a crisis. This work generalizes the paper by\nAmini, Filipovic and Minca (2016) by allowing for differing liquidation\nstrategies. The main result is a proof of sufficient conditions for the\nexistence of an equilibrium liquidation strategy with corresponding unique\nclearing payments and liquidation prices. An algorithm for computing the\nmaximal clearing payments and prices is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.00937v2"
    },
    {
        "title": "Copula based hierarchical risk aggregation - Tree dependent sampling and\n  the space of mild tree dependence",
        "authors": [
            "Fabio Derendinger"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  The ability to adequately model risks is crucial for insurance companies. The\nmethod of \"Copula-based hierarchical risk aggregation\" by Arbenz et al. offers\na flexible way in doing so and has attracted much attention recently. We\nbriefly introduce the aggregation tree model as well as the sampling algorithm\nproposed by they authors.\n  An important characteristic of the model is that the joint distribution of\nall risk is not fully specified unless an additional assumption (known as\n\"conditional independence assumption\") is added. We show that there is\nnumerical evidence that the sampling algorithm yields an approximation of the\ndistribution uniquely specified by the conditional independence assumption. We\npropose a modified algorithm and provide a proof that under certain conditions\nthe said distribution is indeed approximated by our algorithm.\n  We further determine the space of feasible distributions for a given\naggregation tree model in case we drop the conditional independence assumption.\nWe study the impact of the input parameters and the tree structure, which\nallows conclusions of the way the aggregation tree should be designed.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03564v2"
    },
    {
        "title": "On the Fractal Geometry of the Balance Sheet and the Fractal Index of\n  Insolvency Risk",
        "authors": [
            "A. K. M. Azhar",
            "Vincent B. Y. Gan",
            "W. A. T. Wan Abdullah",
            "H. Zainuddin"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  This paper reviews the economic and theoretical foundations of insolvency\nrisk measurement and capital adequacy rules. The proposed new measure of\ninsolvency risk is constructed by disentangling assets, debt and equity at the\nmicro-prudential firm level. This new risk index is the Firm Insolvency Risk\nIndex (FIRI) which is symmetrical, proportional and scale invariant. We\ndemonstrate that the balance sheet can be shown to evolve with a fractal\npattern. As such we construct a fractal index that can measure the risk of\nassets. This index can differentiate between the similarity and dissimilarity\nin asset risk, and it will also possess the properties of being self-similar\nand invariant to firm characteristics that make up its asset composition hence\ninvariant to all types of risk derived from assets. Self-similarity and scale\ninvariance across the cross section allows direct comparison of degrees of risk\nin assets. This is by comparing the risk dissimilarity of assets. Being\nnaturally bounded to its highest upper bound, (0,2], the fractal index is able\nto serve like a risk thermometer. We assign geometric probabilities of\ninsolvency P (equity is equal or less than 0 conditional on debt being greater\nthan 0).\n",
        "pdf_link": "http://arxiv.org/pdf/1512.09280v1"
    },
    {
        "title": "Asset correlation estimation for inhomogeneous exposure pools",
        "authors": [
            "Christoph Wunderer"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  A possible data source for the estimation of asset correlations is default\ntime series. This study investigates the systematic error that is made if the\nexposure pool underlying a default time series is assumed to be homogeneous\nwhen in reality it is not. We find that the asset correlation will always be\nunderestimated if homogeneity with respect to the probability of default (PD)\nis wrongly assumed, and the error is the larger the more spread out the PD is\nwithin the exposure pool. If the exposure pool is inhomogeneous with respect to\nthe asset correlation itself then the error may be going in both directions,\nbut for most PD- and asset correlation ranges relevant in practice the asset\ncorrelation is systematically underestimated. Both effects stack up and the\nerror tends to become even larger if in addition a negative correlation between\nasset correlation and PD is assumed, which is plausible in many circumstances\nand consistent with the Basel RWA formula. It is argued that the generic\ninhomogeneity effect described is one of the reasons why asset correlations\nmeasured from default data tend to be lower than asset correlations derived\nfrom asset value data.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.02028v2"
    },
    {
        "title": "Fatou Property, representations, and extensions of law-invariant risk\n  measures on general Orlicz spaces",
        "authors": [
            "Niushan Gao",
            "Denny H. Leung",
            "Cosimo Munari",
            "Foivos Xanthos"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We provide a variety of results for (quasi)convex, law-invariant functionals\ndefined on a general Orlicz space, which extend well-known results in the\nsetting of bounded random variables. First, we show that Delbaen's\nrepresentation of convex functionals with the Fatou property, which fails in a\ngeneral Orlicz space, can be always achieved under the assumption of\nlaw-invariance. Second, we identify the range of Orlicz spaces where the\ncharacterization of the Fatou property in terms of norm lower semicontinuity by\nJouini, Schachermayer and Touzi continues to hold. Third, we extend Kusuoka's\nrepresentation to a general Orlicz space. Finally, we prove a version of the\nextension result by Filipovi\\'{c} and Svindland by replacing norm lower\nsemicontinuity with the (generally non-equivalent) Fatou property. Our results\nhave natural applications to the theory of risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.05967v2"
    },
    {
        "title": "Obligations with Physical Delivery in a Multi-Layered Financial Network",
        "authors": [
            "Zachary Feinstein"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  This paper provides a general framework for modeling financial contagion in a\nsystem with obligations in multiple illiquid assets (e.g., currencies). In so\ndoing, we develop a multi-layered financial network that extends the single\nnetwork of Eisenberg and Noe (2001). In particular, we develop a financial\ncontagion model with fire sales that allows institutions to both buy and sell\nassets to cover their liabilities in the different assets and act as utility\nmaximizers.\n  We prove that, under standard assumptions and without market impacts,\nequilibrium portfolio holdings exist and are unique. However, with market\nimpacts, we prove that equilibrium portfolio holdings and market prices exist\nwhich clear the multi-layered financial system. In general, though, these\nclearing solutions are not unique. We extend this result by considering the\nt\\^atonnement process to find the unique attained equilibrium. The attained\nequilibrium need not be continuous with respect to the initial shock; these\npoints of discontinuity match those stresses in which a financial crisis\nbecomes a systemic crisis. We further provide mathematical formulations for\npayment rules and utility functions satisfying the necessary conditions for\nthese existence and uniqueness results.\n  We demonstrate the value of our model through illustrative numerical case\nstudies. In particular, we study a counterfactual scenario on the event that\nGreece re-instituted the drachma on a dataset from the European Banking\nAuthority.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.07936v4"
    },
    {
        "title": "Reverse stress testing interbank networks",
        "authors": [
            "Daniel Grigat",
            "Fabio Caccioli"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We reverse engineer dynamics of financial contagion to find the scenario of\nsmallest exogenous shock that, should it occur, would lead to a given final\nsystemic loss. This reverse stress test can be used to identify the potential\ntriggers of systemic events, and it removes the arbitrariness in the selection\nof shock scenarios in stress testing. We consider in particular the case of\ndistress propagation in an interbank market, and we study a network of 44\nEuropean banks, which we reconstruct using data collected from Bloomberg. By\nlooking at the distribution across banks of the size of smallest exogenous\nshocks we rank banks in terms of their systemic importance, and we show the\neffectiveness of a policy with capital requirements based on this ranking. We\nalso study the properties of smallest exogenous shocks as a function of the\nlargest eigenvalue $\\lambda_{\\rm max}$ of the matrix of interbank leverages,\nwhich determines the endogenous amplification of shocks. We find that the size\nof smallest exogenous shocks reduces and that the distribution across banks\nbecomes more localized as $\\lambda_{\\rm max}$ increases.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.08744v2"
    },
    {
        "title": "Solvency II, or How to Sweep the Downside Risk Under the Carpet",
        "authors": [
            "Stefan Weber"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Under Solvency II the computation of capital requirements is based on value\nat risk (V@R). V@R is a quantile-based risk measure and neglects extreme risks\nin the tail. V@R belongs to the family of distortion risk measures. A serious\ndeficiency of V@R is that firms can hide their total downside risk in corporate\nnetworks, unless a consolidated solvency balance sheet is required for each\neconomic scenario. In this case, they can largely reduce their total capital\nrequirements via appropriate transfer agreements within a network structure\nconsisting of sufficiently many entities and thereby circumvent capital\nregulation. We prove several versions of such a result for general distortion\nrisk measures of V@R-type, explicitly construct suitable allocations of the\nnetwork portfolio, and finally demonstrate how these findings can be extended\nbeyond distortion risk measures. We also discuss why consolidation requirements\ncannot completely eliminate this problem. Capital regulation should thus be\nbased on coherent or convex risk measures like average value at risk or\nexpectiles.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.08901v2"
    },
    {
        "title": "Counterparty Trading Limits Revisited:CSAs, IM, SwapAgent(r), from PFE\n  to PFL",
        "authors": [
            "Chris Kenyon",
            "Mourad Berrahoui",
            "Benjamin Poncet"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  The utility of Potential Future Exposure (PFE) for counterparty trading\nlimits is being challenged by new market developments, notably widespread\nregulatory Initial Margin (using 99% 10-day exposure), and netting of trade and\ncollateral flows. However PFE has pre-existing challenges w.r.t.\nportfolios/distributions, collateralization, netting set seniority, and\noverlaps with CVA. We introduce Potential Future Loss (PFL) which combines\nexpected shortfall (ES) and loss given default (LGD) as a replacement for PFE.\nWith two additional variants Adjusted PFL (aPFL) and Protected Adjusted PFL\n(paPFL) these deal with both new and pre-existing challenges. We provide a\ntheoretical background and numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.03161v2"
    },
    {
        "title": "Large deviations for risk measures in finite mixture models",
        "authors": [
            "Valeria Bignozzi",
            "Claudio Macci",
            "Lea Petrella"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Due to their heterogeneity, insurance risks can be properly described as a\nmixture of different fixed models, where the weights assigned to each model may\nbe estimated empirically from a sample of available data. If a risk measure is\nevaluated on the estimated mixture instead of the (unknown) true one, then it\nis important to investigate the committed error. In this paper we study the\nasymptotic behaviour of estimated risk measures, as the data sample size tends\nto infinity, in the fashion of large deviations. We obtain large deviation\nresults by applying the contraction principle, and the rate functions are given\nby a suitable variational formula; explicit expressions are available for\nmixtures of two models. Finally, our results are applied to the most common\nrisk measures, namely the quantiles, the Expected Shortfall and the shortfall\nrisk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.03252v2"
    },
    {
        "title": "Sequential Design and Spatial Modeling for Portfolio Tail Risk\n  Measurement",
        "authors": [
            "Michael Ludkovski",
            "James Risk"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We consider calculation of capital requirements when the underlying economic\nscenarios are determined by simulatable risk factors. In the respective nested\nsimulation framework, the goal is to estimate portfolio tail risk, quantified\nvia VaR or TVaR of a given collection of future economic scenarios representing\nfactor levels at the risk horizon. Traditionally, evaluating portfolio losses\nof an outer scenario is done by computing a conditional expectation via\ninner-level Monte Carlo and is computationally expensive. We introduce several\ninter-related machine learning techniques to speed up this computation, in\nparticular by properly accounting for the simulation noise. Our main workhorse\nis an advanced Gaussian Process (GP) regression approach which uses\nnonparametric spatial modeling to efficiently learn the relationship between\nthe stochastic factors defining scenarios and corresponding portfolio value.\nLeveraging this emulator, we develop sequential algorithms that adaptively\nallocate inner simulation budgets to target the quantile region. The GP\nframework also yields better uncertainty quantification for the resulting\nVaR/TVaR estimators that reduces bias and variance compared to existing\nmethods. We illustrate the proposed strategies with two case-studies in two and\nsix dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.05204v2"
    },
    {
        "title": "Network models of financial systemic risk: A review",
        "authors": [
            "Fabio Caccioli",
            "Paolo Barucca",
            "Teruyoshi Kobayashi"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  The global financial system can be represented as a large complex network in\nwhich banks, hedge funds and other financial institutions are interconnected to\neach other through visible and invisible financial linkages. Recently, a lot of\nattention has been paid to the understanding of the mechanisms that can lead to\na breakdown of this network. This can happen when the existing financial links\nturn from being a means of risk diversification to channels for the propagation\nof risk across financial institutions. In this review article, we summarize\nrecent developments in the modeling of financial systemic risk. We focus in\nparticular on network approaches, such as models of default cascades due to\nbilateral exposures or to overlapping portfolios, and we also report on recent\nfindings on the empirical structure of interbank networks. The current review\nprovides a landscape of the newly arising interdisciplinary field lying at the\nintersection of several disciplines, such as network science, physics,\nengineering, economics, and ecology.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.11512v1"
    },
    {
        "title": "Variants of the Smith-Wilson method with a view towards applications",
        "authors": [
            "Thomas Viehmann"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We propose two variants of the Smith-Wilson method for practical application\nin the insurance industry. Our first variant relaxes the Smith-Wilson energy\nand can be used to incorporate less reliable market data with a certain weight\nrather than disregarding it completely. This is particularly useful for\nderiving yield curves in the IFRS 17 accounting regime, where there is a\nmandate to incorporate all available market data.\n  A second variant incorporates the requirement to reach the ultimate forward\nrate at a prescribed term into the problem formulation. This provides a natural\nway to fulfil the Solvency II convergence requirement and is more elegant than\nthe current methodology adapting the term-scale parameter to control\nconvergence.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.06363v1"
    },
    {
        "title": "When Risks and Uncertainties Collide: Mathematical Finance for Arbitrage\n  Markets in a Quantum Mechanical View",
        "authors": [
            "Simone Farinelli",
            "Hideyuki Takada"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Geometric arbitrage theory reformulates a generic asset model possibly\nallowing for arbitrage by packaging all asset and their forward dynamics into a\nstochastic principal fibre bundle, with a connection whose parallel transport\nencodes discounting and portfolio rebalancing, and whose curvature measures, in\nthis geometric language, the instantaneous arbitrage capability generated by\nthe market itself. The asset and market portfolio dynamics have a quantum\nmechanical description, which is constructed by quantizing the deterministic\nversion of the stochastic Lagrangian system describing a market allowing for\narbitrage. Results, obtained by solving the Schroedinger equation, coincide\nwith those obtained by solving the stochastic Euler Lagrange equations derived\nby a variational principle and providing therefore consistency.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07164v3"
    },
    {
        "title": "Relative Bound and Asymptotic Comparison of Expectile with Respect to\n  Expected Shortfall",
        "authors": [
            "Samuel Drapeau",
            "Mekonnen Tadese"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Expectile bears some interesting properties in comparison to the industry\nwide expected shortfall in terms of assessment of tail risk. We study the\nrelationship between expectile and expected shortfall using duality results and\nthe link to optimized certainty equivalent. Lower and upper bounds of expectile\nare derived in terms of expected shortfall as well as a characterization of\nexpectile in terms of expected shortfall. Further, we study the asymptotic\nbehavior of expectile with respect to expected shortfall as the confidence\nlevel goes to $1$ in terms of extreme value distributions. We use concentration\ninequalities to illustrate that the estimation of value at risk requires larger\nsample size than expected shortfall and expectile for heavy tail distributions\nwhen $\\alpha$ is close to $1$. Illustrating the formulation of expectile in\nterms of expected shortfall, we also provide explicit or semi-explicit\nexpressions of expectile and some simulation results for some classical\ndistributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09729v3"
    },
    {
        "title": "Post-Last Exit Time Process and its Application to Loss-Given-Default\n  Distribution",
        "authors": [
            "Masahiko Egami",
            "Rusudan Kevkhishvili"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We study a linear diffusion process after its last exit time from a certain\nregular point. Rather than treating the process as newly born at the last exit\ntime, we view the whole path and separate the original process before and after\nthe last exit time. This enables us not only to identify the transition\nsemigroup, boundary behavior, entrance law, and reverse of the post-last exit\ntime process, but also to establish a financial model for estimating the\nloss-given-default distribution of corporate debt (an all-time important open\nproblem).\n",
        "pdf_link": "http://arxiv.org/pdf/2009.00868v3"
    },
    {
        "title": "Dependent Conditional Value-at-Risk for Aggregate Risk Models",
        "authors": [
            "Bony Josaphat",
            "Khreshna Syuhada"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Risk measure forecast and model have been developed in order to not only\nprovide better forecast but also preserve its (empirical) property especially\ncoherent property. Whilst the widely used risk measure of Value-at-Risk (VaR)\nhas shown its performance and benefit in many applications, it is in fact not a\ncoherent risk measure. Conditional VaR (CoVaR), defined as mean of losses\nbeyond VaR, is one of alternative risk measures that satisfies coherent\nproperty. There has been several extensions of CoVaR such as Modified CoVaR\n(MCoVaR) and Copula CoVaR (CCoVaR). In this paper, we propose another risk\nmeasure, called Dependent CoVaR (DCoVaR), for a target loss that depends on\nanother random loss, including model parameter treated as random loss. It is\nfound that our DCoVaR outperforms than both MCoVaR and CCoVaR. Numerical\nsimulation is carried out to illustrate the proposed DCoVaR. In addition, we do\nan empirical study of financial returns data to compute the DCoVaR forecast for\nheteroscedastic process.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.02904v1"
    },
    {
        "title": "Simulation Methods for Robust Risk Assessment and the Distorted Mix\n  Approach",
        "authors": [
            "Sojung Kim",
            "Stefan Weber"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Uncertainty requires suitable techniques for risk assessment. Combining\nstochastic approximation and stochastic average approximation, we propose an\nefficient algorithm to compute the worst case average value at risk in the face\nof tail uncertainty. Dependence is modelled by the distorted mix method that\nflexibly assigns different copulas to different regions of multivariate\ndistributions. We illustrate the application of our approach in the context of\nfinancial markets and cyber risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03653v2"
    },
    {
        "title": "A Machine Learning Based Regulatory Risk Index for Cryptocurrencies",
        "authors": [
            "Xinwen Ni",
            "Wolfgang Karl Härdle",
            "Taojun Xie"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Cryptocurrencies' values often respond aggressively to major policy changes,\nbut none of the existing indices informs on the market risks associated with\nregulatory changes. In this paper, we quantify the risks originating from new\nregulations on FinTech and cryptocurrencies (CCs), and analyse their impact on\nmarket dynamics. Specifically, a Cryptocurrency Regulatory Risk IndeX (CRRIX)\nis constructed based on policy-related news coverage frequency. The unlabeled\nnews data are collected from the top online CC news platforms and further\nclassified using a Latent Dirichlet Allocation model and Hellinger distance.\nOur results show that the machine-learning-based CRRIX successfully captures\nmajor policy-changing moments. The movements for both the VCRIX, a market\nvolatility index, and the CRRIX are synchronous, meaning that the CRRIX could\nbe helpful for all participants in the cryptocurrency market. The algorithms\nand Python code are available for research purposes on www.quantlet.de.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.12121v3"
    },
    {
        "title": "An AI approach to measuring financial risk",
        "authors": [
            "Lining Yu",
            "Wolfgang Karl Härdle",
            "Lukas Borke",
            "Thijs Benschop"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  AI artificial intelligence brings about new quantitative techniques to assess\nthe state of an economy. Here we describe a new measure for systemic risk: the\nFinancial Risk Meter (FRM). This measure is based on the penalization parameter\n(lambda) of a linear quantile lasso regression. The FRM is calculated by taking\nthe average of the penalization parameters over the 100 largest US publicly\ntraded financial institutions. We demonstrate the suitability of this AI based\nrisk measure by comparing the proposed FRM to other measures for systemic risk,\nsuch as VIX, SRISK and Google Trends. We find that mutual Granger causality\nexists between the FRM and these measures, which indicates the validity of the\nFRM as a systemic risk measure. The implementation of this project is carried\nout using parallel computing, the codes are published on www.quantlet.de with\nkeyword FRM. The R package RiskAnalytics is another tool with the purpose of\nintegrating and facilitating the research, calculation and analysis methods\naround the FRM project. The visualization and the up-to-date FRM can be found\non hu.berlin/frm.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13222v1"
    },
    {
        "title": "A comparison of techniques for dynamic multivariate risk measures",
        "authors": [
            "Zachary Feinstein",
            "Birgit Rudloff"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  This paper contains an overview of results for dynamic multivariate risk\nmeasures. We provide the main results of four different approaches. We will\nprove under which assumptions results within these approaches coincide, and how\nproperties like primal and dual representation and time consistency in the\ndifferent approaches compare to each other.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.2151v3"
    },
    {
        "title": "Risk averse fractional trading using the current drawdown",
        "authors": [
            "Stanislaus Maier-Paape"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  In this paper the fractional trading ansatz of money management is\nreconsidered with special attention to chance and risk parts in the goal\nfunction of the related optimization problem. By changing the goal function\nwith due regards to other risk measures like current drawdowns, the optimal\nfraction solutions reflect the needs of risk averse investors better than the\noriginal optimal f solution of Ralph Vince.\n  Keywords: fractional trading, optimal f, current drawdown, terminal wealth\nrelative, risk aversion\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02985v1"
    },
    {
        "title": "Parameter uncertainty and reserve risk under Solvency II",
        "authors": [
            "Andreas Fröhlich",
            "Annegret Weng"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  In this article we consider the parameter risk in the context of internal\nmodelling of the reserve risk under Solvency II.\n  We discuss two opposed perspectives on parameter uncertainty and point out\nthat standard methods of classical reserving focusing on the estimation error\nof claims reserves are in general not appropriate to model the impact of\nparameter uncertainty upon the actual risk of economic losses from the\nundertakings's perspective.\n  Referring to the requirements of Solvency II we assess methods to model\nparameter uncertainty for the reserve risk by comparing the probability of\nsolvency actually attained when modelling the solvency risk capital requirement\nbased on the respective method to the required confidence level. Using the\nsimple example of a normal model we show that the bootstrapping approach is not\nappropriate to model parameter uncertainty according to this criterion. We then\npresent an adaptation of the approach proposed in \\cite {froehlich2014}.\nExperimental results demonstrate that this new method yields a risk capital\nmodel for the reserve risk achieving the required confidence level in good\napproximation.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.03066v2"
    },
    {
        "title": "The hierarchical generalized linear model and the bootstrap estimator of\n  the error of prediction of loss reserves in a non-life insurance company",
        "authors": [
            "Alicja Wolny-Dominiak"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  This paper presents the hierarchical generalized linear model (HGLM) for loss\nreserving in a non-life insurance company. Because in this case the error of\nprediction is expressed by a complex analytical formula, the error bootstrap\nestimator is proposed instead. Moreover, the bootstrap procedure is used to\nobtain full information about the error by applying quantiles of the absolute\nprediction error. The full R code is available on the Github\nhttps://github.com/woali/BootErrorLossReserveHGLM.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.04126v1"
    },
    {
        "title": "European banking supervision, the role of stress test. Some brief\n  considerations",
        "authors": [
            "Simone Manduchi"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  A quick review of European financial stability institutions and the role of\nstress tests in the current juridical system.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.05227v1"
    },
    {
        "title": "Optimal proportional reinsurance and investment for stochastic factor\n  models",
        "authors": [
            "Matteo Brachetta",
            "Claudia Ceci"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this work we investigate the optimal proportional reinsurance-investment\nstrategy of an insurance company which wishes to maximize the expected\nexponential utility of its terminal wealth in a finite time horizon. Our goal\nis to extend the classical Cramer-Lundberg model introducing a stochastic\nfactor which affects the intensity of the claims arrival process, described by\na Cox process, as well as the insurance and reinsurance premia. Using the\nclassical stochastic control approach based on the Hamilton-Jacobi-Bellman\nequation we characterize the optimal strategy and provide a verification result\nfor the value function via classical solutions of two backward partial\ndifferential equations. Existence and uniqueness of these solutions are\ndiscussed. Results under various premium calculation principles are illustrated\nand a new premium calculation rule is proposed in order to get more realistic\nstrategies and to better fit our stochastic factor model. Finally, numerical\nsimulations are performed to obtain sensitivity analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01223v1"
    },
    {
        "title": "Stochastic Deflator for an Economic Scenario Generator with Five Factors",
        "authors": [
            "Po-Keng Cheng",
            "Frédéric Planchet"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this paper, we implement a stochastic deflator with five economic and\nfinancial risk factors: interest rates, market price of risk, stock prices,\ndefault intensities, and convenience yields. We examine the deflator with\ndifferent financial assets, such as stocks, zero-coupon bonds, vanilla options,\nand corporate coupon bonds. We find required regularity conditions to implement\nour stochastic deflator. Our numerical results show the reliability of the\ndeflator approach in pricing financial derivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.02991v3"
    },
    {
        "title": "Multivariate risk measures in the non-convex setting",
        "authors": [
            "Andreas Haier",
            "Ilya Molchanov"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  The family of admissible positions in a transaction costs model is a random\nclosed set, which is convex in case of proportional transaction costs. However,\nthe convexity fails, e.g. in case of fixed transaction costs or when only a\nfinite number of transfers are possible. The paper presents an approach to\nmeasure risks of such positions based on the idea of considering all selections\nof the portfolio and checking if one of them is acceptable. Properties and\nbasic examples of risk measures of non-convex portfolios are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.00766v2"
    },
    {
        "title": "Modelling Extremal Dependence for Operational Risk by a Bipartite Graph",
        "authors": [
            "Oliver Kley",
            "Claudia Klüppelberg",
            "Sandra Paterlini"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We introduce a statistical model for operational losses based on heavy-tailed\ndistributions and bipartite graphs, which captures the event type and business\nline structure of operational risk data. The model explicitly takes into\naccount the Pareto tails of losses and the heterogeneous dependence structures\nbetween them. We then derive estimators for individual as well as aggregated\ntail risk, measured in terms of Value-at-Risk and Conditional-Tail-Expectation\nfor very high confidence levels, and provide also an asymptotically full\ncapital allocation method. Estimation methods for such tail risk measures and\ncapital allocations are also proposed and tested on simulated data. Finally, by\nhaving access to real-world operational risk losses from the Italian banking\nsystem, we show that even with a small number of observations, the proposed\nestimation methods produce reliable estimates, and that quantifying dependence\nby means of the empirical network has a big impact on estimates at both\nindividual and aggregate level, as well as for capital allocations.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.03041v1"
    },
    {
        "title": "Risk management with machine-learning-based algorithms",
        "authors": [
            "Simon Fécamp",
            "Joseph Mikael",
            "Xavier Warin"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We propose some machine-learning-based algorithms to solve hedging problems\nin incomplete markets. Sources of incompleteness cover illiquidity, untradable\nrisk factors, discrete hedging dates and transaction costs. The proposed\nalgorithms resulting strategies are compared to classical stochastic control\ntechniques on several payoffs using a variance criterion. One of the proposed\nalgorithm is flexible enough to be used with several existing risk criteria. We\nfurthermore propose a new moment-based risk criteria.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.05287v4"
    },
    {
        "title": "Risk Management with Tail Quasi-Linear Means",
        "authors": [
            "Nicole Bäuerle",
            "Tomer Shushi"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We generalize Quasi-Linear Means by restricting to the tail of the risk\ndistribution and show that this can be a useful quantity in risk management\nsince it comprises in its general form the Value at Risk, the Tail Value at\nRisk and the Entropic Risk Measure in a unified way. We then investigate the\nfundamental properties of the proposed measure and show its unique features and\nimplications in the risk measurement process. Furthermore, we derive formulas\nfor truncated elliptical models of losses and provide formulas for selected\nmembers of such models.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.06941v3"
    },
    {
        "title": "The ineffectiveness of coherent risk measures",
        "authors": [
            "John Armstrong",
            "Damiano Brigo"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We show that coherent risk measures are ineffective in curbing the behaviour\nof investors with limited liability or excessive tail-risk seeking behaviour if\nthe market admits statistical arbitrage opportunities which we term\n$\\rho$-arbitrage for a risk measure $\\rho$. We show how to determine\nanalytically whether such $\\rho$-arbitrage portfolios exist in complete markets\nand in the Markowitz model. We also consider realistic numerical examples of\nincomplete markets and determine whether expected shortfall constraints are\nineffective in these markets. We find that the answer depends heavily upon the\nprobability model selected by the risk manager but that it is certainly\npossible for expected shortfall constraints to be ineffective in realistic\nmarkets. Since value at risk constraints are weaker than expected shortfall\nconstraints, our results can be applied to value at risk. By contrast, we show\nthat reasonable expected utility constraints are effective in any\narbitrage-free market.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10015v2"
    },
    {
        "title": "Spectral risk measures and uncertainty",
        "authors": [
            "Mohammed Berkhouch",
            "Ghizlane Lakhnati",
            "Marcelo Brutti Righi"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Risk assessment under different possible scenarios is a source of uncertainty\nthat may lead to concerning financial losses. We address this issue, first, by\nadapting a robust framework to the class of spectral risk measures. Second, we\npropose a Deviation-based approach to quantify uncertainty. Furthermore, the\ntheory is illustrated with a practical case study from NASDAQ index.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.07716v1"
    },
    {
        "title": "Testing Sharpe ratio: luck or skill?",
        "authors": [
            "Eric Benhamou",
            "David Saltiel",
            "Beatrice Guez",
            "Nicolas Paris"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Sharpe ratio (sometimes also referred to as information ratio) is widely used\nin asset management to compare and benchmark funds and asset managers. It\ncomputes the ratio of the (excess) net return over the strategy standard\ndeviation. However, the elements to compute the Sharpe ratio, namely, the\nexpected returns and the volatilities are unknown numbers and need to be\nestimated statistically. This means that the Sharpe ratio used by funds is\nlikely to be error prone because of statistical estimation errors. In this\npaper, we provide various tests to measure the quality of the Sharpe ratios. By\nquality, we are aiming at measuring whether a manager was indeed lucky of\nskillful. The test assesses this through the statistical significance of the\nSharpe ratio. We not only look at the traditional Sharpe ratio but also compute\na modified Sharpe insensitive to used Capital. We provide various statistical\ntests that can be used to precisely quantify the fact that the Sharpe is\nstatistically significant. We illustrate in particular the number of trades for\na given Sharpe level that provides statistical significance as well as the\nimpact of auto-correlation by providing reference tables that provides the\nminimum required Sharpe ratio for a given time period and correlation. We also\nprovide for a Sharpe ratio of 0.5, 1.0, 1.5 and 2.0 the skill percentage given\nthe auto-correlation level.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08042v2"
    },
    {
        "title": "How big should a Stress Shock be?",
        "authors": [
            "David G Maher"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Stress shocks are often calculated as multiples of the standard deviation of\na history set. This paper investigates how many standard deviations are\nrequired to guarantee that this shock exceeds any observation within the\nhistory set, given the additional constraint of kurtosis. The results of this\nanalysis are then used to validate the shocks produced by some stress test\nmodels, in particular that of Brace-Lauer-Rado. A secondary application of our\nresults is to investigate three known extensions of Chebyshev's Inequality\nwhere the kurtosis is known. It is found that our results give a tighter bound\nthan the well-known inequalities.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.10164v1"
    },
    {
        "title": "An assets-liabilities dynamical model of banking system and systemic\n  risk governance",
        "authors": [
            "Lorella Fatone",
            "Francesca Mariani"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We consider the problem of governing systemic risk in an assets-liabilities\ndynamical model of banking system. In the model considered each bank is\nrepresented by its assets and its liabilities.The capital reserves of a bank\nare the difference between assets and liabilities of the bank. A bank is\nsolvent when its capital reserves are greater or equal to zero otherwise the\nbank is failed.The banking system dynamics is defined by an initial value\nproblem for a system of stochastic differential equations whose independent\nvariable is time and whose dependent variables are the assets and the\nliabilities of the banks.The banking system model presented generalizes those\ndiscussed in [4],[3] and describes a homogeneous population of banks. The main\nfeatures of the model are a cooperation mechanism among banks and the\npossibility of the (direct) intervention of the monetary authority in the\nbanking system dynamics. We call systemic risk or systemic event in a bounded\ntime interval the fact that in that time interval at least a given fraction of\nthe banks fails. The probability of systemic risk in a bounded time interval is\nevaluated using statistical simulation. The systemic risk governance pursues\nthe goal of keeping the probability of systemic risk in a bounded time interval\nbetween two given thresholds.The monetary authority is responsible for the\nsystemic risk governance.The governance consists in the choice of the assets\nand of the liabilities of a kind of \"ideal bank\" as functions of time and in\nthe choice of the rules that regulate the cooperation mechanism among\nbanks.These rules are obtained solving an optimal control problem for the\npseudo mean field approximation of the banking system model. The governance\ninduces the banks of the system to behave like the \"ideal bank\". Shocks acting\non the assets or on the liabilities of the banks are simulated.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.12431v1"
    },
    {
        "title": "Cross-sectional Learning of Extremal Dependence among Financial Assets",
        "authors": [
            "Xing Yan",
            "Qi Wu",
            "Wen Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We propose a novel probabilistic model to facilitate the learning of\nmultivariate tail dependence of multiple financial assets. Our method allows\none to construct from known random vectors, e.g., standard normal,\nsophisticated joint heavy-tailed random vectors featuring not only distinct\nmarginal tail heaviness, but also flexible tail dependence structure. The\nnovelty lies in that pairwise tail dependence between any two dimensions is\nmodeled separately from their correlation, and can vary respectively according\nto its own parameter rather than the correlation parameter, which is an\nessential advantage over many commonly used methods such as multivariate $t$ or\nelliptical distribution. It is also intuitive to interpret, easy to track, and\nsimple to sample comparing to the copula approach. We show its flexible tail\ndependence structure through simulation. Coupled with a GARCH model to\neliminate serial dependence of each individual asset return series, we use this\nnovel method to model and forecast multivariate conditional distribution of\nstock returns, and obtain notable performance improvements in multi-dimensional\ncoverage tests. Besides, our empirical finding about the asymmetry of tails of\nthe idiosyncratic component as well as the market component is interesting and\nworth to be well studied in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.13425v3"
    },
    {
        "title": "Adaptive Financial Fraud Detection in Imbalanced Data with Time-Varying\n  Poisson Processes",
        "authors": [
            "Régis Houssou",
            "Jérôme Bovay",
            "Stephan Robert"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  This paper discusses financial fraud detection in imbalanced dataset using\nhomogeneous and non-homogeneous Poisson processes. The probability of\npredicting fraud on the financial transaction is derived. Applying our\nmethodology to the financial dataset shows a better predicting power than a\nbaseline approach, especially in the case of higher imbalanced data.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04308v1"
    },
    {
        "title": "Modality for Scenario Analysis and Maximum Likelihood Allocation",
        "authors": [
            "Takaaki Koike",
            "Marius Hofert"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We study the variability of a risk from the statistical viewpoint of\nmultimodality of the conditional loss distribution given that the aggregate\nloss equals an exogenously provided capital. This conditional distribution\nserves as a building block for calculating risk allocations such as the Euler\ncapital allocation of Value-at-Risk. A superlevel set of this conditional\ndistribution can be interpreted as a set of severe and plausible stress\nscenarios the given capital is supposed to cover. We show that various\ndistributional properties of this conditional distribution, such as modality,\ndependence and tail behavior, are inherited from those of the underlying joint\nloss distribution. Among these properties, we find that modality of the\nconditional distribution is an important feature in risk assessment related to\nthe variety of risky scenarios likely to occur in a stressed situation. Under\nunimodality, we introduce a novel risk allocation method called maximum\nlikelihood allocation (MLA), defined as the mode of the conditional\ndistribution given the total capital. Under multimodality, a single vector of\nallocations can be less sound. To overcome this issue, we investigate the\nso-called multimodalty adjustment to increase the soundness of risk\nallocations. Properties of the conditional distribution, MLA and multimodality\nadjustment are demonstrated in numerical experiments. In particular, we observe\nthat negative dependence among losses typically leads to multimodality, and\nthus a higher multimodality adjustment can be required.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.02950v2"
    },
    {
        "title": "Nonparametric Expected Shortfall Forecasting Incorporating Weighted\n  Quantiles",
        "authors": [
            "Giuseppe Storti",
            "Chao Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  A new semi-parametric Expected Shortfall (ES) estimation and forecasting\nframework is proposed. The proposed approach is based on a two-step estimation\nprocedure. The first step involves the estimation of Value-at-Risk (VaR) at\ndifferent quantile levels through a set of quantile time series regressions.\nThen, the ES is computed as a weighted average of the estimated quantiles. The\nquantiles weighting structure is parsimoniously parameterized by means of a\nBeta weight function whose coefficients are optimized by minimizing a joint VaR\nand ES loss function of the Fissler-Ziegel class. The properties of the\nproposed approach are first evaluated with an extensive simulation study using\ntwo data generating processes. Two forecasting studies with different\nout-of-sample sizes are then conducted, one of which focuses on the 2008 Global\nFinancial Crisis (GFC) period. The proposed models are applied to 7 stock\nmarket indices and their forecasting performances are compared to those of a\nrange of parametric, non-parametric and semi-parametric models, including\nGARCH, Conditional AutoRegressive Expectile (CARE), joint VaR and ES quantile\nregression models and simple average of quantiles. The results of the\nforecasting experiments provide clear evidence in support of proposed models.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04868v3"
    },
    {
        "title": "Range Value-at-Risk: Multivariate and Extreme Values",
        "authors": [
            "Roba Bairakdar",
            "Lu Cao",
            "Melina Mailhot"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  The concept of univariate Range Value-at-Risk, presented by Cont et al.\n(2010), is extended in the multidimensional setting. Traditional risk measures\nare not well suited when dealing with heavy-tail distributions and infinite\ntail expectations. The multivariate definitions of robust truncated tail\nexpectations are provided to overcome this problem. Robustness and other\nproperties as well as empirical estimators are derived. Closed-form expressions\nand special cases in the extreme value framework are also discussed. Numerical\nand graphical examples are provided to examine the accuracy of the empirical\nestimators.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.12473v1"
    },
    {
        "title": "Losing money with a high Sharpe ratio",
        "authors": [
            "Vladimir Vovk"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  A simple example shows that losing all money is compatible with a very high\nSharpe ratio (as computed after losing all money). However, the only way that\nthe Sharpe ratio can be high while losing money is that there is a period in\nwhich all or almost all money is lost. This note explores the best achievable\nSharpe and Sortino ratios for investors who lose money but whose one-period\nreturns are bounded below (or both below and above) by a known constant.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0706v1"
    },
    {
        "title": "Scenarios and their Aggregation in the Regulatory Risk Measurement\n  Environment",
        "authors": [
            "Andreas Haier",
            "Thorsten Pfeiffer"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We define scenarios, propose different methods of aggregating them, discuss\ntheir properties and benchmark them against quadrant requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0646v1"
    },
    {
        "title": "Funding Liquidity, Debt Tenor Structure, and Creditor's Belief: An\n  Exogenous Dynamic Debt Run Model",
        "authors": [
            "Gechun Liang",
            "Eva Lütkebohmert",
            "Wei Wei"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We propose a unified structural credit risk model incorporating both\ninsolvency and illiquidity risks, in order to investigate how a firm's default\nprobability depends on the liquidity risk associated with its financing\nstructure. We assume the firm finances its risky assets by mainly issuing\nshort- and long-term debt. Short-term debt can have either a discrete or a more\nrealistic staggered tenor structure. At rollover dates of short-term debt,\ncreditors face a dynamic coordination problem. We show that a unique threshold\nstrategy (i.e., a debt run barrier) exists for short-term creditors to decide\nwhen to withdraw their funding, and this strategy is closely related to the\nsolution of a non-standard optimal stopping time problem with control\nconstraints. We decompose the total credit risk into an insolvency component\nand an illiquidity component based on such an endogenous debt run barrier\ntogether with an exogenous insolvency barrier.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.3513v3"
    },
    {
        "title": "Capital allocation and risk appetite under Solvency II framework",
        "authors": [
            "Ivan Granito",
            "Paolo De Angelis"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  The aim of this paper is to introduce a method for computing the allocated\nSolvency II Capital Requirement (SCR) of each Risk which the company is exposed\nto, taking in account for the diversification effect among different risks. The\nmethod suggested is based on the Euler principle. We show that it has very\nsuitable properties like coherence in the sense of Denault (2001) and RORAC\ncompatibility, and practical implications for the companies that use the\nstandard formula. Further, we show how this approach can be used to evaluate\nthe underwriting and reinsurance policies and to define a measure of the\nCompany's risk appetite, based on the capital at risk return.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.02934v1"
    },
    {
        "title": "A composition between risk and deviation measures",
        "authors": [
            "Marcelo Brutti Righi"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  The intuition of risk is based on two main concepts: loss and variability. In\nthis paper, we present a composition of risk and deviation measures, which\ncontemplate these two concepts. Based on the proposed Limitedness axiom, we\nprove that this resulting composition, based on properties of the two\ncomponents, is a coherent risk measure. Similar results for the cases of convex\nand co-monotone risk measures are exposed. We also provide examples of known\nand new risk measures constructed under this framework in order to highlight\nthe importance of our approach, especially the role of the Limitedness axiom.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06943v5"
    },
    {
        "title": "Singular Problems for Integro-Differential Equations in Dynamic\n  Insurance Models",
        "authors": [
            "Tatiana Belkina",
            "Nadezhda Konyukhova",
            "Sergey Kurochkin"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  A second order linear integro-differential equation with Volterra integral\noperator and strong singularities at the endpoints (zero and infinity) is\nconsidered. Under limit conditions at the singular points, and some natural\nassumptions, the problem is a singular initial problem with limit normalizing\nconditions at infinity. An existence and uniqueness theorem is proved and\nasymptotic representations of the solution are given. A numerical algorithm for\nevaluating the solution is proposed, calculations and their interpretation are\ndiscussed. The main singular problem under study describes the survival\n(non-ruin) probability of an insurance company on infinite time interval (as a\nfunction of initial surplus) in the Cramer-Lundberg dynamic insurance model\nwith an exponential claim size distribution and certain company's strategy at\nthe financial market assuming investment of a fixed part of the surplus\n(capital) into risky assets (shares) and the rest of it into a risk free asset\n(bank deposit). Accompanying \"degenerate\" problems are also considered that\nhave an independent meaning in risk theory\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08666v1"
    },
    {
        "title": "New copulas based on general partitions-of-unity (part III) - the\n  continuous case (extended version)",
        "authors": [
            "Dietmar Pfeifer",
            "Andreas Mändle",
            "Olena Ragulina",
            "Côme Girschig"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this paper we discuss a natural extension of infinite discrete\npartition-of-unity copulas which were recently introduced in the literature to\ncontinuous partition of copulas with possible applications in risk management\nand other fields. We present a general simple algorithm to generate such\ncopulas on the basis of the empirical copula from high-dimensional data sets.\nIn particular, our constructions also allow for an implementation of positive\ntail dependence which sometimes is a desirable property of copula modelling, in\nparticular for internal models under Solvency II.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00957v4"
    },
    {
        "title": "A Dynamic Model of Central Counterparty Risk",
        "authors": [
            "Tomasz R. Bielecki",
            "Igor Cialenco",
            "Shibi Feng"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We introduce a dynamic model of the default waterfall of derivatives CCPs and\npropose a risk sensitive method for sizing the initial margin (IM), and the\ndefault fund (DF) and its allocation among clearing members. Using a Markovian\nstructure model of joint credit migrations, our evaluation of DF takes into\naccount the joint credit quality of clearing members as they evolve over time.\nAnother important aspect of the proposed methodology is the use of the time\nconsistent dynamic risk measures for computation of IM and DF. We carry out a\ncomprehensive numerical study, where, in particular, we analyze the advantages\nof the proposed methodology and its comparison with the currently prevailing\nmethods used in industry.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.02012v1"
    },
    {
        "title": "Jumping VaR: Order Statistics Volatility Estimator for Jumps\n  Classification and Market Risk Modeling",
        "authors": [
            "Luca Spadafora",
            "Francesca Sivero",
            "Nicola Picchiotti"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  This paper proposes a new integrated variance estimator based on order\nstatistics within the framework of jump-diffusion models. Its ability to\ndisentangle the integrated variance from the total process quadratic variation\nis confirmed by both simulated and empirical tests. For practical purposes, we\nintroduce an iterative algorithm to estimate the time-varying volatility and\nthe occurred jumps of log-return time series. Such estimates enable the\ndefinition of a new market risk model for the Value at Risk forecasting. We\nshow empirically that this procedure outperforms the standard historical\nsimulation method applying standard back-testing approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.07021v2"
    },
    {
        "title": "Spatial risk measures and rate of spatial diversification",
        "authors": [
            "Erwan Koch"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  An accurate assessment of the risk of extreme environmental events is of\ngreat importance for populations, authorities and the\nbanking/insurance/reinsurance industry. Koch (2017) introduced a notion of\nspatial risk measure and a corresponding set of axioms which are well suited to\nanalyze the risk due to events having a spatial extent, precisely such as\nenvironmental phenomena. The axiom of asymptotic spatial homogeneity is of\nparticular interest since it allows one to quantify the rate of spatial\ndiversification when the region under consideration becomes large. In this\npaper, we first investigate the general concepts of spatial risk measures and\ncorresponding axioms further and thoroughly explain the usefulness of this\ntheory for both actuarial science and practice. Second, in the case of a\ngeneral cost field, we give sufficient conditions such that spatial risk\nmeasures associated with expectation, variance, Value-at-Risk as well as\nexpected shortfall and induced by this cost field satisfy the axioms of\nasymptotic spatial homogeneity of order $0$, $-2$, $-1$ and $-1$, respectively.\nLast but not least, in the case where the cost field is a function of a\nmax-stable random field, we provide conditions on both the function and the\nmax-stable field ensuring the latter properties. Max-stable random fields are\nrelevant when assessing the risk of extreme events since they appear as a\nnatural extension of multivariate extreme-value theory to the level of random\nfields. Overall, this paper improves our understanding of spatial risk measures\nas well as of their properties with respect to the space variable and\ngeneralizes many results obtained in Koch (2017).\n",
        "pdf_link": "http://arxiv.org/pdf/1803.07041v6"
    },
    {
        "title": "On the Basel Liquidity Formula for Elliptical Distributions",
        "authors": [
            "Janine Balter",
            "Alexander J. McNeil"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  A justification of the Basel liquidity formula for risk capital in the\ntrading book is given under the assumption that market risk-factor changes form\na Gaussian white noise process over 10-day time steps and changes to P&L are\nlinear in the risk-factor changes. A generalization of the formula is derived\nunder the more general assumption that risk-factor changes are multivariate\nelliptical. It is shown that the Basel formula tends to be conservative when\nthe elliptical distributions are from the heavier-tailed generalized hyperbolic\nfamily. As a by-product of the analysis a Fourier approach to calculating\nexpected shortfall for general symmetric loss distributions is developed.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.07590v1"
    },
    {
        "title": "A unifying approach to constrained and unconstrained optimal reinsurance",
        "authors": [
            "Yuxia Huang",
            "Chuancun Yin"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this paper, we study two classes of optimal reinsurance models from\nperspectives of both insurers and reinsurers by minimizing their convex\ncombination where the risk is measured by a distortion risk measure and the\npremium is given by a distortion premium principle. Firstly, we show that how\noptimal reinsurance models for the unconstrained optimization problem and\nconstrained optimization problems can be formulated in a unified way. Secondly,\nwe propose a geometric approach to solve optimal reinsurance problems directly.\nThis paper considers a class of increasing convex ceded loss functions and\nderives the explicit solutions of the optimal reinsurance which can be in forms\nof quota-share, stop-loss, change-loss, the combination of quota-share and\nchange-loss or the combination of change-loss and change-loss with different\nretentions. Finally, we consider two specific cases: Value at Risk (VaR) and\nTail Value at Risk (TVaR).\n",
        "pdf_link": "http://arxiv.org/pdf/1807.06892v1"
    },
    {
        "title": "Scalar multivariate risk measures with a single eligible asset",
        "authors": [
            "Zachary Feinstein",
            "Birgit Rudloff"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this paper we present results on scalar risk measures in markets with\ntransaction costs. Such risk measures are defined as the minimal capital\nrequirements in the cash asset. First, some results are provided on the dual\nrepresentation of such risk measures, with particular emphasis given on the\nspace of dual variables as (equivalent) martingale measures and prices\nconsistent with the market model. Then, these dual representations are used to\nobtain the main results of this paper on time consistency for scalar risk\nmeasures in markets with frictions. It is well known from the superhedging risk\nmeasure in markets with transaction costs, as in Jouini and Kallal (1995), Roux\nand Zastawniak (2016), and Loehne and Rudloff (2014), that the usual scalar\nconcept of time consistency is too strong and not satisfied. We will show that\na weaker notion of time consistency can be defined, which corresponds to the\nusual scalar time consistency but under any fixed consistent pricing process.\nWe will prove the equivalence of this weaker notion of time consistency and a\ncertain type of backward recursion with respect to the underlying risk measure\nwith a fixed consistent pricing process. Several examples are given, with\nspecial emphasis on the superhedging risk measure.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.10694v5"
    },
    {
        "title": "Panel quantile regressions for estimating and predicting the\n  Value--at--Risk of commodities",
        "authors": [
            "František Čech",
            "Jozef Baruník"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  This paper investigates how realized and option implied volatilities are\nrelated to the future quantiles of commodity returns. Whereas realized\nvolatility measures ex-post uncertainty, volatility implied by option prices\nreveals the market's expectation and is often used as an ex-ante measure of the\ninvestor sentiment. Using a flexible panel quantile regression framework, we\nshow how the future conditional quantiles of commodities returns depend on both\nex-post and ex-ante uncertainty measures. Empirical analysis of the most liquid\ncommodities covering main sectors including energy, food, agricultural,\nprecious and industrial metals reveal several important stylized facts about\nthe data. We document common patterns of the dependence between future quantile\nreturns and ex-post as well as ex-ante volatilities. We further show that\nconditional returns distribution is platykurtic and time-invariant. The\napproach can serve as a useful risk management tools for investors interested\nin commodity future contracts.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.11823v1"
    },
    {
        "title": "A Thermodynamic Picture of Financial Market and Model Risk",
        "authors": [
            "Yu Feng"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  By treating the financial market as a thermodynamic system, we establish a\none-to-one correspondence between thermodynamic variables and economic\nquantities. Measured by the expected loss under the worst-case scenario,\nfinancial risk caused by model uncertainty is regarded as a result of the\ninteraction between financial market and external information sources. This\nforms a thermodynamic picture in which a closed system interacts with an\nexternal reservoir, reaching its equilibrium at the worst-case scenario. The\nseverity of the worst-case scenario depends on the rate of heat dissipation,\ncaused by information sources reducing the entropy of the system. This\nthermodynamic picture leads to simple and natural derivation of the\ncharacterization rules of the worst-case risk, and gives its Lagrangian and\nHamiltonian forms. With its help financial practitioners may evaluate risks\nutilizing both equilibrium and non-equilibrium thermodynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00151v1"
    },
    {
        "title": "Hedging longevity risk in defined contribution pension schemes",
        "authors": [
            "Ankush Agarwal",
            "Christian-Oliver Ewald",
            "Yongjie Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Pension schemes all over the world are under increasing pressure to\nefficiently hedge the longevity risk posed by ageing populations. In this work,\nwe study an optimal investment problem for a defined contribution pension\nscheme which decides to hedge the longevity risk using a mortality-linked\nsecurity, typically a longevity bond. The pension scheme invests in the risky\nassets available in the market, including the longevity bond, by using the\ncontributions from a representative scheme member to ensure a minimum guarantee\nsuch that the member is able to purchase a lifetime annuity upon retirement. We\ntransform this constrained optimal investment problem into an unconstrained\nproblem by replicating a self-financing portfolio of future contributions from\nthe member and the minimum guarantee provided by the scheme. We solve the\nresulting optimisation problem using the dynamic programming principle and\nthrough a series of numerical studies reveal that the longevity risk has an\nimportant impact on the performance of investment strategies. Our results\nprovide mathematical evidence supporting the use of mortality-linked securities\nfor efficient hedging of the longevity risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10229v4"
    },
    {
        "title": "calculation worst-case Value-at-Risk prediction using empirical data\n  under model uncertainty",
        "authors": [
            "Wentao Hu"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Quantification of risk positions under model uncertainty is of crucial\nimportance from both viewpoints of external regulation and internal management.\nThe concept of model uncertainty, sometimes also referred to as model\nambiguity. Although we know the family of models, we cannot precisely decide\nwhich one to use. Given the set $\\mathcal{P}$, the value of the risk measure\n$\\rho$ varies in a range over the set of all possible models. The largest value\nin such a range is referred to as a worst-case value, and the corresponding\nmodel is called a worst scenario. Value-at-Risk(VaR) has become a very popular\nrisk-measurement tool since it was first proposed. Naturally, WVaR(worst-case\nValue-at-Risk) attracts the attention of many researchers. Although many\nliteratures investigated WVaR, the implications for empirical data analysis\nremain rare. In this paper, we proposed a special model uncertainty market\nmodel to simply the $\\mathcal{P}$ to a set contain finite number of probability\ndistributions. The model has the structure of the two-layer mixed distribution\nmodel. We used change point detection method to divide the returns series and\nthen used EM algorithm to estimate the parameters. Finally, we calculated VaR,\nWVaR(worst-case Value-at-Risk) and BVaR(best-case Value-at-Risk) for four\nfinancial markets and then analyzed their different performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.00982v1"
    },
    {
        "title": "Dynamic Dependence Modeling in financial time series",
        "authors": [
            "Yali Dou",
            "Haiyan Liu",
            "Georgios Aivaliotis"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  This paper explores the dependence modeling of financial assets in a dynamic\nway and its critical role in measuring risk. Two new methods, called\nAccelerated Moving Window method and Bottom-up method are proposed to detect\nthe change of copula. The performance of these two methods together with Binary\nSegmentation \\cite{vostrikova1981detection} and Moving Window method\n\\cite{guegan2009forecasting} is compared based on simulated data. The\nbest-performing method is applied to Standard \\& Poor 500 and Nasdaq indices.\nValue-at-Risk and Expected Shortfall are computed from the dynamic and the\nstatic model respectively to illustrate the effectiveness of the best method as\nwell as the importance of dynamic dependence modeling through backtesting.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.05130v1"
    },
    {
        "title": "Nash Equilibria in Optimal Reinsurance Bargaining",
        "authors": [
            "Michail Anthropelos",
            "Tim J. Boonen"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We introduce a strategic behavior in reinsurance bilateral transactions,\nwhere agents choose the risk preferences they will appear to have in the\ntransaction. Within a wide class of risk measures, we identify agents'\nstrategic choices to a range of risk aversion coefficients. It is shown that at\nthe strictly beneficial Nash equilibria, agents appear homogeneous with respect\nto their risk preferences. While the game does not cause any loss of total\nwelfare gain, its allocation between agents is heavily affected by the agents'\nstrategic behavior. This allocation is reflected in the reinsurance premium,\nwhile the insurance indemnity remains the same in all strictly beneficial Nash\nequilibria. Furthermore, the effect of agents' bargaining power vanishes\nthrough the game procedure and the agent who gets more welfare gain is the one\nwho has an advantage in choosing the common risk aversion at the equilibrium.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01739v2"
    },
    {
        "title": "Value adjustments and dynamic hedging of reinsurance counterparty risk",
        "authors": [
            "Claudia Ceci",
            "Katia Colaneri",
            "Rdiger Frey",
            "Verena Köck"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Reinsurance counterparty credit risk (RCCR) is the risk of a loss arising\nfrom the fact that a reinsurance company is unable to fulfill her contractual\nobligations towards the ceding insurer. RCCR is an important risk category for\ninsurance companies which, so far, has been addressed mostly via qualitative\napproaches. In this paper we therefore study value adjustments and dynamic\nhedging for RCCR. We propose a novel model that accounts for contagion effects\nbetween the default of the reinsurer and the price of the reinsurance contract.\nWe characterize the value adjustment in a reinsurance contract via a partial\nintegro-differential equation (PIDE) and derive the hedging strategies using a\nquadratic method. The paper closes with a simulation study which shows that\ndynamic hedging strategies have the potential to significantly reduce RCCR.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.04354v1"
    },
    {
        "title": "Dual Representation of Expectile based Expected Shortfall and Its\n  Properties",
        "authors": [
            "Samuel Drapeau",
            "Mekonnen Tadese"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  The expectile can be considered as a generalization of quantile. While\nexpected shortfall is a quantile based risk measure, we study its counterpart\n-- the expectile based expected shortfall -- where expectile takes the place of\nquantile. We provide its dual representation in terms of Bochner integral.\nAmong other properties, we show that it is bounded from below in terms of\nconvex combinations of expected shortfalls, and also from above by the smallest\nlaw invariant, coherent and comonotonic risk measure, for which we give the\nexplicit formulation of the corresponding distortion function. As a benchmark\nto the industry standard expected shortfall we further provide its comparative\nasymptotic behavior in terms of extreme value distributions. Based on these\nresults, we finally compute explicitly the expectile based expected shortfall\nfor some selected class of distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.03245v1"
    },
    {
        "title": "Cyber bonds and their pricing models",
        "authors": [
            "Oleg Kolesnikov",
            "Alexander Markov",
            "Daulet Smagulov",
            "Sergejs Solovjovs"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Motivated by the developments in cyber risk treatment in the finance\nindustry, we propose a general framework of cyber bond, whose main purpose is\nto insure (compensate) losses of a cyber attack. Based on a database of\npublicly available cyber events, we determine cyber loss distribution\nparameters and use them to numerically simulate cyber bond price, yield, and\nother characteristics. We also consider two possible approaches to cyber bond\ncoupon calculation.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.06698v1"
    },
    {
        "title": "Mathematical Modeling of Systemic Risk in Financial Networks: Managing\n  Default Contagion and Fire Sales",
        "authors": [
            "Daniel Ritter"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  As impressively shown by the financial crisis in 2007/08, contagion effects\nin financial networks harbor a great threat for the stability of the entire\nsystem. Without sufficient capital requirements for banks and other financial\ninstitutions, shocks that are locally confined at first can spread through the\nentire system and be significantly amplified by various contagion channels. The\naim of this thesis is thus to investigate in detail two selected contagion\nchannels of this so-called systemic risk, provide mathematical models and\nderive consequences for the systemic risk management of financial institutions.\nThe first contagion channel we consider is default contagion. The underlying\neffect is here that insolvent institutions cannot service their debt or other\nfinancial obligations anymore - at least partially. Debtors and other directly\nimpacted parties in the system are thus forced to write off their losses and\ncan possibly be driven into insolvency themselves due to their incurred\nfinancial losses. This on the other hand starts a new round in the default\ncontagion process. In our model we simplistically describe each institution by\nall the financial positions it is exposed to as well as its initial capital. In\ndoing so, our starting point is the work of Detering et al. (2017) - a model\nfor contagion in unweighted networks - which particularly considers the exact\nnetwork configuration to be random and derives asymptotic results for large\nnetworks. We extend this model such that weighted networks can be considered\nand an application to financial networks becomes possible. More precisely, for\nany given initial shock we deduce an explicit asymptotic expression for the\ntotal damage caused in the system by contagion and provide a necessary and\nsufficient criterion for an unshocked financial system to be stable against\nsmall shocks. Moreover, ...\n",
        "pdf_link": "http://arxiv.org/pdf/1911.07313v1"
    },
    {
        "title": "Artificial intelligence approach to momentum risk-taking",
        "authors": [
            "Ivan Cherednik"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We propose a mathematical model of momentum risk-taking, which is essentially\nreal-time risk management focused on short-term volatility of stock markets.\nIts implementation, our fully automated momentum equity trading system\npresented systematically, proved to be successful in extensive historical and\nreal-time experiments. Momentum risk-taking is one of the key components of\ngeneral decision-making, a challenge for artificial intelligence and machine\nlearning with deep roots in cognitive science; its variants beyond stock\nmarkets are discussed. We begin with a new algebraic-type theory of news impact\non share-prices, which describes well their power growth, periodicity, and the\nmarket phenomena like price targets and profit-taking. This theory generally\nrequires Bessel and hypergeometric functions. Its discretization results in\nsome tables of bids, which are basically expected returns for main investment\nhorizons, the key in our trading system. The ML procedures we use are similar\nto those in neural networking. A preimage of our approach is the new contract\ncard game provided at the end, a combination of bridge and poker. Relations to\nrandom processes and the fractional Brownian motion are outlined.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.08448v4"
    },
    {
        "title": "Denting the FRTB IMA computational challenge via Orthogonal Chebyshev\n  Sliding Technique",
        "authors": [
            "Mariano Zeron-Medina Laris",
            "Ignacio Ruiz"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  In this paper we introduce a new technique based on high-dimensional\nChebyshev Tensors that we call \\emph{Orthogonal Chebyshev Sliding Technique}.\nWe implemented this technique inside the systems of a tier-one bank, and used\nit to approximate Front Office pricing functions in order to reduce the\nsubstantial computational burden associated with the capital calculation as\nspecified by FRTB IMA. In all cases, the computational burden reductions\nobtained were of more than $90\\%$, while keeping high degrees of accuracy, the\nlatter obtained as a result of the mathematical properties enjoyed by Chebyshev\nTensors.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.10948v2"
    },
    {
        "title": "Suffocating Fire Sales",
        "authors": [
            "Nils Detering",
            "Thilo Meyer-Brandis",
            "Konstantinos Panagiotou",
            "Daniel Ritter"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Fire sales are among the major drivers of market instability in modern\nfinancial systems. Due to iterated distressed selling and the associated price\nimpact, initial shocks to some institutions can be amplified dramatically\nthrough the network induced by portfolio overlaps. In this paper, we develop a\nmathematical framework that allows us to investigate central characteristics\nthat drive or hinder the propagation of distress. We investigate single systems\nas well as ensembles of systems that are alike, where similarity is measured in\nterms of the empirical distribution of all defining properties of a system.\nThis asymptotic approach ensures a great deal of robustness to statistical\nuncertainty and temporal fluctuations. A characterization of those systems that\nare resilient to small shocks emerges, and we provide criteria that regulators\nmight exploit in order to assess the stability of a financial system.\n  We illustrate the application of these criteria for some exemplary\nconfigurations in the context of capital requirements and test the\napplicability of our results for systems of moderate size by Monte Carlo\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.08110v3"
    },
    {
        "title": "iConViz: Interactive Visual Exploration of the Default Contagion Risk of\n  Networked-Guarantee Loans",
        "authors": [
            "Zhibin Niu",
            "Runlin Li",
            "Junqi Wu",
            "Dawei Cheng",
            "Jiawan Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Groups of enterprises can serve as guarantees for one another and form\ncomplex networks when obtaining loans from commercial banks. During economic\nslowdowns, corporate default may spread like a virus and lead to large-scale\ndefaults or even systemic financial crises. To help financial regulatory\nauthorities and banks manage the risk associated with networked loans, we\nidentified the default contagion risk, a pivotal issue in developing preventive\nmeasures, and established iConVis, an interactive visual analysis tool that\nfacilitates the closed-loop analysis process. A novel financial metric, the\ncontagion effect, was formulated to quantify the infectious consequences of\nguarantee chains in this type of network. Based on this metric, we designed and\nimplement a series of novel and coordinated views that address the analysis of\nfinancial problems. Experts evaluated the system using real-world financial\ndata. The proposed approach grants practitioners the ability to avoid previous\nad hoc analysis methodologies and extend coverage of the conventional Capital\nAccord to the banking industry.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.09542v3"
    },
    {
        "title": "Optimal Insurance to Minimize the Probability of Ruin: Inverse Survival\n  Function Formulation",
        "authors": [
            "Bahman Angoshtari",
            "Virginia R. Young"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We find the optimal indemnity to minimize the probability of ruin when\npremium is calculated according to the distortion premium principle with a\nproportional risk load, and admissible indemnities are such that both the\nindemnity and retention are non-decreasing functions of the underlying loss. We\nreformulate the problem with the inverse survival function as the control\nvariable and show that deductible insurance with maximum limit is optimal. Our\nmain contribution is in solving this problem via the inverse survival function.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.03798v1"
    },
    {
        "title": "Insurance valuation: A two-step generalised regression approach",
        "authors": [
            "Karim Barigou",
            "Valeria Bignozzi",
            "Andreas Tsanakas"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Current approaches to fair valuation in insurance often follow a two-step\napproach, combining quadratic hedging with application of a risk measure on the\nresidual liability, to obtain a cost-of-capital margin. In such approaches, the\npreferences represented by the regulatory risk measure are not reflected in the\nhedging process. We address this issue by an alternative two-step hedging\nprocedure, based on generalised regression arguments, which leads to portfolios\nthat are neutral with respect to a risk measure, such as Value-at-Risk or the\nexpectile. First, a portfolio of traded assets aimed at replicating the\nliability is determined by local quadratic hedging. Second, the residual\nliability is hedged using an alternative objective function. The risk margin is\nthen defined as the cost of the capital required to hedge the residual\nliability. In the case quantile regression is used in the second step, yearly\nsolvency constraints are naturally satisfied; furthermore, the portfolio is a\nrisk minimiser among all hedging portfolios that satisfy such constraints. We\npresent a neural network algorithm for the valuation and hedging of insurance\nliabilities based on a backward iterations scheme. The algorithm is fairly\ngeneral and easily applicable, as it only requires simulated paths of risk\ndrivers.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.04364v2"
    },
    {
        "title": "Parametric measures of variability induced by risk measures",
        "authors": [
            "Fabio Bellini",
            "Tolulope Fadina",
            "Ruodu Wang",
            "Yunran Wei"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We present a general framework for a comparative theory of variability\nmeasures, with a particular focus on the recently introduced one-parameter\nfamilies of inter-Expected Shortfall differences and inter-expectile\ndifferences, that are explored in detail and compared with the widely known and\napplied inter-quantile differences. From the mathematical point of view, our\nmain result is a characterization of symmetric and comonotonic variability\nmeasures as mixtures of inter-Expected Shortfall differences, under a few\nadditional technical conditions. Further, we study the stochastic orders\ninduced by the pointwise comparison of inter-Expected Shortfall and\ninter-expectile differences, and discuss their relationship with the dilation\norder. From the statistical point of view, we establish asymptotic consistency\nand normality of the natural estimators and provide a rule of the thumb for\ncross-comparisons. Finally, we study the empirical behaviour of the considered\nclasses of variability measures on the S&P 500 Index under various economic\nregimes, and explore the comparability of different time series according to\nthe introduced stochastic orders.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.05219v3"
    },
    {
        "title": "How dark is the dark side of diversification?",
        "authors": [
            "Pedro Cadenas",
            "Henryk Gzyl",
            "Hyun Woong Park"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Against the widely held belief that diversification at banking institutions\ncontributes to the stability of the financial system, Wagner (2010) found that\ndiversification actually makes systemic crisis more likely. While it is true,\nas Wagner asserts, that the probability of joint default of the diversified\nportfolios is larger; we contend that, as common practice, the effect of\ndiversification is examined with respect to a risk measure like VaR. We find\nthat when banks use VaR, diversification does reduce individual and systemic\nrisk. This, in turn, generates a different set of incentives for banks and\nregulators.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.12154v1"
    },
    {
        "title": "Life insurance policies with cash flows subject to random interest rate\n  changes",
        "authors": [
            "David R. Baños"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  The main purpose of this work is to derive a partial differential equation\nfor the reserves of life insurance liabilities subject to stochastic interest\nrates where the benefits and premiums depend directly on changes in the\ninterest rate curve. In particular, we allow the payment streams to depend on\nthe performance of an overnight technical interest rate, making them stochastic\nas well. This opens up for considering new types of contracts based on the\nperformance of the insurer's returns on their own investments. We provide\nexplicit solutions for the reserves when the premiums and benefits vary\naccording to interest rate levels or averages under the Vasicek model and\nconduct some simulations computing reserve surfaces numerically. We also give\nan example of a reinsurance treaty taking over pension payments when the\ninsurer's average returns fall under some specified threshold.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15541v1"
    },
    {
        "title": "Insurance Business and Sustainable Development",
        "authors": [
            "Dietmar Pfeifer",
            "Vivien Langen"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this study, we will discuss recent developments in risk management of the\nglobal financial and insurance business with respect to sustainable\ndevelopment. So far climate change aspects have been the dominant aspect in\nmanaging sustainability risks and opportunities, accompanied by the development\nof several legislative initiatives triggered by supervisory authorities.\nHowever, a sole concentration on these aspects misses out other important\neconomic and social facets of sustainable development goals formulated by the\nUN. Such aspects have very recently come into the focus of the European\nCommittee concerning the Solvency II project for the European insurance\nindustry. Clearly the new legislative expectations can be better handled by\nlarger insurance companies and holdings than by small- and medium-sized mutual\ninsurance companies which are numerous in central Europe, due to their historic\ndevelopment starting in the late medieval ages and early modern times. We\ntherefore also concentrate on strategies within the risk management of such\nsmall- and medium-sized enterprises that can be achieved without much effort,\nin particular those that are not directly related to climate change.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.02612v1"
    },
    {
        "title": "Can a regulatory risk measure induce profit-maximizing risk capital\n  allocations? The case of Conditional Tail Expectation",
        "authors": [
            "Nawaf Mohammed",
            "Edward Furman",
            "Jianxi Su"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Risk capital allocations (RCAs) are an important tool in quantitative risk\nmanagement, where they are utilized to, e.g., gauge the profitability of\ndistinct business units, determine the price of a new product, and conduct the\nmarginal economic capital analysis. Nevertheless, the notion of RCA has been\nliving in the shadow of another, closely related notion, of risk measure (RM)\nin the sense that the latter notion often shapes the fashion in which the\nformer notion is implemented. In fact, as the majority of the RCAs known\nnowadays are induced by RMs, the popularity of the two are apparently very much\ncorrelated. As a result, it is the RCA that is induced by the Conditional Tail\nExpectation (CTE) RM that has arguably prevailed in scholarly literature and\napplications. Admittedly, the CTE RM is a sound mathematical object and an\nimportant regulatory RM, but its appropriateness is controversial in, e.g.,\nprofitability analysis and pricing. In this paper, we address the question as\nto whether or not the RCA induced by the CTE RM may concur with alternatives\nthat arise from the context of profit maximization. More specifically, we\nprovide exhaustive description of all those probabilistic model settings, in\nwhich the mathematical and regulatory CTE RM may also reflect the risk\nperception of a profit-maximizing insurer.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.05003v3"
    },
    {
        "title": "Thiele's Differential Equation Based on Markov Jump Processes with\n  Non-countable State Space",
        "authors": [
            "Emmanuel Coffie",
            "Sindre Duedahl",
            "Frank Proske"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In modern life insurance, Markov processes in continuous time on a finite or\nat least countable state space have been over the years an important tool for\nthe modelling of the states of an insured. Motivated by applications in\ndisability insurance, we propose in this paper a model for insurance states\nbased on Markov jump processes with more general state spaces. We use this\nmodel to derive a new type of Thiele's differential equation which e.g. allows\nfor a consistent calculation of reserves in disability insurance based on\ntwo-parameter continuous time rehabilitation rates.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10047v1"
    },
    {
        "title": "Optimal management of DC pension fund under relative performance ratio\n  and VaR constraint",
        "authors": [
            "Guohui Guan",
            "Zongxia Liang",
            "Yi xia"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper, we investigate the optimal management of defined contribution\n(abbr. DC) pension plan under relative performance ratio and Value-at-Risk\n(abbr. VaR) constraint. Inflation risk is introduced in this paper and the\nfinancial market consists of cash, inflation-indexed zero coupon bond and a\nstock. The goal of the pension manager is to maximize the performance ratio of\nthe real terminal wealth under VaR constraint. An auxiliary process is\nintroduced to transform the original problem into a self-financing problem\nfirst. Combining linearization method, Lagrange dual method, martingale method\nand concavification method, we obtain the optimal terminal wealth under\ndifferent cases. For convex penalty function, there are fourteen cases while\nfor concave penalty function, there are six cases. Besides, when the penalty\nfunction and reward function are both power functions, the explicit forms of\nthe optimal investment strategies are obtained. Numerical examples are shown in\nthe end of this paper to illustrate the impacts of the performance ratio and\nVaR constraint.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.04352v1"
    },
    {
        "title": "Problems with Risk Matrices Using Ordinal Scales",
        "authors": [
            "Michael Krisper"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper, we discuss various problems in the usage and definition of\nrisk matrices. We give an overview of the general process of risk assessment\nwith risk matrices and ordinal scales. Furthermore, we explain the fallacies in\neach phase of this process and give hints on which decisions may lead to more\nproblems than others and how to avoid them. Among those 24 discussed problems\nare ordinal scales, semi-quantitative arithmetics, range compression, risk\ninversion, ambiguity, and neglection of uncertainty. Finally, we make a case\nfor avoiding risk matrices altogether and instead propose using fully\nquantitative risk assessment methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.05440v1"
    },
    {
        "title": "Cheapest-to-Deliver Collateral: A Common Factor Approach",
        "authors": [
            "Felix L. Wolf",
            "Lech A. Grzelak",
            "Griselda Deelstra"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  The collateral choice option gives the collateral posting party the\nopportunity to switch between different collateral currencies which is\nwell-known to impact the asset price. Quantification of the option's value is\nof practical importance but remains challenging under the assumption of\nstochastic rates, as it is determined by an intractable distribution which\nrequires involved approximations. Indeed, many practitioners still rely on\ndeterministic spreads between the rates for valuation. We develop a scalable\nand stable stochastic model of the collateral spreads under the assumption of\nconditional independence. This allows for a common factor approximation which\nadmits analytical results from which further estimators are obtained. We show\nthat in modelling the spreads between collateral rates, a second order model\nyields accurate results for the value of the collateral choice option. The\nmodel remains precise for a wide range of model parameters and is numerically\nefficient even for a large number of collateral currencies.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.06107v2"
    },
    {
        "title": "Delay stochastic interest rate model with jump and strong convergence in\n  Monte Carlo simulations",
        "authors": [
            "Emmanuel Coffie"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper, we study analytical properties of the solutions to the\ngeneralised delay Ait-Sahalia-type interest rate model with Poisson-driven\njump. Since this model does not have explicit solution, we employ several new\ntruncated Euler-Maruyama (EM) techniques to investigate finite time strong\nconvergence theory of the numerical solutions under the local Lipschitz\ncondition plus the Khasminskii-type condition. We justify the strong\nconvergence result for Monte Carlo calibration and valuation of some debt and\nderivative instruments.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.07651v3"
    },
    {
        "title": "Robust equilibrium strategies in a defined benefit pension plan game",
        "authors": [
            "Guohui Guan",
            "Jiaqi Hu",
            "Zongxia Liang"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  This paper investigates the robust {non-zero-sum} games in an aggregated\n{overfunded} defined benefit (abbr. DB) pension plan. The sponsoring firm is\nconcerned with the investment performance of the fund surplus while the\nparticipants act as a union to claim a share of the fund surplus. The financial\nmarket consists of one risk-free asset and $n$ risky assets. The firm and the\nunion both are ambiguous about the financial market and care about the robust\nstrategies under the worst case scenario. {The union's objective is to maximize\nthe expected discounted utility of the additional benefits, the firm's two\ndifferent objectives are to maximizing the expected discounted utility of the\nfund surplus and the probability of the fund surplus reaching an upper level\nbefore hitting a lower level in the worst case scenario.} We formulate the\nrelated two robust non-zero-sum games for the firm and the union. Explicit\nforms and optimality of the solutions are shown by stochastic dynamic\nprogramming method. In the end of this paper, numerical results are illustrated\nto depict the economic behaviours of the robust equilibrium strategies in these\ntwo different games.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09121v1"
    },
    {
        "title": "Diversification quotients based on VaR and ES",
        "authors": [
            "Xia Han",
            "Liyuan Lin",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The diversification quotient (DQ) is recently introduced for quantifying the\ndegree of diversification of a stochastic portfolio model. It has an axiomatic\nfoundation and can be defined through a parametric class of risk measures.\nSince the Value-at-Risk (VaR) and the Expected Shortfall (ES) are the most\nprominent risk measures widely used in both banking and insurance, we\ninvestigate DQ constructed from VaR and ES in this paper. In particular, for\nthe popular models of elliptical and multivariate regular varying (MRV)\ndistributions, explicit formulas are available. The portfolio optimization\nproblems for the elliptical and MRV models are also studied. Our results\nfurther reveal favourable features of DQ, both theoretically and practically,\ncompared to traditional diversification indices based on a single risk measure.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.03517v3"
    },
    {
        "title": "Validation of machine learning based scenario generators",
        "authors": [
            "Gero Junike",
            "Solveig Flaig",
            "Ralf Werner"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Machine learning (ML) methods are becoming increasingly important in the\ndesign economic scenario generators for internal models. Validation of\ndata-driven models differs from classical theory-based models. We discuss two\nnovel aspects of such a validation: first, checking dependencies between risk\nfactors and second, detecting unwanted memorization effects. The first task\nbecomes necessary since in ML-based methods dependencies are no longer derived\nfrom a financial-mathematical theory but are driven by data. The need for the\nlatter task arises since it cannot be ruled out that ML-based models merely\nreproduce the empirical data rather than generating new scenarios. To address\nthe first issue, we propose to use an existing test from the literature. For\nthe second issue, we introduce and discuss a novel memorization ratio.\nNumerical experiments based on real market data are included and an\nautoencoder-based scenario generator is validated with these two methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.12719v4"
    },
    {
        "title": "Real Output Costs of Financial Crises: A Loss Distribution Approach",
        "authors": [
            "Daniel Kapp",
            "Marco Vega"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We study cross-country GDP losses due to financial crises in terms of\nfrequency (number of loss events per period) and severity (loss per\noccurrence). We perform the Loss Distribution Approach (LDA) to estimate a\nmulti-country aggregate GDP loss probability density function and the\npercentiles associated to extreme events due to financial crises.\n  We find that output losses arising from financial crises are strongly\nheterogeneous and that currency crises lead to smaller output losses than debt\nand banking crises.\n  Extreme global financial crises episodes, occurring with a one percent\nprobability every five years, lead to losses between 2.95% and 4.54% of world\nGDP.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.0967v2"
    },
    {
        "title": "Time consistency of dynamic risk measures in markets with transaction\n  costs",
        "authors": [
            "Zachary Feinstein",
            "Birgit Rudloff"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  The paper concerns primal and dual representations as well as time\nconsistency of set-valued dynamic risk measures. Set-valued risk measures\nappear naturally when markets with transaction costs are considered and capital\nrequirements can be made in a basket of currencies or assets. Time consistency\nof scalar risk measures can be generalized to set-valued risk measures in\ndifferent ways. The most intuitive generalization is called time consistency.\nWe will show that the equivalence between a recursive form of the risk measure\nand time consistency, which is a central result in the scalar case, does not\nhold in the set-valued framework. Instead, we propose an alternative\ngeneralization, which we will call multi-portfolio time consistency and show in\nthe main result of the paper that this property is indeed equivalent to the\nrecursive form as well as to an additive property for the acceptance sets.\nMulti-portfolio time consistency is a stronger property than time consistency.\nIn the scalar case, both notions coincide.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1483v2"
    },
    {
        "title": "Risk aggregation and stochastic claims reserving in disability insurance",
        "authors": [
            "Boualem Djehiche",
            "Björn Löfdahl"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We consider a large, homogeneous portfolio of life or disability annuity\npolicies. The policies are assumed to be independent conditional on an external\nstochastic process representing the economic-demographic environment. Using a\nconditional law of large numbers, we establish the connection between claims\nreserving and risk aggregation for large portfolios. Further, we derive a\npartial differential equation for moments of present values. Moreover, we show\nhow statistical multi-factor intensity models can be approximated by one-factor\nmodels, which allows for solving the PDEs very efficiently. Finally, we give a\nnumerical example where moments of present values of disability annuities are\ncomputed using finite difference methods and Monte Carlo simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.3589v2"
    },
    {
        "title": "Interconnected risk contributions: an heavy-tail approach to analyse US\n  financial sectors",
        "authors": [
            "M. Bernardi",
            "L. Petrella"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In this paper we consider a multivariate model-based approach to measure the\ndynamic evolution of tail risk interdependence among US banks, financial\nservices and insurance sectors. To deeply investigate the risk contribution of\ninsurers we consider separately life and non-life companies. To achieve this\ngoal we apply the multivariate student-t Markov Switching model and the\nMultiple-CoVaR (CoES) risk measures introduced in Bernardi et. al. (2013b) to\naccount for both the known stylised characteristics of the data and the\ncontemporaneous joint distress events affecting financial sectors. Our\nempirical investigation finds that banks appear to be the major source of risk\nfor all the remaining sectors, followed by the financial services and the\ninsurance sectors, showing that insurance sector significantly contributes as\nwell to the overall risk. Moreover, we find that the role of each sector in\ncontributing to other sectors distress evolves over time accordingly to the\ncurrent predominant financial condition, implying different interconnection\nstrength.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.6408v2"
    },
    {
        "title": "Elimination of systemic risk in financial networks by means of a\n  systemic risk transaction tax",
        "authors": [
            "Sebastian Poledna",
            "Stefan Thurner"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  Financial markets are exposed to systemic risk (SR), the risk that a major\nfraction of the system ceases to function, and collapses. It has recently\nbecome possible to quantify SR in terms of underlying financial networks where\nnodes represent financial institutions, and links capture the size and maturity\nof assets (loans), liabilities, and other obligations, such as derivatives. We\ndemonstrate that it is possible to quantify the share of SR that individual\nliabilities within a financial network contribute to the overall SR. We use\nempirical data of nationwide interbank liabilities to show that the marginal\ncontribution to overall SR of liabilities for a given size varies by a factor\nof a thousand. We propose a tax on individual transactions that is proportional\nto their marginal contribution to overall SR. If a transaction does not\nincrease SR it is tax-free. With an agent-based model (CRISIS macro-financial\nmodel) we demonstrate that the proposed \"Systemic Risk Tax\" (SRT) leads to a\nself-organised restructuring of financial networks that are practically free of\nSR. The SRT can be seen as an insurance for the public against costs arising\nfrom cascading failure. ABM predictions are shown to be in remarkable agreement\nwith the empirical data and can be used to understand the relation of credit\nrisk and SR.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.8026v3"
    },
    {
        "title": "Default Probability Estimation via Pair Copula Constructions",
        "authors": [
            "Luciana Dalla Valle",
            "Maria Elena De Giuli",
            "Claudia Tarantola",
            "Claudio Manelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In this paper we present a novel approach for firm default probability\nestimation. The methodology is based on multivariate contingent claim analysis\nand pair copula constructions. For each considered firm, balance sheet data are\nused to assess the asset value, and to compute its default probability. The\nasset pricing function is expressed via a pair copula construction, and it is\napproximated via Monte Carlo simulations. The methodology is illustrated\nthrough an application to the analysis of both operative and defaulted firms.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.1309v3"
    },
    {
        "title": "Set-valued shortfall and divergence risk measures",
        "authors": [
            "Çağın Ararat",
            "Andreas H. Hamel",
            "Birgit Rudloff"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  Risk measures for multivariate financial positions are studied in a\nutility-based framework. Under a certain incomplete preference relation,\nshortfall and divergence risk measures are defined as the optimal values of\nspecific set minimization problems. The dual relationship between these two\nclasses of multivariate risk measures is constructed via a recent Lagrange\nduality for set optimization. In particular, it is shown that a shortfall risk\nmeasure can be written as an intersection over a family of divergence risk\nmeasures indexed by a scalarization parameter. Examples include set-valued\nversions of the entropic risk measure and the average value at risk. As a\nsecond step, the minimization of these risk measures subject to trading\nopportunities is studied in a general convex market in discrete time. The\noptimal value of the minimization problem, called the market risk measure, is\nalso a set-valued risk measure. A dual representation for the market risk\nmeasure that decomposes the effects of the original risk measure and the\nfrictions of the market is proved.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.4905v2"
    },
    {
        "title": "On a Transform Method for the Efficient Computation of Conditional VaR\n  (and VaR) with Application to Loss Models with Jumps and Stochastic\n  Volatility",
        "authors": [
            "Alessandro Ramponi"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In this paper we consider Fourier transform techniques to efficiently compute\nthe Value-at-Risk and the Conditional Value-at-Risk of an arbitrary loss random\nvariable, characterized by having a computable generalized characteristic\nfunction. We exploit the property of these risk measures of being the solution\nof an elementary optimization problem of convex type in one dimension for which\nFast and Fractional Fourier transform can be implemented. An application to\nunivariate loss models driven by L\\'{e}vy or stochastic volatility risk factors\ndynamic is finally reported.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.1072v1"
    },
    {
        "title": "A General Duality Relation with Applications in Quantitative Risk\n  Management",
        "authors": [
            "Raphael Hauser",
            "Sergey Shahverdyan",
            "Paul Embrechts"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  A fundamental problem in risk management is the robust aggregation of\ndifferent sources of risk in a situation where little or no data are available\nto infer information about their dependencies. A popular approach to solving\nthis problem is to formulate an optimization problem under which one maximizes\na risk measure over all multivariate distributions that are consistent with the\navailable data. In several special cases of such models, there exist dual\nproblems that are easier to solve or approximate, yielding robust bounds on the\naggregated risk. In this chapter we formulate a general optimization problem,\nwhich can be seen as a doubly infinite linear programming problem, and we show\nthat the associated dual generalizes several well known special cases and\nextends to new risk management models we propose.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.0852v1"
    },
    {
        "title": "An initial approach to Risk Management of Funding Costs",
        "authors": [
            "Damiano Brigo",
            "Cyril Durand"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In this note we sketch an initial tentative approach to funding costs\nanalysis and management for contracts with bilateral counterparty risk in a\nsimplified setting. We depart from the existing literature by analyzing the\nissue of funding costs and benefits under the assumption that the associated\nrisks cannot be hedged properly. We also model the treasury funding spread by\nmeans of a stochastic Weighted Cost of Funding Spread (WCFS) which helps\ndescribing more realistic financing policies of a financial institution. We\nelaborate on some limitations in replication-based Funding / Credit Valuation\nAdjustments we worked on ourselves in the past, namely CVA, DVA, FVA and\nrelated quantities as generally discussed in the industry. We advocate as a\ndifferent possibility, when replication is not possible, the analysis of the\nfunding profit and loss distribution and explain how long term funding spreads,\nwrong way risk and systemic risk are generally overlooked in most of the\ncurrent literature on risk measurement of funding costs. As a matter of initial\nillustration, we discuss in detail the funding management of interest rate\nswaps with bilateral counterparty risk in the simplified setup of our framework\nthrough numerical examples and via a few simplified assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2034v1"
    },
    {
        "title": "Verification of internal risk measure estimates",
        "authors": [
            "Mark H. A. Davis"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  This paper concerns sequential computation of risk measures for financial\ndata and asks how, given a risk measurement procedure, we can tell whether the\nanswers it produces are `correct'. We draw the distinction between `external'\nand `internal' risk measures and concentrate on the latter, where we observe\ndata in real time, make predictions and observe outcomes. It is argued that\nevaluation of such procedures is best addressed from the point of view of\nprobability forecasting or Dawid's theory of `prequential statistics' [Dawid,\nJRSS(A)1984]. We introduce a concept of `calibration' of a risk measure in a\ndynamic setting, following the precepts of Dawid's weak and strong prequential\nprinciples, and examine its application to quantile forecasting (VaR -- value\nat risk) and to mean estimation (applicable to CVaR -- expected shortfall). The\nrelationship between these ideas and `elicitability' [Gneiting, JASA 2011] is\nexamined. We show in particular that VaR has special properties not shared by\nany other risk measure. Turning to CVaR we argue that its main deficiency is\nthe unquantifiable tail dependence of estimators. In a final section we show\nthat a simple data-driven feedback algorithm can produce VaR estimates on\nfinancial data that easily pass both the consistency test and a further\nnewly-introduced statistical test for independence of a binary sequence.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4382v2"
    },
    {
        "title": "Risk in a large claims insurance market with bipartite graph structure",
        "authors": [
            "Oliver Kley",
            "Claudia Kluppelberg",
            "Gesine Reinert"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We model the influence of sharing large exogeneous losses to the reinsurance\nmarket by a bipartite graph. Using Pareto-tailed claims and multivariate\nregular variation we obtain asymptotic results for the Value-at-Risk and the\nConditional Tail Expectation. We show that the dependence on the network\nstructure plays a fundamental role in their asymptotic behaviour. As is\nwell-known in a non-network setting, if the Pareto exponent is larger than 1,\nthen for the individual agent (reinsurance company) diversification is\nbeneficial, whereas when it is less than 1, concentration on a few objects is\nthe better strategy. An additional aspect of this paper is the amount of\nuninsured losses which have to be convered by society. In the situation of\nnetworks of agents, in our setting diversification is never detrimental\nconcerning the amount of uninsured losses. If the Pareto-tailed claims have\nfinite mean, diversification turns out to be never detrimental, both for\nsociety and for individual agents. In contrast, if the Pareto-tailed claims\nhave infinite mean, a conflicting situation may arise between the incentives of\nindividual agents and the interest of some regulator to keep risk for society\nsmall. We explain the influence of the network structure on diversification\neffects in different network scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.8671v3"
    },
    {
        "title": "Signs of dependence and heavy tails in non-life insurance data",
        "authors": [
            "Jonas Alm"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In this paper we study data from the yearly reports the four major Swedish\nnon-life insurers have sent to the Swedish Financial Supervisory Authority\n(FSA). We aim at finding marginal distributions of, and dependence between,\nlosses on the five largest lines of business (LoBs) in order to create models\nfor Solvency Capital Requirement (SCR) calculation. We try to use data in an\noptimal way by sensibly defining an accounting year loss in terms of actuarial\nliability predictions, and by pooling observations from several companies when\npossible to decrease the uncertainty about the underlying distributions and\ntheir parameters. We find that dependence between LoBs is weaker in our data\nthan what is assumed in the Solvency II standard formula. We also find\ndependence between companies that may affect financial stability, and must be\ntaken into account when estimating loss distribution parameters. Moreover, we\ndiscuss under what circumstances an insurer is better (or worse) off using an\ninternal model for SCR calculation instead of the standard formula.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.00833v1"
    },
    {
        "title": "Systemic Risk Management in Financial Networks with Credit Default Swaps",
        "authors": [
            "Matt V. Leduc",
            "Sebastian Poledna",
            "Stefan Thurner"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We study insolvency cascades in an interbank system when banks are allowed to\ninsure their loans with credit default swaps (CDS) sold by other banks. We show\nthat, by properly shifting financial exposures from one institution to another,\na CDS market can be designed to rewire the network of interbank exposures in a\nway that makes it more resilient to insolvency cascades. A regulator can use\ninformation about the topology of the interbank network to devise a systemic\ninsurance surcharge that is added to the CDS spread. CDS contracts are thus\neffectively penalized according to how much they contribute to increasing\nsystemic risk. CDS contracts that decrease systemic risk remain untaxed. We\nsimulate this regulated CDS market using an agent-based model (CRISIS\nmacro-financial model) and we demonstrate that it leads to an interbank system\nthat is more resilient to insolvency cascades.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.02156v2"
    },
    {
        "title": "Credit risk: Taking fluctuating asset correlations into account",
        "authors": [
            "Thilo A. Schmitt",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  In structural credit risk models, default events and the ensuing losses are\nboth derived from the asset values at maturity. Hence it is of utmost\nimportance to choose a distribution for these asset values which is in\naccordance with empirical data. At the same time, it is desirable to still\npreserve some analytical tractability. We achieve both goals by putting forward\nan ensemble approach for the asset correlations. Consistently with the data, we\nview them as fluctuating quantities, for which we may choose the average\ncorrelation as homogeneous. Thereby we can reduce the number of parameters to\ntwo, the average correlation between assets and the strength of the\nfluctuations around this average value. Yet, the resulting asset value\ndistribution describes the empirical data well. This allows us to derive the\ndistribution of credit portfolio losses. With Monte-Carlo simulations for the\nValue at Risk and Expected Tail Loss we validate the assumptions of our\napproach and demonstrate the necessity of taking fluctuating correlations into\naccount.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.03015v1"
    },
    {
        "title": "On a law of large numbers for insurance risks",
        "authors": [
            "Yumiharu Nakano"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  This note presents a kind of the strong law of large numbers for an insurance\nrisk caused by a single catastrophic event rather than by an accumulation of\nindependent and identically distributed risks. We derive this result by a large\ndiversification effect resulting from optimal allocation of the risk to many\nreinsurers or investors.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.03171v1"
    },
    {
        "title": "Crunching Mortality and Life Insurance Portfolios with extended\n  CreditRisk+",
        "authors": [
            "Jonas Hirz",
            "Uwe Schmock",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Using an extended version of the credit risk model CreditRisk+, we develop a\nflexible framework with numerous applications amongst which we find stochastic\nmortality modelling, forecasting of death causes as well as profit and loss\nmodelling of life insurance and annuity portfolios which can be used in\n(partial) internal models under Solvency II. Yet, there exists a fast and\nnumerically stable algorithm to derive loss distributions exactly, even for\nlarge portfolios. We provide various estimation procedures based on publicly\navailable data. Compared to the Lee-Carter model, we have a more flexible\nframework, get tighter bounds and can directly extract several sources of\nuncertainty. Straight-forward model validation techniques are available.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04557v4"
    },
    {
        "title": "Sharp convex bounds on the aggregate sums--An alternative proof",
        "authors": [
            "Chuancun Yin",
            "Dan Zhu"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  It is well known that a random vector with given marginal distributions is\ncomonotonic if and only if it has the largest sum with respect to the convex\norder [ Kaas, Dhaene, Vyncke, Goovaerts, Denuit (2002), A simple geometric\nproof that comonotonic risks have the convex-largest sum, ASTIN Bulletin 32,\n71-80. Cheung (2010), Characterizing a comonotonic random vector by the\ndistribution of the sum of its components, Insurance: Mathematics and Economics\n47(2), 130-136] and that a random vector with given marginal distributions is\nmutually exclusive if and only if it has the minimal convex sum [Cheung and Lo\n(2014), Characterizing mutual exclusivity as the strongest negative\nmultivariate dependence structure, Insurance: Mathematics and Economics 55,\n180-190]. In this note, we give a new proof of this two results using the\ntheories of distortion risk measure and expected utility.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.05373v2"
    },
    {
        "title": "On the properties of the Lambda value at risk: robustness, elicitability\n  and consistency",
        "authors": [
            "Matteo Burzoni",
            "Ilaria Peri",
            "Chiara Maria Ruffo"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Recently, financial industry and regulators have enhanced the debate on the\ngood properties of a risk measure. A fundamental issue is the evaluation of the\nquality of a risk estimation. On the one hand, a backtesting procedure is\ndesirable for assessing the accuracy of such an estimation and this can be\nnaturally achieved by elicitable risk measures. For the same objective, an\nalternative approach has been introduced by Davis (2016) through the so-called\nconsistency property. On the other hand, a risk estimation should be less\nsensitive with respect to small changes in the available data set and exhibit\nqualitative robustness. A new risk measure, the Lambda value at risk (Lambda\nVaR), has been recently proposed by Frittelli et al. (2014), as a\ngeneralization of VaR with the ability to discriminate the risk among P&L\ndistributions with different tail behaviour. In this article, we show that\nLambda VaR also satisfies the properties of robustness, elicitability and\nconsistency under some conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.09491v2"
    },
    {
        "title": "Clearing algorithms and network centrality",
        "authors": [
            "Christoph Siebenbrunner"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  I show that the solution of a standard clearing model commonly used in\ncontagion analyses for financial systems can be expressed as a specific form of\na generalized Katz centrality measure under conditions that correspond to a\nsystem-wide shock. This result provides a formal explanation for earlier\nempirical results which showed that Katz-type centrality measures are closely\nrelated to contagiousness. It also allows assessing the assumptions that one is\nmaking when using such centrality measures as systemic risk indicators. I\nconclude that these assumptions should be considered too strong and that, from\na theoretical perspective, clearing models should be given preference over\ncentrality measures in systemic risk analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00284v1"
    },
    {
        "title": "An Explicit Default Contagion Model and Its Application to Credit\n  Derivatives Pricing",
        "authors": [
            "Dianfa Chen",
            "Jun Deng",
            "Jianfen Feng",
            "Bin Zou"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We propose a novel credit default model that takes into account the impact of\nmacroeconomic information and contagion effect on the defaults of obligors. We\nuse a set-valued Markov chain to model the default process, which is the set of\nall defaulted obligors in the group. We obtain analytic characterizations for\nthe default process, and use them to derive pricing formulas in explicit forms\nfor synthetic collateralized debt obligations (CDOs). Furthermore, we use\nmarket data to calibrate the model and conduct numerical studies on the tranche\nspreads of CDOs. We find evidence to support that systematic default risk\ncoupled with default contagion could have the leading component of the total\ndefault risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.06285v3"
    },
    {
        "title": "Extreme portfolio loss correlations in credit risk",
        "authors": [
            "Andreas Mühlbacher",
            "Thomas Guhr"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  The stability of the financial system is associated with systemic risk\nfactors such as the concurrent default of numerous small obligors. Hence it is\nof utmost importance to study the mutual dependence of losses for different\ncreditors in the case of large, overlapping credit portfolios. We analytically\ncalculate the multivariate joint loss distribution of several credit portfolios\non a non-stationary market. To take fluctuating asset correlations into account\nwe use an random matrix approach which preserves, as a much appreciated side\neffect, analytical tractability and drastically reduces the number of\nparameters. We show that for two disjoint credit portfolios diversification\ndoes not work in a correlated market. Additionally we find large concurrent\nportfolio losses to be rather likely. We show that significant correlations of\nthe losses emerge not only for large portfolios with thousands of credit\ncontracts but also for small portfolios consisting of a few credit contracts\nonly. Furthermore we include subordination levels, which were established in\ncollateralized debt obligations to protect the more senior tranches from high\nlosses. We analytically corroborate the observation that an extreme loss of the\nsubordinated creditor is likely to also yield a large loss of the senior\ncreditor.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.09809v1"
    },
    {
        "title": "Generating VaR scenarios with product beta distributions",
        "authors": [
            "Dietmar Pfeifer",
            "Olena Ragulina"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We propose a Monte Carlo simulation method to generate stress tests by VaR\nscenarios under Solvency II for dependent risks on the basis of observed data.\nThis is of particular interest for the construction of Internal Models and\nrequirements on evaluation processes formulated in the Commission Delegated\nRegulation. The approach is based on former work on partition-ofunity copulas,\nhowever with a direct scenario estimation of the joint density by product beta\ndistributions after a suitable transformation of the original data.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02457v2"
    },
    {
        "title": "Dynamic Initial Margin via Chebyshev Tensors",
        "authors": [
            "Ignacio Ruiz",
            "Mariano Zeron"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We present two methods, based on Chebyshev tensors, to compute dynamic\nsensitivities of financial instruments within a Monte Carlo simulation. These\nmethods are implemented and run in a Monte Carlo engine to compute Dynamic\nInitial Margin as defined by ISDA (SIMM). We show that the levels of accuracy,\nspeed and implementation efforts obtained, compared to the benchmark (DIM\nobtained calling pricing functions such as are found in risk engines), are\nbetter than those obtained by alternative methods presented in the literature,\nsuch as regressions (\\cite{Zhu Chan}) and Deep Neural Nets (\\cite{DNNs IM}).\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08221v2"
    },
    {
        "title": "Risk sharing for capital requirements with multidimensional security\n  markets",
        "authors": [
            "Felix-Benedikt Liebrich",
            "Gregor Svindland"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We consider the risk sharing problem for capital requirements induced by\ncapital adequacy tests and security markets. The agents involved in the sharing\nprocedure may be heterogeneous in that they apply varying capital adequacy\ntests and have access to different security markets. We discuss conditions\nunder which there exists a representative agent. Thereafter, we study two\nframeworks of capital adequacy more closely, polyhedral constraints and\ndistribution based constraints. We prove existence of optimal risk allocations\nand equilibria within these frameworks and elaborate on their robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10015v1"
    },
    {
        "title": "Monotone Sharpe ratios and related measures of investment performance",
        "authors": [
            "Mikhail Zhitlukhin"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We introduce a new measure of performance of investment strategies, the\nmonotone Sharpe ratio. We study its properties, establish a connection with\ncoherent risk measures, and obtain an efficient representation for using in\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10193v2"
    },
    {
        "title": "A framework for simulating systemic risk and its application to the\n  South African banking sector",
        "authors": [
            "Nadine M Walters",
            "Conrad Beyers",
            "Gusti van Zyl",
            "Rolf van den Heever"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We present a network-based framework for simulating systemic risk that\nconsiders shock propagation in banking systems. In particular, the framework\nallows the modeller to reflect a top-down framework where a shock to one bank\nin the system affects the solvency and liquidity position of other banks,\nthrough systemic market risks and consequential liquidity strains. We\nillustrate the framework with an application using South African bank balance\nsheet data. Spikes in simulated assessments of systemic risk agree closely with\nspikes in documented subjective assessments of this risk. This indicates that\nnetwork models can be useful for monitoring systemic risk levels. The model\nresults are sensitive to liquidity risk and market sentiment and therefore the\nrelated parameters are important considerations when using a network approach\nto systemic risk modelling.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04223v1"
    },
    {
        "title": "A sparse grid approach to balance sheet risk measurement",
        "authors": [
            "Cyril Bénézet",
            "Jérémie Bonnefoy",
            "Jean-François Chassagneux",
            "Shuoqing Deng",
            "Camilo Garcia Trillos",
            "Lionel Lenôtre"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this work, we present a numerical method based on a sparse grid\napproximation to compute the loss distribution of the balance sheet of a\nfinancial or an insurance company. We first describe, in a stylised way, the\nassets and liabilities dynamics that are used for the numerical estimation of\nthe balance sheet distribution. For the pricing and hedging model, we chose a\nclassical Black & Scholes model with a stochastic interest rate following a\nHull & White model. The risk management model describing the evolution of the\nparameters of the pricing and hedging model is a Gaussian model. The new\nnumerical method is compared with the traditional nested simulation approach.\nWe review the convergence of both methods to estimate the risk indicators under\nconsideration. Finally, we provide numerical results showing that the sparse\ngrid approach is extremely competitive for models with moderate dimension.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.08706v1"
    },
    {
        "title": "Psychological model of the investor and manager behavior in risk",
        "authors": [
            "O. A. Malafeyev",
            "A. N. Malova",
            "A. E. Tsybaeva"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  All people have to make risky decisions in everyday life. And we do not know\nhow true they are. But is it possible to mathematically assess the correctness\nof our choice? This article discusses the model of decision making under risk\non the example of project management. This is a game with two players, one of\nwhich is Investor, and the other is the Project Manager. Each player makes a\nrisky decision for himself, based on his past experience. With the help of a\nmathematical model, the players form a level of confidence, depending on who\nthe player accepts the strategy or does not accept. The project manager\nassesses the costs and compares them with the level of confidence. An investor\nevaluates past results. Also visit the case where the strategy of the player\naccepts the part.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.08772v1"
    },
    {
        "title": "SlideVaR: a risk measure with variable risk attitudes",
        "authors": [
            "Wentao Hu"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  To find a trade-off between profitability and prudence, financial\npractitioners need to choose appropriate risk measures. Two key points are:\nFirstly, investors' risk attitudes under uncertainty conditions should be an\nimportant reference for risk measures. Secondly, risk attitudes are not\nabsolute. For different market performance, investors have different risk\nattitudes. We proposed a new risk measure named SlideVaR which sufficiently\nreflects the different subjective attitudes of investors and the impact of\nmarket changes on investors' attitudes. We proposed the concept of risk-tail\nregion and risk-tail sub-additivity and proved that SlideVaR satisfies several\nimportant mathematical properties. Moreover, SlideVaR has a simple and\nintuitive form of expression for practical application. Several simulate and\nempirical computations show that SlideVaR has obvious advantages in markets\nwhere the state changes frequently.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11855v1"
    },
    {
        "title": "On the Concavity of Expected Shortfall",
        "authors": [
            "Mikhail Tselishchev"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  It is well known that Expected Shortfall (also called Average Value-at-Risk)\nis a convex risk measure, i. e. Expected Shortfall of a convex linear\ncombination of arbitrary risk positions is not greater than a convex linear\ncombination with the same weights of Expected Shortfalls of the same risk\npositions. In this short paper we prove that Expected Shortfall is a concave\nrisk measure with respect to probability distributions, i. e. Expected\nShortfall of a finite mixture of arbitrary risk positions is not lower than the\nlinear combination of Expected Shortfalls of the same risk positions (with the\nsame weights as in the mixture).\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00640v1"
    },
    {
        "title": "Quantifying Life Insurance Risk using Least-Squares Monte Carlo",
        "authors": [
            "Claus Baumgart",
            "Johannes Krebs",
            "Robert Lempertseder",
            "Oliver Pfaffel"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  This article presents a stochastic framework to quantify the biometric risk\nof an insurance portfolio in solvency regimes such as Solvency II or the Swiss\nSolvency Test (SST). The main difficulty in this context constitutes in the\nproper representation of long term risks in the profit-loss distribution over a\none year horizon. This will be resolved by using least-squares Monte Carlo\nmethods to quantify the impact of new experience on the annual re-valuation of\nthe portfolio. Therefore our stochastic model can be seen as an example for an\ninternal model, as allowed under Solvency II or the SST. Since our model does\nnot rely upon nested simulations it is computationally fast and easy to\nimplement.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03951v1"
    },
    {
        "title": "A hierarchical reserving model for reported non-life insurance claims",
        "authors": [
            "Jonas Crevecoeur",
            "Jens Robben",
            "Katrien Antonio"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Traditional non-life reserving models largely neglect the vast amount of\ninformation collected over the lifetime of a claim. This information includes\ncovariates describing the policy, claim cause as well as the detailed history\ncollected during a claim's development over time. We present the hierarchical\nreserving model as a modular framework for integrating a claim's history and\nclaim-specific covariates into the development process. Hierarchical reserving\nmodels decompose the joint likelihood of the development process over time.\nMoreover, they are tailored to the portfolio at hand by adding a layer to the\nmodel for each of the events registered during the development of a claim (e.g.\nsettlement, payment). Layers are modelled with statistical learning (e.g.\ngeneralized linear models) or machine learning methods (e.g. gradient boosting\nmachines) and use claim-specific covariates. As a result of its flexibility,\nthis framework incorporates many existing reserving models, ranging from\naggregate models designed for run-off triangles to individual models using\nclaim-specific covariates. This connection allows us to develop a data-driven\nstrategy for choosing between aggregate and individual reserving; an important\ndecision for reserving practitioners. We illustrate our method with a case\nstudy on a real insurance data set and deduce new insights in the covariates\ndriving the development of claims. Moreover, we evaluate the method's\nperformance on a large number of simulated portfolios representing several\nrealistic development scenarios and demonstrate the flexibility and robustness\nof the hierarchical reserving model.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.12692v3"
    },
    {
        "title": "Inf-convolution and optimal risk sharing with countable sets of risk\n  measures",
        "authors": [
            "Marcelo Brutti Righi",
            "Marlon Ruoso Moresco"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  The inf-convolution of risk measures is directly related to risk sharing and\ngeneral equilibrium, and it has attracted considerable attention in\nmathematical finance and insurance problems. However, the theory is restricted\nto finite sets of risk measures. This study extends the inf-convolution of risk\nmeasures in its convex-combination form to a countable (not necessarily finite)\nset of alternatives. The intuitive meaning of this approach is to represent a\ngeneralization of the current finite convex weights to the countable case.\nSubsequently, we extensively generalize known properties and results to this\nframework. Specifically, we investigate the preservation of properties, dual\nrepresentations, optimal allocations, and self-convolution.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05797v4"
    },
    {
        "title": "Convex Risk Measures based on Divergence",
        "authors": [
            "Paul Dommel",
            "Alois Pichler"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Risk measures connect probability theory or statistics to optimization,\nparticularly to convex optimization. They are nowadays standard in applications\nof finance and in insurance involving risk aversion. This paper investigates a\nwide class of risk measures on Orlicz spaces. The characterizing function\ndescribes the decision maker's risk assessment towards increasing losses. We\nlink the risk measures to a crucial formula developed by Rockafellar for the\nAverage Value-at-Risk based on convex duality, which is fundamental in\ncorresponding optimization problems. We characterize the dual and provide\ncomplementary representations.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.07648v2"
    },
    {
        "title": "Complex risk statistics with scenario analysis",
        "authors": [
            "Fei Sun",
            "Yichuan Dong"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Complex risk is a critical factor for both intelligent systems and risk\nmanagement. In this paper, we consider a special class of risk statistics,\nnamed complex risk statistics. Our result provides a new approach for\naddressing complex risk, especially in deep neural networks. By further\ndeveloping the properties related to complex risk statistics, we are able to\nderive dual representation for such risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.09255v3"
    },
    {
        "title": "Predicting tail events in a RIA-EVT-Copula framework",
        "authors": [
            "Wei-Zhen Li",
            "Jin-Rui Zhai",
            "Zhi-Qiang Jiang",
            "Gang-Jin Wang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Predicting the occurrence of tail events is of great importance in financial\nrisk management. By employing the method of peak-over-threshold (POT) to\nidentify the financial extremes, we perform a recurrence interval analysis\n(RIA) on these extremes. We find that the waiting time between consecutive\nextremes (recurrence interval) follow a $q$-exponential distribution and the\nsizes of extremes above the thresholds (exceeding size) conform to a\ngeneralized Pareto distribution. We also find that there is a significant\ncorrelation between recurrence intervals and exceeding sizes. We thus model the\njoint distribution of recurrence intervals and exceeding sizes through\nconnecting the two corresponding marginal distributions with the Frank and AMH\ncopula functions, and apply this joint distribution to estimate the hazard\nprobability to observe another extreme in $\\Delta t$ time since the last\nextreme happened $t$ time ago. Furthermore, an extreme predicting model based\non RIA-EVT-Copula is proposed by applying a decision-making algorithm on the\nhazard probability. Both in-sample and out-of-sample tests reveal that this new\nextreme forecasting framework has better performance in prediction comparing\nwith the forecasting model based on the hazard probability only estimated from\nthe distribution of recurrence intervals. Our results not only shed a new light\non understanding the occurring pattern of extremes in financial markets, but\nalso improve the accuracy to predict financial extremes for risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03190v2"
    },
    {
        "title": "The PCL Framework: A strategic approach to comprehensive risk management\n  in response to climate change impacts",
        "authors": [
            "Youssef Nassef"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  The PCL framework provides a comprehensive climate risk management approach\ngrounded in the assessment of societal values of financial and non-financial\nloss tolerability. The framework optimizes response action across three main\nclusters, namely preemptive adaptation (P) or risk reduction, contingent\narrangements (C), and loss acceptance (L); without a predetermined hierarchy\nacross them. The PCL Framework aims at including the three clusters of outlay\nwithin a single continuum, and with the main policy outcome being a balanced\nportfolio of actions across the three clusters by way of an optimization\nmodule, such that the aggregate outlay is optimized in the long-term. It is\nproposed that the approach be applied separately for each hazard to which the\ntarget community is exposed. While it is currently applied to climate-related\nrisk management, the methodology can be repurposed for use in other contexts\nwhere societal buy-in is central.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06144v1"
    },
    {
        "title": "Risk Modelling on Liquidations with Lévy Processes",
        "authors": [
            "Aili Zhang",
            "Ping Chen",
            "Shuanming Li",
            "Wenyuan Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  It has been decades since the academic world of ruin theory defined the\ninsolvency of an insurance company as the time when its surplus falls below\nzero. This simplification, however, needs careful adaptions to imitate the\nreal-world liquidation process. Inspired by Broadie et al. (2007) and Li et al.\n(2020), this paper uses a three-barrier model to describe the financial stress\ntowards bankruptcy of an insurance company. The financial status of the insurer\nis divided into solvent, insolvent and liquidated three states, where the\ninsurer's surplus process at the state of solvent and insolvent is modelled by\ntwo spectrally negative L\\'{e}vy processes, which have been taken as good\ncandidates to model insurance risks. We provide a rigorous definition of the\ntime of liquidation ruin in this three-barrier model. By adopting the\ntechniques of excursions in the fluctuation theory, we study the joint\ndistribution of the time of liquidation, the surplus at liquidation and the\nhistorical high of the surplus until liquidation, which generalizes the known\nresults on the classical expected discounted penalty function in Gerber and\nShiu (1998). The results have semi-explicit expressions in terms of the scale\nfunctions and the L\\'{e}vy triplets associated with the two underlying L\\'{e}vy\nprocesses. The special case when the two underlying L\\'{e}vy processes coincide\nwith each other is also studied, where our results are expressed compactly via\nonly the scale functions. The corresponding results have good consistency with\nthe existing literatures on Parisian ruin with (or without) a lower barrier in\nLandriault et al. (2014), Baurdoux et al. (2016) and Frostig and Keren-Pinhasik\n(2019). Besides, numerical examples are provided to illustrate the underlying\nfeatures of liquidation ruin.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.01426v1"
    },
    {
        "title": "Analytical scores for stress scenarios",
        "authors": [
            "Pierre Cohort",
            "Jacopo Corbetta",
            "Ismail Laachir"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  In this work, inspired by the Archer-Mouy-Selmi approach, we present two\nmethodologies for scoring the stress test scenarios used by CCPs for sizing\ntheir Default Funds. These methodologies can be used by risk managers to\ncompare different sets of scenarios and could be particularly useful when\nevaluating the relevance of adding new scenarios to a pre-existing set.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.02567v1"
    },
    {
        "title": "Computation of bonus in multi-state life insurance",
        "authors": [
            "Jamaal Ahmad",
            "Kristian Buchardt",
            "Christian Furrer"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We consider computation of market values of bonus payments in multi-state\nwith-profit life insurance. The bonus scheme consists of additional benefits\nbought according to a dividend strategy that depends on the past realization of\nfinancial risk, the current individual insurance risk, the number of additional\nbenefits currently held, and so-called portfolio-wide means describing the\nshape of the insurance business. We formulate numerical procedures that\nefficiently combine simulation of financial risk with classic methods for the\noutstanding insurance risk. Special attention is given to the case where the\nnumber of additional benefits bought only depends on the financial risk.\nMethods and results are illustrated via a numerical example.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04051v5"
    },
    {
        "title": "Modeling and measuring incurred claims risk liabilities for a multi-line\n  property and casualty insurer",
        "authors": [
            "Carlos Andrés Araiza Iturria",
            "Frédéric Godin",
            "Mélina Mailhot"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We propose a stochastic model allowing property and casualty insurers with\nmultiple business lines to measure their liabilities for incurred claims risk\nand calculate associated capital requirements. Our model includes many\ndesirable features which enable reproducing empirical properties of loss ratio\ndynamics. For instance, our model integrates a double generalized linear model\nrelying on accident semester and development lag effects to represent both the\nmean and dispersion of loss ratio distributions, an autocorrelation structure\nbetween loss ratios of the various development lags, and a hierarchical copula\nmodel driving the dependence across the various business lines. The model\nallows for a joint simulation of loss triangles and the quantification of the\noverall portfolio risk through risk measures. Consequently, a diversification\nbenefit associated to the economic capital requirements can be measured, in\naccordance with IFRS 17 standards which allow for the recognition of such\nbenefit. The allocation of capital across business lines based on the Euler\nallocation principle is then illustrated. The implementation of our model is\nperformed by estimating its parameters based on a car insurance data obtained\nfrom the General Insurance Statistical Agency (GISA), and by conducting\nnumerical simulations whose results are then presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.07068v1"
    },
    {
        "title": "Adjusted Expected Shortfall",
        "authors": [
            "Matteo Burzoni",
            "Cosimo Munari",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We introduce and study the main properties of a class of convex risk measures\nthat refine Expected Shortfall by simultaneously controlling the expected\nlosses associated with different portions of the tail distribution. The\ncorresponding adjusted Expected Shortfalls quantify risk as the minimum amount\nof capital that has to be raised and injected into a financial position $X$ to\nensure that Expected Shortfall $ES_p(X)$ does not exceed a pre-specified\nthreshold $g(p)$ for every probability level $p\\in[0,1]$. Through the choice of\nthe benchmark risk profile $g$ one can tailor the risk assessment to the\nspecific application of interest. We devote special attention to the study of\nrisk profiles defined by the Expected Shortfall of a benchmark random loss, in\nwhich case our risk measures are intimately linked to second-order stochastic\ndominance.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.08829v2"
    },
    {
        "title": "Tile test for back-testing risk evaluation",
        "authors": [
            "Gilles Zumbach"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  A new test for measuring the accuracy of financial market risk estimations is\nintroduced. It is based on the probability integral transform (PIT) of the ex\npost realized returns using the ex ante probability distributions underlying\nthe risk estimation. If the forecast is correct, the result of the PIT, that we\ncalled probtile, should be an iid random variable with a uniform distribution.\nThe new test measures the variance of the number of probtiles in a tiling over\nthe whole sample. Using different tilings allow to check the dynamic and the\ndistributional aspect of risk methodologies. The new test is very powerful, and\nnew benchmarks need to be introduced to take into account subtle mean reversion\neffects induced by some risk estimations. The test is applied on 2 data sets\nfor risk horizons of 1 and 10 days. The results show unambiguously the\nimportance of capturing correctly the dynamic of the financial market, and\nexclude some broadly used risk methodologies.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12431v1"
    },
    {
        "title": "Tail-risk protection: Machine Learning meets modern Econometrics",
        "authors": [
            "Bruno Spilak",
            "Wolfgang Karl Härdle"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Tail risk protection is in the focus of the financial industry and requires\nsolid mathematical and statistical tools, especially when a trading strategy is\nderived. Recent hype driven by machine learning (ML) mechanisms has raised the\nnecessity to display and understand the functionality of ML tools. In this\npaper, we present a dynamic tail risk protection strategy that targets a\nmaximum predefined level of risk measured by Value-At-Risk while controlling\nfor participation in bull market regimes. We propose different weak\nclassifiers, parametric and non-parametric, that estimate the exceedance\nprobability of the risk level from which we derive trading signals in order to\nhedge tail events. We then compare the different approaches both with\nstatistical and trading strategy performance, finally we propose an ensemble\nclassifier that produces a meta tail risk protection strategy improving both\ngeneralization and trading performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.03315v4"
    },
    {
        "title": "Deep learning for CVA computations of large portfolios of financial\n  derivatives",
        "authors": [
            "Kristoffer Andersson",
            "Cornelis W. Oosterlee"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  In this paper, we propose a neural network-based method for CVA computations\nof a portfolio of derivatives. In particular, we focus on portfolios consisting\nof a combination of derivatives, with and without true optionality,\n\\textit{e.g.,} a portfolio of a mix of European- and Bermudan-type derivatives.\nCVA is computed, with and without netting, for different levels of WWR and for\ndifferent levels of credit quality of the counterparty. We show that the CVA is\noverestimated with up to 25\\% by using the standard procedure of not adjusting\nthe exercise strategy for the default-risk of the counterparty. For the\nExpected Shortfall of the CVA dynamics, the overestimation was found to be more\nthan 100\\% in some non-extreme cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.13843v1"
    },
    {
        "title": "Quantifying the trade-off between income stability and the number of\n  members in a pooled annuity fund",
        "authors": [
            "Thomas Bernhardt",
            "Catherine Donnelly"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  The number of people who receive a stable income for life from a closed\npooled annuity fund is studied. Income stability is defined as keeping the\nincome within a specified tolerance of the initial income in a fixed proportion\nof future scenarios. The focus is on quantifying the effect of the number of\nmembers, which drives the level of idiosyncratic longevity risk in the fund, on\nthe income stability. To do this, investment returns are held constant and\nsystematic longevity risk is omitted. An analytical expression that closely\napproximates the number of fund members who receive a stable income is derived\nand is seen to be independent of the mortality model. An application of the\nresult is to calculate the length of time for which the pooled annuity fund can\nprovide the desired level of income stability\n",
        "pdf_link": "http://arxiv.org/pdf/2010.16009v1"
    },
    {
        "title": "Adaptive Bernstein Copulas and Risk Management",
        "authors": [
            "Dietmar Pfeifer",
            "Olena Ragulina"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We present a constructive approach to Bernstein copulas with an admissible\ndiscrete skeleton in arbitrary dimensions when the underlying marginal grid\nsizes are smaller than the number of observations. This prevents an overfitting\nof the estimated dependence model and reduces the simulation effort for\nBernstein copulas a lot. In a case study, we compare different approaches of\nBernstein and Gaussian copulas w.r.t. the estimation of risk measures in risk\nmanagement.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.00909v4"
    },
    {
        "title": "Dynamic sensitivities and Initial Margin via Chebyshev Tensors",
        "authors": [
            "Mariano Zeron",
            "Ignacio Ruiz"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  This paper presents how to use Chebyshev Tensors to compute dynamic\nsensitivities of financial instruments within a Monte Carlo simulation. Dynamic\nsensitivities are then used to compute Dynamic Initial Margin as defined by\nISDA (SIMM). The technique is benchmarked against the computation of dynamic\nsensitivities obtained by using pricing functions like the ones found in risk\nengines. We obtain high accuracy and computational gains for FX swaps and\nSpread Options.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.04544v1"
    },
    {
        "title": "Generating unfavourable VaR scenarios with patchwork copulas",
        "authors": [
            "Dietmar Pfeifer",
            "Olena Ragulina"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  The central idea of the paper is to present a general simple patchwork\nconstruction principle for multivariate copulas that create unfavourable VaR\n(i.e. Value at Risk) scenarios while maintaining given marginal distributions.\nThis is of particular interest for the construction of Internal Models in the\ninsurance industry under Solvency II in the European Union. The method is\nexemplified with a 19-dimensional real-life data set of insurance losses.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.06281v5"
    },
    {
        "title": "Smart Close-out Netting",
        "authors": [
            "Akber Datoo",
            "Christopher D. Clack"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Smart Close-out Netting aims to standardise and automate specific operational\naspects of the legal and regulatory processes of close-out netting for\nprudentially regulated financial institutions. This article provides a review,\nanalysis and perspective of these operational processes, their benefits for\nprudentially regulated trading institutions, their current inefficiencies, and\nthe extent to which they are amenable to standardisation and automation. The\nmain concepts of Smart Close-out Netting are introduced, including the use of a\ncontrolled natural language in legal opinions and the use of a data-driven\nframework during netting determination.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.07379v1"
    },
    {
        "title": "A Risk Based approach for the Solvency Capital requirement for Health\n  Plans",
        "authors": [
            "Fabio Baione",
            "Davide Biancalana",
            "Paolo De Angelis"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  The study deals with the assessment of risk measures for Health Plans in\norder to assess the Solvency Capital Requirement. For the estimation of the\nindividual health care expenditure for several episode types, we suggest an\noriginal approach based on a three-part regression model. We propose three\nGeneralized Linear Models (GLM) to assess claim counts, the allocation of each\nclaim to a specific episode and the severity average expenditures respectively.\nOne of the main practical advantages of our proposal is the reduction of the\nregression models compared to a traditional approach, where several two-part\nmodels for each episode types are requested. As most health plans require\nco-payments or co-insurance, considering at this stage the non-linearity\ncondition of the reimbursement function, we adopt a Montecarlo simulation to\nassess the health plan costs. The simulation approach provides the probability\ndistribution of the Net Asset Value of the Health Plan and the estimate of\nseveral risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.09254v1"
    },
    {
        "title": "The short-run impact of COVID-19 on the activity in the insurance\n  industry in the Republic of North Macedonia",
        "authors": [
            "Viktor Stojkoski",
            "Petar Jolakoski",
            "Igor Ivanovski"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  This paper investigates the impact of the COVID-19 pandemic on the insurance\nindustry in the Republic of North Macedonia during the first half of 2020. By\nutilizing seasonal autoregressive models and data for 11 insurance classes, we\nfind that the insurance activity shrank by more than 10% compared to what was\nexpected. The total loss in the industry was, however, much less than the\namount of funds made available by the Insurance Supervision Agency. This was\nbecause the pandemic induced changes in the activity structure - the share of\nMotor vehicles class fell at the expense of the property classes.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.10826v1"
    },
    {
        "title": "Assessing Systemic Risk in the Insurance Sector via Network Theory",
        "authors": [
            "Gian Paolo Clemente",
            "Alessandra Cornaro"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We provide a framework for detecting relevant insurance companies in a\nsystemic risk perspective. Among the alternative methodologies for measuring\nsystemic risk, we propose a complex network approach where insurers are linked\nto form a global interconnected system. We model the reciprocal influence\nbetween insurers calibrating edge weights on the basis of specific risk\nmeasures. Therefore, we provide a suitable network indicator, the Weighted\nEffective Resistance Centrality, able to catch which is the effect of a\nspecific vertex on the network robustness. By means of this indicator, we\nassess the prominence of a company in spreading and receiving risk from the\nothers.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.11394v1"
    },
    {
        "title": "Modeling surrender risk in life insurance: theoretical and experimental\n  insight",
        "authors": [
            "Mark Kiermayer"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Surrender poses one of the major risks to life insurance and a sound modeling\nof its true probability has direct implication on the risk capital demanded by\nthe Solvency II directive. We add to the existing literature by performing\nextensive experiments that present highly practical results for various\nmodeling approaches, including XGBoost, random forest, GLM and neural networks.\nFurther, we detect shortcomings of prevalent model assessments, which are in\nessence based on a confusion matrix. Our results indicate that accurate label\npredictions and a sound modeling of the true probability can be opposing\nobjectives. We illustrate this with the example of resampling. While resampling\nis capable of improving label prediction in rare event settings, such as\nsurrender, and thus is commonly applied, we show theoretically and numerically\nthat models trained on resampled data predict significantly biased event\nprobabilities. Following a probabilistic perspective on surrender, we further\npropose time-dependent confidence bands on predicted mean surrender rates as a\ncomplementary assessment and demonstrate its benefit. This evaluation takes a\nvery practical, going concern perspective, which respects that the composition\nof a portfolio, as well as the nature of underlying risk drivers might change\nover time.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.11590v2"
    },
    {
        "title": "Estimating value at risk and conditional tail expectation for extreme\n  and aggregate risks",
        "authors": [
            "Suman Thapa",
            "Yiqiang Q. Zhao"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper, we investigate risk measures such as value at risk (VaR) and\nthe conditional tail expectation (CTE) of the extreme (maximum and minimum) and\nthe aggregate (total) of two dependent risks. In finance, insurance and the\nother fields, when people invest their money in two or more dependent or\nindependent markets, it is very important to know the extreme and total risk\nbefore the investment. To find these risk measures for dependent cases is quite\nchallenging, which has not been reported in the literature to the best of our\nknowledge. We use the FGM copula for modelling the dependence as it is\nrelatively simple for computational purposes and has empirical successes. The\nmarginal of the risks are considered as exponential and pareto, separately, for\nthe case of extreme risk and as exponential for the case of the total risk. The\neffect of the degree of dependency on the VaR and CTE of the extreme and total\nrisks is analyzed. We also make comparisons for the dependent and independent\nrisks. Moreover, we propose a new risk measure called median of tail (MoT) and\ninvestigate MoT for the extreme and aggregate dependent risks.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.12402v1"
    },
    {
        "title": "Modelling uncertainty in financial tail risk: a forecast combination and\n  weighted quantile approach",
        "authors": [
            "Giuseppe Storti",
            "Chao Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  A novel forecast combination and weighted quantile based tail-risk\nforecasting framework is proposed, aiming to reduce the impact of modelling\nuncertainty in tail-risk forecasting. The proposed approach is based on a\ntwo-step estimation procedure. The first step involves the combination of\nValue-at-Risk (VaR) forecasts at a grid of quantile levels. A range of\nparametric and semi-parametric models is selected as the model universe in the\nforecast combination procedure. The quantile forecast combination weights are\nestimated by optimizing the quantile loss. In the second step, the Expected\nShortfall (ES) is computed as a weighted average of combined quantiles. The\nquantiles weighting structure for ES forecasting is determined by minimizing a\nstrictly consistent joint VaR and ES loss function of the Fissler-Ziegel class.\nThe proposed framework is applied to six stock market indices and its\nforecasting performance is compared to each individual model in the universe, a\nsimple average approach and a weighted quantile approach. The forecasting\nresults support the proposed framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.04918v2"
    },
    {
        "title": "Risk Aggregation under Dependence Uncertainty and an Order Constraint",
        "authors": [
            "Yuyu Chen",
            "Liyuan Lin",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We study the aggregation of two risks when the marginal distributions are\nknown and the dependence structure is unknown, under the additional constraint\nthat one risk is smaller than or equal to the other. Risk aggregation problems\nwith the order constraint are closely related to the recently introduced notion\nof the directional lower (DL) coupling. The largest aggregate risk in concave\norder (thus, the smallest aggregate risk in convex order) is attained by the DL\ncoupling. These results are further generalized to calculate the best-case and\nworst-case values of tail risk measures. In particular, we obtain analytical\nformulas for bounds on Value-at-Risk. Our numerical results suggest that the\nnew bounds on risk measures with the extra order constraint can greatly improve\nthose with full dependence uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.07718v2"
    },
    {
        "title": "Probability Premium and Attitude Towards Probability",
        "authors": [
            "Louis R. Eeckhoudt",
            "Roger J. A. Laeven"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Employing a generalized definition of Pratt (1964) and Arrow's (1965, 1971)\nprobability premium, we introduce a new concept of attitude towards\nprobability. We illustrate in a problem of risk sharing that whether attitude\ntowards probability is a first-order or second-order phenomenon has important\neconomic applications. By developing a local approximation to the probability\npremium, we show that the canonical rank-dependent utility model usually\nexhibits attitude towards probability of first order, whereas under the dual\ntheory with smooth probability weighting functions attitude towards probability\nis a second-order trait.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.00054v1"
    },
    {
        "title": "ESG, Risk, and (Tail) Dependence",
        "authors": [
            "Karoline Bax",
            "Özge Sahin",
            "Claudia Czado",
            "Sandra Paterlini"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  While environmental, social, and governance (ESG) trading activity has been a\ndistinctive feature of financial markets, the debate if ESG scores can also\nconvey information regarding a company's riskiness remains open. Regulatory\nauthorities, such as the European Banking Authority (EBA), have acknowledged\nthat ESG factors can contribute to risk. Therefore, it is important to model\nsuch risks and quantify what part of a company's riskiness can be attributed to\nthe ESG scores. This paper aims to question whether ESG scores can be used to\nprovide information on (tail) riskiness. By analyzing the (tail) dependence\nstructure of companies with a range of ESG scores, that is within an ESG rating\nclass, using high-dimensional vine copula modelling, we are able to show that\nrisk can also depend on and be directly associated with a specific ESG rating\nclass. Empirical findings on real-world data show positive not negligible ESG\nrisks determined by ESG scores, especially during the 2008 crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.07248v2"
    },
    {
        "title": "On the Return Distributions of a Basket of Cryptocurrencies and\n  Subsequent Implications",
        "authors": [
            "Christoph J. Börner",
            "Ingo Hoffmann",
            "Jonas Krettek",
            "Lars M. Kürzinger",
            "Tim Schmitz"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  This paper evaluates and assesses the risk associated with capital allocation\nin cryptocurrencies (CCs). In this regard, we take a basket of 27 CCs and the\nCC index EWCI$^-$ into account. After considering a series of statistical tests\nwe find the stable distribution (SDI) to be the most appropriate to model the\nbody of CCs returns. However, as we find the SDI to possess less favorable\nproperties in the tail area for high quantiles, the generalized Pareto\ndistribution is adapted for a more precise risk assessment. We use a\ncombination of both distributions to calculate the Value at Risk and the\nConditional Value at Risk, indicating two subgroups of CCs with differing risk\ncharacteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.12334v1"
    },
    {
        "title": "A Bayesian realized threshold measurement GARCH framework for financial\n  tail risk forecasting",
        "authors": [
            "Chao Wang",
            "Richard Gerlach"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  This paper proposes an innovative threshold measurement equation to be\nemployed in a Realized-GARCH framework. The proposed framework incorporates a\nnonlinear threshold regression specification to consider the leverage effect\nand model the contemporaneous dependence between the observed realized measure\nand hidden volatility. A Bayesian Markov Chain Monte Carlo method is adapted\nand employed for model estimation, with its validity assessed via a simulation\nstudy. The validity of incorporating the proposed measurement equation in\nRealized-GARCH type models is evaluated via an empirical study, forecasting the\n1% and 2.5% Value-at-Risk and Expected Shortfall on six market indices with two\ndifferent out-of-sample sizes. The proposed framework is shown to be capable of\nproducing competitive tail risk forecasting results in comparison to the GARCH\nand Realized-GARCH type models.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.00288v2"
    },
    {
        "title": "Forecasting VaR and ES using a joint quantile regression and\n  implications in portfolio allocation",
        "authors": [
            "Luca Merlo",
            "Lea Petrella",
            "Valentina Raponi"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper we propose a multivariate quantile regression framework to\nforecast Value at Risk (VaR) and Expected Shortfall (ES) of multiple financial\nassets simultaneously, extending Taylor (2019). We generalize the Multivariate\nAsymmetric Laplace (MAL) joint quantile regression of Petrella and Raponi\n(2019) to a time-varying setting, which allows us to specify a dynamic process\nfor the evolution of both VaR and ES of each asset. The proposed methodology\naccounts for the dependence structure among asset returns. By exploiting the\nproperties of the MAL distribution, we then propose a new portfolio\noptimization method that minimizes the portfolio risk and controls for\nwell-known characteristics of financial data. We evaluate the advantages of the\nproposed approach on both simulated and real data, using weekly returns on\nthree major stock market indices. We show that our method outperforms other\nexisting models and provides more accurate risk measure forecasts compared to\nunivariate ones.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.06518v1"
    },
    {
        "title": "Hierarchical contagions in the interdependent financial network",
        "authors": [
            "William A. Barnett",
            "Xue Wang",
            "Hai-Chuan Xu",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We derive the default cascade model and the fire-sale spillover model in a\nunified interdependent framework. The interactions among banks include not only\ndirect cross-holding, but also indirect dependency by holding mutual assets\noutside the banking system. Using data extracted from the European Banking\nAuthority, we present the interdependency network composed of 48 banks and 21\nasset classes. For the robustness, we employ three methods, called\n$\\textit{Anan}$, $\\textit{Ha\\l{}a}$ and $\\textit{Maxe}$, to reconstruct the\nasset/liability cross-holding network. Then we combine the external portfolio\nholdings of each bank to compute the interdependency matrix. The\ninterdependency network is much denser than the direct cross-holding network,\nshowing the complex latent interaction among banks. Finally, we perform\nmacroprudential stress tests for the European banking system, using the adverse\nscenario in EBA stress test as the initial shock. For different reconstructed\nnetworks, we illustrate the hierarchical cascades and show that the failure\nhierarchies are roughly the same except for a few banks, reflecting the\noverlapping portfolio holding accounts for the majority of defaults. We also\ncalculate systemic vulnerability and individual vulnerability, which provide\nimportant information for supervision and relevant management actions.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.14168v2"
    },
    {
        "title": "Risk contributions of lambda quantiles",
        "authors": [
            "Akif Ince",
            "Ilaria Peri",
            "Silvana Pesenti"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Risk contributions of portfolios form an indispensable part of risk adjusted\nperformance measurement. The risk contribution of a portfolio, e.g., in the\nEuler or Aumann-Shapley framework, is given by the partial derivatives of a\nrisk measure applied to the portfolio profit and loss in direction of the asset\nunits. For risk measures that are not positively homogeneous of degree 1,\nhowever, known capital allocation principles do not apply. We study the class\nof lambda quantile risk measures that includes the well-known Value-at-Risk as\na special case but for which no known allocation rule is applicable. We prove\ndifferentiability and derive explicit formulae of the derivatives of lambda\nquantiles with respect to their portfolio composition, that is their risk\ncontribution. For this purpose, we define lambda quantiles on the space of\nportfolio compositions and consider generic (also non-linear) portfolio\noperators.\n  We further derive the Euler decomposition of lambda quantiles for generic\nportfolios and show that lambda quantiles are homogeneous in the space of\nportfolio compositions, with a homogeneity degree that depends on the portfolio\ncomposition and the lambda function. This result is in stark contrast to the\npositive homogeneity properties of risk measures defined on the space of random\nvariables which admit a constant homogeneity degree. We introduce a generalised\nversion of Euler contributions and Euler allocation rule, which are compatible\nwith risk measures of any homogeneity degree and non-linear but homogeneous\nportfolios. These concepts are illustrated by a non-linear portfolio using\nfinancial market data.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.14824v3"
    },
    {
        "title": "Environmental, Social, Governance scores and the Missing pillar -- Why\n  does missing information matter?",
        "authors": [
            "Özge Sahin",
            "Karoline Bax",
            "Claudia Czado",
            "Sandra Paterlini"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Environmental, Social, and Governance (ESG) scores measure companies'\nperformance concerning sustainability and societal impact and are organized on\nthree pillars: Environmental (E), Social (S), and Governance (G). These\ncomplementary non-financial ESG scores should provide information about the ESG\nperformance and risks of different companies. However, the extent of not yet\npublished ESG information makes the reliability of ESG scores questionable. To\nexplicitly denote the not yet published information on ESG category scores, a\nnew pillar, the so-called Missing (M) pillar, is formulated. Environmental,\nSocial, Governance, and Missing (ESGM) scores are introduced to consider the\npotential release of new information in the future. Furthermore, an\noptimization scheme is proposed to compute ESGM scores, linking them to the\ncompanies' riskiness. By relying on the data provided by Refinitiv, we show\nthat the ESGM scores strengthen the companies' risk relationship. These new\nscores could benefit investors and practitioners as ESG exclusion strategies\nusing only ESG scores might exclude assets with a low score solely because of\ntheir missing information and not necessarily because of a low ESG merit.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.15466v3"
    },
    {
        "title": "Market regime classification with signatures",
        "authors": [
            "Paul Bilokon",
            "Antoine Jacquier",
            "Conor McIndoe"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We provide a data-driven algorithm to classify market regimes for time\nseries. We utilise the path signature, encoding time series into\neasy-to-describe objects, and provide a metric structure which establishes a\nconnection between separation of regimes and clustering of points.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.00066v1"
    },
    {
        "title": "Reverse Sensitivity Analysis for Risk Modelling",
        "authors": [
            "Silvana M. Pesenti"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We consider the problem where a modeller conducts sensitivity analysis of a\nmodel consisting of random input factors, a corresponding random output of\ninterest, and a baseline probability measure. The modeller seeks to understand\nhow the model (the distribution of the input factors as well as the output)\nchanges under a stress on the output's distribution. Specifically, for a stress\non the output random variable, we derive the unique stressed distribution of\nthe output that is closest in the Wasserstein distance to the baseline output's\ndistribution and satisfies the stress. We further derive the stressed model,\nincluding the stressed distribution of the inputs, which can be calculated in a\nnumerically efficient way from a set of baseline Monte Carlo samples and which\nis implemented in the R package SWIM on CRAN.\n  The proposed reverse sensitivity analysis framework is model-free and allows\nfor stresses on the output such as (a) the mean and variance, (b) any\ndistortion risk measure including the Value-at-Risk and Expected-Shortfall, and\n(c) expected utility type constraints, thus making the reverse sensitivity\nanalysis framework suitable for risk models.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.01065v2"
    },
    {
        "title": "Numerical approximation of hybrid Poisson-jump Ait-Sahalia-type interest\n  rate model with delay",
        "authors": [
            "Emmanuel Coffie"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  While the original Ait-Sahalia interest rate model has been found\nconsiderable use as a model for describing time series evolution of interest\nrates, it may not possess adequate specifications to explain responses of\ninterest rates to empirical phenomena such as volatility 'skews' and 'smiles',\njump behaviour, market regulatory lapses, economic crisis, financial clashes,\npolitical instability, among others collectively. The aim of this paper is to\npropose a modified version of this model by incorporating additional features\nto collectively describe these empirical phenomena adequately. Moreover, due to\nlack of a closed-form solution to the proposed model, we employ several new\ntruncated EM techniques to examine this model and justify the scheme within\nMonte Carlo framework to compute expected payoffs of some financial quantities\nsuch as a bond and a barrier option.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.03712v4"
    },
    {
        "title": "On the Selection of Loss Severity Distributions to Model Operational\n  Risk",
        "authors": [
            "Daniel Hadley",
            "Harry Joe",
            "Natalia Nolde"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Accurate modeling of operational risk is important for a bank and the finance\nindustry as a whole to prepare for potentially catastrophic losses. One\napproach to modeling operational is the loss distribution approach, which\nrequires a bank to group operational losses into risk categories and select a\nloss frequency and severity distribution for each category. This approach\nestimates the annual operational loss distribution, and a bank must set aside\ncapital, called regulatory capital, equal to the 0.999 quantile of this\nestimated distribution. In practice, this approach may produce unstable\nregulatory capital calculations from year-to-year as selected loss severity\ndistribution families change. This paper presents truncation probability\nestimates for loss severity data and a consistent quantile scoring function on\nannual loss data as useful severity distribution selection criteria that may\nlead to more stable regulatory capital. Additionally, the Sinh-arcSinh\ndistribution is another flexible candidate family for modeling loss severities\nthat can be easily estimated using the maximum likelihood approach. Finally, we\nrecommend that loss frequencies below the minimum reporting threshold be\ncollected so that loss severity data can be treated as censored data.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.03979v1"
    },
    {
        "title": "Single Event Transition Risk: A Measure for Long Term Carbon Exposure",
        "authors": [
            "Suryadeepto Nag",
            "Siddhartha P. Chakrabarty",
            "Sankarshan Basu"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Although there is a growing consensus that a low-carbon transition will be\nnecessary to mitigate the accelerated climate change, the magnitude of\ntransition-risk for investors is difficult to measure exactly. Investors are\ntherefore constrained by the unavailability of suitable measures to quantify\nthe magnitude of the risk and are forced to use the likes of absolute emissions\ndata or ESG scores in order to manage their portfolios. In this article, we\ndefine the Single Event Transition Risk (SETR) and illustrate how it can be\nused to approximate the magnitude of the total exposure of the price of a share\nto low-carbon transition. We also discuss potential applications of the single\nevent framework and the SETR as a risk measure and discuss future direction on\nhow this can be extended to a system with multiple transition events.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06518v2"
    },
    {
        "title": "Default Distances Based on the CEV-KMV Model",
        "authors": [
            "Wen Su"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  This paper presents a new method to assess default risk based on applying the\nCEV process to the KMV model. We find that the volatility of the firm asset\nvalue may not be a constant, so we assume the firm's asset value dynamics are\ngiven by the CEV process $\\frac{dV_A}{V_A} = \\mu_A dt + \\delta V_A^{\\beta-1}dB$\nand use the equivalent volatility method to estimate parameters. Focus on the\ndistances to default, our CEV-KMV model fits the market better when forecasting\nthe credit risk compared to the classical KMV model. Besides, The estimation\nresults show the $\\beta>1$ for non ST companies while $\\beta<1$ for ST\ncompanies, which means their difference in the local volatility structure: ST\nvolatility is decreasing with respect to the firm's asset while non ST\nvolatility is increasing.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.10226v2"
    },
    {
        "title": "Capital Requirements and Claims Recovery: A New Perspective on Solvency\n  Regulation",
        "authors": [
            "Cosimo Munari",
            "Lutz Wilhelmy",
            "Stefan Weber"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Protection of creditors is a key objective of financial regulation. Where the\nprotection needs are high, i.e., in banking and insurance, regulatory solvency\nrequirements are an instrument to prevent that creditors incur losses on their\nclaims. The current regulatory requirements based on Value at Risk and Average\nValue at Risk limit the probability of default of financial institutions, but\nthey fail to control the size of recovery on creditors' claims in the case of\ndefault. We resolve this failure by developing a novel risk measure, Recovery\nValue at Risk. Our conceptual approach can flexibly be extended and allows the\nconstruction of general recovery risk measures for various risk management\npurposes. By design, these risk measures control recovery on creditors' claims\nand integrate the protection needs of creditors into the incentive structure of\nthe management. We provide detailed case studies and applications: We analyze\nhow recovery risk measures react to the joint distributions of assets and\nliabilities on firms' balance sheets and compare the corresponding capital\nrequirements with the current regulatory benchmarks based on Value at Risk and\nAverage Value at Risk. We discuss how to calibrate recovery risk measures to\nhistoric regulatory standards. Finally, we show that recovery risk measures can\nbe applied to performance-based management of business divisions of firms and\nthat they allow for a tractable characterization of optimal tradeoffs between\nrisk and return in the context of investment management.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.10635v1"
    },
    {
        "title": "A bridge between Local GAAP and Solvency II frameworks to quantify\n  Capital Requirement for demographic risk",
        "authors": [
            "Gian Paolo Clemente",
            "Francesco Della Corte",
            "Nino Savelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  The paper provides a stochastic model useful for assessing the capital\nrequirement for demographic risk. The model extends to the market consistent\ncontext classical methodologies developed in a local accounting framework. In\nparticular we provide a unique formulation for different non-participating life\ninsurance contracts and we prove analytically that the valuation of demographic\nprofit can be significantly affected by the financial conditions in the market.\nA case study has been also developed considering a portfolio of life insurance\ncontracts. Results prove the effectiveness of the model in highlighting main\ndrivers of capital requirement evaluation, also compared to local GAAP\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.10891v1"
    },
    {
        "title": "On the link between monetary and star-shaped risk measures",
        "authors": [
            "Marlon Moresco",
            "Marcelo Brutti Righi"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Recently, Castagnoli et al. (2021) introduce the class of star-shaped risk\nmeasures as a generalization of convex and coherent ones, proving that there is\na representation as the pointwise minimum of some family composed by convex\nrisk measures. Concomitantly, Jia et al. (2020) prove a similar representation\nresult for monetary risk measures, which are more general than star-shaped\nones. Then, there is a question on how both classes are connected. In this\nletter, we provide an answer by casting light on the importance of the\nacceptability of 0, which is linked to the property of normalization. We then\nshow that under mild conditions, a monetary risk measure is only a translation\naway from star-shapedness.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.13500v1"
    },
    {
        "title": "Delta Hedging with Transaction Costs: Dynamic Multiscale Strategy using\n  Neural Nets",
        "authors": [
            "G. Mazzei",
            "F. G. Bellora",
            "J. A. Serur"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In most real scenarios the construction of a risk-neutral portfolio must be\nperformed in discrete time and with transaction costs. Two human imposed\nconstraints are the risk-aversion and the profit maximization, which together\ndefine a nonlinear optimization problem with a model-dependent solution. In\nthis context, an optimal fixed frequency hedging strategy can be determined a\nposteriori by maximizing a sharpe ratio simil path dependent reward function.\nSampling from Heston processes, a convolutional neural network was trained to\ninfer which period is optimal using partial information, thus leading to a\ndynamic hedging strategy in which the portfolio is hedged at various\nfrequencies, each weighted by the probability estimate of that frequency being\noptimal.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.12337v1"
    },
    {
        "title": "Value-at-Risk forecasting model based on normal inverse Gaussian\n  distribution driven by dynamic conditional score",
        "authors": [
            "Shijia Song",
            "Handong Li"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Under the framework of dynamic conditional score, we propose a parametric\nforecasting model for Value-at-Risk based on the normal inverse Gaussian\ndistribution (Hereinafter NIG-DCS-VaR), which creatively incorporates intraday\ninformation into daily VaR forecast. NIG specifies an appropriate distribution\nto return and the semi-additivity of the NIG parameters makes it feasible to\nimprove the estimation of daily return in light of intraday return, and thus\nthe VaR can be explicitly obtained by calculating the quantile of the\nre-estimated distribution of daily return. We conducted an empirical analysis\nusing two main indexes of the Chinese stock market, and a variety of\nbacktesting approaches as well as the model confidence set approach prove that\nthe VaR forecasts of NIG-DCS model generally gain an advantage over those of\nrealized GARCH (RGARCH) models. Especially when the risk level is relatively\nhigh, NIG-DCS-VaR beats RGARCH-VaR in terms of coverage ability and\nindependence.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.02492v1"
    },
    {
        "title": "A Method for Predicting VaR by Aggregating Generalized Distributions\n  Driven by the Dynamic Conditional Score",
        "authors": [
            "Shijia Song",
            "Handong Li"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Constructing a more effective value at risk (VaR) prediction model has long\nbeen a goal in financial risk management. In this paper, we propose a novel\nparametric approach and provide a standard paradigm to demonstrate the\nmodeling. We establish a dynamic conditional score (DCS) model based on\nhigh-frequency data and a generalized distribution (GD), namely, the GD-DCS\nmodel, to improve the forecasts of daily VaR. The model assumes that intraday\nreturns at different moments are independent of each other and obey the same\nkind of GD, whose dynamic parameters are driven by DCS. By predicting the\nmotion law of the time-varying parameters, the conditional distribution of\nintraday returns is determined; then, the bootstrap method is used to simulate\ndaily returns. An empirical analysis using data from the Chinese stock market\nshows that Weibull-Pareto -DCS model incorporating high-frequency data is\nsuperior to traditional benchmark models, such as RGARCH, in the prediction of\nVaR at high risk levels, which proves that this approach contributes to the\nimprovement of risk measurement tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.02953v1"
    },
    {
        "title": "Star-shaped acceptability indexes",
        "authors": [
            "Marcelo Brutti Righi"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We propose the star-shaped acceptability indexes as generalizations of both\nthe approaches of Cherny and Madan (2009) and Rosazza Gianin and Sgarra (2013)\nin the same vein as star-shaped risk measures generalize both the classes of\ncoherent and convex risk measures in Castagnoli et al. (2022). We characterize\nacceptability indexes through star-shaped risk measures, star-shaped acceptance\nsets, and as the minimum of a family of quasi-concave acceptability indexes.\nFurther, we introduce concrete examples under our approach linked to Value at\nRisk, risk-adjusted reward on capital, reward-based gain-loss ratio, monotone\nreward-deviation ratio, and robust acceptability indexes.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.08630v3"
    },
    {
        "title": "A Meta Path Based Evaluation Method for Enterprise Credit Risk",
        "authors": [
            "Marui Du",
            "Yue Ma",
            "Zuoquan Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Nowadays small and medium-sized enterprises have become an essential part of\nthe national economy. With the increasing number of such enterprises, how to\nevaluate their credit risk becomes a hot issue. Unlike big enterprises with\nmassive data to analyze, it is hard to find enough information of small\nenterprises to assess their financial status. Limited by the lack of primary\ndata, how to inference small enterprises' credit risk from secondary data, like\ninformation of their upstream, downstream, parent, and subsidiary enterprises\nattracts big attention from industry and academy. Targeting on accurately\nevaluating the credit risk of the small and medium-sized enterprise (SME), in\nthis paper, we exploit the representative power of Information Network on\nvarious kinds of SME entities and SME relationships to solve the problem. A\nnovel feature named meta path feature proposed to measure the credit risk,\nwhich makes us able to evaluate the financial status of SMEs from various\nperspectives. Experiments show that our method is effective to identify SMEs\nwith credit risks.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.11594v4"
    },
    {
        "title": "Wealth heterogeneity in a closed pooled annuity fund",
        "authors": [
            "Thomas Bernhardt",
            "Ge Qu"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  The stability of income payments in a pooled annuity fund is studied. In\nthose funds, members receive a fluctuating income depending on their\nexperienced mortality in exchange for their pension savings. The focus is on\ndescribing the influence of different initial savings on the ability of the\nfund to provide a stable income in retirement. Because of this, members\ncoincide in their characteristics except for their initial savings. We identify\na term, which we dub ``implied number of homogeneous members'', that directly\nlinks the initial savings to the size of the income fluctuations. Our main\ncontribution is the analysis of this term and the development of a criterion to\nanswer the question of whether or not a given group of same-aged people should\npool their funds together.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.13467v2"
    },
    {
        "title": "Zero-Liquidation Loans: A Structured Product Approach to DeFi Lending",
        "authors": [
            "Aetienne Sardon"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Zero-Liquidation loans allow DeFi users to borrow USDC against their ETH\nholdings, but without the risk of being liquidated in case of LTV shortfalls.\nThis is achieved by giving users the option to repay their loans, either in\nUSDC or through their previously pledged ETH (the concept can be generalized to\nother currency pairs as well). Liquidity providers, on the other hand side, are\ncompensated with a higher yield for bearing the ETH downside risk. A positive\nside effect of zero-liquidation loans is that they are more robust against\nflash crashes and have a lower financial contagion effect than current lending\nand borrowing protocols.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.13533v1"
    },
    {
        "title": "Cyber Risk Frequency, Severity and Insurance Viability",
        "authors": [
            "Matteo Malavasi",
            "Gareth W. Peters",
            "Pavel V. Shevchenko",
            "Stefan Trück",
            "Jiwook Jang",
            "Georgy Sofronov"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this study an exploration of insurance risk transfer is undertaken for the\ncyber insurance industry in the United States of America, based on the leading\nindustry dataset of cyber events provided by Advisen. We seek to address two\ncore unresolved questions. First, what factors are the most significant\ncovariates that may explain the frequency and severity of cyber loss events and\nare they heterogeneous over cyber risk categories? Second, is cyber risk\ninsurable in regards to the required premiums, risk pool sizes and how would\nthis decision vary with the insured companies industry sector and size? We\naddress these questions through a combination of regression models based on the\nclass of Generalised Additive Models for Location Shape and Scale (GAMLSS) and\na class of ordinal regressions. These models will then form the basis for our\nanalysis of frequency and severity of cyber risk loss processes. We investigate\nthe viability of insurance for cyber risk using a utility modelling framework\nwith premium calculated by classical certainty equivalence analysis utilising\nthe developed regression models. Our results provide several new key insights\ninto the nature of insurability of cyber risk and rigorously address the two\ninsurance questions posed in a real data driven case study analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.03366v2"
    },
    {
        "title": "A General Surplus Decomposition Principle in Life Insurance",
        "authors": [
            "Julian Jetses",
            "Marcus C. Christiansen"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In with-profit life insurance, the prudent valuation of future insurance\nliabilities leads to systematic surplus that mainly belongs to the\npolicyholders and is redistributed as bonus. For a fair and lawful\nredistribution of surplus the insurer needs to decompose the total portfolio\nsurplus with respect to the contributions of individual policies and with\nrespect to different risk sources. For this task, actuaries have a number of\nheuristic decomposition formulas, but an overarching decomposition principle is\nstill missing. This paper fills that gap by introducing a so-called ISU\ndecomposition principle that bases on infinitesimal sequential updates of the\ninsurer's valuation basis. It is shown that the existing heuristic\ndecomposition formulas can be replicated as ISU decompositions. Furthermore,\nalternative decomposition principles and their relation to the ISU\ndecomposition principle are discussed. The generality of the ISU concept makes\nit a useful tool also beyond classical surplus decompositions in life\ninsurance.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.12967v1"
    },
    {
        "title": "Extremal Analysis of Flooding Risk and Management",
        "authors": [
            "Chengxiu Ling",
            "Jiayi Li",
            "Yixuan Liu",
            "Zhiyan Cai"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Catastrophic losses caused by natural disasters receive a growing concern\nabout the severe rise in magnitude and frequency. The constructions of\ninsurance and financial management scheme become increasingly necessary to\ndiversify the disaster risks. Given the frequency and severity of floods in\nChina, this paper investigates the extreme analysis of flood-related huge\nlosses and extreme precipitations using Peaks-Over-Threshold method and Point\nProcess (PP) model. These findings are further utilized for both designs of\nflood zoning insurance and flooding catastrophic bond: (1) Using the\nextrapolation approach in Extreme Value Theory (EVT), the estimated\nValue-at-Risk (VaR) and conditional VaR (CVaR) are given to determine the\ncross-regional insurance premium together with the Grey Relational Analysis\n(GRA) and the Technique for Order Preference by Similarity to an Ideal Solution\n(TOPSIS). The flood risk vulnerability and threat are analyzed with both the\ngeography and economic factors into considerations, leading to the three\nlayered premium levels of the 19 flood-prone provinces. (2) To hedge the risk\nfor insurers and reinsurers to the financial market, we design a flooding\ncatastrophe bond with considerate trigger choices and the pricing mechanism to\nbalance the benefits of both reinsurers and investors. To reflect both the\nmarket price of catastrophe risk and the low-correlated financial interest\nrisk, we utilize the pricing mechanism of Tang and Yuan (2021) to analyze the\npricing sensitivity against the tail risk of the flooding disaster and the\ndistortion magnitude and the market risk through the distortion magnitude\ninvolved in Wang's transform. Finally, constructive suggestions and policies\nare proposed concerning the flood risk warning and prevention.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.00562v1"
    },
    {
        "title": "Time-consistent mean-variance reinsurance-investment problem with\n  long-range dependent mortality rate",
        "authors": [
            "Ling Wang",
            "Mei Choi Chiu",
            "Hoi Ying Wong"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  This paper investigates the time-consistent mean-variance\nreinsurance-investment (RI) problem faced by life insurers. Inspired by recent\nfindings that mortality rates exhibit long-range dependence (LRD), we examine\nthe effect of LRD on RI strategies. We adopt the Volterra mortality model\nproposed in Wang et al.(2021) to incorporate LRD into the mortality rate\nprocess and describe insurance claims using a compound Poisson process with the\nintensity represented by stochastic mortality rate. Under the open-loop\nequilibrium mean-variance criterion, we derive explicit equilibrium RI controls\nand study the uniqueness of these controls in cases of constant and\nstate-dependent risk aversion. We simultaneously resolve difficulties arising\nfrom unbounded non-Markovian parameters and sudden increases in the insurer's\nwealth process. We also use a numerical study to reveal the influence of LRD on\nequilibrium strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06602v1"
    },
    {
        "title": "On the decomposition of an insurer's profits and losses",
        "authors": [
            "Marcus C. Christiansen"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Current reporting standards for insurers require a decomposition of observed\nprofits and losses in such a way that changes in the insurer's balance sheet\ncan be attributed to specified risk factors. Generating such a decomposition is\na nontrivial task because balance sheets generally depend on the risk factors\nin a non-linear way. This paper starts from an axiomatic perspective on profit\nand loss decompositions and finds that the axioms necessarily lead to\ninfinitesimal sequential updating (ISU) decompositions, provided that the\nlatter exist and are stable, whereas the current practice is rather to use\nsequential updating (SU) decompositions. The generality of the axiomatic\napproach makes the results useful also beyond insurance applications wherever\nprofits and losses shall be additively decomposed in a risk-oriented manner.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.11265v1"
    },
    {
        "title": "Pricing Time-to-Event Contingent Cash Flows: A Discrete-Time Survival\n  Analysis Approach",
        "authors": [
            "Jackson P. Lautier",
            "Vladimir Pozdnyakov",
            "Jun Yan"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Prudent management of insurance investment portfolios requires competent\nasset pricing of fixed-income assets with time-to-event contingent cash flows,\nsuch as consumer asset-backed securities (ABS). Current market pricing\ntechniques for these assets either rely on a non-random time-to-event model or\nmay not utilize detailed asset-level data that is now available with most\npublic transactions. We first establish a framework capable of yielding\nestimates of the time-to-event random variable from securitization data, which\nis discrete and often subject to left-truncation and right-censoring. We then\nshow that the vector of discrete-time hazard rate estimators is asymptotically\nmultivariate normal with independent components, which has not yet been done in\nthe statistical literature in the case of both left-truncation and\nright-censoring. The time-to-event distribution estimates are then fed into our\ncash flow model, which is capable of calculating a formulaic price of a pool of\ntime-to-event contingent cash flows vis-\\'{a}-vis calculating an expected\npresent value with respect to the estimated time-to-event distribution. In an\napplication to a subset of 29,845 36-month leases from the Mercedes-Benz Auto\nLease Trust 2017-A (MBALT 2017-A) bond, our pricing model yields estimates\ncloser to the actual realized future cash flows than the non-random\ntime-to-event model, especially as the fitting window increases. Finally, in\ncertain settings, the asymptotic properties of the hazard rate estimators allow\ninvestors to assess the potential uncertainty of the price point estimates,\nwhich we illustrate for a subset of 493 24-month leases from MBALT 2017-A.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.04981v2"
    },
    {
        "title": "Multinomial Backtesting of Distortion Risk Measures",
        "authors": [
            "Sören Bettels",
            "Sojung Kim",
            "Stefan Weber"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We extend the scope of risk measures for which backtesting models are\navailable by proposing a multinomial backtesting method for general distortion\nrisk measures. The method relies on a stratification and randomization of risk\nlevels. We illustrate the performance of our methods in numerical case studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06319v4"
    },
    {
        "title": "Model Aggregation for Risk Evaluation and Robust Optimization",
        "authors": [
            "Tiantian Mao",
            "Ruodu Wang",
            "Qinyu Wu"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We introduce a new approach for prudent risk evaluation based on stochastic\ndominance, which will be called the model aggregation (MA) approach. In\ncontrast to the classic worst-case risk (WR) approach, the MA approach produces\nnot only a robust value of risk evaluation but also a robust distributional\nmodel, independent of any specific risk measure. The MA risk evaluation can be\ncomputed through explicit formulas in the lattice theory of stochastic\ndominance, and under some standard assumptions, the MA robust optimization\nadmits a convex-program reformulation. The MA approach for Wasserstein and\nmean-variance uncertainty sets admits explicit formulas for the obtained robust\nmodels. Via an equivalence property between the MA and the WR approaches, new\naxiomatic characterizations are obtained for the Value-at-Risk (VaR) and the\nExpected Shortfall (ES, also known as CVaR). The new approach is illustrated\nwith various risk measures and examples from portfolio optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.06370v4"
    },
    {
        "title": "Forecasting the distribution of long-horizon returns with time-varying\n  volatility",
        "authors": [
            "Hwai-Chung Ho"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The study of long-horizon returns has received a great deal of attention in\nrecent years (see, for example, Boudoukh, Richardson, and Whitelaw (2008),\nNeuberger (2012) and Lee (2013), Fama and French (2018)). While most of the\ndiscussions are concerned with some practical issues in investment, few have\ntouched the important aspect on risk management. The approach adopted in this\narticle is to predict the future distribution of the returns of a fixed\nlong-horizon by which the risk measures of interest that come in the form of a\ndistributional functional such as the value at risk (VaR) and the conditional\ntail expectation (CTE) can be easily derived. The characteristic feature of our\napproach which requires no specification of the volatility dynamics nor\nparametric assumptions of the shock distribution extends the work by Ho et al.\n(2016) and Ho ( 2017) to a more general volatility dynamics that includes both\nthe widely-used SV model and the GARCH model (Bollerslev, 1986) as special\ncases.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07457v1"
    },
    {
        "title": "Estimating and backtesting risk under heavy tails",
        "authors": [
            "Marcin Pitera",
            "Thorsten Schmidt"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  While the estimation of risk is an important question in the daily business\nof banking and insurance, many existing plug-in estimation procedures suffer\nfrom an unnecessary bias. This often leads to the underestimation of risk and\nnegatively impacts backtesting results, especially in small sample cases. In\nthis article we show that the link between estimation bias and backtesting can\nbe traced back to the dual relationship between risk measures and the\ncorresponding performance measures, and discuss this in reference to\nvalue-at-risk, expected shortfall and expectile value-at-risk.\n  Motivated by the consistent underestimation of risk by plug-in procedures, we\npropose a new algorithm for bias correction and show how to apply it for\ngeneralized Pareto distributions to the i.i.d.\\ setting and to a GARCH(1,1)\ntime series. In particular, we show that the application of our algorithm leads\nto gain in efficiency when heavy tails or heteroscedasticity exists in the\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.10454v2"
    },
    {
        "title": "Building a Dynamic System of Advanced Risk Management and Risk\n  Assessment of the Company",
        "authors": [
            "Denis S. Gusev",
            "Elena G. Demidova",
            "Olga A. Novikova"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The purpose of the research presented in this article is to develop a dynamic\nsystem for forecasting and minimizing the risks of an industrial company based\non their quantitative assessment. The article considers the conceptual\napparatus of the essential content of risk management of an industrial\nenterprise, reviews the theoretical aspects of risk management systems and the\nmost significant risk management methods from a practical point of view. The\nmethodological apparatus of qualitative and quantitative analysis and risk\nassessment has been expanded on the basis of some conditionality of risk\nclassification features identified and a systematic approach to the\nclassification of risks of an industrial enterprise has been proposed, taking\ninto account the dynamics of their impact on the object, the stages of building\na dynamic risk management system are given. The article substantiates the need\nto supplement the dynamic risk management system of industrial enterprises with\nmethods of qualitative and quantitative risk assessment in order to form\neffective risk management strategies\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00556v1"
    },
    {
        "title": "Tail Risk of Electricity Futures",
        "authors": [
            "Juan Ignacio Peña",
            "Rosa Rodriguez",
            "Silvia Mayoral"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper compares the in-sample and out-of-sample performance of several\nmodels for computing the tail risk of one-month and one-year electricity\nfutures contracts traded in the NordPool, French, German, and Spanish markets\nin 2008-2017. As measures of tail risk, we use the one-day-ahead Value-at-Risk\n(VaR) and the Expected Shortfall (ES). With VaR, the AR (1)-GARCH (1,1) model\nwith Student-t distribution is the best-performing specification with 88% cases\nin which the Fisher test accepts the model, with a success rate of 94% in the\nleft tail and of 81% in the right tail. The model passes the test of model\nadequacy in the 100% of the cases in the NordPool and German markets, but only\nin the 88% and 63% of the cases in the Spanish and French markets. With ES,\nthis model passes the test of model adequacy in 100% of cases in all markets.\nHistorical Simulation and Quantile Regression-based approaches misestimate tail\nrisks. The right-hand tail of the returns is more difficult to model than the\nleft-hand tail and therefore financial regulators and the administrators of\nfutures markets should take these results into account when setting additional\nregulatory capital requirements and margin account regulations to short\npositions.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.01732v1"
    },
    {
        "title": "Time-zero Efficiency of European Power Derivatives Markets",
        "authors": [
            "Juan Ignacio Peña",
            "Rosa Rodriguez"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We study time-zero efficiency of electricity derivatives markets. By\ntime-zero efficiency is meant a sequence of prices of derivatives contracts\nhaving the same underlying asset but different times to maturity which implies\nthat prices comply with a set of efficiency conditions that prevent profitable\ntime-zero arbitrage opportunities. We investigate whether statistical tests,\nbased on the law of one price, and trading rules, based on price differentials\nand no-arbitrage violations, are useful for assessing time-zero efficiency. We\napply tests and trading rules to daily data of three European power markets:\nGermany, France and Spain. In the case of the German market, after considering\nliquidity availability and transaction costs, results are not inconsistent with\ntime-zero efficiency. However, in the case of the French and Spanish markets,\nlimitations in liquidity and representativeness are challenges that prevent\ndefinite conclusions. Liquidity in French and Spanish markets should improve by\nusing pricing and marketing incentives. These incentives should attract more\nparticipants into the electricity derivatives exchanges and should encourage\nthem to settle OTC trades in clearinghouses. Publication of statistics on\nprices, volumes and open interest per type of participant should be promoted.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.01737v1"
    },
    {
        "title": "Derivatives Holdings and Systemic Risk in the U.S. Banking Sector",
        "authors": [
            "Sergio Mayordomo",
            "Maria Rodriguez-Moreno",
            "Juan Ignacio Peña"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Foreign exchange and credit derivatives increase the bank's contributions to\nsystemic risk. Interest rate derivatives decrease it. The proportion of\nnon-performing loans over total loans and the leverage ratio have stronger\nimpact on systemic risk than derivatives holdings.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02254v1"
    },
    {
        "title": "Industry Characteristics and Financial Risk Spillovers",
        "authors": [
            "Wan-Chien Chiua",
            "Juan Ignacio Peña",
            "Chih-Wei Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper proposes a new measure of tail risk spillover. The empirical\napplication provides evidence of significant volatility and tail risk\nspillovers from the financial sector to many real economy sectors in the U.S.\neconomy in the period from 2001 to 2011. These spillovers increase in crisis\nperiods. The conditional coexceedance in a given sector is positively related\nto its amount of debt financing, and negatively related to its relative\nvaluation and investment. Real economy sectors which require substantial\nexternal financing, and whose value and investment activity are relatively\nlower, are prime candidates for depreciation in the wake of crisis in the\nfinancial sector.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02263v1"
    },
    {
        "title": "Are all Credit Default Swap Databases equal?",
        "authors": [
            "Sergio Mayordomo",
            "Juan Ignacio Peña",
            "Eduardo S. Schwartz"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We compare the five major sources of corporate Credit Default Swap prices:\nGFI, Fenics, Reuters, CMA, and Markit, using the most liquid single name 5-year\nCDS in the iTraxx and CDX indexes from 2004 to 2010. Deviations from the common\ntrend among prices in the different databases are not random but are explained\nby idiosyncratic factors, financing costs, global risk, and other trading\nfactors. The CMA quotes lead the price discovery process. Moreover, we find\nthat there is not a full agreement among databases in the results of the price\ndiscovery analysis between stock and CDS returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02273v1"
    },
    {
        "title": "Measuring Systemic Risk: Common Factor Exposures and Tail Dependence\n  Effects",
        "authors": [
            "Wan-Chien Chiu",
            "Juan Ignacio Peña",
            "Chih-Wei Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We model systemic risk using a common factor that accounts for market-wide\nshocks and a tail dependence factor that accounts for linkages among extreme\nstock returns. Specifically, our theoretical model allows for firm-specific\nimpacts of infrequent and extreme events. Using data on the four sectors of the\nU.S. financial industry from 1996 to 2011, we uncover two key empirical\nfindings. First, disregarding the effect of the tail dependence factor leads to\na downward bias in the measurement of systemic risk, especially during weak\neconomic times. Second, when these measures serve as leading indicators of the\nSt. Louis Fed Financial Stress Index, measures that include a tail dependence\nfactor offer better forecasting ability than measures based on a common factor\nonly.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02276v1"
    },
    {
        "title": "Derivatives Risks as Costs in a One-Period Network Model",
        "authors": [
            "Dorinel Bastide",
            "Stéphane Crépey",
            "Samuel Drapeau",
            "Mekonnen Tadese"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We present a one-period XVA model encompassing bilateral and centrally\ncleared trading in a unified framework with explicit formulas for most\nquantities at hand. We illustrate possible uses of this framework for running\nstress test exercises on a financial network from a clearing member's\nperspective or for optimizing the porting of the portfolio of a defaulted\nclearing member.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03248v2"
    },
    {
        "title": "On the Dynamics of Solid, Liquid and Digital Gold Futures",
        "authors": [
            "Toshiko Matsui",
            "Ali Al-Ali",
            "William J. Knottenbelt"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper examines the determinants of the volatility of futures prices and\nbasis for three commodities: gold, oil and bitcoin -- often dubbed solid,\nliquid and digital gold -- by using contract-by-contract analysis which has\nbeen previously applied to crude oil futures volatility investigations. By\nextracting the spot and futures daily prices as well as the maturity, trading\nvolume and open interest data for the three assets from 18th December 2017 to\n30th November 2021, we find a positive and significant role for trading volume\nand a possible negative influence of open interest, when significant, in\nshaping the volatility in all three assets, supporting earlier findings in the\ncontext of oil futures. Additionally, we find maturity has a relatively\npositive significance for bitcoin and oil futures price volatility.\nFurthermore, our analysis demonstrates that maturity affects the basis of\nbitcoin and gold positively -- confirming the general theory that the basis\nconverges to zero as maturity nears for bitcoin and gold -- while oil is\naffected in both directions.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.09845v1"
    },
    {
        "title": "Extremal Dependence in Australian Electricity Markets",
        "authors": [
            "Lin Han",
            "Ivor Cribben",
            "Stefan Trueck"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Electricity markets are significantly more volatile than other comparable\nfinancial or commodity markets. Extreme price outcomes and their transmission\nbetween regions pose significant risks for market participants. We examine the\ndependence between extreme spot price outcomes in the Australian National\nElectricity Market (NEM). We investigate extremal dependence both in a\nunivariate and multivariate setting, applying the extremogram developed by\nDavis and Mikosch (2009) and Davis et al. (2011, 2012). We measure the\npersistence of extreme prices within individual regional markets and the\ntransmission of extreme prices across different regions. With both 5-minute and\n30-minute price data, we find that extreme prices are more persistent in the\nmarket with a higher share of intermittent renewable energy. We also find that\npersistence of extreme prices is more prevalent in more concentrated markets.\nWe also show significant extremal price dependence between different regions,\nwhich is typically stronger between physically interconnected markets. The\ndependence structure of extreme prices shows asymmetric and time-dependent\npatterns. Applying the extremograms, we further show the effectiveness of the\nAustralian Energy Market Commission's 2016 rebidding rule with respect to\nreducing the share of isolated price spikes that are often considered as an\nindication of strategic bidding. Our results provide important information for\nhedging decisions of market participants and for policy makers who aim to\nreduce market volatility and extreme price outcomes through effective\nregulations which guide the trading behaviour of market participants as well as\nimproved network interconnections.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.09970v1"
    },
    {
        "title": "The Nature of Losses from Cyber-Related Events: Risk Categories and\n  Business Sectors",
        "authors": [
            "Pavel V. Shevchenko",
            "Jiwook Jang",
            "Matteo Malavasi",
            "Gareth W. Peters",
            "Georgy Sofronov",
            "Stefan Trück"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  In this study we examine the nature of losses from cyber related events\nacross different risk categories and business sectors. Using a leading industry\ndataset of cyber events, we evaluate the relationship between the frequency and\nseverity of individual cyber-related events and the number of affected records.\nWe find that the frequency of reported cyber related events has substantially\nincreased between 2008 and 2016. Furthermore, the frequency and severity of\nlosses depend on the business sector and type of cyber threat: the most\nsignificant cyber loss event categories, by number of events, were related to\ndata breaches and the unauthorized disclosure of data, while cyber extortion,\nphishing, spoofing and other social engineering practices showed substantial\ngrowth rates. Interestingly, we do not find a distinct pattern between the\nfrequency of events, the loss severity, and the number of affected records as\noften alluded to in the literature. We also analyse the severity distribution\nof cyber related events across all risk categories and business sectors. This\nanalysis reveals that cyber risks are heavy-tailed, i.e., cyber risk events\nhave a higher probability to produce extreme losses than events whose severity\nfollows an exponential distribution. Furthermore, we find that the frequency\nand severity of cyber related losses exhibits a very dynamic and time varying\nnature.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.10189v2"
    },
    {
        "title": "Cyber Loss Model Risk Translates to Premium Mispricing and Risk\n  Sensitivity",
        "authors": [
            "Gareth W. Peters",
            "Matteo Malavasi",
            "Georgy Sofronov",
            "Pavel V. Shevchenko",
            "Stefan Trück",
            "Jiwook Jang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We focus on model risk and risk sensitivity when addressing the insurability\nof cyber risk. The standard statistical approaches to assessment of\ninsurability and potential mispricing are enhanced in several aspects involving\nconsideration of model risk. Model risk can arise from model uncertainty, and\nparameters uncertainty. We demonstrate how to quantify the effect of model risk\nin this analysis by incorporating various robust estimators for key model\nparameter estimates that apply in both marginal and joint cyber risk loss\nprocess modelling. We contrast these robust techniques with standard methods\npreviously used in studying insurabilty of cyber risk. This allows us to\naccurately assess the critical impact that robust estimation can have on tail\nindex estimation for heavy tailed loss models, as well as the effect of robust\ndependence analysis when quantifying joint loss models and insurance portfolio\ndiversification. We argue that the choice of such methods is akin to a form of\nmodel risk and we study the risk sensitivity that arise from choices relating\nto the class of robust estimation adopted and the impact of the settings\nassociated with such methods on key actuarial tasks such as premium calculation\nin cyber insurance. Through this analysis we are able to address the question\nthat, to the best of our knowledge, no other study has investigated in the\ncontext of cyber risk: is model risk present in cyber risk data, and how does\nis it translate into premium mispricing? We believe our findings should\ncomplement existing studies seeking to explore insurability of cyber losses. In\norder to ensure our findings are based on realistic industry informed loss\ndata, we have utilised one of the leading industry cyber loss datasets obtained\nfrom Advisen, which represents a comprehensive data set on cyber monetary\nlosses, from which we form our analysis and conclusions.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.10588v3"
    },
    {
        "title": "Tail-GAN: Learning to Simulate Tail Risk Scenarios",
        "authors": [
            "Rama Cont",
            "Mihai Cucuringu",
            "Renyuan Xu",
            "Chao Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The estimation of loss distributions for dynamic portfolios requires the\nsimulation of scenarios representing realistic joint dynamics of their\ncomponents, with particular importance devoted to the simulation of tail risk\nscenarios. We propose a novel data-driven approach that utilizes Generative\nAdversarial Network (GAN) architecture and exploits the joint elicitability\nproperty of Value-at-Risk (VaR) and Expected Shortfall (ES). Our proposed\napproach is capable of learning to simulate price scenarios that preserve tail\nrisk features for benchmark trading strategies, including consistent statistics\nsuch as VaR and ES.\n  We prove a universal approximation theorem for our generator for a broad\nclass of risk measures. In addition, we show that the training of the GAN may\nbe formulated as a max-min game, leading to a more effective approach for\ntraining. Our numerical experiments show that, in contrast to other data-driven\nscenario generators, our proposed scenario simulation method correctly captures\ntail risk for both static and dynamic portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.01664v3"
    },
    {
        "title": "Bridging the gap between pricing and reserving with an occurrence and\n  development model for non-life insurance claims",
        "authors": [
            "Jonas Crevecoeur",
            "Katrien Antonio",
            "Stijn Desmedt",
            "Alexandre Masquelein"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Due to the presence of reporting and settlement delay, claim data sets\ncollected by non-life insurance companies are typically incomplete, facing\nright censored claim count and claim severity observations. Current practice in\nnon-life insurance pricing tackles these right censored data via a two-step\nprocedure. First, best estimates are computed for the number of claims that\noccurred in past exposure periods and the ultimate claim severities, using the\nincomplete, historical claim data. Second, pricing actuaries build predictive\nmodels to estimate technical, pure premiums for new contracts by treating these\nbest estimates as actual observed outcomes, hereby neglecting their inherent\nuncertainty. We propose an alternative approach that brings valuable insights\nfor both non-life pricing as well as reserving. As such we effectively bridge\nthese two key actuarial tasks that have traditionally been discussed in silos.\nHereto we develop a granular occurrence and development model for non-life\nclaims that tackles reserving and at the same time resolves the inconsistency\nin traditional pricing techniques between actual observations and imputed best\nestimates. We illustrate our proposed model on an insurance as well as a\nreinsurance portfolio. The advantages of our proposed strategy are most\ncompelling in the reinsurance illustration where large uncertainties in the\nbest estimates originate from long reporting and settlement delays, low claim\nfrequencies and heavy (even extreme) claim sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.07145v2"
    },
    {
        "title": "On a Stochastic Model of Diversification",
        "authors": [
            "Maria Logvaneva",
            "Mikhail Tselishchev"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We propose a definition of diversification as a binary relationship between\nfinancial portfolios. According to it, a convex linear combination of several\nrisk positions with some weights is considered to be less risky than the\nprobabilistic mixture of the same risk positions with the same weights. It\nturns out to be that the proposed partial ordering coincides with the\nwell-known second order stochastic dominance, but allows to take a look at it\nfrom another perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.01284v1"
    },
    {
        "title": "Second-order accuracy metrics for scoring models and their practical use",
        "authors": [
            "M. V. Pomazanov"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The paper proposes new second-order accuracy metrics for scoring or rating\nmodels, which show the target preference of the model, it is better to diagnose\ngood objects or better to diagnose bad ones for a constant generally accepted\npredictive power determined by the first order metric that is known as the Gini\nindex. There are two metrics, they have both an integral representation and a\nnumerical one. The numerical representation of metrics is of two types, the\nfirst of which is based on binary events to evaluate the model, the second on\nthe default probability given by the model. Comparison of the results of\ncalculating the metrics allows you to validate the calibration settings of the\nscoring or rating model and reveals its distortions. The article provides\nexamples of calculating second-order accuracy metrics for ratings of several\nrating agencies, as well as for the well known approach to calibration based on\nvan der Burg's ROC curves.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.07989v2"
    },
    {
        "title": "Calibrating distribution models from PELVE",
        "authors": [
            "Hirbod Assa",
            "Liyuan Lin",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The Value-at-Risk (VaR) and the Expected Shortfall (ES) are the two most\npopular risk measures in banking and insurance regulation. To bridge between\nthe two regulatory risk measures, the Probability Equivalent Level of VaR-ES\n(PELVE) was recently proposed to convert a level of VaR to that of ES. It is\nstraightforward to compute the value of PELVE for a given distribution model.\nIn this paper, we study the converse problem of PELVE calibration, that is, to\nfind a distribution model that yields a given PELVE, which may either be\nobtained from data or from expert opinion. We discuss separately the cases when\none-point, two-point, n-point and curve constraints are given. In the most\ncomplicated case of a curve constraint, we convert the calibration problem to\nthat of an advanced differential equation. We apply the model calibration\ntechniques to estimation and simulation for datasets used in insurance. We\nfurther study some technical properties of PELVE by offering a few new results\non monotonicity and convergence.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.08882v4"
    },
    {
        "title": "Will claim history become a deprecated rating factor? An optimal design\n  method for the real-time road risk model",
        "authors": [
            "Jiamin Yu"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  With the popularity of Telematics and Self-driving, more and more rating\nfactors, such as mileage, route, driving behavior, etc., are introduced into\nactuarial models. There are quite a few doubts and disputes on the rationality\nand accuracy of the selection of rating variables, but it does not involve the\nwidely accepted historical claim records. Recently, Tesla Insurance released a\nnew generation of Safety Score-based insurance, irrespective of accident\nhistory. Forward-looking experts and scholars began to discuss whether claim\nhistory will disappear in the future auto insurance rate-making system.\nTherefore, this paper proposes a new risk variable elimination method as well\nas a real-time road risk model design framework and concludes that claim\nhistory will be regarded as a \"noise\" factor and deprecated in the\nPay-How-You-Drive model.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.11585v1"
    },
    {
        "title": "Numerical Method for Highly Non-linear Mean-reverting Asset Price Model\n  with CEV-type Process",
        "authors": [
            "Emmanuel Coffie"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  It is well documented from various empirical studies that the volatility\nprocess of an asset price dynamics is stochastic. This phenomenon called for a\nnew approach to describing the random evolution of volatility through time with\nstochastic models. In this paper, we propose a mean-reverting theta-rho model\nfor asset price dynamics where the volatility diffusion factor of this model\nfollows a highly non-linear CEV-type process. Since this model lacks a\nclosed-form formula, we construct a new truncated EM method to study it\nnumerically under the Khasminskii-type condition. We justify that the truncated\nEM solutions can be used to evaluate a path-dependent financial product.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00634v1"
    },
    {
        "title": "Volatility Sensitive Bayesian Estimation of Portfolio VaR and CVaR",
        "authors": [
            "Taras Bodnar",
            "Vilhelm Niklasson",
            "Erik Thorsén"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  In this paper, a new way to integrate volatility information for estimating\nvalue at risk (VaR) and conditional value at risk (CVaR) of a portfolio is\nsuggested. The new method is developed from the perspective of Bayesian\nstatistics and it is based on the idea of volatility clustering. By specifying\nthe hyperparameters in a conjugate prior based on two different rolling window\nsizes, it is possible to quickly adapt to changes in volatility and\nautomatically specify the degree of certainty in the prior. This constitutes an\nadvantage in comparison to existing Bayesian methods that are less sensitive to\nsuch changes in volatilities and also usually lack standardized ways of\nexpressing the degree of belief. We illustrate our new approach using both\nsimulated and empirical data. Compared to some other well known homoscedastic\nand heteroscedastic models, the new method provides a good alternative for risk\nestimation, especially during turbulent periods where it can quickly adapt to\nchanging market conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.01444v1"
    },
    {
        "title": "Machine learning techniques in joint default assessment",
        "authors": [
            "Margherita Doria",
            "Elisa Luciano",
            "Patrizia Semeraro"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper studies the consequences of capturing non-linear dependence among\nthe covariates that drive the default of different obligors and the overall\nriskiness of their credit portfolio. Joint default modeling is, without loss of\ngenerality, the classical Bernoulli mixture model. Using an application to a\ncredit card dataset we show that, even when Machine Learning techniques perform\nonly slightly better than Logistic Regression in classifying individual\ndefaults as a function of the covariates, they do outperform it at the\nportfolio level. This happens because they capture linear and non-linear\ndependence among the covariates, whereas Logistic Regression only captures\nlinear dependence. The ability of Machine Learning methods to capture\nnon-linear dependence among the covariates produces higher default correlation\ncompared with Logistic Regression. As a consequence, on our data, Logistic\nRegression underestimates the riskiness of the credit portfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.01524v2"
    },
    {
        "title": "Eine empirische Analyse der Skalierung von Value-at-Risk Schaetzungen",
        "authors": [
            "Marita Kuhlmann"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  In practice, the value-at-risk (VaR) for a longer holding period is often\nscaled using the 'square root of time rule'. The VaR is determined for a\nshorter holding period and then scaled up according to the desired holding\nperiod. For example, the Basel rules allow banks to scale up the 1-day VaR by\nthe square root of ten to determine the 10-day VaR. It can be seen from the\nresults of this thesis that scaling can also provide good and accurate\nestimates of VaR. However, it is probably much more important to consider that,\ndepending on the methods or data set involved, there may also be significant\nconsequences for risk provisioning. Particularly, since scaling does not always\navoid the occurrence of losses that exceed the VaR estimate on a frequent basis\nover a period of time. Overall, the permission to use the square root of time\nrule in the regulatory framework should be reconsidered.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02123v1"
    },
    {
        "title": "A Stochastic Climate Model -- An approach to calibrate the\n  Climate-Extended Risk Model (CERM)",
        "authors": [
            "Jean-Baptiste Gaudemet",
            "Jules Deschamps",
            "Olivier Vinciguerra"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The initial Climate-Extended Risk Model (CERM) addresses the estimate of\nclimate-related financial risk embedded within a bank loan portfolio, through a\nclimatic extension of the Basel II IRB model. It uses a Gaussian copula model\ncalibrated with non stationary macro-correlations in order to reflect the\nfuture evolution of climate-related financial risks. In this complementary\narticle, we propose a stochastic forward-looking methodology to calibrate\nclimate macro-correlation evolution from scientific climate data, for physical\nand transition efforts specifically. We assume a global physical and transition\nrisk, likened to persistent greenhouse gas (GHG) concentration in the\natmosphere. The economic risk is considered stationary and can therefore be\ncalibrated with a backward-looking methodology. We present 4 key principles to\nmodel the GDP and we propose to model the economic, physical and transition\neffort factors with three interdependent stochastic processes allowing for a\ncalibration with seven well defined parameters. These parameters can be\ncalibrated using public data. This new approach means not only to evaluate\nclimate risks without picking any specific scenario but also allows to fill the\ngap between current one year approach of regulatory and economic capital models\nand the necessarily long-term view of climate risks by designing a framework to\nevaluate the resulting credit loss on each step (typically yearly) of the\ntransition path. This new approach could prove instrumental in the 2022 context\nof central banks weighing the pros and cons of a climate capital charge.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02581v1"
    },
    {
        "title": "Method of indirect estimation of default probability dynamics for\n  industry-target segments according to the data of Bank of Russia",
        "authors": [
            "Mikhail Pomazanov"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  A direct method for calculating default rates by industry and target\ncorporate segments is not possible given the lack of statistical data. The\nproposed paper considers a model for filtering the dynamics of the probability\nof default of corporate companies and other borrowers based on indirect data on\nthe dynamics of overdue debt supplied by the Bank of Russia. The model is based\non the equation of the balance of total and overdue debts, the missing links of\nthe corresponding time series are built using the Hodrick_Prescott filtering\nmethod. In retail lending segments (mortgage, consumer lending), default\nstatistics are available and supplied by Credit Bureaus. The presented method\nis validated on this statistic. Over a historical limited period, validation\nhas shown that the result is trustworthy. The resulting default probability\nseries are exogenous variables for macro_economic modelling of sectoral credit\nrisks.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.05984v1"
    },
    {
        "title": "Risks and Returns of Uniswap V3 Liquidity Providers",
        "authors": [
            "Lioba Heimbach",
            "Eric Schertenleib",
            "Roger Wattenhofer"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Trade execution on Decentralized Exchanges (DEXes) is automatic and does not\nrequire individual buy and sell orders to be matched. Instead, liquidity\naggregated in pools from individual liquidity providers enables trading between\ncryptocurrencies. The largest DEX measured by trading volume, Uniswap V3,\npromises a DEX design optimized for capital efficiency. However, Uniswap V3\nrequires far more decisions from liquidity providers than previous DEX designs.\n  In this work, we develop a theoretical model to illustrate the choices faced\nby Uniswap V3 liquidity providers and their implications. Our model suggests\nthat providing liquidity on Uniswap V3 is highly complex and requires many\nconsiderations from a user. Our supporting data analysis of the risks and\nreturns of real Uniswap V3 liquidity providers underlines that liquidity\nproviding in Uniswap V3 is incredibly complicated, and performances can vary\nwildly. While there are simple and profitable strategies for liquidity\nproviders in liquidity pools characterized by negligible price volatilities,\nthese strategies only yield modest returns. Instead, significant returns can\nonly be obtained by accepting increased financial risks and at the cost of\nactive management. Thus, providing liquidity has become a game reserved for\nsophisticated players with the introduction of Uniswap V3, where retail traders\ndo not stand a chance.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.08904v2"
    },
    {
        "title": "Baseline validation of a bias-mitigated loan screening model based on\n  the European Banking Authority's trust elements of Big Data & Advanced\n  Analytics applications using Artificial Intelligence",
        "authors": [
            "Alessandro Danovi",
            "Marzio Roma",
            "Davide Meloni",
            "Stefano Olgiati",
            "Fernando Metelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The goal of our 4-phase research project was to test if a\nmachine-learning-based loan screening application (5D) could detect bad loans\nsubject to the following constraints: a) utilize a minimal-optimal number of\nfeatures unrelated to the credit history, gender, race or ethnicity of the\nborrower (BiMOPT features); b) comply with the European Banking Authority and\nEU Commission principles on trustworthy Artificial Intelligence (AI). All\ndatasets have been anonymized and pseudoanonymized. In Phase 0 we selected a\nsubset of 10 BiMOPT features out of a total of 84 features; in Phase I we\ntrained 5D to detect bad loans in a historical dataset extracted from a\nmandatory report to the Bank of Italy consisting of 7,289 non-performing loans\n(NPLs) closed in the period 2010-2021; in Phase II we assessed the baseline\nperformance of 5D on a distinct validation dataset consisting of an active\nportolio of 63,763 outstanding loans (performing and non-performing) for a\ntotal financed value of over EUR 11.5 billion as of December 31, 2021; in Phase\nIII we will monitor the baseline performance for a period of 5 years (2023-27)\nto assess the prospective real-world bias-mitigation and performance of the 5D\nsystem and its utility in credit and fintech institutions. At baseline, 5D\ncorrectly detected 1,461 bad loans out of a total of 1,613 (Sensitivity = 0.91,\nPrevalence = 0.0253;, Positive Predictive Value = 0.19), and correctly\nclassified 55,866 out of the other 62,150 exposures (Specificity = 0.90,\nNegative Predictive Value = 0.997). Our preliminary results support the\nhypothesis that Big Data & Advanced Analytics applications based on AI can\nmitigate bias and improve consumer protection in the loan screening process\nwithout compromising the efficacy of the credit risk assessment. Further\nvalidation is required to assess the prospective performance and utility of 5D\nin credit and fintech institutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08938v1"
    },
    {
        "title": "Leverage Ratio: An empirical study of the European banking system",
        "authors": [
            "Jatin Dhingra",
            "Kartikeya Singh",
            "Siddhartha P. Chakrabarty"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper empirically analyzes a dataset published by the European Banking\nAuthority. Our main aim was to study how the Leverage Ratio is affected by\nadverse financial scenarios. This was be followed by observing how Leverage\nRatio exposures are correlated to various other financial variables and how\nvarious regression techniques can be used to explain the correlation.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.12095v1"
    },
    {
        "title": "Diversification quotients: Quantifying diversification via risk measures",
        "authors": [
            "Xia Han",
            "Liyuan Lin",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We establish the first axiomatic theory for diversification indices using six\nintuitive axioms: non-negativity, location invariance, scale invariance,\nrationality, normalization, and continuity. The unique class of indices\nsatisfying these axioms, called the diversification quotients (DQs), are\ndefined based on a parametric family of risk measures. A further axiom of\nportfolio convexity pins down DQ based on coherent risk measures. DQ has many\nattractive properties, and it can address several theoretical and practical\nlimitations of existing indices. In particular, for the popular risk measures\nValue-at-Risk and Expected Shortfall, the corresponding DQ admits simple\nformulas and it is efficient to optimize in portfolio selection. Moreover, it\ncan properly capture tail heaviness and common shocks, which are neglected by\ntraditional diversification indices. When illustrated with financial data, DQ\nis intuitive to interpret, and its performance is competitive against other\ndiversification indices.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13679v6"
    },
    {
        "title": "Increasing countries financial resilience through global catastrophe\n  risk pooling",
        "authors": [
            "Alessio Ciullo",
            "Eric Strobl",
            "Simona Meiler",
            "Olivia Martius",
            "David N. Bresch"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Extreme weather events can have severe impacts on national economies, leading\nthe recovery of low- to middle-income countries to become reliant on foreign\nfinancial aid. Foreign aid, however, is slow and uncertain. Therefore, the\nSendai Framework and the Paris Agreement advocate for more resilient financial\ninstruments like sovereign catastrophe risk pools. Existing pools, however,\nmight not fully exploit financial resilience potentials because they were not\ndesigned with the goal of maximizing risk diversification and they pool risk\nonly regionally. To address this, we introduce a method that forms pools\nmaximizing risk diversification and which selects countries with low bilateral\ncorrelations or low shares in the pool risk. We apply the method to explore the\nbenefits of global pooling with respect to regional pooling. We find that\nglobal pooling increases risk diversification, it lowers countries shares in\nthe pool risk and it increases the number of countries profiting from risk\npooling.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13895v1"
    },
    {
        "title": "Static Hedging of Freight Risk under Model Uncertainty",
        "authors": [
            "Georgios I. Papayiannis"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Freight rate derivatives constitute a very popular financial tool in shipping\nindustry, that allows to the market participants and the individuals operating\nin the field, to reassure their financial positions against the risk occurred\nby the volatility of the freight rates. The special structure of the shipping\nmarket attracted the interest of both academics and practitioners, since\npricing of the related traded options which are written on non-storable assets\n(i.e. the freight service) is not a trivial task. Management of freight risk is\nof major importance to preserve the viability of shipping operations,\nespecially in periods where shocks appear in the world economy, which\nintroduces uncertainty in the freight rate prices. In practice, the reduction\nof freight risk is almost exclusively performed by constructing hedging\nportfolios relying on freight rate options. These portfolios needs to be robust\nto the market uncertainties, i.e. to choose the portfolio which returns will be\nas less as it gets affected by the market changes. Especially, for time periods\nwhere the future states of the market (even in short term) are extremely\nambiguous, i.e. there are a number of different scenarios that can occur, it is\nof great importance for the firms to decide robustly to these uncertainties. In\nthis work, a framework for the robust treatment of model uncertainty in (a)\nmodeling the freight rates dynamics employing the notion of Wasserstein\nbarycenter and (b) in choosing the optimal hedging strategy for freight risk\nmanagement, is proposed. A carefully designed simulation study in the discussed\nhedging problem, employing standard modelling approaches in freight rates\nliterature, illustrates the capabilities of the proposed method with very\nsatisfactory results in approximating the optimal strategy even in high noise\ncases.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.00862v1"
    },
    {
        "title": "Star-Shaped deviations",
        "authors": [
            "Marcelo Brutti Righi",
            "Marlon Ruoso Moresco"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We propose the Star-Shaped deviation measures in the same vein as Star-Shaped\nrisk measures and Star-Shaped acceptability indexes. We characterize\nStar-Shaped deviation measures through Star-Shaped acceptance sets and as the\nminimum of a family of Convex deviation measures. We also expose an interplay\nbetween Star-Shaped risk measures and deviation measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.08613v1"
    },
    {
        "title": "Sensitivities and Hedging of the Collateral Choice Option",
        "authors": [
            "Griselda Deelstra",
            "Lech A. Grzelak",
            "Felix L. Wolf"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The collateral choice option allows a collateral-posting party the\nopportunity to change the type of security in which the collateral is\ndeposited. Due to non-zero collateral basis spreads, this optionality\nsignificantly impacts asset valuation. Because of the complexity of valuing the\noption, many practitioners resort to deterministic assumptions on the\ncollateral rates. In this article, we focus on a valuation model of the\ncollateral choice option based on stochastic dynamics. Intrinsic differences in\nthe resulting collateral choice option valuation and its implications for\ncollateral management are presented. We obtain sensitivities of the collateral\nchoice option price under both the deterministic and the stochastic model, and\nwe show that the stochastic model attributes risks to all involved collateral\ncurrencies. Besides an inability to capture volatility effects, the\ndeterministic model exhibits a digital structure in which only the\ncheapest-to-deliver currency influences the valuation at a given time. We\nfurther consider hedging an asset with the collateral choice option by a\nportfolio of domestic and foreign zero-coupon bonds that do not carry the\ncollateral choice option. We propose static hedging strategies based on the\ncrossing times of the deterministic model and based on variance-minimization\nunder the stochastic model. We show how the weights of this model can be\nexplicitly determined with the semi-analytical common factor approach and we\nshow in numerical experiments that this strategy offers good hedging\nperformance under minimized variance.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.10373v2"
    },
    {
        "title": "Phase-type representations of stochastic interest rates with\n  applications to life insurance",
        "authors": [
            "Jamaal Ahmad",
            "Mogens Bladt"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The purpose of the present paper is to incorporate stochastic interest rates\ninto a matrix-approach to multi-state life insurance, where formulas for\nreserves, moments of future payments and equivalence premiums can be obtained\nas explicit formulas in terms of product integrals or matrix exponentials. To\nthis end we consider the Markovian interest model, where the rates are\npiecewise deterministic (or even constant) in the different states of a Markov\njump process, and which is shown to integrate naturally into the matrix\nframework. The discounting factor then becomes the price of a zero-coupon bond\nwhich may or may not be correlated with the biometric insurance process.\nAnother nice feature about the Markovian interest model is that the price of\nthe bond coincides with the survival function of a phase-type distributed\nrandom variable. This, in particular, allows for calibrating the Markovian\ninterest rate models using a maximum likelihood approach to observed data\n(prices) or to theoretical models like e.g. a Vasicek model. Due to the\ndenseness of phase-type distributions, we can approximate the price behaviour\nof any zero-coupon bond with interest rates bounded from below by choosing the\nnumber of possible interest rate values sufficiently large. For observed data\nmodels with few data points, lower dimensions will usually suffice, while for\ntheoretical models the dimensionality is only a computational issue.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.11292v2"
    },
    {
        "title": "Comparing and quantifying tail dependence",
        "authors": [
            "Karl Friedrich Siburg",
            "Christopher Strothmann",
            "Gregor Weiß"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We introduce a new stochastic order for the tail dependence between random\nvariables. We then study different measures of tail dependence which are\nmonotone in the proposed order, thereby extending various known tail dependence\ncoefficients from the literature. We apply our concepts in an empirical study\nwhere we investigate the tail dependence for different pairs of S&P 500 stocks\nand indices, and illustrate the advantage of our measures of tail dependence\nover the classical tail dependence coefficient.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.10319v1"
    },
    {
        "title": "A risk measurement approach from risk-averse stochastic optimization of\n  score functions",
        "authors": [
            "Marcelo Brutti Righi",
            "Fernanda Maria Müller",
            "Marlon Ruoso Moresco"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We propose a risk measurement approach for a risk-averse stochastic problem.\nWe provide results that guarantee that our problem has a solution. We\ncharacterize and explore the properties of the argmin as a risk measure and the\nminimum as a deviation measure. We provide a connection between linear\nregression models and our framework. Based on this conception, we consider\nconditional risk and provide a connection between the minimum deviation\nportfolio and linear regression. Moreover, we also link the optimal replication\nhedging to our framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.14809v2"
    },
    {
        "title": "Convex Risk Measures for the Aggregation of Multiple Information Sources\n  and Applications in Insurance",
        "authors": [
            "Georgios I. Papayiannis",
            "Athanasios N. Yannacopoulos"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We propose a novel class of convex risk measures, based on the concept of the\nFr\\'echet mean, designed in order to handle uncertainty which arises from\nmultiple information sources regarding the risk factors of interest. The\nproposed risk measures robustly characterize the exposure of the firm, by\nfiltering out appropriately the partial information available in individual\nsources into an aggregate model for the risk factors of interest. Importantly,\nthe proposed risks can be expressed in closed analytic forms allowing for\ninteresting qualitative interpretations as well as comparative statics and thus\nfacilitate their use in the everyday risk management process of the insurance\nfirms. The potential use of the proposed risk measures in insurance is\nillustrated by two concrete applications, capital risk allocation and premia\ncalculation under uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.05211v1"
    },
    {
        "title": "A stochastic volatility model for the valuation of temperature\n  derivatives",
        "authors": [
            "Aurélien Alfonsi",
            "Nerea Vadillo"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper develops a new stochastic volatility model for the temperature\nthat is a natural extension of the Ornstein-Uhlenbeck model proposed by Benth\nand Benth (2007). This model allows to be more conservative regarding extreme\nevents while keeping tractability. We give a method based on Conditional Least\nSquares to estimate the parameters on daily data and estimate our model on\neight major European cities. We then show how to calculate efficiently the\naverage payoff of weather derivatives both by Monte-Carlo and Fourier transform\ntechniques. This new model allows to better assess the risk related to\ntemperature volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.05918v2"
    },
    {
        "title": "Non-maturing deposits modelling in a Ornstein-Uhlenbeck framework",
        "authors": [
            "Marina Marena",
            "Andrea Romeo",
            "Patrizia Semeraro"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper builds a multivariate L\\'evy-driven Ornstein-Uhlenbeck process for\nthe management of non-maturing deposits, that are a major source of funding for\nbanks. The contribution of the paper is both theoretical and operational. On\nthe theoretical side, the novelty of this model is to include three independent\nsources of randomness in a L\\'evy framework: market interest rates, deposit\nrates and deposit volumes. The choice of a L\\'evy background driving process\nallows us to model rare but severe events. On the operational side, we propose\na procedure to include severe volume outflows with positive probability in\nfuture scenarios simulation, explaining its implementation with an illustrative\nexample using Italian banking sector data.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.13314v1"
    },
    {
        "title": "Monte-Carlo Estimation of CoVaR",
        "authors": [
            "Weihuan Huang",
            "Nifei Lin",
            "L. Jeff Hong"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  ${\\rm CoVaR}$ is one of the most important measures of financial systemic\nrisks. It is defined as the risk of a financial portfolio conditional on\nanother financial portfolio being at risk. In this paper we first develop a\nMonte-Carlo simulation-based batching estimator of CoVaR and study its\nconsistency and asymptotic normality. We show that the optimal rate of\nconvergence of the batching estimator is $n^{-1/3}$, where $n$ is the sample\nsize. We then develop an importance-sampling inspired estimator under the\ndelta-gamma approximations to the portfolio losses, and we show that the rate\nof convergence of the estimator is $n^{-1/2}$. Numerical experiments support\nour theoretical findings and show that both estimators work well.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.06148v1"
    },
    {
        "title": "Creating an Optimal Portfolio of Crops Using Price Forecasting to\n  Increase ROI for Indian Farmers",
        "authors": [
            "Akshai Gaddam",
            "Sravan Malla",
            "Sandhya Dasari",
            "Narayana Darapaneni",
            "Mukesh Kumar Shukla"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The Indian agricultural sector being in a constant phase of upgradation, has\nbeen on the road to modernization for the last couple of years. The fundamental\nsource of livelihood for over 70 percent of the population living in rural\nparts of the country is still agriculture. The average Indian farmer, although\nhas access to raw and trend data pertaining to crop prices, yield and demand\nfrom Indian government and private websites, still struggles to make the right\nchoices. They are constantly faced with the dilemma of choosing the right crop\nto address market demand and fetch them a decent profit. Since the process of\nshortlisting crops amongst the many suitable ones isn't completely scientific\nand usually dictated by area tradition, this project has aimed to address that\nissue by forecasting the price of those crops and uses that to create an\noptimal portfolio that the farmers can obtain to arrive at a data-driven\ndecision for crop selection with optimal estimated ROI.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.01951v1"
    },
    {
        "title": "Stressing Dynamic Loss Models",
        "authors": [
            "Emma Kroell",
            "Silvana M. Pesenti",
            "Sebastian Jaimungal"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Stress testing, and in particular, reverse stress testing, is a prominent\nexercise in risk management practice. Reverse stress testing, in contrast to\n(forward) stress testing, aims to find an alternative but plausible model such\nthat under that alternative model, specific adverse stresses (i.e. constraints)\nare satisfied. Here, we propose a reverse stress testing framework for dynamic\nmodels. Specifically, we consider a compound Poisson process over a finite time\nhorizon and stresses composed of expected values of functions applied to the\nprocess at the terminal time. We then define the stressed model as the\nprobability measure under which the process satisfies the constraints and which\nminimizes the Kullback-Leibler divergence to the reference compound Poisson\nmodel.\n  We solve this optimization problem, prove existence and uniqueness of the\nstressed probability measure, and provide a characterization of the\nRadon-Nikodym derivative from the reference model to the stressed model. We\nfind that under the stressed measure, the intensity and the severity\ndistribution of the process depend on time and state. We illustrate the dynamic\nstress testing by considering stresses on VaR and both VaR and CVaR jointly and\nprovide illustrations of how the stochastic process is altered under these\nstresses. We generalize the framework to multivariate compound Poisson\nprocesses and stresses at times other than the terminal time. We illustrate the\napplicability of our framework by considering ``what if'' scenarios, where we\nanswer the question: What is the severity of a stress on a portfolio component\nat an earlier time such that the aggregate portfolio exceeds a risk threshold\nat the terminal time? Furthermore, for general constraints, we propose an\nalgorithm to simulate sample paths under the stressed measure, thus allowing to\ncompare the effects of stresses on the dynamics of the process.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.03221v3"
    },
    {
        "title": "Ranking Critical Tools in the Implementation of Lean Six Sigma as an\n  Integrated Management System in Portugal",
        "authors": [
            "David Ferreira",
            "Pedro Cunha"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Lean Six Sigma (LSS) is a comprehensive and powerful strategy for processes\nimprovement and products. There is a cornucopia of tools for its implementation\nand 37 among them were selected to carry out an evaluation based on three\nfactors, namely: Frequency of use of the tool, difficulty in implementing,\nimportance and impact of the tool in the implementation of LSS. An online\nsurvey was conducted with Portuguese consultants and it included questions on\nthe profile, and the companies they worked, as well as the degree of impact of\nthe tools used. Consultants were asked to choose ten tools, ranking them in\norder of importance. The frequencies with which each tool had been cited were\ncounted. A procedure was then developed to identify the know-how of consultants\nto establish a ranking of LSS tools. It was created an ordering list of tools,\nwhich emphasized in: Honshin Kanri, VOC, VSM. The results presented are\nparticularly relevant when is considered the importance of understanding the\nrequirements for a successful implementation of Lean Six Sigma management\nsystem in the organizations.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.00088v1"
    },
    {
        "title": "Optimal investment under partial information and robust VaR-type\n  constraint",
        "authors": [
            "Nicole Bäuerle",
            "An Chen"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper extends the utility maximization literature by combining partial\ninformation and (robust) regulatory constraints. Partial information is\ncharacterized by the fact that the stock price itself is observable by the\noptimizing financial institution, but the outcome of the market price of the\nrisk $\\theta$ is unknown to the institution. The regulator develops either a\ncongruent or distinct perception of the market price of risk in comparison to\nthe financial institution when imposing the Value-at-Risk (VaR) constraint. We\nalso discuss a robust VaR constraint in which the regulator uses a worst-case\nmeasure. The solution to our optimization problem takes the same form as in the\nfull information case: optimal wealth can be expressed as a decreasing function\nof state price density. The optimal wealth is equal to the minimum regulatory\nfinancing requirement in the intermediate economic states. The key distinction\nlies in the fact that the price density in the final state depends on the\noverall evolution of the estimated market price of risk, denoted as\n$\\hat{\\theta}(s)$ or that the upper boundary of the intermediate region\nexhibits stochastic behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.04394v2"
    },
    {
        "title": "Risk measurement of joint risk of portfolios: a liquidity shortfall\n  aspect",
        "authors": [
            "Shuo Gong",
            "Yijun Hu",
            "Linxiao Wei"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper presents a novel axiomatic framework of measuring the joint risk\nof a portfolio consisting of several financial positions. From the liquidity\nshortfall aspect, we construct a distortion-type risk measure to measure the\njoint risk of portfolios, which we referred to as multivariate distortion joint\nrisk measure, representing the liquidity shortfall caused by the joint risk of\nportfolios. After its fundamental properties have been studied, we\naxiomatically characterize it by proposing a novel set of axioms. Furthermore,\nbased on the representations for multivariate distortion joint risk measures,\nwe also propose a new class of vector-valued multivariate distortion joint risk\nmeasures, as well as with sensible financial interpretation. Their fundamental\nproperties are also investigated. It turns out that this new class is large\nenough, as it can not only induce new vector-valued multivariate risk measures,\nbut also recover some popular vector-valued multivariate risk measures known in\nthe literature with alternative financial interpretation. Examples are given to\nillustrate the proposed multivariate distortion joint risk measures. This paper\nmainly gives some theoretical results, helping one to have an insight look at\nthe measurement of joint risk of portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.04848v2"
    },
    {
        "title": "Optimal Systemic Risk Bailout: A PGO Approach Based on Neural Network",
        "authors": [
            "Shuhua Xiao",
            "Jiali Ma",
            "Li Xia",
            "Shushang Zhu"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The bailout strategy is crucial to cushion the massive loss caused by\nsystemic risk in the financial system. There is no closed-form formulation of\nthe optimal bailout problem, making solving it difficult. In this paper, we\nregard the issue of the optimal bailout (capital injection) as a black-box\noptimization problem, where the black box is characterized as a fixed-point\nsystem that follows the E-N framework for measuring the systemic risk of the\nfinancial system. We propose the so-called ``Prediction-Gradient-Optimization''\n(PGO) framework to solve it, where the ``Prediction'' means that the objective\nfunction without a closed-form is approximated and predicted by a neural\nnetwork, the ``Gradient'' is calculated based on the former approximation, and\nthe ``Optimization'' procedure is further implemented within a gradient\nprojection algorithm to solve the problem. Comprehensive numerical simulations\ndemonstrate that the proposed approach is promising for systemic risk\nmanagement.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.05235v1"
    },
    {
        "title": "The limitations of comonotonic additive risk measures: a literature\n  review",
        "authors": [
            "Samuel Solgon Santos",
            "Marcelo Brutti Righi",
            "Eduardo de Oliveira Horta"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Risk measures satisfying the axiom of comonotonic additivity are extensively\nstudied, arguably because of the plethora of results indicating interesting\naspects of such risk measures. Recent research, however, has shown that this\naxiom is incompatible with central properties in specific contexts. In this\npaper, we present a literature review of these incompatibilities. In addition,\nwe use the Choquet representation of comonotonic additive risk measures to show\nthey cannot be surplus invariant.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.13864v2"
    },
    {
        "title": "Performance attribution with respect to interest rates, FX, carry, and\n  residual market risks",
        "authors": [
            "Jan-Frederik Mai"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We develop a method to decompose the PnL of a portfolio of assets into four\nparts: (a) PnL due to FX rate changes, (b) PnL due to interest rate changes,\n(c) carry gain due to time passing, (d) PnL due to residual market risk changes\n(credit risk, liquidity risk, volatility risk etc.). We demonstrate the\nusefulness of our approach by decomposing the performance of an FX- and\ninterest rate-hedged negative basis position in our fund XAIA Credit Basis II,\nand we apply the methodology to decompose the performance of our fund XAIA\nCredit Debt Capital in the first quarter of 2022 into PnL contributions of the\nsingle positions.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.01010v1"
    },
    {
        "title": "Risk sharing, measuring variability, and distortion riskmetrics",
        "authors": [
            "Jean-Gabriel Lauzier",
            "Liyuan Lin",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We address the problem of sharing risk among agents with preferences modelled\nby a general class of comonotonic additive and law-based functionals that need\nnot be either monotone or convex. Such functionals are called distortion\nriskmetrics, which include many statistical measures of risk and variability\nused in portfolio optimization and insurance. The set of Pareto-optimal\nallocations is characterized under various settings of general or comonotonic\nrisk sharing problems. We solve explicitly Pareto-optimal allocations among\nagents using the Gini deviation, the mean-median deviation, or the\ninter-quantile difference as the relevant variability measures. The latter is\nof particular interest, as optimal allocations are not comonotonic in the\npresence of inter-quantile difference agents; instead, the optimal allocation\nfeatures a mixture of pairwise counter-monotonic structures, showing some\npatterns of extremal negative dependence.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.04034v1"
    },
    {
        "title": "Pairwise counter-monotonicity",
        "authors": [
            "Jean-Gabriel Lauzier",
            "Liyuan Lin",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We systematically study pairwise counter-monotonicity, an extremal notion of\nnegative dependence. A stochastic representation and an invariance property are\nestablished for this dependence structure. We show that pairwise\ncounter-monotonicity implies negative association, and it is equivalent to\njoint mix dependence if both are possible for the same marginal distributions.\nWe find an intimate connection between pairwise counter-monotonicity and risk\nsharing problems for quantile agents. This result highlights the importance of\nthis extremal negative dependence structure in optimal allocations for agents\nwho are not risk averse in the classic sense.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11701v3"
    },
    {
        "title": "A fixed point approach for computing actuarially fair Pareto optimal\n  risk-sharing rules",
        "authors": [
            "Fallou Niakh"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Risk-sharing is one way to pool risks without the need for a third party. To\nensure the attractiveness of such a system, the rule should be accepted and\nunderstood by all participants. A desirable risk-sharing rule should fulfill\nactuarial fairness and Pareto optimality while being easy to compute. This\npaper establishes a one-to-one correspondence between an actuarially fair\nPareto optimal (AFPO) risk-sharing rule and a fixed point of a specific\nfunction. A fast numerical method for computing these risk-sharing rules is\nalso derived. As a result, we are able to compute AFPO risk-sharing rules for a\nlarge number of heterogeneous participants in this framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.05421v2"
    },
    {
        "title": "Cost of Implementation of Basel III reforms in Bangladesh -- A Panel\n  data analysis",
        "authors": [
            "Dipti Rani Hazra",
            "Md. Shah Naoaj",
            "Mohammed Mahinur Alam",
            "Abdul Kader"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Inspired by the recent debate on the macroeconomic implications of the new\nbank regulatory standards known as Basel III, we tried to find out in this\nstudy that the impact of Basel III liquidity and capital requirements in\nBangladesh proposed by Basel Committee on Banking Supervision (BCBS, 2010a). A\nsmall set of macro variables, using a sample of 22 private commercial banks\noperating in Bangladesh for the period of 2010-2014, are used to estimate\nlong-run relationships among the variables. The macroeconomic variables are\nincluded The profitability of banks, GDP, banks' lending to private sector, Net\nStable Funding Ratio, Tier 1 capital Ratio, Interest rate spread, real interest\nrate. The cost is quantified using Driscoll and Kraay panel data models with\nfixed effect. Impact of higher capital and liquidity requirement on Interest\nrate spread and lending to private sector of banks were considered as the cost\nto the economy as a whole whereas impact of higher capital and liquidity\nrequirement on profitability of banks(ROE) was considered as the cost of banks.\nHere it is found that, the interest rate level is positively affected by the\ntighter liquidity and capital requirements which driven toward lessen of the\nprivate sector lending of banks. The return on equity of banks varies\nnegatively with the liquidity and capital. The economic costs are considerably\nbelow the estimated positive benefit that the reform should have by reducing\nthe probability of banking crises and the associated banking losses (BCBS,\n2010b).\n",
        "pdf_link": "http://arxiv.org/pdf/2303.11414v1"
    },
    {
        "title": "Robust optimized certainty equivalents and quantiles for loss positions\n  with distribution uncertainty",
        "authors": [
            "Weiwei Li",
            "Dejian Tian"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The paper investigates the robust optimized certainty equivalents and\nanalyzes the relevant properties of them as risk measures for loss positions\nwith distribution uncertainty. On this basis, the robust generalized quantiles\nare proposed and discussed. The robust expectiles with two specific\npenalization functions $\\varphi_{1}$ and $\\varphi_{2}$ are further considered\nrespectively. The robust expectiles with $\\varphi_{1}$ are proved to be\ncoherent risk measures, and the dual representation theorems are established.\nIn addition, the effect of penalization functions on the robust expectiles and\nits comparison with expectiles are examined and simulated numerically.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.04396v1"
    },
    {
        "title": "Exploring the Determinants of Capital Adequacy in Commercial Banks: A\n  Study of Bangladesh's Banking Sector",
        "authors": [
            "Md Shah Naoaj"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This study investigates the factors that influence the capital adequacy of\ncommercial banks in Bangladesh using panel data from 28 banks over the period\nof 2013-2019. Three analytical methods, including the Fixed Effect model,\nRandom Effect model, and Pooled Ordinary Least Square (POLS) method, are\nemployed to analyze two versions of the capital adequacy ratio, namely the\nCapital Adequacy Ratio (CAR) and Tier 1 Capital Ratio. The study reveals that\ncapital adequacy is significantly affected by several independent variables,\nwith leverage and liquidity risk having a negative and positive relationship,\nrespectively. Additionally, the study finds a positive correlation between real\nGDP and net profit and capital adequacy, while inflation has a negative\ncorrelation. For the Tier 1 Ratio, the study shows no significant relationship\nbetweenleverage and liquidity risk, but a positive correlation with the number\nof employees, net profit, and real GDP, while a negative correlation with size\nand GDP deflator. Pooled OLS analysis reveals a negative correlation with\nleverage, size, and inflation for both CAR and Tier 1 Capital Ratio, and a\npositive correlation with liquidity risk, net profit, and real GDP. Based on\nthe Hausman test, the Random Effect model is deemed moresuitable for this\ndataset. These findings have important implications for policymakers,\ninvestors, and bank managers in Bangladesh by providing insights into the\nfactors that impact the capital ratios of commercial banks.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.05935v1"
    },
    {
        "title": "Credit Risk and Financial Performance of Commercial Banks: Evidence from\n  Vietnam",
        "authors": [
            "Ha Nguyen"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Credit risk is a crucial topic in the field of financial stability,\nespecially at this time given the profound impact of the ongoing pandemic on\nthe world economy. This study provides insight into the impact of credit risk\non the financial performance of 26 commercial banks in Vietnam for the period\nfrom 2006 to 2016. The financial performance of commercial banks is measured by\nreturn on assets (ROA), return on equity (ROE), and Net interest margin (NIM);\ncredit risk is measured by the Non-performing loan ratio (NPLR); control\nvariables are measured by bank-specific characteristics, including bank size\n(SIZE), loan loss provision ratio (LLPR), and capital adequacy ratio (CAR), and\nmacroeconomic factors such as annual gross domestic product (GDP) growth and\nannual inflation rate (INF). The assumption tests show that models have\nautocorrelation, non-constant variance, and endogeneity. Hence, a dynamic\nDifference Generalized Method of Moments (dynamic Difference GMM) approach is\nemployed to thoroughly address these problems. The empirical results show that\nthe financial performance of commercial banks measured by ROE and NIM persists\nfrom one year to the next. Furthermore, SIZE and NPLR variables have a\nsignificant negative effect on ROA and ROE but not on NIM. There is no evidence\nfound in support of the LLPR and CAR variables on models. The effect of GDP\ngrowth is statistically significant and positive on ROA, ROE, and NIM, whereas\nthe INF is only found to have a significant positive impact on ROA and NIM.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.08217v2"
    },
    {
        "title": "Dynamic star-shaped risk measures and $g$-expectations",
        "authors": [
            "Dejian Tian",
            "Xunlian Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Motivated by the results of static monetary or star-shaped risk measures, the\npaper investigates the representation theorems in the dynamic framework. We\nshow that dynamic monetary risk measures can be represented as the lower\nenvelope of a family of dynamic convex risk measures, and normalized dynamic\nstar-shaped risk measures can be represented as the lower envelope of a family\nof normalized dynamic convex risk measures. The link between dynamic monetary\nrisk measures and dynamic star-shaped risk measures are established. Besides,\nthe sensitivity and time consistency problems are also studied. A specific\nnormalized time consistent dynamic star-shaped risk measures induced by $ g\n$-expectations are illustrated and discussed in detail.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.02481v1"
    },
    {
        "title": "Study on Intelligent Forecasting of Credit Bond Default Risk",
        "authors": [
            "Kai Ren"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Credit risk in the China's bond market has become increasingly evident,\ncreating a progressively escalating risk of default for credit bond investors.\nGiven the current incomplete and inaccurate bond information disclosure, timely\ntracking and forecasting the individual credit bond default risks have become\nessential to maintain market stability and ensure healthy development. This\npaper proposes an Intelligent Forecasting Framework for Default Risk that\nprovides precise day-by-day default risk prediction. In this framework, we\nfirst summarize the factors that impact credit bond defaults and construct a\nrisk index system. Then, we employ a combined default probability annotation\nmethod based on the evolutionary characteristics of bond default risk. The\nmethod considers the weighted average of Variational Bayesian Gaussian Mixture\nestimation, Market Index estimation, and Default Trend Backward estimation for\ndaily default risk annotation of matured or defaulted bonds according to the\nrisk index system. Moreover, to mine time-series correlation and\ncross-sectional index correlation features efficiently, an intelligent\nprediction model for Chinese credit bond default risk is designed using the\nConvLSTM neural network and trained with structured feature data. The\nexperiments demonstrate that the predicted individual bond risk is slightly\nhigher and substantially more responsive to fluctuations than the risk\nindicated by authoritative ratings, thereby improving on the inadequacies of\ninflated and untimely bond ratings. Consequently, this study's findings offer\nmultiple insights for regulators, issuers, and investors.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.12142v3"
    },
    {
        "title": "A closed form model-free approximation for the Initial Margin of option\n  portfolios",
        "authors": [
            "Claude Martini",
            "Arianna Mingone"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Central clearing counterparty houses (CCPs) play a fundamental role in\nmitigating the counterparty risk for exchange traded options. CCPs cover for\npossible losses during the liquidation of a defaulting member's portfolio by\ncollecting initial margins from their members. In this article we analyze the\ncurrent state of the art in the industry for computing initial margins for\noptions, whose core component is generally based on a VaR or Expected Shortfall\nrisk measure. We derive an approximation formula for the VaR at short horizons\nin a model-free setting. This innovating formula has promising features and\nbehaves in a much more satisfactory way than the classical Filtered Historical\nSimulation-based VaR in our numerical experiments. In addition, we consider the\nneural-SDE model for normalized call prices proposed by [Cohen et al.,\narXiv:2202.07148, 2022] and obtain a quasi-explicit formula for the VaR and a\nclosed formula for the short term VaR in this model, due to its conditional\naffine structure.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16346v1"
    },
    {
        "title": "A cohort-based Partial Internal Model for demographic risk",
        "authors": [
            "Francesco Della Corte",
            "Gian Paolo Clemente",
            "Nino Savelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We investigate the quantification of demographic risk in a framework\nconsistent with the market-consistent valuation imposed by Solvency II. We\nprovide compact formulas for evaluating inflows and outflows of a portfolio of\ninsurance policies based on a cohort approach. In this context, we maintain the\nhighest level of generality in order to consider both traditional policies and\nequity-linked policies: therefore, we propose a market-consistent valuation of\nthe liabilities. In the second step we evaluate the Solvency Capital\nRequirement of the idiosyncratic risk, linked to accidental mortality, and the\nsystematic risk one, also known as trend risk, proposing a formal closed\nformula for the former and an algorithm for the latter. We show that accidental\nvolatility depends on the intrinsic characteristics of the policies of the\ncohort (Sums-at-Risk), on the age of the policyholders and on the variability\nof the sums insured; trend risk depends both on accidental volatility and on\nthe longevity forecasting model used.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.03090v1"
    },
    {
        "title": "Systemic risk indicator based on implied and realized volatility",
        "authors": [
            "Paweł Sakowski",
            "Rafał Sieradzki",
            "Robert Ślepaczuk"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We propose a new measure of systemic risk to analyze the impact of the major\nfinancial market turmoils in the stock markets from 2000 to 2023 in the USA,\nEurope, Brazil, and Japan. Our Implied Volatility Realized Volatility Systemic\nRisk Indicator (IVRVSRI) shows that the reaction of stock markets varies across\ndifferent geographical locations and the persistence of the shocks depends on\nthe historical volatility and long-term average volatility level in a given\nmarket. The methodology applied is based on the logic that the simpler is\nalways better than the more complex if it leads to the same results. Such an\napproach significantly limits model risk and substantially decreases\ncomputational burden. Robustness checks show that IVRVSRI is a precise and\nvalid measure of the current systemic risk in the stock markets. Moreover, it\ncan be used for other types of assets and high-frequency data. The forecasting\nability of various SRIs (including CATFIN, CISS, IVRVSRI, SRISK, and Cleveland\nFED) with regard to weekly returns of S&P 500 index is evaluated based on the\nsimple linear, quasi-quantile, and quantile regressions. We show that IVRVSRI\nhas the strongest predicting power among them.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.05719v1"
    },
    {
        "title": "Incident-Specific Cyber Insurance",
        "authors": [
            "Wing Fung Chong",
            "Daniel Linders",
            "Zhiyu Quan",
            "Linfeng Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In the current market practice, many cyber insurance products offer a\ncoverage bundle for losses arising from various types of incidents, such as\ndata breaches and ransomware attacks, and the coverage for each incident type\ncomes with a separate limit and deductible. Although this gives prospective\ncyber insurance buyers more flexibility in customizing the coverage and better\nmanages the risk exposures of sellers, it complicates the decision-making\nprocess in determining the optimal amount of risks to retain and transfer for\nboth parties. This paper aims to build an economic foundation for these\nincident-specific cyber insurance products with a focus on how\nincident-specific indemnities should be designed for achieving Pareto\noptimality for both the insurance seller and buyer. Real data on cyber\nincidents is used to illustrate the feasibility of this approach. Several\nimplementation improvement methods for practicality are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00921v1"
    },
    {
        "title": "Loan portfolio management and Liquidity Risk: The impact of limited\n  liability and haircut",
        "authors": [
            "Deb Narayan Barik",
            "Siddhartha P. Chakrabarty"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In this article, we consider the problem of a bank's loan portfolio in the\ncontext of liquidity risk, while allowing for the limited liability protection\nenjoyed by the bank. Accordingly, we construct a novel loan portfolio model\nwith limited liability, while maintaining a threshold level of haircut in the\nportfolio. For the constructed three-time step loan portfolio, at the initial\ntime, the bank raises capital via debt and equity, investing the same in\nseveral classes of loans, while at the final time, the bank either meets its\nliabilities or becomes insolvent. At the intermediate time step, a fraction of\nthe deposits are withdrawn, resulting in liquidation of some of the bank's\nassets. The liquidated portfolio is designed with the goal of minimizing the\nliquidation cost. Our theoretical results show that model with the haircut\nconstraint leads to lesser liquidity risk, as compared to the scenario of no\nhaircut constraint being imposed. Finally, we present numerical results to\nillustrate the theoretical results which were obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.06525v1"
    },
    {
        "title": "Optimal Robust Reinsurance with Multiple Insurers",
        "authors": [
            "Emma Kroell",
            "Sebastian Jaimungal",
            "Silvana M. Pesenti"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We study a reinsurer who faces multiple sources of model uncertainty. The\nreinsurer offers contracts to $n$ insurers whose claims follow compound Poisson\nprocesses representing both idiosyncratic and systemic sources of loss. As the\nreinsurer is uncertain about the insurers' claim severity distributions and\nfrequencies, they design reinsurance contracts that maximise their expected\nwealth subject to an entropy penalty. Insurers meanwhile seek to maximise their\nexpected utility without ambiguity. We solve this continuous-time Stackelberg\ngame for general reinsurance contracts and find that the reinsurer prices under\na distortion of the barycentre of the insurers' models. We apply our results to\nproportional reinsurance and excess-of-loss reinsurance contracts, and\nillustrate the solutions numerically. Furthermore, we solve the related problem\nwhere the reinsurer maximises, still under ambiguity, their expected utility\nand compare the solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.11828v5"
    },
    {
        "title": "Uncertainty Propagation and Dynamic Robust Risk Measures",
        "authors": [
            "Marlon Moresco",
            "Mélina Mailhot",
            "Silvana M. Pesenti"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We introduce a framework for quantifying propagation of uncertainty arising\nin a dynamic setting. Specifically, we define dynamic uncertainty sets designed\nexplicitly for discrete stochastic processes over a finite time horizon. These\ndynamic uncertainty sets capture the uncertainty surrounding stochastic\nprocesses and models, accounting for factors such as distributional ambiguity.\nExamples of uncertainty sets include those induced by the Wasserstein distance\nand f-divergences.\n  We further define dynamic robust risk measures as the supremum of all\ncandidates' risks within the uncertainty set. In an axiomatic way, we discuss\nconditions on the uncertainty sets that lead to well-known properties of\ndynamic robust risk measures, such as convexity and coherence. Furthermore, we\ndiscuss the necessary and sufficient properties of dynamic uncertainty sets\nthat lead to time-consistencies of dynamic robust risk measures. We find that\nuncertainty sets stemming from f-divergences lead to strong time-consistency\nwhile the Wasserstein distance results in a new time-consistent notion of weak\nrecursiveness. Moreover, we show that a dynamic robust risk measure is strong\ntime-consistent or weak recursive if and only if it admits a recursive\nrepresentation of one-step conditional robust risk measures arising from static\nuncertainty sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.12856v4"
    },
    {
        "title": "Time consistency of dynamic risk measures and dynamic performance\n  measures generated by distortion functions",
        "authors": [
            "Tomasz R. Bielecki",
            "Igor Cialenco",
            "Hao Liu"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The aim of this work is to study risk measures generated by distortion\nfunctions in a dynamic discrete time setup, and to investigate the\ncorresponding dynamic coherent acceptability indices (DCAIs) generated by\nfamilies of such risk measures. First we show that conditional version of\nChoquet integrals indeed are dynamic coherent risk measures (DCRMs), and also\nintroduce the class of dynamic weighted value at risk measures. We prove that\nthese two classes of risk measures coincides. In the spirit of robust\nrepresentations theorem for DCAIs, we establish some relevant properties of\nfamilies of DCRMs generated by distortion functions, and then define and study\nthe corresponding DCAIs. Second, we study the time consistency of DCRMs and\nDCAIs generated by distortion functions. In particular, we prove that such\nDCRMs are sub-martingale time consistent, but they are not super-martingale\ntime consistent. We also show that DCRMs generated by distortion functions are\nnot weakly acceptance time consistent. We also present several widely used\nclasses of distortion functions and derive some new representations of these\ndistortions.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.02570v2"
    },
    {
        "title": "Resolving a Clearing Member's Default, A Radner Equilibrium Approach",
        "authors": [
            "Dorinel Bastide",
            "Stéphane Crépey",
            "Samuel Drapeau",
            "Mekonnen Tadese"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  For vanilla derivatives that constitute the bulk of investment banks' hedging\nportfolios, central clearing through central counterparties (CCPs) has become\nhegemonic. A key mandate of a CCP is to provide an efficient and proper\nclearing member default resolution procedure. When a clearing member defaults,\nthe CCP can hedge and auction or liquidate its positions. The counterparty\ncredit risk cost of auctioning has been analyzed in terms of XVA metrics in\nBastide, Cr{\\'e}pey, Drapeau, and Tadese (2023). In this work we assess the\ncosts of hedging or liquidating. This is done by comparing pre- and\npost-default market equilibria, using a Radner equilibrium approach for\nportfolio allocation and price discovery in each case. We show that the Radner\nequilibria uniquely exist and we provide both analytical and numerical\nsolutions for the latter in elliptically distributed markets. Using such tools,\na CCP could decide rationally on which market to hedge and auction or liquidate\ndefaulted portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02608v2"
    },
    {
        "title": "Mean-field Libor market model and valuation of long term guarantees",
        "authors": [
            "Florian Gach",
            "Simon Hochgerner",
            "Eva Kienbacher",
            "Gabriel Schachinger"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Existence and uniqueness of solutions to the multi-dimensional mean-field\nLibor market model (introduced by [7]) is shown. This is used as the basis for\na numerical asset-liability management (ALM) model capable of calculating\nfuture discretionary benefits in accordance with Solvency~II regulation. This\nALM model is complimented with aggregated life insurance data to perform a\nrealistic numerical study. This yields numerical evidence for heuristic\nassumptions which allow to derive estimators of lower and upper bounds for\nfuture discretionary benefits. These estimators are applied to publicly\navailable life insurance data.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.09022v2"
    },
    {
        "title": "Beyond VaR and CVaR: Topological Risk Measures in Financial Markets",
        "authors": [
            "Amit Kumar Jha"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This paper introduces a novel approach to financial risk assessment by\nincorporating topological data analysis (TDA), specifically cohomology groups,\ninto the evaluation of equities portfolios. The study aims to go beyond\ntraditional risk measures like Value at Risk (VaR) and Conditional Value at\nRisk (CVaR), offering a more nuanced understanding of market complexities.\nUsing last one year daily real-world closing price return data for three\nequities Apple, Microsoft and Google , we developed a new topological\nriskmeasure, termed Topological VaR Distance (TVaRD). Preliminary results\nindicate a significant change in the density of the point cloud representing\nthe financial time series during stress conditions, suggesting that TVaRD may\noffer additional insights into portfolio risk and has the potential to\ncomplement existing risk management tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.14604v2"
    },
    {
        "title": "Human Reliability Assessment method applied to investigate human factors\n  in NDT -- The case of the interpretation of radiograms in the French nuclear\n  sector",
        "authors": [
            "Justin Larouze",
            "Etienne Martin",
            "Pierre Calmon"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This communication reports on a study carried out in the context of the\ncollaborative FOEHN project (Human and Organizational Factors in\nNon-Destructive Evaluation) supported by the French National Research Agency.\nThe motivation of this project comes from the observation that human and\nOrganizational factors (HOF) are not sufficiently considered by the NDT\ncommunity. Its goal is to analyse and model the influence of the HOF on\nselected cases of study in the perspective of a better evaluation of the\nperformance of inspections. The communication is focused on a radiographic test\n(RT) case of study in which it appeared that several successive inspections had\nfailed to detect an existing in-service defect. The analysis and modelling of\nHOF related to interpretation of films has been achieved in the framework of\nthe CREAM (Cognitive and Reliability and Error Analysis Method). A survey has\nbeen conducted during the training and the maintaining of the proficiency of\nNDT (Non Destructive Testing) operators. This was followed by a non-participant\nobservation of operators on site and several individual interviews including a\nsample of people covering the main organizational and hierarchical roles (eg.\nproject management, management, operations, invigilation). The exchange with\nthe HOF experts resulted in a hierarchical analysis of ''radiogram\ninterpretation'' tasks (31 sub-tasks) and a list of contextual and\norganizational factors that may affect the performance of interpretation of\nfilms by the operator. From such a description the CREAM method allows to\ndetermine critical tasks and probability of ``errors'' linked to a limited set\nof ``Common Performance Conditions'' (CPC). The first conclusions of this study\nare that the model CREAM seems well-adapted to the estimation of the impact of\nHOF on NDT performances. The next phases should be to apply it to other tasks\n(here only radiograph interpretation) and techniques. The expected benefit of\nthis study is to provide tools for the evaluation and optimisation of NDT\nimplementation.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.14697v1"
    },
    {
        "title": "On Technical Bases and Surplus in Life Insurance",
        "authors": [
            "Oytun Haçarız",
            "Torsten Kleinow",
            "Angus S. Macdonald"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We revisit surplus on general life insurance contracts, represented by Markov\nmodels. We classify technical bases in terms of boundary conditions in Thiele's\nequation(s), allowing more general regulations than Scandinavian-style\n`first-order/second-order' regimes, and replacing the traditional retrospective\npolicy value. We propose a `canonical' model with three technical bases\n(premium, valuation, accumulation) and show how each pair of bases defines\npremium loadings and surplus. Along with a `true' or `real-world' experience\nbasis, this expands fundamental results of Ramlau-Hansen (1988a). We conclude\nwith two applications: lapse-supported business; and the\nretrospectively-oriented regime proposed by M{\\o}ller & Steffensen (2007).\n",
        "pdf_link": "http://arxiv.org/pdf/2310.16927v1"
    },
    {
        "title": "Markov Decision Processes with Risk-Sensitive Criteria: An Overview",
        "authors": [
            "Nicole Bäuerle",
            "Anna Jaśkiewicz"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The paper provides an overview of the theory and applications of\nrisk-sensitive Markov decision processes. The term 'risk-sensitive' refers here\nto the use of the Optimized Certainty Equivalent as a means to measure\nexpectation and risk. This comprises the well-known entropic risk measure and\nConditional Value-at-Risk. We restrict our considerations to stationary\nproblems with an infinite time horizon. Conditions are given under which\noptimal policies exist and solution procedures are explained. We present both\nthe theory when the Optimized Certainty Equivalent is applied recursively as\nwell as the case where it is applied to the cumulated reward. Discounted as\nwell as non-discounted models are reviewed\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06896v1"
    },
    {
        "title": "Set-valued intrinsic measures of systemic risk",
        "authors": [
            "Jana Hlavinova",
            "Birgit Rudloff",
            "Alexander Smirnow"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In recent years, it has become apparent that an isolated microprudential\napproach to capital adequacy requirements of individual institutions is\ninsufficient. It can increase the homogeneity of the financial system and\nultimately the cost to society. For this reason, the focus of the financial and\nmathematical literature has shifted towards the macroprudential regulation of\nthe financial network as a whole. In particular, systemic risk measures have\nbeen discussed as a risk measurement and mitigation tool. In this spirit, we\nadopt a general approach of multivariate, set-valued risk measures and combine\nit with the notion of intrinsic risk measures. In order to define the risk of a\nfinancial position, intrinsic risk measures utilise only internal capital,\nwhich is received when part of the currently held assets are sold, instead of\nrelying on external capital. We translate this methodology into the systemic\nframework and show that systemic intrinsic risk measures have desirable\nproperties such as the set-valued equivalents of monotonicity and\nquasi-convexity. Furthermore, for convex acceptance sets we derive a dual\nrepresentation of the systemic intrinsic risk measure. We apply our methodology\nto a modified Eisenberg-Noe network of banks and discuss the appeal of this\napproach from a regulatory perspective, as it does not elevate the financial\nsystem with external capital. We show evidence that this approach allows to\nmitigate systemic risk by moving the network towards more stable assets.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14588v1"
    },
    {
        "title": "Optimal insurance with mean-deviation measures",
        "authors": [
            "Tim J. Boonen",
            "Xia Han"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This paper studies an optimal insurance contracting problem in which the\npreferences of the decision maker given by the sum of the expected loss and a\nconvex, increasing function of a deviation measure. As for the deviation\nmeasure, our focus is on convex signed Choquet integrals (such as the Gini\ncoefficient and a convex distortion risk measure minus the expected value) and\non the standard deviation. We find that if the expected value premium principle\nis used, then stop-loss indemnities are optimal, and we provide a precise\ncharacterization of the corresponding deductible. Moreover, if the premium\nprinciple is based on Value-at-Risk or Expected Shortfall, then a particular\nlayer-type indemnity is optimal, in which there is coverage for small losses up\nto a limit, and additionally for losses beyond another deductible. The\nstructure of these optimal indemnities remains unchanged if there is a limit on\nthe insurance premium budget. If the unconstrained solution is not feasible,\nthen the deductible is increased to make the budget constraint binding. We\nprovide several examples of these results based on the Gini coefficient and the\nstandard deviation.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.01813v1"
    },
    {
        "title": "A hypothesis test for the long-term calibration in rating systems with\n  overlapping time windows",
        "authors": [
            "Patrick Kurth",
            "Max Nendel",
            "Jan Streicher"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We present a statistical test that can be used to verify supervisory\nrequirements concerning overlapping time windows for the long-term calibration\nin rating systems. In a first step, we show that the long-run default rate is\napproximately normally distributed with respect to random effects in default\nrealization. We then perform a detailed analysis of the correlation effects\ncaused by the overlapping time windows and solve the problem of an unknown\ndistribution of default probabilities for the long-run default rate. In this\ncontext, we present several methods for a conservative calibration test that\ncan deal with the unknown variance in the test statistic. We present a test for\nindividual rating grades, and then pass to the portfolio level by suitably\nadapting the test statistic. We conclude with comparative statics analysing the\neffect of persisting customers and the number of customers per reference date.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.14765v1"
    },
    {
        "title": "Spurious Default Probability Projections in Credit Risk Stress Testing\n  Models",
        "authors": [
            "Bernd Engelmann"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Credit risk stress testing has become an important risk management device\nwhich is used both by banks internally and by regulators. Stress testing is\ncomplex because it essentially means projecting a bank's full balance sheet\nconditional on a macroeconomic scenario over multiple years. Part of the\ncomplexity stems from using a wide range of model parameters for, e.g., rating\ntransition, write-off rules, prepayment, or origination of new loans. A typical\nparameterization of a credit risk stress test model specifies parameters linked\nto an average economic, the through-the-cycle, state. These parameters are\ntransformed to a stressed state by utilizing a macroeconomic model. It will be\nshown that the model parameterization implies a unique through-the-cycle\nportfolio which is unrelated to a bank's current portfolio. Independent of the\nstress imposed to the model, the current portfolio will have a tendency to\npropagate towards the through-the-cycle portfolio. This could create unwanted\nspurious effects on projected portfolio default rates especially when a stress\ntest model's parameterization is inconsistent with a bank's current portfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.08892v1"
    },
    {
        "title": "An Exploration to the Correlation Structure and Clustering of\n  Macroeconomic Variables",
        "authors": [
            "Garvit Arora",
            "Shubhangi Tiwari",
            "Ying Wu",
            "Xuan Mei"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  As a quantitative characterization of the complicated economy, Macroeconomic\nVariables (MEVs), including GDP, inflation, unemployment, income, spending,\ninterest rate, etc., are playing a crucial role in banks' portfolio management\nand stress testing exercise. In recent years, especially during the COVID-19\nperiod and the current high inflation environment, people are frequently\ntalking about the changing \"correlation structure\" of MEVs. In this paper, we\nuse a principal component based algorithm to perform unsupervised clustering on\nMEVs so we can quantify and better understand MEVs' correlation structure in\nany given period. We also demonstrate how this method can be used to visualize\nhistorical MEVs pattern changes between 2000 and 2022. Further, we use this\nmethod to compare different hypothetical and/or historical macroeconomic\nscenarios and present our key findings. One of these interesting observations\nis that, for a list of 132 transformations derived from 44 targeted MEVs that\ncover 5 different aspects of the U.S. economy (which takes as a subset the 10+\nkey MEVs published by FRB), compared to benign years where there are typically\n20-25 clusters, during the great financial crisis (GFC), i.e., 2007-2010, they\nexhibited a more synchronized and less diversified pattern of movement, forming\nroughly 15 clusters. We also see this contrast in the hypothetical CCAR2023 FRB\nscenarios where the Severely Adverse scenario has 15 clusters and the Baseline\nscenario has 21 clusters. We provide our interpretation to this observation and\nhope this research can inspire and benefit researchers from different domains\nall over the world.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10162v3"
    },
    {
        "title": "Forecasting and Backtesting Gradient Allocations of Expected Shortfall",
        "authors": [
            "Takaaki Koike",
            "Cathy W. S. Chen",
            "Edward M. H. Lin"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Capital allocation is a procedure for quantifying the contribution of each\nsource of risk to aggregated risk. The gradient allocation rule, also known as\nthe Euler principle, is a prevalent rule of capital allocation under which the\nallocated capital captures the diversification benefit of the marginal risk as\na component of overall risk. This research concentrates on Expected Shortfall\n(ES) as a regulatory standard and focuses on the gradient allocations of ES,\nalso called ES contributions (ESCs). We present the comprehensive treatment of\nbacktesting the tuple of ESCs in the framework of the traditional and\ncomparative backtests based on the concepts of joint identifiability and\nmulti-objective elicitability. For robust forecast evaluation against the\nchoice of scoring function, we also extend the Murphy diagram, a graphical tool\nto check whether one forecast dominates another under a class of scoring\nfunctions, to the case of ESCs. Finally, leveraging the recent concept of\nmulti-objective elicitability, we propose a novel semiparametric model for\nforecasting dynamic ESCs based on a compositional regression model. In an\nempirical analysis of stock returns we evaluate and compare a variety of models\nfor forecasting dynamic ESCs and demonstrate the outstanding performance of the\nproposed model.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.11701v2"
    },
    {
        "title": "Improving Business Insurance Loss Models by Leveraging InsurTech\n  Innovation",
        "authors": [
            "Zhiyu Quan",
            "Changyue Hu",
            "Panyi Dong",
            "Emiliano A. Valdez"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Recent transformative and disruptive advancements in the insurance industry\nhave embraced various InsurTech innovations. In particular, with the rapid\nprogress in data science and computational capabilities, InsurTech is able to\nintegrate a multitude of emerging data sources, shedding light on opportunities\nto enhance risk classification and claims management. This paper presents a\ngroundbreaking effort as we combine real-life proprietary insurance claims\ninformation together with InsurTech data to enhance the loss model, a\nfundamental component of insurance companies' risk management. Our study\nfurther utilizes various machine learning techniques to quantify the predictive\nimprovement of the InsurTech-enhanced loss model over that of the insurance\nin-house. The quantification process provides a deeper understanding of the\nvalue of the InsurTech innovation and advocates potential risk factors that are\nunexplored in traditional insurance loss modeling. This study represents a\nsuccessful undertaking of an academic-industry collaboration, suggesting an\ninspiring path for future partnerships between industry and academic\ninstitutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.16723v1"
    },
    {
        "title": "Partial Law Invariance and Risk Measures",
        "authors": [
            "Yi Shen",
            "Zachary Van Oosten",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We introduce the concept of partial law invariance, generalizing the concepts\nof law invariance and probabilistic sophistication widely used in decision\ntheory, as well as statistical and financial applications. This new concept is\nmotivated by practical considerations of decision making under uncertainty,\nthus connecting the literature on decision theory and that on financial risk\nmanagement. We fully characterize partially law-invariant coherent risk\nmeasures via a novel representation formula. Strong partial law invariance is\ndefined to bridge the gap between the above characterization and the classic\nrepresentation formula of Kusuoka. We propose a few classes of new risk\nmeasures, including partially law-invariant versions of the Expected Shortfall\nand the entropic risk measures, and illustrate their applications in risk\nassessment under different types of uncertainty. We provide a tractable\noptimization formula for computing a class of partially law-invariant coherent\nrisk measures and give a numerical example.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.17265v4"
    },
    {
        "title": "'Egalitarian pooling and sharing of longevity risk', a.k.a. 'The many\n  ways to skin a tontine cat'",
        "authors": [
            "Jan L. M. Dhaene",
            "Moshe A. Milevsky"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  There is little disagreement among insurance actuaries and financial\neconomists about the societal benefits of longevity-risk pooling in the form of\nlife annuities, defined benefit pensions, self-annuitization funds, and even\ntontine schemes. Indeed, the discounted value or cost of providing an income\nfor life is lower -- in other words, the amount of upfront capital required to\ngenerate a similar income stream with the same level of statistical safety is\nlower -- when participants pool their financial resources versus going it\nalone. Moreover, when participants' financial circumstances and lifespans are\nhomogenous, there is consensus on how to share the \"winnings\" among survivors,\nnamely by distributing them equally among survivors, a.k.a. a uniform rule.\nAlas, what is lesser-known and much more problematic is allocating the winnings\nin such a pool when participants differ in wealth (contributions) and health\n(longevity), especially when the pools are relatively small in size. The same\nproblems arise when viewed from the dual perspective of decentralized risk\nsharing (DRS). The positive correlation between health and income and the fact\nthat wealthier participants are likely to live longer is a growing concern\namong pension and retirement policymakers. With that motivation in mind, this\npaper offers a modelling framework for distributing longevity-risk pools'\nincome and benefits (or tontine winnings) when participants are heterogeneous.\nSimilar to the nascent literature on decentralized risk sharing, there are\nseveral equally plausible arrangements for sharing benefits (a.k.a. \"skinning\nthe cat\") among survivors. Moreover, the selected rule depends on the extent of\nsocial cohesion within the longevity risk pool, ranging from solidarity and\naltruism to pure individualism. In sum, actuarial science cannot really offer\nor guarantee uniqueness, only a methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.00855v1"
    },
    {
        "title": "Optimal dynamic climate adaptation pathways: a case study of New York\n  City",
        "authors": [
            "Chi Truong",
            "Matteo Malavasi",
            "Han Li",
            "Stefan Trueck",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Assessing climate risk and its potential impacts on our cities and economies\nis of fundamental importance. Extreme weather events, such as hurricanes,\nfloods, and storm surges can lead to catastrophic damages. We propose a\nflexible approach based on real options analysis and extreme value theory,\nwhich enables the selection of optimal adaptation pathways for a portfolio of\nclimate adaptation projects. We model the severity of extreme sea level events\nusing the block maxima approach from extreme value theory, and then develop a\nreal options framework, factoring in climate change, sea level rise\nuncertainty, and the growth in asset exposure. We then apply the proposed\nframework to a real-world problem, considering sea level data as well as\ndifferent adaptation investment options for New York City. Our research can\nassist governments and policy makers in taking informed decisions about optimal\nadaptation pathways and more specifically about reducing flood and storm surge\nrisk in a dynamic settings.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.02745v1"
    },
    {
        "title": "Semi-parametric financial risk forecasting incorporating multiple\n  realized measures",
        "authors": [
            "Rangika Peiris",
            "Chao Wang",
            "Richard Gerlach",
            "Minh-Ngoc Tran"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  A semi-parametric joint Value-at-Risk (VaR) and Expected Shortfall (ES)\nforecasting framework employing multiple realized measures is developed. The\nproposed framework extends the realized exponential GARCH model to be\nsemi-parametrically estimated, via a joint loss function, whilst extending\nexisting quantile time series models to incorporate multiple realized measures.\nA quasi-likelihood is built, employing the asymmetric Laplace distribution that\nis directly linked to a joint loss function, which enables Bayesian inference\nfor the proposed model. An adaptive Markov Chain Monte Carlo method is used for\nthe model estimation. The empirical section evaluates the performance of the\nproposed framework with six stock markets from January 2000 to June 2022,\ncovering the period of COVID-19. Three realized measures, including 5- minute\nrealized variance, bi-power variation, and realized kernel, are incorporated\nand evaluated in the proposed framework. One-step-ahead VaR and ES forecasting\nresults of the proposed model are compared to a range of parametric and\nsemi-parametric models, lending support to the effectiveness of the proposed\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09985v3"
    },
    {
        "title": "A new characterization of second-order stochastic dominance",
        "authors": [
            "Yuanying Guan",
            "Muqiao Huang",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We provide a new characterization of second-order stochastic dominance, also\nknown as increasing concave order. The result has an intuitive interpretation\nthat adding a risk with negative expected value in adverse scenarios makes the\nresulting position generally less desirable for risk-averse agents. A similar\ncharacterization is also found for convex order and increasing convex order.\nThe proof techniques for the main result are based on properties of Expected\nShortfall, a family of risk measures that is popular in banking and insurance\nregulation. Applications in risk management and insurance are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.13355v3"
    },
    {
        "title": "Using CPI in Loss Given Default Forecasting Models for Commercial Real\n  Estate Portfolio",
        "authors": [
            "Ying Wu",
            "Garvit Arora",
            "Xuan Mei"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Forecasting the loss given default (LGD) for defaulted Commercial Real Estate\n(CRE) loans poses a significant challenge due to the extended resolution and\nworkout time associated with such defaults, particularly in CCAR and CECL\nframework where the utilization of post-default information, including\nmacroeconomic variables (MEVs) such as unemployment (UER) and various rates, is\nrestricted. The current environment of persistent inflation and resultant\nelevated rates further compounds the uncertainty surrounding predictive LGD\nmodels. In this paper, we leverage both internal and public data sources,\nincluding observations from the COVID-19 period, to present a list of evidence\nindicating that the growth rates of the Consumer Price, such as Year-over-Year\n(YoY) growth and logarithmic growth, are good leading indicators for various\nCRE related rates and indices. These include the Federal Funds Effective Rate\nand CRE market sales price indices in key locations such as Los Angeles, New\nYork, and nationwide, encompassing both apartment and office segments.\nFurthermore, with CRE LGD data we demonstrate how incorporating CPI at the time\nof default can improve the accuracy of predicting CRE workout LGD. This is\nparticularly helpful in addressing the common issue of early downturn\nunderestimation encountered in CRE LGD models.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.15498v1"
    },
    {
        "title": "Set-valued Star-Shaped Risk Measures",
        "authors": [
            "Bingchu Nie",
            "Dejian Tian",
            "Long Jiang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this paper, we introduce a new class of set-valued risk measures, named\nset-valued star-shaped risk measures. Motivated by the results of scalar\nmonetary and star-shaped risk measures, this paper investigates the\nrepresentation theorems in the set-valued framework. It is demonstrated that\nset-valued risk measures can be represented as the union of a family of\nset-valued convex risk measures, and set-valued normalized star-shaped risk\nmeasures can be represented as the union of a family of set-valued normalized\nconvex risk measures. The link between set-valued risk measures and set-valued\nstar-shaped risk measures is also established.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18014v1"
    },
    {
        "title": "Spanning Multi-Asset Payoffs With ReLUs",
        "authors": [
            "Sébastien Bossu",
            "Stéphane Crépey",
            "Hoang-Dung Nguyen"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We propose a distributional formulation of the spanning problem of a\nmulti-asset payoff by vanilla basket options. This problem is shown to have a\nunique solution if and only if the payoff function is even and absolutely\nhomogeneous, and we establish a Fourier-based formula to calculate the\nsolution. Financial payoffs are typically piecewise linear, resulting in a\nsolution that may be derived explicitly, yet may also be hard to numerically\nexploit. One-hidden-layer feedforward neural networks instead provide a natural\nand efficient numerical alternative for discrete spanning. We test this\napproach for a selection of archetypal payoffs and obtain better hedging\nresults with vanilla basket options compared to industry-favored approaches\nbased on single-asset vanilla hedges.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.14231v3"
    },
    {
        "title": "Enhancing Anomaly Detection in Financial Markets with an LLM-based\n  Multi-Agent Framework",
        "authors": [
            "Taejin Park"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper introduces a Large Language Model (LLM)-based multi-agent\nframework designed to enhance anomaly detection within financial market data,\ntackling the longstanding challenge of manually verifying system-generated\nanomaly alerts. The framework harnesses a collaborative network of AI agents,\neach specialised in distinct functions including data conversion, expert\nanalysis via web research, institutional knowledge utilization or\ncross-checking and report consolidation and management roles. By coordinating\nthese agents towards a common objective, the framework provides a comprehensive\nand automated approach for validating and interpreting financial data\nanomalies. I analyse the S&P 500 index to demonstrate the framework's\nproficiency in enhancing the efficiency, accuracy and reduction of human\nintervention in financial market monitoring. The integration of AI's autonomous\nfunctionalities with established analytical methods not only underscores the\nframework's effectiveness in anomaly detection but also signals its broader\napplicability in supporting financial market monitoring.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.19735v1"
    },
    {
        "title": "Risk exchange under infinite-mean Pareto models",
        "authors": [
            "Yuyu Chen",
            "Paul Embrechts",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We study the optimal decisions of agents who aim to minimize their risks by\nallocating their positions over extremely heavy-tailed (i.e., infinite-mean)\nand possibly dependent losses. The loss distributions of our focus are\nsuper-Pareto distributions which include the class of extremely heavy-tailed\nPareto distributions. For a portfolio of super-Pareto losses,\nnon-diversification is preferred by decision makers equipped with well-defined\nand monotone risk measures. The phenomenon that diversification is not\nbeneficial in the presence of super-Pareto losses is further illustrated by an\nequilibrium analysis in a risk exchange market. First, agents with super-Pareto\nlosses will not share risks in a market equilibrium. Second, transferring\nlosses from agents bearing super-Pareto losses to external parties without any\nlosses may arrive at an equilibrium which benefits every party involved. The\nempirical studies show that extremely heavy tails exist in real datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.20171v1"
    },
    {
        "title": "Coherent risk measures and uniform integrability",
        "authors": [
            "Muqiao Huang",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We establish a profound connection between coherent risk measures, a\nprominent object in quantitative finance, and uniform integrability, a\nfundamental concept in probability theory. Instead of working with absolute\nvalues of random variables, which is convenient in studying integrability, we\nwork directly with random loses and gains, which have clear financial\ninterpretation. We introduce a technical tool called the folding score of\ndistortion risk measures. The analysis of the folding score allows us to\nconvert some conditions on absolute values to those on gains and losses. As our\nmain results, we obtain three sets of equivalent conditions for uniform\nintegrability. In particular, a set is uniformly integrable if and only if one\ncan find a coherent distortion risk measure that is bounded on the set, but not\nfinite on $L^1$.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.03783v2"
    },
    {
        "title": "Derivatives of Risk Measures",
        "authors": [
            "Battulga Gankhuu"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper provides the first and second order derivatives of any risk\nmeasures, including VaR and ES for continuous and discrete portfolio loss\nrandom variable variables. Also, we give asymptotic results of the first and\nsecond order conditional moments for heavy-tailed portfolio loss random\nvariable.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09646v3"
    },
    {
        "title": "Extremal cases of distortion risk measures with partial information",
        "authors": [
            "Mengshuo Zhao",
            "Narayanaswamy Balakrishnan",
            "Chuancun Yin"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper considers the best- and worst-case of a general class of\ndistortion risk measures when only partial information regarding the underlying\ndistributions is available. Specifically, explicit sharp lower and upper bounds\nfor a general class of distortion risk measures are derived based on the first\ntwo moments along with some shape information, such as symmetry/unimodality\nproperty of the underlying distributions. The proposed approach provides a\nunified framework for extremal problems of distortion risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13637v2"
    },
    {
        "title": "The checkerboard copula and dependence concepts",
        "authors": [
            "Liyuan Lin",
            "Ruodu Wang",
            "Ruixun Zhang",
            "Chaoyi Zhao"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We study the problem of choosing the copula when the marginal distributions\nof a random vector are not all continuous. Inspired by four motivating examples\nincluding simulation from copulas, stress scenarios, co-risk measures, and\ndependence measures, we propose to use the checkerboard copula, that is,\nintuitively, the unique copula with a distribution that is as uniform as\npossible within regions of flexibility. We show that the checkerboard copula\nhas the largest Shannon entropy, which means that it carries the least\ninformation among all possible copulas for a given random vector. Furthermore,\nthe checkerboard copula preserves the dependence information of the original\nrandom vector, leading to two applications in the context of diversification\npenalty and impact portfolios. The numerical and empirical results illustrate\nthe benefits of using the checkerboard copula in the calculation of co-risk\nmeasures.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.15023v2"
    },
    {
        "title": "Value-at-Risk- and Expectile-based Systemic Risk Measures and\n  Second-order Asymptotics: With Applications to Diversification",
        "authors": [
            "Bingzhen Geng",
            "Yang Liu",
            "Yimiao Zhao"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The systemic risk measure plays a crucial role in analyzing individual losses\nconditioned on extreme system-wide disasters. In this paper, we provide a\nunified asymptotic treatment for systemic risk measures. First, we classify\nthem into two families of Value-at-Risk- (VaR-) and expectile-based systemic\nrisk measures. While VaR has been extensively studied, in the latter family, we\npropose two new systemic risk measures named the Individual Conditional\nExpectile (ICE) and the Systemic Individual Conditional Expectile (SICE), as\nalternatives to Marginal Expected Shortfall (MES) and Systemic Expected\nShortfall (SES). Second, to characterize general mutually dependent and\nheavy-tailed risks, we adopt a modeling framework where the system, represented\nby a vector of random loss variables, follows a multivariate Sarmanov\ndistribution with a common marginal exhibiting second-order regular variation.\nThird, we provide second-order asymptotic results for both families of systemic\nrisk measures. This analytical framework offers a more accurate estimate\ncompared to traditional first-order asymptotics. Through numerical and\nanalytical examples, we demonstrate the superiority of second-order asymptotics\nin accurately assessing systemic risk. Further, we conduct a comprehensive\ncomparison between VaR-based and expectile-based systemic risk measures.\nExpectile-based measures output higher risk evaluation than VaR-based ones,\nemphasizing the former's potential advantages in reporting extreme events and\ntail risk. As a financial application, we use the asymptotic treatment to\ndiscuss the diversification benefits associated with systemic risk measures.\nThe expectile-based diversification benefits consistently deduce an\nunderestimation and suggest a conservative approximation, while the VaR-based\ndiversification benefits consistently deduce an overestimation and suggest\nbehaving optimistically.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18029v1"
    },
    {
        "title": "Backtesting Expected Shortfall: Accounting for both duration and\n  severity with bivariate orthogonal polynomials",
        "authors": [
            "Sullivan Hué",
            "Christophe Hurlin",
            "Yang Lu"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We propose an original two-part, duration-severity approach for backtesting\nExpected Shortfall (ES). While Probability Integral Transform (PIT) based ES\nbacktests have gained popularity, they have yet to allow for separate testing\nof the frequency and severity of Value-at-Risk (VaR) violations. This is a\ncrucial aspect, as ES measures the average loss in the event of such\nviolations. To overcome this limitation, we introduce a backtesting framework\nthat relies on the sequence of inter-violation durations and the sequence of\nseverities in case of violations. By leveraging the theory of (bivariate)\northogonal polynomials, we derive orthogonal moment conditions satisfied by\nthese two sequences. Our approach includes a straightforward, model-free Wald\ntest, which encompasses various unconditional and conditional coverage\nbacktests for both VaR and ES. This test aids in identifying any mis-specified\ncomponents of the internal model used by banks to forecast ES. Moreover, it can\nbe extended to analyze other systemic risk measures such as Marginal Expected\nShortfall. Simulation experiments indicate that our test exhibits good finite\nsample properties for realistic sample sizes. Through application to two stock\nindices, we demonstrate how our methodology provides insights into the reasons\nfor rejections in testing ES validity.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.02012v3"
    },
    {
        "title": "A note on continuity and asymptotic consistency of measures of risk and\n  variability",
        "authors": [
            "Niushan Gao",
            "Foivos Xanthos"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this short note, we show that every convex, order bounded above functional\non a Frechet lattice is automatically norm continuous. This improves a result\nin \\cite{RS06} and applies to many deviation and variability measures. We also\nshow that an order-continuous, law-invariant functional on an Orlicz space is\nstrongly consistent everywhere, extending a result in \\cite{KSZ14}.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.09766v2"
    },
    {
        "title": "Resilience Analysis of Multi-modal Logistics Service Network Through\n  Robust Optimization with Budget-of-Uncertainty",
        "authors": [
            "Yaxin Pang",
            "Shenle Pan",
            "Eric Ballot"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Supply chain resilience analysis aims to identify the critical elements in\nthe supply chain, measure its reliability, and analyze solutions for improving\nvulnerabilities. While extensive methods like stochastic approaches have been\ndominant, robust optimization-widely applied in robust planning under\nuncertainties without specific probability distributions-remains relatively\nunderexplored for this research problem. This paper employs robust optimization\nwith budget-of-uncertainty as a tool to analyze the resilience of multi-modal\nlogistics service networks under time uncertainty. We examine the interactive\neffects of three critical factors: network size, disruption scale, disruption\ndegree. The computational experiments offer valuable managerial insights for\npractitioners and researchers.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12565v1"
    },
    {
        "title": "Worst-cases of distortion riskmetrics and weighted entropy with partial\n  information",
        "authors": [
            "Baishuai Zuo",
            "Chuancun Yin"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this paper, we discuss the worst-case of distortion riskmetrics for\ngeneral distributions when only partial information (mean and variance) is\nknown. This result is applicable to general class of distortion risk measures\nand variability measures. Furthermore, we also consider worst-case of weighted\nentropy for general distributions when only partial information is available.\nSpecifically, we provide some applications for entropies, weighted entropies\nand risk measures. The commonly used entropies include Gini functional,\ncumulative residual entropy, tail-Gini functional, cumulative Tsallis past\nentropy, extended Gini coefficient and so on. The risk measures contain some\npremium principles and shortfalls based on entropy. The shortfalls include the\nGini shortfall, extended Gini shortfall, shortfall of cumulative residual\nentropy and shortfall of cumulative residual Tsallis entropy with order\n$\\alpha$.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.19075v1"
    },
    {
        "title": "Beyond probability-impact matrices in project risk management: A\n  quantitative methodology for risk prioritisation",
        "authors": [
            "Fernando Acebes",
            "José Manuel González-Varona",
            "Adolfo López-Paredes",
            "Javier Pajares"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The project managers who deal with risk management are often faced with the\ndifficult task of determining the relative importance of the various sources of\nrisk that affect the project. This prioritisation is crucial to direct\nmanagement efforts to ensure higher project profitability. Risk matrices are\nwidely recognised tools by academics and practitioners in various sectors to\nassess and rank risks according to their likelihood of occurrence and impact on\nproject objectives. However, the existing literature highlights several\nlimitations to use the risk matrix. In response to the weaknesses of its use,\nthis paper proposes a novel approach for prioritising project risks. Monte\nCarlo Simulation (MCS) is used to perform a quantitative prioritisation of\nrisks with the simulation software MCSimulRisk. Together with the definition of\nproject activities, the simulation includes the identified risks by modelling\ntheir probability and impact on cost and duration. With this novel methodology,\na quantitative assessment of the impact of each risk is provided, as measured\nby the effect that it would have on project duration and its total cost. This\nallows the differentiation of critical risks according to their impact on\nproject duration, which may differ if cost is taken as a priority objective.\nThis proposal is interesting for project managers because they will, on the one\nhand, know the absolute impact of each risk on their project duration and cost\nobjectives and, on the other hand, be able to discriminate the impacts of each\nrisk independently on the duration objective and the cost objective.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.20679v1"
    },
    {
        "title": "Analisis cuantitativo de riesgos utilizando \"MCSimulRisk\" como\n  herramienta didactica",
        "authors": [
            "Fernando Acebes",
            "David Curto",
            "Juan de Anton",
            "Felix Villafanez"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Risk management is a fundamental discipline in project management, which\nincludes, among others, quantitative risk analysis. Throughout several years of\nteaching, we have observed difficulties in students performing Monte Carlo\nSimulation within the quantitative analysis of risks. This article aims to\npresent MCSimulRisk as a teaching tool that allows students to perform Monte\nCarlo simulation and apply it to projects of any complexity simply and\nintuitively. This tool allows for incorporating any uncertainty identified in\nthe project into the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.20688v1"
    },
    {
        "title": "Project Risk Management from the bottom-up: Activity Risk Index",
        "authors": [
            "Fernando Acebes",
            "Javier Pajares",
            "Jose M Gonzalez-Varona",
            "Adolfo Lopez-Paredes"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Project managers need to manage risks throughout the project lifecycle and,\nthus, need to know how changes in activity durations influence project duration\nand risk. We propose a new indicator (the Activity Risk Index, ARI) that\nmeasures the contribution of each activity to the total project risk while it\nis underway. In particular, the indicator informs us about what activities\ncontribute the most to the project's uncertainty so that project managers can\npay closer attention to the performance of these activities. The main\ndifference between our indicator and other activity sensitivity metrics in the\nliterature (e.g. cruciality, criticality, significance, or schedule sensitivity\nindices) is that our indicator is based on the Schedule Risk Baseline concept\ninstead of on cost or schedule baselines. The new metric not only provides\ninformation at the beginning of the project, but also while it is underway.\nFurthermore, the ARI is the only one to offer a normalized result: if we add\nits value for each activity, the total sum is 100%.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.00078v1"
    },
    {
        "title": "Stochastic Earned Value Analysis using Monte Carlo Simulation and\n  Statistical Learning Techniques",
        "authors": [
            "Fernando Acebes",
            "M Pereda",
            "David Poza",
            "Javier Pajares",
            "Jose M Galan"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The aim of this paper is to describe a new an integrated methodology for\nproject control under uncertainty. This proposal is based on Earned Value\nMethodology and risk analysis and presents several refinements to previous\nmethodologies. More specifically, the approach uses extensive Monte Carlo\nsimulation to obtain information about the expected behavior of the project.\nThis dataset is exploited in several ways using different statistical learning\nmethodologies in a structured fashion. Initially, simulations are used to\ndetect if project deviations are a consequence of the expected variability\nusing Anomaly Detection algorithms. If the project follows this expected\nvariability, probabilities of success in cost and time and expected cost and\ntotal duration of the project can be estimated using classification and\nregression approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.02589v1"
    },
    {
        "title": "Adaptive combinations of tail-risk forecasts",
        "authors": [
            "Alessandra Amendola",
            "Vincenzo Candila",
            "Antonio Naimoli",
            "Giuseppe Storti"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In order to meet the increasingly stringent global standards of banking\nmanagement and regulation, several methods have been proposed in the literature\nfor forecasting tail risk measures such as the Value-at-Risk (VaR) and Expected\nShortfall (ES). However, regardless of the approach used, there are several\nsources of uncertainty, including model specifications, data-related issues and\nthe estimation procedure, which can significantly affect the accuracy of VaR\nand ES measures. Aiming to mitigate the influence of these sources of\nuncertainty and improve the predictive performance of individual models, we\npropose novel forecast combination strategies based on the Model Confidence Set\n(MCS). In particular, consistent joint VaR and ES loss functions within the MCS\nframework are used to adaptively combine forecasts generated by a wide range of\nparametric, semi-parametric, and non-parametric models. Our results reveal that\nthe proposed combined predictors provide a suitable alternative for forecasting\nrisk measures, passing the usual backtests, entering the set of superior models\nof the MCS, and usually exhibiting lower standard deviations than other model\nspecifications.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.06235v1"
    },
    {
        "title": "Lessons From Model Risk Management in Financial Institutions for\n  Academic Research",
        "authors": [
            "Mahmood Alaghmandan",
            "Olga Streltchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this paper, we discuss aspects of model risk management in financial\ninstitutions which could be adopted by academic institutions to improve the\nprocess of conducting academic research, identify and mitigate existing\nlimitations, decrease the possibility of erroneous results, and prevent\nfraudulent activities.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.14776v1"
    },
    {
        "title": "The Merton's Default Risk Model for Public Company",
        "authors": [
            "Battulga Gankhuu"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this paper, we developed the Merton's structural model for public\ncompanies under an assumption that liabilities of the companies are observed.\nUsing Campbell and Shiller's approximation method, we obtain formulas of\nrisk-neutral equity and liability values and default probabilities for the\npublic companies. Also, the paper provides ML estimators of suggested model's\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.18121v3"
    },
    {
        "title": "CAESar: Conditional Autoregressive Expected Shortfall",
        "authors": [
            "Federico Gatta",
            "Fabrizio Lillo",
            "Piero Mazzarisi"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In financial risk management, Value at Risk (VaR) is widely used to estimate\npotential portfolio losses. VaR's limitation is its inability to account for\nthe magnitude of losses beyond a certain threshold. Expected Shortfall (ES)\naddresses this by providing the conditional expectation of such exceedances,\noffering a more comprehensive measure of tail risk. Despite its benefits, ES is\nnot elicitable on its own, complicating its direct estimation. However, joint\nelicitability with VaR allows for their combined estimation. Building on this,\nwe propose a new methodology named Conditional Autoregressive Expected\nShortfall (CAESar), inspired by the CAViaR model. CAESar handles dynamic\npatterns flexibly and includes heteroskedastic effects for both VaR and ES,\nwith no distributional assumption on price returns. CAESar involves a\nthree-step process: estimating VaR via CAViaR regression, formulating ES in an\nautoregressive manner, and jointly estimating VaR and ES while ensuring a\nmonotonicity constraint to avoid crossing quantiles. By employing various\nbacktesting procedures, we show the effectiveness of CAESar through extensive\nsimulations and empirical testing on daily financial data. Our results\ndemonstrate that CAESar outperforms existing regression methods in terms of\nforecasting performance, making it a robust tool for financial risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.06619v1"
    },
    {
        "title": "Credit Risk Assessment Model for UAE Commercial Banks: A Machine\n  Learning Approach",
        "authors": [
            "Aditya Saxena",
            "Dr Parizad Dungore"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Credit ratings are becoming one of the primary references for financial\ninstitutions of the country to assess credit risk in order to accurately\npredict the likelihood of business failure of an individual or an enterprise.\nFinancial institutions, therefore, depend on credit rating tools and services\nto help them predict the ability of creditors to meet financial persuasions.\nConventional credit rating is broadly categorized into two classes namely: good\ncredit and bad credit. This approach lacks adequate precision to perform credit\nrisk analysis in practice. Related studies have shown that data-driven machine\nlearning algorithms outperform many conventional statistical approaches in\nsolving this type of problem, both in terms of accuracy and efficiency. The\npurpose of this paper is to construct and validate a credit risk assessment\nmodel using Linear Discriminant Analysis as a dimensionality reduction\ntechnique to discriminate good creditors from bad ones and identify the best\nclassifier for credit assessment of commercial banks based on real-world data.\nThis will help commercial banks to avoid monetary losses and prevent financial\ncrisis\n",
        "pdf_link": "http://arxiv.org/pdf/2407.12044v1"
    },
    {
        "title": "Analyzing selected cryptocurrencies spillover effects on global\n  financial indices: Comparing risk measures using conventional and\n  eGARCH-EVT-Copula approaches",
        "authors": [
            "Shafique Ur Rehman",
            "Touqeer Ahmad",
            "Wu Dash Desheng",
            "Amirhossein Karamoozian"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This study examines the interdependence between cryptocurrencies and\ninternational financial indices, such as MSCI World and MSCI Emerging Markets.\nWe compute the value at risk, expected shortfall (ES), and range value at risk\n(RVaR) and investigate the dynamics of risk spillover. We employ a hybrid\napproach to derive these risk measures that integrate GARCH models, extreme\nvalue models, and copula functions. This framework uses a bivariate portfolio\napproach involving cryptocurrency data and traditional financial indices. To\nestimate the above risks of these portfolio structures, we employ symmetric and\nasymmetric GARCH and both tail flexible EVT models as marginal to model the\nmarginal distribution of each return series and apply different copula\nfunctions to connect the pairs of marginal distributions into a multivariate\ndistribution. The empirical findings indicate that the eGARCH EVT-based copula\nmodel adeptly captures intricate dependencies, surpassing conventional\nmethodologies like Historical simulations and t-distributed parametric in VaR\nestimation. At the same time, the HS method proves superior for ES, and the\nt-distributed parametric method outperforms RVaR. Eventually, the\nDiebold-Yilmaz approach will be applied to compute risk spillovers between four\nsets of asset sequences. This phenomenon implies that cryptocurrencies reveal\nsubstantial spillover effects among themselves but minimal impact on other\nassets. From this, it can be concluded that cryptocurrencies propose\ndiversification benefits and do not provide hedging advantages within an\ninvestor's portfolio. Our results underline RVaR superiority over ES regarding\nregulatory arbitrage and model misspecification. The conclusions of this study\nwill benefit investors and financial market professionals who aspire to\ncomprehend digital currencies as a novel asset class and attain perspicuity in\nregulatory arbitrage.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.15766v1"
    },
    {
        "title": "Counter-monotonic risk allocations and distortion risk measures",
        "authors": [
            "Mario Ghossoub",
            "Qinghua Ren",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In risk-sharing markets with aggregate uncertainty, characterizing\nPareto-optimal allocations when agents might not be risk averse is a\nchallenging task, and the literature has only provided limited explicit results\nthus far. In particular, Pareto optima in such a setting may not necessarily be\ncomonotonic, in contrast to the case of risk-averse agents. In fact, when\nmarket participants are risk-seeking, Pareto-optimal allocations are\ncounter-monotonic. Counter-monotonicity of Pareto optima also arises in some\nsituations for quantile-optimizing agents. In this paper, we provide a\nsystematic study of efficient risk sharing in markets where allocations are\nconstrained to be counter-monotonic. The preferences of the agents are modelled\nby a common distortion risk measure, or equivalently, by a common Yaari dual\nutility. We consider three different settings: risk-averse agents, risk-seeking\nagents, and those with an inverse S-shaped distortion function. In each case,\nwe provide useful characterizations of optimal allocations, for both the\ncounter-monotonic market and the unconstrained market. To illustrate our\nresults, we consider an application to a portfolio choice problem for a\nportfolio manager tasked with managing the investments of a group of clients,\nwith varying levels of risk aversion or risk seeking. We determine explicitly\nthe optimal investment strategies in this case. Our results confirm the\nintuition that a manager investing on behalf of risk-seeking agents tends to\ninvest more in risky assets than a manager acting on behalf of risk-averse\nagents.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16099v1"
    },
    {
        "title": "On the Separability of Vector-Valued Risk Measures",
        "authors": [
            "Çağın Ararat",
            "Zachary Feinstein"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Risk measures for random vectors have been considered in multi-asset markets\nwith transaction costs and financial networks in the literature. While the\ntheory of set-valued risk measures provide an axiomatic framework for assigning\nto a random vector its set of all capital requirements or allocation vectors,\nthe actual decision-making process requires an additional rule to select from\nthis set. In this paper, we define vector-valued risk measures by an analogous\nlist of axioms and show that, in the convex and lower semicontinuous case, such\nfunctionals always ignore the dependence structures of the input random\nvectors. We also show that set-valued risk measures do not have this issue as\nlong as they do not reduce to a vector-valued functional. Finally, we\ndemonstrate that our results also generalize to the conditional setting. These\nresults imply that convex vector-valued risk measures are not suitable for\ndefining capital allocation rules for a wide range of financial applications\nincluding systemic risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.16878v1"
    },
    {
        "title": "An Integrated Approach to Importance Sampling and Machine Learning for\n  Efficient Monte Carlo Estimation of Distortion Risk Measures in Black Box\n  Models",
        "authors": [
            "Sören Bettels",
            "Stefan Weber"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Distortion risk measures play a critical role in quantifying risks associated\nwith uncertain outcomes. Accurately estimating these risk measures in the\ncontext of computationally expensive simulation models that lack analytical\ntractability is fundamental to effective risk management and decision making.\nIn this paper, we propose an efficient important sampling method for distortion\nrisk measures in such models that reduces the computational cost through\nmachine learning. We demonstrate the applicability and efficiency of the Monte\nCarlo method in numerical experiments on various distortion risk measures and\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.02401v2"
    },
    {
        "title": "Risk sharing with Lambda value at risk under heterogeneous beliefs",
        "authors": [
            "Peng Liu",
            "Andreas Tsanakas",
            "Yunran Wei"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this paper, we study the risk sharing problem among multiple agents using\nLambda Value-at-Risk as their preference functional, under heterogeneous\nbeliefs, where beliefs are represented by several probability measures. We\nobtain semi-explicit formulas for the inf-convolution of multiple Lambda\nValue-at-Risk measures under heterogeneous beliefs and the explicit forms of\nthe corresponding optimal allocations. To show the impact of belief\nheterogeneity, we consider three cases: homogeneous beliefs, conditional\nbeliefs and absolutely continuous beliefs. For those cases, we find more\nexplicit expressions for the inf-convolution, showing the influence of the\nrelation of the beliefs on the inf-convolution. Moreover, we consider, in a\ntwo-agent setting, the inf-convolution of one Lambda Value-at-Risk and a\ngeneral risk measure, including expected utility, distortion risk measures and\nLambda Value-at-Risk as special cases, with differing beliefs. The expression\nof the inf-convolution and the form of the optimal allocation are obtained. In\nall above cases we demonstrate that trivial outcomes arise when both belief\ninconsistency and risk tolerance are high. Finally, we discuss risk sharing for\nan alternative definition of Lambda Value-at-Risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.03147v2"
    },
    {
        "title": "The Concentration Risk Indicator: Raising the Bar for Financial\n  Stability and Portfolio Performance Measurement",
        "authors": [
            "Ravi Kashyap"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We have developed a novel risk management measure called the concentration\nrisk indicator (CRI). The CRI has been created to address drawbacks with\nprevailing methodologies and to supplement existing methods. Modified and\nadapted from the Herfindahl-Hirschman (HH) index, the CRI can give a single\nnumeric score that can be helpful to evaluate the extent of risks that arise\nfrom holding concentrated portfolios. We discuss how the CRI can become an\nindicator of financial stability at any desired aggregation unit: regional,\nnational or international level. We show how the CRI can be easily applied to\ninsurance risk and to any product portfolio mix. The CRI is particularly\napplicable to the current facet of the decentralized terrain, wherein the\nmajority of the wealth is restricted to a small number of tokens. We calculate\nand report the CRI -- along with other risk metrics -- for individual assets\nand portfolios of crypto assets using a daily data sample from January 01, 2019\nuntil August 10, 2022. The CRI is an example of developing metrics that can\nuseful for sending concise yet powerful messages to the relevant audience. This\ntactic -- which can be described as marketing the benefits of any product or\nservice by using concepts from multiple disciplines -- of creating new metrics\ngoes further beyond the use of metrics to evaluate marketing efficacy. The\nsimplicity of our metric -- and the intuitive explanations we have provided for\nthe CRI -- makes it straightforward to properly articulate a strong -- clear\nand positive -- signal as part of marketing campaigns. The development -- and\nimplementation -- of new risk management metrics will have greater impact when\na wider rigorous risk management process has been established. We discuss\nseveral topics related to bringing about more improved risk management across\nall types of institutions and assets.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07271v1"
    },
    {
        "title": "Infinite-mean models in risk management: Discussions and recent advances",
        "authors": [
            "Yuyu Chen",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In statistical analysis, many classic results require the assumption that\nmodels have finite mean or variance, including the most standard versions of\nthe laws of large numbers and the central limit theorems. Such an assumption\nmay not be completely innocent, and it may not be appropriate for datasets with\nheavy tails (e.g., catastrophic losses), relevant to financial risk management.\nIn this paper, we discuss the importance of infinite-mean models in economics,\nfinance, and related fields, with recent results and examples. We emphasize\nthat many results or intuitions that hold for finite-mean models turn out to\nfail or even flip for infinite-mean models. Due to the breakdown of standard\nthinking for infinite-mean models, we argue that if the possibility of using\ninfinite-mean models cannot be excluded, great caution should be taken when\napplying classic methods that are usually designed for finite-mean cases in\nfinance and insurance.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.08678v2"
    },
    {
        "title": "Using Fermat-Torricelli points in assessing investment risks",
        "authors": [
            "Sergey Yekimov"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The use of Fermat-Torricelli points can be an effective mathematical tool for\nanalyzing numerical series that have a large variance, a pronounced nonlinear\ntrend, or do not have a normal distribution of a random variable. Linear\ndependencies are very rare in nature. Smoothing numerical series by\nconstructing Fermat-Torricelli points reduces the influence of the random\ncomponent on the final result.\n  The presence of a normal distribution of a random variable for numerical\nseries that relate to long time intervals is an exception to the rule rather\nthan an axiom. The external environment (international economic relations,\nscientific and technological progress, political events) is constantly\nchanging, which in turn, in general, does not give grounds to assert that under\nthese conditions a random variable satisfies the requirements of the\nGauss-Markov theorem.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09267v1"
    },
    {
        "title": "Contemporaneous and lagged spillovers between agriculture, crude oil,\n  carbon emission allowance, and climate change",
        "authors": [
            "Yan-Hong Yang",
            "Ying-Hui Shao",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this paper, we examine the dynamic spillovers among the crude oil, carbon\nemission allowance, climate change, and agricultural markets. Adopting a novel\n$R^2$ decomposed connectedness approach, our empirical analysis reveals several\nkey findings. The overall TCI dynamics have been mainly dominated by\ncontemporaneous dynamics rather than the lagged dynamics. We also find climate\nchange has significant spillovers to other markets. Moreover, there are\nheterogeneous spillover effects among agricultural markets. Specially, corn is\nthe biggest risk contributor to this system, while barley is the major risk\nreceiver of shocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09669v2"
    },
    {
        "title": "Optimal insurance design with Lambda-Value-at-Risk",
        "authors": [
            "Tim J. Boonen",
            "Yuyu Chen",
            "Xia Han",
            "Qiuqi Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper explores optimal insurance solutions based on the\nLambda-Value-at-Risk ($\\Lambda\\VaR$). If the expected value premium principle\nis used, our findings confirm that, similar to the VaR model, a truncated\nstop-loss indemnity is optimal in the $\\Lambda\\VaR$ model. We further provide a\nclosed-form expression of the deductible parameter under certain conditions.\nMoreover, we study the use of a $\\Lambda'\\VaR$ as premium principle as well,\nand show that full or no insurance is optimal. Dual stop-loss is shown to be\noptimal if we use a $\\Lambda'\\VaR$ only to determine the risk-loading in the\npremium principle. Moreover, we study the impact of model uncertainty,\nconsidering situations where the loss distribution is unknown but falls within\na defined uncertainty set. Our findings indicate that a truncated stop-loss\nindemnity is optimal when the uncertainty set is based on a likelihood ratio.\nHowever, when uncertainty arises from the first two moments of the loss\nvariable, we provide the closed-form optimal deductible in a stop-loss\nindemnity.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09799v1"
    },
    {
        "title": "A novel k-generation propagation model for cyber risk and its\n  application to cyber insurance",
        "authors": [
            "Na Ren",
            "Xin Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The frequent occurrence of cyber risks and their serious economic\nconsequences have created a growth market for cyber insurance. The calculation\nof aggregate losses, an essential step in insurance pricing, has attracted\nconsiderable attention in recent years. This research develops a path-based\nk-generation risk contagion model in a tree-shaped network structure that\nincorporates the impact of the origin contagion location and the heterogeneity\nof security levels on contagion probability and local loss, distinguishing it\nfrom most existing models. Furthermore, we discuss the properties of\nk-generation risk contagion among multi-paths using the concept of d-separation\nin Bayesian network (BN), and derive explicit expressions for the mean and\nvariance of local loss on a single path. By combining these results, we compute\nthe mean and variance values for aggregate loss across the entire network until\ntime $t$, which is crucial for accurate cyber insurance pricing. Finally,\nthrough numerical calculations and relevant probability properties, we have\nobtained several findings that are valuable to risk managers and insurers.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.14151v1"
    },
    {
        "title": "Lapse-supported life insurance and adverse selection",
        "authors": [
            "Oytun Haçarız",
            "Torsten Kleinow",
            "Angus S. Macdonald"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  If individuals at the highest mortality risk are also least likely to lapse a\nlife insurance policy, then lapse-supported premiums magnify adverse selection\ncosts. As an example, we model 'Term to 100' contracts, and risk as revealed by\ngenetic test results. We identify three methods of managing lapse surplus:\neliminating it by design; disposing of it retrospectively (through\nparticipation); or disposing of it prospectively (through lapse-supported\npremiums). We then assume a heterogeneous population in which: (a) insurers\ncannot identify individuals at high mortality risk; (b) a secondary market\nexists that prevents high-risk policies from lapsing; (c) financial\nunderwriting is lax or absent; and (d) life insurance policies may even be\ninitiated by third parties as a financial investment (STOLI). Adverse selection\nlosses under (a) are typically very small, but under (b) can be increased by\nmultiples, and under (c) and (d) increased almost without limit. We note that\nthe different approaches to modeling lapses used in studies of adverse\nselection and genetic testing appear to be broadly equivalent and robust.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.01843v1"
    },
    {
        "title": "Quantifying Seasonal Weather Risk in Indian Markets: Stochastic Model\n  for Risk-Averse State-Specific Temperature Derivative Pricing",
        "authors": [
            "Soumil Hooda",
            "Shubham Sharma",
            "Kunal Bansal"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This technical report presents a stochastic model for pricing weather\nderivatives and devising hedging strategies tailored to Indian markets. We\nmodel temperature dynamics using a modified Ornstein-Uhlenbeck process with\njumps to account for sudden shocks, such as heatwaves and coldwaves. Historical\ndata from 12 Indian states (1951-2023) is used for calibration, and Monte Carlo\nsimulations are employed under the risk-neutral measure to price Heating Degree\nDays (HDD), Cooling Degree Days (CDD), and extreme event options. Sensitivity\nanalysis reveals that a 20% increase in volatility leads to an approximate 4.2%\nincrease in option prices, highlighting the critical impact of volatility on\nderivative pricing. Results show that HDD options in colder states like\nHimachal Pradesh are significantly more expensive, with prices reaching up to\nINR 684,693, while CDD options in hotter states like Gujarat are priced higher,\nup to INR 262,986. A comprehensive portfolio analysis indicates that investing\nINR 120,000 in HDD put options in Uttar Pradesh yields an expected payoff of\nINR 132,369, resulting in a return on investment (ROI) of 10.3%. Conversely, a\nsimilar investment in Karnataka yields a negative ROI of -66.7% due to its\nmilder climate. Hedging strategies are tailored to each state's climatic risk,\nwith recommendations to buy 90.66 HDD put options at a strike of 90.89 in Uttar\nPradesh and invest in CDD call options in Gujarat. These insights offer\npractical solutions for managing temperature-related financial risk in energy\nand agriculture, providing actionable, state-specific hedging strategies for\ndiverse climatic scenarios in India.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.04541v2"
    },
    {
        "title": "Pareto-Optimal Peer-to-Peer Risk Sharing with Robust Distortion Risk\n  Measures",
        "authors": [
            "Mario Ghossoub",
            "Michael B. Zhu",
            "Wing Fung Chong"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We study Pareto optimality in a decentralized peer-to-peer risk-sharing\nmarket where agents' preferences are represented by robust distortion risk\nmeasures that are not necessarily convex. We obtain a characterization of\nPareto-optimal allocations of the aggregate risk in the market, and we show\nthat the shape of the allocations depends primarily on each agent's assessment\nof the tail of the aggregate risk. We quantify the latter via an index of\nprobabilistic risk aversion, and we illustrate our results using concrete\nexamples of popular families of distortion functions. As an application of our\nresults, we revisit the market for flood risk insurance in the United States.\nWe present the decentralized risk sharing arrangement as an alternative to the\ncurrent centralized market structure, and we characterize the optimal\nallocations in a numerical study with historical flood data. We conclude with\nan in-depth discussion of the advantages and disadvantages of a decentralized\ninsurance scheme in this setting.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05103v1"
    },
    {
        "title": "Managing Basis Risks in Weather Parametric Insurance: A Quantitative\n  Study of Diversification and Key Influencing Factors",
        "authors": [
            "Hang Gao",
            "Shuohua Yang",
            "Xinli Liu"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Weather parametric insurance relies on weather indices rather than actual\nloss assessments, enhancing claims efficiency, reducing moral hazard, and\nimproving fairness. In the context of increasing climate change risks, despite\ngrowing interest and demand,, weather parametric insurance's market share\nremains limited due to inherent basis risk, which is the mismatch between\nactual loss and payout, leading to loss without payout or payout without loss.\nThis paper proposes a novel empirical research using Monte Carlo simulations to\ntest whether basis risk can be managed through diversification and hedged like\nother risks. Key findings include: Firstly, portfolio basis risk and volatility\ndecrease as the number of contracts increases. Secondly, spatial relationships\nsignificantly impact basis risk, with risk levels correlating with the ratio\nbetween insured location, weather station, and disaster footprint radius, and\nthirdly, event severity does not significantly impact basis risk, suggesting\nthat catastrophic disaster severity should not hinder parametric insurance\ndevelopment.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.16599v1"
    },
    {
        "title": "Improving Estimation of Portfolio Risk Using New Statistical Factors",
        "authors": [
            "Xialu Liu",
            "John Guerard",
            "Rong Chen",
            "Ruey Tsay"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Searching for new effective risk factors on stock returns is an important\nresearch topic in asset pricing. Factor modeling is an active research topic in\nstatistics and econometrics, with many new advances. However, these new methods\nhave not been fully utilized in asset pricing application. In this paper, we\nadopt the factor models, especially matrix factor models in various forms, to\nconstruct new statistical factors that explain the variation of stock returns.\nFurthermore, we evaluate the contribution of these statistical factors beyond\nthe existing factors available in the asset pricing literature. To demonstrate\nthe power of the new factors, U.S. monthly stock data are analyzed and the\npartial F test and double selection LASSO method are conducted. The results\nshow that the new statistical factors bring additional information and add\nexplanatory power in asset pricing. Our method opens a new direction for\nportfolio managers to seek additional risk factors to improve the estimation of\nportfolio returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.17182v1"
    },
    {
        "title": "The resilience of China's financial markets: With a focus on the impact\n  of its climate policy uncertainty",
        "authors": [
            "Si-yao Wei",
            "Wei-xing Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Resilience serves to assess the ability of financial markets to resist\nexternal shocks. The intensity and duration, used to indicate resilience, are\ncalculated for China's financial markets in this paper, focusing on the\nperformance of each financial market during and after several crises. Given\nthat climate issues have been recognized as an important source of risk by\nfinancial markets, we also investigate the spillover effects and mechanism of\nChina's climate policy uncertainty on its financial markets resilience. We have\nfound that the two resilience indicators of each market have a relatively\nconsistent trend, but spillovers among markets have different sensitivities to\nthe both. In addition, China's climate policy uncertainty shocks its financial\nmarkets resilience by increasing the investor sentiment index and the\nnon-performing loan ratio of commercial banks and by reducing the capital and\nfinancial account balance. It is further found that China's financial markets'\nconsensus on the unswerving implementation of climate policy, which provides\nthe reference for other countries on how to balance climate policies\nintroduction and financial markets development.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.18422v1"
    },
    {
        "title": "Quantile connectedness across BRICS and international grain futures\n  markets: Insights from the Russia-Ukraine conflict",
        "authors": [
            "Yan-Hong Yang",
            "Ying-Hui Shao",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This study examines the quantile connectedness among grain futures markets in\nBRICS and international markets, with a particular focus on the ongoing and\nescalating impacts of the Russia-Ukraine conflict. The findings reveal\nsignificant heterogeneity in spillover effects across different quantiles and\nmarket conditions. Specifically, the time-varying total connectedness index\n(TCI) consistently fluctuated around 95\\% under both extreme bearish and\nbullish market conditions, markedly higher than in normal market conditions.\nMoreover, across all quantile levels, the TCI was higher during the\npre-outbreak period than in the post-outbreak period. This systemic risk has\nnotably decreased following the onset of the Russia-Ukraine conflict and the\nsubsequent changes to the Black Sea Grain Initiative. Apart from rice, U.S.\ngrain futures maintained a dominant position as benchmarks for international\ngrain prices, exerting substantial influence over the grain futures markets in\nBRICS throughout most of the period. Finally, the study highlights that the\ninfluence of grain type and regional proximity strengthens pairwise\nconnectedness among futures markets, with short-term spillovers being dominant\nand the spillover effect generally symmetric across quantiles.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19307v1"
    },
    {
        "title": "Best- and worst-case Scenarios for GlueVaR distortion risk measure with\n  Incomplete information",
        "authors": [
            "Mengshuo Zhao",
            "Chuancun Yin"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper derives the best- and worst-case GlueVaR distortion risk measure\nwithin a unified framework, based on partial information of the underlying\ndistributions and shape information such as symmetry. In addition, we\ncharacterize the extremal distributions of GlueVaR with convex envelopes of the\ncorresponding distortion functions. As examples, extremal cases of VaR, TVaR\nand RVaR are derived.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19902v1"
    },
    {
        "title": "Compound V3 Economic Audit Report",
        "authors": [
            "Rik Ghosh",
            "Samrat Gupta",
            "Arka Datta",
            "Abhimanyu Nag",
            "Sudipan Sinha"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Compound Finance is a decentralized lending protocol that enables the secure\nand efficient borrowing and lending of cryptocurrencies, utilizing smart\ncontracts and dynamic interest rates based on supply and demand to facilitate\ntransactions. The protocol enables users to supply different crypto assets and\naccrue interest, while borrowers can avail themselves of loans secured by\ncollateralized assets. Our collaboration with Compound Finance focuses on\nharnessing the power of the Chainrisk simulation engine to optimize risk\nparameters of the Compound V3 (Comet) protocol. This report delineates a\ncomprehensive methodology aimed at calculating key risk metrics of the\nprotocol. This optimization framework is pivotal for mitigating systemic risks\nand enhancing the overall stability of the protocol. By leveraging Chainrisk's\nCloud Platform, we conduct millions of simulations to evaluate the protocol's\nValue at Risk (VaR) and Liquidations at Risk (LaR), ultimately providing\nrecommendations for parameter adjustments.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.04085v1"
    },
    {
        "title": "Deviance Voronoi Residuals for Space-Time Point Process Models: An\n  Application to Earthquake Insurance Risk",
        "authors": [
            "Roba Bairakdar",
            "Debbie Dupuis",
            "Melina Mailhot"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Insurance risk arising from catastrophes such as earthquakes a component of\nthe Minimum Capital Test for federally regulated property and casualty\ninsurance companies. Analyzing earthquake insurance risk requires well-fitted\nspatio-temporal point process models. Given the spatial heterogeneity of\nearthquakes, the ability to assess whether the fits are adequate in certain\nlocations is crucial in obtaining usable models. Accordingly, we extend the use\nof Voronoi residuals to calculate deviance Voronoi residuals. We also create a\nsimulation-based approach, in which losses and insurance claim payments are\ncalculated by relying on earthquake hazard maps of Canada. As an alternative to\nthe current guidelines of OSFI, a formula to calculate the country-wide minimum\ncapital test is proposed based on the correlation between the provinces.\nFinally, an interactive web application is provided which allows the user to\nsimulate earthquake damage and the resulting financial losses and insurance\nclaims, at a chosen epicenter location.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.04369v1"
    },
    {
        "title": "Impact of social factors on loan delinquency in microfinance",
        "authors": [
            "Cedric H. A. Koffi",
            "Viani Biatat Djeundje",
            "Olivier Menoukeu Pamen"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper develops multistate models to analyse loan delinquency in the\nmicrofinance sector, using data from Ghana. The models are designed to account\nfor both partial repayments and the short repayment durations typical in\nmicrofinance, focusing on estimating the probability of transitions between two\nor three repayment states, including delinquency. Social variables, such as\nreligious and cultural factors, were found to play a statistically significant\nrole in influencing repayment behavior, highlighting the impact of societal\ndynamics on financial outcomes. We explored both time-independent and\ntime-dependent frailty models to capture unobserved heterogeneity. Overall, the\nfindings emphasize the importance of social factors in delinquency but suggest\nlimited predictive gains from incorporating frailties into multistate models.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.13100v1"
    },
    {
        "title": "Portfolio credit risk with Archimedean copulas: asymptotic analysis and\n  efficient simulation",
        "authors": [
            "Hengxin Cui",
            "Ken Seng Tan",
            "Fan Yang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this paper, we study large losses arising from defaults of a credit\nportfolio. We assume that the portfolio dependence structure is modelled by the\nArchimedean copula family as opposed to the widely used Gaussian copula. The\nresulting model is new, and it has the capability of capturing extremal\ndependence among obligors. We first derive sharp asymptotics for the tail\nprobability of portfolio losses and the expected shortfall. Then we demonstrate\nhow to utilize these asymptotic results to produce two variance reduction\nalgorithms that significantly enhance the classical Monte Carlo methods.\nMoreover, we show that the estimator based on the proposed two-step importance\nsampling method is logarithmically efficient while the estimator based on the\nconditional Monte Carlo method has bounded relative error as the number of\nobligors tends to infinity. Extensive simulation studies are conducted to\nhighlight the efficiency of our proposed algorithms for estimating portfolio\ncredit risk. In particular, the variance reduction achieved by the proposed\nconditional Monte Carlo method, relative to the crude Monte Carlo method, is in\nthe order of millions.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.06640v1"
    },
    {
        "title": "Asymptotic Properties of Generalized Shortfall Risk Measures for\n  Heavy-tailed Risks",
        "authors": [
            "Tiantian Mao",
            "Gilles Stupfler",
            "Fan Yang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We study a general risk measure called the generalized shortfall risk\nmeasure, which was first introduced in Mao and Cai (2018). It is proposed under\nthe rank-dependent expected utility framework, or equivalently induced from the\ncumulative prospect theory. This risk measure can be flexibly designed to\ncapture the decision maker's behavior toward risks and wealth when measuring\nrisk. In this paper, we derive the first- and second-order asymptotic\nexpansions for the generalized shortfall risk measure. Our asymptotic results\ncan be viewed as unifying theory for, among others, distortion risk measures\nand utility-based shortfall risk measures. They also provide a blueprint for\nthe estimation of these measures at extreme levels, and we illustrate this\nprinciple by constructing and studying a quantile-based estimator in a special\ncase. The accuracy of the asymptotic expansions and of the estimator is\nassessed on several numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07212v1"
    },
    {
        "title": "Asymptotics of Sum of Heavy-tailed Risks with Copulas",
        "authors": [
            "Fan Yang",
            "Yi Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We study the tail asymptotics of the sum of two heavy-tailed random\nvariables. The dependence structure is modeled by copulas with the so-called\ntail order property. Examples are presented to illustrate the approach. Further\nfor each example we apply the main results to obtain the asymptotic expansions\nfor Value-at-Risk of aggregate risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.09657v1"
    },
    {
        "title": "On Vulnerability Conditional Risk Measures: Comparisons and Applications\n  in Cryptocurrency Market",
        "authors": [
            "Tong Pu",
            "Yunran Wei",
            "Yiying Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We introduce a novel class of systemic risk measures, the Vulnerability\nConditional risk measures, which try to capture the \"tail risk\" of a risky\nposition in scenarios where one or more market participants is experiencing\nfinancial distress. Various theoretical properties of Vulnerability Conditional\nrisk measures, along with a series of related contribution measures, have been\nconsidered in this paper. We further introduce the backtesting procedures of\nVCoES and MCoES. Through numerical examples, we validate our theoretical\ninsights and further apply our newly proposed risk measures to the empirical\nanalysis of cryptocurrencies, demonstrating their practical relevance and\nutility in capturing systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.09676v1"
    },
    {
        "title": "Comparisons of multivariate contribution measures of risk contagion and\n  their applications in cryptocurrency market",
        "authors": [
            "Limin Wen",
            "Junxue Li",
            "Tong Pu",
            "Yiying Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Conditional risk measures and their associated risk contribution measures are\ncommonly employed in finance and actuarial science for evaluating systemic risk\nand quantifying the effects of risk contagion. This paper introduces various\ntypes of contribution measures based on the MCoVaR, MCoES, and MMME studied in\nOrtega-Jim\\'enez et al. (2021) and Das & Fasen-Hartmann (2018) to assess both\nthe absolute and relative effects of a single risk when other risks in a group\nare in distress. The properties of these contribution risk measures are\nexamined, and sufficient conditions for comparing these measures between two\nsets of random vectors are established using univariate and multivariate\nstochastic orders and stochastic dependence notions. Numerical examples are\npresented for validating the conditions. Finally, a real dataset from the\ncryptocurrency market is also utilized to analyze the contagion effect in terms\nof our proposed contribution measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13384v1"
    },
    {
        "title": "Self-protection and insurance demand with convex premium principles",
        "authors": [
            "Qiqi Li",
            "Wei Wang",
            "Yiying Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In economic analysis, rational decision-makers often take actions to reduce\ntheir risk exposure. These actions include purchasing market insurance and\nimplementing prevention measures to modify the shape of the loss distribution.\nUnder the assumption that the insureds' actions are fully observed by the\ninsurer, this paper investigates the interaction between self-protection and\ninsurance demand when insurance premiums are determined by convex premium\nprinciples within the framework of distortion risk measures. Specifically, the\ninsured selects an optimal proportional insurance share and prevention effort\nto minimize the risk measure of their end-of-period exposure. We explicitly\ncharacterize the optimal combination of prevention effort and insurance demand\nin a self-protection model when the insured adopts tail value-at-risk and\nstrictly convex distortion risk measures, respectively. Additionally, we\nconduct comparative static analyses to illustrate our main findings under\nvarious premium structures, risk aversion levels, and loss distributions. Our\nresults indicate that market insurance and self-protection are complementary,\nsupporting classical insights from the literature regarding corner insurance\npolicies (i.e., null and full insurance) in the absence of ex ante moral\nhazard. Finally, we consider the effects of moral hazard on the interaction\nbetween self-protection and insurance demand. Our findings show that ex ante\nmoral hazard shifts the complementary effect into substitution effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.19436v1"
    },
    {
        "title": "Counter-monotonic Risk Sharing with Heterogeneous Distortion Risk\n  Measures",
        "authors": [
            "Mario Ghossoub",
            "Qinghua Ren",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We study risk sharing among agents with preferences modeled by heterogeneous\ndistortion risk measures, who are not necessarily risk averse. Pareto\noptimality for agents using risk measures is often studied through the lens of\ninf-convolutions, because allocations that attain the inf-convolution are\nPareto optimal, and the converse holds true under translation invariance. Our\nmain focus is on groups of agents who exhibit varying levels of risk seeking.\nUnder mild assumptions, we derive explicit solutions for the unconstrained\ninf-convolution and the counter-monotonic inf-convolution, which can be\nrepresented by a generalization of distortion risk measures. Furthermore, for a\ngroup of agents with different levels of risk aversion or risk seeking, we\nconsider a portfolio manager's problem and explicitly determine the optimal\ninvestment strategies. Interestingly, we observe a counterintuitive phenomenon\nof comparative statics: even if all agents in the group become more risk\nseeking, the portfolio manager acting on behalf of the group may not\nnecessarily allocate a larger proportion of investments to risky assets, which\nis in sharp contrast to the case of risk-averse agents.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.00655v1"
    },
    {
        "title": "On-Chain Credit Risk Score (OCCR Score) in DeFi",
        "authors": [
            "Rik Ghosh",
            "Arka Datta",
            "Vidhi Aggarwal",
            "Sudipan Sinha",
            "Abhimanyu Nag"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Decentralized Finance (DeFi) and smart contracts are the next generation and\nfast-growing market for quick and safe interaction between lenders and\nborrowers. However for maintaining a streamline ecosystem it is necessary to\nunderstand the risk associated with the particular user under consideration. In\nthis paper we have developed 'On Chain Credit Risk Score' of a wallet is an\nanswer to quantifying the risk of the particular wallet, namely the probability\nthat the particular wallet may have a loan liquidated. 'On Chain Credit Risk\nScore (OCCR Score)' of wallets, will help lending borrowing protocols and other\nDeFi institutes to understand the risk involved in giving out loans to a wallet\nand may change the Loan-to-Value (LTV) Ratio and subsequently the Liquidation\nThreshold (LT) if required.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.00710v1"
    },
    {
        "title": "Performance-based variable premium scheme and reinsurance design",
        "authors": [
            "David Landriault",
            "Fangda Liu",
            "Ziyue Shi"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In the literature, insurance and reinsurance pricing is typically determined\nby a premium principle, characterized by a risk measure that reflects the\npolicy seller's risk attitude. Building on the work of Meyers (1980) and Chen\net al. (2016), we propose a new performance-based variable premium scheme for\nreinsurance policies, where the premium depends on both the distribution of the\nceded loss and the actual realized loss. Under this scheme, the insurer and the\nreinsurer face a random premium at the beginning of the policy period. Based on\nthe realized loss, the premium is adjusted into either a ''reward'' or\n''penalty'' scenario, resulting in a discount or surcharge at the end of the\npolicy period. We characterize the optimal reinsurance policy from the\ninsurer's perspective under this new variable premium scheme. In addition, we\nformulate a Bowley optimization problem between the insurer and the monopoly\nreinsurer. Numerical examples demonstrate that, compared to the expected-value\npremium principle, the reinsurer prefers the variable premium scheme as it\nreduces the reinsurer's total risk exposure.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.01704v1"
    },
    {
        "title": "Research on Financial Multi-Asset Portfolio Risk Prediction Model Based\n  on Convolutional Neural Networks and Image Processing",
        "authors": [
            "Zhuohuan Hu",
            "Fu Lei",
            "Yuxin Fan",
            "Zong Ke",
            "Ge Shi",
            "Zichao Li"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In today's complex and volatile financial market environment, risk management\nof multi-asset portfolios faces significant challenges. Traditional risk\nassessment methods, due to their limited ability to capture complex\ncorrelations between assets, find it difficult to effectively cope with dynamic\nmarket changes. This paper proposes a multi-asset portfolio risk prediction\nmodel based on Convolutional Neural Networks (CNN). By utilizing image\nprocessing techniques, financial time series data are converted into\ntwo-dimensional images to extract high-order features and enhance the accuracy\nof risk prediction. Through empirical analysis of data from multiple asset\nclasses such as stocks, bonds, commodities, and foreign exchange, the results\nshow that the proposed CNN model significantly outperforms traditional models\nin terms of prediction accuracy and robustness, especially under extreme market\nconditions. This research provides a new method for financial risk management,\nwith important theoretical significance and practical value.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.03618v1"
    },
    {
        "title": "Tail Risk Alert Based on Conditional Autoregressive VaR by Regression\n  Quantiles and Machine Learning Algorithms",
        "authors": [
            "Zong Ke",
            "Yuchen Yin"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  As the increasing application of AI in finance, this paper will leverage AI\nalgorithms to examine tail risk and develop a model to alter tail risk to\npromote the stability of US financial markets, and enhance the resilience of\nthe US economy. Specifically, the paper constructs a multivariate multilevel\nCAViaR model, optimized by gradient descent and genetic algorithm, to study the\ntail risk spillover between the US stock market, foreign exchange market and\ncredit market. The model is used to provide early warning of related risks in\nUS stocks, US credit bonds, etc. The results show that, by analyzing the\ndirection, magnitude, and pseudo-impulse response of the risk spillover, it is\nfound that the credit market's spillover effect on the stock market and its\nduration are both greater than the spillover effect of the stock market and the\nother two markets on credit market, placing credit market in a central position\nfor warning of extreme risks. Its historical information on extreme risks can\nserve as a predictor of the VaR of other markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.06193v1"
    },
    {
        "title": "Risk spillovers between the BRICS and the U.S. staple grain futures\n  markets",
        "authors": [
            "Ying-Hui Shao",
            "Yan-Hong Yang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This study examines contemporaneous and lagged spillover effects in BRICS\nstaple grain futures markets and their linkages with U.S. markets. The results\nshow that contemporaneous spillovers dominate, while net spillovers are driven\nby lagged connectedness. Systemic risk is lower in intra-BRICS markets compared\nto those including the U.S., highlighting the U.S. grain market's significant\ninfluence. Brazilian and U.S. grains are key net spillover contributors,\nexcluding U.S. rice, while South African staple grains act as major net\nreceivers. Particularly, the spillover between soybeans is the strongest. The\nstudy also reveals heterogeneous impacts of the Russia-Ukraine conflict and\nBlack Sea Grain Initiative on grain futures.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.15738v2"
    },
    {
        "title": "Quantiles under ambiguity and risk sharing",
        "authors": [
            "Peng Liu",
            "Tiantian Mao",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Choquet capacities and integrals are central concepts in decision making\nunder ambiguity or model uncertainty, pioneered by Schmeidler. Motivated by\nrisk optimization problems for quantiles under ambiguity, we study the subclass\nof Choquet integrals, called Choquet quantiles, which generalizes the usual\n(probabilistic) quantiles, also known as Value-at-Risk in finance, from\nprobabilities to capacities. Choquet quantiles share many features with\nprobabilistic quantiles, in terms of axiomatic representation, optimization\nformulas, and risk sharing. We characterize Choquet quantiles via only one\naxiom, called ordinality. We prove that the inf-convolution of Choquet\nquantiles is again a Choquet quantile, leading to explicit optimal allocations\nin risk sharing problems for quantile agents under ambiguity. A new class of\nrisk measures, Choquet Expected Shortfall, is introduced, which enjoys most\nproperties of the coherent risk measure Expected Shortfall. Our theory is\ncomplemented by optimization algorithms, numerical examples, and a stylized\nillustration with financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.19546v1"
    },
    {
        "title": "Dynamic loan portfolio management in a three time step model",
        "authors": [
            "Deb Narayan Barik",
            "Siddhartha P. Chakrabarty"
        ],
        "category": "q-fin.RM",
        "published_year": "2025",
        "summary": "  This paper studies the bank dynamic decision problem in the intermediate time\nstep for a discrete-time setup. We have considered a three-time-step model.\nInitially, the banks raise money through debt and equity and invest in\ndifferent types of loans. It liquidates its assets and raises new funds at the\nintermediate-time step to meet the short-term debt holders claim. Further, it\nhas to meet specific capital requirements given by the regulators. In this\nwork, we have theoretically studied the effect of raising new equity and debt.\nWe show that in some cases, raising equity and debt may increase the return on\nequity, and in some cases, it may decrease the return on equity. We have\ndiscussed several cases and given a bound on the capital that can be raised. We\nhave added an equity holders constraint, which ensures the return on equity and\ndesists the bank from defaulting at the final time point.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07856v1"
    },
    {
        "title": "A note on bequest preferences in utility maximisation for modern\n  tontines",
        "authors": [
            "Thomas Bernhardt"
        ],
        "category": "q-fin.RM",
        "published_year": "2025",
        "summary": "  In this short note, we address two issues in the literature about modern\ntontines with bequest and utility maximisation: how to verify optimal controls\nand the decreasing allocation of funds in the tontine. We want to raise\nawareness in the actuarial community about the dual approach to solve optimal\ncontrol problems when working with power utilities. Additionally, we point out\nthat bequest preferences should be time-dependent or otherwise yield\nunrealistic investment strategies. We base our attempt at modelling bequest\npreferences on common sense rules like 100% payback upon death at the start\nthat vanishes over time. Our modelling shows that the resulting investment\nstrategy almost linearly adjusts the allocation in the tontine from 0% to 100%\nover time.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.08972v1"
    },
    {
        "title": "Average optimality for risk-sensitive control with general state space",
        "authors": [
            "Anna Jaśkiewicz"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  This paper deals with discrete-time Markov control processes on a general\nstate space. A long-run risk-sensitive average cost criterion is used as a\nperformance measure. The one-step cost function is nonnegative and possibly\nunbounded. Using the vanishing discount factor approach, the optimality\ninequality and an optimal stationary strategy for the decision maker are\nestablished.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0394v1"
    },
    {
        "title": "Large portfolio losses: A dynamic contagion model",
        "authors": [
            "Paolo Dai Pra",
            "Wolfgang J. Runggaldier",
            "Elena Sartori",
            "Marco Tolotti"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  Using particle system methodologies we study the propagation of financial\ndistress in a network of firms facing credit risk. We investigate the\nphenomenon of a credit crisis and quantify the losses that a bank may suffer in\na large credit portfolio. Applying a large deviation principle we compute the\nlimiting distributions of the system and determine the time evolution of the\ncredit quality indicators of the firms, deriving moreover the dynamics of a\nglobal financial health indicator. We finally describe a suitable version of\nthe \"Central Limit Theorem\" useful to study large portfolio losses. Simulation\nresults are provided as well as applications to portfolio loss distribution\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.1348v3"
    },
    {
        "title": "A Model for Counterparty Risk with Geometric Attenuation Effect and the\n  Valuation of CDS",
        "authors": [
            "Yunfen Bai",
            "Xinhua Hu",
            "Zhongxing Ye"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  In this paper, a geometric function is introduced to reflect the attenuation\nspeed of impact of one firm's default to its partner. If two firms are\ncompetitions (copartners), the default intensity of one firm will decrease\n(increase) abruptly when the other firm defaults. As time goes on, the impact\nwill decrease gradually until extinct. In this model, the joint distribution\nand marginal distributions of default times are derived by employing the change\nof measure, so can we value the fair swap premium of a CDS.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.3331v1"
    },
    {
        "title": "Valuations and dynamic convex risk measures",
        "authors": [
            "A. Jobert",
            "L. C. G. Rogers"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  This paper approaches the definition and properties of dynamic convex risk\nmeasures through the notion of a family of concave valuation operators\nsatisfying certain simple and credible axioms. Exploring these in the simplest\ncontext of a finite time set and finite sample space, we find natural\nrisk-transfer and time-consistency properties for a firm seeking to spread its\nrisk across a group of subsidiaries.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0232v1"
    },
    {
        "title": "Cash Sub-additive Risk Measures and Interest Rate Ambiguity",
        "authors": [
            "Nicole El Karoui",
            "Claudia Ravanelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  A new class of risk measures called cash sub-additive risk measures is\nintroduced to assess the risk of future financial, nonfinancial and insurance\npositions. The debated cash additive axiom is relaxed into the cash sub\nadditive axiom to preserve the original difference between the numeraire of the\ncurrent reserve amounts and future positions. Consequently, cash sub-additive\nrisk measures can model stochastic and/or ambiguous interest rates or\ndefaultable contingent claims. Practical examples are presented and in such\ncontexts cash additive risk measures cannot be used. Several representations of\nthe cash sub-additive risk measures are provided. The new risk measures are\ncharacterized by penalty functions defined on a set of sub-linear probability\nmeasures and can be represented using penalty functions associated with cash\nadditive risk measures defined on some extended spaces. The issue of the\noptimal risk transfer is studied in the new framework using inf-convolution\ntechniques. Examples of dynamic cash sub-additive risk measures are provided\nvia BSDEs where the generator can locally depend on the level of the cash\nsub-additive risk measure.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4106v1"
    },
    {
        "title": "Incorporating exchange rate risk into PDs and asset correlations",
        "authors": [
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  Intuitively, the default risk of a single borrower is higher when her or his\nassets and debt are denominated in different currencies. Additionally, the\ndefault dependence of borrowers with assets and debt in different currencies\nshould be stronger than in the one-currency case. By combining well-known\nmodels by Merton (1974), Garman and Kohlhagen (1983), and Vasicek (2002) we\ndevelop simple representations of PDs and asset correlations that take into\naccount exchange rate risk. From these results, consistency conditions can be\nderived that link the changes in PD and asset correlation and do not require\nknowledge of hard-to-estimate parameters like asset value volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.3363v1"
    },
    {
        "title": "Good deal bounds induced by shortfall risk",
        "authors": [
            "Takuji Arai"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  We shall provide in this paper good deal pricing bounds for contingent claims\ninduced by the shortfall risk with some loss function. Assumptions we impose on\nloss functions and contingent claims are very mild. We prove that the upper and\nlower bounds of good deal pricing bounds are expressed by convex risk measures\non Orlicz hearts. In addition, we obtain its representation with the minimal\npenalty function. Moreover, we give a representation, for two simple cases, of\ngood deal bounds and calculate the optimal strategies when a claim is traded at\nthe upper or lower bounds of its good deal pricing bound.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.4141v2"
    },
    {
        "title": "Feasibility of Portfolio Optimization under Coherent Risk Measures",
        "authors": [
            "Imre Kondor",
            "Istvan Varga-Haszonits"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  It is shown that the axioms for coherent risk measures imply that whenever\nthere is an asset in a portfolio that dominates the others in a given sample\n(which happens with finite probability even for large samples), then this\nportfolio cannot be optimized under any coherent measure on that sample, and\nthe risk measure diverges to minus infinity. This instability was first\ndiscovered on the special example of Expected Shortfall which is used here both\nas an illustration and as a prompt for generalization.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.2283v3"
    },
    {
        "title": "Calibration of transparency risks: a note",
        "authors": [
            "Jirô Akahori",
            "Yuuki Kanishi",
            "Yuichi Morimura"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  The aim of this research is to give a simple framework to evaluate/quantize\nthe \"transparency\" of a firm. We assume that the process of the firm value is\nonly observable once in a while but is strongly correlated with the stock price\nwhich is observable and tradable. This hybrid type structure make the\ntransparency \"observable\". The implication of the present study is that the\ndepth of the shock to the market caused by the precise accounting information\ndoes reflect the degree of transparency. Furthermore, it can be quantized\nresorting to the calibration method.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.1642v2"
    },
    {
        "title": "Minimizing the Probability of Ruin when Consumption is Ratcheted",
        "authors": [
            "Erhan Bayraktar",
            "Virginia R. Young"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  We assume that an agent's rate of consumption is {\\it ratcheted}; that is, it\nforms a non-decreasing process. Given the rate of consumption, we act as\nfinancial advisers and find the optimal investment strategy for the agent who\nwishes to minimize his probability of ruin.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.2358v1"
    },
    {
        "title": "Heterogeneous credit portfolios and the dynamics of the aggregate losses",
        "authors": [
            "Paolo Dai Pra",
            "Marco Tolotti"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  We study the impact of contagion in a network of firms facing credit risk. We\ndescribe an intensity based model where the homogeneity assumption is broken by\nintroducing a random environment that makes it possible to take into account\nthe idiosyncratic characteristics of the firms. We shall see that our model\ngoes behind the identification of groups of firms that can be considered\nbasically exchangeable. Despite this heterogeneity assumption our model has the\nadvantage of being totally tractable. The aim is to quantify the losses that a\nbank may suffer in a large credit portfolio. Relying on a large deviation\nprinciple on the trajectory space of the process, we state a suitable law of\nlarge number and a central limit theorem useful to study large portfolio\nlosses. Simulation results are provided as well as applications to portfolio\nloss distribution analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.3399v1"
    },
    {
        "title": "Ruin models with investment income",
        "authors": [
            "Jostein Paulsen"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  This survey treats the problem of ruin in a risk model when assets earn\ninvestment income. In addition to a general presentation of the problem, topics\ncovered are a presentation of the relevant integro-differential equations,\nexact and numerical solutions, asymptotic results, bounds on the ruin\nprobability and also the possibility of minimizing the ruin probability by\ninvestment and possibly reinsurance control. The main emphasis is on continuous\ntime models, but discrete time models are also covered. A fairly extensive list\nof references is provided, particularly of papers published after 1998. For\nmore references to papers published before that, the reader can consult [47].\n",
        "pdf_link": "http://arxiv.org/pdf/0806.4125v2"
    },
    {
        "title": "Hazard processes and martingale hazard processes",
        "authors": [
            "Delia Coculescu",
            "Ashkan Nikeghbali"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  In this paper, we provide a solution to two problems which have been open in\ndefault time modeling in credit risk. We first show that if $\\tau$ is an\narbitrary random (default) time such that its Az\\'ema's supermartingale\n$Z_t^\\tau=\\P(\\tau>t|\\F_t)$ is continuous, then $\\tau$ avoids stopping times. We\nthen disprove a conjecture about the equality between the hazard process and\nthe martingale hazard process, which first appeared in \\cite{jenbrutk1}, and we\nshow how it should be modified to become a theorem. The pseudo-stopping times,\nintroduced in \\cite{AshkanYor}, appear as the most general class of random\ntimes for which these two processes are equal. We also show that these two\nprocesses always differ when $\\tau$ is an honest time.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.4958v1"
    },
    {
        "title": "Ruin probabilities under general investments and heavy-tailed claims",
        "authors": [
            "Henrik Hult",
            "Filip Lindskog"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  In this paper we study the asymptotic decay of finite time ruin probabilities\nfor an insurance company that faces heavy-tailed claims, uses predictable\ninvestment strategies and makes investments in risky assets whose prices evolve\naccording to quite general semimartingales. We show that the ruin problem\ncorresponds to determining hitting probabilities for the solution to a randomly\nperturbed stochastic integral equation. We derive a large deviation result for\nthe hitting probabilities that holds uniformly over a family of semimartingales\nand show that this result gives the asymptotic decay of finite time ruin\nprobabilities under arbitrary investment strategies, including optimal\ninvestment strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.4372v1"
    },
    {
        "title": "The instability of downside risk measures",
        "authors": [
            "Istvan Varga-Haszonits",
            "Imre Kondor"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  We study the feasibility and noise sensitivity of portfolio optimization\nunder some downside risk measures (Value-at-Risk, Expected Shortfall, and\nsemivariance) when they are estimated by fitting a parametric distribution on a\nfinite sample of asset returns. We find that the existence of the optimum is a\nprobabilistic issue, depending on the particular random sample, in all three\ncases. At a critical combination of the parameters of these problems we find an\nalgorithmic phase transition, separating the phase where the optimization is\nfeasible from the one where it is not. This transition is similar to the one\ndiscovered earlier for Expected Shortfall based on historical time series. We\nemploy the replica method to compute the phase diagram, as well as to obtain\nthe critical exponent of the estimation error that diverges at the critical\npoint. The analytical results are corroborated by Monte Carlo simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0800v2"
    },
    {
        "title": "The Quantification of Operational Risk using Internal Data, Relevant\n  External Data and Expert Opinions",
        "authors": [
            "Dominik D. Lambrigger",
            "Pavel V. Shevchenko",
            "Mario V. Wüthrich"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  To quantify an operational risk capital charge under Basel II, many banks\nadopt a Loss Distribution Approach. Under this approach, quantification of the\nfrequency and severity distributions of operational risk involves the bank's\ninternal data, expert opinions and relevant external data. In this paper we\nsuggest a new approach, based on a Bayesian inference method, that allows for a\ncombination of these three sources of information to estimate the parameters of\nthe risk frequency and severity distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1361v1"
    },
    {
        "title": "Local Risk Decomposition for High-frequency Trading Systems",
        "authors": [
            "M. Bartolozzi",
            "C. Mellen"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  In the present work we address the problem of evaluating the historical\nperformance of a trading strategy or a certain portfolio of assets. Common\nindicators such as the Sharpe ratio and the risk adjusted return have\nsignificant drawbacks. In particular, they are global indices, that is they do\nnot preserve any 'local' information about the performance dynamics either in\ntime or for a particular investment horizon. This information could be\nfundamental for practitioners as the past performance can be affected by the\nnon-stationarity of financial market. In order to highlight this feature, we\nintroduce the 'local risk decomposition' (LRD) formalism, where dynamical\ninformation about a strategy's performance is retained. This framework,\nmotivated by the multi-scaling techniques used in complex system theory, is\nparticularly suitable for high-frequency trading systems and can be applied\ninto problems of strategy optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.4099v2"
    },
    {
        "title": "Estimating discriminatory power and PD curves when the number of\n  defaults is small",
        "authors": [
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  The intention with this paper is to provide all the estimation concepts and\ntechniques that are needed to implement a two-phases approach to the parametric\nestimation of probability of default (PD) curves. In the first phase of this\napproach, a raw PD curve is estimated based on parameters that reflect\ndiscriminatory power. In the second phase of the approach, the raw PD curve is\ncalibrated to fit a target unconditional PD. The concepts and techniques\npresented include a discussion of different definitions of area under the curve\n(AUC) and accuracy ratio (AR), a simulation study on the performance of\nconfidence interval estimators for AUC, a discussion of the one-parametric\napproach to the estimation of PD curves by van der Burgt (2008) and alternative\napproaches, as well as a simulation study on the performance of the presented\nPD curve estimators. The topics are treated in depth in order to provide the\nfull rationale behind them and to produce results that can be implemented\nimmediately.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3928v2"
    },
    {
        "title": "Systemic Risk in a Unifying Framework for Cascading Processes on\n  Networks",
        "authors": [
            "Jan Lorenz",
            "Stefano Battiston",
            "Frank Schweitzer"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  We introduce a general framework for models of cascade and contagion\nprocesses on networks, to identify their commonalities and differences. In\nparticular, models of social and financial cascades, as well as the fiber\nbundle model, the voter model, and models of epidemic spreading are recovered\nas special cases. To unify their description, we define the net fragility of a\nnode, which is the difference between its fragility and the threshold that\ndetermines its failure. Nodes fail if their net fragility grows above zero and\ntheir failure increases the fragility of neighbouring nodes, thus possibly\ntriggering a cascade. In this framework, we identify three classes depending on\nthe way the fragility of a node is increased by the failure of a neighbour. At\nthe microscopic level, we illustrate with specific examples how the failure\nspreading pattern varies with the node triggering the cascade, depending on its\nposition in the network and its degree. At the macroscopic level, systemic risk\nis measured as the final fraction of failed nodes, $X^\\ast$, and for each of\nthe three classes we derive a recursive equation to compute its value. The\nphase diagram of $X^\\ast$ as a function of the initial conditions, thus allows\nfor a prediction of the systemic risk as well as a comparison of the three\ndifferent model classes. We could identify which model class lead to a\nfirst-order phase transition in systemic risk, i.e. situations where small\nchanges in the initial conditions may lead to a global failure. Eventually, we\ngeneralize our framework to encompass stochastic contagion models. This\nindicates the potential for further generalizations.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.5325v2"
    },
    {
        "title": "Risk Concentration and Diversification: Second-Order Properties",
        "authors": [
            "Matthias Degen",
            "Dominik D. Lambrigger",
            "Johan Segers"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  The quantification of diversification benefits due to risk aggregation plays\na prominent role in the (regulatory) capital management of large firms within\nthe financial industry. However, the complexity of today's risk landscape makes\na quantifiable reduction of risk concentration a challenging task. In the\npresent paper we discuss some of the issues that may arise. The theory of\nsecond-order regular variation and second-order subexponentiality provides the\nideal methodological framework to derive second-order approximations for the\nrisk concentration and the diversification benefit.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.2367v2"
    },
    {
        "title": "Inf-convolution of G-expectations",
        "authors": [
            "Xuepeng Bai",
            "Rainer Buckdahn"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  In this paper we will discuss the optimal risk transfer problems when risk\nmeasures are generated by G-expectations, and we present the relationship\nbetween inf-convolution of G-expectations and the inf-convolution of drivers G.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.5398v1"
    },
    {
        "title": "Time consistency and moving horizons for risk measures",
        "authors": [
            "Samuel N. Cohen",
            "Robert J. Elliott"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  We consider portfolio selection when decisions based on a dynamic risk\nmeasure are affected by the use of a moving horizon, and the possible\ninconsistencies that this creates. By giving a formal treatment of time\nconsistency which is independent of Bellman's equations, we show that there is\na new sense in which these decisions can be seen as consistent.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.1396v2"
    },
    {
        "title": "Dual Representation of Quasiconvex Conditional Maps",
        "authors": [
            "Marco Frittelli",
            "Marco Maggis"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We provide a dual representation of quasiconvex maps between two lattices of\nrandom variables in terms of conditional expectations. This generalizes the\ndual representation of quasiconvex real valued functions and the dual\nrepresentation of conditional convex maps.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3644v2"
    },
    {
        "title": "Risk assessment for uncertain cash flows: Model ambiguity, discounting\n  ambiguity, and the role of bubbles",
        "authors": [
            "Beatrice Acciaio",
            "Hans Foellmer",
            "Irina Penner"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We study the risk assessment of uncertain cash flows in terms of dynamic\nconvex risk measures for processes as introduced in Cheridito, Delbaen, and\nKupper (2006). These risk measures take into account not only the amounts but\nalso the timing of a cash flow. We discuss their robust representation in terms\nof suitably penalized probability measures on the optional sigma-field. This\nyields an explicit analysis both of model and discounting ambiguity. We focus\non supermartingale criteria for different notions of time consistency. In\nparticular we show how bubbles may appear in the dynamic penalization, and how\nthey cause a breakdown of asymptotic safety of the risk assessment procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.3627v1"
    },
    {
        "title": "Dynamic risk measures",
        "authors": [
            "Beatrice Acciaio",
            "Irina Penner"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  This paper gives an overview of the theory of dynamic convex risk measures\nfor random variables in discrete time setting. We summarize robust\nrepresentation results of conditional convex risk measures, and we characterize\nvarious time consistency properties of dynamic risk measures in terms of\nacceptance sets, penalty functions, and by supermartingale properties of risk\nprocesses and penalty functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.3794v1"
    },
    {
        "title": "WARNING: Physics Envy May Be Hazardous To Your Wealth!",
        "authors": [
            "Andrew W. Lo",
            "Mark T. Mueller"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  The quantitative aspirations of economists and financial analysts have for\nmany years been based on the belief that it should be possible to build models\nof economic systems - and financial markets in particular - that are as\npredictive as those in physics. While this perspective has led to a number of\nimportant breakthroughs in economics, \"physics envy\" has also created a false\nsense of mathematical precision in some cases. We speculate on the origins of\nphysics envy, and then describe an alternate perspective of economic behavior\nbased on a new taxonomy of uncertainty. We illustrate the relevance of this\ntaxonomy with two concrete examples: the classical harmonic oscillator with\nsome new twists that make physics look more like economics, and a quantitative\nequity market-neutral strategy. We conclude by offering a new interpretation of\ntail events, proposing an \"uncertainty checklist\" with which our taxonomy can\nbe implemented, and considering the role that quants played in the current\nfinancial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.2688v3"
    },
    {
        "title": "Precautionary Measures for Credit Risk Management in Jump Models",
        "authors": [
            "Masahiko Egami",
            "Kazutoshi Yamazaki"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  Sustaining efficiency and stability by properly controlling the equity to\nasset ratio is one of the most important and difficult challenges in bank\nmanagement. Due to unexpected and abrupt decline of asset values, a bank must\nclosely monitor its net worth as well as market conditions, and one of its\nimportant concerns is when to raise more capital so as not to violate capital\nadequacy requirements. In this paper, we model the tradeoff between avoiding\ncosts of delay and premature capital raising, and solve the corresponding\noptimal stopping problem. In order to model defaults in a bank's loan/credit\nbusiness portfolios, we represent its net worth by Levy processes, and solve\nexplicitly for the double exponential jump diffusion process and for a general\nspectrally negative Levy process.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.0595v3"
    },
    {
        "title": "Simple Fuzzy Score for Russian Public Companies Risk of Default",
        "authors": [
            "Sergey Ivliev"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  The model is aimed to discriminate the 'good' and the 'bad' companies in\nRussian corporate sector based on their financial statements data based on\nRussian Accounting Standards. The data sample consists of 126 Russian public\ncompanies- issuers of Ruble bonds which represent about 36% of total number of\ncorporate bonds issuers. 25 companies have defaulted on their debt in 2008-2009\nwhich represent around 30% of default cases. No SPV companies were included in\nthe sample. The model shows in-sample Gini AR about 73% and gives a reasonable\nand simple rule of mapping to external ratings. The model can be used to\ncalculate implied credit rating for Russian companies which many of them don't\nhave.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.0685v3"
    },
    {
        "title": "Hedging Errors Induced by Discrete Trading Under an Adaptive Trading\n  Strategy",
        "authors": [
            "Mats Brodén",
            "Magnus Wiktorsson"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  Discrete time hedging in a complete diffusion market is considered. The hedge\nportfolio is rebalanced when the absolute difference between delta of the hedge\nportfolio and the derivative contract reaches a threshold level. The rate of\nconvergence of the expected squared hedging error as the threshold level\napproaches zero is analyzed. The results hinge to a great extent on a theorem\nstating that the difference between the hedge ratios normalized by the\nthreshold level tends to a triangular distribution as the threshold level tends\nto zero.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4526v1"
    },
    {
        "title": "Risk measuring under model uncertainty",
        "authors": [
            "Jocelyne Bion-Nadal",
            "Magali Kervarec"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  The framework of this paper is that of risk measuring under uncertainty,\nwhich is when no reference probability measure is given. To every regular\nconvex risk measure on ${\\cal C}_b(\\Omega)$, we associate a unique equivalence\nclass of probability measures on Borel sets, characterizing the riskless non\npositive elements of ${\\cal C}_b(\\Omega)$. We prove that the convex risk\nmeasure has a dual representation with a countable set of probability measures\nabsolutely continuous with respect to a certain probability measure in this\nclass.\n  To get these results we study the topological properties of the dual of the\nBanach space $L^1(c)$ associated to a capacity $c$.\n  As application we obtain that every $G$-expectation $\\E$ has a representation\nwith a countable set of probability measures absolutely continuous with respect\nto a probability measure $P$ such that $P(|f|)=0$ iff $\\E(|f|)=0$. We also\napply our results to the case of uncertain volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.5524v2"
    },
    {
        "title": "Recovery Rates in investment-grade pools of credit assets: A large\n  deviations analysis",
        "authors": [
            "Konstantinos Spiliopoulos",
            "Richard B. Sowers"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We consider the effect of recovery rates on a pool of credit assets. We allow\nthe recovery rate to depend on the defaults in a general way. Using the theory\nof large deviations, we study the structure of losses in a pool consisting of a\ncontinuum of types. We derive the corresponding rate function and show that it\nhas a natural interpretation as the favored way to rearrange recoveries and\nlosses among the different types. Numerical examples are also provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.2711v2"
    },
    {
        "title": "What risk measures are time consistent for all filtrations?",
        "authors": [
            "Samuel N. Cohen"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We study coherent risk measures which are time-consistent for multiple\nfiltrations. We show that a coherent risk measure is time-consistent for every\nfiltration if and only if it is one of four main types. Furthermore, if the\nrisk measure is strictly monotone it is linear, and if the reference\nprobability space is not atomic then it is either linear or an essential\nsupremum.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.0610v1"
    },
    {
        "title": "About the Justification of Experience Rating: Bonus Malus System and a\n  new Poisson Mixture Model",
        "authors": [
            "Magda Schiegl"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  The claim experience of the past is a very important information to calculate\nthe fair price of an insurance contract. In a lot of European countries for\ninstance the prices for motor car insurance depend on the number of claims the\ndriver has reported to the insurance company during the last years. Classically\nthese prices are calculated on the basis of a mixed Poisson model with a gamma\nmixing distribution. The mixing distribution models the car drivers' qualities\nacross the insured portfolio. This is just one example for experience rating.\nIn the classical context the price is equal to the expectation of the Bayesian\nposterior distribution.\n  In some lines of business (especially third party liability and lines with\nexposure to extreme weather events) we that the real world data cannot be\ndescribed well enough by the classical Poisson - gamma model. Therefore we\ninvestigate the influence of the mixing distribution on the posterior\ndistribution conditional on the experienced number of claims. This enables the\napplication of other - more risk adequate premium principles than the\nexpectation principle. We introduce the inverse - gamma distribution as a new\nmixing distribution to model claim numbers and compare it to the classical\ngamma distribution. In both cases a closed analytic representation of the mixed\ndistribution can be found: In the classic case the well known negative binomial\ndistribution, in our new one a representation using the Bessel functions.\nAdditionally we present numerical results about the tail behaviour of the mixed\nPoisson - inverse - gamma distribution. Finally we introduce the concept of\nresolution. It enables us to decide if the classification of risk groups via\nthe number of experienced claims is a risk adequate procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.4142v1"
    },
    {
        "title": "Dynamic Coherent Acceptability Indices and their Applications to Finance",
        "authors": [
            "Tomasz R. Bielecki",
            "Igor Cialenco",
            "Zhao Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  In this paper we present a theoretical framework for studying coherent\nacceptability indices in a dynamic setup. We study dynamic coherent\nacceptability indices and dynamic coherent risk measures, and we establish a\nduality between them. We derive a representation theorem for dynamic coherent\nrisk measures in terms of so called dynamically consistent sequence of sets of\nprobability measures. Based on these results, we give a specific construction\nof dynamic coherent acceptability indices. We also provide examples of dynamic\ncoherent acceptability indices, both abstract and also some that generalize\nselected classical financial measures of portfolio performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4339v2"
    },
    {
        "title": "Ordering of multivariate probability distributions with respect to\n  extreme portfolio losses",
        "authors": [
            "Georg Mainik",
            "Ludger Rüschendorf"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  A new notion of stochastic ordering is introduced to compare multivariate\nstochastic risk models with respect to extreme portfolio losses. In the\nframework of multivariate regular variation comparison criteria are derived in\nterms of ordering conditions on the spectral measures, which allows for\nanalytical or numerical verification in practical applications. Additional\ncomparison criteria in terms of further stochastic orderings are derived. The\napplication examples include worst case and best case scenarios, elliptically\ncontoured distributions, and multivariate regularly varying models with Gumbel,\nArchimedean, and Galambos copulas.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.5171v1"
    },
    {
        "title": "Bankruptcy risk model and empirical tests",
        "authors": [
            "Boris Podobnik",
            "Davor Horvatic",
            "Alexander M. Petersen",
            "Branko Urošević",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We analyze the size dependence and temporal stability of firm bankruptcy risk\nin the US economy by applying Zipf scaling techniques. We focus on a single\nrisk factor-the debt-to-asset ratio R-in order to study the stability of the\nZipf distribution of R over time. We find that the Zipf exponent increases\nduring market crashes, implying that firms go bankrupt with larger values of R.\nBased on the Zipf analysis, we employ Bayes's theorem and relate the\nconditional probability that a bankrupt firm has a ratio R with the conditional\nprobability of bankruptcy for a firm with a given R value. For 2,737 bankrupt\nfirms, we demonstrate size dependence in assets change during the bankruptcy\nproceedings. Prepetition firm assets and petition firm assets follow Zipf\ndistributions but with different exponents, meaning that firms with smaller\nassets adjust their assets more than firms with larger assets during the\nbankruptcy process. We compare bankrupt firms with nonbankrupt firms by\nanalyzing the assets and liabilities of two large subsets of the US economy:\n2,545 Nasdaq members and 1,680 New York Stock Exchange (NYSE) members. We find\nthat both assets and liabilities follow a Pareto distribution. The finding is\nnot a trivial consequence of the Zipf scaling relationship of firm size\nquantified by employees-although the market capitalization of Nasdaq stocks\nfollows a Pareto distribution, the same distribution does not describe NYSE\nstocks. We propose a coupled Simon model that simultaneously evolves both\nassets and debt with the possibility of bankruptcy, and we also consider the\npossibility of firm mergers.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.2670v1"
    },
    {
        "title": "Robust Estimation of Operational Risk",
        "authors": [
            "Nataliya Horbenko",
            "Peter Ruckdeschel",
            "Taehan Bae"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  According to the Loss Distribution Approach, the operational risk of a bank\nis determined as 99.9% quantile of the respective loss distribution, covering\nunexpected severe events. The 99.9% quantile can be considered a tail event. As\nsupported by the Pickands-Balkema-de Haan Theorem, tail events exceeding some\nhigh threshold are usually modeled by a Generalized Pareto Distribution (GPD).\nEstimation of GPD tail quantiles is not a trivial task, in particular if one\ntakes into account the heavy tails of this distribution, the possibility of\nsingular outliers, and, moreover, the fact that data is usually pooled among\nseveral sources. Moreover, if, as is frequently the case, operational losses\nare pooled anonymously, relevance of the fitting data for the respective bank\nis not self-evident. In such situations, robust methods may provide stable\nestimates when classical methods already fail. In this paper, optimally-robust\nprocedures MBRE, OMSE, RMXE are introduced to the application domain of\noperational risk. We apply these procedures to parameter estimation of a GPD at\ndata from Algorithmics Inc. To better understand these results, we provide\nsupportive diagnostic plots adjusted for this context: influence plots,\noutlyingness plots, and QQ plots with robust confidence bands.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.0249v3"
    },
    {
        "title": "The economic default time and the Arcsine law",
        "authors": [
            "Xin Guo",
            "Robert A Jarrow",
            "Adrien de Larrard"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  This paper develops a structural credit risk model to characterize the\ndifference between the economic and recorded default times for a firm. Recorded\ndefault occurs when default is recorded in the legal system. The economic\ndefault time is the last time when the firm is able to pay off its debt prior\nto the legal default time. It has been empirically documented that these two\ntimes are distinct (see Guo, Jarrow, and Lin (2008)). In our model, the\nprobability distribution for the time span between economic and recorded\ndefaults follows a mixture of Arcsine Laws, which is consistent with the\nresults contained in Guo, Jarrow, and Lin. In addition, we show that the\nclassical structural model is a limiting case of our model as the time period\nbetween debt repayment dates goes to zero. As a corollary, we show how the firm\nvalue process's parameters can be estimated using the tail index and\ncorrelation structure of the firm's return.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.0843v2"
    },
    {
        "title": "Spectral Risk Measures with an Application to Futures Clearinghouse\n  Variation Margin Requirements",
        "authors": [
            "John Cotter",
            "Kevin Dowd"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This paper applies an AR(1)-GARCH (1, 1) process to detail the conditional\ndistributions of the return distributions for the S&P500, FT100, DAX, Hang\nSeng, and Nikkei225 futures contracts. It then uses the conditional\ndistribution for these contracts to estimate spectral risk measures, which are\ncoherent risk measures that reflect a user's risk-aversion function. It\ncompares these to more familiar VaR and Expected Shortfall (ES) measures of\nrisk, and also compares the precision and discusses the relative usefulness of\neach of these risk measures in setting variation margins that incorporate\ntime-varying market conditions. The goodness of fit of the model is confirmed\nby a variety of backtests.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5408v1"
    },
    {
        "title": "Exponential Spectral Risk Measures",
        "authors": [
            "Kevin Dowd",
            "John Cotter"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Spectral risk measures are attractive risk measures as they allow the user to\nobtain risk measures that reflect their subjective risk-aversion. This paper\nexamines spectral risk measures based on an exponential utility function, and\nfinds that these risk measures have nice intuitive properties. It also\ndiscusses how they can be estimated using numerical quadrature methods, and how\nconfidence intervals for them can be estimated using a parametric bootstrap.\nIllustrative results suggest that estimated exponential spectral risk measures\nobtained using such methods are quite precise in the presence of normally\ndistributed losses.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5409v1"
    },
    {
        "title": "Margin setting with high-frequency data1",
        "authors": [
            "John Cotter",
            "François Longin"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Both in practice and in the academic literature, models for setting margin\nrequirements in futures markets classically use daily closing price changes.\nHowever, as well documented by research on high-frequency data, financial\nmarkets have recently shown high intraday volatility, which could bring more\nrisk than expected. This paper tries to answer two questions relevant for\nmargin committees in practice: is it right to compute margin levels based on\nclosing prices and ignoring intraday dynamics? Is it justified to implement\nintraday margin calls? The paper focuses on the impact of intraday dynamics of\nmarket prices on daily margin levels. Daily margin levels are obtained in two\nways: first, by using daily price changes defined with different time-intervals\n(say from 3 pm to 3 pm on the following trading day instead of traditional\nclosing times); second, by using 5-minute and 1-hour price changes and scaling\nthe results to one day. Our empirical analysis uses the FTSE 100 futures\ncontract traded on LIFFE.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5412v1"
    },
    {
        "title": "Minimum Capital Requirement Calculations for UK Futures",
        "authors": [
            "John Cotter"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Key to the imposition of appropriate minimum capital requirements on a daily\nbasis requires accurate volatility estimation. Here, measures are presented\nbased on discrete estimation of aggregated high frequency UK futures\nrealisations underpinned by a continuous time framework. Squared and absolute\nreturns are incorporated into the measurement process so as to rely on the\nquadratic variation of a diffusion process and be robust in the presence of fat\ntails. The realized volatility estimates incorporate the long memory property.\nThe dynamics of the volatility variable are adequately captured. Resulting\nrescaled returns are applied to minimum capital requirement calculations.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5416v1"
    },
    {
        "title": "Tail Behaviour of the Euro",
        "authors": [
            "John Cotter"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This paper empirically analyses risk in the Euro relative to other\ncurrencies. Comparisons are made between a sub period encompassing the final\ntransitional stage to full monetary union with a sub period prior to this.\nStability in the face of speculative attack is examined using Extreme Value\nTheory to obtain estimates of tail exchange rate changes. The findings are\nencouraging. The Euro's common risk measures do not deviate substantially from\nother currencies. Also, the Euro is stable in the face of speculative pressure.\nFor example, the findings consistently show the Euro being less risky than the\nYen, and having similar inherent risk to the Deutsche Mark, the currency that\nit is essentially replacing.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5418v1"
    },
    {
        "title": "Varying the VaR for Unconditional and Conditional Environments",
        "authors": [
            "John Cotter"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Accurate forecasting of risk is the key to successful risk management\ntechniques. Using the largest stock index futures from twelve European bourses,\nthis paper presents VaR measures based on their unconditional and conditional\ndistributions for single and multi-period settings. These measures underpinned\nby extreme value theory are statistically robust explicitly allowing for\nfat-tailed densities. Conditional tail estimates are obtained by adjusting the\nunconditional extreme value procedure with GARCH filtered returns. The\nconditional modelling results in iid returns allowing for the use of a simple\nand efficient multi-period extreme value scaling law. The paper examines the\nproperties of these distinct conditional and unconditional trading models. The\npaper finds that the biases inherent in unconditional single and multi-period\nestimates assuming normality extend to the conditional setting.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5649v1"
    },
    {
        "title": "Extreme Spectral Risk Measures: An Application to Futures Clearinghouse\n  Margin Requirements",
        "authors": [
            "John Cotter",
            "Kevin Dowd"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This paper applies the Extreme-Value (EV) Generalised Pareto distribution to\nthe extreme tails of the return distributions for the S&P500, FT100, DAX, Hang\nSeng, and Nikkei225 futures contracts. It then uses tail estimators from these\ncontracts to estimate spectral risk measures, which are coherent risk measures\nthat reflect a user's risk-aversion function. It compares these to VaR and\nExpected Shortfall (ES) risk measures, and compares the precision of their\nestimators. It also discusses the usefulness of these risk measures in the\ncontext of clearinghouses setting initial margin requirements, and compares\nthese to the SPAN measures typically used. Keywords: Spectral risk measures,\nExpected Shortfall, Value at Risk, Extreme Value\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5653v1"
    },
    {
        "title": "Implied correlation from VaR",
        "authors": [
            "John Cotter",
            "François Longin"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Value at risk (VaR) is a risk measure that has been widely implemented by\nfinancial institutions. This paper measures the correlation among asset price\nchanges implied from VaR calculation. Empirical results using US and UK equity\nindexes show that implied correlation is not constant but tends to be higher\nfor events in the left tails (crashes) than in the right tails (booms).\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5655v1"
    },
    {
        "title": "Modelling catastrophic risk in international equity markets: An extreme\n  value approach",
        "authors": [
            "john cotter"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This letter uses the Block Maxima Extreme Value approach to quantify\ncatastrophic risk in international equity markets. Risk measures are generated\nfrom a set threshold of the distribution of returns that avoids the pitfall of\nusing absolute returns for markets exhibiting diverging levels of risk. From an\napplication to leading markets, the letter finds that the Nikkei is more prone\nto catastrophic risk than the FTSE and Dow Jones Indexes.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5656v1"
    },
    {
        "title": "Evaluating the Precision of Estimators of Quantile-Based Risk Measures",
        "authors": [
            "Kevin Dowd",
            "John Cotter"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This paper examines the precision of estimators of Quantile-Based Risk\nMeasures (Value at Risk, Expected Shortfall, Spectral Risk Measures). It first\naddresses the question of how to estimate the precision of these estimators,\nand proposes a Monte Carlo method that is free of some of the limitations of\nexisting approaches. It then investigates the distribution of risk estimators,\nand presents simulation results suggesting that the common practice of relying\non asymptotic normality results might be unreliable with the sample sizes\ncommonly available to them. Finally, it investigates the relationship between\nthe precision of different risk estimators and the distribution of underlying\nlosses (or returns), and yields a number of useful conclusions.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5665v1"
    },
    {
        "title": "Estimating financial risk measures for futures positions: a\n  non-parametric approach",
        "authors": [
            "john cotter",
            "kevin dowd"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This paper presents non-parametric estimates of spectral risk measures\napplied to long and short positions in 5 prominent equity futures contracts. It\nalso compares these to estimates of two popular alternative measures, the\nValue-at-Risk (VaR) and Expected Shortfall (ES). The spectral risk measures are\nconditioned on the coefficient of absolute risk aversion, and the latter two\nare conditioned on the confidence level. Our findings indicate that all risk\nmeasures increase dramatically and their estimators deteriorate in precision\nwhen their respective conditioning parameter increases. Results also suggest\nthat estimates of spectral risk measures and their precision levels are of\ncomparable orders of magnitude as those of more conventional risk measures.\nRunning head: financial risk measures for futures positions\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5666v1"
    },
    {
        "title": "Spectral Risk Measures and the Choice of Risk Aversion Function",
        "authors": [
            "kevin dowd",
            "john cotter"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Spectral risk measures are attractive risk measures as they allow the user to\nobtain risk measures that reflect their risk-aversion functions. To date there\nhas been very little guidance on the choice of risk-aversion functions\nunderlying spectral risk measures. This paper addresses this issue by examining\ntwo popular risk aversion functions, based on exponential and power utility\nfunctions respectively. We find that the former yields spectral risk measures\nwith nice intuitive properties, but the latter yields spectral risk measures\nthat can have perverse properties. More work therefore needs to be done before\nwe can be sure that arbitrary but respectable utility functions will always\nyield 'well-behaved' spectral risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5668v1"
    },
    {
        "title": "Scaling conditional tail probability and quantile estimators",
        "authors": [
            "John Cotter"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  We present a novel procedure for scaling relatively high frequency tail\nprobability and quantile estimates for the conditional distribution of returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5965v1"
    },
    {
        "title": "Hedging: Scaling and the Investor Horizon",
        "authors": [
            "John Cotter",
            "Jim Hanly"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This paper examines the volatility and covariance dynamics of cash and\nfutures contracts that underlie the Optimal Hedge Ratio (OHR) across different\nhedging time horizons. We examine whether hedge ratios calculated over a short\nterm hedging horizon can be scaled and successfully applied to longer term\nhorizons. We also test the equivalence of scaled hedge ratios with those\ncalculated directly from lower frequency data and compare them in terms of\nhedging effectiveness. Our findings show that the volatility and covariance\ndynamics may differ considerably depending on the hedging horizon and this\ngives rise to significant differences between short term and longer term\nhedges. Despite this, scaling provides good hedging outcomes in terms of risk\nreduction which are comparable to those based on direct estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5966v1"
    },
    {
        "title": "Time Varying Risk Aversion: An Application to Energy Hedging",
        "authors": [
            "John Cotter",
            "Jim Hanly"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Risk aversion is a key element of utility maximizing hedge strategies;\nhowever, it has typically been assigned an arbitrary value in the literature.\nThis paper instead applies a GARCH-in-Mean (GARCH-M) model to estimate a\ntime-varying measure of risk aversion that is based on the observed risk\npreferences of energy hedging market participants. The resulting estimates are\napplied to derive explicit risk aversion based optimal hedge strategies for\nboth short and long hedgers. Out-of-sample results are also presented based on\na unique approach that allows us to forecast risk aversion, thereby estimating\nhedge strategies that address the potential future needs of energy hedgers. We\nfind that the risk aversion based hedges differ significantly from simpler OLS\nhedges. When implemented in-sample, risk aversion hedges for short hedgers\noutperform the OLS hedge ratio in a utility based comparison.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5968v1"
    },
    {
        "title": "A Utility Based Approach to Energy Hedging",
        "authors": [
            "John Cotter",
            "Jim Hanly"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  A key issue in the estimation of energy hedges is the hedgers' attitude\ntowards risk which is encapsulated in the form of the hedgers' utility\nfunction. However, the literature typically uses only one form of utility\nfunction such as the quadratic when estimating hedges. This paper addresses\nthis issue by estimating and applying energy market based risk aversion to\ncommonly applied utility functions including log, exponential and quadratic,\nand we incorporate these in our hedging frameworks. We find significant\ndifferences in the optimal hedge strategies based on the utility function\nchosen.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5973v1"
    },
    {
        "title": "Theoretical Sensitivity Analysis for Quantitative Operational Risk\n  Management",
        "authors": [
            "Takashi Kato"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  We study the asymptotic behavior of the difference between the values at risk\nVaR(L) and VaR(L+S) for heavy tailed random variables L and S for application\nin sensitivity analysis of quantitative operational risk management within the\nframework of the advanced measurement approach of Basel II (and III). Here L\ndescribes the loss amount of the present risk profile and S describes the loss\namount caused by an additional loss factor. We obtain different types of\nresults according to the relative magnitudes of the thicknesses of the tails of\nL and S. In particular, if the tail of S is sufficiently thinner than the tail\nof L, then the difference between prior and posterior risk amounts VaR(L+S) -\nVaR(L) is asymptotically equivalent to the expectation (expected loss) of S.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.0359v5"
    },
    {
        "title": "Counterparty Risk and the Impact of Collateralization in CDS Contracts",
        "authors": [
            "Tomasz R. Bielecki",
            "Igor Cialenco",
            "Ismail Iyigunler"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  We analyze the counterparty risk embedded in CDS contracts, in presence of a\nbilateral margin agreement. First, we investigate the pricing of collateralized\ncounterparty risk and we derive the bilateral Credit Valuation Adjustment\n(CVA), unilateral Credit Valuation Adjustment (UCVA) and Debt Valuation\nAdjustment (DVA). We propose a model for the collateral by incorporating all\nrelated factors such as the thresholds, haircuts and margin period of risk. We\nderive the dynamics of the bilateral CVA in a general form with related jump\nmartingales. We also introduce the Spread Value Adjustment (SVA) indicating the\ncounterparty risk adjusted spread. Counterparty risky and the counterparty\nrisk-free spread dynamics are derived and the dynamics of the SVA is found as a\nconsequence. We finally employ a Markovian copula model for default intensities\nand illustrate our findings with numerical results.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2625v3"
    },
    {
        "title": "Is there a bubble in LinkedIn's stock price?",
        "authors": [
            "Robert Jarrow",
            "Younes Kchia",
            "Philip Protter"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Recent academic work has developed a method to determine, in real time, if a\ngiven stock is exhibiting a price bubble. Currently there is speculation in the\nfinancial press concerning the existence of a price bubble in the aftermath of\nthe recent IPO of LinkedIn. We analyze stock price tick data from the short\nlifetime of this stock through May 24, 2011, and we find that LinkedIn has a\nprice bubble.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.5717v1"
    },
    {
        "title": "Tight Approximations of Dynamic Risk Measures",
        "authors": [
            "Dan A. Iancu",
            "Marek Petrik",
            "Dharmashankar Subramanian"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  This paper compares two different frameworks recently introduced in the\nliterature for measuring risk in a multi-period setting. The first corresponds\nto applying a single coherent risk measure to the cumulative future costs,\nwhile the second involves applying a composition of one-step coherent risk\nmappings. We summarize the relative strengths of the two methods, characterize\nseveral necessary and sufficient conditions under which one of the measurements\nalways dominates the other, and introduce a metric to quantify how close the\ntwo risk measures are.\n  Using this notion, we address the question of how tightly a given coherent\nmeasure can be approximated by lower or upper-bounding compositional measures.\nWe exhibit an interesting asymmetry between the two cases: the tightest\npossible upper-bound can be exactly characterized, and corresponds to a popular\nconstruction in the literature, while the tightest-possible lower bound is not\nreadily available. We show that testing domination and computing the\napproximation factors is generally NP-hard, even when the risk measures in\nquestion are comonotonic and law-invariant. However, we characterize conditions\nand discuss several examples where polynomial-time algorithms are possible. One\nsuch case is the well-known Conditional Value-at-Risk measure, which is further\nexplored in our companion paper [Huang, Iancu, Petrik and Subramanian, \"Static\nand Dynamic Conditional Value at Risk\" (2012)]. Our theoretical and algorithmic\nconstructions exploit interesting connections between the study of risk\nmeasures and the theory of submodularity and combinatorial optimization, which\nmay be of independent interest.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.6102v2"
    },
    {
        "title": "Asymptotically optimal discretization of hedging strategies with jumps",
        "authors": [
            "Mathieu Rosenbaum",
            "Peter Tankov"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  In this work, we consider the hedging error due to discrete trading in models\nwith jumps. Extending an approach developed by Fukasawa [In Stochastic Analysis\nwith Financial Applications (2011) 331-346 Birkh\\\"{a}user/Springer Basel AG]\nfor continuous processes, we propose a framework enabling us to\n(asymptotically) optimize the discretization times. More precisely, a\ndiscretization rule is said to be optimal if for a given cost function, no\nstrategy has (asymptotically, for large cost) a lower mean square\ndiscretization error for a smaller cost. We focus on discretization rules based\non hitting times and give explicit expressions for the optimal rules within\nthis class.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5940v3"
    },
    {
        "title": "On Multivariate Extensions of Value-at-Risk",
        "authors": [
            "Areski Cousin",
            "Elena Di Bernadino"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  In this paper, we introduce two alternative extensions of the classical\nunivariate Value-at-Risk (VaR) in a multivariate setting. The two proposed\nmultivariate VaR are vector-valued measures with the same dimension as the\nunderlying risk portfolio. The lower-orthant VaR is constructed from level sets\nof multivariate distribution functions whereas the upper-orthant VaR is\nconstructed from level sets of multivariate survival functions. Several\nproperties have been derived. In particular, we show that these risk measures\nboth satisfy the positive homogeneity and the translation invariance property.\nComparison between univariate risk measures and components of multivariate VaR\nare provided. We also analyze how these measures are impacted by a change in\nmarginal distributions, by a change in dependence structure and by a change in\nrisk level. Illustrations are given in the class of Archimedean copulas.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.1349v3"
    },
    {
        "title": "Killed Brownian motion with a prescribed lifetime distribution and\n  models of default",
        "authors": [
            "Boris Ettinger",
            "Steven N. Evans",
            "Alexandru Hening"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  The inverse first passage time problem asks whether, for a Brownian motion\n$B$ and a nonnegative random variable $\\zeta$, there exists a time-varying\nbarrier $b$ such that $\\mathbb{P}\\{B_s>b(s),0\\leq s\\leq\nt\\}=\\mathbb{P}\\{\\zeta>t\\}$. We study a \"smoothed\" version of this problem and\nask whether there is a \"barrier\" $b$ such that $\n\\mathbb{E}[\\exp(-\\lambda\\int_0^t\\psi(B_s-b(s))\\,ds)]=\\mathbb{P}\\{\\zeta >t\\}$,\nwhere $\\lambda$ is a killing rate parameter, and $\\psi:\\mathbb{R}\\to[0,1]$ is a\nnonincreasing function. We prove that if $\\psi$ is suitably smooth, the\nfunction $t\\mapsto \\mathbb{P}\\{\\zeta>t\\}$ is twice continuously differentiable,\nand the condition $0<-\\frac{d\\log\\mathbb{P}\\{\\zeta>t\\}}{dt}<\\lambda$ holds for\nthe hazard rate of $\\zeta$, then there exists a unique continuously\ndifferentiable function $b$ solving the smoothed problem. We show how this\nresult leads to flexible models of default for which it is possible to compute\nexpected values of contingent claims.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.2976v3"
    },
    {
        "title": "On the Necessity of Five Risk Measures",
        "authors": [
            "Dominique Guégan",
            "Wayne Tarrant"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  The banking systems that deal with risk management depend on underlying risk\nmeasures. Following the Basel II accord, there are two separate methods by\nwhich banks may determine their capital requirement. The Value at Risk measure\nplays an important role in computing the capital for both approaches. In this\npaper we analyze the errors produced by using this measure. We discuss other\nmeasures, demonstrating their strengths and shortcomings. We give examples,\nshowing the need for the information from multiple risk measures in order to\ndetermine a bank's loss distribution. We conclude by suggesting a regulatory\nrequirement of multiple risk measures being reported by banks, giving specific\nrecommendations.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.4414v1"
    },
    {
        "title": "Viewing Risk Measures as Information",
        "authors": [
            "Dominique Gu/'egan",
            "Wayne Tarrant"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Regulation and risk management in banks depend on underlying risk measures.\nIn general this is the only purpose that is seen for risk measures. In this\npaper we suggest that the reporting of risk measures can be used to determine\nthe loss distribution function for a financial entity. We demonstrate that a\nlack of sufficient information can lead to ambiguous risk situations. We give\nexamples, showing the need for the reporting of multiple risk measures in order\nto determine a bank's loss distribution. We conclude by suggesting a regulatory\nrequirement of multiple risk measures being reported by banks, giving specific\nrecommendations.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.4417v1"
    },
    {
        "title": "Resilience to Contagion in Financial Networks",
        "authors": [
            "Hamed Amini",
            "Rama Cont",
            "Andreea Minca"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Propagation of balance-sheet or cash-flow insolvency across financial\ninstitutions may be modeled as a cascade process on a network representing\ntheir mutual exposures. We derive rigorous asymptotic results for the magnitude\nof contagion in a large financial network and give an analytical expression for\nthe asymptotic fraction of defaults, in terms of network characteristics. Our\nresults extend previous studies on contagion in random graphs to inhomogeneous\ndirected graphs with a given degree sequence and arbitrary distribution of\nweights. We introduce a criterion for the resilience of a large financial\nnetwork to the insolvency of a small group of financial institutions and\nquantify how contagion amplifies small shocks to the network. Our results\nemphasize the role played by \"contagious links\" and show that institutions\nwhich contribute most to network instability in case of default have both large\nconnectivity and a large fraction of contagious links. The asymptotic results\nshow good agreement with simulations for networks with realistic sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.5687v1"
    },
    {
        "title": "Bounds for rating override rates",
        "authors": [
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  Overrides of credit ratings are important correctives of ratings that are\ndetermined by statistical rating models. Financial institutions and banking\nregulators agree on this because on the one hand errors with ratings of\ncorporates or banks can have fatal consequences for the lending institutions\nand on the other hand errors by statistical methods can be minimised but not\ncompletely avoided. Nonetheless, rating overrides can be misused in order to\nconceal the real riskiness of borrowers or even entire portfolios. That is why\nrating overrides usually are strictly governed and carefully recorded. It is\nnot clear, however, which frequency of overrides is appropriate for a given\nrating model within a predefined time period. This paper argues that there is a\nnatural error rate associated with a statistical rating model that may be used\nto inform assessment of whether or not an observed override rate is adequate.\nThe natural error rate is closely related to the rating model's discriminatory\npower and can readily be calculated.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.2287v4"
    },
    {
        "title": "Capital requirements with defaultable securities",
        "authors": [
            "Walter Farkas",
            "Pablo Koch-Medina",
            "Cosimo Munari"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We study capital requirements for bounded financial positions defined as the\nminimum amount of capital to invest in a chosen eligible asset targeting a\npre-specified acceptability test. We allow for general acceptance sets and\ngeneral eligible assets, including defaultable bonds. Since the payoff of these\nassets is not necessarily bounded away from zero the resulting risk measures\ncannot be transformed into cash-additive risk measures by a change of\nnumeraire. However, extending the range of eligible assets is important\nbecause, as exemplified by the recent financial crisis, assuming the existence\nof default-free bonds may be unrealistic. We focus on finiteness and continuity\nproperties of these general risk measures. As an application, we discuss\ncapital requirements based on Value-at-Risk and Tail-Value-at-Risk\nacceptability, the two most important acceptability criteria in practice.\nFinally, we prove that there is no optimal choice of the eligible asset. Our\nresults and our examples show that a theory of capital requirements allowing\nfor general eligible assets is richer than the standard theory of cash-additive\nrisk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.4610v6"
    },
    {
        "title": "Large deviations for a mean field model of systemic risk",
        "authors": [
            "Josselin Garnier",
            "George Papanicolaou",
            "Tzu-Wei Yang"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We consider a system of diffusion processes that interact through their\nempirical mean and have a stabilizing force acting on each of them,\ncorresponding to a bistable potential. There are three parameters that\ncharacterize the system: the strength of the intrinsic stabilization, the\nstrength of the external random perturbations, and the degree of cooperation or\ninteraction between them. The latter is the rate of mean reversion of each\ncomponent to the empirical mean of the system. We interpret this model in the\ncontext of systemic risk and analyze in detail the effect of cooperation\nbetween the components, that is, the rate of mean reversion. We show that in a\ncertain regime of parameters increasing cooperation tends to increase the\nstability of the individual agents but it also increases the overall or\nsystemic risk. We use the theory of large deviations of diffusions interacting\nthrough their mean field.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.3536v2"
    },
    {
        "title": "Transmission of distress in a bank credit network",
        "authors": [
            "Yoshiharu Maeno",
            "Satoshi Morinaga",
            "Hirokazu Matsushima",
            "Kenichi Amagai"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  The European sovereign debt crisis has impaired many European banks. The\ndistress on the European banks may transmit worldwide, and result in a\nlarge-scale knock-on default of financial institutions. This study presents a\ncomputer simulation model to analyze the risk of insolvency of banks and\ndefaults in a bank credit network. Simulation experiments reproduce the\nknock-on default, and quantify the impact which is imposed on the number of\nbank defaults by heterogeneity of the bank credit network, the equity capital\nratio of banks, and the capital surcharge on big banks.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.5661v2"
    },
    {
        "title": "On dependence consistency of CoVaR and some other systemic risk measures",
        "authors": [
            "Georg Mainik",
            "Eric Schaanning"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This paper is dedicated to the consistency of systemic risk measures with\nrespect to stochastic dependence. It compares two alternative notions of\nConditional Value-at-Risk (CoVaR) available in the current literature. These\nnotions are both based on the conditional distribution of a random variable Y\ngiven a stress event for a random variable X, but they use different types of\nstress events. We derive representations of these alternative CoVaR notions in\nterms of copulas, study their general dependence consistency and compare their\nperformance in several stochastic models. Our central finding is that\nconditioning on X>=VaR_\\alpha(X) gives a much better response to dependence\nbetween X and Y than conditioning on X=VaR_\\alpha(X). We prove general results\nthat relate the dependence consistency of CoVaR using conditioning on\nX>=VaR_\\alpha(X) to well established results on concordance ordering of\nmultivariate distributions or their copulas. These results also apply to some\nother systemic risk measures, such as the Marginal Expected Shortfall (MES) and\nthe Systemic Impact Index (SII). We provide counterexamples showing that CoVaR\nbased on the stress event X=VaR_\\alpha(X) is not dependence consistent. In\nparticular, if (X,Y) is bivariate normal, then CoVaR based on X=VaR_\\alpha(X)\nis not an increasing function of the correlation parameter. Similar issues\narise in the bivariate t model and in the model with t margins and a Gumbel\ncopula. In all these cases, CoVaR based on X>=VaR_\\alpha(X) is an increasing\nfunction of the dependence parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.3464v3"
    },
    {
        "title": "Smooth Nonparametric Bernstein Vine Copulas",
        "authors": [
            "Gregor Weiß",
            "Marcus Scheffer"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We propose to use nonparametric Bernstein copulas as bivariate pair-copulas\nin high-dimensional vine models. The resulting smooth and nonparametric vine\ncopulas completely obviate the error-prone need for choosing the pair-copulas\nfrom parametric copula families. By means of a simulation study and an\nempirical analysis of financial market data, we show that our proposed smooth\nnonparametric vine copula model is superior to competing parametric vine models\ncalibrated via Akaike's Information Criterion.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.2043v1"
    },
    {
        "title": "Closed form solutions of measures of systemic risk",
        "authors": [
            "Manfred Jaeger-Ambrozewicz"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  This paper derives -- considering a Gaussian setting -- closed form solutions\nof the statistics that Adrian and Brunnermeier and Acharya et al. have\nsuggested as measures of systemic risk to be attached to individual banks. The\nstatistics equal the product of statistic specific Beta-coefficients with the\nmean corrected Value at Risk. Hence, the measures of systemic risks are closely\nrelated to well known concepts of financial economics. Another benefit of the\nanalysis is that it is revealed how the concepts are related to each other.\nAlso, it may be relatively easy to convince the regulators to consider a closed\nform solution, especially so if the statistics involved are well known and can\neasily be communicated to the financial community.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4173v1"
    },
    {
        "title": "The art of probability-of-default curve calibration",
        "authors": [
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  PD curve calibration refers to the transformation of a set of rating grade\nlevel probabilities of default (PDs) to another average PD level that is\ndetermined by a change of the underlying portfolio-wide PD. This paper presents\na framework that allows to explore a variety of calibration approaches and the\nconditions under which they are fit for purpose. We test the approaches\ndiscussed by applying them to publicly available datasets of agency rating and\ndefault statistics that can be considered typical for the scope of application\nof the approaches. We show that the popular 'scaled PDs' approach is\ntheoretically questionable and identify an alternative calibration approach\n('scaled likelihood ratio') that is both theoretically sound and performs\nbetter on the test datasets.\n  Keywords: Probability of default, calibration, likelihood ratio, Bayes'\nformula, rating profile, binary classification.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.3716v6"
    },
    {
        "title": "Multivariate risk measures: a constructive approach based on selections",
        "authors": [
            "Ignacio Cascos",
            "Ilya Molchanov"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Since risky positions in multivariate portfolios can be offset by various\nchoices of capital requirements that depend on the exchange rules and related\ntransaction costs, it is natural to assume that the risk measures of random\nvectors are set-valued. Furthermore, it is reasonable to include the exchange\nrules in the argument of the risk measure and so consider risk measures of\nset-valued portfolios. This situation includes the classical Kabanov's\ntransaction costs model, where the set-valued portfolio is given by the sum of\na random vector and an exchange cone, but also a number of further cases of\nadditional liquidity constraints.\n  We suggest a definition of the risk measure based on calling a set-valued\nportfolio acceptable if it possesses a selection with all individually\nacceptable marginals. The obtained selection risk measure is coherent (or\nconvex), law invariant and has values being upper convex closed sets. We\ndescribe the dual representation of the selection risk measure and suggest\nefficient ways of approximating it from below and from above. In case of\nKabanov's exchange cone model, it is shown how the selection risk measure\nrelates to the set-valued risk measures considered by Kulikov (2008), Hamel and\nHeyde (2010), and Hamel, Heyde and Rudloff (2013).\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1496v6"
    },
    {
        "title": "Leverage-induced systemic risk under Basle II and other credit risk\n  policies",
        "authors": [
            "Sebastian Poledna",
            "Stefan Thurner",
            "J. Doyne Farmer",
            "John Geanakoplos"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We use a simple agent based model of value investors in financial markets to\ntest three credit regulation policies. The first is the unregulated case, which\nonly imposes limits on maximum leverage. The second is Basle II and the third\nis a hypothetical alternative in which banks perfectly hedge all of their\nleverage-induced risk with options. When compared to the unregulated case both\nBasle II and the perfect hedge policy reduce the risk of default when leverage\nis low but increase it when leverage is high. This is because both regulation\npolicies increase the amount of synchronized buying and selling needed to\nachieve deleveraging, which can destabilize the market. None of these policies\nare optimal for everyone: Risk neutral investors prefer the unregulated case\nwith low maximum leverage, banks prefer the perfect hedge policy, and fund\nmanagers prefer the unregulated case with high maximum leverage. No one prefers\nBasle II.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.6114v2"
    },
    {
        "title": "DebtRank-transparency: Controlling systemic risk in financial networks",
        "authors": [
            "Stefan Thurner",
            "Sebastian Poledna"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Banks in the interbank network can not assess the true risks associated with\nlending to other banks in the network, unless they have full information on the\nriskiness of all the other banks. These risks can be estimated by using network\nmetrics (for example DebtRank) of the interbank liability network which is\navailable to Central Banks. With a simple agent based model we show that by\nincreasing transparency by making the DebtRank of individual nodes (banks)\nvisible to all nodes, and by imposing a simple incentive scheme, that reduces\ninterbank borrowing from systemically risky nodes, the systemic risk in the\nfinancial network can be drastically reduced. This incentive scheme is an\neffective regulation mechanism, that does not reduce the efficiency of the\nfinancial network, but fosters a more homogeneous distribution of risk within\nthe system in a self-organized critical way. We show that the reduction of\nsystemic risk is to a large extent due to the massive reduction of cascading\nfailures in the transparent system. An implementation of this minimal\nregulation scheme in real financial networks should be feasible from a\ntechnical point of view.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.6115v1"
    },
    {
        "title": "Ruin Probabilities for Risk Processes with Non-Stationary Arrivals and\n  Subexponential Claims",
        "authors": [
            "Lingjiong Zhu"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  In this paper, we obtain the finite-horizon and infinite-horizon ruin\nprobability asymptotics for risk processes with claims of subexponential tails\nfor non-stationary arrival processes that satisfy a large deviation principle.\nAs a result, the arrival process can be dependent, non-stationary and\nnon-renewal. We give three examples of non-stationary and non-renewal point\nprocesses: Hawkes process, Cox process with shot noise intensity and\nself-correcting point process. We also show some aggregate claims results for\nthese three examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.1940v3"
    },
    {
        "title": "Loss Distribution Approach for Operational Risk Capital Modelling under\n  Basel II: Combining Different Data Sources for Risk Estimation",
        "authors": [
            "Pavel V. Shevchenko",
            "Gareth W. Peters"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The management of operational risk in the banking industry has undergone\nsignificant changes over the last decade due to substantial changes in\noperational risk environment. Globalization, deregulation, the use of complex\nfinancial products and changes in information technology have resulted in\nexposure to new risks very different from market and credit risks. In response,\nBasel Committee for banking Supervision has developed a regulatory framework,\nreferred to as Basel II, that introduced operational risk category and\ncorresponding capital requirements. Over the past five years, major banks in\nmost parts of the world have received accreditation under the Basel II Advanced\nMeasurement Approach (AMA) by adopting the loss distribution approach (LDA)\ndespite there being a number of unresolved methodological challenges in its\nimplementation. Different approaches and methods are still under hot debate. In\nthis paper, we review methods proposed in the literature for combining\ndifferent data sources (internal data, external data and scenario analysis)\nwhich is one of the regulatory requirement for AMA.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.1882v1"
    },
    {
        "title": "The convergence of regional house prices in the USA in the context of\n  the stress testing of financial institutions",
        "authors": [
            "Argyn Kuketayev"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  I studied the convergence of regional house prices to national prices in USA\nby analyzing time-series of house price indices of 9 Census Divisions. I found\nthe evidence of the convergence in some parts of the country using asymmetric\nunit root tests. The fact that the evidence of the convergence is not present\nin large parts of the country raises an issue of execution and interpretation\nof results of Federal Reserve Bank's annual stress testing of the US banking\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3531v1"
    },
    {
        "title": "Modeling catastrophic deaths using EVT with a microsimulation approach\n  to reinsurance pricing",
        "authors": [
            "Matias Leppisaari"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Recently, a marked Poisson process (MPP) model for life catastrophe risk was\nproposed in [6]. We provide a justification and further support for the model\nby considering more general Poisson point processes in the context of extreme\nvalue theory (EVT), and basing the choice of model on statistical tests and\nmodel comparisons. A case study examining accidental deaths in the Finnish\npopulation is provided.\n  We further extend the applicability of the catastrophe risk model by\nconsidering small and big accidents separately; the resulting combined MPP\nmodel can flexibly capture the whole range of accidental death counts. Using\nthe proposed model, we present a simulation framework for pricing (life)\ncatastrophe reinsurance, based on modeling the underlying policies at\nindividual contract level. The accidents are first simulated at population\nlevel, and their effect on a specific insurance company is then determined by\nexplicitly simulating the resulting insured deaths. The proposed\nmicrosimulation approach can potentially lead to more accurate results than the\ntraditional methods, and to a better view of risk, as it can make use of all\nthe information available to the re/insurer and can explicitly accommodate even\ncomplex re/insurance terms and product features. As an example we price several\nexcess reinsurance contracts. The proposed simulation model is also suitable\nfor solvency assessment.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.8604v1"
    },
    {
        "title": "Stochastic areas of diffusions and applications in risk theory",
        "authors": [
            "Zhenyu Cui"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  In this paper we study the stochastic area swept by a regular\ntime-homogeneous diffusion till a stopping time. This unifies some recent\nliterature in this area. Through stochastic time change we establish a link\nbetween the stochastic area and the stopping time of another associated\ntime-homogeneous diffusion. Then we characterize the Laplace transform of the\nstochastic area in terms of the eigenfunctions of the associated diffusion. We\nalso explicitly obtain the integer moments of the stochastic area in terms of\nscale and speed densities of the associated diffusion. Specifically we study in\ndetail three stopping times: the first passage time to a constant level, the\nfirst drawdown time and the Azema-Yor stopping time. We also study the total\noccupation area of the diffusion below a constant level. We show applications\nof the results to a new structural model of default (Yildirim 2006), the Omega\nrisk model of bankruptcy in risk analysis (Gerber, Shiu and Yang 2012), and a\ndiffusion risk model with surplus-dependent tax (Albrecher and Hipp 2007, Li,\nTang and Zhou 2013).\n",
        "pdf_link": "http://arxiv.org/pdf/1312.0283v1"
    },
    {
        "title": "The impact of systemic risk on the diversification benefits of a risk\n  portfolio",
        "authors": [
            "Marc Busse",
            "Michel Dacorogna",
            "Marie Kratz"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  Risk diversification is the basis of insurance and investment. It is thus\ncrucial to study the effects that could limit it. One of them is the existence\nof systemic risk that affects all the policies at the same time. We introduce\nhere a probabilistic approach to examine the consequences of its presence on\nthe risk loading of the premium of a portfolio of insurance policies. This\napproach could be easily generalized for investment risk. We see that, even\nwith a small probability of occurrence, systemic risk can reduce dramatically\nthe diversification benefits. It is clearly revealed via a non-diversifiable\nterm that appears in the analytical expression of the variance of our models.\nWe propose two ways of introducing it and discuss their advantages and\nlimitations. By using both VaR and TVaR to compute the loading, we see that\nonly the latter captures the full effect of systemic risk when its probability\nto occur is low\n",
        "pdf_link": "http://arxiv.org/pdf/1312.0506v1"
    },
    {
        "title": "Dynamic Limit Growth Indices in Discrete Time",
        "authors": [
            "Tomasz R. Bielecki",
            "Igor Cialenco",
            "Marcin Pitera"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We propose a new class of mappings, called Dynamic Limit Growth Indices, that\nare designed to measure the long-run performance of a financial portfolio in\ndiscrete time setup. We study various important properties for this new class\nof measures, and in particular, we provide necessary and sufficient condition\nfor a Dynamic Limit Growth Index to be a dynamic assessment index. We also\nestablish their connection with classical dynamic acceptability indices, and we\nshow how to construct examples of Dynamic Limit Growth Indices using dynamic\nrisk measures and dynamic certainty equivalents. Finally, we propose a new\ndefinition of time consistency, suitable for these indices, and we study time\nconsistency for the most notable representative of this class -- the dynamic\nanalog of risk sensitive criterion.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.1006v2"
    },
    {
        "title": "Densely Entangled Financial Systems",
        "authors": [
            "Bhaskar DasGupta",
            "Lakshmi Kaligounder"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In [1] Zawadoski introduces a banking network model in which the asset and\ncounter-party risks are treated separately and the banks hedge their assets\nrisks by appropriate OTC contracts. In his model, each bank has only two\ncounter-party neighbors, a bank fails due to the counter-party risk only if at\nleast one of its two neighbors default, and such a counter-party risk is a low\nprobability event. Informally, the author shows that the banks will hedge their\nasset risks by appropriate OTC contracts, and, though it may be socially\noptimal to insure against counter-party risk, in equilibrium banks will {\\em\nnot} choose to insure this low probability event.\n  In this paper, we consider the above model for more general network\ntopologies, namely when each node has exactly 2r counter-party neighbors for\nsome integer r>0. We extend the analysis of [1] to show that as the number of\ncounter-party neighbors increase the probability of counter-party risk also\nincreases, and in particular the socially optimal solution becomes privately\nsustainable when each bank hedges its risk to at least n/2 banks, where n is\nthe number of banks in the network, i.e., when 2r is at least n/2, banks not\nonly hedge their asset risk but also hedge its counter-party risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.5208v1"
    },
    {
        "title": "Systemic risk in dynamical networks with stochastic failure criterion",
        "authors": [
            "B. Podobnik",
            "D. Horvatic",
            "M. Bertella",
            "L. Feng",
            "X. Huang",
            "B. Li"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  Complex non-linear interactions between banks and assets we model by two\ntime-dependent Erd\\H{o}s Renyi network models where each node, representing\nbank, can invest either to a single asset (model I) or multiple assets (model\nII). We use dynamical network approach to evaluate the collective financial\nfailure---systemic risk---quantified by the fraction of active nodes. The\nsystemic risk can be calculated over any future time period, divided on\nsub-periods, where within each sub-period banks may contiguously fail due to\nlinks to either (i) assets or (ii) other banks, controlled by two parameters,\nprobability of internal failure $p$ and threshold $T_h$ (\"solvency\" parameter).\nThe systemic risk non-linearly increases with $p$ and decreases with average\nnetwork degree faster when all assets are equally distributed across banks than\nif assets are randomly distributed. The more inactive banks each bank can\nsustain (smaller $T_h$), the smaller the systemic risk---for some $T_h$ values\nin I we report a discontinuity in systemic risk. When contiguous spreading\nbecomes stochastic (ii) controlled by probability $p_2$---a condition for the\nbank to be solvent (active) is stochastic---the systemic risk decreases with\ndecreasing $p_2$. We analyse asset allocation for the U.S. banks.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5623v2"
    },
    {
        "title": "Omega risk model with tax",
        "authors": [
            "Zhenyu Cui"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In this paper we study the Omega risk model with surplus-dependent tax\npayments in a time-homogeneous diffusion setting. The new model incorporates\npractical features from both the Omega risk model(Albrecher and Gerber and Shiu\n(2011)) and the risk model with tax(Albrecher and Hipp (2007)). We explicitly\ncharacterize the Laplace transform of the occupation time of an Azema-Yor\nprocess(e.g. a process refracted by functionals of its running maximum) below a\nconstant level until the first hitting time of another Azema-Yor process or\nuntil an independent exponential time. This result unifies and extends recent\nliterature(Li and Zhou (2013) and Zhang (2014)) incorporating some of their\nresults as special cases. We explicitly characterize the Laplace transform of\nthe time of bankruptcy in the Omega risk model with tax and discuss an\nextension to integral functionals. Finally we present examples using a Brownian\nmotion with drift.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.7680v1"
    },
    {
        "title": "Are credit ratings time-homogeneous and Markov?",
        "authors": [
            "Pedro Lencastre",
            "Frank Raischel",
            "Pedro G. Lind",
            "Tim Rogers"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We introduce a simple approach for testing the reliability of homogeneous\ngenerators and the Markov property of the stochastic processes underlying\nempirical time series of credit ratings. We analyze open access data provided\nby Moody's and show that the validity of these assumptions - existence of a\nhomogeneous generator and Markovianity - is not always guaranteed. Our analysis\nis based on a comparison between empirical transition matrices aggregated over\nfixed time windows and candidate transition matrices generated from\nmeasurements taken over shorter periods. Ratings are widely used in credit\nrisk, and are a key element in risk assessment; our results provide a tool for\nquantifying confidence in predictions extrapolated from these time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.8018v2"
    },
    {
        "title": "Estimating Operational Risk Capital with Greater Accuracy, Precision,\n  and Robustness",
        "authors": [
            "J. D. Opdyke"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  The largest US banks are required by regulatory mandate to estimate the\noperational risk capital they must hold using an Advanced Measurement Approach\n(AMA) as defined by the Basel II/III Accords. Most use the Loss Distribution\nApproach (LDA) which defines the aggregate loss distribution as the convolution\nof a frequency and a severity distribution representing the number and\nmagnitude of losses, respectively. Estimated capital is a Value-at-Risk (99.9th\npercentile) estimate of this annual loss distribution. In practice, the\nseverity distribution drives the capital estimate, which is essentially a very\nhigh quantile of the estimated severity distribution. Unfortunately, because\nthe relevant severities are heavy-tailed AND the quantiles being estimated are\nso high, VaR always appears to be a convex function of the severity parameters,\ncausing all widely-used estimators to generate biased capital estimates\n(apparently) due to Jensen's Inequality. The observed capital inflation is\nsometimes enormous, even at the unit-of-measure (UoM) level (even billions\nUSD). Herein I present an estimator of capital that essentially eliminates this\nupward bias. The Reduced-bias Capital Estimator (RCE) is more consistent with\nthe regulatory intent of the LDA framework than implementations that fail to\nmitigate this bias. RCE also notably increases the precision of the capital\nestimate and consistently increases its robustness to violations of the i.i.d.\ndata presumption (which are endemic to operational risk loss event data). So\nwith greater capital accuracy, precision, and robustness, RCE lowers capital\nrequirements at both the UoM and enterprise levels, increases capital stability\nfrom quarter to quarter, ceteris paribus, and does both while more accurately\nand precisely reflecting regulatory intent. RCE is straightforward to implement\nusing any major statistical software package.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0389v6"
    },
    {
        "title": "On Optimal Reinsurance Policy with Distortion Risk Measures and Premiums",
        "authors": [
            "Hirbod Assa"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In this paper, we consider the problem of optimal reinsurance design, when\nthe risk is measured by a distortion risk measure and the premium is given by a\ndistortion risk premium. First, we show how the optimal reinsurance design for\nthe ceding company, the reinsurance company and the social planner can be\nformulated in the same way. Second, by introducing the marginal indemnification\nfunctions, we characterize the optimal reinsurance contracts. We show that, for\nan optimal policy, the associated marginal indemnification function only takes\nthe values zero and one. We will see how the roles of the market preferences\nand premiums and that of the total risk are separated.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2950v1"
    },
    {
        "title": "On the Depletion Problem for an Insurance Risk Process: New Non-ruin\n  Quantities in Collective Risk Theory",
        "authors": [
            "Zied Ben-Salah",
            "Hélène Guérin",
            "Manuel Morales",
            "Hassan Omidi Firouzi"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  The field of risk theory has traditionally focused on ruin-related\nquantities. In particular, the socalled Expected Discounted Penalty Function\nhas been the object of a thorough study over the years. Although interesting in\ntheir own right, ruin related quantities do not seem to capture path-dependent\nproperties of the reserve. In this article we aim at presenting the\nprobabilistic properties of drawdowns and the speed at which an insurance\nreserve depletes as a consequence of the risk exposure of the company. These\nnew quantities are not ruin related yet they capture important features of an\ninsurance position and we believe it can lead to the design of a meaningful\nrisk measures. Studying drawdowns and speed of depletion for L\\'evy insurance\nrisk processes represent a novel and challenging concept in insurance\nmathematics. In this paper, all these concepts are formally introduced in an\ninsurance setting. Moreover, using recent results in fluctuation theory for\nL\\'evy processes, we derive expressions for the distribution of several\nquantities related to the depletion problem. Of particular interest are the\ndistribution of drawdowns and the Laplace transform for the speed of depletion.\nThese expressions are given for some examples of L\\'evy insurance risk\nprocesses for which they can be calculated, in particular for the classical\nCramer-Lundberg model.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.6952v1"
    },
    {
        "title": "Diversification and Endogenous Financial Networks",
        "authors": [
            "Jean-Cyprien Héam",
            "Erwan Koch"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We test the hypothesis that interconnections across financial institutions\ncan be explained by a diversification motive. This idea stems from the\nempirical evidence of the existence of long-term exposures that cannot be\nexplained by a liquidity motive (maturity or currency mismatch). We model\nendogenous interconnections of heterogenous financial institutions facing\nregulatory constraints using a maximization of their expected utility. Both\ntheoretical and simulation-based results are compared to a stylized genuine\nfinancial network. The diversification motive appears to plausibly explain\ninterconnections among key players. Using our model, the impact of regulation\non interconnections between banks -currently discussed at the Basel Committee\non Banking Supervision- is analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.4618v3"
    },
    {
        "title": "Hedging Conditional Value at Risk with Options",
        "authors": [
            "Maciej J. Capiński"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We present a method of hedging Conditional Value at Risk of a position in\nstock using put options. The result leads to a linear programming problem that\ncan be solved to optimise risk hedging.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.6673v2"
    },
    {
        "title": "Modelling cross-border systemic risk in the European banking sector: a\n  copula approach",
        "authors": [
            "Raffaella Calabrese",
            "Silvia Osmetti"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We propose a new methodology based on the Marshall-Olkin (MO) copula to model\ncross-border systemic risk. The proposed framework estimates the impact of the\nsystematic and idiosyncratic components on systemic risk. Initially, we propose\na maximum-likelihood method to estimate the parameter of the MO copula. In\norder to use the data on non-distressed banks for these estimates, we consider\ntimes to bank failures as censored samples. Hence, we propose an estimation\nprocedure for the MO copula on censored data. The empirical evidence from\nEuropean banks shows that the proposed censored model avoid possible\nunderestimation of the contagion risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1348v1"
    },
    {
        "title": "Impact of credit default swaps on financial contagion",
        "authors": [
            "Yoshiharu Maeno",
            "Kenji Nishiguchi",
            "Satoshi Morinaga",
            "Hirokazu Matsushima"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  It had been believed in the conventional practice that the risk of a bank\ngoing bankrupt is lessened in a straightforward manner by transferring the risk\nof loan defaults. But the failure of American International Group in 2008 posed\na more complex aspect of financial contagion. This study presents an extension\nof the asset network systemic risk model (ANWSER) to investigate whether credit\ndefault swaps mitigate or intensify the severity of financial contagion. A\nprotection buyer bank transfers the risk of every possible debtor bank default\nto protection seller banks. The empirical distribution of the number of bank\nbankruptcies is obtained with the extended model. Systemic capital buffer ratio\nis calculated from the distribution. The ratio quantifies the effective loss\nabsorbency capability of the entire financial system to force back financial\ncontagion. The key finding is that the leverage ratio is a good estimate of a\nsystemic capital buffer ratio as the backstop of a financial system. The risk\ntransfer from small and medium banks to big banks in an interbank network does\nnot mitigate the severity of financial contagion.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1356v1"
    },
    {
        "title": "On the Coherent Risk Measure Representations in the Discrete Probability\n  Spaces",
        "authors": [
            "Kerem Ugurlu"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We give a complete characterization of both comonotone and not comonotone\ncoherent risk measures in the discrete finite probability space, where each\noutcome is equally likely. To the best of our knowledge, this is the first work\nthat characterizes \\textit{and} distinguishes comonotone and not comonotone\ncoherent risk measures via a simplified AVaR representation in this probability\nspace, which is crucial in applications and simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4441v4"
    },
    {
        "title": "Two maxentropic approaches to determine the probability density of\n  compound risk losses",
        "authors": [
            "Erika Gomes-Gonçalves",
            "Henryk Gzyl",
            "Silvia Mayoral"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  Here we present an application of two maxentropic procedures to determine the\nprobability density distribution of compound sums of random variables, using\nonly a finite number of empirically determined fractional moments. The two\nmethods are the Standard method of Maximum Entropy (SME), and the method of\nMaximum Entropy in the Mean (MEM). We shall verify that the reconstructions\nobtained satisfy a variety of statistical quality criteria, and provide good\nestimations of VaR and TVaR, which are important measures for risk management\npurposes. We analyze the performance and robustness of these two procedures in\nseveral numerical examples, in which the frequency of losses is Poisson and the\nindividual losses are lognormal random variables. As side product of the work,\nwe obtain a rather accurate description of the density of the compound random\nvariable. This is an extension of a previous application based on the Standard\nMaximum Entropy approach (SME) where the analytic form of the Laplace transform\nwas available to a case in which only observed or simulated data is used. These\napproaches are also used to develop a procedure to determine the distribution\nof the individual losses through the knowledge of the total loss. Then, in the\ncase of having only historical total losses, it is possible to decompound or\ndisaggregate the random sums in its frequency/severity distributions, through a\nprobabilistic inverse problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.5625v2"
    },
    {
        "title": "Equilibrium in risk-sharing games",
        "authors": [
            "Michail Anthropelos",
            "Constantinos Kardaras"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  The large majority of risk-sharing transactions involve few agents, each of\nwhom can heavily influence the structure and the prices of securities. This\npaper proposes a game where agents' strategic sets consist of all possible\nsharing securities and pricing kernels that are consistent with Arrow-Debreu\nsharing rules. First, it is shown that agents' best response problems have\nunique solutions. The risk-sharing Nash equilibrium admits a finite-dimensional\ncharacterisation and it is proved to exist for arbitrary number of agents and\nbe unique in the two-agent game. In equilibrium, agents declare beliefs on\nfuture random outcomes different than their actual probability assessments, and\nthe risk-sharing securities are endogenously bounded, implying (among other\nthings) loss of efficiency. In addition, an analysis regarding extremely risk\ntolerant agents indicates that they profit more from the Nash risk-sharing\nequilibrium as compared to the Arrow-Debreu one.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.4208v3"
    },
    {
        "title": "A Directional Multivariate Value at Risk",
        "authors": [
            "Raúl Torres",
            "Rosa E. Lillo",
            "Henry Laniado"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In economics, insurance and finance, value at risk (VaR) is a widely used\nmeasure of the risk of loss on a specific portfolio of financial assets. For a\ngiven portfolio, time horizon, and probability $\\alpha$, the $100\\alpha\\%$ VaR\nis defined as a threshold loss value, such that the probability that the loss\non the portfolio over the given time horizon exceeds this value is $\\alpha$.\nThat is to say, it is a quantile of the distribution of the losses, which has\nboth good analytic properties and easy interpretation as a risk measure.\nHowever, its extension to the multivariate framework is not unique because a\nunique definition of multivariate quantile does not exist. In the current\nliterature, the multivariate quantiles are related to a specific partial order\nconsidered in $\\mathbb{R}^{n}$, or to a property of the univariate quantile\nthat is desirable to be extended to $\\mathbb{R}^{n}$. In this work, we\nintroduce a multivariate value at risk as a vector-valued directional risk\nmeasure, based on a directional multivariate quantile, which has recently been\nintroduced in the literature. The directional approach allows the manager to\nconsider external information or risk preferences in her/his analysis. We have\nderived some properties of the risk measure and we have compared the univariate\n\\textit{VaR} over the marginals with the components of the directional\nmultivariate VaR. We have also analyzed the relationship between some families\nof copulas, for which it is possible to obtain closed forms of the multivariate\nVaR that we propose. Finally, comparisons with other alternative multivariate\nVaR given in the literature, are provided in terms of robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.00908v1"
    },
    {
        "title": "Diversification, protection of liability holders and regulatory\n  arbitrage",
        "authors": [
            "Pablo Koch-Medina",
            "Cosimo Munari",
            "Mario Sikic"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Any solvency regime for financial institutions should be aligned with the\nfundamental objectives of regulation: protecting liability holders and securing\nthe stability of the financial system. The first objective leads to consider\nsurplus-invariant capital adequacy tests, i.e. tests that do not depend on the\nsurplus of a financial institution. We provide a complete characterization of\nclosed, convex, surplus-invariant capital adequacy tests that highlights an\ninherent tension between surplus-invariance and the desire to give credit for\ndiversification. The second objective leads to requiring consistency of capital\nadequacy tests across jurisdictions. Of particular importance in this respect\nare capital adequacy tests that remain invariant under a change of\nnum\\'{e}raire. We establish an intimate link between surplus- and num\\'{e}raire\ninvariant tests.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.03252v3"
    },
    {
        "title": "Model risk on credit risk",
        "authors": [
            "J. Molins",
            "E. Vives"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  This paper develops the Jungle model in a credit portfolio framework. The\nJungle model is able to model credit contagion, produce doubly-peaked\nprobability distributions for the total default loss and endogenously generate\nquasi phase transitions, potentially leading to systemic credit events which\nhappen unexpectedly and without an underlying single cause. We show the Jungle\nmodel provides the optimal probability distribution for credit losses, under\nsome reasonable empirical constraints. The Dandelion model, a particular case\nof the Jungle model, is presented, motivated and exactly solved. The Dandelion\nmodel provides an explicit example of doubly-peaked probability distribution\nfor the credit losses. The Diamond model, another instance of the Jungle model,\nexperiences the so called quasi phase transitions; in particular, both the U.S.\nsubprime and the European sovereign crises are shown to be potential examples\nof quasi phase transitions. We argue the three known sources of default\nclustering (contagion, macroeconomic risk factors and frailty) can be\nunderstood under the unifying framework of contagion. We suggest how the Jungle\nmodel is able to explain a series of empirical stylized facts in credit\nportfolios, hard to reconcile by some standard credit portfolio models. We show\nthe Jungle model can handle inhomogeneous portfolios with state-dependent\nrecovery rates. We look at model risk in a credit risk framework under the\nJungle model, especially in relation to systemic risks posed by doubly-peaked\ndistributions and quasi phase transitions.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06984v3"
    },
    {
        "title": "A generic model for spouse's pensions with a view towards the\n  calculation of liabilities",
        "authors": [
            "Alexander Sokol"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  We introduce a generic model for spouse's pensions. The generic model allows\nfor the modeling of various types of spouse's pensions with payments commencing\nat the death of the insured. We derive abstract formulas for cashflows and\nliabilities corresponding to common types of spouse's pensions. We show how the\nstandard formulas from the Danish G82 concession can be obtained as a special\ncase of our generic model. We also derive expressions for liabilities for\nspouse's pensions in models more advanced than found in the G82 concession. The\ngeneric nature of our model and results furthermore enable the calculation of\ncashflows and liabilities using simple estimates of marital behaviour among a\npopulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.02034v1"
    },
    {
        "title": "Agent-based mapping of credit risk for sustainable microfinance",
        "authors": [
            "Joung-Hun Lee",
            "Marko Jusup",
            "Boris Podobnik",
            "Yoh Iwasa"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Inspired by recent ideas on how the analysis of complex financial risks can\nbenefit from analogies with independent research areas, we propose an\nunorthodox framework for mapping microfinance credit risk---a major obstacle to\nthe sustainability of lenders outreaching to the poor. Specifically, using the\nelements of network theory, we constructed an agent-based model that obeys the\nstylised rules of microfinance industry. We found that in a deteriorating\neconomic environment confounded with adverse selection, a form of latent moral\nhazard may cause a regime shift from a high to a low loan repayment\nprobability. An after-the-fact recovery, when possible, required the economic\nenvironment to improve beyond that which led to the shift in the first place.\nThese findings suggest a small set of measurable quantities for mapping\nmicrofinance credit risk and, consequently, for balancing the requirements to\nreasonably price loans and to operate on a fully self-financed basis. We\nillustrate how the proposed mapping works using a 10-year monthly data set from\none of the best-known microfinance representatives, Grameen Bank in Bangladesh.\nFinally, we discuss an entirely new perspective for managing microfinance\ncredit risk based on enticing spontaneous cooperation by building social\ncapital.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.05737v1"
    },
    {
        "title": "Remarks on equality of two distributions under some partial orders",
        "authors": [
            "Chuancun Yin"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In this note we establish some appropriate conditions for stochastic equality\nof two random variables/vectors which are ordered with respect to convex\nordering or with respect to supermodular ordering. Multivariate extensions of\nthis result are also considered.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.04485v1"
    },
    {
        "title": "CEI: a new indicator measuring City Commercial Credit Risk initiated in\n  China",
        "authors": [
            "Ruonan Lin",
            "Yi Gu"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Aiming at quantifying and evaluating the regional commercial environment\nalong with the level of economic development among cities in mainland China,\nthe concept of China City Commercial Environment Credit Index(CEI) was first\nintroduced and established in 2010. In this manuscript, a historical review and\ndetailed introduction of CEI is included, followed by statistical studies. In\nparticular, an independent statistical cross-check for the existing CEI-2012 is\nperformed and significant factors that play the most in influential roles are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.05089v1"
    },
    {
        "title": "Fitting a distribution to Value-at-Risk and Expected Shortfall, with an\n  application to covered bonds",
        "authors": [
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Covered bonds are a specific example of senior secured debt. If the issuer of\nthe bonds defaults the proceeds of the assets in the cover pool are used for\ntheir debt service. If in this situation the cover pool proceeds do not suffice\nfor the debt service, the creditors of the bonds have recourse to the issuer's\nassets and their claims are pari passu with the claims of the creditors of\nsenior unsecured debt. Historically, covered bonds have been very safe\ninvestments. During their more than two hundred years of existence, investors\nnever suffered losses due to missed payments from covered bonds. From a risk\nmanagement perspective, therefore modelling covered bonds losses is mainly of\ninterest for estimating the impact that the asset encumbrance by the cover pool\nhas on the loss characteristics of the issuer's senior unsecured debt. We\nexplore one-period structural modelling approaches for covered bonds and senior\nunsecured debt losses with one and two asset value variables respectively.\nObviously, two-assets models with separate values of the cover pool and the\nissuer's remaining portfolio allow for more realistic modelling. However, we\ndemonstrate that exact calibration of such models may be impossible. We also\ninvestigate a one-asset model in which the riskiness of the cover pool is\nreflected by a risk-based adjustment of the encumbrance ratio of the issuer's\nassets.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.07484v2"
    },
    {
        "title": "Expected Shortfall is jointly elicitable with Value at Risk -\n  Implications for backtesting",
        "authors": [
            "Tobias Fissler",
            "Johanna F. Ziegel",
            "Tilmann Gneiting"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In this note, we comment on the relevance of elicitability for backtesting\nrisk measure estimates. In particular, we propose the use of Diebold-Mariano\ntests, and show how they can be implemented for Expected Shortfall (ES), based\non the recent result of Fissler and Ziegel (2015) that ES is jointly elicitable\nwith Value at Risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.00244v2"
    },
    {
        "title": "Multivariate Shortfall Risk Allocation and Systemic Risk",
        "authors": [
            "Yannick Armenti",
            "Stephane Crepey",
            "Samuel Drapeau",
            "Antonis Papapantoleon"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  The ongoing concern about systemic risk since the outburst of the global\nfinancial crisis has highlighted the need for risk measures at the level of\nsets of interconnected financial components, such as portfolios, institutions\nor members of clearing houses. The two main issues in systemic risk measurement\nare the computation of an overall reserve level and its allocation to the\ndifferent components according to their systemic relevance. We develop here a\npragmatic approach to systemic risk measurement and allocation based on\nmultivariate shortfall risk measures, where acceptable allocations are first\ncomputed and then aggregated so as to minimize costs. We analyze the\nsensitivity of the risk allocations to various factors and highlight its\nrelevance as an indicator of systemic risk. In particular, we study the\ninterplay between the loss function and the dependence structure of the\ncomponents. Moreover, we address the computational aspects of risk allocation.\nFinally, we apply this methodology to the allocation of the default fund of a\nCCP on real data.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05351v4"
    },
    {
        "title": "Risk Quantification in Stochastic Simulation under Input Uncertainty",
        "authors": [
            "Helin Zhu",
            "Tianyi Liu",
            "Enlu Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  When simulating a complex stochastic system, the behavior of output response\ndepends on input parameters estimated from finite real-world data, and the\nfiniteness of data brings input uncertainty into the system. The quantification\nof the impact of input uncertainty on output response has been extensively\nstudied. Most of the existing literature focuses on providing inferences on the\nmean response at the true but unknown input parameter, including point\nestimation and confidence interval construction. Risk quantification of mean\nresponse under input uncertainty often plays an important role in system\nevaluation and control, because it provides inferences on extreme scenarios of\nmean response in all possible input models. To the best of our knowledge, it\nhas rarely been systematically studied in the literature. In this paper, first\nwe introduce risk measures of mean response under input uncertainty, and\npropose a nested Monte Carlo simulation approach to estimate them. Then we\ndevelop asymptotical properties such as consistency and asymptotic normality\nfor the proposed nested risk estimators. We further study the associated budget\nallocation problem for efficient nested risk simulation, and finally use a\nsharing economy example to illustrate the importance of accessing and\ncontrolling risk due to input uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.06015v3"
    },
    {
        "title": "Asyptotic Normality for Maximum Likelihood Estimation and Operational\n  Risk",
        "authors": [
            "Paul Larsen"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Operational risk models commonly employ maximum likelihood estimation (MLE)\nto fit loss data to heavy-tailed distributions. Yet several desirable\nproperties of MLE (e.g. asymptotic normality) are generally valid only for\nlarge sample-sizes, a situation rarely encountered in operational risk. In this\npaper, we study how asymptotic normality does--or does not--hold for common\nseverity distributions in operational risk models. We then apply these results\nto evaluate errors caused by failure of asymptotic normality in constructing\nconfidence intervals around the MLE fitted parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02824v3"
    },
    {
        "title": "Hydroassets Portfolio Management for Intraday Electricity Trading from a\n  Discrete Time Stochastic Optimization Perspective",
        "authors": [
            "Simone Farinelli",
            "Luisa Tibiletti"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Hydro storage system optimization is becoming one of the most challenging\ntasks in Energy Finance. While currently the state-of-the-art of the commercial\nsoftware in the industry implements mainly linear models, we would like to\nintroduce risk aversion and a generic utility function. At the same time, we\naim to develop and implement a computational efficient algorithm, which is not\naffected by the curse of dimensionality and does not utilize subjective\nheuristics to prevent it. For the short term power market we propose a\nsimultaneous solution for both dispatch and bidding problems.\n  Following the Blomvall and Lindberg (2002) interior point model, we set up a\nstochastic multiperiod optimization procedure by means of a \"bushy\" recombining\ntree that provides fast computational results. Inequality constraints are\npacked into the objective function by the logarithmic barrier approach and the\nutility function is approximated by its second order Taylor polynomial. The\noptimal solution for the original problem is obtained as a diagonal sequence\nwhere the first diagonal dimension is the parameter controlling the logarithmic\npenalty and the second is the parameter for the Newton step in the construction\nof the approximated solution. Optimal intraday electricity trading and water\nvalues for hydro assets as shadow prices are computed. The algorithm is\nimplemented in Mathematica.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05837v5"
    },
    {
        "title": "Optimal Investment in a Dual Risk Model",
        "authors": [
            "Arash Fahim",
            "Lingjiong Zhu"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Dual risk models are popular for modeling a venture capital or high tech\ncompany, for which the running cost is deterministic and the profits arrive\nstochastically over time. Most of the existing literature on dual risk models\nconcentrated on the optimal dividend strategies. In this paper, we propose to\nstudy the optimal investment strategy on research and development for the dual\nrisk models to minimize the ruin probability of the underlying company. We will\nalso study the optimization problem when in addition the investment in a risky\nasset is allowed.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04924v2"
    },
    {
        "title": "Law invariant risk measures and information divergences",
        "authors": [
            "Daniel Lacker"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  A one-to-one correspondence is drawn between law invariant risk measures and\ndivergences, which we define as functionals of pairs of probability measures on\narbitrary standard Borel spaces satisfying a few natural properties.\nDivergences include many classical information divergence measures, such as\nrelative entropy and $f$-divergences. Several properties of divergence and\ntheir duality with law invariant risk measures are developed, most notably\nrelating their chain rules or additivity properties with certain notions of\ntime consistency for dynamic law invariant risk measures known as acceptance\nand rejection consistency. These properties are linked also to a peculiar\nproperty of the acceptance sets on the level of distributions, analogous to\nresults of Weber on weak acceptance and rejection consistency. Finally, the\nexamples of shortfall risk measures and optimized certainty equivalents are\ndiscussed in some detail, and it is shown that the relative entropy is\nessentially the only divergence satisfying the chain rule.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07030v2"
    },
    {
        "title": "Liquidity, risk measures, and concentration of measure",
        "authors": [
            "Daniel Lacker"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  Expanding on techniques of concentration of measure, we develop a\nquantitative framework for modeling liquidity risk using convex risk measures.\nThe fundamental objects of study are curves of the form $(\\rho(\\lambda\nX))_{\\lambda \\ge 0}$, where $\\rho$ is a convex risk measure and $X$ a random\nvariable, and we call such a curve a \\emph{liquidity risk profile}. The shape\nof a liquidity risk profile is intimately linked with the tail behavior of the\nunderlying $X$ for some notable classes of risk measures, namely shortfall risk\nmeasures. We exploit this link to systematically bound liquidity risk profiles\nfrom above by other real functions $\\gamma$, deriving tractable necessary and\nsufficient conditions for \\emph{concentration inequalities} of the form\n$\\rho(\\lambda X) \\le \\gamma(\\lambda)$, for all $\\lambda \\ge 0$. These\nconcentration inequalities admit useful dual representations related to\ntransport inequalities, and this leads to efficient uniform bounds for\nliquidity risk profiles for large classes of $X$. On the other hand, some\nmodest new mathematical results emerge from this analysis, including a new\ncharacterization of some classical transport-entropy inequalities. Lastly, the\nanalysis is deepened by means of a surprising connection between time\nconsistency properties of law invariant risk measures and the tensorization of\nconcentration inequalities.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07033v2"
    },
    {
        "title": "Which eligible assets are compatible with comonotonic capital\n  requirements?",
        "authors": [
            "Pablo Koch-Medina",
            "Cosimo Munari",
            "Gregor Svindland"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Within the context of capital adequacy, we study comonotonicity of risk\nmeasures in terms of the primitives of the theory: acceptance sets and\neligible, or reference, assets. We show that comonotonicity cannot be\ncharacterized by the properties of the acceptance set alone and heavily depends\non the choice of the eligible asset. In fact, in many important cases,\ncomonotonicity is only compatible with risk-free eligible assets. The\nincompatibility with risky eligible assets is systematic whenever the\nacceptability criterion is based on Value at Risk or any convex distortion risk\nmeasure such as Expected Shortfall. These findings qualify and arguably call\nfor a critical appraisal of the meaning and the role of comonotonicity within a\ncapital adequacy context.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05477v5"
    },
    {
        "title": "Value-at-Risk and backtesting with the APARCH model and the standardized\n  Pearson type IV distribution",
        "authors": [
            "Stavros Stavroyiannis"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We examine the efficiency of the Asymmetric Power ARCH (APARCH) model in the\ncase where the residuals follow the standardized Pearson type IV distribution.\nThe model is tested with a variety of loss functions and the efficiency is\nexamined via application of several statistical tests and risk measures. The\nresults indicate that the APARCH model with the standardized Pearson type IV\ndistribution is accurate, within the general financial risk modeling\nperspective, providing the financial analyst with an additional skewed\ndistribution for incorporation in the risk management tools.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05749v1"
    },
    {
        "title": "Backtesting Lambda Value at Risk",
        "authors": [
            "Jacopo Corbetta",
            "Ilaria Peri"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  A new risk measure, the lambda value at risk (Lambda VaR), has been recently\nproposed from a theoretical point of view as a generalization of the value at\nrisk (VaR). The Lambda VaR appears attractive for its potential ability to\nsolve several problems of the VaR. In this paper we propose three nonparametric\nbacktesting methodologies for the Lambda VaR which exploit different features.\nTwo of these tests directly assess the correctness of the level of coverage\npredicted by the model. One of these tests is bilateral and provides an\nasymptotic result. A third test assess the accuracy of the Lambda VaR that\ndepends on the choice of the P&L distribution. However, this test requires the\nstorage of more information. Finally, we perform a backtesting exercise and we\ncompare our results with the ones from Hitaj and Peri (2015)\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07599v4"
    },
    {
        "title": "Entangling credit and funding shocks in interbank markets",
        "authors": [
            "Giulio Cimini",
            "Matteo Serri"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Credit and liquidity risks represent main channels of financial contagion for\ninterbank lending markets. On one hand, banks face potential losses whenever\ntheir counterparties are under distress and thus unable to fulfill their\nobligations. On the other hand, solvency constraints may force banks to recover\nlost fundings by selling their illiquid assets, resulting in effective losses\nin the presence of fire sales - that is, when funding shortcomings are\nwidespread over the market. Because of the complex structure of the network of\ninterbank exposures, these losses reverberate among banks and eventually get\namplified, with potentially catastrophic consequences for the whole financial\nsystem. Building on Debt Rank [Battiston et al., 2012], in this work we define\na systemic risk metric that estimates the potential amplification of losses in\ninterbank markets accounting for both credit and liquidity contagion channels:\nthe Debt-Solvency Rank. We implement this framework on a dataset of 183\nEuropean banks that were publicly traded between 2004 and 2013, showing indeed\nthat liquidity spillovers substantially increase systemic risk, and thus cannot\nbe neglected in stress-test scenarios. We also provide additional evidence that\nthe interbank market was extremely fragile up to the 2008 financial crisis,\nbecoming slightly more robust only afterwards.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.06629v1"
    },
    {
        "title": "Network Valuation in Financial Systems",
        "authors": [
            "Paolo Barucca",
            "Marco Bardoscia",
            "Fabio Caccioli",
            "Marco D'Errico",
            "Gabriele Visentin",
            "Guido Caldarelli",
            "Stefano Battiston"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We introduce a general model for the balance-sheet consistent valuation of\ninterbank claims within an interconnected financial system. Our model\nrepresents an extension of clearing models of interdependent liabilities to\naccount for the presence of uncertainty on banks' external assets. At the same\ntime, it also provides a natural extension of classic structural credit risk\nmodels to the case of an interconnected system. We characterize the existence\nand uniqueness of a valuation that maximises individual and total equity values\nfor all banks. We apply our model to the assessment of systemic risk, and in\nparticular for the case of stress-testing. Further, we provide a fixed-point\nalgorithm to carry out the network valuation and the conditions for its\nconvergence.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.05164v3"
    },
    {
        "title": "Enhanced capital-asset pricing model for the reconstruction of bipartite\n  financial networks",
        "authors": [
            "Tiziano Squartini",
            "Assaf Almog",
            "Guido Caldarelli",
            "Iman van Lelyveld",
            "Diego Garlaschelli",
            "Giulio Cimini"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Reconstructing patterns of interconnections from partial information is one\nof the most important issues in the statistical physics of complex networks. A\nparamount example is provided by financial networks. In fact, the spreading and\namplification of financial distress in capital markets is strongly affected by\nthe interconnections among financial institutions. Yet, while the aggregate\nbalance sheets of institutions are publicly disclosed, information on single\npositions is mostly confidential and, as such, unavailable. Standard approaches\nto reconstruct the network of financial interconnection produce unrealistically\ndense topologies, leading to a biased estimation of systemic risk. Moreover,\nreconstruction techniques are generally designed for monopartite networks of\nbilateral exposures between financial institutions, thus failing in reproducing\nbipartite networks of security holdings (\\eg, investment portfolios). Here we\npropose a reconstruction method based on constrained entropy maximization,\ntailored for bipartite financial networks. Such a procedure enhances the\ntraditional {\\em capital-asset pricing model} (CAPM) and allows to reproduce\nthe correct topology of the network. We test this ECAPM method on a dataset,\ncollected by the European Central Bank, of detailed security holdings of\nEuropean institutional sectors over a period of six years (2009-2015). Our\napproach outperforms the traditional CAPM and the recently proposed MECAPM both\nin reproducing the network topology and in estimating systemic risk due to\nfire-sales spillovers. In general, ECAPM can be applied to the whole class of\nweighted bipartite networks described by the fitness model.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.07684v3"
    },
    {
        "title": "Estimation and prediction of credit risk based on rating transition\n  systems",
        "authors": [
            "Jinghai Shao",
            "Siming Li",
            "Yong Li"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Risk management is an important practice in the banking industry. In this\npaper we develop a new methodology to estimate and predict the probability of\ndefault (PD) based on the rating transition matrices, which relates the rating\ntransition matrices to the macroeconomic variables. Our method can overcome the\nshortcomings of the framework of Belkin et al. (1998), and is especially useful\nin predicting the PD and doing stress testing. Simulation is conducted at the\nend, which shows that our method can provide more accurate estimate than that\nobtained by the method of Belkin et al. (1998).\n",
        "pdf_link": "http://arxiv.org/pdf/1607.00448v2"
    },
    {
        "title": "Managing counterparty credit risk via BSDEs",
        "authors": [
            "Andrew Lesniewski",
            "Anja Richter"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We discuss a general dynamic replication approach to counterparty credit risk\nmodeling. This leads to a fundamental jump-process backward stochastic\ndifferential equation (BSDE) for the credit risk adjusted portfolio value. We\nthen reduce the fundamental BSDE to a continuous BSDE. Depending on the close\nout value convention, the reduced fundamental BSDE's solution can be\nrepresented explicitly or through an accurate approximate expression.\nFurthermore, we discuss practical aspects of the approach, important for the\nits industry applications: (i) efficient numerical methodology for solving a\nBSDE driven by a moderate number of Brownian motions, and (ii) factor reduction\nmethodology that allows one to approximately replace a portfolio driven by a\nlarge number of risk factors with a portfolio driven by a moderate number of\nrisk factors.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03237v2"
    },
    {
        "title": "Optimal importance sampling for Lévy Processes",
        "authors": [
            "Adrien Genin",
            "Peter Tankov"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We develop generic and efficient importance sampling estimators for Monte\nCarlo evaluation of prices of single- and multi-asset European and\npath-dependent options in asset price models driven by L\\'evy processes,\nextending earlier works which focused on the Black-Scholes and continuous\nstochastic volatility models. Using recent results from the theory of large\ndeviations on the path space for processes with independent increments, we\ncompute an explicit asymptotic approximation for the variance of the pay-off\nunder an Esscher-style change of measure. Minimizing this asymptotic variance\nusing convex duality, we then obtain an easy to compite asymptotically\nefficient importance sampling estimator of the option price. Numerical tests\nfor European baskets and for Asian options in the variance gamma model show\nconsistent variance reduction with a very small computational overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04621v1"
    },
    {
        "title": "Elicitability and backtesting: Perspectives for banking regulation",
        "authors": [
            "Natalia Nolde",
            "Johanna F. Ziegel"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Conditional forecasts of risk measures play an important role in internal\nrisk management of financial institutions as well as in regulatory capital\ncalculations. In order to assess forecasting performance of a risk measurement\nprocedure, risk measure forecasts are compared to the realized financial losses\nover a period of time and a statistical test of correctness of the procedure is\nconducted. This process is known as backtesting. Such traditional backtests are\nconcerned with assessing some optimality property of a set of risk measure\nestimates. However, they are not suited to compare different risk estimation\nprocedures. We investigate the proposal of comparative backtests, which are\nbetter suited for method comparisons on the basis of forecasting accuracy, but\nnecessitate an elicitable risk measure. We argue that supplementing traditional\nbacktests with comparative backtests will enhance the existing trading book\nregulatory framework for banks by providing the correct incentive for accuracy\nof risk measure forecasts. In addition, the comparative backtesting framework\ncould be used by banks internally as well as by researchers to guide selection\nof forecasting methods. The discussion focuses on three risk measures,\nValue-at-Risk, expected shortfall and expectiles, and is supported by a\nsimulation study and data analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.05498v2"
    },
    {
        "title": "Epidemics of Liquidity Shortages in Interbank Markets",
        "authors": [
            "Giuseppe Brandi",
            "Riccardo Di Clemente",
            "Giulio Cimini"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Financial contagion from liquidity shocks has being recently ascribed as a\nprominent driver of systemic risk in interbank lending markets. Building on\nstandard compartment models used in epidemics, in this work we develop an EDB\n(Exposed-Distressed-Bankrupted) model for the dynamics of liquidity shocks\nreverberation between banks, and validate it on electronic market for interbank\ndeposits data. We show that the interbank network was highly susceptible to\nliquidity contagion at the beginning of the 2007/2008 global financial crisis,\nand that the subsequent micro-prudential and liquidity hoarding policies\nadopted by banks increased the network resilience to systemic risk---yet with\nthe undesired side effect of drying out liquidity from the market. We finally\nshow that the individual riskiness of a bank is better captured by its network\ncentrality than by its participation to the market, along with the currently\ndebated concept of \"too interconnected to fail\".\n",
        "pdf_link": "http://arxiv.org/pdf/1610.03259v2"
    },
    {
        "title": "Model-free bounds on Value-at-Risk using extreme value information and\n  statistical distances",
        "authors": [
            "Thibaut Lux",
            "Antonis Papapantoleon"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We derive bounds on the distribution function, therefore also on the\nValue-at-Risk, of $\\varphi(\\mathbf X)$ where $\\varphi$ is an aggregation\nfunction and $\\mathbf X = (X_1,\\dots,X_d)$ is a random vector with known\nmarginal distributions and partially known dependence structure. More\nspecifically, we analyze three types of available information on the dependence\nstructure: First, we consider the case where extreme value information, such as\nthe distributions of partial minima and maxima of $\\mathbf X$, is available. In\norder to include this information in the computation of Value-at-Risk bounds,\nwe utilize a reduction principle that relates this problem to an optimization\nproblem over a standard Fr\\'echet class, which can then be solved by means of\nthe rearrangement algorithm or using analytical results. Second, we assume that\nthe copula of $\\mathbf X$ is known on a subset of its domain, and finally we\nconsider the case where the copula of $\\mathbf X$ lies in the vicinity of a\nreference copula as measured by a statistical distance. In order to derive\nValue-at-Risk bounds in the latter situations, we first improve the\nFr\\'echet--Hoeffding bounds on copulas so as to include this additional\ninformation on the dependence structure. Then, we translate the improved\nFr\\'echet--Hoeffding bounds to bounds on the Value-at-Risk using the so-called\nimproved standard bounds. In numerical examples we illustrate that the\nadditional information typically leads to a significant improvement of the\nbounds compared to the marginals-only case.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09734v4"
    },
    {
        "title": "Model Spaces for Risk Measures",
        "authors": [
            "Felix-Benedikt Liebrich",
            "Gregor Svindland"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We show how risk measures originally defined in a model free framework in\nterms of acceptance sets and reference assets imply a meaningful underlying\nprobability structure. Hereafter we construct a maximal domain of definition of\nthe risk measure respecting the underlying ambiguity profile. We particularly\nemphasise liquidity effects and discuss the correspondence between properties\nof the risk measure and the structure of this domain as well as\nsubdifferentiability properties.\n  Keywords: Model free risk assessment, extension of risk measures, continuity\nproperties of risk measures, subgradients.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.01137v3"
    },
    {
        "title": "Asymptotic multivariate expectiles",
        "authors": [
            "Véronique Maume-Deschamps",
            "Didier Rullière",
            "Khalil Said"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  In [16], a new family of vector-valued risk measures called multivariate\nexpectiles is introduced. In this paper, we focus on the asymptotic behavior of\nthese measures in a multivariate regular variations context. For models with\nequivalent tails, we propose an estimator of these multivariate asymptotic\nexpectiles, in the Fr{\\'e}chet attraction domain case, with asymptotic\nindependence, or in the comonotonic case.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.07152v2"
    },
    {
        "title": "Murphy Diagrams: Forecast Evaluation of Expected Shortfall",
        "authors": [
            "Johanna F. Ziegel",
            "Fabian Krüger",
            "Alexander Jordan",
            "Fernando Fasciati"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Motivated by the Basel 3 regulations, recent studies have considered joint\nforecasts of Value-at-Risk and Expected Shortfall. A large family of scoring\nfunctions can be used to evaluate forecast performance in this context.\nHowever, little intuitive or empirical guidance is currently available, which\nrenders the choice of scoring function awkward in practice. We therefore\ndevelop graphical checks (Murphy diagrams) of whether one forecast method\ndominates another under a relevant class of scoring functions, and propose an\nassociated hypothesis test. We illustrate these tools with simulation examples\nand an empirical analysis of S&P 500 and DAX returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.04537v1"
    },
    {
        "title": "Equivalence Between Time Consistency and Nested Formula",
        "authors": [
            "Henri Gérard",
            "Michel de Lara",
            "Jean-Philippe Chancelier"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  You are a financial analyst. At the beginning of every week, you are able to\nrank every pair of stochastic processes starting from that week up to the\nhorizon. Suppose that two processes are equal at the beginning of the week.\nYour ranking procedure is time consistent if the ranking does not change\nbetween this week and the next one. In this paper, we propose a minimalist\ndefinition of Time Consistency (TC) between two (assessment) mappings. With\nvery few assumptions, we are able to prove an equivalence between Time\nConsistency and a Nested Formula (NF) between the two mappings. Thus, in a\nsense, two assessments are consistent if and only if one is factored into the\nother. We review the literature and observe that the various definitions of TC\n(or of NF) are special cases of ours, as they always include additional\nassumptions. By stripping off these additional assumptions, we present an\noverview of the literature where the contribution of each author is\nenlightened.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.08633v2"
    },
    {
        "title": "Optimal Risk Allocation in Reinsurance Networks",
        "authors": [
            "Nicole Bäuerle",
            "Alexander Glauner"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  In this paper we consider reinsurance or risk sharing from a macroeconomic\npoint of view. Our aim is to find socially optimal reinsurance treaties. In our\nsetting we assume that there are $n$ insurance companies each bearing a certain\nrisk and one representative reinsurer. The optimization problem is to minimize\nthe sum of all capital requirements of the insurers where we assume that all\ninsurance companies use a form of Range-Value-at-Risk. We show that in case all\ninsurers use Value-at-Risk and the reinsurer's premium principle satisfies\nmonotonicity, then layer reinsurance treaties are socially optimal. For this\nresult we do not need any dependence structure between the risks. In the\ngeneral setting with Range-Value-at-Risk we obtain again the optimality of\nlayer reinsurance treaties under further assumptions, in particular under the\nassumption that the individual risks are positively dependent through the\nstochastic ordering. At the end, we discuss the difference between socially\noptimal reinsurance treaties and individually optimal ones by looking at a\nnumber of special cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.10210v1"
    },
    {
        "title": "Evaluating the role of risk networks on risk identification,\n  classification and emergence",
        "authors": [
            "Christos Ellinas",
            "Neil Allan",
            "Caroline Coombe"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Modern society heavily relies on strongly connected, socio-technical systems.\nAs a result, distinct risks threatening the operation of individual systems can\nno longer be treated in isolation. Consequently, risk experts are actively\nseeking for ways to relax the risk independence assumption that undermines\ntypical risk management models. Prominent work has advocated the use of risk\nnetworks as a way forward. Yet, the inevitable biases introduced during the\ngeneration of these survey-based risk networks limit our ability to examine\ntheir topology, and in turn challenge the utility of the very notion of a risk\nnetwork. To alleviate these concerns, we proposed an alternative methodology\nfor generating weighted risk networks. We subsequently applied this methodology\nto an empirical dataset of financial data. This paper reports our findings on\nthe study of the topology of the resulting risk network. We observed a modular\ntopology, and reasoned on its use as a robust risk classification framework.\nUsing these modules, we highlight a tendency of specialization during the risk\nidentification process, with some firms being solely focused on a subset of the\navailable risk classes. Finally, we considered the independent and systemic\nimpact of some risks and attributed possible mismatches to their emerging\nnature.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05759v1"
    },
    {
        "title": "Gaussian Approximation of a Risk Model with Non-Stationary Hawkes\n  Arrivals of Claims",
        "authors": [
            "Zailei Cheng",
            "Youngsoo Seol"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We consider a classical risk process with arrival of claims following a\nnon-stationary Hawkes process. We study the asymptotic regime when the premium\nrate and the baseline intensity of the claims arrival process are large, and\nclaim size is small. The main goal of the article is to establish a diffusion\napproximation by verifying a functional central limit theorem and to compute\nthe ruin probability in finite-time horizon. Numerical results will also be\ngiven.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.07595v2"
    },
    {
        "title": "Identifying systemically important companies in the entire liability\n  network of a small open economy",
        "authors": [
            "Sebastian Poledna",
            "Abraham Hinteregger",
            "Stefan Thurner"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  To a large extent, the systemic importance of financial institutions is\nrelated to the topology of financial liability networks. In this work we\nreconstruct and analyze the - to our knowledge - largest financial network that\nhas been studied up to now. This financial liability network consists of 51,980\nfirms and 796 banks. It represents 80.2% of total liabilities towards banks by\nfirms and all interbank liabilities from the entire Austrian banking system. We\nfind that firms contribute to systemic risk in similar ways as banks do. In\nparticular, we identify several medium-sized banks and firms with total assets\nbelow 1 bln. EUR that are systemically important in the entire financial\nnetwork. We show that the notion of systemically important financial\ninstitutions (SIFIs) or global and domestic systemically important banks\n(G-SIBs or D-SIBs) can be straightforwardly extended to firms. We find that\nfirms introduce slightly more systemic risk than banks. In Austria in 2008, the\ntotal systemic risk of the interbank network amounts to only 29% of the total\nsystemic risk of the entire financial network, consisting of firms and banks.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.10487v1"
    },
    {
        "title": "Structural changes in the interbank market across the financial crisis\n  from multiple core-periphery analysis",
        "authors": [
            "Sadamori Kojaku",
            "Giulio Cimini",
            "Guido Caldarelli",
            "Naoki Masuda"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Interbank markets are often characterised in terms of a core-periphery\nnetwork structure, with a highly interconnected core of banks holding the\nmarket together, and a periphery of banks connected mostly to the core but not\ninternally. This paradigm has recently been challenged for short time scales,\nwhere interbank markets seem better characterised by a bipartite structure with\nmore core-periphery connections than inside the core. Using a novel\ncore-periphery detection method on the eMID interbank market, we enrich this\npicture by showing that the network is actually characterised by multiple\ncore-periphery pairs. Moreover, a transition from core-periphery to bipartite\nstructures occurs by shortening the temporal scale of data aggregation. We\nfurther show how the global financial crisis transformed the market, in terms\nof composition, multiplicity and internal organisation of core-periphery pairs.\nBy unveiling such a fine-grained organisation and transformation of the\ninterbank market, our method can find important applications in the\nunderstanding of how distress can propagate over financial networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.05139v1"
    },
    {
        "title": "The Italian Pension Gap: a Stochastic Optimal Control Approach",
        "authors": [
            "Alessandro Milazzo",
            "Elena Vigna"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We study the gap between the state pension provided by the Italian pension\nsystem pre-Dini reform and post-Dini reform. The goal is to fill the gap\nbetween the old and the new pension by joining a defined contribution pension\nscheme and adopting an optimal investment strategy that is target-based. We\nfind that it is possible to cover, at least partially, this gap with the\nadditional income of the pension scheme, especially in the presence of late\nretirement and in the presence of stagnant career. Workers with dynamic career\nand workers who retire early are those who are most penalised by the reform.\nResults are intuitive and in line with previous studies on the subject.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.05354v1"
    },
    {
        "title": "Evolution of the Chinese Guarantee Network under Financial Crisis and\n  Stimulus Program",
        "authors": [
            "Yingli Wang",
            "Qingpeng Zhang",
            "Xiaoguang Yang"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Our knowledge about the evolution of guarantee network in downturn period is\nlimited due to the lack of comprehensive data of the whole credit system. Here\nwe analyze the dynamic Chinese guarantee network constructed from a\ncomprehensive bank loan dataset that accounts for nearly 80% total loans in\nChina, during 01/2007-03/2012. The results show that, first, during the\n2007-2008 global financial crisis, the guarantee network became smaller, less\nconnected and more stable because of many bankruptcies; second, the stimulus\nprogram encouraged mutual guarantee behaviors, resulting in highly reciprocal\nand fragile network structure; third, the following monetary policy adjustment\nenhanced the resilience of the guarantee network by reducing mutual guarantees.\nInterestingly, our work reveals that the financial crisis made the network more\nresilient, and conversely, the government bailout degenerated network\nresilience. These counterintuitive findings can provide new insight into the\nresilience of real-world credit system under external shocks or rescues.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.05667v5"
    },
    {
        "title": "Network Sensitivity of Systemic Risk",
        "authors": [
            "Amanah Ramadiah",
            "Domenico Di Gangi",
            "D. Ruggiero Lo Sardo",
            "Valentina Macchiati",
            "Tuan Pham Minh",
            "Francesco Pinotti",
            "Mateusz Wilinski",
            "Paolo Barucca",
            "Giulio Cimini"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  A growing body of studies on systemic risk in financial markets has\nemphasized the key importance of taking into consideration the complex\ninterconnections among financial institutions. Much effort has been put in\nmodeling the contagion dynamics of financial shocks, and to assess the\nresilience of specific financial markets - either using real network data,\nreconstruction techniques or simple toy networks. Here we address the more\ngeneral problem of how shock propagation dynamics depends on the topological\ndetails of the underlying network. To this end we consider different realistic\nnetwork topologies, all consistent with balance sheets information obtained\nfrom real data on financial institutions. In particular, we consider networks\nof varying density and with different block structures, and diversify as well\nin the details of the shock propagation dynamics. We confirm that the systemic\nrisk properties of a financial network are extremely sensitive to its network\nfeatures. Our results can aid in the design of regulatory policies to improve\nthe robustness of financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04325v2"
    },
    {
        "title": "Concentration of dynamic risk measures in a Brownian filtration",
        "authors": [
            "Ludovic Tangpi"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Motivated by liquidity risk in mathematical finance, D. Lacker introduced\nconcentration inequalities for risk measures, i.e. upper bounds on the\n\\emph{liquidity risk profile} of a financial loss. We derive these inequalities\nin the case of time-consistent dynamic risk measures when the filtration is\nassumed to carry a Brownian motion. The theory of backward stochastic\ndifferential equations (BSDEs) and their dual formulation plays a crucial role\nin our analysis. Natural by-products of concentration of risk measures are a\ndescription of the tail behavior of the financial loss and transport-type\ninequalities in terms of the generator of the BSDE, which in the present case\ncan grow arbitrarily fast.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09014v1"
    },
    {
        "title": "A Markov Chain Model for the Cure Rate of Non-Performing Loans",
        "authors": [
            "Vilislav Boutchaktchiev"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  A Markov-chain model is developed for the purpose estimation of the cure rate\nof non-performing loans. The technique is performed collectively, on portfolios\nand it can be applicable in the process of calculation of credit impairment. It\nis efficient in terms of data manipulation costs which makes it accessible even\nto smaller financial institutions. In addition, several other applications to\nportfolio optimization are suggested.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11804v2"
    },
    {
        "title": "On closedness of law-invariant convex sets in rearrangement invariant\n  spaces",
        "authors": [
            "Made Tantrawan",
            "Denny H. Leung"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  This paper presents relations between several types of closedness of a\nlaw-invariant convex set in a rearrangement invariant space $\\mathcal{X}$. In\nparticular, we show that order closedness,\n$\\sigma(\\mathcal{X},\\mathcal{X}_n^\\sim)$-closedness and\n$\\sigma(\\mathcal{X},L^\\infty)$-closedness of a law-invariant convex set in\n$\\mathcal{X}$ are equivalent, where $\\mathcal{X}_n^\\sim$ is the order\ncontinuous dual of $\\mathcal{X}$. We also provide some application to proper\nquasiconvex law-invariant functionals with the Fatou property.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.10374v2"
    },
    {
        "title": "Ordering the smallest claim amounts from two sets of interdependent\n  heterogeneous portfolios",
        "authors": [
            "Hossein Nadeb",
            "Hamzeh Torabi",
            "Ali Dolati"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Let $ X_{\\lambda_1},\\ldots,X_{\\lambda_n}$ be a set of dependent and\nnon-negative random variables share a survival copula and let $Y_i=\nI_{p_i}X_{\\lambda_i}$, $i=1,\\ldots,n$, where $I_{p_1},\\ldots,I_{p_n}$ be\nindependent Bernoulli random variables independent of $X_{\\lambda_i}$'s, with\n${\\rm E}[I_{p_i}]=p_i$, $i=1,\\ldots,n$. In actuarial sciences, $Y_i$\ncorresponds to the claim amount in a portfolio of risks. This paper considers\ncomparing the smallest claim amounts from two sets of interdependent\nportfolios, in the sense of usual and likelihood ratio orders, when the\nvariables in one set have the parameters $\\lambda_1,\\ldots,\\lambda_n$ and\n$p_1,\\ldots,p_n$ and the variables in the other set have the parameters\n$\\lambda^{*}_1,\\ldots,\\lambda^{*}_n$ and $p^*_1,\\ldots,p^*_n$. Also, we present\nsome bounds for survival function of the smallest claim amount in a portfolio.\nTo illustrate validity of the results, we serve some applicable models.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06166v1"
    },
    {
        "title": "An optimization approach to adaptive multi-dimensional capital\n  management",
        "authors": [
            "G. A. Delsing",
            "M. R. H. Mandjes",
            "P. J. C. Spreij",
            "E. M. M. Winands"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Firms should keep capital to offer sufficient protection against the risks\nthey are facing. In the insurance context methods have been developed to\ndetermine the minimum capital level required, but less so in the context of\nfirms with multiple business lines including allocation. The individual capital\nreserve of each line can be represented by means of classical models, such as\nthe conventional Cram\\'{e}r-Lundberg model, but the challenge lies in soundly\nmodelling the correlations between the business lines. We propose a simple yet\nversatile approach that allows for dependence by introducing a common\nenvironmental factor. We present a novel Bayesian approach to calibrate the\nlatent environmental state distribution based on observations concerning the\nclaim processes. The calibration approach is adjusted for an environmental\nfactor that changes over time. The convergence of the calibration procedure\ntowards the true environmental state is deduced. We then point out how to\ndetermine the optimal initial capital of the different business lines under\nspecific constraints on the ruin probability of subsets of business lines. Upon\ncombining the above findings, we have developed an easy-to-implement approach\nto capital risk management in a multi-dimensional insurance risk model.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.08435v1"
    },
    {
        "title": "On occupation times in the red of Lévy risk models",
        "authors": [
            "David Landriault",
            "Bin Li",
            "Mohamed Amine Lkabous"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  In this paper, we obtain analytical expression for the distribution of the\noccupation time in the red (below level $0$) up to an (independent) exponential\nhorizon for spectrally negative L\\'{e}vy risk processes and refracted\nspectrally negative L\\'{e}vy risk processes. This result improves the existing\nliterature in which only the Laplace transforms are known. Due to the close\nconnection between occupation time and many other quantities, we provide a few\napplications of our results including future drawdown, inverse occupation time,\nParisian ruin with exponential delay, and the last time at running maximum. By\na further Laplace inversion to our results, we obtain the distribution of the\noccupation time up to a finite time horizon for refracted Brownian motion risk\nprocess and refracted Cram\\'{e}r-Lundberg risk model with exponential claims.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.03721v2"
    },
    {
        "title": "A Machine Learning approach to Risk Minimisation in Electricity Markets\n  with Coregionalized Sparse Gaussian Processes",
        "authors": [
            "Daniel Poh",
            "Stephen Roberts",
            "Martin Tegnér"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  The non-storability of electricity makes it unique among commodity assets,\nand it is an important driver of its price behaviour in secondary financial\nmarkets. The instantaneous and continuous matching of power supply with demand\nis a key factor explaining its volatility. During periods of high demand,\ncostlier generation capabilities are utilised since electricity cannot be\nstored and this has the impact of driving prices up very quickly. Furthermore,\nthe non-storability also complicates physical hedging. Owing to these, the\nproblem of joint price-quantity risk in electricity markets is a commonly\nstudied theme.\n  We propose using Gaussian Processes (GPs) to tackle this problem since GPs\nprovide a versatile and elegant non-parametric approach for regression and\ntime-series modelling. However, GPs scale poorly with the amount of training\ndata due to a cubic complexity. These considerations suggest that knowledge\ntransfer between price and load is vital for effective hedging, and that a\ncomputationally efficient method is required. To this end, we use the\ncoregionalized (or multi-task) sparse GPs which addresses the aforementioned\nissues.\n  To gauge the performance of our model, we use an average-load strategy as\ncomparator. The latter is a robust approach commonly used by industry. If the\nspot and load are uncorrelated and Gaussian, then hedging with the expected\nload will result in the minimum variance position.\n  Our main contributions are twofold. Firstly, in developing a coregionalized\nsparse GP-based approach for hedging. Secondly, in demonstrating that our\nmodel-based strategy outperforms the comparator, and can thus be employed for\neffective hedging in electricity markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09536v2"
    },
    {
        "title": "Quantum Computation for Pricing the Collateralized Debt Obligations",
        "authors": [
            "Hao Tang",
            "Anurag Pal",
            "Lu-Feng Qiao",
            "Tian-Yu Wang",
            "Jun Gao",
            "Xian-Min Jin"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Collateralized debt obligation (CDO) has been one of the most commonly used\nstructured financial products and is intensively studied in quantitative\nfinance. By setting the asset pool into different tranches, it effectively\nworks out and redistributes credit risks and returns to meet the risk\npreferences for different tranche investors. The copula models of various kinds\nare normally used for pricing CDOs, and the Monte Carlo simulations are\nrequired to get their numerical solution. Here we implement two typical CDO\nmodels, the single-factor Gaussian copula model and Normal Inverse Gaussian\ncopula model, and by applying the conditional independence approach, we manage\nto load each model of distribution in quantum circuits. We then apply quantum\namplitude estimation as an alternative to Monte Carlo simulation for CDO\npricing. We demonstrate the quantum computation results using IBM Qiskit. Our\nwork addresses a useful task in finance instrument pricing, significantly\nbroadening the application scope for quantum computing in finance.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.04110v2"
    },
    {
        "title": "SynthETIC: an individual insurance claim simulator with feature control",
        "authors": [
            "Benjamin Avanzi",
            "Gregory Clive Taylor",
            "Melantha Wang",
            "Bernard Wong"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Recent years have seen rapid increase in the application of machine learning\nto insurance loss reserving. They yield most value when applied to large data\nsets, such as individual claims, or large claim triangles. In short, they are\nlikely to be useful in the analysis of any data set whose volume is sufficient\nto obscure a naked-eye view of its features. Unfortunately, such large data\nsets are in short supply in the actuarial literature. Accordingly, one needs to\nturn to synthetic data. Although the ultimate objective of these methods is\napplication to real data, the use of synthetic data containing features\ncommonly observed in real data is also to be encouraged.\n  While there are a number of claims simulators in existence, each valuable\nwithin its own context, the inclusion of a number of desirable (but\ncomplicated) data features requires further development. Accordingly, in this\npaper we review those desirable features, and propose a new simulator of\nindividual claim experience called `SynthETIC`.\n  Our simulator is publicly available, open source, and fills a gap in the\nnon-life actuarial toolkit. The simulator specifically allows for desirable\n(but optionally complicated) data features typically occurring in practice,\nsuch as variations in rates of settlements and development patterns; as with\nsuperimposed inflation, and various discontinuities, and also enables various\ndependencies between variables. The user has full control of the mechanics of\nthe evolution of an individual claim. As a result, the complexity of the data\nset generated (meaning the level of difficulty of analysis) may be dialled\nanywhere from extremely simple to extremely complex.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.05693v4"
    },
    {
        "title": "Preference Robust Optimization with Quasi-Concave Choice Functions in\n  Multi-Attribute Decision-Making: Characterization and Computation",
        "authors": [
            "Jian Wu",
            "William B. Haskell",
            "Wenjie Huang",
            "Huifu Xu"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  In behavioural economics, a decision maker's (DM's) preferences are often\nexpressed by a preference functional such as expected utility or a distortion\nrisk measure, which assigns a numerical value to a risky prospect. Preference\nrobust optimization (PRO) is about decision making where the DM's preference\nfunctional is ambiguous and the optimal decision is based on the worst-case\npreference functional from a set of plausible ones constructed from available\npartial information about the DM's true preferences. In this paper, we propose\na choice function (a particular class of preference functionals) based PRO\nmodel where the DM's preferences over a prospect space satisfy Von\nNeumann-Morgenstern's (VNM's) axioms of completeness, monotonicity, and\ncontinuity. We concentrate on the class of choice functions which are\nmonotonic, quasi-concave, and multi-attribute. The resulting PRO model is\nbroader than the existing expected utility-based PRO models in that: (a) it\ncaptures a broader class of DM's preferences; and (b) it can be effectively\napplied to multi-attribute decision making problems where the DM's preferences\nover different attributes are related in a nonlinear manner. We propose a\ncutting plane-type method for evaluating the worst-case choice function and\nsolve the resulting PRO problem by solving a sequence of convex optimization\nproblems. We examine the behavior and scalability of the proposed model and\ncomputational schemes numerically on a multi-portfolio optimization problem and\na capital allocation problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.13309v5"
    },
    {
        "title": "Default contagion risks in Russian interbank market",
        "authors": [
            "A. V. Leonidov",
            "E. L. Rumyantsev"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  Systemic risks of default contagion in the Russian interbank market are\ninvestigated. The analysis is based on considering the bow-tie structure of the\nweighted oriented graph describing the structure of the interbank loans. A\nprobabilistic model of interbank contagion explicitly taking into account the\nempirical bow-tie structure reflecting functionality of the corresponding nodes\n(borrowers, lenders, borrowers and lenders simultaneously), degree\ndistributions and disassortativity of the interbank network under consideration\nbased on empirical data is developed. The characteristics of contagion-related\nsystemic risk calculated with this model are shown to be in agreement with\nthose of explicit stress tests.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.1071v3"
    },
    {
        "title": "Bonus--malus systems with different claim types and varying deductibles",
        "authors": [
            "Olena Ragulina"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  The paper deals with bonus-malus systems with different claim types and\nvarying deductibles. The premium relativities are softened for the\npolicyholders who are in the malus zone and these policyholders are subject to\nper claim deductibles depending on their levels in the bonus-malus scale and\nthe types of the reported claims. We introduce such bonus-malus systems and\nstudy their basic properties. In particular, we investigate when it is possible\nto introduce varying deductibles, what restrictions we have and how we can do\nthis. Moreover, we deal with the special case where varying deductibles are\napplied to the claims reported by policyholders occupying the highest level in\nthe bonus-malus scale and consider two allocation principles for the\ndeductibles. Finally, numerical illustrations are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.00917v1"
    },
    {
        "title": "On the statistics of scaling exponents and the Multiscaling Value at\n  Risk",
        "authors": [
            "Giuseppe Brandi",
            "T. Di Matteo"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Scaling and multiscaling financial time series have been widely studied in\nthe literature. The research on this topic is vast and still flourishing. One\nway to analyze the scaling properties of time series is through the estimation\nof their scaling exponents, that are recognized as being valuable measures to\ndiscriminate between random, persistent, and anti-persistent behaviors in these\ntime series. In the literature, several methods have been proposed to study the\nmultiscaling property. In this paper, we use the generalized Hurst exponent\n(GHE) tool and we propose a novel statistical procedure based on GHE which we\nname Relative Normalized and Standardized Generalized Hurst Exponent (RNSGHE).\nThis method is used to robustly estimate and test the multiscaling property\nand, together with a combination of t-tests and F-tests, serves to discriminate\nbetween real and spurious scaling. Furthermore, we introduce a new tool to\nestimate the optimal aggregation time used in our methodology which we name\nAutocororrelation Segmented Regression. We numerically validate this procedure\non simulated time series by using the Multifractal Random Walk (MRW) and we\nthen apply it to real financial data. We present results for times series with\nand without anomalies and we compute the bias that such anomalies introduce in\nthe measurement of the scaling exponents. We also show how the use of proper\nscaling and multiscaling can ameliorate the estimation of risk measures such as\nValue at Risk (VaR). Finally, we propose a methodology based on Monte Carlo\nsimulation, which we name Multiscaling Value at Risk (MSVaR), that takes into\naccount the statistical properties of multiscaling time series. We show that by\nusing this statistical procedure in combination with the robustly estimated\nmultiscaling exponents, the one year forecasted MSVaR mimics the VaR on the\nannual data for the majority of the stocks analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04164v3"
    },
    {
        "title": "Network-Aware Strategies in Financial Systems",
        "authors": [
            "Pál András Papp",
            "Roger Wattenhofer"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We study the incentives of banks in a financial network, where the network\nconsists of debt contracts and credit default swaps (CDSs) between banks. One\nof the most important questions in such a system is the problem of deciding\nwhich of the banks are in default, and how much of their liabilities these\nbanks can pay. We study the payoff and preferences of the banks in the\ndifferent solutions to this problem. We also introduce a more refined model\nwhich allows assigning priorities to payment obligations; this provides a more\nexpressive and realistic model of real-life financial systems, while it always\nensures the existence of a solution.\n  The main focus of the paper is an analysis of the actions that a single bank\ncan execute in a financial system in order to influence the outcome to its\nadvantage. We show that removing an incoming debt, or donating funds to another\nbank can result in a single new solution that is strictly more favorable to the\nacting bank. We also show that increasing the bank's external funds or\nmodifying the priorities of outgoing payments cannot introduce a more favorable\nnew solution into the system, but may allow the bank to remove some unfavorable\nsolutions, or to increase its recovery rate. Finally, we show how the actions\nof two banks in a simple financial system can result in classical game\ntheoretic situations like the prisoner's dilemma or the dollar auction,\ndemonstrating the wide expressive capability of the financial system model.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07566v1"
    },
    {
        "title": "Modelling volatile time series with v-transforms and copulas",
        "authors": [
            "Alexander J. McNeil"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  An approach to the modelling of volatile time series using a class of\nuniformity-preserving transforms for uniform random variables is proposed.\nV-transforms describe the relationship between quantiles of the stationary\ndistribution of the time series and quantiles of the distribution of a\npredictable volatility proxy variable. They can be represented as copulas and\npermit the formulation and estimation of models that combine arbitrary marginal\ndistributions with copula processes for the dynamics of the volatility proxy.\nThe idea is illustrated using a Gaussian ARMA copula process and the resulting\nmodel is shown to replicate many of the stylized facts of financial return\nseries and to facilitate the calculation of marginal and conditional\ncharacteristics of the model including quantile measures of risk. Estimation is\ncarried out by adapting the exact maximum likelihood approach to the estimation\nof ARMA processes and the model is shown to be competitive with standard GARCH\nin an empirical application to Bitcoin return data.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.10135v5"
    },
    {
        "title": "Firms Default Prediction with Machine Learning",
        "authors": [
            "Tesi Aliaj",
            "Aris Anagnostopoulos",
            "Stefano Piersanti"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Academics and practitioners have studied over the years models for predicting\nfirms bankruptcy, using statistical and machine-learning approaches. An earlier\nsign that a company has financial difficulties and may eventually bankrupt is\ngoing in \\emph{default}, which, loosely speaking means that the company has\nbeen having difficulties in repaying its loans towards the banking system.\nFirms default status is not technically a failure but is very relevant for bank\nlending policies and often anticipates the failure of the company. Our study\nuses, for the first time according to our knowledge, a very large database of\ngranular credit data from the Italian Central Credit Register of Bank of Italy\nthat contain information on all Italian companies' past behavior towards the\nentire Italian banking system to predict their default using machine-learning\ntechniques. Furthermore, we combine these data with other information regarding\ncompanies' public balance sheet data. We find that ensemble techniques and\nrandom forest provide the best results, corroborating the findings of Barboza\net al. (Expert Syst. Appl., 2017).\n",
        "pdf_link": "http://arxiv.org/pdf/2002.11705v1"
    },
    {
        "title": "Distress propagation in complex networks: the case of non-linear\n  DebtRank",
        "authors": [
            "Marco Bardoscia",
            "Fabio Caccioli",
            "Juan Ignacio Perotti",
            "Gianna Vivaldo",
            "Guido Caldarelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  We consider a dynamical model of distress propagation on complex networks,\nwhich we apply to the study of financial contagion in networks of banks\nconnected to each other by direct exposures. The model that we consider is an\nextension of the DebtRank algorithm, recently introduced in the literature. The\nmechanics of distress propagation is very simple: When a bank suffers a loss,\ndistress propagates to its creditors, who in turn suffer losses, and so on. The\noriginal DebtRank assumes that losses are propagated linearly between connected\nbanks. Here we relax this assumption and introduce a one-parameter family of\nnon-linear propagation functions. As a case study, we apply this algorithm to a\ndata-set of 183 European banks, and we study how the stability of the system\ndepends on the non-linearity parameter under different stress-test scenarios.\nWe find that the system is characterized by a transition between a regime where\nsmall shocks can be amplified and a regime where shocks do not propagate, and\nthat the overall stability of the system increases between 2008 and 2013.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.04460v2"
    },
    {
        "title": "Worst-Case Expected Shortfall with Univariate and Bivariate Marginals",
        "authors": [
            "Anulekha Dhara",
            "Bikramjit Das",
            "Karthik Natarajan"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Worst-case bounds on the expected shortfall risk given only limited\ninformation on the distribution of the random variables has been studied\nextensively in the literature. In this paper, we develop a new worst-case bound\non the expected shortfall when the univariate marginals are known exactly and\nadditional expert information is available in terms of bivariate marginals.\nSuch expert information allows for one to choose from among the many possible\nparametric families of bivariate copulas. By considering a neighborhood of\ndistance $\\rho$ around the bivariate marginals with the Kullback-Leibler\ndivergence measure, we model the trade-off between conservatism in the\nworst-case risk measure and confidence in the expert information. Our bound is\ndeveloped when the only information available on the bivariate marginals forms\na tree structure in which case it is efficiently computable using convex\noptimization. For consistent marginals, as $\\rho$ approaches $\\infty$, the\nbound reduces to the comonotonic upper bound and as $\\rho$ approaches $0$, the\nbound reduces to the worst-case bound with bivariates known exactly. We also\ndiscuss extensions to inconsistent marginals and instances where the expert\ninformation which might be captured using other parameters such as\ncorrelations.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.04167v1"
    },
    {
        "title": "Contagion in financial systems: A Bayesian network approach",
        "authors": [
            "Carsten Chong",
            "Claudia Klüppelberg"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We develop a structural default model for interconnected financial\ninstitutions in a probabilistic framework. For all possible network structures\nwe characterize the joint default distribution of the system using Bayesian\nnetwork methodologies. Particular emphasis is given to the treatment and\nconsequences of cyclic financial linkages. We further demonstrate how Bayesian\nnetwork theory can be applied to detect contagion channels within the financial\nnetwork, to measure the systemic importance of selected entities on others, and\nto compute conditional or unconditional probabilities of default for single or\nmultiple institutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.04287v2"
    },
    {
        "title": "Intervention On Default Contagion Under Partial Information",
        "authors": [
            "Yang Xu"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We model the default contagion process in a large heterogeneous financial\nnetwork under the interventions of a regulator (a central bank) with only\npartial information which is a more realistic setting than most current\nliterature. We provide the analytical results for the asymptotic optimal\nintervention policies and the asymptotic magnitude of default contagion in\nterms of the network characteristics. We extend the results of Amini et al.\n(2013) to incorporate interventions and the model of Amini et al. (2015); Amini\net al. (2017) to heterogeneous networks with a given degree sequence and\narbitrary initial equity levels. The insights from the results are that the\noptimal intervention policy is \"monotonic\" in terms of the intervention cost,\nthe closeness to invulnerability and connectivity. Moreover, we should keep\nintervening on a bank once we have intervened on it. Our simulation results\nshow a good agreement with the theoretical results.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.02127v1"
    },
    {
        "title": "Research on ruin probability of risk model based on AR(1) series",
        "authors": [
            "Wenhao Li",
            "Bolong Wang",
            "Tianxiang Shen",
            "Ronghua Zhu",
            "Dehui Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  In this text, we establish the risk model based on AR(1) series and propose\nthe basic model which has a dependent structure under intensity of claim\nnumber. Considering some properties of the risk model, we take advantage of\nnewton iteration method to figure out the adjustment coefficient and estimate\nthe exponential upper bound of ruin probability. This is significant to refine\nthe research of ruin theory. As a result, our theory will help develop\ninsurance industry stably.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.10692v1"
    },
    {
        "title": "On Fair Reinsurance Premiums; Capital Injections in a Perturbed Risk\n  Model",
        "authors": [
            "Zied Ben Salah",
            "José Garrido"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  We consider a risk model where deficits after ruin are covered by a new type\nof reinsurance contract that provides capital injections. To allow the\ninsurance company's survival after ruin, the reinsurer injects capital only at\nruin times caused by jumps larger than a chosen retention level. Otherwise\ncapital must be raised from the shareholders for small deficits. The problem\nhere is to determine adequate reinsurance premiums. It seems fair to base the\nnet reinsurance premium on the discounted expected value of any future capital\ninjections. Inspired by the results of Huzak et al. (2004) and Ben Salah (2014)\non successive ruin events, we show that an explicit formula for these\nreinsurance premiums exists in a setting where aggregate claims are modeled by\na subordinator and a Brownian perturbation. Here ruin events are due either to\nBrownian oscillations or jumps and reinsurance capital injections only apply in\nthe latter case. The results are illustrated explicitly for two specific risk\nmodels and in some numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.11065v4"
    },
    {
        "title": "Neural Learning of Online Consumer Credit Risk",
        "authors": [
            "Di Wang",
            "Qi Wu",
            "Wen Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  This paper takes a deep learning approach to understand consumer credit risk\nwhen e-commerce platforms issue unsecured credit to finance customers'\npurchase. The \"NeuCredit\" model can capture both serial dependences in\nmulti-dimensional time series data when event frequencies in each dimension\ndiffer. It also captures nonlinear cross-sectional interactions among different\ntime-evolving features. Also, the predicted default probability is designed to\nbe interpretable such that risks can be decomposed into three components: the\nsubjective risk indicating the consumers' willingness to repay, the objective\nrisk indicating their ability to repay, and the behavioral risk indicating\nconsumers' behavioral differences. Using a unique dataset from one of the\nlargest global e-commerce platforms, we show that the inclusion of shopping\nbehavioral data, besides conventional payment records, requires a deep learning\napproach to extract the information content of these data, which turns out\nsignificantly enhancing forecasting performance than the traditional machine\nlearning methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.01923v1"
    },
    {
        "title": "Semi-parametric Realized Nonlinear Conditional Autoregressive Expectile\n  and Expected Shortfall",
        "authors": [
            "Chao Wang",
            "Richard Gerlach"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  A joint conditional autoregressive expectile and Expected Shortfall framework\nis proposed. The framework is extended through incorporating a measurement\nequation which models the contemporaneous dependence between the realized\nmeasures and the latent conditional expectile. Nonlinear threshold\nspecification is further incorporated into the proposed framework. A Bayesian\nMarkov Chain Monte Carlo method is adapted for estimation, whose properties are\nassessed and compared with maximum likelihood via a simulation study.\nOne-day-ahead VaR and ES forecasting studies, with seven market indices,\nprovide empirical support to the proposed models.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09961v1"
    },
    {
        "title": "Improving Investment Suggestions for Peer-to-Peer (P2P) Lending via\n  Integrating Credit Scoring into Profit Scoring",
        "authors": [
            "Yan Wang",
            "Xuelei Sherry Ni"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  In the peer-to-peer (P2P) lending market, lenders lend the money to the\nborrowers through a virtual platform and earn the possible profit generated by\nthe interest rate. From the perspective of lenders, they want to maximize the\nprofit while minimizing the risk. Therefore, many studies have used machine\nlearning algorithms to help the lenders identify the \"best\" loans for making\ninvestments. The studies have mainly focused on two categories to guide the\nlenders' investments: one aims at minimizing the risk of investment (i.e., the\ncredit scoring perspective) while the other aims at maximizing the profit\n(i.e., the profit scoring perspective). However, they have all focused on one\ncategory only and there is seldom research trying to integrate the two\ncategories together. Motivated by this, we propose a two-stage framework that\nincorporates the credit information into a profit scoring modeling. We\nconducted the empirical experiment on a real-world P2P lending data from the US\nP2P market and used the Light Gradient Boosting Machine (lightGBM) algorithm in\nthe two-stage framework. Results show that the proposed two-stage method could\nidentify more profitable loans and thereby provide better investment guidance\nto the investors compared to the existing one-stage profit scoring alone\napproach. Therefore, the proposed framework serves as an innovative perspective\nfor making investment decisions in P2P lending.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.04536v1"
    },
    {
        "title": "Copula-Based Factor Model for Credit Risk Analysis",
        "authors": [
            "Meng-Jou Lu",
            "Cathy Yi-Hsuan Chen",
            "Wolfgang Karl Härdle"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  A standard quantitative method to access credit risk employs a factor model\nbased on joint multivariate normal distribution properties. By extending a\none-factor Gaussian copula model to make a more accurate default forecast, this\npaper proposes to incorporate a state-dependent recovery rate into the\nconditional factor loading, and model them by sharing a unique common factor.\nThe common factor governs the default rate and recovery rate simultaneously and\ncreates their association implicitly. In accordance with Basel III, this paper\nshows that the tendency of default is more governed by systematic risk rather\nthan idiosyncratic risk during a hectic period. Among the models considered,\nthe one with random factor loading and a state-dependent recovery rate turns\nout to be the most superior on the default prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.12092v2"
    },
    {
        "title": "Risk Measure Estimation On Fiegarch Processes",
        "authors": [
            "Taiane S. Prass",
            "Sílvia R. C. Lopes"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We consider the Fractionally Integrated Exponential Generalized\nAutoregressive Conditional Heteroskedasticity process, denoted by\nFIEGARCH(p,d,q), introduced by Bollerslev and Mikkelsen (1996). We present a\nsimulated study regarding the estimation of the risk measure $VaR_p$ on\nFIEGARCH processes. We consider the distribution function of the portfolio\nlog-returns (univariate case) and the multivariate distribution function of the\nrisk-factor changes (multivariate case). We also compare the performance of the\nrisk measures $VaR_p$, $ES_p$ and MaxLoss for a portfolio composed by stocks of\nfour Brazilian companies.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.5238v1"
    },
    {
        "title": "What do central counterparties default funds really cover? A\n  network-based stress test answer",
        "authors": [
            "Giulia Poce",
            "Giulio Cimini",
            "Andrea Gabrielli",
            "Andrea Zaccaria",
            "Giuditta Baldacci",
            "Marco Polito",
            "Mariangela Rizzo",
            "Silvia Sabatini"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  In the last years, increasing efforts have been put into the development of\neffective stress tests to quantify the resilience of financial institutions.\nHere we propose a stress test methodology for central counterparties based on a\nnetwork characterization of clearing members, whose links correspond to direct\ncredits and debits. This network constitutes the ground for the propagation of\nfinancial distress: equity losses caused by an initial shock with both\nexogenous and endogenous components reverberate within the network and are\namplified through credit and liquidity contagion channels. At the end of the\ndynamics, we determine the vulnerability of each clearing member, which\nrepresents its potential equity loss. We apply the proposed framework to the\nFixed Income asset class of CC&G, the central counterparty operating in Italy\nwhose main cleared securities are Italian Government Bonds. We consider two\ndifferent scenarios: a distributed, plausible initial shock, as well as a shock\ncorresponding to the cover 2 regulatory requirement (the simultaneous default\nof the two most exposed clearing members). Although the two situations lead to\nsimilar results after an unlimited reverberation of shocks on the network, the\ndistress propagation is much more hasty in the latter case, with a large number\nof additional defaults triggered at early stages of the dynamics. Our results\nthus show that setting a default fund to cover insolvencies only on a cover 2\nbasis may not be adequate for taming systemic events, and only very\nconservative default funds, such as CC&G's one, can face total losses due to\nthe shock propagation. Overall, our network-based stress test represents a\nrefined tool for calibrating default fund amounts.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03782v1"
    },
    {
        "title": "How the interbank market becomes systemically dangerous: an agent-based\n  network model of financial distress propagation",
        "authors": [
            "Matteo Serri",
            "Guido Caldarelli",
            "Giulio Cimini"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Assessing the stability of economic systems is a fundamental research focus\nin economics, that has become increasingly interdisciplinary in the currently\ntroubled economic situation. In particular, much attention has been devoted to\nthe interbank lending market as an important diffusion channel for financial\ndistress during the recent crisis. In this work we study the stability of the\ninterbank market to exogenous shocks using an agent-based network framework.\nOur model encompasses several ingredients that have been recognized in the\nliterature as pro-cyclical triggers of financial distress in the banking\nsystem: credit and liquidity shocks through bilateral exposures, liquidity\nhoarding due to counterparty creditworthiness deterioration, target leveraging\npolicies and fire-sales spillovers. But we exclude the possibility of central\nauthorities intervention. We implement this framework on a dataset of 183\nEuropean banks that were publicly traded between 2004 and 2013. We document the\nextreme fragility of the interbank lending market up to 2008, when a systemic\ncrisis leads to total depletion of market equity with an increasing speed of\nmarket collapse. After the crisis instead the system is more resilient to\nsystemic events in terms of residual market equity. However, the speed at which\nthe crisis breaks out reaches a new maximum in 2011, and never goes back to\nvalues observed before 2007. Our analysis points to the key role of the crisis\noutbreak speed, which sets the maximum delay for central authorities\nintervention to be effective.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.04311v1"
    },
    {
        "title": "Multinomial VaR Backtests: A simple implicit approach to backtesting\n  expected shortfall",
        "authors": [
            "Marie Kratz",
            "Yen H. Lok",
            "Alexander J McNeil"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Under the Fundamental Review of the Trading Book (FRTB) capital charges for\nthe trading book are based on the coherent expected shortfall (ES) risk\nmeasure, which show greater sensitivity to tail risk. In this paper it is\nargued that backtesting of expected shortfall - or the trading book model from\nwhich it is calculated - can be based on a simultaneous multinomial test of\nvalue-at-risk (VaR) exceptions at different levels, an idea supported by an\napproximation of ES in terms of multiple quantiles of a distribution proposed\nin Emmer et al. (2015). By comparing Pearson, Nass and likelihood-ratio tests\n(LRTs) for different numbers of VaR levels $N$ it is shown in a series of\nsimulation experiments that multinomial tests with $N\\geq 4$ are much more\npowerful at detecting misspecifications of trading book loss models than\nstandard binomial exception tests corresponding to the case $N=1$. Each test\nhas its merits: Pearson offers simplicity; Nass is robust in its size\nproperties to the choice of $N$; the LRT is very powerful though slightly\nover-sized in small samples and more computationally burdensome. A\ntraffic-light system for trading book models based on the multinomial test is\nproposed and the recommended procedure is applied to a real-data example\nspanning the 2008 financial crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.04851v1"
    },
    {
        "title": "Dual Moments and Risk Attitudes",
        "authors": [
            "Louis R. Eeckhoudt",
            "Roger J. A. Laeven"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  In decision under risk, the primal moments of mean and variance play a\ncentral role to define the local index of absolute risk aversion. In this\npaper, we show that in canonical non-EU models dual moments have to be used\ninstead of, or on par with, their primal counterparts to obtain an equivalent\nindex of absolute risk aversion.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.03347v2"
    },
    {
        "title": "Conditional loss probabilities for systems of economic agents sharing\n  light-tailed claims with analysis of portfolio diversification benefits",
        "authors": [
            "Claudia Klüppelberg",
            "Miriam Isabel Seifert"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We analyze systems of agents sharing light-tailed risky claims issued by\ndifferent financial objects. Assuming exponentially distributed claims, we\nobtain that both agents' and system's losses follow generalized exponential\nmixture distributions. We show that this leads to qualitatively different\nresults on individual and system risks compared to heavy-tailed claims\npreviously studied in the literature. By deducing conditional loss\ndistributions we investigate the impact of stress situations on agents' and\nsystem's losses. Moreover, we present a criterion for agents to decide whether\nholding few objects or portfolio diversification minimizes their risks in\nsystem crisis situations.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07132v1"
    },
    {
        "title": "Bayesian Semi-parametric Realized-CARE Models for Tail Risk Forecasting\n  Incorporating Realized Measures",
        "authors": [
            "Richard Gerlach",
            "Chao Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  A new model framework called Realized Conditional Autoregressive Expectile\n(Realized-CARE) is proposed, through incorporating a measurement equation into\nthe conventional CARE model, in a manner analogous to the Realized-GARCH model.\nCompeting realized measures (e.g. Realized Variance and Realized Range) are\nemployed as the dependent variable in the measurement equation and to drive\nexpectile dynamics. The measurement equation here models the contemporaneous\ndependence between the realized measure and the latent conditional expectile.\nWe also propose employing the quantile loss function as the target criterion,\ninstead of the conventional violation rate, during the expectile level grid\nsearch. For the proposed model, the usual search procedure and asymmetric least\nsquares (ALS) optimization to estimate the expectile level and CARE parameters\nproves challenging and often fails to convergence. We incorporate a fast random\nwalk Metropolis stochastic search method, combined with a more targeted grid\nsearch procedure, to allow reasonably fast and improved accuracy in estimation\nof this level and the associated model parameters. Given the convergence issue,\nBayesian adaptive Markov Chain Monte Carlo methods are proposed for estimation,\nwhilst their properties are assessed and compared with ALS via a simulation\nstudy. In a real forecasting study applied to 7 market indices and 2 individual\nasset returns, compared to the original CARE, the parametric GARCH and\nRealized-GARCH models, one-day-ahead Value-at-Risk and Expected Shortfall\nforecasting results favor the proposed Realized-CARE model, especially when\nincorporating the Realized Range and the sub-sampled Realized Range as the\nrealized measure in the model.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08488v1"
    },
    {
        "title": "Dynamic risk measures with fluctuation of market volatility under\n  Bochne-Lebesgue space",
        "authors": [
            "Fei Sun",
            "Jingchao Li",
            "Jieming Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Starting from the global financial crisis to the more recent disruptions\nbrought about by geopolitical tensions and public health crises, the volatility\nof risk in financial markets has increased significantly. This underscores the\nnecessity for comprehensive risk measures capable of capturing the complexity\nand heightened fluctuations in market volatility. This need is crucial not only\nfor new financial assets but also for the traditional financial market in the\nface of a rapidly changing financial environment and global landscape. In this\npaper, we consider the risk measures on a special space $L^{p(\\cdot)}$, where\nthe variable exponent $p(\\cdot)$ is no longer a given real number as in the\nconventional risk measure space $L^{p}$, but rather a random variable\nreflecting potential fluctuations in volatility within financial markets.\nThrough further development of axioms related to this class of risk measures,\nwe also establish dual representations for them.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01166v8"
    },
    {
        "title": "Quasiconvex risk measures with markets volatility",
        "authors": [
            "Fei Sun",
            "Yijun Hu"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Since the quasiconvex risk measures is a bigger class than the well known\nconvex risk measures, the study of quasiconvex risk measures makes sense\nespecially in the financial markets with volatility. In this paper, we will\nstudy the quasiconvex risk measures defined on a special space $L^{p(\\cdot)}$\nwhere the variable exponent $p(\\cdot)$ is no longer a given real number like\nthe space $L^{p}$, but a random variable, which reflects the possible\nvolatility of the financial markets. The dual representation for this\nquasiconvex risk measures will also provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08701v4"
    },
    {
        "title": "Set-valued risk statistics with the time value of money",
        "authors": [
            "Fei Sun",
            "Xiaozhi Fan",
            "Weitao Liu"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  The time value of money is a critical factor not only in risk analysis, but\nalso in insurance and financial applications. In this paper, we consider a\nspecial class of set-valued risk statistics by introducing the time value of\nmoney. In fact, the risk statistics established by this method is closer to\nfinancial reality than traditional ones. Moreover, this new risk statistic can\nbe uesd for the quantification of portfolio risk. By further developing the\nproperties related to these risk statistics, we are able to derive\nrepresentation results for such risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.00486v4"
    },
    {
        "title": "Set-Valued Risk Measures as Backward Stochastic Difference Inclusions\n  and Equations",
        "authors": [
            "Çağın Ararat",
            "Zachary Feinstein"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Scalar dynamic risk measures for univariate positions in continuous time are\ncommonly represented as backward stochastic differential equations. In the\nmultivariate setting, dynamic risk measures have been defined and studied as\nfamilies of set-valued functionals in the recent literature. There are two\npossible extensions of scalar backward stochastic differential equations for\nthe set-valued framework: (1) backward stochastic differential inclusions,\nwhich evaluate the risk dynamics on the selectors of acceptable capital\nallocations; or (2) set-valued backward stochastic differential equations,\nwhich evaluate the risk dynamics on the full set of acceptable capital\nallocations as a singular object. In this work, the discrete time setting is\ninvestigated with difference inclusions and difference equations in order to\nprovide insights for such differential representations for set-valued dynamic\nrisk measures in continuous time.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06916v3"
    },
    {
        "title": "Pay-As-You-Drive Insurance Pricing Model",
        "authors": [
            "Safoora Zarei",
            "Ali R. Fallahi"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Every time drivers take to the road, and with each mile that they drive,\nexposes themselves and others to the risk of an accident. Insurance premiums\nare only weakly linked to mileage, however, and have lump-sum characteristics\nlargely. The result is too much driving, and too many accidents. In this paper,\nwe introduce some useful theoretical results for Pay-As-You-Drive in Automobile\ninsurances. We consider a counting process and also find the distribution of\ndiscounted collective risk model when the counting process is non-homogeneous\nPoisson.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09273v1"
    },
    {
        "title": "An Artificial Intelligence approach to Shadow Rating",
        "authors": [
            "Angela Rita Provenzano",
            "Daniele Trifirò",
            "Nicola Jean",
            "Giacomo Le Pera",
            "Maurizio Spadaccino",
            "Luca Massaron",
            "Claudio Nordio"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We analyse the effectiveness of modern deep learning techniques in predicting\ncredit ratings over a universe of thousands of global corporate entities\nobligations when compared to most popular, traditional machine-learning\napproaches such as linear models and tree-based classifiers. Our results show a\nadequate accuracy over different rating classes when applying categorical\nembeddings to artificial neural networks (ANN) architectures.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09764v1"
    },
    {
        "title": "Building and Testing Yield Curve Generators for P&C Insurance",
        "authors": [
            "Gary Venter",
            "Kailan Shang"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Interest-rate risk is a key factor for property-casualty insurer capital. P&C\ncompanies tend to be highly leveraged, with bond holdings much greater than\ncapital. For GAAP capital, bonds are marked to market but liabilities are not,\nso shifts in the yield curve can have a significant impact on capital.\nYield-curve scenario generators are one approach to quantifying this risk. They\nproduce many future simulated evolutions of the yield curve, which can be used\nto quantify the probabilities of bond-value changes that would result from\nvarious maturity-mix strategies. Some of these generators are provided as\nblack-box models where the user gets only the projected scenarios. One focus of\nthis paper is to provide methods for testing generated scenarios from such\nmodels by comparing to known distributional properties of yield curves.\n  P&C insurers hold bonds to maturity and manage cash-flow risk by matching\nasset and liability flows. Derivative pricing and stochastic volatility are of\nlittle concern over the relevant time frames. This requires different models\nand model testing than what is common in the broader financial markets.\n  To complicate things further, interest rates for the last decade have not\nbeen following the patterns established in the sixty years following WWII. We\nare now coming out of the period of very low rates, yet are still not returning\nto what had been thought of as normal before that. Modeling and model testing\nare in an evolving state while new patterns emerge.\n  Our analysis starts with a review of the literature on interest-rate model\ntesting, with a P&C focus, and an update of the tests for current market\nbehavior. We then discuss models, and use them to illustrate the fitting and\ntesting methods. The testing discussion does not require the model-building\nsection.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10526v1"
    },
    {
        "title": "Online Quantification of Input Model Uncertainty by Two-Layer Importance\n  Sampling",
        "authors": [
            "Tianyi Liu",
            "Enlu Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Stochastic simulation has been widely used to analyze the performance of\ncomplex stochastic systems and facilitate decision making in those systems.\nStochastic simulation is driven by the input model, which is a collection of\nprobability distributions that model the stochasticity in the system. The input\nmodel is usually estimated using a finite amount of data, which introduces the\nso-called input model uncertainty to the simulation output. How to quantify\ninput uncertainty has been studied extensively, and many methods have been\nproposed for the batch data setting, i.e., when all the data are available at\nonce. However, methods for \"streaming data\" arriving sequentially in time are\nstill in demand, despite that streaming data have become increasingly prevalent\nin modern applications. To fill this gap, we propose a two-layer importance\nsampling framework that incorporates streaming data for online input\nuncertainty quantification. Under this framework, we develop two algorithms\nthat suit different application scenarios: the first scenario is when data come\nat a fast speed and there is no time for any new simulation in between updates;\nthe second is when data come at a moderate speed and a few but limited\nsimulations are allowed at each time stage. We prove the consistency and\nasymptotic convergence rate results, which theoretically show the efficiency of\nour proposed approach. We further demonstrate the proposed algorithms on a\nnumerical example of the news vendor problem.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11172v2"
    },
    {
        "title": "Maximising with-profit pensions without guarantees",
        "authors": [
            "M. Carmen Boado-Penas",
            "Julia Eisenberg",
            "Paul Krühner"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Currently, pension providers are running into trouble mainly due to the\nultra-low interest rates and the guarantees associated to some pension\nbenefits. With the aim of reducing the pension volatility and providing\nadequate pension levels with no guarantees, we carry out mathematical analysis\nof a new pension design in the accumulation phase. The individual's premium is\nsplit into the individual and collective part and invested in funds. In times\nwhen the return from the individual fund exits a predefined corridor, a certain\nnumber of units is transferred to or from the collective account smoothing in\nthis way the volatility of the individual fund. The target is to maximise the\ntotal accumulated capital, consisting of the individual account and a portion\nof the collective account due to a so-called redistribution index, at\nretirement by controlling the corridor width. We also discuss the necessary and\nsufficient conditions that have to be put on the redistribution index in order\nto avoid arbitrage opportunities for contributors.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11858v1"
    },
    {
        "title": "Transforming public pensions: A mixed scheme with a credit granted by\n  the state",
        "authors": [
            "M. Carmen Boado-Penas",
            "Julia Eisenberg",
            "Ralf Korn"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Birth rates have dramatically decreased and, with continuous improvements in\nlife expectancy, pension expenditure is on an irreversibly increasing path.\nThis will raise serious concerns for the sustainability of the public pension\nsystems usually financed on a pay-as-you-go (PAYG) basis where current\ncontributions cover current pension expenditure. With this in mind, the aim of\nthis paper is to propose a mixed pension system that consists of a combination\nof a classical PAYG scheme and an increase of the contribution rate invested in\na funding scheme. The investment of the funding part is designed so that the\nPAYG pension system is financially sustainable at a particular level of\nprobability and at the same time provide some gains to individuals. In this\nsense, we make the individuals be an active part to face the demographic risks\ninherent in the PAYG and re-establish its financial sustainability.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12329v1"
    },
    {
        "title": "Systemic liquidity contagion in the European interbank market",
        "authors": [
            "V. Macchiati",
            "G. Brandi",
            "G. Cimini",
            "G. Caldarelli",
            "D. Paolotti",
            "T. Di Matteo"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Systemic liquidity risk, defined by the IMF as \"the risk of simultaneous\nliquidity difficulties at multiple financial institutions\", is a key topic in\nmacroprudential policy and financial stress analysis. Specialized models to\nsimulate funding liquidity risk and contagion are available but they require\nnot only banks' bilateral exposures data but also balance sheet data with\nsufficient granularity, which are hardly available. Alternatively, risk\nanalyses on interbank networks have been done via centrality measures of the\nunderlying graph capturing the most interconnected and hence more prone to risk\nspreading banks. In this paper, we propose a model which relies on an epidemic\nmodel which simulate a contagion on the interbank market using the funding\nliquidity shortage mechanism as contagion process. The model is enriched with\ncountry and bank risk features which take into account the heterogeneity of the\ninterbank market. The proposed model is particularly useful when full set of\ndata necessary to run specialized models is not available. Since the interbank\nnetwork is not fully available, an economic driven reconstruction method is\nalso proposed to retrieve the interbank network by constraining the standard\nreconstruction methodology to real financial indicators. We show that the\ncontagion model is able to reproduce systemic liquidity risk across different\nyears and countries. This result suggests that the proposed model can be\nsuccessfully used as a valid alternative to more complex ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.13275v2"
    },
    {
        "title": "The Impact of the Choice of Risk and Dispersion Measure on\n  Procyclicality",
        "authors": [
            "Marcel Bräutigam",
            "Marie Kratz"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Procyclicality of historical risk measure estimation means that one tends to\nover-estimate future risk when present realized volatility is high and vice\nversa under-estimate future risk when the realized volatility is low. Out of it\ndifferent questions arise, relevant for applications and theory: What are the\nfactors which affect the degree of procyclicality? More specifically, how does\nthe choice of risk measure affect this? How does this behaviour vary with the\nchoice of realized volatility estimator? How do different underlying model\nassumptions influence the pro-cyclical effect? In this paper we consider three\ndifferent well-known risk measures (Value-at-Risk, Expected Shortfall,\nExpectile), the r-th absolute centred sample moment, for any integer $r>0$, as\nrealized volatility estimator (this includes the sample variance and the sample\nmean absolute deviation around the sample mean) and two models (either an iid\nmodel or an augmented GARCH($p$,$q$) model). We show that the strength of\nprocyclicality depends on these three factors, the choice of risk measure, the\nrealized volatility estimator and the model considered. But, no matter the\nchoices, the procyclicality will always be present.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.00529v1"
    },
    {
        "title": "Tail Granger causalities and where to find them: extreme risk spillovers\n  vs. spurious linkages",
        "authors": [
            "Piero Mazzarisi",
            "Silvia Zaoli",
            "Carlo Campajola",
            "Fabrizio Lillo"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Identifying risk spillovers in financial markets is of great importance for\nassessing systemic risk and portfolio management. Granger causality in tail (or\nin risk) tests whether past extreme events of a time series help predicting\nfuture extreme events of another time series. The topology and connectedness of\nnetworks built with Granger causality in tail can be used to measure systemic\nrisk and to identify risk transmitters. Here we introduce a novel test of\nGranger causality in tail which adopts the likelihood ratio statistic and is\nbased on the multivariate generalization of a discrete autoregressive process\nfor binary time series describing the sequence of extreme events of the\nunderlying price dynamics. The proposed test has very good size and power in\nfinite samples, especially for large sample size, allows inferring the correct\ntime scale at which the causal interaction takes place, and it is flexible\nenough for multivariate extension when more than two time series are considered\nin order to decrease false detections as spurious effect of neglected\nvariables. An extensive simulation study shows the performances of the proposed\nmethod with a large variety of data generating processes and it introduces also\nthe comparison with the test of Granger causality in tail by [Hong et al.,\n2009]. We report both advantages and drawbacks of the different approaches,\npointing out some crucial aspects related to the false detections of Granger\ncausality for tail events. An empirical application to high frequency data of a\nportfolio of US stocks highlights the merits of our novel approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01160v2"
    },
    {
        "title": "On unbalanced data and common shock models in stochastic loss reserving",
        "authors": [
            "Benjamin Avanzi",
            "Gregory Clive Taylor",
            "Phuong Anh Vu",
            "Bernard Wong"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Introducing common shocks is a popular dependence modelling approach, with\nsome recent applications in loss reserving. The main advantage of this approach\nis the ability to capture structural dependence coming from known\nrelationships. In addition, it helps with the parsimonious construction of\ncorrelation matrices of large dimensions. However, complications arise in the\npresence of \"unbalanced data\", that is, when (expected) magnitude of\nobservations over a single triangle, or between triangles, can vary\nsubstantially. Specifically, if a single common shock is applied to all of\nthese cells, it can contribute insignificantly to the larger values and/or\nswamp the smaller ones, unless careful adjustments are made. This problem is\nfurther complicated in applications involving negative claim amounts. In this\npaper, we address this problem in the loss reserving context using a common\nshock Tweedie approach for unbalanced data. We show that the solution not only\nprovides a much better balance of the common shock proportions relative to the\nunbalanced data, but it is also parsimonious. Finally, the common shock Tweedie\nmodel also provides distributional tractability.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03500v2"
    },
    {
        "title": "Proving prediction prudence",
        "authors": [
            "Dirk Tasche"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We study how to perform tests on samples of pairs of observations and\npredictions in order to assess whether or not the predictions are prudent.\nPrudence requires that that the mean of the difference of the\nobservation-prediction pairs can be shown to be significantly negative. For\nsafe conclusions, we suggest testing both unweighted (or equally weighted) and\nweighted means and explicitly taking into account the randomness of individual\npairs. The test methods presented are mainly specified as bootstrap and normal\napproximation algorithms. The tests are general but can be applied in\nparticular in the area of credit risk, both for regulatory and accounting\npurposes.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03698v3"
    },
    {
        "title": "Value-at-Risk substitute for non-ruin capital is fallacious and\n  redundant",
        "authors": [
            "Vsevolod Malinovskii"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  This seemed impossible to use a theoretically adequate but too sophisticated\nrisk measure called non-ruin capital, whence its widespread (including\nregulatory documents) replacement with an inadequate, but simple risk measure\ncalled Value-at-Risk. Conflicting with the idea by Albert Einstein that\n\"everything should be made as simple as possible, but not simpler\", this led to\nfallacious, and even deceitful (but generally accepted) standards and\nrecommendations. Arguing from the standpoint of mathematical theory of risk, we\naim to break this impasse.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05428v1"
    },
    {
        "title": "Parameter estimation of default portfolios using the Merton model and\n  Phase transition",
        "authors": [
            "Masato Hisakado",
            "Shintaro Mori"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We discuss the parameter estimation of the probability of default (PD), the\ncorrelation between the obligors, and a phase transition. In our previous work,\nwe studied the problem using the beta-binomial distribution. A non-equilibrium\nphase transition with an order parameter occurs when the temporal correlation\ndecays by power law. In this article, we adopt the Merton model, which uses an\nasset correlation as the default correlation, and find that a phase transition\noccurs when the temporal correlation decays by power law. When the power index\nis less than one, the PD estimator converges slowly. Thus, it is difficult to\nestimate PD with limited historical data. Conversely, when the power index is\ngreater than one, the convergence speed is inversely proportional to the number\nof samples. We investigate the empirical default data history of several rating\nagencies. The estimated power index is in the slow convergence range when we\nuse long history data. This suggests that PD could have a long memory and that\nit is difficult to estimate parameters due to slow convergence.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.07967v1"
    },
    {
        "title": "Large Portfolio Asymptotics for Loss From Default",
        "authors": [
            "Kay Giesecke",
            "Konstantinos Spiliopoulos",
            "Richard B. Sowers",
            "Justin A. Sirignano"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  We prove a law of large numbers for the loss from default and use it for\napproximating the distribution of the loss from default in large, potentially\nheterogenous portfolios. The density of the limiting measure is shown to solve\na non-linear SPDE, and the moments of the limiting measure are shown to satisfy\nan infinite system of SDEs. The solution to this system leads to %the solution\nto the SPDE through an inverse moment problem, and to the distribution of the\nlimiting portfolio loss, which we propose as an approximation to the loss\ndistribution for a large portfolio. Numerical tests illustrate the accuracy of\nthe approximation, and highlight its computational advantages over a direct\nMonte Carlo simulation of the original stochastic system.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.1272v4"
    },
    {
        "title": "Reconstruction of financial network for robust estimation of systemic\n  risk",
        "authors": [
            "Iacopo Mastromatteo",
            "Elia Zarinelli",
            "Matteo Marsili"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  In this paper we estimate the propagation of liquidity shocks through\ninterbank markets when the information about the underlying credit network is\nincomplete. We show that techniques such as Maximum Entropy currently used to\nreconstruct credit networks severely underestimate the risk of contagion by\nassuming a trivial (fully connected) topology, a type of network structure\nwhich can be very different from the one empirically observed. We propose an\nefficient message-passing algorithm to explore the space of possible network\nstructures, and show that a correct estimation of the network degree of\nconnectedness leads to more reliable estimations for systemic risk. Such\nalgorithm is also able to produce maximally fragile structures, providing a\npractical upper bound for the risk of contagion when the actual network\nstructure is unknown. We test our algorithm on ensembles of synthetic data\nencoding some features of real financial networks (sparsity and heterogeneity),\nfinding that more accurate estimations of risk can be achieved. Finally we find\nthat this algorithm can be used to control the amount of information regulators\nneed to require from banks in order to sufficiently constrain the\nreconstruction of financial networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.6210v2"
    },
    {
        "title": "The Future Has Thicker Tails than the Past: Model Error As Branching\n  Counterfactuals",
        "authors": [
            "Nassim N. Taleb"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  Ex ante forecast outcomes should be interpreted as counterfactuals (potential\nhistories), with errors as the spread between outcomes. Reapplying measurements\nof uncertainty about the estimation errors of the estimation errors of an\nestimation leads to branching counterfactuals. Such recursions of epistemic\nuncertainty have markedly different distributial properties from conventional\nsampling error. Nested counterfactuals of error rates invariably lead to fat\ntails, regardless of the probability distribution used, and to powerlaws under\nsome conditions. A mere .01% branching error rate about the STD (itself an\nerror rate), and .01% branching error rate about that error rate, etc.\n(recursing all the way) results in explosive (and infinite) higher moments than\n1. Missing any degree of regress leads to the underestimation of small\nprobabilities and concave payoffs (a standard example of which is Fukushima).\nThe paper states the conditions under which higher order rates of uncertainty\n(expressed in spreads of counterfactuals) alters the shapes the of final\ndistribution and shows which a priori beliefs about conterfactuals are needed\nto accept the reliability of conventional probabilistic methods (thin tails or\nmildly fat tails).\n",
        "pdf_link": "http://arxiv.org/pdf/1209.2298v1"
    },
    {
        "title": "Conditional Value-at-Risk: Theory and Applications",
        "authors": [
            "Jakob Kisiala"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  This thesis presents the Conditional Value-at-Risk concept and combines an\nanalysis that covers its application as a risk measure and as a vector norm.\nFor both areas of application the theory is revised in detail and examples are\ngiven to show how to apply the concept in practice.\n  In the first part, CVaR as a risk measure is introduced and the analysis\ncovers the mathematical definition of CVaR and different methods to calculate\nit. Then, CVaR optimization is analysed in the context of portfolio selection\nand how to apply CVaR optimization for hedging a portfolio consisting of\noptions. The original contributions in this part are an alternative proof of\nAcerbi's Integral Formula in the continuous case and an explicit programme\nformulation for portfolio hedging.\n  The second part first analyses the Scaled and Non-Scaled CVaR norm as new\nfamily of norms in $\\mathbb{R}^n$ and compares this new norm family to the more\nwidely known $L_p$ norms. Then, model (or signal) recovery problems are\ndiscussed and it is described how appropriate norms can be used to recover a\nsignal with less observations than the dimension of the signal. The last\nchapter of this dissertation then shows how the Non-Scaled CVaR norm can be\nused in this model recovery context. The original contributions in this part\nare an alternative proof of the equivalence of two different characterizations\nof the Scaled CVaR norm, a new proposition that the Scaled CVaR norm is\npiecewise convex, and the entire \\autoref{chapter:Recovery_using_CVaR}. Since\nthe CVaR norm is a rather novel concept, its applications in a model recovery\ncontext have not been researched yet. Therefore, the final chapter of this\nthesis might lay the basis for further research in this area.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.00140v1"
    },
    {
        "title": "Scenario generation for single-period portfolio selection problems with\n  tail risk measures: coping with high dimensions and integer variables",
        "authors": [
            "Jamie Fairbrother",
            "Amanda Turner",
            "Stein Wallace"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In this paper we propose a problem-driven scenario generation approach to the\nsingle-period portfolio selection problem which use tail risk measures such as\nconditional value-at-risk. Tail risk measures are useful for quantifying\npotential losses in worst cases. However, for scenario-based problems these are\nproblematic: because the value of a tail risk measure only depends on a small\nsubset of the support of the distribution of asset returns, traditional\nscenario based methods, which spread scenarios evenly across the whole support\nof the distribution, yield very unstable solutions unless we use a very large\nnumber of scenarios. The proposed approach works by prioritizing the\nconstruction of scenarios in the areas of a probability distribution which\ncorrespond to the tail losses of feasible portfolios.\n  The proposed approach can be applied to difficult instances of the portfolio\nselection problem characterized by high-dimensions, non-elliptical\ndistributions of asset returns, and the presence of integer variables. It is\nalso observed that the methodology works better as the feasible set of\nportfolios becomes more constrained. Based on this fact, a heuristic algorithm\nbased on the sample average approximation method is proposed. This algorithm\nworks by adding artificial constraints to the problem which are gradually\ntightened, allowing one to telescope onto high quality solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.04935v3"
    },
    {
        "title": "Intragroup transfers, intragroup diversification and their risk\n  assessment",
        "authors": [
            "Andreas Haier",
            "Ilya Molchanov",
            "Michael Schmutz"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  When assessing group solvency, an important question is to what extent\nintragroup transfers may be considered, as this determines to which extent\ndiversification can be achieved. We suggest a framework to describe the\nfamilies of admissible transfers that range from the free movement of capital\nto excluding any transactions. The constraints on admissible transactions are\ndescribed as random closed sets. The paper focuses on the corresponding\nsolvency tests that amount to the existence of acceptable selections of the\nrandom sets of admissible transactions.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.06320v2"
    },
    {
        "title": "Credit Risk Meets Random Matrices: Coping with Non-Stationary Asset\n  Correlations",
        "authors": [
            "Andreas Mühlbacher",
            "Thomas Guhr"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We review recent progress in modeling credit risk for correlated assets. We\nstart from the Merton model which default events and losses are derived from\nthe asset values at maturity. To estimate the time development of the asset\nvalues, the stock prices are used whose correlations have a strong impact on\nthe loss distribution, particularly on its tails. These correlations are\nnon-stationary which also influences the tails. We account for the asset\nfluctuations by averaging over an ensemble of random matrices that models the\ntruly existing set of measured correlation matrices. As a most welcome side\neffect, this approach drastically reduces the parameter dependence of the loss\ndistribution, allowing us to obtain very explicit results which show\nquantitatively that the heavy tails prevail over diversification benefits even\nfor small correlations. We calibrate our random matrix model with market data\nand show how it is capable of grasping different market situations.\nFurthermore, we present numerical simulations for concurrent portfolio risks,\ni.e., for the joint probability densities of losses for two portfolios. For the\nconvenience of the reader, we give an introduction to the Wishart random matrix\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00261v1"
    },
    {
        "title": "An ontological investigation of unimaginable events",
        "authors": [
            "Thomas Santoli",
            "Christoph Siebenbrunner"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We show that, under mild assumptions, some unimaginable events - which we\nrefer to as Black Swan events - must necessarily occur. It follows as a\ncorollary of our theorem that any computational model of decision-making under\nuncertainty is incomplete in the sense that not all events that occur can be\ntaken into account. In the context of decision theory we argue that this\nconstitutes a stronger sense of uncertainty than Knightian uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.02570v3"
    },
    {
        "title": "Approximation of Some Multivariate Risk Measures for Gaussian Risks",
        "authors": [
            "E. Hashorva"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Gaussian random vectors exhibit the loss of dimension phenomena, which relate\nto their joint survival tail behaviour. Besides, the fact that the components\nof such vectors are light-tailed complicates the approximations of various\nmultivariate risk measures significantly. In this contribution we derive\nprecise approximations of marginal mean excess, marginal expected shortfall and\nmultivariate conditional tail expectation of Gaussian random vectors and\nhighlight links with conditional limit theorems. Our study indicates that\nsimilar results hold for elliptical and Gaussian like multivariate risks.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.06922v2"
    },
    {
        "title": "Financial Contagion in a Generalized Stochastic Block Model",
        "authors": [
            "Nils Detering",
            "Thilo Meyer-Brandis",
            "Konstantinos Panagiotou",
            "Daniel Ritter"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  One of the most defining features of the global financial network is its\ninherent complex and intertwined structure. From the perspective of systemic\nrisk it is important to understand the influence of this network structure on\ndefault contagion. Using sparse random graphs to model the financial network,\nasymptotic methods turned out powerful to analytically describe the contagion\nprocess and to make statements about resilience. So far, however, they have\nbeen limited to so-called {\\em rank one} models in which informally the only\nnetwork parameter is the degree sequence (see (Amini et. al. 2016) and\n(Detering et. al. 2019) for example) and the contagion process can be described\nby a one dimensional fix-point equation. These networks fail to account for a\npronounced block structure such as core/periphery or a network composed of\ndifferent connected blocks for different countries. We present a much more\ngeneral model here, where we distinguish vertices (institutions) of different\ntypes and let edge probabilities and exposures depend on the types of both, the\nreceiving and the sending vertex plus additional parameters. Our main result\nallows to compute explicitly the systemic damage caused by some initial local\nshock event, and we derive a complete characterisation of resilient\nrespectively non-resilient financial systems. This is the first instance that\ndefault contagion is rigorously studied in a model outside the class of rank\none models and several technical challenges arise. Moreover, in contrast to\nprevious work, in which networks could be classified as resilient or non\nresilient, independent of the distribution of the shock, information about the\nshock becomes important in our model and a more refined resilience condition\narises. Among other applications of our theory we derive resilience conditions\nfor the global network based on subnetwork conditions only.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08169v4"
    },
    {
        "title": "A Semi-parametric Realized Joint Value-at-Risk and Expected Shortfall\n  Regression Framework",
        "authors": [
            "Chao Wang",
            "Richard Gerlach",
            "Qian Chen"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  A new realized conditional autoregressive Value-at-Risk (VaR) framework is\nproposed, through incorporating a measurement equation into the original\nquantile regression model. The framework is further extended by employing\nvarious Expected Shortfall (ES) components, to jointly estimate and forecast\nVaR and ES. The measurement equation models the contemporaneous dependence\nbetween the realized measure (i.e., Realized Variance and Realized Range) and\nthe latent conditional ES. An adaptive Bayesian Markov Chain Monte Carlo method\nis employed for estimation and forecasting, the properties of which are\nassessed and compared with maximum likelihood through a simulation study. In a\ncomprehensive forecasting study on 1% and 2.5 % quantile levels, the proposed\nmodels are compared to a range of parametric, non-parametric and\nsemi-parametric models, based on 7 market indices and 7 individual assets.\nOne-day-ahead VaR and ES forecasting results favor the proposed models,\nespecially when incorporating the sub-sampled Realized Variance and the\nsub-sampled Realized Range in the model.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.02422v2"
    },
    {
        "title": "Regulator-based risk statistics for portfolios",
        "authors": [
            "Xiaochuan Deng",
            "Fei Sun"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Risk statistic is a critical factor not only for risk analysis but also for\nfinancial application. However, the traditional risk statistics may fail to\ndescribe the characteristics of regulator-based risk. In this paper, we\nconsider the regulator-based risk statistics for portfolios. By further\ndeveloping the properties related to regulator-based risk statistics, we are\nable to derive dual representation for such risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.08829v5"
    },
    {
        "title": "Regulator-based risk statistics with scenario analysis",
        "authors": [
            "Xiaochuan Deng",
            "Fei Sun"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  As regulators pay more attentions to losses rather than gains, we are able to\nderive a new class of risk statistics, named regulator-based risk statistics\nwith scenario analysis in this paper. This new class of risk statistics can be\nconsidered as a kind of risk extension of risk statistics introduced by Kou et\nal. \\cite{11}, and also data-based versions of loss-based risk measures\nintroduced by Cont et al. \\cite{5} and Sun et al. \\cite{12}.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11032v2"
    },
    {
        "title": "Tail models and the statistical limit of accuracy in risk assessment",
        "authors": [
            "Ingo Hoffmann",
            "Christoph J. Börner"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  In risk management, tail risks are of crucial importance. The assessment of\nrisks should be carried out in accordance with the regulatory authority's\nrequirement at high quantiles. In general, the underlying distribution function\nis unknown, the database is sparse, and therefore special tail models are used.\nVery often, the generalized Pareto distribution is employed as a basic model,\nand its parameters are determined with data from the tail area. With the\ndetermined tail model, statisticians then calculate the required high\nquantiles. In this context, we consider the possible accuracy of the\ncalculation of the quantiles and determine the finite sample distribution\nfunction of the quantile estimator, depending on the confidence level and the\nparameters of the tail model, and then calculate the finite sample bias and the\nfinite sample variance of the quantile estimator. Finally, we present an impact\nanalysis on the quantiles of an unknown distribution function.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.12113v1"
    },
    {
        "title": "Optimal Dividend Strategy for an Insurance Group with Contagious Default\n  Risk",
        "authors": [
            "Zhuo Jin",
            "Huafu Liao",
            "Yue Yang",
            "Xiang Yu"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  This paper studies the optimal dividend for a multi-line insurance group, in\nwhich each subsidiary runs a product line and is exposed to some external\ncredit risk. The default contagion is considered such that one default event\nmay increase the default probabilities of all surviving subsidiaries. The total\ndividend problem for the insurance group is investigated and we find that the\noptimal dividend strategy is still of the barrier type. Furthermore, we show\nthat the optimal barrier of each subsidiary is modulated by the default state.\nThat is, how many and which subsidiaries have defaulted will determine the\ndividend threshold of each surviving subsidiary. These conclusions are based on\nthe analysis of the associated recursive system of Hamilton-Jacobi-Bellman\nvariational inequalities (HJBVIs). The existence of the classical solution is\nestablished and the verification theorem is proved. In the case of two\nsubsidiaries, the value function and optimal barriers are given in analytical\nforms, allowing us to conclude that the optimal barrier of one subsidiary\ndecreases if the other subsidiary defaults.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.09511v6"
    },
    {
        "title": "The Reactive Beta Model",
        "authors": [
            "Sebastien Valeyre",
            "Denis S. Grebenkov",
            "Sofiane Aboura"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We present a reactive beta model that includes the leverage effect to allow\nhedge fund managers to target a near-zero beta for market neutral strategies.\nFor this purpose, we derive a metric of correlation with leverage effect to\nidentify the relation between the market beta and volatility changes. An\nempirical test based on the most popular market neutral strategies is run from\n2000 to 2015 with exhaustive data sets including 600 US stocks and 600 European\nstocks. Our findings confirm the ability of the reactive beta model to withdraw\nan important part of the bias from the beta estimation and from most popular\nmarket neutral strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00919v1"
    },
    {
        "title": "Unveil stock correlation via a new tensor-based decomposition method",
        "authors": [
            "Giuseppe Brandi",
            "Ruggero Gramatica",
            "Tiziana Di Matteo"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Portfolio allocation and risk management make use of correlation matrices and\nheavily rely on the choice of a proper correlation matrix to be used. In this\nregard, one important question is related to the choice of the proper sample\nperiod to be used to estimate a stable correlation matrix. This paper addresses\nthis question and proposes a new methodology to estimate the correlation matrix\nwhich doesn't depend on the chosen sample period. This new methodology is based\non tensor factorization techniques. In particular, combining and normalizing\nfactor components, we build a correlation matrix which shows emerging\nstructural dependency properties not affected by the sample period. To retrieve\nthe factor components, we propose a new tensor decomposition (which we name\nSlice-Diagonal Tensor (SDT) factorization) and compare it to the two most used\ntensor decompositions, the Tucker and the PARAFAC. We have that the new\nfactorization is more parsimonious than the Tucker decomposition and more\nflexible than the PARAFAC. Moreover, this methodology applied to both simulated\nand empirical data shows results which are robust to two non-parametric tests,\nnamely Kruskal-Wallis and Kolmogorov-Smirnov tests. Since the resulting\ncorrelation matrix features stability and emerging structural dependency\nproperties, it can be used as alternative to other correlation matrices type of\nmeasures, including the Person correlation.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.06126v2"
    },
    {
        "title": "Uncovering the mesoscale structure of the credit default swap market to\n  improve portfolio risk modelling",
        "authors": [
            "Ioannis Anagnostou",
            "Tiziano Squartini",
            "Drona Kandhai",
            "Diego Garlaschelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  One of the most challenging aspects in the analysis and modelling of\nfinancial markets, including Credit Default Swap (CDS) markets, is the presence\nof an emergent, intermediate level of structure standing in between the\nmicroscopic dynamics of individual financial entities and the macroscopic\ndynamics of the market as a whole. This elusive, mesoscopic level of\norganisation is often sought for via factor models that ultimately decompose\nthe market according to geographic regions and economic industries. However, at\na more general level the presence of mesoscopic structure might be revealed in\nan entirely data-driven approach, looking for a modular and possibly\nhierarchical organisation of the empirical correlation matrix between financial\ntime series. The crucial ingredient in such an approach is the definition of an\nappropriate null model for the correlation matrix. Recent research showed that\ncommunity detection techniques developed for networks become intrinsically\nbiased when applied to correlation matrices. For this reason, a method based on\nRandom Matrix Theory has been developed, which identifies the optimal\nhierarchical decomposition of the system into internally correlated and\nmutually anti-correlated communities. Building upon this technique, here we\nresolve the mesoscopic structure of the CDS market and identify groups of\nissuers that cannot be traced back to standard industry/region taxonomies,\nthereby being inaccessible to standard factor models. We use this decomposition\nto introduce a novel default risk model that is shown to outperform more\ntraditional alternatives.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.03014v2"
    },
    {
        "title": "Forecasting volatility with a stacked model based on a hybridized\n  Artificial Neural Network",
        "authors": [
            "E. Ramos-Pérez",
            "P. J. Alonso-González",
            "J. J. Núñez-Velázquez"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  An appropriate calibration and forecasting of volatility and market risk are\nsome of the main challenges faced by companies that have to manage the\nuncertainty inherent to their investments or funding operations such as banks,\npension funds or insurance companies. This has become even more evident after\nthe 2007-2008 Financial Crisis, when the forecasting models assessing the\nmarket risk and volatility failed. Since then, a significant number of\ntheoretical developments and methodologies have appeared to improve the\naccuracy of the volatility forecasts and market risk assessments. Following\nthis line of thinking, this paper introduces a model based on using a set of\nMachine Learning techniques, such as Gradient Descent Boosting, Random Forest,\nSupport Vector Machine and Artificial Neural Network, where those algorithms\nare stacked to predict S&P500 volatility. The results suggest that our\nconstruction outperforms other habitual models on the ability to forecast the\nlevel of volatility, leading to a more accurate assessment of the market risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.16383v2"
    },
    {
        "title": "Every Corporation Owns Its Image: Corporate Credit Ratings via\n  Convolutional Neural Networks",
        "authors": [
            "Bojing Feng",
            "Wenfang Xue",
            "Bindang Xue",
            "Zeyu Liu"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Credit rating is an analysis of the credit risks associated with a\ncorporation, which reflect the level of the riskiness and reliability in\ninvesting. There have emerged many studies that implement machine learning\ntechniques to deal with corporate credit rating. However, the ability of these\nmodels is limited by enormous amounts of data from financial statement reports.\nIn this work, we analyze the performance of traditional machine learning models\nin predicting corporate credit rating. For utilizing the powerful convolutional\nneural networks and enormous financial data, we propose a novel end-to-end\nmethod, Corporate Credit Ratings via Convolutional Neural Networks, CCR-CNN for\nbrevity. In the proposed model, each corporation is transformed into an image.\nBased on this image, CNN can capture complex feature interactions of data,\nwhich are difficult to be revealed by previous machine learning models.\nExtensive experiments conducted on the Chinese public-listed corporate rating\ndataset which we build, prove that CCR-CNN outperforms the state-of-the-art\nmethods consistently.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.03744v1"
    },
    {
        "title": "Dynamic Reinsurance in Discrete Time Minimizing the Insurer's Cost of\n  Capital",
        "authors": [
            "Alexander Glauner"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  In the classical static optimal reinsurance problem, the cost of capital for\nthe insurer's risk exposure determined by a monetary risk measure is minimized\nover the class of reinsurance treaties represented by increasing Lipschitz\nretained loss functions. In this paper, we consider a dynamic extension of this\nreinsurance problem in discrete time which can be viewed as a risk-sensitive\nMarkov Decision Process. The model allows for both insurance claims and premium\nincome to be stochastic and operates with general risk measures and premium\nprinciples. We derive the Bellman equation and show the existence of a\nMarkovian optimal reinsurance policy. Under an infinite planning horizon, the\nmodel is shown to be contractive and the optimal reinsurance policy to be\nstationary. The results are illustrated with examples where the optimal policy\ncan be determined explicitly.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.09648v1"
    },
    {
        "title": "Systemic Risk in Financial Networks: A Survey",
        "authors": [
            "Matthew O. Jackson",
            "Agathe Pernoud"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We provide an overview of the relationship between financial networks and\nsystemic risk. We present a taxonomy of different types of systemic risk,\ndifferentiating between direct externalities between financial organizations\n(e.g., defaults, correlated portfolios and firesales), and perceptions and\nfeedback effects (e.g., bank runs, credit freezes). We also discuss optimal\nregulation and bailouts, measurements of systemic risk and financial\ncentrality, choices by banks' regarding their portfolios and partnerships, and\nthe changing nature of financial networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.12702v1"
    },
    {
        "title": "Sequential Deep Learning for Credit Risk Monitoring with Tabular\n  Financial Data",
        "authors": [
            "Jillian M. Clements",
            "Di Xu",
            "Nooshin Yousefi",
            "Dmitry Efimov"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Machine learning plays an essential role in preventing financial losses in\nthe banking industry. Perhaps the most pertinent prediction task that can\nresult in billions of dollars in losses each year is the assessment of credit\nrisk (i.e., the risk of default on debt). Today, much of the gains from machine\nlearning to predict credit risk are driven by gradient boosted decision tree\nmodels. However, these gains begin to plateau without the addition of expensive\nnew data sources or highly engineered features. In this paper, we present our\nattempts to create a novel approach to assessing credit risk using deep\nlearning that does not rely on new model inputs. We propose a new credit card\ntransaction sampling technique to use with deep recurrent and causal\nconvolution-based neural networks that exploits long historical sequences of\nfinancial data without costly resource requirements. We show that our\nsequential deep learning approach using a temporal convolutional network\noutperformed the benchmark non-sequential tree-based model, achieving\nsignificant financial savings and earlier detection of credit risk. We also\ndemonstrate the potential for our approach to be used in a production\nenvironment, where our sampling technique allows for sequences to be stored\nefficiently in memory and used for fast online learning and inference.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15330v1"
    },
    {
        "title": "Liquidity Stress Testing using Optimal Portfolio Liquidation",
        "authors": [
            "Mike Weber",
            "Iuliia Manziuk",
            "Bastien Baldacci"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We build an optimal portfolio liquidation model for OTC markets, aiming at\nminimizing the trading costs via the choice of the liquidation time. We work in\nthe Locally Linear Order Book framework of \\cite{toth2011anomalous} to obtain\nthe market impact as a function of the traded volume. We find that the optimal\nterminal time for a linear execution of a small order is proportional to the\nsquare root of the ratio between the amount being bought or sold and the\naverage daily volume. Numerical experiments on real market data illustrate the\nmethod on a portfolio of corporate bonds.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.02877v1"
    },
    {
        "title": "Conditional Value at Risk and Partial Moments for the Metalog\n  Distributions",
        "authors": [
            "Valentyn Khokhlov"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  The metalog distributions represent a convenient way to approach many\npractical applications. Their distinctive feature is simple closed-form\nexpressions for quantile functions. This paper contributes to further\ndevelopment of the metalog distributions by deriving the closed-form\nexpressions for the Conditional Value at Risk, a risk measure that is closely\nrelated to the tail conditional expectations. It also addressed the derivation\nof the first-order partial moments and shows that they are convex with respect\nto the vector of the metalog distribution parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10999v1"
    },
    {
        "title": "Multivariate higher order moments in multi-state life insurance",
        "authors": [
            "Jamaal Ahmad"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  It is well-known that combining life annuities and death benefits introduce\nopposite effects in payments with respect to the mortality risk on the lifetime\nof the insured. In a general multi-state framework with multiple product types,\nsuch joint effects are less trivial. In this paper, we consider a multivariate\npayment process in multi-state life insurance, where the components are defined\nin terms of the same Markovian state process. The multivariate present value of\nfuture payments is introduced, and we derive differential equations and product\nintegral representations of its conditional moments and moment generating\nfunction. Special attention is given to pair-wise covariances between two\npresent values, where results closely connected to Hattendorff type of results\nfor the variance are derived. The results are illustrated in a numerical\nexample in a disability model.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.11714v2"
    },
    {
        "title": "Explainable AI in Credit Risk Management",
        "authors": [
            "Branka Hadji Misheva",
            "Joerg Osterrieder",
            "Ali Hirsa",
            "Onkar Kulkarni",
            "Stephen Fung Lin"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Artificial Intelligence (AI) has created the single biggest technology\nrevolution the world has ever seen. For the finance sector, it provides great\nopportunities to enhance customer experience, democratize financial services,\nensure consumer protection and significantly improve risk management. While it\nis easier than ever to run state-of-the-art machine learning models, designing\nand implementing systems that support real-world finance applications have been\nchallenging. In large part because they lack transparency and explainability\nwhich are important factors in establishing reliable technology and the\nresearch on this topic with a specific focus on applications in credit risk\nmanagement. In this paper, we implement two advanced post-hoc model agnostic\nexplainability techniques called Local Interpretable Model Agnostic\nExplanations (LIME) and SHapley Additive exPlanations (SHAP) to machine\nlearning (ML)-based credit scoring models applied to the open-access data set\noffered by the US-based P2P Lending Platform, Lending Club. Specifically, we\nuse LIME to explain instances locally and SHAP to get both local and global\nexplanations. We discuss the results in detail and present multiple comparison\nscenarios by using various kernels available for explaining graphs generated\nusing SHAP values. We also discuss the practical challenges associated with the\nimplementation of these state-of-art eXplainabale AI (XAI) methods and document\nthem for future reference. We have made an effort to document every technical\naspect of this research, while at the same time providing a general summary of\nthe conclusions.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.00949v1"
    },
    {
        "title": "A delayed dual risk model",
        "authors": [
            "Lingjiong Zhu"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In this paper, we study a dual risk model with delays in the spirit of\nDassios-Zhao. When a new innovation occurs, there is a delay before the\ninnovation turns into a profit. We obtain large initial surplus asymptotics for\nthe ruin probability and ruin time distributions. For some special cases, we\nget closed-form formulas. Numerical illustrations will also be provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.06450v1"
    },
    {
        "title": "The Probability Conflation: A Reply",
        "authors": [
            "Nassim Nicholas Taleb",
            "Ron Richman",
            "Marcos Carreira",
            "James Sharpe"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We respond to Tetlock et al. (2022) showing 1) how expert judgment fails to\nreflect tail risk, 2) the lack of compatibility between forecasting tournaments\nand tail risk assessment methods (such as extreme value theory). More\nimportantly, we communicate a new result showing a greater gap between the\nproperties of tail expectation and those of the corresponding probability.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.10985v1"
    },
    {
        "title": "A Goal Programming Model with Satisfaction Function for Risk Management\n  and Optimal Portfolio Diversification",
        "authors": [
            "Davide La Torre",
            "Marco Maggis"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We extend the classical risk minimization model with scalar risk measures to\nthe general case of set-valued risk measures. The problem we obtain is a\nset-valued optimization model and we propose a goal programming-based approach\nwith satisfaction function to obtain a solution which represents the best\ncompromise between goals and the achievement levels. Numerical examples are\nprovided to illustrate how the method works in practical situations.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1783v2"
    },
    {
        "title": "Complete duality for quasiconvex dynamic risk measures on modules of the\n  $L^{p}$-type",
        "authors": [
            "Marco Frittelli",
            "Marco Maggis"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  In the conditional setting we provide a complete duality between quasiconvex\nrisk measures defined on $L^{0}$ modules of the $L^{p}$ type and the\nappropriate class of dual functions. This is based on a general result which\nextends the usual Penot-Volle representation for quasiconvex real valued maps.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1788v2"
    },
    {
        "title": "Risk Measures on $\\mathcal{P}(\\mathbb{R})$ and Value At Risk with\n  Probability/Loss function",
        "authors": [
            "Marco Frittelli",
            "Marco Maggis",
            "Ilaria Peri"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We propose a generalization of the classical notion of the $V@R_{\\lambda}$\nthat takes into account not only the probability of the losses, but the balance\nbetween such probability and the amount of the loss. This is obtained by\ndefining a new class of law invariant risk measures based on an appropriate\nfamily of acceptance sets. The $V@R_{\\lambda}$ and other known law invariant\nrisk measures turn out to be special cases of our proposal. We further prove\nthe dual representation of Risk Measures on $\\mathcal{P}(% \\mathbb{R}).$\n",
        "pdf_link": "http://arxiv.org/pdf/1201.2257v4"
    },
    {
        "title": "Quasi self-dual exponential Lévy processes",
        "authors": [
            "Thorsten Rheinländer",
            "Michael Schmutz"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  The important application of semi-static hedging in financial markets\nnaturally leads to the notion of quasi self-dual processes. The focus of our\nstudy is to give new characterizations of quasi self-duality for exponential\nL\\'evy processes such that the resulting market does not admit arbitrage\nopportunities. We derive a set of equivalent conditions for the stochastic\nlogarithm of quasi self-dual martingale models and derive a further\ncharacterization of these models not depending on the L\\'evy-Khintchine\nparametrization. Since for non-vanishing order parameter two martingale\nproperties have to be satisfied simultaneously, there is a non-trivial relation\nbetween the order and shift parameter representing carrying costs in financial\napplications. This leads to an equation containing an integral term which has\nto be inverted in applications. We first discuss several important properties\nof this equation and, for some well-known models, we derive a family of\nclosed-form inversion formulae leading to parameterizations of sets of possible\ncombinations in the corresponding parameter spaces of well-known L\\'evy driven\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.5132v1"
    },
    {
        "title": "Capital adequacy tests and limited liability of financial institutions",
        "authors": [
            "Pablo Koch-Medina",
            "Santiago Moreno-Bromberg",
            "Cosimo Munari"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  The theory of acceptance sets and their associated risk measures plays a key\nrole in the design of capital adequacy tests. The objective of this paper is to\ninvestigate, in the context of bounded financial positions, the class of\nsurplus-invariant acceptance sets. These are characterized by the fact that\nacceptability does not depend on the positive part, or surplus, of a capital\nposition. We argue that surplus invariance is a reasonable requirement from a\nregulatory perspective, because it focuses on the interests of liability\nholders of a financial institution. We provide a dual characterization of\nsurplus-invariant, convex acceptance sets, and show that the combination of\nsurplus invariance and coherence leads to a narrow range of capital adequacy\ntests, essentially limited to scenario-based tests. Finally, we emphasize the\nadvantages of dealing with surplus-invariant acceptance sets as the primary\nobject rather than directly with risk measures, such as loss-based and\nexcess-invariant risk measures, which have been recently studied by Cont,\nDeguest, and He (2013) and by Staum (2013), respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.3133v2"
    },
    {
        "title": "Stochastic model of a pension plan",
        "authors": [
            "Paz Grimberg",
            "Zeev Schuss"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  Structuring a viable pension plan is a problem that arises in the study of\nfinancial contracts pricing and bears special importance these days.\nDeterministic pension models often rely on projections that are based on\nseveral assumptions concerning the \"average\" long-time behavior of the stock\nmarket. Our aim here is to examine some of the popular \"average\" assumptions in\na more realistic setting of a stochastic model. Thus, we examine the contention\nthat investment in the stock market is similar to gambling in a casino, while\npurchasing companies, after due diligence, is safer under the premise that\nacting as a holding company that wholly owns other companies avoids some of the\nstock market risks. We show that the stock market index faithfully reflects its\ncompanies' profits at the time of their publication. We compare the shifted\nhistorical dynamics of the S\\&P500's aggregated financial earnings to its\nvalue, and find a high degree of correlation. We conclude that there is no\nbenefit to a pension fund in wholly owning a super trust. We verify, by\nexamining historical data, that stock earnings follow an exponential\n(geometric) Brownian motion and estimate its parameters. The robustness of this\nmodel is examined by an estimate of a pensioner's accumulated assets over a\nsaving period. We also estimate the survival probability and mean survival time\nof the accumulated individual fund with pension consumption over the residual\nlife of the pensioner.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.0517v1"
    },
    {
        "title": "Classical mechanics of economic networks",
        "authors": [
            "Nima Dehmamy",
            "Sergey V. Buldyrev",
            "Shlomo Havlin",
            "H. Eugene Stanley",
            "Irena Vodenska"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  Financial networks are dynamic. To assess their systemic importance to the\nworld-wide economic network and avert losses we need models that take the time\nvariations of the links and nodes into account. Using the methodology of\nclassical mechanics and Laplacian determinism we develop a model that can\npredict the response of the financial network to a shock. We also propose a way\nof measuring the systemic importance of the banks, which we call BankRank.\nUsing European Bank Authority 2011 stress test exposure data, we apply our\nmodel to the bipartite network connecting the largest institutional debt\nholders of the troubled European countries (Greece, Italy, Portugal, Spain, and\nIreland). From simulating our model we can determine whether a network is in a\n\"stable\" state in which shocks do not cause major losses, or a \"unstable\" state\nin which devastating damages occur. Fitting the parameters of the model, which\nplay the role of physical coupling constants, to Eurozone crisis data shows\nthat before the Eurozone crisis the system was mostly in a \"stable\" regime, and\nthat during the crisis it transitioned into an \"unstable\" regime. The numerical\nsolutions produced by our model match closely the actual time-line of events of\nthe crisis. We also find that, while the largest holders are usually more\nimportant, in the unstable regime smaller holders also exhibit systemic\nimportance. Our model also proves useful for determining the vulnerability of\nbanks and assets to shocks. This suggests that our model may be a useful tool\nfor simulating the response dynamics of shared portfolio networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.0104v2"
    },
    {
        "title": "Impact of shadow banks on financial contagion",
        "authors": [
            "Yoshiharu Maeno",
            "Kenji Nishiguchi",
            "Satoshi Morinaga",
            "Hirokazu Matsushima"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  An asset network systemic risk (ANWSER) model is presented to investigate the\nimpact of how shadow banks are intermingled in a financial system on the\nseverity of financial contagion. Particularly, the focus of this study is the\nimpact of the following three representative topologies of an interbank loan\nnetwork between shadow banks and regulated banks. (1) Random mixing network:\nshadow banks and regulated banks are intermingled randomly. (2)\nAsset-correlated mixing network: banks having bigger assets are a regulated\nbank and other banks are shadow banks. (3) Layered mixing network: banks in a\nshadow bank layer are connected to banks in a regulated bank layer with some\ninterbank loans.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4847v1"
    },
    {
        "title": "qGaussian model of default",
        "authors": [
            "Yuri A. Katz"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We present the qGaussian generalization of the Merton framework, which takes\ninto account slow fluctuations of the volatility of the firms market value of\nfinancial assets. The minimal version of the model depends on the Tsallis\nentropic parameter q and the generalized distance to default. The empirical\nfoundation and implications of the model are illustrated by the study of 645\nNorth American industrial firms during the financial crisis, 2006 - 2012. All\ndefaulters in the sample have exceptionally large, corresponding to unusually\nfat-tailed unconditional distributions of log-asset-returns. Using Receiver\nOperating Characteristic curves, we demonstrate the high forecasting power of\nthe model in prediction of 1-year defaults. Our study suggests that the level\nof complexity of the realized time series, quantified by q, should be taken\ninto account to improve valuations of default risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.6841v1"
    },
    {
        "title": "Multivariate Stop loss Mixed Erlang Reinsurance risk: Aggregation,\n  Capital allocation and Default risk",
        "authors": [
            "Gildas Ratovomirija"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In this paper, we address the aggregation of dependent stop loss reinsurance\nrisks where the dependence among the ceding insurer(s) risks is governed by the\nSarmanov distribution and each individual risk belongs to the class of Erlang\nmixtures. We investigate the effects of the ceding insurer(s) risk dependencies\non the reinsurer risk profile by deriving a closed formula for the distribution\nfunction of the aggregated stop loss reinsurance risk. Furthermore,\ndiversification effects from aggregating reinsurance risks are examined by\nderiving a closed expression for the risk capital needed for the whole\nportfolio of the reinsurer and also the allocated risk capital for each\nbusiness unit under the TVaR capital allocation principle. Moreover, given the\nrisk capital that the reinsurer holds, we express the default probability of\nthe reinsurer analytically. In case the reinsurer is in default, we determine\nanalytical expressions for the amount of the aggregate reinsured unpaid losses\nand the unpaid losses of each reinsured line of business of the ceding\ninsurer(s). These results are illustrated by numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.07297v1"
    },
    {
        "title": "On bivariate lifetime modelling in life insurance applications",
        "authors": [
            "François Dufresne",
            "Enkelejd Hashorva",
            "Gildas Ratovomirija",
            "Youssouf Toukourou"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Insurance and annuity products covering several lives require the modelling\nof the joint distribution of future lifetimes. In the interest of simplifying\ncalculations, it is common in practice to assume that the future lifetimes\namong a group of people are independent. However, extensive research over the\npast decades suggests otherwise. In this paper, a copula approach is used to\nmodel the dependence between lifetimes within a married couple \\eH{using data\nfrom a large Canadian insurance company}. As a novelty, the age difference and\nthe \\eH{gender} of the elder partner are introduced as an argument of the\ndependence parameter. \\green{Maximum likelihood techniques are} thus\nimplemented for the parameter estimation. Not only do the results make clear\nthat the correlation decreases with age difference, but also the dependence\nbetween the lifetimes is higher when husband is older than wife. A\ngoodness-of-fit procedure is applied in order to assess the validity of the\nmodel. Finally, considering several products available on the life insurance\nmarket, the paper concludes with practical illustrations.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04351v1"
    },
    {
        "title": "Robust Optimal Risk Sharing and Risk Premia in Expanding Pools",
        "authors": [
            "Thomas Knispel",
            "Roger J. A. Laeven",
            "Gregor Svindland"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  We consider the problem of optimal risk sharing in a pool of cooperative\nagents. We analyze the asymptotic behavior of the certainty equivalents and\nrisk premia associated with the Pareto optimal risk sharing contract as the\npool expands. We first study this problem under expected utility preferences\nwith an objectively or subjectively given probabilistic model. Next, we develop\na robust approach by explicitly taking uncertainty about the probabilistic\nmodel (ambiguity) into account. The resulting robust certainty equivalents and\nrisk premia compound risk and ambiguity aversion. We provide explicit results\non their limits and rates of convergence, induced by Pareto optimal risk\nsharing in expanding pools.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.06979v1"
    },
    {
        "title": "The Value of A Statistical Life in Absence of Panel Data: What can we\n  do?",
        "authors": [
            "Andrés Riquelme",
            "Marcela Parada"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  In this paper I show how reliable estimates of the Value of a Statistical\nLife (VSL) can be obtained using cross sectional data using Garen's\ninstrumental variable (IV) approach. The increase in the range confidence\nintervals due to the IV setup can be reduced by a factor of 3 by using a proxy\nto risk attitude. In order state the \"precision\" of the cross sectional VSL\nestimates I estimate the VSL using Chilean panel data and use them as benchmark\nfor different cross sectional specifications. The use of the proxy eliminates\nneed for using hard-to-find instruments for the job risk level and narrows the\nconfidence intervals for the workers in the Chilean labor market for the year\n2009.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00568v1"
    },
    {
        "title": "Unbiased estimation of risk",
        "authors": [
            "Marcin Pitera",
            "Thorsten Schmidt"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  The estimation of risk measures recently gained a lot of attention, partly\nbecause of the backtesting issues of expected shortfall related to\nelicitability. In this work we shed a new and fundamental light on optimal\nestimation procedures of risk measures in terms of bias. We show that once the\nparameters of a model need to be estimated, one has to take additional care\nwhen estimating risks. The typical plug-in approach, for example, introduces a\nbias which leads to a systematic underestimation of risk. In this regard, we\nintroduce a novel notion of unbiasedness to the estimation of risk which is\nmotivated by economic principles. In general, the proposed concept does not\ncoincide with the well-known statistical notion of unbiasedness. We show that\nan appropriate bias correction is available for many well-known estimators. In\nparticular, we consider value-at-risk and expected shortfall (tail\nvalue-at-risk). In the special case of normal distributions, closed-formed\nsolutions for unbiased estimators can be obtained. We present a number of\nmotivating examples which show the outperformance of unbiased estimators in\nmany circumstances. The unbiasedness has a direct impact on backtesting and\ntherefore adds a further viewpoint to established statistical properties.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.02615v4"
    },
    {
        "title": "Financial contagion in investment funds",
        "authors": [
            "Leonardo dos Santos Pinheiro",
            "Flavio Codeco Coelho"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Many new models for measuring financial contagion have been presented\nrecently. While these models have not been specified for investment funds\ndirectly, there are many similarities that could be explored to extend the\nmodels. In this work we explore ideas developed about financial contagion to\ncreate a network of investment funds using both cross-holding of quotas and a\nbipartite network of funds and assets. Using data from the Brazilian asset\nmanagement market we analyze not only the contagion pattern but also the\nstructure of this network and how this model can be used to assess the\nstability of the market.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.03458v1"
    },
    {
        "title": "Value-at-Risk: The Effect of Autoregression in a Quantile Process",
        "authors": [
            "Khizar Qureshi"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Value-at-Risk (VaR) is an institutional measure of risk favored by financial\nregulators. VaR may be interpreted as a quantile of future portfolio values\nconditional on the information available, where the most common quantile used\nis 95%. Here we demonstrate Conditional Autoregressive Value at Risk, first\nintroduced by Engle, Manganelli (2001). CAViaR suggests that negative/positive\nreturns are not i.i.d., and that there is significant autocorrelation. The\nmodel is tested using data from 1986- 1999 and 1999-2009 for GM, IBM, XOM, SPX,\nand then validated via the dynamic quantile test. Results suggest that the\ntails (upper/lower quantile) of a distribution of returns behave differently\nthan the core.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.04940v1"
    },
    {
        "title": "Computational aspects of robust optimized certainty equivalents and\n  option pricing",
        "authors": [
            "Daniel Bartl",
            "Samuel Drapeau",
            "Ludovic Tangpi"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Accounting for model uncertainty in risk management and option pricing leads\nto infinite dimensional optimization problems which are both analytically and\nnumerically intractable. In this article we study when this hurdle can be\novercome for the so-called optimized certainty equivalent risk measure (OCE) --\nincluding the average value-at-risk as a special case. First we focus on the\ncase where the uncertainty is modeled by a nonlinear expectation penalizing\ndistributions that are \"far\" in terms of optimal-transport distance\n(Wasserstein distance for instance) from a given baseline distribution. It\nturns out that the computation of the robust OCE reduces to a finite\ndimensional problem, which in some cases can even be solved explicitly. This\nprinciple also applies to the shortfall risk measure as well as for the pricing\nof European options. Further, we derive convex dual representations of the\nrobust OCE for measurable claims without any assumptions on the set of\ndistributions. Finally, we give conditions on the latter set under which the\nrobust average value-at-risk is a tail risk measure.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.10186v3"
    },
    {
        "title": "Loss Data Analytics",
        "authors": [
            "Edward Frees"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Loss Data Analytics is an interactive, online, freely available text. The\nidea behind the name Loss Data Analytics is to integrate classical loss data\nmodels from applied probability with modern analytic tools. In particular, we\nseek to recognize that big data (including social media and usage based\ninsurance) are here and high speed computation is readily available.\n  The online version contains many interactive objects (quizzes, computer\ndemonstrations, interactive graphs, video, and the like) to promote deeper\nlearning. A subset of the book is available for offline reading in pdf and EPUB\nformats. The online text will be available in multiple languages to promote\naccess to a worldwide audience.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.06718v1"
    },
    {
        "title": "Suboptimal Control of Dividends under Exponential Utility",
        "authors": [
            "Julia Eisenberg",
            "Paul Krühner"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We consider an insurance company modelling its surplus process by a Brownian\nmotion with drift. Our target is to maximise the expected exponential utility\nof discounted dividend payments, given that the dividend rates are bounded by\nsome constant. The utility function destroys the linearity and the time\nhomogeneity of the considered problem. The value function depends not only on\nthe surplus, but also on time. Numerical considerations suggest that the\noptimal strategy, if it exists, is of a barrier type with a non-linear barrier.\nIn the related article by granditz et al., it has been observed that standard\nnumerical methods break down in certain parameter cases and no close form\nsolution has been found.\n  For these reasons, we offer a new method allowing to estimate the distance of\nan arbitrary smooth enough function to the value function. Applying this\nmethod, we investigate the goodness of the most obvious suboptimal strategies -\npayout on the maximal rate, and constant barrier strategies - by measuring the\ndistance of its performance function to the value function.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01983v2"
    },
    {
        "title": "The distortion principle for insurance pricing: properties,\n  identification and robustness",
        "authors": [
            "Daniela Escobar",
            "Georg Pflug"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Distortion (Denneberg 1990) is a well known premium calculation principle for\ninsurance contracts. In this paper, we study sensitivity properties of\ndistortion functionals w.r.t. the assumptions for risk aversion as well as\nrobustness w.r.t. ambiguity of the loss distribution. Ambiguity is measured by\nthe Wasserstein distance. We study variances of distances for probability\nmodels and identify some worst case distributions. In addition to the direct\nproblem we also investigate the inverse problem, that is how to identify the\ndistortion density on the basis of observations of insurance premia.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.06592v1"
    },
    {
        "title": "Capturing Model Risk and Rating Momentum in the Estimation of\n  Probabilities of Default and Credit Rating Migrations",
        "authors": [
            "Marius Pfeuffer",
            "Goncalo dos Reis",
            "Greig smith"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We present two methodologies on the estimation of rating transition\nprobabilities within Markov and non-Markov frameworks. We first estimate a\ncontinuous-time Markov chain using discrete (missing) data and derive a simpler\nexpression for the Fisher information matrix, reducing the computational time\nneeded for the Wald confidence interval by a factor of a half. We provide an\nefficient procedure for transferring such uncertainties from the generator\nmatrix of the Markov chain to the corresponding rating migration probabilities\nand, crucially, default probabilities.\n  For our second contribution, we assume access to the full (continuous) data\nset and propose a tractable and parsimonious self-exciting marked point\nprocesses model able to capture the non-Markovian effect of rating momentum.\nCompared to the Markov model, the non-Markov model yields higher probabilities\nof default in the investment grades, but also lower default probabilities in\nsome speculative grades. Both findings agree with empirical observations and\nhave clear practical implications.\n  We illustrate all methods using data from Moody's proprietary corporate\ncredit ratings data set. Implementations are available in the R package ctmcd.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.09889v2"
    },
    {
        "title": "Representation Results for Law Invariant Recursive Dynamic Deviation\n  Measures and Risk Sharing",
        "authors": [
            "Mitja Stadje"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this paper we analyze a dynamic recursive extension of the (static) notion\nof a deviation measure and its properties. We study distribution invariant\ndeviation measures and show that the only dynamic deviation measure which is\nlaw invariant and recursive is the variance. We also solve the problem of\noptimal risk-sharing generalizing classical risk-sharing results for variance\nthrough a dynamic inf-convolution problem involving a transformation of the\noriginal dynamic deviation measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.09615v2"
    },
    {
        "title": "Systemic Risk: Conditional Distortion Risk Measures",
        "authors": [
            "Jan Dhaene",
            "Roger J. A. Laeven",
            "Yiying Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  In this paper, we introduce the rich classes of conditional distortion (CoD)\nrisk measures and distortion risk contribution ($\\Delta$CoD) measures as\nmeasures of systemic risk and analyze their properties and representations. The\nclasses include the well-known conditional Value-at-Risk, conditional Expected\nShortfall, and risk contribution measures in terms of the VaR and ES as special\ncases. Sufficient conditions are presented for two random vectors to be ordered\nby the proposed CoD-risk measures and distortion risk contribution measures.\nThese conditions are expressed using the conventional stochastic dominance,\nincreasing convex/concave, dispersive, and excess wealth orders of the\nmarginals and canonical positive/negative stochastic dependence notions.\nNumerical examples are provided to illustrate our theoretical findings. This\npaper is the second in a triplet of papers on systemic risk by the same\nauthors. In \\cite{DLZorder2018a}, we introduce and analyze some new stochastic\norders related to systemic risk. In a third (forthcoming) paper, we attribute\nsystemic risk to the different participants in a given risky environment.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04689v2"
    },
    {
        "title": "Mathematical Analysis of Dynamic Risk Default in Microfinance",
        "authors": [
            "Mohammed Kaicer",
            "Abdelilah Kaddar"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  In this work we will develop a new approach to solve the non repayment\nproblem in microfinance due to the problem of asymmetric information. This\napproach is based on modeling and simulation of ordinary differential systems\nwhere time remains a primordial component, they thus enable microfinance\ninstitutions to manage their risk portfolios by a prediction of numbers of\nsolvent and insolvent borrowers ever a period, in order to define or redefine\nits development strategy, investment and management in an area, where the\npopulation is often poor and in need a mechanism of financial inclusion.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04937v1"
    },
    {
        "title": "Multivariate Modeling of Natural Gas Spot Trading Hubs Incorporating\n  Futures Market Realized Volatility",
        "authors": [
            "Michael Weylandt",
            "Yu Han",
            "Katherine B. Ensor"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Financial markets for Liquified Natural Gas (LNG) are an important and\nrapidly-growing segment of commodities markets. Like other commodities markets,\nthere is an inherent spatial structure to LNG markets, with different price\ndynamics for different points of delivery hubs. Certain hubs support highly\nliquid markets, allowing efficient and robust price discovery, while others are\nhighly illiquid, limiting the effectiveness of standard risk management\ntechniques. We propose a joint modeling strategy, which uses high-frequency\ninformation from thickly-traded hubs to improve volatility estimation and risk\nmanagement at thinly traded hubs. The resulting model has superior in- and\nout-of-sample predictive performance, particularly for several commonly used\nrisk management metrics, demonstrating that joint modeling is indeed possible\nand useful. To improve estimation, a Bayesian estimation strategy is employed\nand data-driven weakly informative priors are suggested. Our model is robust to\nsparse data and can be effectively used in any market with similar irregular\npatterns of data availability.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.10152v1"
    },
    {
        "title": "Portfolio Optimization with Expectile and Omega Functions",
        "authors": [
            "Alexander Wagner",
            "Stan Uryasev"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  This paper proves equivalences of portfolio optimization problems with\nnegative expectile and omega ratio. We derive subgradients for the negative\nexpectile as a function of the portfolio from a known dual representation of\nexpectile and general theory about subgradients of risk measures. We also give\nan elementary derivation of the gradient of negative expectile under some\nassumptions and provide an example where negative expectile is demonstrably not\ndifferentiable. We conducted a case study and solved portfolio optimization\nproblems with negative expectile objective and constraint.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.14005v1"
    },
    {
        "title": "Non-asymptotic convergence rates for the plug-in estimation of risk\n  measures",
        "authors": [
            "Daniel Bartl",
            "Ludovic Tangpi"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Let $\\rho$ be a general law--invariant convex risk measure, for instance the\naverage value at risk, and let $X$ be a financial loss, that is, a real random\nvariable. In practice, either the true distribution $\\mu$ of $X$ is unknown, or\nthe numerical computation of $\\rho(\\mu)$ is not possible. In both cases, either\nrelying on historical data or using a Monte-Carlo approach, one can resort to\nan i.i.d.\\ sample of $\\mu$ to approximate $\\rho(\\mu)$ by the finite sample\nestimator $\\rho(\\mu_N)$ (where $\\mu_N$ denotes the empirical measure of $\\mu$).\nIn this article we investigate convergence rates of $\\rho(\\mu_N)$ to\n$\\rho(\\mu)$. We provide non-asymptotic convergence rates for both the deviation\nprobability and the expectation of the estimation error. The sharpness of these\nconvergence rates is analyzed. Our framework further allows for hedging, and\nthe convergence rates we obtain depend neither on the dimension of the\nunderlying assets, nor on the number of options available for trading.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10479v2"
    },
    {
        "title": "Towards Explainability of Machine Learning Models in Insurance Pricing",
        "authors": [
            "Kevin Kuo",
            "Daniel Lupton"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Machine learning methods have garnered increasing interest among actuaries in\nrecent years. However, their adoption by practitioners has been limited, partly\ndue to the lack of transparency of these methods, as compared to generalized\nlinear models. In this paper, we discuss the need for model interpretability in\nproperty & casualty insurance ratemaking, propose a framework for explaining\nmodels, and present a case study to illustrate the framework.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10674v1"
    },
    {
        "title": "Modelling and understanding count processes through a Markov-modulated\n  non-homogeneous Poisson process framework",
        "authors": [
            "Benjamin Avanzi",
            "Greg Taylor",
            "Bernard Wong",
            "Alan Xian"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  The Markov-modulated Poisson process is utilised for count modelling in a\nvariety of areas such as queueing, reliability, network and insurance claims\nanalysis. In this paper, we extend the Markov-modulated Poisson process\nframework through the introduction of a flexible frequency perturbation\nmeasure. This contribution enables known information of observed event arrivals\nto be naturally incorporated in a tractable manner, while the hidden Markov\nchain captures the effect of unobservable drivers of the data. In addition to\nincreases in accuracy and interpretability, this method supplements analysis of\nthe latent factors. Further, this procedure naturally incorporates data\nfeatures such as over-dispersion and autocorrelation. Additional insights can\nbe generated to assist analysis, including a procedure for iterative model\nimprovement.\n  Implementation difficulties are also addressed with a focus on dealing with\nlarge data sets, where latent models are especially advantageous due the large\nnumber of observations facilitating identification of hidden factors. Namely,\ncomputational issues such as numerical underflow and high processing cost arise\nin this context and in this paper, we produce procedures to overcome these\nproblems.\n  This modelling framework is demonstrated using a large insurance data set to\nillustrate theoretical, practical and computational contributions and an\nempirical comparison to other count models highlight the advantages of the\nproposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13888v2"
    },
    {
        "title": "A multivariate evolutionary generalised linear model framework with\n  adaptive estimation for claims reserving",
        "authors": [
            "Benjamin Avanzi",
            "Gregory Clive Taylor",
            "Phuong Anh Vu",
            "Bernard Wong"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  In this paper, we develop a multivariate evolutionary generalised linear\nmodel (GLM) framework for claims reserving, which allows for dynamic features\nof claims activity in conjunction with dependency across business lines to\naccurately assess claims reserves. We extend the traditional GLM reserving\nframework on two fronts: GLM fixed factors are allowed to evolve in a recursive\nmanner, and dependence is incorporated in the specification of these factors\nusing a common shock approach.\n  We consider factors that evolve across accident years in conjunction with\nfactors that evolve across calendar years. This two-dimensional evolution of\nfactors is unconventional as a traditional evolutionary model typically\nconsiders the evolution in one single time dimension. This creates challenges\nfor the estimation process, which we tackle in this paper. We develop the\nformulation of a particle filtering algorithm with parameter learning\nprocedure. This is an adaptive estimation approach which updates evolving\nfactors of the framework recursively over time.\n  We implement and illustrate our model with a simulated data set, as well as a\nset of real data from a Canadian insurer.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06880v1"
    },
    {
        "title": "Characterizing the memory capacity of transmon qubit reservoirs",
        "authors": [
            "Samudra Dasgupta",
            "Kathleen E. Hamilton",
            "Arnab Banerjee"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Quantum Reservoir Computing (QRC) exploits the dynamics of quantum ensemble\nsystems for machine learning. Numerical experiments show that quantum systems\nconsisting of 5-7 qubits possess computational capabilities comparable to\nconventional recurrent neural networks of 100 to 500 nodes. Unlike traditional\nneural networks, we do not understand the guiding principles of reservoir\ndesign for high-performance information processing. Understanding the memory\ncapacity of quantum reservoirs continues to be an open question. In this study,\nwe focus on the task of characterizing the memory capacity of quantum\nreservoirs built using transmon devices provided by IBM. Our hybrid reservoir\nachieved a Normalized Mean Square Error (NMSE) of 6x10^{-4} which is comparable\nto recent benchmarks. The Memory Capacity characterization of a n-qubit\nreservoir showed a systematic variation with the complexity of the topology and\nexhibited a peak for the configuration with n-1 self-loops. Such a peak\nprovides a basis for selecting the optimal design for forecasting tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.08240v7"
    },
    {
        "title": "On the modelling of multivariate counts with Cox processes and dependent\n  shot noise intensities",
        "authors": [
            "Benjamin Avanzi",
            "Gregory Clive Taylor",
            "Bernard Wong",
            "Xinda Yang"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  In this paper, we develop a method to model and estimate several, _dependent_\ncount processes, using granular data. Specifically, we develop a multivariate\nCox process with shot noise intensities to jointly model the arrival process of\ncounts (e.g. insurance claims). The dependency structure is introduced via\nmultivariate shot noise _intensity_ processes which are connected with the help\nof L\\'evy copulas. In aggregate, our approach allows for (i) over-dispersion\nand auto-correlation within each line of business; (ii) realistic features\ninvolving time-varying, known covariates; and (iii) parsimonious dependence\nbetween processes without requiring simultaneous primary (e.g. accidents)\nevents.\n  The explicit incorporation of time-varying, known covariates can accommodate\ncharacteristics of real data and hence facilitate implementation in practice.\nIn an insurance context, these could be changes in policy volumes over time, as\nwell as seasonality patterns and trends, which may explain some of the\nrelationship (dependence) between multiple claims processes, or at least help\ntease out those relationships.\n  Finally, we develop a filtering algorithm based on the reversible-jump Markov\nChain Monte Carlo (RJMCMC) method to estimate the latent stochastic intensities\nand illustrate model calibration using real data from the AUSI data set.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11169v2"
    },
    {
        "title": "A Bivariate Compound Dynamic Contagion Process for Cyber Insurance",
        "authors": [
            "Jiwook Jang",
            "Rosy Oh"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  As corporates and governments become more digital, they become vulnerable to\nvarious forms of cyber attack. Cyber insurance products have been used as risk\nmanagement tools, yet their pricing does not reflect actual risk, including\nthat of multiple, catastrophic and contagious losses. For the modelling of\naggregate losses from cyber events, in this paper we introduce a bivariate\ncompound dynamic contagion process, where the bivariate dynamic contagion\nprocess is a point process that includes both externally excited joint jumps,\nwhich are distributed according to a shot noise Cox process and two separate\nself-excited jumps, which are distributed according to the branching structure\nof a Hawkes process with an exponential fertility rate, respectively. We\nanalyse the theoretical distributional properties for these processes\nsystematically, based on the piecewise deterministic Markov process developed\nby Davis (1984) and the univariate dynamic contagion process theory developed\nby Dassios and Zhao (2011). The analytic expression of the Laplace transform of\nthe compound process and its moments are presented, which have the potential to\nbe applicable to a variety of problems in credit, insurance, market and other\noperational risks. As an application of this process, we provide insurance\npremium calculations based on its moments. Numerical examples show that this\ncompound process can be used for the modelling of aggregate losses from cyber\nevents. We also provide the simulation algorithm for statistical analysis,\nfurther business applications and research.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04758v1"
    },
    {
        "title": "A comparative study of forecasting Corporate Credit Ratings using Neural\n  Networks, Support Vector Machines, and Decision Trees",
        "authors": [
            "Parisa Golbayani",
            "Ionuţ Florescu",
            "Rupak Chatterjee"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Credit ratings are one of the primary keys that reflect the level of\nriskiness and reliability of corporations to meet their financial obligations.\nRating agencies tend to take extended periods of time to provide new ratings\nand update older ones. Therefore, credit scoring assessments using artificial\nintelligence has gained a lot of interest in recent years. Successful machine\nlearning methods can provide rapid analysis of credit scores while updating\nolder ones on a daily time scale. Related studies have shown that neural\nnetworks and support vector machines outperform other techniques by providing\nbetter prediction accuracy. The purpose of this paper is two fold. First, we\nprovide a survey and a comparative analysis of results from literature applying\nmachine learning techniques to predict credit rating. Second, we apply\nourselves four machine learning techniques deemed useful from previous studies\n(Bagged Decision Trees, Random Forest, Support Vector Machine and Multilayer\nPerceptron) to the same datasets. We evaluate the results using a 10-fold cross\nvalidation technique. The results of the experiment for the datasets chosen\nshow superior performance for decision tree based models. In addition to the\nconventional accuracy measure of classifiers, we introduce a measure of\naccuracy based on notches called \"Notch Distance\" to analyze the performance of\nthe above classifiers in the specific context of credit rating. This measure\ntells us how far the predictions are from the true ratings. We further compare\nthe performance of three major rating agencies, Standard $\\&$ Poors, Moody's\nand Fitch where we show that the difference in their ratings is comparable with\nthe decision tree prediction versus the actual rating on the test dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.06617v1"
    },
    {
        "title": "Geometric Brownian Motion under Stochastic Resetting: A Stationary yet\n  Non-ergodic Process",
        "authors": [
            "Viktor Stojkoski",
            "Trifce Sandev",
            "Ljupco Kocarev",
            "Arnab Pal"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We study the effects of stochastic resetting on geometric Brownian motion\n(GBM), a canonical stochastic multiplicative process for non-stationary and\nnon-ergodic dynamics. Resetting is a sudden interruption of a process, which\nconsecutively renews its dynamics. We show that, although resetting renders GBM\nstationary, the resulting process remains non-ergodic. Quite surprisingly, the\neffect of resetting is pivotal in manifesting the non-ergodic behavior. In\nparticular, we observe three different long-time regimes: a quenched state, an\nunstable and a stable annealed state depending on the resetting strength.\nNotably, in the last regime, the system is self-averaging and thus the sample\naverage will always mimic ergodic behavior establishing a stand alone feature\nfor GBM under resetting. Crucially, the above-mentioned regimes are well\nseparated by a self-averaging time period which can be minimized by an optimal\nresetting rate. Our results can be useful to interpret data emanating from\nstock market collapse or reconstitution of investment portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.01571v3"
    },
    {
        "title": "Ordering results between the largest claims arising from two general\n  heterogeneous portfolios",
        "authors": [
            "Sangita Das",
            "Suchandan Kayal"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  This work is entirely devoted to compare the largest claims from two\nheterogeneous portfolios. It is assumed that the claim amounts in an insurance\nportfolio are nonnegative absolutely continuous random variables and belong to\na general family of distributions. The largest claims have been compared based\non various stochastic orderings. The established sufficient conditions are\nassociated with the matrices and vectors of model parameters. Applications of\nthe results are provided for the purpose of illustration.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.08605v1"
    },
    {
        "title": "Asymptotic Analysis of Risk Premia Induced by Law-Invariant Risk\n  Measures",
        "authors": [
            "Thomas Knispel",
            "Roger J. A. Laeven",
            "Gregor Svindland"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We analyze the limiting behavior of the risk premium associated with the\nPareto optimal risk sharing contract in an infinitely expanding pool of risks\nunder a general class of law-invariant risk measures encompassing\nrank-dependent utility preferences. We show that the corresponding convergence\nrate is typically only $n^{1/2}$ instead of the conventional $n$, with $n$ the\nmultiplicity of risks in the pool, depending upon the precise risk preferences.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.01730v1"
    },
    {
        "title": "Optimal Insurance to Maximize RDEU Under a Distortion-Deviation Premium\n  Principle",
        "authors": [
            "Xiaoqing Liang",
            "Ruodu Wang",
            "Virginia Young"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper, we study an optimal insurance problem for a risk-averse\nindividual who seeks to maximize the rank-dependent expected utility (RDEU) of\nher terminal wealth, and insurance is priced via a general distortion-deviation\npremium principle. We prove necessary and sufficient conditions satisfied by\nthe optimal solution and consider three ambiguity orders to further determine\nthe optimal indemnity. Finally, we analyze examples under three\ndistortion-deviation premium principles to explore the specific conditions\nunder which no insurance or deductible insurance is optimal.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.02656v2"
    },
    {
        "title": "Deep Risk Model: A Deep Learning Solution for Mining Latent Risk Factors\n  to Improve Covariance Matrix Estimation",
        "authors": [
            "Hengxu Lin",
            "Dong Zhou",
            "Weiqing Liu",
            "Jiang Bian"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Modeling and managing portfolio risk is perhaps the most important step to\nachieve growing and preserving investment performance. Within the modern\nportfolio construction framework that built on Markowitz's theory, the\ncovariance matrix of stock returns is a required input to calculate portfolio\nrisk. Traditional approaches to estimate the covariance matrix are based on\nhuman-designed risk factors, which often require tremendous time and effort to\ndesign better risk factors to improve the covariance estimation. In this work,\nwe formulate the quest of mining risk factors as a learning problem and propose\na deep learning solution to effectively ``design'' risk factors with neural\nnetworks. The learning objective is also carefully set to ensure the learned\nrisk factors are effective in explaining the variance of stock returns as well\nas having desired orthogonality and stability. Our experiments on the stock\nmarket data demonstrate the effectiveness of the proposed solution: our method\ncan obtain $1.9\\%$ higher explained variance measured by $R^2$ and also reduce\nthe risk of a global minimum variance portfolio. The incremental analysis\nfurther supports our design of both the architecture and the learning\nobjective.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.05201v2"
    },
    {
        "title": "Correlation scenarios and correlation stress testing",
        "authors": [
            "N. Packham",
            "F. Woebbeking"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We develop a general approach for stress testing correlations of financial\nasset portfolios. The correlation matrix of asset returns is specified in a\nparametric form, where correlations are represented as a function of risk\nfactors, such as country and industry factors. A sparse factor structure\nlinking assets and risk factors is built using Bayesian variable selection\nmethods. Regular calibration yields a joint distribution of economically\nmeaningful stress scenarios of the factors. As such, the method also lends\nitself as a reverse stress testing framework: using the Mahalanobis distance or\nhighest density regions (HDR) on the joint risk factor distribution allows to\ninfer worst-case correlation scenarios. We give examples of stress tests on a\nlarge portfolio of European and North American stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06839v2"
    },
    {
        "title": "A New Attempt to Identify Long-term Precursors for Endogenous Financial\n  Crises in the Market Correlation Structures",
        "authors": [
            "Anton J. Heckens",
            "Thomas Guhr"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Prediction of events in financial markets is every investor's dream and,\nusually, wishful thinking. From a more general, economic and societal\nviewpoint, the identification of indicators for large events is highly\ndesirable to assess systemic risks. Unfortunately, the very nature of financial\nmarkets, particularly the predominantly non-Markovian character as well as\nnon-stationarity, make this challenge a formidable one, leaving little hope for\nfully fledged answers. Nevertheless, it is called for to collect pieces of\nevidence in a variety of observables to be assembled like the pieces of a\npuzzle that eventually might help to catch a glimpse of long-term indicators or\nprecursors for large events - if at all in a statistical sense. Here, we\npresent a new piece for this puzzle. We use the quasi-stationary market states\nwhich exist in the time evolution of the correlation structure in financial\nmarkets. Recently, we identified such market states relative to the collective\nmotion of the market as a whole. We study their precursor properties in the US\nstock markets over 16 years, including two endogenous crises, the dot-com\nbubble burst and the pre-phase of the Lehman Brothers crash. We identify\ncertain interesting features and critically discuss their suitability as\nindicators.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.09048v2"
    },
    {
        "title": "Minimizing ruin probability under dependencies for insurance pricing",
        "authors": [
            "Ragnar Levy Gudmundarson",
            "Manuel Guerra",
            "Alexandra Bugalho de Moura"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this work the ruin probability of the Lundberg risk process is used as a\ncriterion for determining the optimal security loading of premia in the\npresence of price-sensitive demand for insurance. Both single and aggregated\nclaim processes are considered and the independent and the dependent cases are\nanalyzed. For the single-risk case, we show that the optimal loading does not\ndepend on the initial reserve. In the multiple risk case we account for\narbitrary dependency structures between different risks and for dependencies\nbetween the probabilities of a client acquiring policies for different risks.\nIn this case, the optimal loadings depend on the initial reserve. In all cases\nthe loadings minimizing the ruin probability do not coincide with the loadings\nmaximizing the expected profit.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.10075v1"
    },
    {
        "title": "Dual representations of quasiconvex compositions with applications to\n  systemic risk",
        "authors": [
            "Çağın Ararat",
            "Mücahit Aygün"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Motivated by the problem of finding dual representations for quasiconvex\nsystemic risk measures in financial mathematics, we study quasiconvex\ncompositions in an abstract infinite-dimensional setting. We calculate an\nexplicit formula for the penalty function of the composition in terms of the\npenalty functions of the ingredient functions. The proof makes use of a\nnonstandard minimax inequality (rather than equality as in the standard case)\nthat is available in the literature. In the second part of the paper, we apply\nour results in concrete probabilistic settings for systemic risk measures, in\nparticular, in the context of Eisenberg-Noe clearing model. We also provide\nnovel economic interpretations of the dual representations in these settings.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.12910v1"
    },
    {
        "title": "SPLICE: A Synthetic Paid Loss and Incurred Cost Experience Simulator",
        "authors": [
            "Benjamin Avanzi",
            "Gregory Clive Taylor",
            "Melantha Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper, we first introduce a simulator of cases estimates of incurred\nlosses, called `SPLICE` (Synthetic Paid Loss and Incurred Cost Experience). In\nthree modules, case estimates are simulated in continuous time, and a record is\noutput for each individual claim. Revisions for the case estimates are also\nsimulated as a sequence over the lifetime of the claim, in a number of\ndifferent situations. Furthermore, some dependencies in relation to case\nestimates of incurred losses are incorporated, particularly recognizing certain\nproperties of case estimates that are found in practice. For example, the\nmagnitude of revisions depends on ultimate claim size, as does the distribution\nof the revisions over time. Some of these revisions occur in response to\noccurrence of claim payments, and so `SPLICE` requires input of simulated\nper-claim payment histories. The claim data can be summarized by accident and\npayment \"periods\" whose duration is an arbitrary choice (e.g. month, quarter,\netc.) available to the user.\n  `SPLICE` is a fully documented R package that is publicly available and open\nsource (on CRAN). It is built on an existing simulator of individual claim\nexperience called `SynthETIC` (Avanzi et al., 2021a,b), which offers flexible\nmodelling of occurrence, notification, as well as the timing and magnitude of\nindividual partial payments. This is in contrast with the incurred losses,\nwhich constitute the additional contribution of `SPLICE`. The inclusion of\nincurred loss estimates provides a facility that almost no other simulators do.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04058v4"
    },
    {
        "title": "Risk Measurement, Risk Entropy, and Autonomous Driving Risk Modeling",
        "authors": [
            "Jiamin Yu"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  It has been for a long time to use big data of autonomous vehicles for\nperception, prediction, planning, and control of driving. Naturally, it is\nincreasingly questioned why not using this big data for risk management and\nactuarial modeling. This article examines the emerging technical difficulties,\nnew ideas, and methods of risk modeling under autonomous driving scenarios.\nCompared with the traditional risk model, the novel model is more consistent\nwith the real road traffic and driving safety performance. More importantly, it\nprovides technical feasibility for realizing risk assessment and car insurance\npricing under a computer simulation environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.07211v1"
    },
    {
        "title": "Marginals Versus Copulas: Which Account For More Model Risk In\n  Multivariate Risk Forecasting?",
        "authors": [
            "Simon Fritzsch",
            "Maike Timphus",
            "Gregor Weiss"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Copulas. We study the model risk of multivariate risk models in a\ncomprehensive empirical study on Copula-GARCH models used for forecasting\nValue-at-Risk and Expected Shortfall. To determine whether model risk inherent\nin the forecasting of portfolio risk is caused by the candidate marginal or\ncopula models, we analyze different groups of models in which we fix either the\nmarginals, the copula, or neither. Model risk is economically significant, is\nespecially high during periods of crisis, and is almost completely due to the\nchoice of the copula. We then propose the use of the model confidence set\nprocedure to narrow down the set of available models and reduce model risk for\nCopula-GARCH risk models. Our proposed approach leads to a significant\nimprovement in the mean absolute deviation of one day ahead forecasts by our\nvarious candidate risk models.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.10946v1"
    },
    {
        "title": "Systemic risk in interbank networks: disentangling balance sheets and\n  network effects",
        "authors": [
            "Alessandro Ferracci",
            "Giulio Cimini"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We study the difference between the level of systemic risk that is\nempirically measured on an interbank network and the risk that can be deduced\nfrom the balance sheets composition of the participating banks. Using\ngeneralised DebtRank dynamics, we measure observed systemic risk on e-MID\nnetwork data (augmented by BankFocus information) and compare it with the\nexpected systemic risk of a null model network, obtained through an appropriate\nmaximum-entropy approach constraining relevant balance sheet variables. We show\nthat the aggregate levels of observed and expected systemic risks are usually\ncompatible but differ significantly during turbulent times (in our case, after\nthe default of Lehman Brothers and the VLTRO implementation by the ECB). At the\nindividual level instead, banks are typically more or less risky than what\ntheir balance sheet prescribes due to their position in the network. Our\nresults confirm on one hand that balance sheet information used within a proper\nmaximum-entropy network model provides good aggregate estimates of systemic\nrisk, and on the other hand the importance of knowing the empirical details of\nthe network for conducting precise stress tests on individual banks, especially\nafter systemic events.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14360v2"
    },
    {
        "title": "Assessing the impact of the COVID-19 shock on a stochastic\n  multi-population mortality model",
        "authors": [
            "Jens Robben",
            "Katrien Antonio",
            "Sander Devriendt"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We aim to assess the impact of a pandemic data point on the calibration of a\nstochastic multi-population mortality projection model and its resulting\nprojections for future mortality rates. Throughout the paper we put focus on\nthe Li & Lee mortality model, which has become a standard for projecting\nmortality in Belgium and the Netherlands. We calibrate this mortality model on\nannual deaths and exposures at the level of individual ages. This type of\nmortality data is typically collected, produced and reported with a significant\ndelay of -- for some countries -- several years on a platform such as the Human\nMortality Database. To enable a timely evaluation of the impact of a pandemic\ndata point we have to rely on other data sources (e.g. the Short-Term Mortality\nFluctuations Data series) that swiftly publish weekly mortality data collected\nin age buckets. To be compliant with the design and calibration strategy of the\nLi & Lee model, we have to transform the weekly mortality data collected in age\nbuckets to yearly, age-specific observations. Therefore, our paper constructs a\nprotocol to ungroup the deaths and exposures registered in age buckets to\nindividual ages. To evaluate the impact of a pandemic shock, like COVID-19 in\nthe year 2020, we weigh this data point in either the calibration or projection\nstep. Obviously, the more weight we place on this data point, the more impact\nwe observe on future estimated mortality rates and life expectancies. Our paper\nallows to quantify this impact and provides actuaries and actuarial\nassociations with a framework to generate scenarios of future mortality under\nvarious assessments of the pandemic data point.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.10164v1"
    },
    {
        "title": "How Does Risk Hedging Impact Operations? Insights from a Price-Setting\n  Newsvendor Model",
        "authors": [
            "Liao Wang",
            "Jin Yao",
            "Xiaowei Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  If a financial asset's price movement impacts a firm's product demand, the\nfirm can respond to the impact by adjusting its operational decisions. For\nexample, in the automotive industry, car makers decrease the selling prices of\nfuel-inefficient cars when the oil price rises. Meanwhile, the firm can\nimplement a risk-hedging strategy using the financial asset jointly with its\noperational decisions. Motivated by this, we develop and solve a general\nrisk-management model integrating risk hedging into a price-setting newsvendor.\nThe optimal hedging strategy is calculated analytically, which leads to an\nexplicit objective function for optimizing price and ``virtual production\nquantity'' (VPQ). (The latter determines the service level, i.e., the demand\nfulfillment probability.) We find that hedging generally reduces the optimal\nprice {when the firm sets the target mean return as its production-only maximum\nexpected profit. With the same condition on the target mean return}, hedging\nalso reduces the optimal VPQ when the asset price trend positively impacts\nproduct demand; meanwhile, it may increase the VPQ by a small margin when the\nimpact is negative. We construct the return-risk efficient frontier that\ncharacterizes the optimal return-risk trade-off. Our numerical study using data\nfrom a prominent automotive manufacturer shows that the markdowns in price and\nreduction in VPQ are small under our model and that the hedging strategy\nsubstantially reduces risk without materially reducing operational profit.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.01026v4"
    },
    {
        "title": "Stochastic measure distortions induced by quantile processes for risk\n  quantification and valuation",
        "authors": [
            "Holly Brannelly",
            "Andrea Macrina",
            "Gareth W. Peters"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We develop a novel stochastic valuation and premium calculation principle\nbased on probability measure distortions that are induced by quantile processes\nin continuous time. Necessary and sufficient conditions are derived under which\nthe quantile processes satisfy first- and second-order stochastic dominance.\nThe introduced valuation principle relies on stochastic ordering so that the\nvaluation risk-loading, and thus risk premiums, generated by the measure\ndistortion is an ordered parametric family. The quantile processes are\ngenerated by a composite map consisting of a distribution and a quantile\nfunction. The distribution function accounts for model risk in relation to the\nempirical distribution of the risk process, while the quantile function models\nthe response to the risk source as perceived by, e.g., a market agent. This\ngives rise to a system of subjective probability measures that indexes a\nstochastic valuation principle susceptible to probability measure distortions.\nWe use the Tukey-$gh$ family of quantile processes driven by Brownian motion in\nan example that demonstrates stochastic ordering. We consider the conditional\nexpectation under the distorted measure as a member of the time-consistent\nclass of dynamic valuation principles, and extend it to the setting where the\ndriving risk process is multivariate. This requires the introduction of a\ncopula function in the composite map for the construction of quantile\nprocesses, which presents another new element in the risk quantification and\nmodelling framework based on probability measure distortions induced by\nquantile processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.02045v1"
    },
    {
        "title": "Multivariate matrix-exponential affine mixtures and their applications\n  in risk theory",
        "authors": [
            "Eric C. K. Cheung",
            "Oscar Peralta",
            "Jae-Kyung Woo"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper, a class of multivariate matrix-exponential affine mixtures\nwith matrix-exponential marginals is proposed. The class is shown to possess\nvarious attractive properties such as closure under size-biased Esscher\ntransform, order statistics, residual lifetime and higher order equilibrium\ndistributions. This allows for explicit calculations of various actuarial\nquantities of interest. The results are applied in a wide range of actuarial\nproblems including multivariate risk measures, aggregate loss, large claims\nreinsurance, weighted premium calculations and risk capital allocation.\nFurthermore, a multiplicative background risk model with dependent risks is\nconsidered and its capital allocation rules are provided as well. We finalize\nby discussing a calibration scheme based on complete data and potential avenues\nof research.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11122v1"
    },
    {
        "title": "Optimal Support for Distressed Subsidiaries -- a Systemic Risk\n  Perspective",
        "authors": [
            "Maxim Bichuch",
            "Nils Detering"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We consider a network of bank holdings, where every holding has two\nsubsidiaries of different types. A subsidiary can trade with another holding's\nsubsidiary of the same type. Holdings support their subsidiaries up to a\ncertain level when they would otherwise fail to honor their financial\nobligations. We investigate the spread of contagion in this banking network\nwhen the number of bank holdings is large, and find the final number of\ndefaulted subsidiaries under different rules for the holding support. We also\nconsider resilience of this multilayered network to small shocks. Our work\nsheds light onto the role that holding structures can play in the amplification\nof financial stress. We find that depending on the capitalization of the\nnetwork, a holding structure can be beneficial as compared to smaller separated\nentities. In other instances, it can be harmful and actually increase\ncontagion. We illustrate our results in a numerical case study and also\ndetermine the optimal level of holding support from a regulator perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.12731v3"
    },
    {
        "title": "Combining Intra-Risk and Contagion Risk for Enterprise Bankruptcy\n  Prediction Using Graph Neural Networks",
        "authors": [
            "Yu Zhao",
            "Shaopeng Wei",
            "Yu Guo",
            "Qing Yang",
            "Xingyan Chen",
            "Qing Li",
            "Fuzhen Zhuang",
            "Ji Liu",
            "Gang Kou"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Predicting the bankruptcy risk of small and medium-sized enterprises (SMEs)\nis an important step for financial institutions when making decisions about\nloans. Existing studies in both finance and AI research fields, however, tend\nto only consider either the intra-risk or contagion risk of enterprises,\nignoring their interactions and combinatorial effects. This study for the first\ntime considers both types of risk and their joint effects in bankruptcy\nprediction. Specifically, we first propose an enterprise intra-risk encoder\nbased on statistically significant enterprise risk indicators for its\nintra-risk learning. Then, we propose an enterprise contagion risk encoder\nbased on enterprise relation information from an enterprise knowledge graph for\nits contagion risk embedding. In particular, the contagion risk encoder\nincludes both the newly proposed Hyper-Graph Neural Networks and Heterogeneous\nGraph Neural Networks, which can model contagion risk in two different aspects,\ni.e. common risk factors based on hyperedges and direct diffusion risk from\nneighbors, respectively. To evaluate the model, we collect real-world\nmulti-sources data on SMEs and build a novel benchmark dataset called SMEsD. We\nprovide open access to the dataset, which is expected to further promote\nresearch on financial risk analysis. Experiments on SMEsD against twelve\nstate-of-the-art baselines demonstrate the effectiveness of the proposed model\nfor bankruptcy prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03874v4"
    },
    {
        "title": "Probability equivalent level of Value at Risk and higher-order Expected\n  Shortfalls",
        "authors": [
            "Matyas Barczy",
            "Fanni K. Nedényi",
            "László Sütő"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We investigate the probability equivalent level of Value at Risk and\n$n^{\\mathrm{th}}$-order Expected Shortfall (called PELVE_n), which can be\nconsidered as a variant of the notion of the probability equivalent level of\nValue at Risk and Expected Shortfall (called PELVE) due to Li and Wang (2022).\nWe study the finiteness, uniqueness and several properties of PELVE_n, we\ncalculate PELVE_n of some notable distributions, PELVE_2 of a random variable\nhaving generalized Pareto excess distribution, and we describe the asymptotic\nbehaviour of PELVE_2 of regularly varying distributions as the level tends to\n$0$. Some properties of $n^{\\mathrm{th}}$-order Expected Shortfall are also\ninvestigated. Among others, it turns out that the Gini Shortfall at some level\n$p\\in[0,1)$ corresponding to a (loading) parameter $\\lambda\\geq 0$ is the\nlinear combination of the Expected Shortfall at level $p$ and the\n$2^{\\mathrm{nd}}$-order Expected Shortfall at level $p$ with coefficients\n$1-2\\lambda$ and $2\\lambda$, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.09770v3"
    },
    {
        "title": "Honour Thesis: A Joint Value at Risk and Expected Shortfall Combination\n  Framework and its Applications in the Cryptocurrency Market",
        "authors": [
            "Zhengkun Li"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Value at risk and expected shortfall are increasingly popular tail risk\nmeasures in the financial risk management field. Both academia and financial\ninstitutions are working to improve tail risk forecasts in order to meet the\nrequirements of the Basel Capital Accord; it states that one purpose of risk\nmanagement and measuring risk accuracy is, since extreme movements cannot\nalways be avoided, financial institutions can prepare for these extreme returns\nby capital allocation, and putting aside the appropriate amount of capital so\nas to avoid default in times of extreme price or index movements. Forecast\ncombination has drawn much attention, as a combined forecast can outperform the\nindividual forecasts under certain conditions. We propose two methodology, one\nis a semiparametric combination framework that can jointly produce combined\nvalue at risk and expected shortfall forecasts, another one is a parametric\nregression framework named as Quantile-ES regression that can produce combined\nexpected shortfall forecasts. The favourability of the semiparametric\ncombination framework has been presented via an empirical study - application\nin cryptocurrency markets with high-frequency data where the necessity of risk\nmanagement application increases as the cryptocurrency market becomes more\npopular and mature. Additionally, the general framework of the parametric\nQuantile-ES regression has been presented via a simulation study, whereas it\nstill need to be improved in the future. The contributions of this work include\nbut are not limited to the enabling of the combination of expected shortfall\nforecasts and the application of risk management procedures in the\ncryptocurrency market with high-frequency data.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.10918v1"
    },
    {
        "title": "A contagion process with self-exciting jumps in credit risk applications",
        "authors": [
            "Puneet Pasricha",
            "Dharmaraja Selvamuthu",
            "Selvaraju Natarajan"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The modeling of the probability of joint default or total number of defaults\namong the firms is one of the crucial problems to mitigate the credit risk\nsince the default correlations significantly affect the portfolio loss\ndistribution and hence play a significant role in allocating capital for\nsolvency purposes. In this article, we derive a closed-form expression for the\nprobability of default of a single firm and the probability of the total number\nof defaults by any time $t$ in a homogeneous portfolio of firms. We use a\ncontagion process to model the arrival of credit events that causes the default\nand develop a framework that allows firms to have resistance against default\nunlike the standard intensity-based models. We assume the point process driving\nthe credit events to be composed of a systematic and an idiosyncratic\ncomponent, whose intensities are independently specified by a mean-reverting\naffine jump-diffusion process with self-exciting jumps. The proposed framework\nis competent of capturing the feedback effect, an empirically observed\nphenomenon in the default events. We further demonstrate how the proposed\nframework can be used to price synthetic collateralized debt obligation (CDO)\nand obtain a closed-form solution for tranche spread. Finally, we present the\nsensitivity analysis to demonstrate the effect of different parameters\ngoverning the contagion effect on the spread of tranches and the expected loss\nof the CDO.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12946v1"
    },
    {
        "title": "A novel approach to rating transition modelling via Machine Learning and\n  SDEs on Lie groups",
        "authors": [
            "Kevin Kamm",
            "Michelle Muniz"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  In this paper, we introduce a novel methodology to model rating transitions\nwith a stochastic process. To introduce stochastic processes, whose values are\nvalid rating matrices, we noticed the geometric properties of stochastic\nmatrices and its link to matrix Lie groups. We give a gentle introduction to\nthis topic and demonstrate how It\\^o-SDEs in R will generate the desired model\nfor rating transitions. To calibrate the rating model to historical data, we\nuse a Deep-Neural-Network (DNN) called TimeGAN to learn the features of a time\nseries of historical rating matrices. Then, we use this DNN to generate\nsynthetic rating transition matrices. Afterwards, we fit the moments of the\ngenerated rating matrices and the rating process at specific time points, which\nresults in a good fit. After calibration, we discuss the quality of the\ncalibrated rating transition process by examining some properties that a time\nseries of rating matrices should satisfy, and we will see that this geometric\napproach works very well.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.15699v1"
    },
    {
        "title": "Multivariate backtests and copulas for risk evaluation",
        "authors": [
            "Boris David",
            "Gilles Zumbach"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Risk evaluation is a forecast, and its validity must be backtested.\nProbability distribution forecasts are used in this work and allow for more\npowerful validations compared to point forecasts. Our aim is to use bivariate\ncopulas in order to characterize the in-sample copulas and to validate\nout-of-sample a bivariate forecast. For both set-ups, probability integral\ntransforms (PIT) and Rosenblatt transforms are used to map the problem into an\nindependent copula. For this simple copula, statistical tests can be applied to\nvalidate the choice of the in-sample copula or the validity of the bivariate\nforecast. The salient results are that a Student copula describes well the\ndependencies between financial time series (regardless of the correlation), and\nthat the bivariate forecasts provided by a risk methodology based on historical\ninnovations performs correctly out-of-sample. A prerequisite is to remove the\nheteroskedasticity in order to have stationary time series, in this work a\nlong-memory ARCH volatility model is used.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.03896v2"
    },
    {
        "title": "On the closed-form expected NPVs of double barrier strategies for\n  regular diffusions",
        "authors": [
            "Chongrui Zhu"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The core of the research is to provide the explicit expression for the\nexpected net present values (NPVs) of double barrier strategies for regular\ndiffusions on the real line without solving differential equations. Under the\nso-called bail-out setting, the value of the expected NPVs of an insurance\ncompany varies according to the choice of a pair of policies, which consist of\ndividend payments paid out and capital injections received. In the case of the\ndouble barrier strategy, the expected NPVs are expressible with the help of\ncertain types of functions allowing explicit expression in some cases, which is\ncalled the bivariate $q$-scale function in the article. This is accomplished by\nmaking use of a perturbation technique in \\cite{czarna2014dividend}, which\ncould lead to the linear equation system. In addition, a condition ensuring the\nexistence of an optimal (upper) barrier level is presented. In the end,\nexamples fitting the condition for selecting the optimal barrier are given.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08922v2"
    },
    {
        "title": "Combining Retrospective Approximation with Importance Sampling for\n  Optimising Conditional Value at Risk",
        "authors": [
            "Anand Deo",
            "Karthyek Murthy",
            "Tirtho Sarker"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper investigates the use of retrospective approximation solution\nparadigm in solving risk-averse optimization problems effectively via\nimportance sampling (IS). While IS serves as a prominent means for tackling the\nlarge sample requirements in estimating tail risk measures such as Conditional\nValue at Risk (CVaR), its use in optimization problems driven by CVaR is\ncomplicated by the need to tailor the IS change of measure differently to\ndifferent optimization iterates and the circularity which arises as a\nconsequence. The proposed algorithm overcomes these challenges by employing a\nunivariate IS transformation offering uniform variance reduction in a\nretrospective approximation procedure well-suited for tuning the IS parameter\nchoice. The resulting simulation based approximation scheme enjoys both the\ncomputational efficiency bestowed by retrospective approximation and\nlogarithmically efficient variance reduction offered by importance sampling\n",
        "pdf_link": "http://arxiv.org/pdf/2206.12835v1"
    },
    {
        "title": "Systemic-risk and evolutionary stable strategies in a financial network",
        "authors": [
            "Indrajit Saha",
            "Veeraruna Kavitha"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We consider a financial network represented at any time instance by a random\nliability graph which evolves over time. The agents connect through credit\ninstruments borrowed from each other or through direct lending, and these\ncreate the liability edges. These random edges are modified (locally) by the\nagents over time, as they learn from their experiences and (possibly imperfect)\nobservations. The settlement of the liabilities of various agents at the end of\nthe contract period (at any time instance) can be expressed as solutions of\nrandom fixed point equations. Our first step is to derive the solutions of\nthese equations (asymptotically and one for each time instance), using a recent\nresult on random fixed point equations. The agents, at any time instance, adapt\none of the two available strategies, risky or less risky investments, with an\naim to maximize their returns. We aim to study the emerging strategies of such\nreplicator dynamics that drives the financial network. We theoretically reduce\nthe analysis of the complex system to that of an appropriate ordinary\ndifferential equation (ODE). Using the attractors of the resulting ODE we show\nthat the replicator dynamics converges to one of the two pure evolutionary\nstable strategies (all risky or all less risky agents); one can have mixed\nlimit only when the observations are imperfect. We verify our theoretical\nfindings using exhaustive Monte Carlo simulations. The dynamics avoid the\nemergence of the systemic-risk regime (where majority default). However, if all\nthe agents blindly adapt risky strategy it can lead to systemic risk regime.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.07574v2"
    },
    {
        "title": "Quasi-Logconvex Measures of Risk",
        "authors": [
            "Roger J. A. Laeven",
            "Emanuela Rosazza Gianin"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper introduces and fully characterizes the novel class of\nquasi-logconvex measures of risk, to stand on equal footing with the rich class\nof quasi-convex measures of risk. Quasi-logconvex risk measures naturally\ngeneralize logconvex return risk measures, just like quasi-convex risk measures\ngeneralize convex monetary risk measures. We establish their dual\nrepresentation and analyze their taxonomy in a few (sub)classification results.\nFurthermore, we characterize quasi-logconvex risk measures in terms of\nproperties of families of acceptance sets and provide their law-invariant\nrepresentation. Examples and applications to portfolio choice and capital\nallocation are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.07694v1"
    },
    {
        "title": "Transaction time models in multi-state life insurance",
        "authors": [
            "Kristian Buchardt",
            "Christian Furrer",
            "Oliver Lunding Sandqvist"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  In life insurance contracts, benefits and premiums are typically paid\ncontingent on the biometric state of the insured. Due to delays between the\noccurrence, reporting, and settlement of changes to the biometric state, the\nstate process is not fully observable in real-time. This fact implies that the\nclassic multi-state models for the biometric state of the insured are not able\nto describe the development of the policy in real-time, which encompasses\nhandling of incurred-but-not-reported and reported-but-not-settled claims. We\ngive a fundamental treatment of the problem in the setting of continuous-time\nmulti-state life insurance by introducing a new class of models: transaction\ntime models. The relation between the transaction time model and the classic\nmodel is studied and a result linking the present values in the two models is\nderived. The results and their practical implications are illustrated for\ndisability coverages, where we obtain explicit expressions for the transaction\ntime reserve in specific models.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.06902v2"
    },
    {
        "title": "Measuring Tail Risks",
        "authors": [
            "Kan Chen",
            "Tuoyuan Cheng"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Value at risk (VaR) and expected shortfall (ES) are common high\nquantile-based risk measures adopted in financial regulations and risk\nmanagement. In this paper, we propose a tail risk measure based on the most\nprobable maximum size of risk events (MPMR) that can occur over a length of\ntime. MPMR underscores the dependence of the tail risk on the risk management\ntime frame. Unlike VaR and ES, MPMR does not require specifying a confidence\nlevel. We derive the risk measure analytically for several well-known\ndistributions. In particular, for the case where the size of the risk event\nfollows a power law or Pareto distribution, we show that MPMR also scales with\nthe number of observations $n$ (or equivalently the length of the time\ninterval) by a power law, $\\text{MPMR}(n) \\propto n^{\\eta}$, where $\\eta$ is\nthe scaling exponent. The scale invariance allows for reasonable estimations of\nlong-term risks based on the extrapolation of more reliable estimations of\nshort-term risks. The scaling relationship also gives rise to a robust and\nlow-bias estimator of the tail index (TI) $\\xi$ of the size distribution, $\\xi\n= 1/\\eta$. We demonstrate the use of this risk measure for describing the tail\nrisks in financial markets as well as the risks associated with natural hazards\n(earthquakes, tsunamis, and excessive rainfall).\n",
        "pdf_link": "http://arxiv.org/pdf/2209.07092v2"
    },
    {
        "title": "Modeling and Pricing Cyber Insurance -- Idiosyncratic, Systematic, and\n  Systemic Risks",
        "authors": [
            "Kerstin Awiszus",
            "Thomas Knispel",
            "Irina Penner",
            "Gregor Svindland",
            "Alexander Voß",
            "Stefan Weber"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The paper provides a comprehensive overview of modeling and pricing cyber\ninsurance and includes clear and easily understandable explanations of the\nunderlying mathematical concepts. We distinguish three main types of cyber\nrisks: idiosyncratic, systematic, and systemic cyber risks. While for\nidiosyncratic and systematic cyber risks, classical actuarial and financial\nmathematics appear to be well-suited, systemic cyber risks require more\nsophisticated approaches that capture both network and strategic interactions.\nIn the context of pricing cyber insurance policies, issues of interdependence\narise for both systematic and systemic cyber risks; classical actuarial\nvaluation needs to be extended to include more complex methods, such as\nconcepts of risk-neutral valuation and (set-valued) monetary risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.07415v2"
    },
    {
        "title": "Towards a Better Microcredit Decision",
        "authors": [
            "Mengnan Song",
            "Jiasong Wang",
            "Suisui Su"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Reject inference comprises techniques to infer the possible repayment\nbehavior of rejected cases. In this paper, we model credit in a brand new view\nby capturing the sequential pattern of interactions among multiple stages of\nloan business to make better use of the underlying causal relationship.\nSpecifically, we first define 3 stages with sequential dependence throughout\nthe loan process including credit granting(AR), withdrawal application(WS) and\nrepayment commitment(GB) and integrate them into a multi-task architecture.\nInside stages, an intra-stage multi-task classification is built to meet\ndifferent business goals. Then we design an Information Corridor to express\nsequential dependence, leveraging the interaction information between customer\nand platform from former stages via a hierarchical attention module controlling\nthe content and size of the information channel. In addition, semi-supervised\nloss is introduced to deal with the unobserved instances. The proposed\nmulti-stage interaction sequence(MSIS) method is simple yet effective and\nexperimental results on a real data set from a top loan platform in China show\nthe ability to remedy the population bias and improve model generalization\nability.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.07574v1"
    },
    {
        "title": "Extreme Measures in Continuous Time Conic Finace",
        "authors": [
            "Yoshihiro Shirai"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Dynamic spectral risk measures define a claim's valuation bounds as supremum\nand infimum of expectations of the claim's payoff over a dominated set of\nmeasures. The measures at which such extrema are attained are called extreme\nmeasures. We determine explicit expressions for their Radon-Nykodim derivatives\nwith respect to the common dominating measure. Based on the formulas found, we\nestimate the extreme measures in two cases. First, the dominating measure is\ncalibrated to mid prices of options and valuation bounds are given by options\nbid and ask prices. Second, the dominating measure is estimated from historical\nmid equity prices and valuation bounds are given by historical 5-day high and\nlow prices. In both experiments, we find that the market determines upper\nbounds by testing scenarios in which losses are significantly lower than\nexpected under the dominating measure, while lower bounds by ones in which\ngains are only slightly lower than in the base case.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.13671v2"
    },
    {
        "title": "Supply Chain Characteristics as Predictors of Cyber Risk: A\n  Machine-Learning Assessment",
        "authors": [
            "Kevin Hu",
            "Retsef Levi",
            "Raphael Yahalom",
            "El Ghali Zerhouni"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper provides the first large-scale data-driven analysis to evaluate\nthe predictive power of different attributes for assessing risk of cyberattack\ndata breaches. Furthermore, motivated by rapid increase in third party enabled\ncyberattacks, the paper provides the first quantitative empirical evidence that\ndigital supply-chain attributes are significant predictors of enterprise cyber\nrisk. The paper leverages outside-in cyber risk scores that aim to capture the\nquality of the enterprise internal cybersecurity management, but augment these\nwith supply chain features that are inspired by observed third party\ncyberattack scenarios, as well as concepts from network science research. The\nmain quantitative result of the paper is to show that supply chain network\nfeatures add significant detection power to predicting enterprise cyber risk,\nrelative to merely using enterprise-only attributes. Particularly, compared to\na base model that relies only on internal enterprise features, the supply chain\nnetwork features improve the out-of-sample AUC by 2.3\\%. Given that each cyber\ndata breach is a low probability high impact risk event, these improvements in\nthe prediction power have significant value. Additionally, the model highlights\nseveral cybersecurity risk drivers related to third party cyberattack and\nbreach mechanisms and provides important insights as to what interventions\nmight be effective to mitigate these risks.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15785v5"
    },
    {
        "title": "A Data-driven Case-based Reasoning in Bankruptcy Prediction",
        "authors": [
            "Wei Li",
            "Wolfgang Karl Härdle",
            "Stefan Lessmann"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  There has been intensive research regarding machine learning models for\npredicting bankruptcy in recent years. However, the lack of interpretability\nlimits their growth and practical implementation. This study proposes a\ndata-driven explainable case-based reasoning (CBR) system for bankruptcy\nprediction. Empirical results from a comparative study show that the proposed\napproach performs superior to existing, alternative CBR systems and is\ncompetitive with state-of-the-art machine learning models. We also demonstrate\nthat the asymmetrical feature similarity comparison mechanism in the proposed\nCBR system can effectively capture the asymmetrically distributed nature of\nfinancial attributes, such as a few companies controlling more cash than the\nmajority, hence improving both the accuracy and explainability of predictions.\nIn addition, we delicately examine the explainability of the CBR system in the\ndecision-making process of bankruptcy prediction. While much research suggests\na trade-off between improving prediction accuracy and explainability, our\nfindings show a prospective research avenue in which an explainable model that\nthoroughly incorporates data attributes by design can reconcile the dilemma.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00921v1"
    },
    {
        "title": "Fallen Angel Bonds Investment and Bankruptcy Predictions Using Manual\n  Models and Automated Machine Learning",
        "authors": [
            "Harrison Mateika",
            "Juannan Jia",
            "Linda Lillard",
            "Noah Cronbaugh",
            "Will Shin"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  The primary aim of this research was to find a model that best predicts which\nfallen angel bonds would either potentially rise up back to investment grade\nbonds and which ones would fall into bankruptcy. To implement the solution, we\nthought that the ideal method would be to create an optimal machine learning\nmodel that could predict bankruptcies. Among the many machine learning models\nout there we decided to pick four classification methods: logistic regression,\nKNN, SVM, and NN. We also utilized an automated methods of Google Cloud's\nmachine learning.\n  The results of our model comparisons showed that the models did not predict\nbankruptcies very well on the original data set with the exception of Google\nCloud's machine learning having a high precision score. However, our\nover-sampled and feature selection data set did perform very well. This could\nlikely be due to the model being over-fitted to match the narrative of the\nover-sampled data (as in, it does not accurately predict data outside of this\ndata set quite well). Therefore, we were not able to create a model that we are\nconfident that would predict bankruptcies.\n  However, we were able to find value out of this project in two key ways. The\nfirst is that Google Cloud's machine learning model in every metric and in\nevery data set either outperformed or performed on par with the other models.\nThe second is that we found that utilizing feature selection did not reduce\npredictive power that much. This means that we can reduce the amount of data to\ncollect for future experimentation regarding predicting bankruptcies.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.03454v2"
    },
    {
        "title": "Risk Sharing with Deep Neural Networks",
        "authors": [
            "Matteo Burzoni",
            "Alessandro Doldi",
            "Enea Monzio Compagnoni"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We consider the problem of optimally sharing a financial position among\nagents with potentially different reference risk measures. The problem is\nequivalent to computing the infimal convolution of the risk metrics and finding\nthe so-called optimal allocations. We propose a neural network-based framework\nto solve the problem and we prove the convergence of the approximated\ninf-convolution, as well as the approximated optimal allocations, to the\ncorresponding theoretical values. We support our findings with several\nnumerical experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.11752v2"
    },
    {
        "title": "Elicitability of Return Risk Measures",
        "authors": [
            "Mücahit Aygün",
            "Fabio Bellini",
            "Roger J. A. Laeven"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Informally, a risk measure is said to be elicitable if there exists a\nsuitable scoring function such that minimizing its expected value recovers the\nrisk measure. In this paper, we analyze the elicitability properties of the\nclass of return risk measures (i.e., normalized, monotone and positively\nhomogeneous risk measures). First, we provide dual representation results for\nconvex and geometrically convex return risk measures. Next, we establish new\naxiomatic characterizations of Orlicz premia (i.e., Luxemburg norms). More\nspecifically, we prove, under different sets of conditions, that Orlicz premia\nnaturally arise as the only elicitable return risk measures. Finally, we\nprovide a general family of strictly consistent scoring functions for Orlicz\npremia, a myriad of specific examples and a mixture representation suitable for\nconstructing Murphy diagrams.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.13070v2"
    },
    {
        "title": "Probabilistic Overview of Probabilities of Default for Low Default\n  Portfolios by K. Pluto and D. Tasche",
        "authors": [
            "Andrius Grigutis"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This article gives a probabilistic overview of the widely used method of\ndefault probability estimation proposed by K. Pluto and D. Tasche. There are\nlisted detailed assumptions and derivation of the inequality where the\nprobability of default is involved under the influence of systematic factor.\nThe author anticipates adding more clarity, especially for early career\nanalysts or scholars, regarding the assumption of borrowers' independence,\nconditional independence and interaction between the probability distributions\nsuch as binomial, beta, normal and others. There is also shown the relation\nbetween the probability of default and the joint distribution of\n$\\sqrt{\\varrho}X-\\sqrt{1-\\varrho}Y$, where $X$, including but not limiting, is\nthe standard normal, $Y$ admits, including but not limiting, the beta-normal\ndistribution and $X,\\,Y$ are independent.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.06148v1"
    },
    {
        "title": "Real Options Technique as a Tool of Strategic Risk Management",
        "authors": [
            "Volodymyr Savchuk"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The real options approach is now considered an effective alternative to the\ncorporate DCF model for a feasibility study. The current paper offers a\npractical methodology employing binomial trees and real options techniques for\nevaluating investment projects. A general computation procedure is suggested\nfor the decision tree with two active stages of real options, which correspond\nto additional investments. The suggested technique can be used for most real\noptions, which are practically essential regarding enterprise strategy. The\nspecial case named Binomial-Random-Cash-Flow Real Options Model with random\noutcomes is developed as the next step of real options modelling. Project Value\nat Risk is introduced and used as a criterion of investment project feasibility\nunder the assumption regarding random outcomes. In particular, the Gaussian\nprobability distribution is used for modelling option outcomes uncertainty. The\nchoice of the Gaussian distribution is caused by the desire to obtain estimates\nin the final analytical form. Choosing another distribution for random outcomes\nleads to using Monte Carlo simulation, for which a general framework is\ndeveloped by demonstrating some instances. The author could avoid the\ncomputational complexity that makes these solutions feasible for business\npractice.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09176v1"
    },
    {
        "title": "Adjust factor with volatility model using MAXFLAT low-pass filter and\n  construct portfolio in China A share market",
        "authors": [
            "Ke Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In the field of quantitative finance, volatility models, such as ARCH, GARCH,\nFIGARCH, SV, EWMA, play the key role in risk and portfolio management.\nMeanwhile, factor investing is more and more famous since mid of 20 century.\nCAPM, Fama French three factor model, Fama French five-factor model, MSCI Barra\nfactor model are mentioned and developed during this period. In this paper, we\nwill show why we need adjust group of factors by our MAXFLAT low-pass\nvolatility model. All of our experiments are under China's CSI 300 and CSI 500\nuniverse which represent China's large cap stocks and mid-small cap stocks. Our\nresult shows adjust factors by MAXFLAT volatility model have better performance\nin both large cap and small cap universe than original factors or other risk\nadjust factors in China A share. Also the portfolio constructed by MAXFLAT risk\nadjust factors have continuous excess return and lower beta compare with\nbenchmark index.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.04676v2"
    },
    {
        "title": "Probabilistic Forecast-based Portfolio Optimization of Electricity\n  Demand at Low Aggregation Levels",
        "authors": [
            "Jungyeon Park",
            "Estêvão Alvarenga",
            "Jooyoung Jeon",
            "Ran Li",
            "Fotios Petropoulos",
            "Hokyun Kim",
            "Kwangwon Ahn"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In the effort to achieve carbon neutrality through a decentralized\nelectricity market, accurate short-term load forecasting at low aggregation\nlevels has become increasingly crucial for various market participants'\nstrategies. Accurate probabilistic forecasts at low aggregation levels can\nimprove peer-to-peer energy sharing, demand response, and the operation of\nreliable distribution networks. However, these applications require not only\nprobabilistic demand forecasts, which involve quantification of the forecast\nuncertainty, but also determining which consumers to include in the aggregation\nto meet electricity supply at the forecast lead time. While research papers\nhave been proposed on the supply side, no similar research has been conducted\non the demand side. This paper presents a method for creating a portfolio that\noptimally aggregates demand for a given energy demand, minimizing forecast\ninaccuracy of overall low-level aggregation. Using probabilistic load forecasts\nproduced by either ARMA-GARCH models or kernel density estimation (KDE), we\npropose three approaches to creating a portfolio of residential households'\ndemand: Forecast Validated, Seasonal Residual, and Seasonal Similarity. An\nevaluation of probabilistic load forecasts demonstrates that all three\napproaches enhance the accuracy of forecasts produced by random portfolios,\nwith the Seasonal Residual approach for Korea and Ireland outperforming the\nothers in terms of both accuracy and computational efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.09474v1"
    },
    {
        "title": "Modeling and evaluating conditional quantile dynamics in VaR forecasts",
        "authors": [
            "Fabrizio Cipollini",
            "Giampiero M. Gallo",
            "Alessandro Palandri"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We focus on the time-varying modeling of VaR at a given coverage $\\tau$,\nassessing whether the quantiles of the distribution of the returns standardized\nby their conditional means and standard deviations exhibit predictable\ndynamics. Models are evaluated via simulation, determining the merits of the\nasymmetric Mean Absolute Deviation as a loss function to rank forecast\nperformances. The empirical application on the Fama-French 25 value-weighted\nportfolios with a moving forecast window shows substantial improvements in\nforecasting conditional quantiles by keeping the predicted quantile unchanged\nunless the empirical frequency of violations falls outside a data-driven\ninterval around $\\tau$.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.20067v1"
    },
    {
        "title": "Expectile Quadrangle and Applications",
        "authors": [
            "Viktor Kuzmenko",
            "Anton Malandii",
            "Stan Uryasev"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The paper explores the concept of the \\emph{expectile risk measure} within\nthe framework of the Fundamental Risk Quadrangle (FRQ) theory. According to the\nFRQ theory, a quadrangle comprises four stochastic functions associated with a\nrandom variable: ``error'', ``regret'', ``risk'', and ``deviation''. These\nfunctions are interconnected through a stochastic function known as the\n``statistic''. Expectile is a risk measure that, similar to VaR (quantile) and\nCVaR (superquantile), can be employed in risk management. While quadrangles\nbased on VaR and CVaR statistics are well-established and widely used, the\npaper focuses on the recently proposed quadrangles based on expectile. The aim\nof this paper is to rigorously examine the properties of these Expectile\nQuadrangles, with particular emphasis on a quadrangle that encompasses\nexpectile as both a statistic and a measure of risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16351v2"
    },
    {
        "title": "Divergence Based Quadrangle and Applications",
        "authors": [
            "Anton Malandii",
            "Siddhartha Gupte",
            "Cheng Peng",
            "Stan Uryasev"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This paper introduces a novel framework for assessing risk and\ndecision-making in the presence of uncertainty, the \\emph{$\\varphi$-Divergence\nQuadrangle}. This approach expands upon the traditional Risk Quadrangle, a\nmodel that quantifies uncertainty through four key components: \\emph{risk,\ndeviation, regret}, and \\emph{error}. The $\\varphi$-Divergence Quadrangle\nincorporates the $\\varphi$-divergence as a measure of the difference between\nprobability distributions, thereby providing a more nuanced understanding of\nrisk. Importantly, the $\\varphi$-Divergence Quadrangle is closely connected\nwith the distributionally robust optimization based on the $\\varphi$-divergence\napproach through the duality theory of convex functionals. To illustrate its\npracticality and versatility, several examples of the $\\varphi$-Divergence\nQuadrangle are provided, including the Quantile Quadrangle. The final portion\nof the paper outlines a case study implementing regression with the Entropic\nValue-at-Risk Quadrangle. The proposed $\\varphi$-Divergence Quadrangle presents\na refined methodology for understanding and managing risk, contributing to the\nongoing development of risk assessment and management strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16525v2"
    },
    {
        "title": "Dynamic Return and Star-Shaped Risk Measures via BSDEs",
        "authors": [
            "Roger J. A. Laeven",
            "Emanuela Rosazza Gianin",
            "Marco Zullino"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This paper establishes characterization results for dynamic return and\nstar-shaped risk measures induced via backward stochastic differential\nequations (BSDEs). We first characterize a general family of static star-shaped\nfunctionals in a locally convex Fr\\'echet lattice. Next, employing the\nPasch-Hausdorff envelope, we build a suitable family of convex drivers of BSDEs\ninducing a corresponding family of dynamic convex risk measures of which the\ndynamic return and star-shaped risk measures emerge as the essential minimum.\nFurthermore, we prove that if the set of star-shaped supersolutions of a BSDE\nis not empty, then there exists, for each terminal condition, at least one\nconvex BSDE with a non-empty set of supersolutions, yielding the minimal\nstar-shaped supersolution. We illustrate our theoretical results in a few\nexamples and demonstrate their usefulness in two applications, to capital\nallocation and portfolio choice.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.03447v2"
    },
    {
        "title": "Importance Sampling for Minimization of Tail Risks: A Tutorial",
        "authors": [
            "Anand Deo",
            "Karthyek Murthy"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This paper provides an introductory overview of how one may employ importance\nsampling effectively as a tool for solving stochastic optimization formulations\nincorporating tail risk measures such as Conditional Value-at-Risk.\nApproximating the tail risk measure by its sample average approximation, while\nappealing due to its simplicity and universality in use, requires a large\nnumber of samples to be able to arrive at risk-minimizing decisions with high\nconfidence. This is primarily due to the rarity with which the relevant tail\nevents get observed in the samples. In simulation, Importance Sampling is among\nthe most prominent methods for substantially reducing the sample requirement\nwhile estimating probabilities of rare events. Can importance sampling be used\nfor optimization as well? If so, what are the ingredients required for making\nimportance sampling an effective tool for optimization formulations involving\nrare events? This tutorial aims to provide an introductory overview of the two\nkey ingredients in this regard, namely, (i) how one may arrive at an importance\nsampling change of measure prescription at every decision, and (ii) the\nprominent techniques available for integrating such a prescription within a\nsolution paradigm for stochastic optimization formulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.04676v1"
    },
    {
        "title": "The Effect of COVID-19 on Cryptocurrencies and the Stock Market\n  Volatility -- A Two-Stage DCC-EGARCH Model Analysis",
        "authors": [
            "Apostolos Ampountolas"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This research examines the correlations between the return volatility of\ncryptocurrencies, global stock market indices, and the spillover effects of the\nCOVID-19 pandemic. For this purpose, we employed a two-stage multivariate\nvolatility exponential GARCH (EGARCH) model with an integrated dynamic\nconditional correlation (DCC) approach to measure the impact on the financial\nportfolio returns from 2019 to 2020. Moreover, we used value-at-risk (VaR) and\nvalue-at-risk measurements based on the Cornish-Fisher expansion (CFVaR). The\nempirical results show significant long- and short-term spillover effects. The\ntwo-stage multivariate EGARCH model's results show that the conditional\nvolatilities of both asset portfolios surge more after positive news and\nrespond well to previous shocks. As a result, financial assets have low\nunconditional volatility and the lowest risk when there are no external\ninterruptions. Despite the financial assets' sensitivity to shocks, they\nexhibit some resistance to fluctuations in market confidence. The VaR\nperformance comparison results with the assets portfolios differ. During the\nCOVID-19 outbreak, the Dow (DJI) index reports VaR's highest loss, followed by\nthe S&P500. Conversely, the CFVaR reports negative risk results for the entire\ncryptocurrency portfolio during the pandemic, except for the Ethereum (ETH).\n",
        "pdf_link": "http://arxiv.org/pdf/2307.09137v1"
    },
    {
        "title": "Ein neuer Ansatz zur Frequenzmodellierung im Versicherungswesen (A new\n  Approach to frequency modeling in risk theory)",
        "authors": [
            "Dietmar Pfeifer"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The collective risk model differentiates usually between claims frequencies\n(and their distribution) and claim sizes (and their distribution). For the\nclaims frequencies typically classical discrete distributions are considered,\nsuch as Binomial-, Negative binomial- or Poisson distributions. Since these\ndistributions sometimes do not really fit to the data we propose a different\napproach here for claim frequencies via random proportions of the number of\ninsurance contracts. This approach also allows for a statistical\ngoodness-of-fit test via quantile-quantile-plots and can likewise be applied to\nthe modelling of claim size distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.04483v1"
    },
    {
        "title": "Probability of Default modelling with Lévy-driven Ornstein-Uhlenbeck\n  processes and applications in credit risk under the IFRS 9",
        "authors": [
            "Kyriakos Georgiou",
            "Athanasios N. Yannacopoulos"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In this paper we develop a framework for estimating Probability of Default\n(PD) based on stochastic models governing an appropriate asset value processes.\nIn particular, we build upon a L\\'evy-driven Ornstein-Uhlenbeck process and\nconsider a generalized model that incorporates multiple latent variables\naffecting the evolution of the process. We obtain an Integral Equation (IE)\nformulation for the corresponding PD as a function of the initial position of\nthe asset value process and the time until maturity, from which we then prove\nthat the PD function satisfies an appropriate Partial Integro-Differential\nEquation (PIDE). These representations allow us to show that appropriate weak\n(viscosity) as well as strong solutions exist, and develop subsequent numerical\nschemes for the estimation of the PD function. Such a framework is necessary\nunder the newly introduced International Financial Reporting Standards (IFRS) 9\nregulation, which has imposed further requirements on the sophistication and\nrigor underlying credit modelling methodologies. We consider special cases of\nthe generalized model that can be used for applications to credit risk\nmodelling and provide examples specific to provisioning under IFRS 9, and more.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.12384v1"
    },
    {
        "title": "Measuring risk contagion in financial networks with CoVaR",
        "authors": [
            "Bikramjit Das",
            "Vicky Fasen-Hartmann"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The stability of a complex financial system may be assessed by measuring risk\ncontagion between various financial institutions with relatively high exposure.\nWe consider a financial network model using a bipartite graph of financial\ninstitutions (e.g., banks, investment companies, insurance firms) on one side\nand financial assets on the other. Following empirical evidence, returns from\nsuch risky assets are modeled by heavy-tailed distributions, whereas their\njoint dependence is characterized by copula models exhibiting a variety of tail\ndependence behavior. We consider CoVaR, a popular measure of risk contagion and\nstudy its asymptotic behavior under broad model assumptions. We further propose\nthe Extreme CoVaR Index (ECI) for capturing the strength of risk contagion\nbetween risk entities in such networks, which is particularly useful for models\nexhibiting asymptotic independence. The results are illustrated by providing\nprecise expressions of CoVaR and ECI when the dependence of the assets is\nmodeled using two well-known multivariate dependence structures: the Gaussian\ncopula and the Marshall-Olkin copula.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.15511v2"
    },
    {
        "title": "Risk factor aggregation and stress testing",
        "authors": [
            "Natalie Packham"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Stress testing refers to the application of adverse financial or\nmacroeconomic scenarios to a portfolio. For this purpose, financial or\nmacroeconomic risk factors are linked with asset returns, typically via a\nfactor model. We expand the range of risk factors by adapting\ndimension-reduction techniques from unsupervised learning, namely PCA and\nautoencoders. This results in aggregated risk factors, encompassing a global\nfactor, factors representing broad geographical regions, and factors specific\nto cyclical and defensive industries. As the adapted PCA and autoencoders\nprovide an interpretation of the latent factors, this methodology is also\nvaluable in other areas where dimension-reduction and explainability are\ncrucial.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.04511v1"
    },
    {
        "title": "On the evolution of data breach reporting patterns and frequency in the\n  United States: a cross-state analysis",
        "authors": [
            "Benjamin Avanzi",
            "Xingyun Tan",
            "Greg Taylor",
            "Bernard Wong"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Understanding the emergence of data breaches is crucial for cyber insurance.\nHowever, analyses of data breach frequency trends in the current literature\nlead to contradictory conclusions. We put forward that those discrepancies may\nbe (at least partially) due to inconsistent data collection standards, as well\nas reporting patterns, over time and space. We set out to carefully control\nboth. In this paper, we conduct a joint analysis of state Attorneys General's\npublications on data breaches across eight states (namely, California,\nDelaware, Indiana, Maine, Montana, North Dakota, Oregon, and Washington), all\nof which are subject to established data collection standards-namely, state\ndata breach (mandatory) notification laws. Thanks to our explicit recognition\nof these notification laws, we are capable of modelling frequency of breaches\nin a consistent and comparable way over time. Hence, we are able to isolate and\ncapture the complexities of reporting patterns, adequately estimate IBNRs, and\nyield a highly reliable assessment of historical frequency trends in data\nbreaches. Our analysis also provides a comprehensive comparison of data breach\nfrequency across the eight U.S. states, extending knowledge on state-specific\ndifferences in cyber risk, which has not been extensively discussed in the\ncurrent literature. Furthermore, we uncover novel features not previously\ndiscussed in the literature, such as differences in cyber risk frequency trends\nbetween large and small data breaches. Overall, we find that the reporting\ndelays are lengthening. We also elicit commonalities and heterogeneities in\nreporting patterns across states, severity levels, and time periods. After\nadequately estimating IBNRs, we find that frequency is relatively stable before\n2020 and increasing after 2020. This is consistent across states. Implications\nof our findings for cyber insurance are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.04786v2"
    },
    {
        "title": "Understanding and managing blockchain protocol risks",
        "authors": [
            "Alex Nathan",
            "Dimosthenis Kaponis",
            "Saul Lustgarten"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This paper addresses the issue of blockchain protocol risks, a foundational\ncategory of risks affecting Distributed Ledger Technology (DLT) which underpins\ndigital assets, smart contracts, and decentralised applications. It presents a\ncomprehensive risk management framework developed in collaboration with\nfinancial institutions, blockchain development teams and regulators that\napplies a traditional risk management taxonomy to address certain overlooked\nblockchain protocol risks. The approach offers a structured way to identify,\nmeasure, monitor and report blockchain protocol risks. The paper provides\nreal-world use cases to demonstrate the practicality and implementation of the\nproposed framework. The findings of this work contribute to the evolving\nunderstanding of blockchain protocol risks and provide valuable insights on how\nthese risks affect the adoption of DLT by financial institutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.10797v1"
    },
    {
        "title": "Mack's estimator motivated by large exposure asymptotics in a compound\n  Poisson setting",
        "authors": [
            "Nils Engler",
            "Filip Lindskog"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The distribution-free chain ladder of Mack justified the use of the chain\nladder predictor and enabled Mack to derive an estimator of conditional mean\nsquared error of prediction for the chain ladder predictor. Classical insurance\nloss models, i.e. of compound Poisson type, are not consistent with Mack's\ndistribution-free chain ladder. However, for a sequence of compound Poisson\nloss models indexed by exposure (e.g. number of contracts), we show that the\nchain ladder predictor and Mack's estimator of conditional mean squared error\nof prediction can be derived by considering large exposure asymptotics. Hence,\nquantifying chain ladder prediction uncertainty can be done with Mack's\nestimator without relying on the validity of the model assumptions of the\ndistribution-free chain ladder.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.12056v1"
    },
    {
        "title": "Law-Invariant Return and Star-Shaped Risk Measures",
        "authors": [
            "Roger J. A. Laeven",
            "Emanuela Rosazza Gianin",
            "Marco Zullino"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  This paper presents novel characterization results for classes of\nlaw-invariant star-shaped functionals. We begin by establishing\ncharacterizations for positively homogeneous and star-shaped functionals that\nexhibit second- or convex-order stochastic dominance consistency. Building on\nthese characterizations, we proceed to derive Kusuoka-type representations for\nthese functionals, shedding light on their mathematical structure and intimate\nconnections to Value-at-Risk and Expected Shortfall. Furthermore, we offer\nrepresentations of general law-invariant star-shaped functionals as\nrobustifications of Value-at-Risk. Notably, our results are versatile,\naccommodating settings that may, or may not, involve monotonicity and/or\ncash-additivity. All of these characterizations are developed within a general\nlocally convex topological space of random variables, ensuring the broad\napplicability of our results in various financial, insurance and probabilistic\ncontexts.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.19552v1"
    },
    {
        "title": "Tail Risk and Systemic Risk Estimation of Cryptocurrencies: an\n  Expectiles and Marginal Expected Shortfall based approach",
        "authors": [
            "Andrea Teruzzi"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The issue related to the quantification of the tail risk of cryptocurrencies\nis considered in this paper. The statistical methods used in the study are\nthose concerning recent developments in Extreme Value Theory (EVT) for weakly\ndependent data. This research proposes an expectile-based approach for\nassessing the tail risk of dependent data. Expectile is a summary statistic\nthat generalizes the concept of mean, as the quantile generalizes the concept\nof the median. We present the empirical findings for a dataset of\ncryptocurrencies. We propose a method for dynamically evaluating the level of\nthe expectiles by estimating the level of the expectiles of the residuals of a\nheteroscedastic regression, such as a GARCH model. Finally, we introduce the\nMarginal Expected Shortfall (MES) as a tool for measuring the marginal impact\nof single assets on systemic shortfalls. In our case of interest, we are\nfocused on the impact of a single cryptocurrency on the systemic risk of the\nwhole cryptocurrency market. In particular, we present an expectile-based MES\nfor dependent data.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.17239v1"
    },
    {
        "title": "Cross-Domain Behavioral Credit Modeling: transferability from private to\n  central data",
        "authors": [
            "O. Didkovskyi",
            "N. Jean",
            "G. Le Pera",
            "C. Nordio"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper introduces a credit risk rating model for credit risk assessment\nin quantitative finance, aiming to categorize borrowers based on their\nbehavioral data. The model is trained on data from Experian, a widely\nrecognized credit bureau, to effectively identify instances of loan defaults\namong bank customers. Employing state-of-the-art statistical and machine\nlearning techniques ensures the model's predictive accuracy. Furthermore, we\nassess the model's transferability by testing it on behavioral data from the\nBank of Italy, demonstrating its potential applicability across diverse\ndatasets during prediction. This study highlights the benefits of incorporating\nexternal behavioral data to improve credit risk assessment in financial\ninstitutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.09778v1"
    },
    {
        "title": "Cash non-additive risk measures: horizon risk and generalized entropy",
        "authors": [
            "Giulia Di Nunno",
            "Emanuela Rosazza Gianin"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Horizon risk (see arXiv:2301.04971) is studied in the context of cash\nnon-additive fully-dynamic risk measures induced by BSDEs. Furthermore, we\nintroduce a risk measure based on generalized Tsallis entropy which can\ndynamically evaluate the riskiness of losses considering both horizon risk and\ninterest rate uncertainty. The new q-entropic risk measure on losses can be\nused as a quantification of capital requirement.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.14443v3"
    },
    {
        "title": "Analysis of an aggregate loss model in a Markov renewal regime",
        "authors": [
            "Pepa Ramírez-Cobo",
            "Emilio Carrizosa",
            "Rosa Elvira Lillo"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this article we consider an aggregate loss model with dependent losses.\nThe losses occurrence process is governed by a two-state Markovian arrival\nprocess (MAP2), a Markov renewal process process that allows for (1) correlated\ninter-losses times, (2) non-exponentially distributed inter-losses times and,\n(3) overdisperse losses counts. Some quantities of interest to measure\npersistence in the loss occurrence process are obtained. Given a real\noperational risk database, the aggregate loss model is estimated by fitting\nseparately the inter-losses times and severities. The MAP2 is estimated via\ndirect maximization of the likelihood function, and severities are modeled by\nthe heavy-tailed, double-Pareto Lognormal distribution. In comparison with the\nfit provided by the Poisson process, the results point out that taking into\naccount the dependence and overdispersion in the inter-losses times\ndistribution leads to higher capital charges.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.14553v2"
    },
    {
        "title": "Tail risk forecasting with semi-parametric regression models by\n  incorporating overnight information",
        "authors": [
            "Cathy W. S. Chen",
            "Takaaki Koike",
            "Wei-Hsuan Shau"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This research incorporates realized volatility and overnight information into\nrisk models, wherein the overnight return often contributes significantly to\nthe total return volatility. Extending a semi-parametric regression model based\non asymmetric Laplace distribution, we propose a family of RES-CAViaR-oc models\nby adding overnight return and realized measures as a nowcasting technique for\nsimultaneously forecasting Value-at-Risk (VaR) and expected shortfall (ES). We\nutilize Bayesian methods to estimate unknown parameters and forecast VaR and ES\njointly for the proposed model family. We also conduct extensive backtests\nbased on joint elicitability of the pair of VaR and ES during the out-of sample\nperiod. Our empirical study on four international stock indices confirms that\novernight return and realized volatility are vital in tail risk forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.07134v1"
    },
    {
        "title": "Interbank network reconstruction enforcing density and reciprocity",
        "authors": [
            "Valentina Macchiati",
            "Piero Mazzarisi",
            "Diego Garlaschelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Networks of financial exposures are the key propagators of risk and distress\namong banks, but their empirical structure is not publicly available because of\nconfidentiality. This limitation has triggered the development of methods of\nnetwork reconstruction from partial, aggregate information. Unfortunately, even\nthe best methods available fail in replicating the number of directed cycles,\nwhich on the other hand play a crucial role in determining graph spectra and\nhence the degree of network stability and systemic risk. Here we address this\nchallenge by exploiting the hypothesis that the statistics of higher-order\ncycles is strongly constrained by that of the shortest ones, i.e. by the amount\nof dyads with reciprocated links. First, we provide a detailed analysis of link\nreciprocity on the e-MID dataset of Italian banks, finding that correlations\nbetween reciprocal links systematically increase with the temporal resolution,\ntypically changing from negative to positive around a timescale of up to 50\ndays. Then, we propose a new network reconstruction method capable of\nenforcing, only from the knowledge of aggregate interbank assets and\nliabilities, both a desired sparsity and a desired link reciprocity. We confirm\nthat the addition of reciprocity dramatically improves the prediction of\nseveral structural and spectral network properties, including the largest real\neigenvalue and the eccentricity of the elliptical distribution of the other\neigenvalues in the complex plane. These results illustrate the importance of\ncorrectly addressing the temporal resolution and the resulting level of\nreciprocity in the reconstruction of financial networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.11136v2"
    },
    {
        "title": "Valuing insurance against small probability risks: A meta-analysis",
        "authors": [
            "Selim Mankaï",
            "Sébastien Marchand",
            "Ngoc Ha Le"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The demand for voluntary insurance against low-probability, high-impact risks\nis lower than expected. To assess the magnitude of the demand, we conduct a\nmeta-analysis of contingent valuation studies using a dataset of experimentally\nelicited and survey-based estimates. We find that the average stated\nwillingness to pay (WTP) for insurance is 87% of expected losses. We perform a\nmeta-regression analysis to examine the heterogeneity in aggregate WTP across\nthese studies. The meta-regression reveals that information about loss\nprobability and probability levels positively influence relative willingness to\npay, whereas respondents' average income and age have a negative effect.\nMoreover, we identify cultural sub-factors, such as power distance and\nuncertainty avoidance, that provided additional explanations for differences in\nWTP across international samples. Methodological factors related to the\nsampling and data collection process significantly influence the stated WTP.\nOur results, robust to model specification and publication bias, are relevant\nto current debates on stated preferences for low-probability risks management.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.16375v1"
    },
    {
        "title": "Modeling and Analysis of Crypto-Backed Over-Collateralized Stable\n  Derivatives in DeFi",
        "authors": [
            "Zhenbang Feng",
            "Hardhik Mohanty",
            "Bhaskar Krishnamachari"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In decentralized finance (DeFi), stablecoins like DAI are designed to offer a\nstable value amidst the fluctuating nature of cryptocurrencies. We examine the\nclass of crypto-backed stable derivatives, with a focus on mechanisms for price\nstabilization, which is exemplified by the well-known stablecoin DAI from\nMakerDAO. For simplicity, we focus on a single-collateral setting. We introduce\na belief parameter to the simulation model of DAI in a previous work (DAISIM),\nreflecting market sentiments about the value and stability of DAI, and show\nthat it better matches the expected behavior when this parameter is set to a\nsufficiently high value. We also propose a simple mathematical model of DAI\nprice to explain its stability and dependency on ETH price. Finally, we analyze\npossible risk factors associated with these stable derivatives to provide\nvaluable insights for stakeholders in the DeFi ecosystem.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18119v1"
    },
    {
        "title": "Properties of the entropic risk measure EVaR in relation to selected\n  distributions",
        "authors": [
            "Yuliya Mishura",
            "Kostiantyn Ralchenko",
            "Petro Zelenko",
            "Volodymyr Zubchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Entropic Value-at-Risk (EVaR) measure is a convenient coherent risk measure.\nDue to certain difficulties in finding its analytical representation, it was\npreviously calculated explicitly only for the normal distribution. We succeeded\nto overcome these difficulties and to calculate Entropic Value-at-Risk (EVaR)\nmeasure for Poisson, compound Poisson, Gamma, Laplace, exponential,\nchi-squared, inverse Gaussian distribution and normal inverse Gaussian\ndistribution with the help of Lambert function that is a special function,\ngenerally speaking, with two branches.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.01468v1"
    },
    {
        "title": "On Geometrically Convex Risk Measures",
        "authors": [
            "Mücahit Aygün",
            "Fabio Bellini",
            "Roger J. A. Laeven"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Geometrically convex functions constitute an interesting class of functions\nobtained by replacing the arithmetic mean with the geometric mean in the\ndefinition of convexity. As recently suggested, geometric convexity may be a\nsensible property for financial risk measures ([7,13,4]).\n  We introduce a notion of GG-convex conjugate, parallel to the classical\nnotion of convex conjugate introduced by Fenchel, and we discuss its\nproperties. We show how GG-convex conjugation can be axiomatized in the spirit\nof the notion of general duality transforms introduced in [2,3].\n  We then move to the study of GG-convex risk measures, which are defined as\nGG-convex functionals defined on suitable spaces of random variables. We derive\na general dual representation that extends analogous expressions presented in\n[4] under the additional assumptions of monotonicity and positive homogeneity.\nAs a prominent example, we study the family of Orlicz risk measures. Finally,\nwe introduce multiplicative versions of the convex and of the increasing convex\norder and discuss related consistency properties of law-invariant GG-convex\nrisk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06188v1"
    },
    {
        "title": "Financial Default Prediction via Motif-preserving Graph Neural Network\n  with Curriculum Learning",
        "authors": [
            "Daixin Wang",
            "Zhiqiang Zhang",
            "Yeyu Zhao",
            "Kai Huang",
            "Yulin Kang",
            "Jun Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  User financial default prediction plays a critical role in credit risk\nforecasting and management. It aims at predicting the probability that the user\nwill fail to make the repayments in the future. Previous methods mainly extract\na set of user individual features regarding his own profiles and behaviors and\nbuild a binary-classification model to make default predictions. However, these\nmethods cannot get satisfied results, especially for users with limited\ninformation. Although recent efforts suggest that default prediction can be\nimproved by social relations, they fail to capture the higher-order topology\nstructure at the level of small subgraph patterns. In this paper, we fill in\nthis gap by proposing a motif-preserving Graph Neural Network with curriculum\nlearning (MotifGNN) to jointly learn the lower-order structures from the\noriginal graph and higherorder structures from multi-view motif-based graphs\nfor financial default prediction. Specifically, to solve the problem of weak\nconnectivity in motif-based graphs, we design the motif-based gating mechanism.\nIt utilizes the information learned from the original graph with good\nconnectivity to strengthen the learning of the higher-order structure. And\nconsidering that the motif patterns of different samples are highly unbalanced,\nwe propose a curriculum learning mechanism on the whole learning process to\nmore focus on the samples with uncommon motif distributions. Extensive\nexperiments on one public dataset and two industrial datasets all demonstrate\nthe effectiveness of our proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06482v1"
    },
    {
        "title": "A Markov approach to credit rating migration conditional on economic\n  states",
        "authors": [
            "Michael Kalkbrener",
            "Natalie Packham"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We develop a model for credit rating migration that accounts for the impact\nof economic state fluctuations on default probabilities. The joint process for\nthe economic state and the rating is modelled as a time-homogeneous Markov\nchain. While the rating process itself possesses the Markov property only under\nrestrictive conditions, methods from Markov theory can be used to derive the\nrating process' asymptotic behaviour. We use the mathematical framework to\nformalise and analyse different rating philosophies, such as point-in-time\n(PIT) and through-the-cycle (TTC) ratings. Furthermore, we introduce stochastic\norders on the bivariate process' transition matrix to establish a consistent\nnotion of \"better\" and \"worse\" ratings. Finally, the construction of PIT and\nTTC ratings is illustrated on a Merton-type firm-value process.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.14868v1"
    },
    {
        "title": "Innovative Application of Artificial Intelligence Technology in Bank\n  Credit Risk Management",
        "authors": [
            "Shuochen Bi",
            "Wenqing Bao"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  With the rapid growth of technology, especially the widespread application of\nartificial intelligence (AI) technology, the risk management level of\ncommercial banks is constantly reaching new heights. In the current wave of\ndigitalization, AI has become a key driving force for the strategic\ntransformation of financial institutions, especially the banking industry. For\ncommercial banks, the stability and safety of asset quality are crucial, which\ndirectly relates to the long-term stable growth of the bank. Among them, credit\nrisk management is particularly core because it involves the flow of a large\namount of funds and the accuracy of credit decisions. Therefore, establishing a\nscientific and effective credit risk decision-making mechanism is of great\nstrategic significance for commercial banks. In this context, the innovative\napplication of AI technology has brought revolutionary changes to bank credit\nrisk management. Through deep learning and big data analysis, AI can accurately\nevaluate the credit status of borrowers, timely identify potential risks, and\nprovide banks with more accurate and comprehensive credit decision support. At\nthe same time, AI can also achieve realtime monitoring and early warning,\nhelping banks intervene before risks occur and reduce losses.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.18183v1"
    },
    {
        "title": "Calibration of the rating transition model for high and low default\n  portfolios",
        "authors": [
            "Jian He",
            "Asma Khedher",
            "Peter Spreij"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this paper we develop Maximum likelihood (ML) based algorithms to\ncalibrate the model parameters in credit rating transition models. Since the\ncredit rating transition models are not Gaussian linear models, the celebrated\nKalman filter is not suitable to compute the likelihood of observed migrations.\nTherefore, we develop a Laplace approximation of the likelihood function and as\na result the Kalman filter can be used in the end to compute the likelihood\nfunction. This approach is applied to so-called high-default portfolios, in\nwhich the number of migrations (defaults) is large enough to obtain high\naccuracy of the Laplace approximation. By contrast, low-default portfolios have\na limited number of observed migrations (defaults). Therefore, in order to\ncalibrate low-default portfolios, we develop a ML algorithm using a particle\nfilter (PF) and Gaussian process regression. Experiments show that both\nalgorithms are efficient and produce accurate approximations of the likelihood\nfunction and the ML estimates of the model parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.00576v1"
    },
    {
        "title": "Explainable Risk Classification in Financial Reports",
        "authors": [
            "Xue Wen Tan",
            "Stanley Kok"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Every publicly traded company in the US is required to file an annual 10-K\nfinancial report, which contains a wealth of information about the company. In\nthis paper, we propose an explainable deep-learning model, called FinBERT-XRC,\nthat takes a 10-K report as input, and automatically assesses the post-event\nreturn volatility risk of its associated company. In contrast to previous\nsystems, our proposed model simultaneously offers explanations of its\nclassification decision at three different levels: the word, sentence, and\ncorpus levels. By doing so, our model provides a comprehensive interpretation\nof its prediction to end users. This is particularly important in financial\ndomains, where the transparency and accountability of algorithmic predictions\nplay a vital role in their application to decision-making processes. Aside from\nits novel interpretability, our model surpasses the state of the art in\npredictive accuracy in experiments on a large real-world dataset of 10-K\nreports spanning six years.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.01881v3"
    },
    {
        "title": "Coherent Risk Measure on $L^0$: NA Condition, Pricing and Dual\n  Representation",
        "authors": [
            "Emmanuel Lepinette",
            "Duc Thinh Vu"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The NA condition is one of the pillars supporting the classical theory of\nfinancial mathematics. We revisit this condition for financial market models\nwhere a dynamic risk-measure defined on $L^0$ is fixed to characterize the\nfamily of acceptable wealths that play the role of non negative financial\npositions. We provide in this setting a new version of the fundamental theorem\nof asset pricing and we deduce a dual characterization of the super-hedging\nprices (called risk-hedging prices) of a European option. Moreover, we show\nthat the set of all risk-hedging prices is closed under NA. At last, we provide\na dual representation of the risk-measure on $L^0$ under some conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06764v1"
    },
    {
        "title": "Application of Natural Language Processing in Financial Risk Detection",
        "authors": [
            "Liyang Wang",
            "Yu Cheng",
            "Ao Xiang",
            "Jingyu Zhang",
            "Haowei Yang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper explores the application of Natural Language Processing (NLP) in\nfinancial risk detection. By constructing an NLP-based financial risk detection\nmodel, this study aims to identify and predict potential risks in financial\ndocuments and communications. First, the fundamental concepts of NLP and its\ntheoretical foundation, including text mining methods, NLP model design\nprinciples, and machine learning algorithms, are introduced. Second, the\nprocess of text data preprocessing and feature extraction is described.\nFinally, the effectiveness and predictive performance of the model are\nvalidated through empirical research. The results show that the NLP-based\nfinancial risk detection model performs excellently in risk identification and\nprediction, providing effective risk management tools for financial\ninstitutions. This study offers valuable references for the field of financial\nrisk management, utilizing advanced NLP techniques to improve the accuracy and\nefficiency of financial risk detection.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.09765v2"
    },
    {
        "title": "Hedging in Sequential Experiments",
        "authors": [
            "Thomas Cook",
            "Patrick Flaherty"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Experimentation involves risk. The investigator expends time and money in the\npursuit of data that supports a hypothesis. In the end, the investigator may\nfind that all of these costs were for naught and the data fail to reject the\nnull. Furthermore, the investigator may not be able to test other hypotheses\nwith the same data set in order to avoid false positives due to p-hacking.\nTherefore, there is a need for a mechanism for investigators to hedge the risk\nof financial and statistical bankruptcy in the business of experimentation.\n  In this work, we build on the game-theoretic statistics framework to enable\nan investigator to hedge their bets against the null hypothesis and thus avoid\nruin. First, we describe a method by which the investigator's test martingale\nwealth process can be capitalized by solving for the risk-neutral price. Then,\nwe show that a portfolio that comprises the risky test martingale and a\nrisk-free process is still a test martingale which enables the investigator to\nselect a particular risk-return position using Markowitz portfolio theory.\nFinally, we show that a function that is derivative of the test martingale\nprocess can be constructed and used as a hedging instrument by the investigator\nor as a speculative instrument by a risk-seeking investor who wants to\nparticipate in the potential returns of the uncertain experiment wealth\nprocess. Together, these instruments enable an investigator to hedge the risk\nof ruin and they enable a investigator to efficiently hedge experimental risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.15867v1"
    },
    {
        "title": "Multiple split approach -- multidimensional probabilistic forecasting of\n  electricity markets",
        "authors": [
            "Katarzyna Maciejowska",
            "Weronika Nitka"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In this article, a multiple split method is proposed that enables\nconstruction of multidimensional probabilistic forecasts of a selected set of\nvariables. The method uses repeated resampling to estimate uncertainty of\nsimultaneous multivariate predictions. This nonparametric approach links the\ngap between point and probabilistic predictions and can be combined with\ndifferent point forecasting methods. The performance of the method is evaluated\nwith data describing the German short-term electricity market. The results show\nthat the proposed approach provides highly accurate predictions. The gains from\nmultidimensional forecasting are the largest when functions of variables, such\nas price spread or residual load, are considered.\n  Finally, the method is used to support a decision process of a moderate\ngeneration utility that produces electricity from wind energy and sells it on\neither a day-ahead or an intraday market. The company makes decisions under\nhigh uncertainty because it knows neither the future production level nor the\nprices. We show that joint forecasting of both market prices and fundamentals\ncan be used to predict the distribution of a profit, and hence helps to design\na strategy that balances a level of income and a trading risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.07795v1"
    },
    {
        "title": "Testing for the Asymmetric Optimal Hedge Ratios: With an Application to\n  Bitcoin",
        "authors": [
            "Abdulnasser Hatemi-J"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Reducing financial risk is of paramount importance to investors, financial\ninstitutions, and corporations. Since the pioneering contribution of Johnson\n(1960), the optimal hedge ratio based on futures is regularly utilized. The\ncurrent paper suggests an explicit and efficient method for testing the null\nhypothesis of a symmetric optimal hedge ratio against an asymmetric alternative\none within a multivariate setting. If the null is rejected, the position\ndependent optimal hedge ratios can be estimated via the suggested model. This\napproach is expected to enhance the accuracy of the implemented hedging\nstrategies compared to the standard methods since it accounts for the fact that\nthe source of risk depends on whether the investor is a buyer or a seller of\nthe risky asset. An application is provided using spot and futures prices of\nBitcoin. The results strongly support the view that the optimal hedge ratio for\nthis cryptocurrency is position dependent. The investor that is long in Bitcoin\nhas a much higher conditional optimal hedge ratio compared to the one that is\nshort in the asset. The difference between the two conditional optimal hedge\nratios is statistically significant, which has important repercussions for\nimplementing risk management strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19932v2"
    },
    {
        "title": "Bonus-malus Systems vs Delays in Claims Reporting and Settlement:\n  Analysis of Ruin Probabilities",
        "authors": [
            "Dhiti Osatakul",
            "Shuanming Li",
            "Xueyuan Wu"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Our paper explores a discrete-time risk model with time-varying premiums,\ninvestigating two types of correlated claims: main claims and by-claims.\nSettlement of the by-claims can be delayed for one time period, representing\nreal-world insurance practices. We examine two premium principles based on\nreported and settled claims, using recursively computable finite-time ruin\nprobabilities to evaluate the performance of time-varying premiums. Our\nfindings suggest that, under specific assumptions, a higher probability of\nby-claim settlement delays leads to lower ruin probabilities. Moreover, a\nstronger correlation between main claims and their associated by-claims results\nin higher ruin probabilities. Lastly, the premium adjustment principles based\non settled claims experience contribute to higher ruin probabilities compared\nto those based on reported claims experience, assuming all other factors remain\nconstant. Notably, this difference becomes more pronounced when there is a high\nlikelihood of by-claim delays.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.00003v1"
    },
    {
        "title": "Loss-based Bayesian Sequential Prediction of Value at Risk with a\n  Long-Memory and Non-linear Realized Volatility Model",
        "authors": [
            "Rangika Peiris",
            "Minh-Ngoc Tran",
            "Chao Wang",
            "Richard Gerlach"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  A long memory and non-linear realized volatility model class is proposed for\ndirect Value at Risk (VaR) forecasting. This model, referred to as RNN-HAR,\nextends the heterogeneous autoregressive (HAR) model, a framework known for\nefficiently capturing long memory in realized measures, by integrating a\nRecurrent Neural Network (RNN) to handle non-linear dynamics. Loss-based\ngeneralized Bayesian inference with Sequential Monte Carlo is employed for\nmodel estimation and sequential prediction in RNN HAR. The empirical analysis\nis conducted using daily closing prices and realized measures from 2000 to 2022\nacross 31 market indices. The proposed models one step ahead VaR forecasting\nperformance is compared against a basic HAR model and its extensions. The\nresults demonstrate that the proposed RNN-HAR model consistently outperforms\nall other models considered in the study.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.13588v1"
    },
    {
        "title": "Quantifying the degree of risk aversion of spectral risk measures",
        "authors": [
            "E. Ruben van Beesten"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  I propose a functional on the space of spectral risk measures that quantifies\ntheir ``degree of risk aversion''. This quantification formalizes the idea that\nsome risk measures are ``more risk-averse'' than others. I construct the\nfunctional using two axioms: a normalization on the space of CVaRs and a\nlinearity axiom. I present two formulas for the functional and discuss several\nproperties and interpretations.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.15675v2"
    },
    {
        "title": "Claims processing and costs under capacity constraints",
        "authors": [
            "Filip Lindskog",
            "Mario V. Wüthrich"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Random delays between the occurrence of accident events and the corresponding\nreporting times of insurance claims is a standard feature of insurance data.\nThe time lag between the reporting and the processing of a claim depends on\nwhether the claim can be processed without delay as it arrives or whether it\nremains unprocessed for some time because of temporarily insufficient\nprocessing capacity that is shared between all incoming claims. We aim to\nexplain and analyze the nature of processing delays and build-up of backlogs.\nWe show how to select processing capacity optimally in order to minimize claims\ncosts, taking delay-adjusted costs and fixed costs for claims settlement\ncapacity into account. Theoretical results are combined with a large-scale\nnumerical study that demonstrates practical usefulness of our proposal.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.09091v1"
    },
    {
        "title": "Research and Design of a Financial Intelligent Risk Control Platform\n  Based on Big Data Analysis and Deep Machine Learning",
        "authors": [
            "Shuochen Bi",
            "Yufan Lian",
            "Ziyue Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In the financial field of the United States, the application of big data\ntechnology has become one of the important means for financial institutions to\nenhance competitiveness and reduce risks. The core objective of this article is\nto explore how to fully utilize big data technology to achieve complete\nintegration of internal and external data of financial institutions, and create\nan efficient and reliable platform for big data collection, storage, and\nanalysis. With the continuous expansion and innovation of financial business,\ntraditional risk management models are no longer able to meet the increasingly\ncomplex market demands. This article adopts big data mining and real-time\nstreaming data processing technology to monitor, analyze, and alert various\nbusiness data. Through statistical analysis of historical data and precise\nmining of customer transaction behavior and relationships, potential risks can\nbe more accurately identified and timely responses can be made. This article\ndesigns and implements a financial big data intelligent risk control platform.\nThis platform not only achieves effective integration, storage, and analysis of\ninternal and external data of financial institutions, but also intelligently\ndisplays customer characteristics and their related relationships, as well as\nintelligent supervision of various risk information\n",
        "pdf_link": "http://arxiv.org/pdf/2409.10331v1"
    },
    {
        "title": "Systemic Risk Asymptotics in a Renewal Model with Multiple Business\n  Lines and Heterogeneous Claims",
        "authors": [
            "Bingzhen Geng",
            "Yang Liu",
            "Hongfu Wan"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Systemic risk is receiving increasing attention in the insurance industry, as\nthese risks can have severe impacts on the entire financial system. In this\npaper, we propose a multi-dimensional L/'{e}vy process-based renewal risk model\nwith heterogeneous insurance claims, where every dimension indicates a business\nline of an insurer. We use the systemic expected shortfall (SES) and marginal\nexpected shortfall (MES) defined with a Value-at-Risk (VaR) target level as the\nmeasurement of systemic risks. Assuming that all the claim sizes are pairwise\nasymptotically independent (PAI), we derive asymptotic formulas for the tail\nprobabilities of discounted aggregate claims and total loss, which holds\nuniformly for all time horizons. We further obtain the asymptotics of the above\nsystemic risk measures. The main technical issues involve the treatment of\nuniform convergence in the dynamic time setting. Finally, we conduct a Monte\nCarlo numerical study and verify that our asymptotics are accurate and\nconvenient in computation.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00158v1"
    },
    {
        "title": "Application of AI in Credit Risk Scoring for Small Business Loans: A\n  case study on how AI-based random forest model improves a Delphi model\n  outcome in the case of Azerbaijani SMEs",
        "authors": [
            "Nigar Karimova"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The research investigates how the application of a machine-learning random\nforest model improves the accuracy and precision of a Delphi model. The context\nof the research is Azerbaijani SMEs and the data for the study has been\nobtained from a financial institution which had gathered it from the\nenterprises (as there is no public data on local SMEs, it was not practical to\nverify the data independently). The research used accuracy, precision, recall\nand F-1 scores for both models to compare them and run the algorithms in\nPython. The findings showed that accuracy, precision, recall and F- 1 all\nimprove considerably (from 0.69 to 0.83, from 0.65 to 0.81, from 0.56 to 0.77\nand from 0.58 to 0.79, respectively). The implications are that by applying AI\nmodels in credit risk modeling, financial institutions can improve the accuracy\nof identifying potential defaulters which would reduce their credit risk. In\naddition, an unfair rejection of credit access for SMEs would also go down\nhaving a significant contribution to an economic growth in the economy.\nFinally, such ethical issues as transparency of algorithms and biases in\nhistorical data should be taken on board while making decisions based on AI\nalgorithms in order to reduce mechanical dependence on algorithms that cannot\nbe justified in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.05330v1"
    },
    {
        "title": "An Innovative Attention-based Ensemble System for Credit Card Fraud\n  Detection",
        "authors": [
            "Mehdi Hosseini Chagahi",
            "Niloufar Delfan",
            "Saeed Mohammadi Dashtaki",
            "Behzad Moshiri",
            "Md. Jalil Piran"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Detecting credit card fraud (CCF) holds significant importance due to its\nrole in safeguarding consumers from unauthorized transactions that have the\npotential to result in financial detriment and negative impacts on their credit\nrating. It aids financial institutions in upholding the reliability of their\npayment mechanisms and circumventing the expensive procedure of compensating\nfor deceitful transactions. The utilization of Artificial Intelligence\nmethodologies demonstrated remarkable efficacy in the identification of credit\ncard fraud instances. Within this study, we present a unique attention-based\nensemble model. This model is enhanced by adding an attention layer for\nintegration of first layer classifiers' predictions and a selection layer for\nchoosing the best integrated value. The attention layer is implemented with two\naggregation operators: dependent ordered weighted averaging (DOWA) and induced\nordered weighted averaging (IOWA). The performance of the IOWA operator is very\nclose to the learning algorithm in neural networks which is based on the\ngradient descent optimization method, and performing the DOWA operator is based\non weakening the classifiers that make outlier predictions compared to other\nlearners. Both operators have a sufficient level of complexity for the\nrecognition of complex patterns. Accuracy and diversity are the two criteria we\nuse for selecting the classifiers whose predictions are to be integrated by the\ntwo aggregation operators. Using a bootstrap forest, we identify the 13 most\nsignificant features of the dataset that contribute the most to CCF detection\nand use them to feed the proposed model. Exhibiting its efficacy, the ensemble\nmodel attains an accuracy of 99.95% with an area under the curve (AUC) of 1.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.09069v1"
    },
    {
        "title": "Time-Series Foundation Model for Value-at-Risk Forecasting",
        "authors": [
            "Anubha Goel",
            "Puneet Pasricha",
            "Juho Kanniainen"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This study is the first to explore the performance of a time-series\nfoundation model for Value-at-Risk (VaR) forecasting. Foundation models,\npre-trained on vast and varied datasets, can be used in a zero-shot setting\nwith relatively minimal data or further improved through finetuning. We compare\nthe performance of Google's model, called TimesFM, against conventional\nparametric and non-parametric models, including GARCH, Generalized\nAutoregressive Score (GAS), and empirical quantile estimates, using daily\nreturns from the S\\&P 100 index and its constituents over 19 years. Our\nbacktesting results indicate that in terms of the actual-over-expected ratio,\nthe fine-tuned TimesFM model consistently outperforms traditional methods.\nRegarding the quantile score loss function, it achieves performance comparable\nto the best econometric approach, the GAS model. Overall, the foundation model\nis either the best or among the top performers in forecasting VaR across the\n0.01, 0.025, 0.05, and 0.1 VaR levels. Fine-tuning significantly improves\naccuracy, indicating that zero-shot use is not optimal for VaR forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.11773v5"
    },
    {
        "title": "Sensitivity Analysis of Ruin of an Insurance Company in Ghana",
        "authors": [
            "Daniel Tawiah Pabifio"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  An insurance company, as a risk bearer, is exposed to the likelihood of\nrunning into ruin. This is the situation where the initial surplus falls below\nzero. There is the need to find the required start-up capital to hedge against\ninsolvency. Most researchers, irrespective of whether the test for claim\ndependency holds or not, assume claim independence in their computing of ruin\nprobabilities to start up their initial capital. The objective of this study is\nto carry out comparative sensitivity analysis of ruin probability under both\nassumptions of dependence and independence, irrespective of whether the data\nexhibits independence or not, based on data from an insurance company in Ghana.\nSecondary data from an insurance company was obtained from the National\nInsurance Commission (NIC) for the period of 2013 to 2017. The study employed\ncopulas to determine the claim dependence among the various insurance products\nand the company in general. The study concluded that when there is dependence\nin the claim data, computing the ruin probability based on the assumption of\nindependence results in underestimation. Among the various insurance products,\nthe most profitable insurance product was motor insurance, and Fire and Allied\ninsurance exhibited the highest dependency. At a higher start-up capital, when\nclaims are dependent, assuming independence in calculating the ruin probability\nresults in a significant difference. Hence, it was recommended that insurance\ncompanies should adopt the assumption of dependence between the claims data as\nthe initial reserves become larger, particularly for larger insurance\ncompanies, to avoid misleading results. Also, awareness of the perils and\nconsequences of fire outbreaks and disasters should be raised to the general\npublic to reduce the risk of frequent occurrence.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.11846v1"
    },
    {
        "title": "Optimization of Actuarial Neural Networks with Response Surface\n  Methodology",
        "authors": [
            "Belguutei Ariuntugs",
            "Kehelwala Dewage Gayan Madurang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In the data-driven world of actuarial science, machine learning (ML) plays a\ncrucial role in predictive modeling, enhancing risk assessment and pricing\nstrategies. Neural networks, specifically combined actuarial neural networks\n(CANN), are vital for tasks such as mortality forecasting and pricing. However,\noptimizing hyperparameters (e.g., learning rates, layers) is essential for\nresource efficiency.\n  This study utilizes a factorial design and response surface methodology (RSM)\nto optimize CANN performance. RSM effectively explores the hyperparameter space\nand captures potential curvature, outperforming traditional grid search. Our\nresults show accurate performance predictions, identifying critical\nhyperparameters. By dropping statistically insignificant hyperparameters, we\nreduced runs from 288 to 188, with negligible loss in accuracy, achieving\nnear-optimal out-of-sample Poisson deviance loss.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12824v1"
    },
    {
        "title": "Efficient Nested Estimation of CoVaR: A Decoupled Approach",
        "authors": [
            "Nifei Lin",
            "Yingda Song",
            "L. Jeff Hong"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper addresses the estimation of the systemic risk measure known as\nCoVaR, which quantifies the risk of a financial portfolio conditional on\nanother portfolio being at risk. We identify two principal challenges:\nconditioning on a zero-probability event and the repricing of portfolios. To\ntackle these issues, we propose a decoupled approach utilizing smoothing\ntechniques and develop a model-independent theoretical framework grounded in a\nfunctional perspective. We demonstrate that the rate of convergence of the\ndecoupled estimator can achieve approximately $O_{\\rm P}(\\Gamma^{-1/2})$, where\n$\\Gamma$ represents the computational budget. Additionally, we establish the\nsmoothness of the portfolio loss functions, highlighting its crucial role in\nenhancing sample efficiency. Our numerical results confirm the effectiveness of\nthe decoupled estimators and provide practical insights for the selection of\nappropriate smoothing techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.01319v1"
    },
    {
        "title": "A Personal data Value at Risk Approach",
        "authors": [
            "Luis Enriquez"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  What if the main data protection vulnerability is risk management? Data\nProtection merges three disciplines: data protection law, information security,\nand risk management. Nonetheless, very little research has been made on the\nfield of data protection risk management, where subjectivity and superficiality\nare the dominant state of the art. Since the GDPR tells you what to do, but not\nhow to do it, the solution for approaching GDPR compliance is still a gray\nzone, where the trend is using the rule of thumb. Considering that the most\nimportant goal of risk management is to reduce uncertainty in order to take\ninformed decisions, risk management for the protection of the rights and\nfreedoms of the data subjects cannot be disconnected from the impact\nmaterialization that data controllers and processors need to assess. This paper\nproposes a quantitative approach to data protection risk-based compliance from\na data controllers perspective, with the aim of proposing a mindset change,\nwhere data protection impact assessments can be improved by using data\nprotection analytics, quantitative risk analysis, and calibrating expert\nopinions.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.03217v2"
    },
    {
        "title": "Some remarks on the effect of risk sharing and diversification for\n  infinite mean risks",
        "authors": [
            "Alfred Müller"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The basic principle of any version of insurance is the paradigm that\nexchanging risk by sharing it in a pool is beneficial for the participants. In\ncase of independent risks with a finite mean this is the case for risk averse\ndecision makers. The situation may be very different in case of infinite mean\nmodels. In that case it is known that risk sharing may have a negative effect,\nwhich is sometimes called the nondiversification trap. This phenomenon is well\nknown for infinite mean stable distributions. In a series of recent papers\nsimilar results for infinite mean Pareto and Fr\\'echet distributions have been\nobtained. We further investigate this property by showing that many of these\nresults can be obtained as special cases of a simple result demonstrating that\nthis holds for any distribution that is more skewed than a Cauchy distribution.\nWe also relate this to the situation of deadly catastrophic risks, where we\nassume a positive probability for an infinite value. That case gives a very\nsimple intuition why this phenomenon can occur for such catastrophic risks. We\nalso mention several open problems and conjectures in this context.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.10139v1"
    },
    {
        "title": "KACDP: A Highly Interpretable Credit Default Prediction Model",
        "authors": [
            "Kun Liu",
            "Jin Zhao"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In the field of finance, the prediction of individual credit default is of\nvital importance. However, existing methods face problems such as insufficient\ninterpretability and transparency as well as limited performance when dealing\nwith high-dimensional and nonlinear data. To address these issues, this paper\nintroduces a method based on Kolmogorov-Arnold Networks (KANs). KANs is a new\ntype of neural network architecture with learnable activation functions and no\nlinear weights, which has potential advantages in handling complex\nmulti-dimensional data. Specifically, this paper applies KANs to the field of\nindividual credit risk prediction for the first time and constructs the\nKolmogorov-Arnold Credit Default Predict (KACDP) model. Experiments show that\nthe KACDP model outperforms mainstream credit default prediction models in\nperformance metrics (ROC_AUC and F1 values). Meanwhile, through methods such as\nfeature attribution scores and visualization of the model structure, the\nmodel's decision-making process and the importance of different features are\nclearly demonstrated, providing transparent and interpretable decision-making\nbasis for financial institutions and meeting the industry's strict requirements\nfor model interpretability. In conclusion, the KACDP model constructed in this\npaper exhibits excellent predictive performance and satisfactory\ninterpretability in individual credit risk prediction, providing an effective\nway to address the limitations of existing methods and offering a new and\npractical credit risk prediction tool for financial institutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.17783v1"
    },
    {
        "title": "Advanced Risk Prediction and Stability Assessment of Banks Using Time\n  Series Transformer Models",
        "authors": [
            "Wenying Sun",
            "Zhen Xu",
            "Wenqing Zhang",
            "Kunyuan Ma",
            "You Wu",
            "Mengfang Sun"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper aims to study the prediction of the bank stability index based on\nthe Time Series Transformer model. The bank stability index is an important\nindicator to measure the health status and risk resistance of financial\ninstitutions. Traditional prediction methods are difficult to adapt to complex\nmarket changes because they rely on single-dimensional macroeconomic data. This\npaper proposes a prediction framework based on the Time Series Transformer,\nwhich uses the self-attention mechanism of the model to capture the complex\ntemporal dependencies and nonlinear relationships in financial data. Through\nexperiments, we compare the model with LSTM, GRU, CNN, TCN and RNN-Transformer\nmodels. The experimental results show that the Time Series Transformer model\noutperforms other models in both mean square error (MSE) and mean absolute\nerror (MAE) evaluation indicators, showing strong prediction ability. This\nshows that the Time Series Transformer model can better handle multidimensional\ntime series data in bank stability prediction, providing new technical\napproaches and solutions for financial risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.03606v1"
    },
    {
        "title": "Leveraging Convolutional Neural Network-Transformer Synergy for\n  Predictive Modeling in Risk-Based Applications",
        "authors": [
            "Yuhan Wang",
            "Zhen Xu",
            "Yue Yao",
            "Jinsong Liu",
            "Jiating Lin"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  With the development of the financial industry, credit default prediction, as\nan important task in financial risk management, has received increasing\nattention. Traditional credit default prediction methods mostly rely on machine\nlearning models, such as decision trees and random forests, but these methods\nhave certain limitations in processing complex data and capturing potential\nrisk patterns. To this end, this paper proposes a deep learning model based on\nthe combination of convolutional neural networks (CNN) and Transformer for\ncredit user default prediction. The model combines the advantages of CNN in\nlocal feature extraction with the ability of Transformer in global dependency\nmodeling, effectively improving the accuracy and robustness of credit default\nprediction. Through experiments on public credit default datasets, the results\nshow that the CNN+Transformer model outperforms traditional machine learning\nmodels, such as random forests and XGBoost, in multiple evaluation indicators\nsuch as accuracy, AUC, and KS value, demonstrating its powerful ability in\ncomplex financial data modeling. Further experimental analysis shows that\nappropriate optimizer selection and learning rate adjustment play a vital role\nin improving model performance. In addition, the ablation experiment of the\nmodel verifies the advantages of the combination of CNN and Transformer and\nproves the complementarity of the two in credit default prediction. This study\nprovides a new idea for credit default prediction and provides strong support\nfor risk assessment and intelligent decision-making in the financial field.\nFuture research can further improve the prediction effect and generalization\nability by introducing more unstructured data and improving the model\narchitecture.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.18222v1"
    },
    {
        "title": "A Dynamic Spillover Effect Investigation on Cryptocurrency Market Before\n  and After Pandemic",
        "authors": [
            "Wenjie Lan"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper distinguishes between risk resonance and risk diversification\nrelationships in the cryptocurrency market based on the newly developed\nasymmetric breakpoint approach, and analyzes the risk propagation mechanism\namong cryptocurrencies under extreme events. In addition, through the lens of\nnode association and network structure, this paper explores the dynamic\nevolutionary relationship of cryptocurrency risk association before and after\nthe epidemic. In addition, the driving mechanism of the cryptocurrency risk\nmovement is analyzed in a depth with the epidemic indicators. The findings show\nthat the effect of propagation of risk among cryptocurrencies becomes more\nsignificant under the influence of the new crown outbreak. At the same time,\nthe increase in the number of confirmed cases exacerbated the risk spillover\neffect among cryptocurrencies, while the risk resonance effect that exists\nbetween the crude oil market and the cryptocurrency market amplified the extent\nof the outbreak's impact on cryptocurrencies. However, other financial markets\nare relatively independent of the cryptocurrency market. This study proposes a\nstrategy to deal with the spread of cryptocurrency risks from the perspective\nof a public health crisis, providing a useful reference basis for improving the\nregulatory mechanism of cryptocurrencies.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.19983v1"
    },
    {
        "title": "Machine and Deep Learning for Credit Scoring: A compliant approach",
        "authors": [
            "Abdollah Rida"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Credit Scoring is one of the problems banks and financial institutions have\nto solve on a daily basis. If the state-of-the-art research in Machine and Deep\nLearning for finance has reached interesting results about Credit Scoring\nmodels, usage of such models in a heavily regulated context such as the one in\nbanks has never been done so far. Our work is thus a tentative to challenge the\ncurrent regulatory status-quo and introduce new BASEL 2 and 3 compliant\ntechniques, while still answering the Federal Reserve Bank and the European\nCentral Bank requirements. With the help of Gradient Boosting Machines (mainly\nXGBoost) we challenge an actual model used by BANK A for scoring through the\ndoor Auto Loan applicants. We prove that the usage of such algorithms for\nCredit Scoring models drastically improves performance and default capture\nrate. Furthermore, we leverage the power of Shapley Values to prove that these\nrelatively simple models are not as black-box as the current regulatory system\nthinks they are, and we attempt to explain the model outputs and Credit Scores\nwithin the BANK A Model Design and Validation framework\n",
        "pdf_link": "http://arxiv.org/pdf/2412.20225v1"
    },
    {
        "title": "Discussion of \"2004 IMS Medallion Lecture: Local Rademacher complexities\n  and oracle inequalities in risk minimization\" by V. Koltchinskii",
        "authors": [
            "Peter L. Bartlett",
            "Shahar Mendelson"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  Discussion of \"2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization\" by V. Koltchinskii [arXiv:0708.0083]\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0089v1"
    },
    {
        "title": "Discussion of ``2004 IMS Medallion Lecture: Local Rademacher\n  complexities and oracle inequalities in risk minimization'' by V.\n  Koltchinskii",
        "authors": [
            "Stéphan Clémençon",
            "Gábor Lugosi",
            "Nicolas Vayatis"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0098v1"
    },
    {
        "title": "Discussion of ``2004 IMS Medallion Lecture: Local Rademacher\n  complexities and oracle inequalities in risk minimization'' by V.\n  Koltchinskii",
        "authors": [
            "Xiaotong Shen",
            "Lifeng Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0121v1"
    },
    {
        "title": "Discussion of ``2004 IMS Medallion Lecture: Local Rademacher\n  complexities and oracle inequalities in risk minimization'' by V.\n  Koltchinskii",
        "authors": [
            "A. B. Tsybakov"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0124v1"
    },
    {
        "title": "Discussion of ``2004 IMS Medallion Lecture: Local Rademacher\n  complexities and oracle inequalities in risk minimization'' by V.\n  Koltchinskii",
        "authors": [
            "Sara van de Geer"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  Discussion of ``2004 IMS Medallion Lecture: Local Rademacher complexities and\noracle inequalities in risk minimization'' by V. Koltchinskii [arXiv:0708.0083]\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0132v1"
    },
    {
        "title": "Maturity-independent risk measures",
        "authors": [
            "Thaleia Zariphopoulou",
            "Gordan Zitkovic"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  The new notion of maturity-independent risk measures is introduced and\ncontrasted with the existing risk measurement concepts. It is shown, by means\nof two examples, one set on a finite probability space and the other in a\ndiffusion framework, that, surprisingly, some of the widely utilized risk\nmeasures cannot be used to build maturity-independent counterparts. We\nconstruct a large class of maturity-independent risk measures and give\nrepresentative examples in both continuous- and discrete-time financial models.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.3892v2"
    },
    {
        "title": "Convex Risk Measures: Lebesgue Property on one Period and Multi Period\n  Risk Measures and Application in Capital Allocation Problem",
        "authors": [
            "Hirbod Assa"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  In this work we study the Lebesgue property for convex risk measures on the\nspace of bounded c\\`adl\\`ag random processes ($\\mathcal{R}^\\infty$). Lebesgue\nproperty has been defined for one period convex risk measures in \\cite{Jo} and\nearlier had been studied in \\cite{De} for coherent risk measures. We introduce\nand study the Lebesgue property for convex risk measures in the multi period\nframework. We give presentation of all convex risk measures with Lebesgue\nproperty on bounded c\\`adl\\`ag processes. To do that we need to have a complete\ndescription of compact sets of $\\mathcal{A}^1$. The main mathematical\ncontribution of this paper is the characterization of the compact sets of\n$\\mathcal{A}^p$ (including $\\mathcal{A}^1$). At the final part of this paper,\nwe will solve the Capital Allocation Problem when we work with coherent risk\nmeasures.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.3209v1"
    },
    {
        "title": "Deterministic definition of the capital risk",
        "authors": [
            "Anna Szczypinska",
            "Edward W. Piotrowski"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  In this paper we propose a look at the capital risk problem inspired by\ndeterministic, known from classical mechanics, problem of juggling. We propose\ncapital equivalents to the Newton's laws of motion and on this basis we\ndetermine the most secure form of credit repayment with regard to maximisation\nof profit. Then we extend the Newton's laws to models in linear spaces of\narbitrary dimension with the help of matrix rates of return. The matrix rates\ndescribe the evolution of multidimensional capital and they are sensitive to\nboth quantitative changes of individual elements and flows between them. This\nallows us for simultaneous analysis of evolution of complex capital in both\ncontinuous and discrete time models.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.3129v1"
    },
    {
        "title": "Hidden Noise Structure and Random Matrix Models of Stock Correlations",
        "authors": [
            "Ivailo I. Dimov",
            "Petter N. Kolm",
            "Lee Maclin",
            "Dan Y. C. Shiber"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  We find a novel correlation structure in the residual noise of stock market\nreturns that is remarkably linked to the composition and stability of the top\nfew significant factors driving the returns, and moreover indicates that the\nnoise band is composed of multiple subbands that do not fully mix. Our findings\nallow us to construct effective generalized random matrix theory market models\nthat are closely related to correlation and eigenvector clustering. We show how\nto use these models in a simulation that incorporates heavy tails. Finally, we\ndemonstrate how a subtle purely stationary risk estimation bias can arise in\nthe conventional cleaning prescription.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.1383v3"
    },
    {
        "title": "A duality approach to the worst case value at risk for a sum of\n  dependent random variables with known covariances",
        "authors": [
            "Brice Franke",
            "Michael Stolz"
        ],
        "category": "q-fin.RM",
        "published_year": "2009",
        "summary": "  We propose an approach to the aggregation of risks which is based on\nestimation of simple quantities (such as covariances) associated to a vector of\ndependent random variables, and which avoids the use of parametric families of\ncopulae. Our main result demonstrates that the method leads to bounds on the\nworst case Value at Risk for a sum of dependent random variables. Its proof\napplies duality theory for infinite dimensional linear programs.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.1841v1"
    },
    {
        "title": "Theoretical and numerical Analysis on Optimal dividend policy of an\n  insurance company with positive transaction cost and higher solvency",
        "authors": [
            "Zongxia Liang",
            "Jicheng Yao"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  Based on a point of view that solvency and security are first, this paper\nconsiders regular-singular stochastic optimal control problem of a large\ninsurance company facing positive transaction cost asked by reinsurer under\nsolvency constraint. The company controls proportional reinsurance and dividend\npay-out policy to maximize the expected present value of the dividend pay-outs\nuntil the time of bankruptcy. The paper aims at deriving the optimal retention\nratio, dividend payout level, explicit value function of the insurance company\nvia stochastic analysis and PDE methods. The results present the best\nequilibrium point between maximization of dividend pay-outs and minimization of\nrisks. The paper also gets a risk-based capital standard to ensure the capital\nrequirement of can cover the total given risk. We present numerical results to\nmake analysis how the model parameters, such as, volatility, premium rate, and\nrisk level, impact on risk-based capital standard, optimal retention ratio,\noptimal dividend payout level and the company's profit.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1356v4"
    },
    {
        "title": "Optimal dividend and investing control of a insurance company with\n  higher solvency constraints",
        "authors": [
            "Zongxia Liang",
            "Jianping Huang"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  This paper considers optimal control problem of a large insurance company\nunder a fixed insolvency probability. The company controls proportional\nreinsurance rate, dividend pay-outs and investing process to maximize the\nexpected present value of the dividend pay-outs until the time of bankruptcy.\nThis paper aims at describing the optimal return function as well as the\noptimal policy. As a by-product, the paper theoretically sets a risk-based\ncapital standard to ensure the capital requirement of can cover the total risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1360v2"
    },
    {
        "title": "Optimization of dividend and reinsurance strategies under ruin\n  probability constraint",
        "authors": [
            "Zongxia Liang",
            "Jicheng Yao"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  This paper considers nonlinear regular-singular stochastic optimal control of\nlarge insurance company. The company controls the reinsurance rate and dividend\npayout process to maximize the expected present value of the dividend pay-outs\nuntil the time of bankruptcy. However, if the optimal dividend barrier is too\nlow to be acceptable, it will make the company result in bankruptcy soon.\nMoreover, although risk and return should be highly correlated, over-risking is\nnot a good recipe for high return, the supervisors of the company have to\nimpose their preferred risk level and additional charge on firm seeking\nservices beyond or lower than the preferred risk level. These indeed are\nnonlinear regular-singular stochastic optimal problems under ruin probability\nconstraints. This paper aims at solving this kind of the optimal problems, that\nis, deriving the optimal retention ratio,dividend payout level, optimal return\nfunction and optimal control strategy of the insurance company. As a\nby-product, the paper also sets a risk-based capital standard to ensure the\ncapital requirement of can cover the total given risk, and the effect of the\nrisk level on optimal retention ratio, dividend payout level and optimal\ncontrol strategy are also presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1361v5"
    },
    {
        "title": "Optimal control of a big financial company with debt liability under\n  bankrupt probability constraints",
        "authors": [
            "Zongxia Liang",
            "Bin Sun"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  This paper considers an optimal control of a big financial company with debt\nliability under bankrupt probability constraints. The company, which faces\nconstant liability payments and has choices to choose various\nproduction/business policies from an available set of control policies with\ndifferent expected profits and risks, controls the business policy and dividend\npayout process to maximize the expected present value of the dividends until\nthe time of bankruptcy. However, if the dividend payout barrier is too low to\nbe acceptable, it may result in the company's bankruptcy soon. In order to\nprotect the shareholders' profits, the managements of the company impose a\nreasonable and normal constraint on their dividend strategy, that is, the\nbankrupt probability associated with the optimal dividend payout barrier should\nbe smaller than a given risk level within a fixed time horizon. This paper aims\nat working out the optimal control policy as well as optimal return function\nfor the company under bankrupt probability constraint by stochastic analysis,\nPDE methods and variational inequality approach. Moreover, we establish a\nrisk-based capital standard to ensure the capital requirement of can cover the\ntotal given risk by numerical analysis and give reasonable economic\ninterpretation for the results.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.5376v4"
    },
    {
        "title": "Optimal Dividend and reinsurance strategy of a Property Insurance\n  Company under Catastrophe Risk",
        "authors": [
            "Zongxia Liang",
            "Lin He",
            "Jiaoling Wu"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We consider an optimal control problem of a property insurance company with\nproportional reinsurance strategy. The insurance business brings in catastrophe\nrisk, such as earthquake and flood. The catastrophe risk could be partly\nreduced by reinsurance. The management of the company controls the reinsurance\nrate and dividend payments process to maximize the expected present value of\nthe dividends before bankruptcy. This is the first time to consider the\ncatastrophe risk in property insurance model, which is more realistic. We\nestablish the solution of the problem by the mixed singular-regular control of\njump diffusions. We first derive the optimal retention ratio, the optimal\ndividend payments level, the optimal return function and the optimal control\nstrategy of the property insurance company, then the impacts of the catastrophe\nrisk and key model parameters on the optimal return function and the optimal\ncontrol strategy of the company are discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1269v1"
    },
    {
        "title": "Superhedging and Dynamic Risk Measures under Volatility Uncertainty",
        "authors": [
            "Marcel Nutz",
            "H. Mete Soner"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  We consider dynamic sublinear expectations (i.e., time-consistent coherent\nrisk measures) whose scenario sets consist of singular measures corresponding\nto a general form of volatility uncertainty. We derive a c\\`adl\\`ag nonlinear\nmartingale which is also the value process of a superhedging problem. The\nsuperhedging strategy is obtained from a representation similar to the optional\ndecomposition. Furthermore, we prove an optional sampling theorem for the\nnonlinear martingale and characterize it as the solution of a second order\nbackward SDE. The uniqueness of dynamic extensions of static sublinear\nexpectations is also studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.2958v2"
    },
    {
        "title": "Quantum Financial Economics - Risk and Returns",
        "authors": [
            "Carlos Pedro Gonçalves"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Financial volatility risk and its relation to a business cycle-related\nintrinsic time is addressed through a multiple round evolutionary quantum game\nequilibrium leading to turbulence and multifractal signatures in the financial\nreturns and in the risk dynamics. The model is simulated and the results are\ncompared with actual financial volatility data.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.2562v2"
    },
    {
        "title": "On the Computational Complexity of Measuring Global Stability of Banking\n  Networks",
        "authors": [
            "Piotr Berman",
            "Bhaskar DasGupta",
            "Lakshmi Kaligounder",
            "Marek Karpinski"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Threats on the stability of a financial system may severely affect the\nfunctioning of the entire economy, and thus considerable emphasis is placed on\nthe analyzing the cause and effect of such threats. The financial crisis in the\ncurrent and past decade has shown that one important cause of instability in\nglobal markets is the so-called financial contagion, namely the spreading of\ninstabilities or failures of individual components of the network to other,\nperhaps healthier, components. This leads to a natural question of whether the\nregulatory authorities could have predicted and perhaps mitigated the current\neconomic crisis by effective computations of some stability measure of the\nbanking networks. Motivated by such observations, we consider the problem of\ndefining and evaluating stabilities of both homogeneous and heterogeneous\nbanking networks against propagation of synchronous idiosyncratic shocks given\nto a subset of banks. We formalize the homogeneous banking network model of\nNier et al. and its corresponding heterogeneous version, formalize the\nsynchronous shock propagation procedures, define two appropriate stability\nmeasures and investigate the computational complexities of evaluating these\nmeasures for various network topologies and parameters of interest. Our results\nand proofs also shed some light on the properties of topologies and parameters\nof the network that may lead to higher or lower stabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.3546v5"
    },
    {
        "title": "Estimating financial risk using piecewise Gaussian processes",
        "authors": [
            "I. Garcia",
            "J. Jimenez"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  We present a computational method for measuring financial risk by estimating\nthe Value at Risk and Expected Shortfall from financial series. We have made\ntwo assumptions: First, that the predictive distributions of the values of an\nasset are conditioned by information on the way in which the variable evolves\nfrom similar conditions, and secondly, that the underlying random processes can\nbe described using piecewise Gaussian processes. The performance of the method\nwas evaluated by using it to estimate VaR and ES for a daily data series taken\nfrom the S&P500 index and applying a backtesting procedure recommended by the\nBasel Committee on Banking Supervision. The results indicated a satisfactory\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2889v1"
    },
    {
        "title": "Comparative and qualitative robustness for law-invariant risk measures",
        "authors": [
            "Volker Krätschmer",
            "Alexander Schied",
            "Henryk Zähle"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  When estimating the risk of a P&L from historical data or Monte Carlo\nsimulation, the robustness of the estimate is important. We argue here that\nHampel's classical notion of qualitative robustness is not suitable for risk\nmeasurement and we propose and analyze a refined notion of robustness that\napplies to tail-dependent law-invariant convex risk measures on Orlicz space.\nThis concept of robustness captures the tradeoff between robustness and\nsensitivity and can be quantified by an index of qualitative robustness. By\nmeans of this index, we can compare various risk measures, such as distortion\nrisk measures, in regard to their degree of robustness. Our analysis also\nyields results that are of independent interest such as continuity properties\nand consistency of estimators for risk measures, or a Skorohod representation\ntheorem for {\\psi}-weak convergence.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.2458v6"
    },
    {
        "title": "Representation Theory for Risk On Markowitz-Tversky-Kahneman Topology",
        "authors": [
            "Godfrey Charles-Cadogan"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  We introduce a representation theory for risk operations on locally compact\ngroups in a partition of unity on a topological manifold for\nMarkowitz-Tversky-Kahneman (MTK) reference points. We identify (1) risk torsion\ninduced by the flip rate for risk averse and risk seeking behaviour, and (2) a\nstructure constant or coupling of that torsion in the paracompact manifold. The\nrisk torsion operator extends by continuity to prudence and maxmin expected\nutility (MEU) operators, as well as other behavioural operators introduced by\nthe Italian school. In our erstwhile chaotic dynamical system, induced by\nbehavioural rotations of probability domains, the loss aversion index is an\nunobserved gauge transformation; and reference points are hyperbolic on the\nutility hypersurface characterized by the special unitary group SU(n). We\nidentify conditions for existence of harmonic utility functions on paracompact\nMTK manifolds induced by transformation groups. And we use those mathematical\nobjects to estimate: (1) loss aversion index from infinitesimal tangent\nvectors; and (2) value function from a classic Dirichlet problem for first exit\ntime of Brownian motion from regular points on the boundary of MTK base\ntopology.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.2665v1"
    },
    {
        "title": "How We Tend To Overestimate Powerlaw Tail Exponents",
        "authors": [
            "Nassim N. Taleb"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  In the presence of a layer of metaprobabilities (from uncertainty concerning\nthe parameters), the asymptotic tail exponent corresponds to the lowest\npossible tail exponent regardless of its probability. The problem explains\n\"Black Swan\" effects, i.e., why measurements tend to chronically underestimate\ntail contributions, rather than merely deliver imprecise but unbiased\nestimates.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.1966v1"
    },
    {
        "title": "A Copula Based Bayesian Approach for Paid-Incurred Claims Models for\n  Non-Life Insurance Reserving",
        "authors": [
            "Gareth W. Peters",
            "Alice X. D. Dong",
            "Robert Kohn"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  Our article considers the class of recently developed stochastic models that\ncombine claims payments and incurred losses information into a coherent\nreserving methodology. In particular, we develop a family of Heirarchical\nBayesian Paid-Incurred-Claims models, combining the claims reserving models of\nHertig et al. (1985) and Gogol et al. (1993). In the process we extend the\nindependent log-normal model of Merz et al. (2010) by incorporating different\ndependence structures using a Data-Augmented mixture Copula Paid-Incurred\nclaims model.\n  The utility and influence of incorporating both payment and incurred losses\ninto estimating of the full predictive distribution of the outstanding loss\nliabilities and the resulting reserves is demonstrated in the following cases:\n(i) an independent payment (P) data model; (ii) the independent\nPayment-Incurred Claims (PIC) data model of Merz et al. (2010); (iii) a novel\ndependent lag-year telescoping block diagonal Gaussian Copula PIC data model\nincorporating conjugacy via transformation; (iv) a novel data-augmented mixture\nArchimedean copula dependent PIC data model.\n  Inference in such models is developed via a class of adaptive Markov chain\nMonte Carlo sampling algorithms. These incorporate a data-augmentation\nframework utilized to efficiently evaluate the likelihood for the copula based\nPIC model in the loss reserving triangles. The adaptation strategy is based on\nrepresenting a positive definite covariance matrix by the exponential of a\nsymmetric matrix as proposed by Leonard et al. (1992).\n",
        "pdf_link": "http://arxiv.org/pdf/1210.3849v4"
    },
    {
        "title": "Lévy Information and the Aggregation of Risk Aversion",
        "authors": [
            "Dorje C. Brody",
            "Lane P. Hughston"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  When investors have heterogeneous attitudes towards risk, it is reasonable to\nassume that each investor has a pricing kernel, and that these individual\npricing kernels are aggregated to form a market pricing kernel. The various\ninvestors are then buyers or sellers depending on how their individual pricing\nkernels compare to that of the market. In Brownian-based models, we can\nrepresent such heterogeneous attitudes by letting the market price of risk be a\nrandom variable, the distribution of which corresponds to the variability of\nattitude across the market. If the flow of market information is determined by\nthe movements of prices, then neither the Brownian driver nor the market price\nof risk are directly visible: the filtration is generated by an \"information\nprocess\" given by a combination of the two. We show that the market pricing\nkernel is then given by the harmonic mean of the individual pricing kernels\nassociated with the various market participants. Remarkably, with an\nappropriate definition of L\\'evy information one draws the same conclusion in\nthe case when asset prices can jump. As a consequence we are led to a rather\ngeneral scheme for the management of investments in heterogeneous markets\nsubject to jump risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2964v2"
    },
    {
        "title": "Measuring Model Risk",
        "authors": [
            "Thomas Breuer",
            "Imre Csiszar"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We propose to interpret distribution model risk as sensitivity of expected\nloss to changes in the risk factor distribution, and to measure the\ndistribution model risk of a portfolio by the maximum expected loss over a set\nof plausible distributions defined in terms of some divergence from an\nestimated distribution. The divergence may be relative entropy, a Bregman\ndistance, or an $f$-divergence. We give formulas for the calculation of\ndistribution model risk and explicitly determine the worst case distribution\nfrom the set of plausible distributions. We also give formulas for the\nevaluation of divergence preferences describing ambiguity averse decision\nmakers.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.4832v1"
    },
    {
        "title": "Coherence and elicitability",
        "authors": [
            "Johanna F. Ziegel"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The risk of a financial position is usually summarized by a risk measure. As\nthis risk measure has to be estimated from historical data, it is important to\nbe able to verify and compare competing estimation procedures. In statistical\ndecision theory, risk measures for which such verification and comparison is\npossible, are called elicitable. It is known that quantile based risk measures\nsuch as value at risk are elicitable. In this paper we show that law-invariant\nspectral risk measures such as expected shortfall are not elicitable unless\nthey reduce to minus the expected value. Hence, it is unclear how to perform\nforecast verification or comparison. However, the class of elicitable\nlaw-invariant coherent risk measures does not reduce to minus the expected\nvalue. We show that it consists of certain expectiles.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.1690v3"
    },
    {
        "title": "Understanding Operational Risk Capital Approximations: First and Second\n  Orders",
        "authors": [
            "Gareth W. Peters",
            "Rodrigo S. Targino",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We set the context for capital approximation within the framework of the\nBasel II / III regulatory capital accords. This is particularly topical as the\nBasel III accord is shortly due to take effect. In this regard, we provide a\nsummary of the role of capital adequacy in the new accord, highlighting along\nthe way the significant loss events that have been attributed to the\nOperational Risk class that was introduced in the Basel II and III accords.\nThen we provide a semi-tutorial discussion on the modelling aspects of capital\nestimation under a Loss Distributional Approach (LDA). Our emphasis is to focus\non the important loss processes with regard to those that contribute most to\ncapital, the so called high consequence, low frequency loss processes. This\nleads us to provide a tutorial overview of heavy tailed loss process modelling\nin OpRisk under Basel III, with discussion on the implications of such tail\nassumptions for the severity model in an LDA structure. This provides\npractitioners with a clear understanding of the features that they may wish to\nconsider when developing OpRisk severity models in practice. From this\ndiscussion on heavy tailed severity models, we then develop an understanding of\nthe impact such models have on the right tail asymptotics of the compound loss\nprocess and we provide detailed presentation of what are known as first and\nsecond order tail approximations for the resulting heavy tailed loss process.\nFrom this we develop a tutorial on three key families of risk measures and\ntheir equivalent second order asymptotic approximations: Value-at-Risk (Basel\nIII industry standard); Expected Shortfall (ES) and the Spectral Risk Measure.\nThese then form the capital approximations.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.2910v1"
    },
    {
        "title": "From Text to Bank Interrelation Maps",
        "authors": [
            "Samuel Rönnqvist",
            "Peter Sarlin"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  In the wake of the ongoing global financial crisis, interdependencies among\nbanks have come into focus in trying to assess systemic risk. To date, such\nanalysis has largely been based on numerical data. By contrast, this study\nattempts to gain further insight into bank interconnections by tapping into\nfinancial discussion. Co-mentions of bank names are turned into a network,\nwhich can be visualized and analyzed quantitatively, in order to illustrate\ncharacteristics of individual banks and the network as a whole. The approach\nallows for the study of temporal dynamics of the network, to highlight changing\npatterns of discussion that reflect real-world events, the current financial\ncrisis in particular. For instance, it depicts how connections from distressed\nbanks to other banks and supervisory authorities have emerged and faded over\ntime, as well as how global shifts in network structure coincide with severe\ncrisis episodes. The usage of textual data holds an additional advantage in the\npossibility of gaining a more qualitative understanding of an observed\ninterrelation, through its context. We illustrate our approach using a case\nstudy on Finnish banks and financial institutions. The data set comprises 3.9M\nposts from online, financial and business-related discussion, during the years\n2004 to 2012. Future research includes analyzing European news articles with a\nbroader perspective, and a focus on improving semantic description of\nrelations.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3856v1"
    },
    {
        "title": "Computation of ruin probabilities for general discrete-time Markov\n  models",
        "authors": [
            "Ilya Tkachev",
            "Alessandro Abate"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We study the ruin problem over a risk process described by a discrete-time\nMarkov model. In contrast to previous studies that focused on the asymptotic\nbehaviour of ruin probabilities for large values of the initial capital, we\nprovide a new technique to compute the quantity of interest for any initial\nvalue, and with any given precision. Rather than focusing on a particular model\nfor risk processes, we give a general characterization of the ruin probability\nby providing corresponding recursions and fixpoint equations. Since such\nequations for the ruin probability are ill-posed in the sense that they do not\nallow for unique solutions, we approximate the ruin probability by a\ntwo-barrier ruin probability, for which fixpoint equations are well-posed. We\nalso show how good the introduced approximation is by providing an explicit\nbound on the error and by characterizing the cases when the error converges to\nzero. The presented technique and results are supported by two computational\nexamples over models known in the literature, one of which is extremely\nheavy-tailed.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5152v1"
    },
    {
        "title": "Modeling systemic risks in financial markets",
        "authors": [
            "Abhijnan Rej"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  We survey systemic risks to financial markets and present a high-level\ndescription of an algorithm that measures systemic risk in terms of coupled\nnetworks.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.3764v1"
    },
    {
        "title": "Optimal insurance purchase strategies via optimal multiple stopping\n  times",
        "authors": [
            "Rodrigo S. Targino",
            "Gareth W. Peters",
            "Georgy Sofronov",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  In this paper we study a class of insurance products where the policy holder\nhas the option to insure $k$ of its annual Operational Risk losses in a horizon\nof $T$ years. This involves a choice of $k$ out of $T$ years in which to apply\nthe insurance policy coverage by making claims against losses in the given\nyear. The insurance product structure presented can accommodate any kind of\nannual mitigation, but we present three basic generic insurance policy\nstructures that can be combined to create more complex types of coverage.\nFollowing the Loss Distributional Approach (LDA) with Poisson distributed\nannual loss frequencies and Inverse-Gaussian loss severities we are able to\ncharacterize in closed form analytical expressions for the multiple optimal\ndecision strategy that minimizes the expected Operational Risk loss over the\nnext $T$ years. For the cases where the combination of insurance policies and\nLDA model does not lead to closed form expressions for the multiple optimal\ndecision rules, we also develop a principled class of closed form\napproximations to the optimal decision rule. These approximations are developed\nbased on a class of orthogonal Askey polynomial series basis expansion\nrepresentations of the annual loss compound process distribution and functions\nof this annual loss.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.0424v1"
    },
    {
        "title": "Risk Margin Quantile Function Via Parametric and Non-Parametric Bayesian\n  Quantile Regression",
        "authors": [
            "Alice X. D. Dong",
            "Jennifer S. K. Chan",
            "Gareth W. Peters"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We develop quantile regression models in order to derive risk margin and to\nevaluate capital in non-life insurance applications. By utilizing the entire\nrange of conditional quantile functions, especially higher quantile levels, we\ndetail how quantile regression is capable of providing an accurate estimation\nof risk margin and an overview of implied capital based on the historical\nvolatility of a general insurers loss portfolio. Two modelling frameworks are\nconsidered based around parametric and nonparametric quantile regression models\nwhich we develop specifically in this insurance setting.\n  In the parametric quantile regression framework, several models including the\nflexible generalized beta distribution family, asymmetric Laplace (AL)\ndistribution and power Pareto distribution are considered under a Bayesian\nregression framework. The Bayesian posterior quantile regression models in each\ncase are studied via Markov chain Monte Carlo (MCMC) sampling strategies.\n  In the nonparametric quantile regression framework, that we contrast to the\nparametric Bayesian models, we adopted an AL distribution as a proxy and\ntogether with the parametric AL model, we expressed the solution as a scale\nmixture of uniform distributions to facilitate implementation. The models are\nextended to adopt dynamic mean, variance and skewness and applied to analyze\ntwo real loss reserve data sets to perform inference and discuss interesting\nfeatures of quantile regression for risk margin calculations.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.2492v1"
    },
    {
        "title": "Improving predictability of time series using maximum entropy methods",
        "authors": [
            "Gregor Chliamovitch",
            "Alexandre Dupuis",
            "Bastien Chopard",
            "Anton Golub"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We discuss how maximum entropy methods may be applied to the reconstruction\nof Markov processes underlying empirical time series and compare this approach\nto usual frequency sampling. It is shown that, at least in low dimension, there\nexists a subset of the space of stochastic matrices for which the MaxEnt method\nis more efficient than sampling, in the sense that shorter historical samples\nhave to be considered to reach the same accuracy. Considering short samples is\nof particular interest when modelling smoothly non-stationary processes, for\nthen it provides, under some conditions, a powerful forecasting tool. The\nmethod is illustrated for a discretized empirical series of exchange rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.7805v1"
    },
    {
        "title": "Risk aggregation with empirical margins: Latin hypercubes, empirical\n  copulas, and convergence of sum distributions",
        "authors": [
            "Georg Mainik"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  This paper studies convergence properties of multivariate distributions\nconstructed by endowing empirical margins with a copula. This setting includes\nLatin Hypercube Sampling with dependence, also known as the Iman--Conover\nmethod. The primary question addressed here is the convergence of the component\nsum, which is relevant to risk aggregation in insurance and finance.\n  This paper shows that a CLT for the aggregated risk distribution is not\navailable, so that the underlying mathematical problem goes beyond classic\nfunctional CLTs for empirical copulas. This issue is relevant to Monte-Carlo\nbased risk aggregation in all multivariate models generated by plugging\nempirical margins into a copula.\n  Instead of a functional CLT, this paper establishes strong uniform\nconsistency of the estimated sum distribution function and provides a\nsufficient criterion for the convergence rate $O(n^{-1/2})$ in probability.\nThese convergence results hold for all copulas with bounded densities. Examples\nwith unbounded densities include bivariate Clayton and Gauss copulas. The\nconvergence results are not specific to the component sum and hold also for any\nother componentwise non-decreasing aggregation function. On the other hand,\nconvergence of estimates for the joint distribution is much easier to prove,\nincluding CLTs.\n  Beyond Iman--Conover estimates, the results of this paper apply to\nmultivariate distributions obtained by plugging empirical margins into an exact\ncopula or by plugging exact margins into an empirical copula.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02749v1"
    },
    {
        "title": "What's in a ball? Constructing and characterizing uncertainty sets",
        "authors": [
            "Thomas Kruse",
            "Judith C. Schneider",
            "Nikolaus Schweizer"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  In the presence of model risk, it is well-established to replace classical\nexpected values by worst-case expectations over all models within a fixed\nradius from a given reference model. This is the \"robustness\" approach. We show\nthat previous methods for measuring this radius, e.g. relative entropy or\npolynomial divergences, are inadequate for reference models which are\nmoderately heavy-tailed such as lognormal models. Worst cases are either\ninfinitely pessimistic, or they rule out the possibility of fat-tailed \"power\nlaw\" models as plausible alternatives. We introduce a new family of divergence\nmeasures which captures intermediate levels of pessimism.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.01675v1"
    },
    {
        "title": "Preference Elicitation and Robust Optimization with Multi-Attribute\n  Quasi-Concave Choice Functions",
        "authors": [
            "William B. Haskell",
            "Wenjie Huang",
            "Huifu Xu"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Decision maker's preferences are often captured by some choice functions\nwhich are used to rank prospects. In this paper, we consider ambiguity in\nchoice functions over a multi-attribute prospect space. Our main result is a\nrobust preference model where the optimal decision is based on the worst-case\nchoice function from an ambiguity set constructed through preference\nelicitation with pairwise comparisons of prospects. Differing from existing\nworks in the area, our focus is on quasi-concave choice functions rather than\nconcave functions and this enables us to cover a wide range of utility/risk\npreference problems including multi-attribute expected utility and $S$-shaped\naspirational risk preferences. The robust choice function is increasing and\nquasi-concave but not necessarily translation invariant, a key property of\nmonetary risk measures. We propose two approaches based respectively on the\nsupport functions and level functions of quasi-concave functions to develop\ntractable formulations of the maximin preference robust optimization model. The\nformer gives rise to a mixed integer linear programming problem whereas the\nlatter is equivalent to solving a sequence of convex risk minimization\nproblems. To assess the effectiveness of the proposed robust preference\noptimization model and numerical schemes, we apply them to a security budget\nallocation problem and report some preliminary results from experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06632v1"
    },
    {
        "title": "Stochastic derivative estimation for max-stable random fields",
        "authors": [
            "Erwan Koch",
            "Christian Y. Robert"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We consider expected performances based on max-stable random fields and we\nare interested in their derivatives with respect to the spatial dependence\nparameters of those fields. Max-stable fields, such as the Brown--Resnick and\nSmith fields, are very popular in spatial extremes. We focus on the two most\npopular unbiased stochastic derivative estimation approaches: the likelihood\nratio method (LRM) and the infinitesimal perturbation analysis (IPA). LRM\nrequires the multivariate density of the max-stable field to be explicit, and\nIPA necessitates the computation of the derivative with respect to the\nparameters for each simulated value. We propose convenient and tractable\nconditions ensuring the validity of LRM and IPA in the cases of the\nBrown--Resnick and Smith field, respectively. Obtaining such conditions is\nintricate owing to the very structure of max-stable fields. Then we focus on\nrisk and dependence measures, which constitute one of the several frameworks\nwhere our theoretical results can be useful. We perform a simulation study\nwhich shows that both LRM and IPA perform well in various configurations, and\nprovide a real case study that is valuable for the insurance industry.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.05893v3"
    },
    {
        "title": "Can Deep Learning Predict Risky Retail Investors? A Case Study in\n  Financial Risk Behavior Forecasting",
        "authors": [
            "Yaodong Yang",
            "Alisa Kolesnikova",
            "Stefan Lessmann",
            "Tiejun Ma",
            "Ming-Chien Sung",
            "Johnnie E. V. Johnson"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  The paper examines the potential of deep learning to support decisions in\nfinancial risk management. We develop a deep learning model for predicting\nwhether individual spread traders secure profits from future trades. This task\nembodies typical modeling challenges faced in risk and behavior forecasting.\nConventional machine learning requires data that is representative of the\nfeature-target relationship and relies on the often costly development,\nmaintenance, and revision of handcrafted features. Consequently, modeling\nhighly variable, heterogeneous patterns such as trader behavior is challenging.\nDeep learning promises a remedy. Learning hierarchical distributed\nrepresentations of the data in an automatic manner (e.g. risk taking behavior),\nit uncovers generative features that determine the target (e.g., trader's\nprofitability), avoids manual feature engineering, and is more robust toward\nchange (e.g. dynamic market conditions). The results of employing a deep\nnetwork for operational risk forecasting confirm the feature learning\ncapability of deep learning, provide guidance on designing a suitable network\narchitecture and demonstrate the superiority of deep learning over machine\nlearning and rule-based benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.06175v3"
    },
    {
        "title": "Stochastic comparisons of the largest claim amounts from two sets of\n  interdependent heterogeneous portfolios",
        "authors": [
            "Hossein Nadeb",
            "Hamzeh Torabi",
            "Ali Dolati"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Let $ X_{\\lambda_1},\\ldots,X_{\\lambda_n}$ be dependent non-negative random\nvariables and $Y_i=I_{p_i} X_{\\lambda_i}$, $i=1,\\ldots,n$, where\n$I_{p_1},\\ldots,I_{p_n}$ are independent Bernoulli random variables independent\nof $X_{\\lambda_i}$'s, with ${\\rm E}[I_{p_i}]=p_i$, $i=1,\\ldots,n$. In actuarial\nsciences, $Y_i$ corresponds to the claim amount in a portfolio of risks. In\nthis paper, we compare the largest claim amounts of two sets of interdependent\nportfolios, in the sense of usual stochastic order, when the variables in one\nset have the parameters $\\lambda_1,\\ldots,\\lambda_n$ and $p_1,\\ldots,p_n$ and\nthe variables in the other set have the parameters\n$\\lambda^{*}_1,\\ldots,\\lambda^{*}_n$ and $p^*_1,\\ldots,p^*_n$. For\nillustration, we apply the results to some important models in actuary.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.08343v1"
    },
    {
        "title": "Optimal Network Compression",
        "authors": [
            "Hamed Amini",
            "Zachary Feinstein"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  This paper introduces a formulation of the optimal network compression\nproblem for financial systems. This general formulation is presented for\ndifferent levels of network compression or rerouting allowed from the initial\ninterbank network. We prove that this problem is, generically, NP-hard. We\nfocus on objective functions generated by systemic risk measures under shocks\nto the financial network. We use this framework to study the (sub)optimality of\nthe maximally compressed network. We conclude by studying the optimal\ncompression problem for specific networks; this permits us to study, e.g., the\nso-called robust fragility of certain network topologies more generally as well\nas the potential benefits and costs of network compression. In particular,\nunder systematic shocks and heterogeneous financial networks the robust\nfragility results of Acemoglu et al. (2015) no longer hold generally.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.08733v5"
    },
    {
        "title": "Where Do Thin Tails Come From?",
        "authors": [
            "Nassim Nicholas Taleb"
        ],
        "category": "q-fin.RM",
        "published_year": "2013",
        "summary": "  The literature of heavy tails (typically) starts with a random walk and finds\nmechanisms that lead to fat tails under aggregation. We follow the inverse\nroute and show how starting with fat tails we get to thin-tails when deriving\nthe probability distribution of the response to a random variable. We introduce\na general dose-response curve and argue that the left and right-boundedness or\nsaturation of the response in natural things leads to thin-tails, even when the\n\"underlying\" random variable at the source of the exposure is fat-tailed.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.6695v2"
    },
    {
        "title": "A risk management approach to capital allocation",
        "authors": [
            "Véronique Maume-Deschamps",
            "Didier Rullière",
            "Khalil Said"
        ],
        "category": "q-fin.RM",
        "published_year": "2015",
        "summary": "  The European insurance sector will soon be faced with the application of\nSolvency 2 regulation norms. It will create a real change in risk management\npractices. The ORSA approach of the second pillar makes the capital allocation\nan important exercise for all insurers and specially for groups. Considering\nmulti-branches firms, capital allocation has to be based on a multivariate risk\nmodeling. Several allocation methods are present in the literature and insurers\npractices. In this paper, we present a new risk allocation method, we study its\ncoherence using an axiomatic approach, and we try to define what the best\nallocation choice for an insurance group is.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04125v1"
    },
    {
        "title": "Statistical validation of financial time series via visibility graph",
        "authors": [
            "Matteo Serafino",
            "Andrea Gabrielli",
            "Guido Caldarelli",
            "Giulio Cimini"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  Statistical physics of complex systems exploits network theory not only to\nmodel, but also to effectively extract information from many dynamical\nreal-world systems. A pivotal case of study is given by financial systems:\nmarket prediction represents an unsolved scientific challenge yet with crucial\nimplications for society, as financial crises have devastating effects on real\neconomies. Thus, nowadays the quest for a robust estimator of market efficiency\nis both a scientific and institutional priority. In this work we study the\nvisibility graphs built from the time series of several trade market indices.\nWe propose a validation procedure for each link of these graphs against a null\nhypothesis derived from ARCH-type modeling of such series. Building on this\nframework, we devise a market indicator that turns out to be highly correlated\nand even predictive of financial instability periods.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.10980v1"
    },
    {
        "title": "Value-at-Risk Prediction in R with the GAS Package",
        "authors": [
            "David Ardia",
            "Kris Boudt",
            "Leopoldo Catania"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  GAS models have been recently proposed in time-series econometrics as\nvaluable tools for signal extraction and prediction. This paper details how\nfinancial risk managers can use GAS models for Value-at-Risk (VaR) prediction\nusing the novel GAS package for R. Details and code snippets for prediction,\ncomparison and backtesting with GAS models are presented. An empirical\napplication considering Dow Jones Index constituents investigates the VaR\nforecasting performance of GAS models.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.06010v1"
    },
    {
        "title": "Controlling systemic risk - network structures that minimize it and node\n  properties to calculate it",
        "authors": [
            "Sebastian M. Krause",
            "Hrvoje Štefančić",
            "Vinko Zlatić",
            "Guido Caldarelli"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Evaluation of systemic risk in networks of financial institutions in general\nrequires information of inter-institution financial exposures. In the framework\nof Debt Rank algorithm, we introduce an approximate method of systemic risk\nevaluation which requires only node properties, such as total assets and\nliabilities, as inputs. We demonstrate that this approximation captures a large\nportion of systemic risk measured by Debt Rank. Furthermore, using Monte Carlo\nsimulations, we investigate network structures that can amplify systemic risk.\nIndeed, while no topology in general sense is {\\em a priori} more stable if the\nmarket is liquid [1], a larger complexity is detrimental for the overall\nstability [2]. Here we find that the measure of scalar assortativity correlates\nwell with level of systemic risk. In particular, network structures with high\nsystemic risk are scalar assortative, meaning that risky banks are mostly\nexposed to other risky banks. Network structures with low systemic risk are\nscalar disassortative, with interactions of risky banks with stable banks.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.08483v1"
    },
    {
        "title": "Neural Networks and Value at Risk",
        "authors": [
            "Alexander Arimond",
            "Damian Borth",
            "Andreas Hoepner",
            "Michael Klawunn",
            "Stefan Weisheit"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Utilizing a generative regime switching framework, we perform Monte-Carlo\nsimulations of asset returns for Value at Risk threshold estimation. Using\nequity markets and long term bonds as test assets in the global, US, Euro area\nand UK setting over an up to 1,250 weeks sample horizon ending in August 2018,\nwe investigate neural networks along three design steps relating (i) to the\ninitialization of the neural network, (ii) its incentive function according to\nwhich it has been trained and (iii) the amount of data we feed. First, we\ncompare neural networks with random seeding with networks that are initialized\nvia estimations from the best-established model (i.e. the Hidden Markov). We\nfind latter to outperform in terms of the frequency of VaR breaches (i.e. the\nrealized return falling short of the estimated VaR threshold). Second, we\nbalance the incentive structure of the loss function of our networks by adding\na second objective to the training instructions so that the neural networks\noptimize for accuracy while also aiming to stay in empirically realistic regime\ndistributions (i.e. bull vs. bear market frequencies). In particular this\ndesign feature enables the balanced incentive recurrent neural network (RNN) to\noutperform the single incentive RNN as well as any other neural network or\nestablished approach by statistically and economically significant levels.\nThird, we half our training data set of 2,000 days. We find our networks when\nfed with substantially less data (i.e. 1,000 days) to perform significantly\nworse which highlights a crucial weakness of neural networks in their\ndependence on very large data sets ...\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01686v2"
    },
    {
        "title": "Stochastic modeling of assets and liabilities with mortality risk",
        "authors": [
            "Sergio Alvares Maffra",
            "John Armstrong",
            "Teemu Pennanen"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  This paper describes a general approach for stochastic modeling of assets\nreturns and liability cash-flows of a typical pensions insurer. On the asset\nside, we model the investment returns on equities and various classes of\nfixed-income instruments including short- and long-maturity fixed-rate bonds as\nwell as index-linked and corporate bonds. On the liability side, the risks are\ndriven by future mortality developments as well as price and wage inflation.\nAll the risk factors are modeled as a multivariate stochastic process that\ncaptures the dynamics and the dependencies across different risk factors. The\nmodel is easy to interpret and to calibrate to both historical data and to\nforecasts or expert views concerning the future. The simple structure of the\nmodel allows for efficient computations. The construction of a million\nscenarios takes only a few minutes on a personal computer. The approach is\nillustrated with an asset-liability analysis of a defined benefit pension fund.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09974v1"
    },
    {
        "title": "Credit risk - A structural model with jumps and correlations",
        "authors": [
            "Rudi Schäfer",
            "Markus Sjölin",
            "Andreas Sundin",
            "Michal Wolanski",
            "Thomas Guhr"
        ],
        "category": "q-fin.RM",
        "published_year": "2007",
        "summary": "  We set up a structural model to study credit risk for a portfolio containing\nseveral or many credit contracts. The model is based on a jump--diffusion\nprocess for the risk factors, i.e. for the company assets. We also include\ncorrelations between the companies. We discuss that models of this type have\nmuch in common with other problems in statistical physics and in the theory of\ncomplex systems. We study a simplified version of our model analytically.\nFurthermore, we perform extensive numerical simulations for the full model. The\nobservables are the loss distribution of the credit portfolio, its moments and\nother quantities derived thereof. We compile detailed information about the\nparameter dependence of these observables. In the course of setting up and\nanalyzing our model, we also give a review of credit risk modeling for a\nphysics audience.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.3478v2"
    },
    {
        "title": "How big is too big? Critical Shocks for Systemic Failure Cascades",
        "authors": [
            "Claudio J. Tessone",
            "Antonios Garas",
            "Beniamino Guerra",
            "Frank Schweitzer"
        ],
        "category": "q-fin.RM",
        "published_year": "2012",
        "summary": "  External or internal shocks may lead to the collapse of a system consisting\nof many agents. If the shock hits only one agent initially and causes it to\nfail, this can induce a cascade of failures among neighoring agents. Several\ncritical constellations determine whether this cascade remains finite or\nreaches the size of the system, i.e. leads to systemic risk. We investigate the\ncritical parameters for such cascades in a simple model, where agents are\ncharacterized by an individual threshold \\theta_i determining their capacity to\nhandle a load \\alpha\\theta_i with 1-\\alpha being their safety margin. If agents\nfail, they redistribute their load equally to K neighboring agents in a regular\nnetwork. For three different threshold distributions P(\\theta), we derive\nanalytical results for the size of the cascade, X(t), which is regarded as a\nmeasure of systemic risk, and the time when it stops. We focus on two different\nregimes, (i) EEE, an external extreme event where the size of the shock is of\nthe order of the total capacity of the network, and (ii) RIE, a random internal\nevent where the size of the shock is of the order of the capacity of an agent.\nWe find that even for large extreme events that exceed the capacity of the\nnetwork finite cascades are still possible, if a power-law threshold\ndistribution is assumed. On the other hand, even small random fluctuations may\nlead to full cascades if critical conditions are met. Most importantly, we\ndemonstrate that the size of the \"big\" shock is not the problem, as the\nsystemic risk only varies slightly for changes of 10 to 50 percent of the\nexternal shock. Systemic risk depends much more on ingredients such as the\nnetwork topology, the safety margin and the threshold distribution, which gives\nhints on how to reduce systemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0959v3"
    },
    {
        "title": "Mortality data reliability in an internal model",
        "authors": [
            "Fabrice Balland",
            "Alexandre Boumezoued",
            "Laurent Devineau",
            "Marine Habart",
            "Tom Popa"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  In this paper, we discuss the impact of some mortality data anomalies on an\ninternal model capturing longevity risk in the Solvency 2 framework. In\nparticular, we are concerned with abnormal cohort effects such as those for\ngenerations 1919 and 1920, for which the period tables provided by the Human\nMortality Database show particularly low and high mortality rates respectively.\nTo provide corrected tables for the three countries of interest here (France,\nItaly and West Germany), we use the approach developed by Boumezoued (2016) for\ncountries for which the method applies (France and Italy), and provide an\nextension of the method for West Germany as monthly fertility histories are not\nsufficient to cover the generations of interest. These mortality tables are\ncrucial inputs to stochastic mortality models forecasting future scenarios,\nfrom which the extreme 0,5% longevity improvement can be extracted, allowing\nfor the calculation of the Solvency Capital Requirement (SCR). More precisely,\nto assess the impact of such anomalies in the Solvency II framework, we use a\nsimplified internal model based on three usual stochastic models to project\nmortality rates in the future combined with a closure table methodology for\nolder ages. Correcting this bias obviously improves the data quality of the\nmortality inputs, which is of paramount importance today, and slightly\ndecreases the capital requirement. Overall, the longevity risk assessment\nremains stable, as well as the selection of the stochastic mortality model. As\na collateral gain of this data quality improvement, the more regular estimated\nparameters allow for new insights and a refined assessment regarding longevity\nrisk.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00464v1"
    },
    {
        "title": "Stochastic ordering of Gini indexes for multivariate elliptical random\n  variables",
        "authors": [
            "Chuancun Yin"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  In this paper, we establish the stochastic ordering of the Gini indexes for\nmultivariate elliptical risks which generalized the corresponding results for\nmultivariate normal risks. It is shown that several conditions on dispersion\nmatrices and the components of dispersion matrices of multivariate normal risks\nfor the monotonicity of the Gini index in the usual stochastic order proposed\nby Samanthi, Wei and Brazauskas (2016) and Kim and Kim (2019) are also suitable\nfor multivariate elliptical risks. We also study the tail probability of Gini\nindex for multivariate elliptical risks and revised a large deviation result\nfor the Gini indexes of multivariate normal risks in Kim and Kim (2019).\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01943v3"
    },
    {
        "title": "Forecast Encompassing Tests for the Expected Shortfall",
        "authors": [
            "Timo Dimitriadis",
            "Julie Schnaitmann"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  We introduce new forecast encompassing tests for the risk measure Expected\nShortfall (ES). The ES currently receives much attention through its\nintroduction into the Basel III Accords, which stipulate its use as the primary\nmarket risk measure for the international banking regulation. We utilize joint\nloss functions for the pair ES and Value at Risk to set up three ES\nencompassing test variants. The tests are built on misspecification robust\nasymptotic theory and we investigate the finite sample properties of the tests\nin an extensive simulation study. We use the encompassing tests to illustrate\nthe potential of forecast combination methods for different financial assets.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04569v3"
    },
    {
        "title": "Mortality rate forecasting: can recurrent neural networks beat the\n  Lee-Carter model?",
        "authors": [
            "Gábor Petneházi",
            "József Gáll"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  This article applies a long short-term memory recurrent neural network to\nmortality rate forecasting. The model can be trained jointly on the mortality\nrate history of different countries, ages, and sexes. The RNN-based method\nseems to outperform the popular Lee-Carter model.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05501v2"
    },
    {
        "title": "An Unethical Optimization Principle",
        "authors": [
            "Nicholas Beale",
            "Heather Battey",
            "Anthony C. Davison",
            "Robert S. MacKay"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  If an artificial intelligence aims to maximise risk-adjusted return, then\nunder mild conditions it is disproportionately likely to pick an unethical\nstrategy unless the objective function allows sufficiently for this risk. Even\nif the proportion ${\\eta}$ of available unethical strategies is small, the\nprobability ${p_U}$ of picking an unethical strategy can become large; indeed\nunless returns are fat-tailed ${p_U}$ tends to unity as the strategy space\nbecomes large. We define an Unethical Odds Ratio Upsilon (${\\Upsilon}$) that\nallows us to calculate ${p_U}$ from ${\\eta}$, and we derive a simple formula\nfor the limit of ${\\Upsilon}$ as the strategy space becomes large. We give an\nalgorithm for estimating ${\\Upsilon}$ and ${p_U}$ in finite cases and discuss\nhow to deal with infinite strategy spaces. We show how this principle can be\nused to help detect unethical strategies and to estimate ${\\eta}$. Finally we\nsketch some policy implications of this work.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.05116v1"
    },
    {
        "title": "On the optimality of joint periodic and extraordinary dividend\n  strategies",
        "authors": [
            "Benjamin Avanzi",
            "Hayden Lau",
            "Bernard Wong"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  In this paper, we model the cash surplus (or equity) of a risky business with\na Brownian motion. Owners can take cash out of the surplus in the form of\n\"dividends\", subject to transaction costs. However, if the surplus hits 0 then\nruin occurs and the business cannot operate any more.\n  We consider two types of dividend distributions: (i) periodic, regular ones\n(that is, dividends can be paid only at countable many points in time,\naccording to a specific arrival process); and (ii) extraordinary dividend\npayments that can be made immediately at any time (that is, the dividend\ndecision time space is continuous and matches that of the surplus process).\nBoth types of dividends attract proportional transaction costs, and\nextraordinary distributions also attracts fixed transaction costs, a realistic\nfeature. A dividend strategy that involves both types of distributions\n(periodic and extraordinary) is qualified as \"hybrid\".\n  We determine which strategies (either periodic, immediate, or hybrid) are\noptimal, that is, we show which are the strategies that maximise the expected\npresent value of dividends paid until ruin, net of transaction costs.\nSometimes, a liquidation strategy (which pays out all monies and stops the\nprocess) is optimal. Which strategy is optimal depends on the profitability of\nthe business, and the level of (proportional and fixed) transaction costs.\nResults are illustrated.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.00717v2"
    },
    {
        "title": "Quantitative Statistical Robustness for Tail-Dependent Law Invariant\n  Risk Measures",
        "authors": [
            "Wei Wang",
            "Huifu Xu",
            "Tiejun Ma"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  When estimating the risk of a financial position with empirical data or Monte\nCarlo simulations via a tail-dependent law invariant risk measure such as the\nConditional Value-at-Risk (CVaR), it is important to ensure the robustness of\nthe statistical estimator particularly when the data contain noise. Kratscher\net al. [1] propose a new framework to examine the qualitative robustness of\nestimators for tail-dependent law invariant risk measures on Orlicz spaces,\nwhich is a step further from earlier work for studying the robustness of risk\nmeasurement procedures by Cont et al. [2]. In this paper, we follow the stream\nof research to propose a quantitative approach for verifying the statistical\nrobustness of tail-dependent law invariant risk measures. A distinct feature of\nour approach is that we use the Fortet-Mourier metric to quantify the variation\nof the true underlying probability measure in the analysis of the discrepancy\nbetween the laws of the plug-in estimators of law invariant risk measure based\non the true data and perturbed data, which enables us to derive an explicit\nerror bound for the discrepancy when the risk functional is Lipschitz\ncontinuous with respect to a class of admissible laws. Moreover, the newly\nintroduced notion of Lipschitz continuity allows us to examine the degree of\nrobustness for tail-dependent risk measures. Finally, we apply our quantitative\napproach to some well-known risk measures to illustrate our theory.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.15491v1"
    },
    {
        "title": "Explainable AI for Interpretable Credit Scoring",
        "authors": [
            "Lara Marie Demajo",
            "Vince Vella",
            "Alexiei Dingli"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  With the ever-growing achievements in Artificial Intelligence (AI) and the\nrecent boosted enthusiasm in Financial Technology (FinTech), applications such\nas credit scoring have gained substantial academic interest. Credit scoring\nhelps financial experts make better decisions regarding whether or not to\naccept a loan application, such that loans with a high probability of default\nare not accepted. Apart from the noisy and highly imbalanced data challenges\nfaced by such credit scoring models, recent regulations such as the `right to\nexplanation' introduced by the General Data Protection Regulation (GDPR) and\nthe Equal Credit Opportunity Act (ECOA) have added the need for model\ninterpretability to ensure that algorithmic decisions are understandable and\ncoherent. An interesting concept that has been recently introduced is\neXplainable AI (XAI), which focuses on making black-box models more\ninterpretable. In this work, we present a credit scoring model that is both\naccurate and interpretable. For classification, state-of-the-art performance on\nthe Home Equity Line of Credit (HELOC) and Lending Club (LC) Datasets is\nachieved using the Extreme Gradient Boosting (XGBoost) model. The model is then\nfurther enhanced with a 360-degree explanation framework, which provides\ndifferent explanations (i.e. global, local feature-based and local\ninstance-based) that are required by different people in different situations.\nEvaluation through the use of functionallygrounded, application-grounded and\nhuman-grounded analysis show that the explanations provided are simple,\nconsistent as well as satisfy the six predetermined hypotheses testing for\ncorrectness, effectiveness, easy understanding, detail sufficiency and\ntrustworthiness.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.03749v1"
    },
    {
        "title": "The Causal Learning of Retail Delinquency",
        "authors": [
            "Yiyan Huang",
            "Cheuk Hang Leung",
            "Xing Yan",
            "Qi Wu",
            "Nanbo Peng",
            "Dongdong Wang",
            "Zhixiang Huang"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  This paper focuses on the expected difference in borrower's repayment when\nthere is a change in the lender's credit decisions. Classical estimators\noverlook the confounding effects and hence the estimation error can be\nmagnificent. As such, we propose another approach to construct the estimators\nsuch that the error can be greatly reduced. The proposed estimators are shown\nto be unbiased, consistent, and robust through a combination of theoretical\nanalysis and numerical testing. Moreover, we compare the power of estimating\nthe causal quantities between the classical estimators and the proposed\nestimators. The comparison is tested across a wide range of models, including\nlinear regression models, tree-based models, and neural network-based models,\nunder different simulated datasets that exhibit different levels of causality,\ndifferent degrees of nonlinearity, and different distributional properties.\nMost importantly, we apply our approaches to a large observational dataset\nprovided by a global technology firm that operates in both the e-commerce and\nthe lending business. We find that the relative reduction of estimation error\nis strikingly substantial if the causal effects are accounted for correctly.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.09448v1"
    },
    {
        "title": "Multivariate tail covariance for generalized skew-elliptical\n  distributions",
        "authors": [
            "Baishuai Zuo",
            "Chuancun Yin"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper, the multivariate tail covariance (MTCov) for generalized\nskew-elliptical distributions is considered. Some special cases for this\ndistribution, such as generalized skew-normal, generalized skew student-t,\ngeneralized skew-logistic and generalized skew-Laplace distributions, are also\nconsidered. In order to test the theoretical feasibility of our results, the\nMTCov for skewed and non skewed normal distributions are computed and compared.\nFinally, we give a special formula of the MTCov for generalized skew-elliptical\ndistributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.05201v1"
    },
    {
        "title": "Assessment of creditworthiness models privacy-preserving training with\n  synthetic data",
        "authors": [
            "Ricardo Muñoz-Cancino",
            "Cristián Bravo",
            "Sebastián A. Ríos",
            "Manuel Graña"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Credit scoring models are the primary instrument used by financial\ninstitutions to manage credit risk. The scarcity of research on behavioral\nscoring is due to the difficult data access. Financial institutions have to\nmaintain the privacy and security of borrowers' information refrain them from\ncollaborating in research initiatives. In this work, we present a methodology\nthat allows us to evaluate the performance of models trained with synthetic\ndata when they are applied to real-world data. Our results show that synthetic\ndata quality is increasingly poor when the number of attributes increases.\nHowever, creditworthiness assessment models trained with synthetic data show a\nreduction of 3\\% of AUC and 6\\% of KS when compared with models trained with\nreal data. These results have a significant impact since they encourage credit\nrisk investigation from synthetic data, making it possible to maintain\nborrowers' privacy and to address problems that until now have been hampered by\nthe availability of information.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01212v1"
    },
    {
        "title": "Law-invariant risk measures: extension properties and qualitative\n  robustness",
        "authors": [
            "Pablo Koch-Medina",
            "Cosimo Munari"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  We characterize when a convex risk measure associated to a law-invariant\nacceptance set in $L^\\infty$ can be extended to $L^p$, $1\\leq p<\\infty$,\npreserving finiteness and continuity. This problem is strongly connected to the\nstatistical robustness of the corresponding risk measures. Special attention is\npaid to concrete examples including risk measures based on expected utility,\nmax-correlation risk measures, and distortion risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.3121v1"
    },
    {
        "title": "On the Measurement of Economic Tail Risk",
        "authors": [
            "Steven Kou",
            "Xianhua Peng"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  This paper attempts to provide a decision-theoretic foundation for the\nmeasurement of economic tail risk, which is not only closely related to utility\ntheory but also relevant to statistical model uncertainty. The main result is\nthat the only risk measures that satisfy a set of economic axioms for the\nChoquet expected utility and the statistical property of elicitability (i.e.\nthere exists an objective function such that minimizing the expected objective\nfunction yields the risk measure) are the mean functional and the median\nshortfall, which is the median of tail loss distribution. Elicitability is\nimportant for backtesting. We also extend the result to address model\nuncertainty by incorporating multiple scenarios. As an application, we argue\nthat median shortfall is a better alternative than expected shortfall for\nsetting capital requirements in Basel Accords.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.4787v3"
    },
    {
        "title": "Optimal Monitoring and Mitigation of Systemic Risk in Financial Networks",
        "authors": [
            "Zhang Li",
            "Xiaojun Lin",
            "Borja Peleato-Inarrea",
            "Ilya Pollak"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  This paper studies the problem of optimally allocating a cash injection into\na financial system in distress. Given a one-period borrower-lender network in\nwhich all debts are due at the same time and have the same seniority, we\naddress the problem of allocating a fixed amount of cash among the nodes to\nminimize the weighted sum of unpaid liabilities. Assuming all the loan amounts\nand asset values are fixed and that there are no bankruptcy costs, we show that\nthis problem is equivalent to a linear program. We develop a duality-based\ndistributed algorithm to solve it which is useful for applications where it is\ndesirable to avoid centralized data gathering and computation. We also consider\nthe problem of minimizing the expectation of the weighted sum of unpaid\nliabilities under the assumption that the net external asset holdings of all\ninstitutions are stochastic. We show that this problem is a two-stage\nstochastic linear program. To solve it, we develop two algorithms based on:\nBenders decomposition algorithm and projected stochastic gradient descent. We\nshow that if the defaulting nodes never pay anything, the deterministic optimal\ncash injection allocation problem is an NP-hard mixed-integer linear program.\nHowever, modern optimization software enables the computation of very accurate\nsolutions to this problem on a personal computer in a few seconds for network\nsizes comparable with the size of the US banking system. In addition, we\naddress the problem of allocating the cash injection amount so as to minimize\nthe number of nodes in default. For this problem, we develop two heuristic\nalgorithms: a reweighted l1 minimization algorithm and a greedy algorithm. We\nillustrate these two algorithms using three synthetic network structures for\nwhich the optimal solution can be calculated exactly. We also compare these two\nalgorithms on three types random networks which are more complex.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2570v2"
    },
    {
        "title": "A new multivariate dependence measure based on comonotonicity",
        "authors": [
            "Ying Zhang",
            "Chuancun Yin"
        ],
        "category": "q-fin.RM",
        "published_year": "2014",
        "summary": "  In this paper we introduce a new multivariate dependence measure based on\ncomonotonicity by means of product moment which motivated by the recent papers\nof Koch and Schepper (ASTIN Bulletin 41 (2011) 191-213) and Dhaene et al.\n(Journal of Computational and Applied Mathematics 263 (2014) 78-87). Some\ndifferences and relations between the new dependence measure and other\nmultivariate measures are an- alyzed. We also give several characteristics of\nthis measure and estimations based on the definitions and its property are\npresented.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.7845v1"
    },
    {
        "title": "Statistically validated network of portfolio overlaps and systemic risk",
        "authors": [
            "Stanislao Gualdi",
            "Giulio Cimini",
            "Kevin Primicerio",
            "Riccardo Di Clemente",
            "Damien Challet"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  Common asset holding by financial institutions, namely portfolio overlap, is\nnowadays regarded as an important channel for financial contagion with the\npotential to trigger fire sales and thus severe losses at the systemic level.\nIn this paper we propose a method to assess the statistical significance of the\noverlap between pairs of heterogeneously diversified portfolios, which then\nallows us to build a validated network of financial institutions where links\nindicate potential contagion channels due to realized portfolio overlaps. The\nmethod is implemented on a historical database of institutional holdings\nranging from 1999 to the end of 2013, but can be in general applied to any\nbipartite network where the presence of similar sets of neighbors is of\ninterest. We find that the proportion of validated network links (i.e., of\nstatistically significant overlaps) increased steadily before the 2007-2008\nglobal financial crisis and reached a maximum when the crisis occurred. We\nargue that the nature of this measure implies that systemic risk from fire\nsales liquidation was maximal at that time. After a sharp drop in 2008,\nsystemic risk resumed its growth in 2009, with a notable acceleration in 2013,\nreaching levels not seen since 2007. We finally show that market trends tend to\nbe amplified in the portfolios identified by the algorithm, such that it is\npossible to have an informative signal about financial institutions that are\nabout to suffer (enjoy) the most significant losses (gains).\n",
        "pdf_link": "http://arxiv.org/pdf/1603.05914v2"
    },
    {
        "title": "News-sentiment networks as a risk indicator",
        "authors": [
            "Thomas Forss",
            "Peter Sarlin"
        ],
        "category": "q-fin.RM",
        "published_year": "2017",
        "summary": "  To understand the relationship between news sentiment and company stock price\nmovements, and to better understand connectivity among companies, we define an\nalgorithm for measuring sentiment-based network risk. The algorithm ranks\ncompanies in networks of co-occurrences, and measures sentiment-based risk, by\ncalculating both individual risks and aggregated network risks. We extract\nrelative sentiment for companies to get a measure of individual company risk,\nand input it into our risk model together with co-occurrences of companies\nextracted from news on a quarterly basis. We can show that the highest\nquarterly risk value outputted by our risk model, is correlated to a higher\nchance of stock price decline, up to 70 days after a risk measurement. Our\nresults show that the highest difference in the probability of stock price\ndecline, compared to the benchmark containing all risk values for the same\nperiod, is during the interval from 21 to 30 days after a quarterly\nmeasurement. The highest average probability of company stock price decline, is\nfound at a delay of 28 days, after a company has reached its maximum risk\nvalue. The highest probability differences for a daily decline were calculated\nto be 13 percentage points.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.05812v2"
    },
    {
        "title": "Calculating CVaR and bPOE for Common Probability Distributions With\n  Application to Portfolio Optimization and Density Estimation",
        "authors": [
            "Matthew Norton",
            "Valentyn Khokhlov",
            "Stan Uryasev"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  Conditional Value-at-Risk (CVaR) and Value-at-Risk (VaR), also called the\nsuperquantile and quantile, are frequently used to characterize the tails of\nprobability distribution's and are popular measures of risk. Buffered\nProbability of Exceedance (bPOE) is a recently introduced characterization of\nthe tail which is the inverse of CVaR, much like the CDF is the inverse of the\nquantile. These quantities can prove very useful as the basis for a variety of\nrisk-averse parametric engineering approaches. Their use, however, is often\nmade difficult by the lack of well-known closed-form equations for calculating\nthese quantities for commonly used probability distribution's. In this paper,\nwe derive formulas for the superquantile and bPOE for a variety of common\nunivariate probability distribution's. Besides providing a useful collection\nwithin a single reference, we use these formulas to incorporate the\nsuperquantile and bPOE into parametric procedures. In particular, we consider\ntwo: portfolio optimization and density estimation. First, when portfolio\nreturns are assumed to follow particular distribution families, we show that\nfinding the optimal portfolio via minimization of bPOE has advantages over\nsuperquantile minimization. We show that, given a fixed threshold, a single\nportfolio is the minimal bPOE portfolio for an entire class of distribution's\nsimultaneously. Second, we apply our formulas to parametric density estimation\nand propose the method of superquantile's (MOS), a simple variation of the\nmethod of moment's (MM) where moment's are replaced by superquantile's at\ndifferent confidence levels. With the freedom to select various combinations of\nconfidence levels, MOS allows the user to focus the fitting procedure on\ndifferent portions of the distribution, such as the tail when fitting\nheavy-tailed asymmetric data.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11301v2"
    },
    {
        "title": "A hybrid neural network model based on improved PSO and SA for\n  bankruptcy prediction",
        "authors": [
            "Fatima Zahra Azayite",
            "Said Achchab"
        ],
        "category": "q-fin.RM",
        "published_year": "2019",
        "summary": "  Predicting firm's failure is one of the most interesting subjects for\ninvestors and decision makers. In this paper, a bankruptcy prediction model is\nproposed based on Artificial Neural networks (ANN). Taking into consideration\nthat the choice of variables to discriminate between bankrupt and non-bankrupt\nfirms influences significantly the model's accuracy and considering the problem\nof local minima, we propose a hybrid ANN based on variables selection\ntechniques. Moreover, we evolve the convergence of Particle Swarm Optimization\n(PSO) by proposing a training algorithm based on an improved PSO and Simulated\nAnnealing. A comparative performance study is reported, and the proposed hybrid\nmodel shows a high performance and convergence in the context of missing data.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12179v1"
    },
    {
        "title": "Application of Deep Neural Networks to assess corporate Credit Rating",
        "authors": [
            "Parisa Golbayani",
            "Dan Wang",
            "Ionut Florescu"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Recent literature implements machine learning techniques to assess corporate\ncredit rating based on financial statement reports. In this work, we analyze\nthe performance of four neural network architectures (MLP, CNN, CNN2D, LSTM) in\npredicting corporate credit rating as issued by Standard and Poor's. We analyze\ncompanies from the energy, financial and healthcare sectors in US. The goal of\nthe analysis is to improve application of machine learning algorithms to credit\nassessment. To this end, we focus on three questions. First, we investigate if\nthe algorithms perform better when using a selected subset of features, or if\nit is better to allow the algorithms to select features themselves. Second, is\nthe temporal aspect inherent in financial data important for the results\nobtained by a machine learning algorithm? Third, is there a particular neural\nnetwork architecture that consistently outperforms others with respect to input\nfeatures, sectors and holdout set? We create several case studies to answer\nthese questions and analyze the results using ANOVA and multiple comparison\ntesting procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.02334v1"
    },
    {
        "title": "Covariance matrix filtering with bootstrapped hierarchies",
        "authors": [
            "Christian Bongiorno",
            "Damien Challet"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Statistical inference of the dependence between objects often relies on\ncovariance matrices. Unless the number of features (e.g. data points) is much\nlarger than the number of objects, covariance matrix cleaning is necessary to\nreduce estimation noise. We propose a method that is robust yet flexible enough\nto account for fine details of the structure covariance matrix. Robustness\ncomes from using a hierarchical ansatz and dependence averaging between\nclusters; flexibility comes from a bootstrap procedure. This method finds\nseveral possible hierarchical structures in DNA microarray gene expression\ndata, and leads to lower realized risk in global minimum variance portfolios\nthan current filtering methods when the number of data points is relatively\nsmall.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05807v1"
    },
    {
        "title": "A new multilayer network construction via Tensor learning",
        "authors": [
            "Giuseppe Brandi",
            "T. Di Matteo"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Multilayer networks proved to be suitable in extracting and providing\ndependency information of different complex systems. The construction of these\nnetworks is difficult and is mostly done with a static approach, neglecting\ntime delayed interdependences. Tensors are objects that naturally represent\nmultilayer networks and in this paper, we propose a new methodology based on\nTucker tensor autoregression in order to build a multilayer network directly\nfrom data. This methodology captures within and between connections across\nlayers and makes use of a filtering procedure to extract relevant information\nand improve visualization. We show the application of this methodology to\ndifferent stationary fractionally differenced financial data. We argue that our\nresult is useful to understand the dependencies across three different aspects\nof financial risk, namely market risk, liquidity risk, and volatility risk.\nIndeed, we show how the resulting visualization is a useful tool for risk\nmanagers depicting dependency asymmetries between different risk factors and\naccounting for delayed cross dependencies. The constructed multilayer network\nshows a strong interconnection between the volumes and prices layers across all\nthe stocks considered while a lower number of interconnections between the\nuncertainty measures is identified.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.05367v1"
    },
    {
        "title": "Modeling Institutional Credit Risk with Financial News",
        "authors": [
            "Tam Tran-The"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  Credit risk management, the practice of mitigating losses by understanding\nthe adequacy of a borrower's capital and loan loss reserves, has long been\nimperative to any financial institution's long-term sustainability and growth.\nMassMutual is no exception. The company is keen on effectively monitoring\ndowngrade risk, or the risk associated with the event when credit rating of a\ncompany deteriorates. Current work in downgrade risk modeling depends on\nmultiple variations of quantitative measures provided by third-party rating\nagencies and risk management consultancy companies. As these structured\nnumerical data become increasingly commoditized among institutional investors,\nthere has been a wide push into using alternative sources of data, such as\nfinancial news, earnings call transcripts, or social media content, to possibly\ngain a competitive edge in the industry. The volume of qualitative information\nor unstructured text data has exploded in the past decades and is now available\nfor due diligence to supplement quantitative measures of credit risk. This\npaper proposes a predictive downgrade model using solely news data represented\nby neural network embeddings. The model standalone achieves an Area Under the\nReceiver Operating Characteristic Curve (AUC) of more than 80 percent. The\noutput probability from this news model, as an additional feature, improves the\nperformance of our benchmark model using only quantitative measures by more\nthan 5 percent in terms of both AUC and recall rate. A qualitative evaluation\nalso indicates that news articles related to our predicted downgrade events are\nspecially relevant and high-quality in our business context.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.08204v1"
    },
    {
        "title": "Parsimonious Quantile Regression of Financial Asset Tail Dynamics via\n  Sequential Learning",
        "authors": [
            "Xing Yan",
            "Weizhong Zhang",
            "Lin Ma",
            "Wei Liu",
            "Qi Wu"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  We propose a parsimonious quantile regression framework to learn the dynamic\ntail behaviors of financial asset returns. Our model captures well both the\ntime-varying characteristic and the asymmetrical heavy-tail property of\nfinancial time series. It combines the merits of a popular sequential neural\nnetwork model, i.e., LSTM, with a novel parametric quantile function that we\nconstruct to represent the conditional distribution of asset returns. Our model\nalso captures individually the serial dependences of higher moments, rather\nthan just the volatility. Across a wide range of asset classes, the\nout-of-sample forecasts of conditional quantiles or VaR of our model outperform\nthe GARCH family. Further, the proposed approach does not suffer from the issue\nof quantile crossing, nor does it expose to the ill-posedness comparing to the\nparametric probability density function approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08263v1"
    },
    {
        "title": "Estimating and backtesting risk under heavy tails",
        "authors": [
            "Marcin Pitera",
            "Thorsten Schmidt"
        ],
        "category": "q-fin.RM",
        "published_year": "2020",
        "summary": "  While the {estimation} of risk is an important question in the daily business\nof banking and insurance, many existing plug-in estimation procedures suffer\nfrom an unnecessary bias. This often leads to the underestimation of risk and\nnegatively impacts backtesting results, especially in small sample cases. In\nthis article we show that the link between estimation bias and backtesting can\nbe traced back to the dual relationship between risk measures and the\ncorresponding performance measures, and discuss this in reference to\nvalue-at-risk, expected shortfall and expectile value-at-risk. Motivated by the\nconsistent underestimation of risk by plug-in procedures, we propose a new\nalgorithm for bias correction and show how to apply it for generalized Pareto\ndistributions to the i.i.d. setting and to a GARCH(1,1) time series. In\nparticular, we show that the application of our algorithm leads to gain in\nefficiency when heavy tails or heteroscedasticity exists in the data.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.09937v3"
    },
    {
        "title": "Limit Theorems for Default Contagion and Systemic Risk",
        "authors": [
            "Hamed Amini",
            "Zhongyuan Cao",
            "Agnes Sulem"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  We consider a general tractable model for default contagion and systemic risk\nin a heterogeneous financial network, subject to an exogenous macroeconomic\nshock. We show that, under some regularity assumptions, the default cascade\nmodel could be transferred to a death process problem represented by\nballs-and-bins model. We also reduce the dimension of the problem by\nclassifying banks according to different types, in an appropriate type space.\nThese types may be calibrated to real-world data by using machine learning\ntechniques. We then state various limit theorems regarding the final size of\ndefault cascade over different types. In particular, under suitable assumptions\non the degree and threshold distributions, we show that the final size of\ndefault cascade has asymptotically Gaussian fluctuations. We next state limit\ntheorems for different system-wide wealth aggregation functions and show how\nthe systemic risk measure, in a given stress test scenario, could be related to\nthe structure and heterogeneity of financial networks. We finally show how\nthese results could be used by a social planner to optimally target\ninterventions during a financial crisis, with a budget constraint and under\npartial information of the financial network.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.00248v1"
    },
    {
        "title": "Enabling Machine Learning Algorithms for Credit Scoring -- Explainable\n  Artificial Intelligence (XAI) methods for clear understanding complex\n  predictive models",
        "authors": [
            "Przemysław Biecek",
            "Marcin Chlebus",
            "Janusz Gajda",
            "Alicja Gosiewska",
            "Anna Kozak",
            "Dominik Ogonowski",
            "Jakub Sztachelski",
            "Piotr Wojewnik"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Rapid development of advanced modelling techniques gives an opportunity to\ndevelop tools that are more and more accurate. However as usually, everything\ncomes with a price and in this case, the price to pay is to loose\ninterpretability of a model while gaining on its accuracy and precision. For\nmanagers to control and effectively manage credit risk and for regulators to be\nconvinced with model quality the price to pay is too high. In this paper, we\nshow how to take credit scoring analytics in to the next level, namely we\npresent comparison of various predictive models (logistic regression, logistic\nregression with weight of evidence transformations and modern artificial\nintelligence algorithms) and show that advanced tree based models give best\nresults in prediction of client default. What is even more important and\nvaluable we also show how to boost advanced models using techniques which allow\nto interpret them and made them more accessible for credit risk practitioners,\nresolving the crucial obstacle in widespread deployment of more complex, 'black\nbox' models like random forests, gradient boosted or extreme gradient boosted\ntrees. All this will be shown on the large dataset obtained from the Polish\nCredit Bureau to which all the banks and most of the lending companies in the\ncountry do report the credit files. In this paper the data from lending\ncompanies were used. The paper then compares state of the art best practices in\ncredit risk modelling with new advanced modern statistical tools boosted by the\nlatest developments in the field of interpretability and explainability of\nartificial intelligence algorithms. We believe that this is a valuable\ncontribution when it comes to presentation of different modelling tools but\nwhat is even more important it is showing which methods might be used to get\ninsight and understanding of AI methods in credit risk context.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.06735v1"
    },
    {
        "title": "A Sparsity Algorithm with Applications to Corporate Credit Rating",
        "authors": [
            "Dan Wang",
            "Zhi Chen",
            "Ionut Florescu"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In Artificial Intelligence, interpreting the results of a Machine Learning\ntechnique often termed as a black box is a difficult task. A counterfactual\nexplanation of a particular \"black box\" attempts to find the smallest change to\nthe input values that modifies the prediction to a particular output, other\nthan the original one. In this work we formulate the problem of finding a\ncounterfactual explanation as an optimization problem. We propose a new\n\"sparsity algorithm\" which solves the optimization problem, while also\nmaximizing the sparsity of the counterfactual explanation. We apply the\nsparsity algorithm to provide a simple suggestion to publicly traded companies\nin order to improve their credit ratings. We validate the sparsity algorithm\nwith a synthetically generated dataset and we further apply it to quarterly\nfinancial statements from companies in financial, healthcare and IT sectors of\nthe US market. We provide evidence that the counterfactual explanation can\ncapture the nature of the real statement features that changed between the\ncurrent quarter and the following quarter when ratings improved. The empirical\nresults show that the higher the rating of a company the greater the \"effort\"\nrequired to further improve credit rating.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.10306v1"
    },
    {
        "title": "Predicting Credit Risk for Unsecured Lending: A Machine Learning\n  Approach",
        "authors": [
            "K. S. Naik"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  Since the 1990s, there have been significant advances in the technology space\nand the e-Commerce area, leading to an exponential increase in demand for\ncashless payment solutions. This has led to increased demand for credit cards,\nbringing along with it the possibility of higher credit defaults and hence\nhigher delinquency rates, over a period of time. The purpose of this research\npaper is to build a contemporary credit scoring model to forecast credit\ndefaults for unsecured lending (credit cards), by employing machine learning\ntechniques. As much of the customer payments data available to lenders, for\nforecasting Credit defaults, is imbalanced (skewed), on account of a limited\nsubset of default instances, this poses a challenge for predictive modelling.\nIn this research, this challenge is addressed by deploying Synthetic Minority\nOversampling Technique (SMOTE), a proven technique to iron out such imbalances,\nfrom a given dataset. On running the research dataset through seven different\nmachine learning models, the results indicate that the Light Gradient Boosting\nMachine (LGBM) Classifier model outperforms the other six classification\ntechniques. Thus, our research indicates that the LGBM classifier model is\nbetter equipped to deliver higher learning speeds, better efficiencies and\nmanage larger data volumes. We expect that deployment of this model will enable\nbetter and timely prediction of credit defaults for decision-makers in\ncommercial lending institutions and banks.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.02206v1"
    },
    {
        "title": "The application of techniques derived from artificial intelligence to\n  the prediction of the solvency of bank customers: case of the application of\n  the cart type decision tree (dt)",
        "authors": [
            "Karim Amzile",
            "Rajaa Amzile"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  In this study we applied the CART-type Decision Tree (DT-CART) method derived\nfrom artificial intelligence technique to the prediction of the solvency of\nbank customers, for this we used historical data of bank customers. However we\nhave adopted the process of Data Mining techniques, for this purpose we started\nwith a data preprocessing in which we clean the data and we deleted all rows\nwith outliers or missing values as well as rows with empty columns, then we\nfixed the variable to be explained (dependent or Target) and we also thought to\neliminate all explanatory (independent) variables that are not significant\nusing univariate analysis as well as the correlation matrix, then we applied\nour CART decision tree method using the SPSS tool. After completing our process\nof building our model (AD-CART), we started the process of evaluating and\ntesting the performance of our model, by which we found that the accuracy and\nprecision of our model is 71%, so we calculated the error ratios, and we found\nthat the error rate equal to 29%, this allowed us to conclude that our model at\na fairly good level in terms of precision, predictability and very precisely in\npredicting the solvency of our banking customers.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.13001v1"
    },
    {
        "title": "Risk Assessment with Generic Energy Storage under Exogenous and\n  Endogenous Uncertainty",
        "authors": [
            "Ning Qi",
            "Lin Cheng",
            "Yuxiang Wan",
            "Yingrui Zhuang",
            "Zeyu Liu"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Current risk assessment ignores the stochastic nature of energy storage\navailability itself and thus lead to potential risk during operation. This\npaper proposes the redefinition of generic energy storage (GES) that is allowed\nto offer probabilistic reserve. A data-driven unified model with exogenous and\nendogenous uncertainty (EXU & EDU) description is presented for four typical\ntypes of GES. Moreover, risk indices are proposed to assess the impact of\noverlooking (EXU & EDU) of GES. Comparative results between EXU & EDU are\nillustrated in distribution system with day-ahead chance-constrained\noptimization (CCO) and more severe risks are observed for the latter, which\nindicate that system operator (SO) should adopt novel strategies for EDU\nuncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.13991v1"
    },
    {
        "title": "Heterogeneous Information Network based Default Analysis on Banking\n  Micro and Small Enterprise Users",
        "authors": [
            "Zheng Zhang",
            "Yingsheng Ji",
            "Jiachen Shen",
            "Xi Zhang",
            "Guangwen Yang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Risk assessment is a substantial problem for financial institutions that has\nbeen extensively studied both for its methodological richness and its various\npractical applications. With the expansion of inclusive finance, recent\nattentions are paid to micro and small-sized enterprises (MSEs). Compared with\nlarge companies, MSEs present a higher exposure rate to default owing to their\ninsecure financial stability. Conventional efforts learn classifiers from\nhistorical data with elaborate feature engineering. However, the main obstacle\nfor MSEs involves severe deficiency in credit-related information, which may\ndegrade the performance of prediction. Besides, financial activities have\ndiverse explicit and implicit relations, which have not been fully exploited\nfor risk judgement in commercial banks. In particular, the observations on real\ndata show that various relationships between company users have additional\npower in financial risk analysis. In this paper, we consider a graph of banking\ndata, and propose a novel HIDAM model for the purpose. Specifically, we attempt\nto incorporate heterogeneous information network with rich attributes on\nmulti-typed nodes and links for modeling the scenario of business banking\nservice. To enhance feature representation of MSEs, we extract interactive\ninformation through meta-paths and fully exploit path information. Furthermore,\nwe devise a hierarchical attention mechanism respectively to learn the\nimportance of contents inside each meta-path and the importance of different\nmetapahs. Experimental results verify that HIDAM outperforms state-of-the-art\ncompetitors on real-world banking data.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.11849v2"
    },
    {
        "title": "Nonparametric Value-at-Risk via Sieve Estimation",
        "authors": [
            "Philipp Ratz"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Artificial Neural Networks (ANN) have been employed for a range of modelling\nand prediction tasks using financial data. However, evidence on their\npredictive performance, especially for time-series data, has been mixed.\nWhereas some applications find that ANNs provide better forecasts than more\ntraditional estimation techniques, others find that they barely outperform\nbasic benchmarks. The present article aims to provide guidance as to when the\nuse of ANNs might result in better results in a general setting. We propose a\nflexible nonparametric model and extend existing theoretical results for the\nrate of convergence to include the popular Rectified Linear Unit (ReLU)\nactivation function and compare the rate to other nonparametric estimators.\nFinite sample properties are then studied with the help of Monte-Carlo\nsimulations to provide further guidance. An application to estimate the\nValue-at-Risk of portfolios of varying sizes is also considered to show the\npractical implications.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.07101v1"
    },
    {
        "title": "Cyber Risk Assessment for Capital Management",
        "authors": [
            "Wing Fung Chong",
            "Runhuan Feng",
            "Hins Hu",
            "Linfeng Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper introduces a two-pillar cyber risk management framework to address\nthe pervasive challenges in managing cyber risk. The first pillar, cyber risk\nassessment, combines insurance frequency-severity models with cybersecurity\ncascade models to capture the unique nature of cyber risk. The second pillar,\ncyber capital management, facilitates informed allocation of capital for a\nbalanced cyber risk management strategy, including cybersecurity investments,\ninsurance coverage, and reserves. A case study, based on historical cyber\nincident data and realistic assumptions, demonstrates the necessity of\ncomprehensive cost-benefit analysis for budget-constrained companies with\ncompeting objectives in cyber risk management. In addition, sensitivity\nanalysis highlights the dependence of the optimal strategy on factors such as\nthe price of cybersecurity controls and their effectiveness. The framework's\nimplementation across a diverse range of companies yields general insights on\ncyber risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.08435v4"
    },
    {
        "title": "Multi-Dimensional self-exciting NBD process and Default portfolios",
        "authors": [
            "Masato Hisakado",
            "Kodai Hattori",
            "Shintaro Mori"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  In this study, we apply a multidimensional self-exciting negative binomial\ndistribution (SE-NBD) process to default portfolios with 13 sectors. The SE-NBD\nprocess is a Poisson process with a gamma-distributed intensity function. We\nextend the SE-NBD process to a multidimensional process. Using the\nmultidimensional SE-NBD process (MD-SE-NBD), we can estimate interactions\nbetween these 13 sectors as a network. By applying impact analysis, we can\nclassify upstream and downstream sectors. The upstream sectors are real-estate\nand financial institution (FI) sectors. From these upstream sectors, shock\nspreads to the downstream sectors. This is an amplifier of the shock. This is\nconsistent with the analysis of bubble bursts. We compare these results to the\nmultidimensional Hawkes process (MD-Hawkes) that has a zero-variance intensity\nfunction.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.14146v2"
    },
    {
        "title": "Estimating value at risk: LSTM vs. GARCH",
        "authors": [
            "Weronika Ormaniec",
            "Marcin Pitera",
            "Sajad Safarveisi",
            "Thorsten Schmidt"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Estimating value-at-risk on time series data with possibly heteroscedastic\ndynamics is a highly challenging task. Typically, we face a small data problem\nin combination with a high degree of non-linearity, causing difficulties for\nboth classical and machine-learning estimation algorithms. In this paper, we\npropose a novel value-at-risk estimator using a long short-term memory (LSTM)\nneural network and compare its performance to benchmark GARCH estimators.\n  Our results indicate that even for a relatively short time series, the LSTM\ncould be used to refine or monitor risk estimation processes and correctly\nidentify the underlying risk dynamics in a non-parametric fashion. We evaluate\nthe estimator on both simulated and market data with a focus on\nheteroscedasticity, finding that LSTM exhibits a similar performance to GARCH\nestimators on simulated data, whereas on real market data it is more sensitive\ntowards increasing or decreasing volatility and outperforms all existing\nestimators of value-at-risk in terms of exception rate and mean quantile score.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.10539v1"
    },
    {
        "title": "An unexpected stochastic dominance: Pareto distributions, dependence,\n  and diversification",
        "authors": [
            "Yuyu Chen",
            "Paul Embrechts",
            "Ruodu Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We find the perhaps surprising inequality that the weighted average of\nindependent and identically distributed Pareto random variables with infinite\nmean is larger than one such random variable in the sense of first-order\nstochastic dominance. This result holds for more general models including\nsuper-Pareto distributions, negative dependence, and triggering events, and\nyields superadditivity of the risk measure Value-at-Risk for these models.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.08471v4"
    },
    {
        "title": "E-backtesting",
        "authors": [
            "Qiuqi Wang",
            "Ruodu Wang",
            "Johanna Ziegel"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  In the recent Basel Accords, the Expected Shortfall (ES) replaces the\nValue-at-Risk (VaR) as the standard risk measure for market risk in the banking\nsector, making it the most important risk measure in financial regulation. One\nof the most challenging tasks in risk modeling practice is to backtest ES\nforecasts provided by financial institutions. To design a model-free\nbacktesting procedure for ES, we make use of the recently developed techniques\nof e-values and e-processes. Backtest e-statistics are introduced to formulate\ne-processes for risk measure forecasts, and unique forms of backtest\ne-statistics for VaR and ES are characterized using recent results on\nidentification functions. For a given backtest e-statistic, a few criteria for\noptimally constructing the e-processes are studied. The proposed method can be\nnaturally applied to many other risk measures and statistical quantities. We\nconduct extensive simulation studies and data analysis to illustrate the\nadvantages of the model-free backtesting method, and compare it with the ones\nin the literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.00991v5"
    },
    {
        "title": "Assessing the difference between integrated quantiles and integrated\n  cumulative distribution functions",
        "authors": [
            "Yunran Wei",
            "Ricardas Zitikis"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  This paper offers a mathematical invention that shows how to convert\nintegrated quantiles, which often appear in risk measures, into integrated\ncumulative distribution functions, which are technically more tractable from\nvarious perspectives. The invention helps to avoid a number of technical\nassumptions that have been traditionally imposed when working with quantities\ncontaining quantiles. In particular it helps to completely avoid the\nrequirement of the existence of a probability density function. The developed\nresults explain and illustrate the invention, whose byproducts include the\nassessment of model uncertainty and misspecification, and the derivation of\nstatistical inference results.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.16880v3"
    },
    {
        "title": "Multimodal Generative Models for Bankruptcy Prediction Using Textual\n  Data",
        "authors": [
            "Rogelio A. Mancisidor",
            "Kjersti Aas"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Textual data from financial filings, e.g., the Management's Discussion &\nAnalysis (MDA) section in Form 10-K, has been used to improve the prediction\naccuracy of bankruptcy models. In practice, however, we cannot obtain the MDA\nsection for all public companies, which limits the use of MDA data in\ntraditional bankruptcy models, as they need complete data to make predictions.\nThe two main reasons for the lack of MDA are: (i) not all companies are obliged\nto submit the MDA and (ii) technical problems arise when crawling and scrapping\nthe MDA section. To solve this limitation, this research introduces the\nConditional Multimodal Discriminative (CMMD) model that learns multimodal\nrepresentations that embed information from accounting, market, and textual\ndata modalities. The CMMD model needs a sample with all data modalities for\nmodel training. At test time, the CMMD model only needs access to accounting\nand market modalities to generate multimodal representations, which are further\nused to make bankruptcy predictions and to generate words from the missing MDA\nmodality. With this novel methodology, it is realistic to use textual data in\nbankruptcy prediction models, since accounting and market data are available\nfor all companies, unlike textual data. The empirical results of this research\nshow that if financial regulators, or investors, were to use traditional models\nusing MDA data, they would only be able to make predictions for 60% of the\ncompanies. Furthermore, the classification performance of our proposed\nmethodology is superior to that of a large number of traditional classifier\nmodels, taking into account all the companies in our sample.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.08405v5"
    },
    {
        "title": "A Comprehensive Survey on Enterprise Financial Risk Analysis from Big\n  Data Perspective",
        "authors": [
            "Yu Zhao",
            "Huaming Du",
            "Qing Li",
            "Fuzhen Zhuang",
            "Ji Liu",
            "Gang Kou"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Enterprise financial risk analysis aims at predicting the future financial\nrisk of enterprises. Due to its wide and significant application, enterprise\nfinancial risk analysis has always been the core research topic in the fields\nof Finance and Management. Based on advanced computer science and artificial\nintelligence technologies, enterprise risk analysis research is experiencing\nrapid developments and making significant progress. Therefore, it is both\nnecessary and challenging to comprehensively review the relevant studies.\nAlthough there are already some valuable and impressive surveys on enterprise\nrisk analysis from the perspective of Finance and Management, these surveys\nintroduce approaches in a relatively isolated way and lack recent advances in\nenterprise financial risk analysis. In contrast, this paper attempts to provide\na systematic literature survey of enterprise risk analysis approaches from Big\nData perspective, which reviews more than 250 representative articles in the\npast almost 50 years (from 1968 to 2023). To the best of our knowledge, this is\nthe first and only survey work on enterprise financial risk from Big Data\nperspective. Specifically, this survey connects and systematizes the existing\nenterprise financial risk studies, i.e. to summarize and interpret the\nproblems, methods, and spotlights in a comprehensive way. In particular, we\nfirst introduce the issues of enterprise financial risks in terms of their\ntypes,granularity, intelligence, and evaluation metrics, and summarize the\ncorresponding representative works. Then, we compare the analysis methods used\nto learn enterprise financial risk, and finally summarize the spotlights of the\nmost representative works. Our goal is to clarify current cutting-edge research\nand its possible future directions to model enterprise risk, aiming to fully\nunderstand the mechanisms of enterprise risk generation and contagion.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.14997v3"
    },
    {
        "title": "Empirical Asset Pricing via Ensemble Gaussian Process Regression",
        "authors": [
            "Damir Filipović",
            "Puneet Pasricha"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  We introduce an ensemble learning method based on Gaussian Process Regression\n(GPR) for predicting conditional expected stock returns given stock-level and\nmacro-economic information. Our ensemble learning approach significantly\nreduces the computational complexity inherent in GPR inference and lends itself\nto general online learning tasks. We conduct an empirical analysis on a large\ncross-section of US stocks from 1962 to 2016. We find that our method dominates\nexisting machine learning models statistically and economically in terms of\nout-of-sample $R$-squared and Sharpe ratio of prediction-sorted portfolios.\nExploiting the Bayesian nature of GPR, we introduce the mean-variance optimal\nportfolio with respect to the prediction uncertainty distribution of the\nexpected stock returns. It appeals to an uncertainty averse investor and\nsignificantly dominates the equal- and value-weighted prediction-sorted\nportfolios, which outperform the S&P 500.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.01048v2"
    },
    {
        "title": "A Time Series Approach to Explainability for Neural Nets with\n  Applications to Risk-Management and Fraud Detection",
        "authors": [
            "Marc Wildi",
            "Branka Hadji Misheva"
        ],
        "category": "q-fin.RM",
        "published_year": "2022",
        "summary": "  Artificial intelligence is creating one of the biggest revolution across\ntechnology driven application fields. For the finance sector, it offers many\nopportunities for significant market innovation and yet broad adoption of AI\nsystems heavily relies on our trust in their outputs. Trust in technology is\nenabled by understanding the rationale behind the predictions made. To this\nend, the concept of eXplainable AI emerged introducing a suite of techniques\nattempting to explain to users how complex models arrived at a certain\ndecision. For cross-sectional data classical XAI approaches can lead to\nvaluable insights about the models' inner workings, but these techniques\ngenerally cannot cope well with longitudinal data (time series) in the presence\nof dependence structure and non-stationarity. We here propose a novel XAI\ntechnique for deep learning methods which preserves and exploits the natural\ntime ordering of the data.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.02906v1"
    },
    {
        "title": "Auto.gov: Learning-based On-chain Governance for Decentralized Finance\n  (DeFi)",
        "authors": [
            "Jiahua Xu",
            "Daniel Perez",
            "Yebo Feng",
            "Benjamin Livshits"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In recent years, decentralized finance (DeFi) has experienced remarkable\ngrowth, with various protocols such as lending protocols and automated market\nmakers (AMMs) emerging. Traditionally, these protocols employ off-chain\ngovernance, where token holders vote to modify parameters. However, manual\nparameter adjustment, often conducted by the protocol's core team, is\nvulnerable to collusion, compromising the integrity and security of the system.\nFurthermore, purely deterministic, algorithm-based approaches may expose the\nprotocol to novel exploits and attacks.\n  In this paper, we present \"Auto.gov\", a learning-based on-chain governance\nframework for DeFi that enhances security and reduces susceptibility to\nattacks. Our model leverages a deep Q- network (DQN) reinforcement learning\napproach to propose semi-automated, intuitive governance proposals with\nquantitative justifications. This methodology enables the system to efficiently\nadapt to and mitigate the negative impact of malicious behaviors, such as price\noracle attacks, more effectively than benchmark models. Our evaluation\ndemonstrates that Auto.gov offers a more reactive, objective, efficient, and\nresilient solution compared to existing manual processes, thereby significantly\nbolstering the security and, ultimately, enhancing the profitability of DeFi\nprotocols.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.09551v2"
    },
    {
        "title": "Defining and comparing SICR-events for classifying impaired loans under\n  IFRS 9",
        "authors": [
            "Arno Botha",
            "Esmerelda Oberholzer",
            "Janette Larney",
            "Riaan de Jongh"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  The IFRS 9 accounting standard requires the prediction of credit\ndeterioration in financial instruments, i.e., significant increases in credit\nrisk (SICR). However, the definition of such a SICR-event is inherently\nambiguous, given its current reliance on evaluating the change in the estimated\nprobability of default (PD) against some arbitrary threshold. We examine the\nshortcomings of this PD-comparison approach and propose an alternative\nframework for generating SICR-definitions based on three parameters:\ndelinquency, stickiness, and the outcome period. Having varied these framework\nparameters, we obtain 27 unique SICR-definitions and fit logistic regression\nmodels accordingly using rich South African mortgage and macroeconomic data.\nFor each definition and corresponding model, the resulting SICR-rates are\nanalysed at the portfolio-level on their stability over time and their\nresponsiveness to economic downturns. At the account-level, we compare both the\naccuracy and dynamicity of the SICR-predictions, and discover several\ninteresting trends and trade-offs. These results can help any bank with\nappropriately setting the three framework parameters in defining SICR-events\nfor prediction purposes. We demonstrate this process by comparing the\nbest-performing SICR-model to the PD-comparison approach, and show the latter's\ninferiority as an early-warning system. Our work can therefore guide the\nformulation, modelling, and testing of any SICR-definition, thereby promoting\nthe timeous recognition of credit losses; the main imperative of IFRS 9.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.03080v3"
    },
    {
        "title": "Learning to Predict Short-Term Volatility with Order Flow Image\n  Representation",
        "authors": [
            "Artem Lensky",
            "Mingyu Hao"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Introduction: The paper addresses the challenging problem of predicting the\nshort-term realized volatility of the Bitcoin price using order flow\ninformation. The inherent stochastic nature and anti-persistence of price pose\ndifficulties in accurate prediction.\n  Methods: To address this, we propose a method that transforms order flow data\nover a fixed time interval (snapshots) into images. The order flow includes\ntrade sizes, trade directions, and limit order book, and is mapped into image\ncolour channels. These images are then used to train both a simple 3-layer\nConvolutional Neural Network (CNN) and more advanced ResNet-18 and ConvMixer,\nwith additionally supplementing them with hand-crafted features. The models are\nevaluated against classical GARCH, Multilayer Perceptron trained on raw data,\nand a naive guess method that considers current volatility as a prediction.\n  Results: The experiments are conducted using price data from January 2021 and\nevaluate model performance in terms of root mean square error (RMSPE). The\nresults show that our order flow representation with a CNN as a predictive\nmodel achieves the best performance, with an RMSPE of 0.85+/-1.1 for the model\nwith aggregated features and 1.0+/-1.4 for the model without feature\nsupplementation. ConvMixer with feature supplementation follows closely. In\ncomparison, the RMSPE for the naive guess method was 1.4+/-3.0.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.02472v2"
    },
    {
        "title": "On heavy-tailed risks under Gaussian copula: the effects of marginal\n  transformation",
        "authors": [
            "Bikramjit Das",
            "Vicky Fasen-Hartmann"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In this paper, we compute multivariate tail risk probabilities where the\nmarginal risks are heavy-tailed and the dependence structure is a Gaussian\ncopula. The marginal heavy-tailed risks are modeled using regular variation\nwhich leads to a few interesting consequences. First, as the threshold\nincreases, we note that the rate of decay of probabilities of tail sets vary\ndepending on the type of tail sets considered and the Gaussian correlation\nmatrix. Second, we discover that although any multivariate model with a\nGaussian copula admits the so called asymptotic tail independence property, the\njoint tail behavior under heavier tailed marginal variables is structurally\ndistinct from that under Gaussian marginal variables. The results obtained are\nillustrated using examples and simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.05004v1"
    },
    {
        "title": "Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock\n  Recommendation via Split Variational Adversarial Training",
        "authors": [
            "Jiezhu Cheng",
            "Kaizhu Huang",
            "Zibin Zheng"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In the stock market, a successful investment requires a good balance between\nprofits and risks. Based on the learning to rank paradigm, stock recommendation\nhas been widely studied in quantitative finance to recommend stocks with higher\nreturn ratios for investors. Despite the efforts to make profits, many existing\nrecommendation approaches still have some limitations in risk control, which\nmay lead to intolerable paper losses in practical stock investing. To\neffectively reduce risks, we draw inspiration from adversarial learning and\npropose a novel Split Variational Adversarial Training (SVAT) method for\nrisk-aware stock recommendation. Essentially, SVAT encourages the stock model\nto be sensitive to adversarial perturbations of risky stock examples and\nenhances the model's risk awareness by learning from perturbations. To generate\nrepresentative adversarial examples as risk indicators, we devise a variational\nperturbation generator to model diverse risk factors. Particularly, the\nvariational architecture enables our method to provide a rough risk\nquantification for investors, showing an additional advantage of\ninterpretability. Experiments on several real-world stock market datasets\ndemonstrate the superiority of our SVAT method. By lowering the volatility of\nthe stock recommendation model, SVAT effectively reduces investment risks and\noutperforms state-of-the-art baselines by more than 30% in terms of\nrisk-adjusted profits. All the experimental data and source code are available\nat\nhttps://drive.google.com/drive/folders/14AdM7WENEvIp5x5bV3zV_i4Aev21C9g6?usp=sharing.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.11043v2"
    },
    {
        "title": "UQ for Credit Risk Management: A deep evidence regression approach",
        "authors": [
            "Ashish Dhiman"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Machine Learning has invariantly found its way into various Credit Risk\napplications. Due to the intrinsic nature of Credit Risk, quantifying the\nuncertainty of the predicted risk metrics is essential, and applying\nuncertainty-aware deep learning models to credit risk settings can be very\nhelpful. In this work, we have explored the application of a scalable UQ-aware\ndeep learning technique, Deep Evidence Regression and applied it to predicting\nLoss Given Default. We contribute to the literature by extending the Deep\nEvidence Regression methodology to learning target variables generated by a\nWeibull process and provide the relevant learning framework. We demonstrate the\napplication of our approach to both simulated and real-world data.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04967v2"
    },
    {
        "title": "Risk-reducing design and operations toolkit: 90 strategies for managing\n  risk and uncertainty in decision problems",
        "authors": [
            "Alexander Gutfraind"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Uncertainty is a pervasive challenge in decision analysis, and decision\ntheory recognizes two classes of solutions: probabilistic models and cognitive\nheuristics. However, engineers, public planners and other decision-makers\ninstead use a third class of strategies that could be called RDOT\n(Risk-reducing Design and Operations Toolkit). These include incorporating\nrobustness into designs, contingency planning, and others that do not fall into\nthe categories of probabilistic models or cognitive heuristics. Moreover,\nidentical strategies appear in several domains and disciplines, pointing to an\nimportant shared toolkit.\n  The focus of this paper is to develop a catalog of such strategies and\ndevelop a framework for them. The paper finds more than 90 examples of such\nstrategies falling into six broad categories and argues that they provide an\nefficient response to decision problems that are seemingly intractable due to\nhigh uncertainty. It then proposes a framework to incorporate them into\ndecision theory using multi-objective optimization.\n  Overall, RDOT represents an overlooked class of responses to uncertainty.\nBecause RDOT strategies do not depend on accurate forecasting or estimation,\nthey could be applied fruitfully to certain decision problems affected by high\nuncertainty and make them much more tractable.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.03133v1"
    },
    {
        "title": "Quantifying Credit Portfolio sensitivity to asset correlations with\n  interpretable generative neural networks",
        "authors": [
            "Sergio Caprioli",
            "Emanuele Cagliero",
            "Riccardo Crupi"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In this research, we propose a novel approach for the quantification of\ncredit portfolio Value-at-Risk (VaR) sensitivity to asset correlations with the\nuse of synthetic financial correlation matrices generated with deep learning\nmodels. In previous work Generative Adversarial Networks (GANs) were employed\nto demonstrate the generation of plausible correlation matrices, that capture\nthe essential characteristics observed in empirical correlation matrices\nestimated on asset returns. Instead of GANs, we employ Variational Autoencoders\n(VAE) to achieve a more interpretable latent space representation. Through our\nanalysis, we reveal that the VAE latent space can be a useful tool to capture\nthe crucial factors impacting portfolio diversification, particularly in\nrelation to credit portfolio sensitivity to asset correlations changes.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08652v2"
    },
    {
        "title": "On the impact of insurance on households susceptible to random\n  proportional losses: An analysis of poverty trapping",
        "authors": [
            "Kira Henshaw",
            "Jorge M. Ramirez",
            "José M. Flores-Contró",
            "Enrique A. Thomann",
            "Sooie-Hoe Loke",
            "Corina Constantinescu"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In this paper, we consider a risk process with deterministic growth and\nmultiplicative jumps to model the capital of a low-income household. Reflecting\nthe high-risk nature of the low-income environment, capital losses are assumed\nto be proportional to the level of accumulated capital at the jump time. Our\naim is to derive the probability that a household falls below the poverty line,\ni.e. the trapping probability, where ``trapping\" occurs when the level of\ncapital of a household holds falls below the poverty line, to an area from\nwhich it is difficult to escape without external help. Considering the\nremaining proportion of capital to be distributed as a special case of the beta\ndistribution, closed-form expressions for the trapping probability are obtained\nvia analysis of the Laplace transform of the infinitesimal generator of the\nprocess. To study the impact of insurance on this probability, introduction of\nan insurance product offering proportional coverage is presented. The\ninfinitesimal generator of the insured process gives rise to non-local\ndifferential equations. To overcome this, we propose a recursive method for\nderiving a closed-form solution of the integro-differential equation associated\nwith the infinitesimal generator of the insured process and provide a numerical\nestimation method for obtaining the trapping probability. Constraints on the\nrate parameters of the process that prevent certain trapping are derived in\nboth the uninsured and insured cases using classical results from risk theory.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.09295v1"
    },
    {
        "title": "Joint model for longitudinal and spatio-temporal survival data",
        "authors": [
            "Victor Medina-Olivares",
            "Finn Lindgren",
            "Raffaella Calabrese",
            "Jonathan Crook"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  In credit risk analysis, survival models with fixed and time-varying\ncovariates are widely used to predict a borrower's time-to-event. When the\ntime-varying drivers are endogenous, modelling jointly the evolution of the\nsurvival time and the endogenous covariates is the most appropriate approach,\nalso known as the joint model for longitudinal and survival data. In addition\nto the temporal component, credit risk models can be enhanced when including\nborrowers' geographical information by considering spatial clustering and its\nvariation over time. We propose the Spatio-Temporal Joint Model (STJM) to\ncapture spatial and temporal effects and their interaction. This Bayesian\nhierarchical joint model reckons the survival effect of unobserved\nheterogeneity among borrowers located in the same region at a particular time.\nTo estimate the STJM model for large datasets, we consider the Integrated\nNested Laplace Approximation (INLA) methodology. We apply the STJM to predict\nthe time to full prepayment on a large dataset of 57,258 US mortgage borrowers\nwith more than 2.5 million observations. Empirical results indicate that\nincluding spatial effects consistently improves the performance of the joint\nmodel. However, the gains are less definitive when we additionally include\nspatio-temporal interactions.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.04008v1"
    },
    {
        "title": "Credit Risk Meets Large Language Models: Building a Risk Indicator from\n  Loan Descriptions in P2P Lending",
        "authors": [
            "Mario Sanz-Guerrero",
            "Javier Arroyo"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,\nlinking borrowers with lenders through online platforms. However, P2P lending\nfaces the challenge of information asymmetry, as lenders often lack sufficient\ndata to assess the creditworthiness of borrowers. This paper proposes a novel\napproach to address this issue by leveraging the textual descriptions provided\nby borrowers during the loan application process. Our methodology involves\nprocessing these textual descriptions using a Large Language Model (LLM), a\npowerful tool capable of discerning patterns and semantics within the text.\nTransfer learning is applied to adapt the LLM to the specific task at hand.\n  Our results derived from the analysis of the Lending Club dataset show that\nthe risk score generated by BERT, a widely used LLM, significantly improves the\nperformance of credit risk classifiers. However, the inherent opacity of\nLLM-based systems, coupled with uncertainties about potential biases,\nunderscores critical considerations for regulatory frameworks and engenders\ntrust-related concerns among end-users, opening new avenues for future research\nin the dynamic landscape of P2P lending and artificial intelligence.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.16458v2"
    },
    {
        "title": "On the Potential of Network-Based Features for Fraud Detection",
        "authors": [
            "Catayoun Azarm",
            "Erman Acar",
            "Mickey van Zeelt"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Online transaction fraud presents substantial challenges to businesses and\nconsumers, risking significant financial losses. Conventional rule-based\nsystems struggle to keep pace with evolving fraud tactics, leading to high\nfalse positive rates and missed detections. Machine learning techniques offer a\npromising solution by leveraging historical data to identify fraudulent\npatterns. This article explores using the personalised PageRank (PPR) algorithm\nto capture the social dynamics of fraud by analysing relationships between\nfinancial accounts. The primary objective is to compare the performance of\ntraditional features with the addition of PPR in fraud detection models.\nResults indicate that integrating PPR enhances the model's predictive power,\nsurpassing the baseline model. Additionally, the PPR feature provides unique\nand valuable information, evidenced by its high feature importance score.\nFeature stability analysis confirms consistent feature distributions across\ntraining and test datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09495v2"
    },
    {
        "title": "The TruEnd-procedure: Treating trailing zero-valued balances in credit\n  data",
        "authors": [
            "Arno Botha",
            "Tanja Verster",
            "Roelinde Bester"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  A novel procedure is presented for finding the true but latent endpoints\nwithin the repayment histories of individual loans. The monthly observations\nbeyond these true endpoints are false, largely due to operational failures that\ndelay account closure, thereby corrupting some loans in the dataset with\n`false' observations. Detecting these false observations is difficult at scale\nsince each affected loan history might have a different sequence of zero (or\nvery small) month-end balances that persist towards the end. Identifying these\ntrails of diminutive balances would require an exact definition of a \"small\nbalance\", which can be found using our so-called TruEnd-procedure. We\ndemonstrate this procedure and isolate the ideal small-balance definition using\nresidential mortgages from a large South African bank. Evidently, corrupted\nloans are remarkably prevalent and have excess histories that are surprisingly\nlong, which ruin the timing of certain risk events and compromise any\nsubsequent time-to-event model such as survival analysis. Excess histories can\nbe discarded using the ideal small-balance definition, which demonstrably\nimproves the accuracy of both the predicted timing and severity of risk events,\nwithout materially impacting the monetary value of the portfolio. The resulting\nestimates of credit losses are lower and less biased, which augurs well for\nraising accurate credit impairments under the IFRS 9 accounting standard. Our\nwork therefore addresses a pernicious data error, which highlights the pivotal\nrole of data preparation in producing credible forecasts of credit risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.17008v2"
    },
    {
        "title": "Hedging American Put Options with Deep Reinforcement Learning",
        "authors": [
            "Reilly Pickard",
            "Finn Wredenhagen",
            "Julio DeJesus",
            "Mario Schlener",
            "Yuri Lawryshyn"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This article leverages deep reinforcement learning (DRL) to hedge American\nput options, utilizing the deep deterministic policy gradient (DDPG) method.\nThe agents are first trained and tested with Geometric Brownian Motion (GBM)\nasset paths and demonstrate superior performance over traditional strategies\nlike the Black-Scholes (BS) Delta, particularly in the presence of transaction\ncosts. To assess the real-world applicability of DRL hedging, a second round of\nexperiments uses a market calibrated stochastic volatility model to train DRL\nagents. Specifically, 80 put options across 8 symbols are collected, stochastic\nvolatility model coefficients are calibrated for each symbol, and a DRL agent\nis trained for each of the 80 options by simulating paths of the respective\ncalibrated model. Not only do DRL agents outperform the BS Delta method when\ntesting is conducted using the same calibrated stochastic volatility model data\nfrom training, but DRL agents achieves better results when hedging the true\nasset path that occurred between the option sale date and the maturity. As\nsuch, not only does this study present the first DRL agents tailored for\nAmerican put option hedging, but results on both simulated and empirical market\ntesting data also suggest the optimality of DRL agents over the BS Delta method\nin real-world scenarios. Finally, note that this study employs a model-agnostic\nChebyshev interpolation method to provide DRL agents with option prices at each\ntime step when a stochastic volatility model is used, thereby providing a\ngeneral framework for an easy extension to more complex underlying asset\nprocesses.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06774v1"
    },
    {
        "title": "Large Language Model in Financial Regulatory Interpretation",
        "authors": [
            "Zhiyu Cao",
            "Zachary Feinstein"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This study explores the innovative use of Large Language Models (LLMs) as\nanalytical tools for interpreting complex financial regulations. The primary\nobjective is to design effective prompts that guide LLMs in distilling verbose\nand intricate regulatory texts, such as the Basel III capital requirement\nregulations, into a concise mathematical framework that can be subsequently\ntranslated into actionable code. This novel approach aims to streamline the\nimplementation of regulatory mandates within the financial reporting and risk\nmanagement systems of global banking institutions. A case study was conducted\nto assess the performance of various LLMs, demonstrating that GPT-4 outperforms\nother models in processing and collecting necessary information, as well as\nexecuting mathematical calculations. The case study utilized numerical\nsimulations with asset holdings -- including fixed income, equities, currency\npairs, and commodities -- to demonstrate how LLMs can effectively implement the\nBasel III capital adequacy requirements.\n  Keywords: Large Language Models, Prompt Engineering, LLMs in Finance, Basel\nIII, Minimum Capital Requirements, LLM Ethics\n",
        "pdf_link": "http://arxiv.org/pdf/2405.06808v2"
    },
    {
        "title": "On Joint Marginal Expected Shortfall and Associated Contribution Risk\n  Measures",
        "authors": [
            "Tong Pu",
            "Yifei Zhang",
            "Yiying Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Systemic risk is the risk that a company- or industry-level risk could\ntrigger a huge collapse of another or even the whole institution. Various\nsystemic risk measures have been proposed in the literature to quantify the\ndomino and (relative) spillover effects induced by systemic risks such as the\nwell-known CoVaR, CoES, MES and CoD risk measures, and associated contribution\nmeasures. This paper proposes another new type of systemic risk measure, called\nthe joint marginal expected shortfall (JMES), to measure whether the MES of one\nentity's risk-taking adds to another one or the overall risk conditioned on the\nevent that the entity is already in some specified distress level. We further\nintroduce two useful systemic risk contribution measures based on the\ndifference function or relative ratio function of the JMES and the conventional\nES, respectively. Some basic properties of these proposed measures are studied\nsuch as monotonicity, comonotonic additivity, non-identifiability and\nnon-elicitability. For both risk measures and two different vectors of\nbivariate risks, we establish sufficient conditions imposed on copula\nstructure, stress levels, and stochastic orders to compare these new measures.\nWe further provide some numerical examples to illustrate our main findings. A\nreal application in analyzing the risk contagion among several stock market\nindices is implemented to show the performances of our proposed measures\ncompared with other commonly used measures including CoVaR, CoES, MES, and\ntheir associated contribution measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.07549v1"
    },
    {
        "title": "Optimizing Deep Reinforcement Learning for American Put Option Hedging",
        "authors": [
            "Reilly Pickard",
            "F. Wredenhagen",
            "Y. Lawryshyn"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper contributes to the existing literature on hedging American options\nwith Deep Reinforcement Learning (DRL). The study first investigates\nhyperparameter impact on hedging performance, considering learning rates,\ntraining episodes, neural network architectures, training steps, and\ntransaction cost penalty functions. Results highlight the importance of\navoiding certain combinations, such as high learning rates with a high number\nof training episodes or low learning rates with few training episodes and\nemphasize the significance of utilizing moderate values for optimal outcomes.\nAdditionally, the paper warns against excessive training steps to prevent\ninstability and demonstrates the superiority of a quadratic transaction cost\npenalty function over a linear version. This study then expands upon the work\nof Pickard et al. (2024), who utilize a Chebyshev interpolation option pricing\nmethod to train DRL agents with market calibrated stochastic volatility models.\nWhile the results of Pickard et al. (2024) showed that these DRL agents achieve\nsatisfactory performance on empirical asset paths, this study introduces a\nnovel approach where new agents at weekly intervals to newly calibrated\nstochastic volatility models. Results show DRL agents re-trained using weekly\nmarket data surpass the performance of those trained solely on the sale date.\nFurthermore, the paper demonstrates that both single-train and weekly-train DRL\nagents outperform the Black-Scholes Delta method at transaction costs of 1% and\n3%. This practical relevance suggests that practitioners can leverage readily\navailable market data to train DRL agents for effective hedging of options in\ntheir portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.08602v1"
    },
    {
        "title": "Research on Credit Risk Early Warning Model of Commercial Banks Based on\n  Neural Network Algorithm",
        "authors": [
            "Yu Cheng",
            "Qin Yang",
            "Liyang Wang",
            "Ao Xiang",
            "Jingyu Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  In the realm of globalized financial markets, commercial banks are confronted\nwith an escalating magnitude of credit risk, thereby imposing heightened\nrequisites upon the security of bank assets and financial stability. This study\nharnesses advanced neural network techniques, notably the Backpropagation (BP)\nneural network, to pioneer a novel model for preempting credit risk in\ncommercial banks. The discourse initially scrutinizes conventional financial\nrisk preemptive models, such as ARMA, ARCH, and Logistic regression models,\ncritically analyzing their real-world applications. Subsequently, the\nexposition elaborates on the construction process of the BP neural network\nmodel, encompassing network architecture design, activation function selection,\nparameter initialization, and objective function construction. Through\ncomparative analysis, the superiority of neural network models in preempting\ncredit risk in commercial banks is elucidated. The experimental segment selects\nspecific bank data, validating the model's predictive accuracy and\npracticality. Research findings evince that this model efficaciously enhances\nthe foresight and precision of credit risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10762v2"
    },
    {
        "title": "Improving Realized LGD Approximation: A Novel Framework with XGBoost for\n  Handling Missing Cash-Flow Data",
        "authors": [
            "Zuzanna Kostecka",
            "Robert Ślepaczuk"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  The scope for the accurate calculation of the Loss Given Default (LGD)\nparameter is comprehensive in terms of financial data. In this research, we aim\nto explore methods for improving the approximation of realized LGD in\nconditions of limited access to the cash-flow data. We enhance the performance\nof the method which relies on the differences between exposure values (delta\noutstanding approach) by employing machine learning (ML) techniques. The\nresearch utilizes the data from the mortgage portfolio of one of the European\ncountries and assumes a close resemblance to similar economic contexts. It\nincorporates non-financial variables and macroeconomic data related to the\nhousing market, improving the accuracy of loss severity approximation. The\nproposed methodology attempts to mitigate the country-specific (related to the\nlocal legal) or portfolio-specific factors in aim to show the general advantage\nof applying ML techniques, rather than case-specific relation. We developed an\nXGBoost model that does not rely on cash-flow data yet enhances the accuracy of\nrealized LGD estimation compared to results obtained with the delta outstanding\napproach. A novel aspect of our work is the detailed exploration of the delta\noutstanding approach and the methodology for addressing conditions of limited\naccess to cash-flow data through machine learning models.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.17308v1"
    },
    {
        "title": "Systemic values-at-risk and their sample-average approximations",
        "authors": [
            "Wissam AlAli",
            "Çağın Ararat"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper investigates the convergence properties of sample-average\napproximations (SAA) for set-valued systemic risk measures. We assume that the\nsystemic risk measure is defined using a general aggregation function with some\ncontinuity properties and value-at-risk applied as a monetary risk measure. We\nfocus on the theoretical convergence of its SAA under Wijsman and Hausdorff\ntopologies for closed sets. After building the general theory, we provide an\nin-depth study of an important special case where the aggregation function is\ndefined based on the Eisenberg-Noe network model. In this case, we provide\nmixed-integer programming formulations for calculating the SAA sets via their\nweighted-sum and norm-minimizing scalarizations. To demonstrate the\napplicability of our findings, we conduct a comprehensive sensitivity analysis\nby generating a financial network based on the preferential attachment model\nand modeling the economic disruptions via a Pareto distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.08511v1"
    },
    {
        "title": "EX-DRL: Hedging Against Heavy Losses with EXtreme Distributional\n  Reinforcement Learning",
        "authors": [
            "Parvin Malekzadeh",
            "Zissis Poulos",
            "Jacky Chen",
            "Zeyu Wang",
            "Konstantinos N. Plataniotis"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Recent advancements in Distributional Reinforcement Learning (DRL) for\nmodeling loss distributions have shown promise in developing hedging strategies\nin derivatives markets. A common approach in DRL involves learning the\nquantiles of loss distributions at specified levels using Quantile Regression\n(QR). This method is particularly effective in option hedging due to its direct\nquantile-based risk assessment, such as Value at Risk (VaR) and Conditional\nValue at Risk (CVaR). However, these risk measures depend on the accurate\nestimation of extreme quantiles in the loss distribution's tail, which can be\nimprecise in QR-based DRL due to the rarity and extremity of tail data, as\nhighlighted in the literature. To address this issue, we propose EXtreme DRL\n(EX-DRL), which enhances extreme quantile prediction by modeling the tail of\nthe loss distribution with a Generalized Pareto Distribution (GPD). This method\nintroduces supplementary data to mitigate the scarcity of extreme quantile\nobservations, thereby improving estimation accuracy through QR. Comprehensive\nexperiments on gamma hedging options demonstrate that EX-DRL improves existing\nQR-based models by providing more precise estimates of extreme quantiles,\nthereby improving the computation and reliability of risk metrics for complex\nfinancial risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.12446v2"
    },
    {
        "title": "Unveiling the Potential of Graph Neural Networks in SME Credit Risk\n  Assessment",
        "authors": [
            "Bingyao Liu",
            "Iris Li",
            "Jianhua Yao",
            "Yuan Chen",
            "Guanming Huang",
            "Jiajing Wang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper takes the graph neural network as the technical framework,\nintegrates the intrinsic connections between enterprise financial indicators,\nand proposes a model for enterprise credit risk assessment. The main research\nwork includes: Firstly, based on the experience of predecessors, we selected 29\nenterprise financial data indicators, abstracted each indicator as a vertex,\ndeeply analyzed the relationships between the indicators, constructed a\nsimilarity matrix of indicators, and used the maximum spanning tree algorithm\nto achieve the graph structure mapping of enterprises; secondly, in the\nrepresentation learning phase of the mapped graph, a graph neural network model\nwas built to obtain its embedded representation. The feature vector of each\nnode was expanded to 32 dimensions, and three GraphSAGE operations were\nperformed on the graph, with the results pooled using the Pool operation, and\nthe final output of three feature vectors was averaged to obtain the graph's\nembedded representation; finally, a classifier was constructed using a\ntwo-layer fully connected network to complete the prediction task. Experimental\nresults on real enterprise data show that the model proposed in this paper can\nwell complete the multi-level credit level estimation of enterprises.\nFurthermore, the tree-structured graph mapping deeply portrays the intrinsic\nconnections of various indicator data of the company, and according to the ROC\nand other evaluation criteria, the model's classification effect is significant\nand has good \"robustness\".\n",
        "pdf_link": "http://arxiv.org/pdf/2409.17909v1"
    },
    {
        "title": "Tail Risk Analysis for Financial Time Series",
        "authors": [
            "Anna Kiriliouk",
            "Chen Zhou"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This book chapter illustrates how to apply extreme value statistics to\nfinancial time series data. Such data often exhibits strong serial dependence,\nwhich complicates assessment of tail risks. We discuss the two main approches\nto tail risk estimation, unconditional and conditional quantile forecasting. We\nuse the S&P 500 index as a case study to assess serial (extremal) dependence,\nperform an unconditional and conditional risk analysis, and apply backtesting\nmethods. Additionally, the chapter explores the impact of serial dependence on\nmultivariate tail dependence.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.18643v1"
    },
    {
        "title": "A Spatio-Temporal Machine Learning Model for Mortgage Credit Risk:\n  Default Probabilities and Loan Portfolios",
        "authors": [
            "Pascal Kündig",
            "Fabio Sigrist"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We introduce a novel machine learning model for credit risk by combining\ntree-boosting with a latent spatio-temporal Gaussian process model accounting\nfor frailty correlation. This allows for modeling non-linearities and\ninteractions among predictor variables in a flexible data-driven manner and for\naccounting for spatio-temporal variation that is not explained by observable\npredictor variables. We also show how estimation and prediction can be done in\na computationally efficient manner. In an application to a large U.S. mortgage\ncredit risk data set, we find that both predictive default probabilities for\nindividual loans and predictive loan portfolio loss distributions obtained with\nour novel approach are more accurate compared to conventional independent\nlinear hazard models and also linear spatio-temporal models. Using\ninterpretability tools for machine learning models, we find that the likely\nreasons for this outperformance are strong interaction and non-linear effects\nin the predictor variables and the presence of large spatio-temporal frailty\neffects.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.02846v1"
    },
    {
        "title": "Differentiable Inductive Logic Programming for Fraud Detection",
        "authors": [
            "Boris Wolfson",
            "Erman Acar"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Current trends in Machine Learning prefer explainability even when it comes\nat the cost of performance. Therefore, explainable AI methods are particularly\nimportant in the field of Fraud Detection. This work investigates the\napplicability of Differentiable Inductive Logic Programming (DILP) as an\nexplainable AI approach to Fraud Detection. Although the scalability of DILP is\na well-known issue, we show that with some data curation such as cleaning and\nadjusting the tabular and numerical data to the expected format of background\nfacts statements, it becomes much more applicable. While in processing it does\nnot provide any significant advantage on rather more traditional methods such\nas Decision Trees, or more recent ones like Deep Symbolic Classification, it\nstill gives comparable results. We showcase its limitations and points to\nimprove, as well as potential use cases where it can be much more useful\ncompared to traditional methods, such as recursive rule learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.21928v1"
    },
    {
        "title": "Debiasing Alternative Data for Credit Underwriting Using Causal\n  Inference",
        "authors": [
            "Chris Lam"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Alternative data provides valuable insights for lenders to evaluate a\nborrower's creditworthiness, which could help expand credit access to\nunderserved groups and lower costs for borrowers. But some forms of alternative\ndata have historically been excluded from credit underwriting because it could\nact as an illegal proxy for a protected class like race or gender, causing\nredlining. We propose a method for applying causal inference to a supervised\nmachine learning model to debias alternative data so that it might be used for\ncredit underwriting. We demonstrate how our algorithm can be used against a\npublic credit dataset to improve model accuracy across different racial groups,\nwhile providing theoretically robust nondiscrimination guarantees.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.22382v2"
    },
    {
        "title": "Conditional Forecasting of Margin Calls using Dynamic Graph Neural\n  Networks",
        "authors": [
            "Matteo Citterio",
            "Marco D'Errico",
            "Gabriele Visentin"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  We introduce a novel Dynamic Graph Neural Network (DGNN) architecture for\nsolving conditional $m$-steps ahead forecasting problems in temporal financial\nnetworks. The proposed DGNN is validated on simulated data from a temporal\nfinancial network model capturing stylized features of Interest Rate Swaps\n(IRSs) transaction networks, where financial entities trade swap contracts\ndynamically and the network topology evolves conditionally on a reference rate.\nThe proposed model is able to produce accurate conditional forecasts of net\nvariation margins up to a $21$-day horizon by leveraging conditional\ninformation under pre-determined stress test scenarios. Our work shows that the\nnetwork dynamics can be successfully incorporated into stress-testing\npractices, thus providing regulators and policymakers with a crucial tool for\nsystemic risk monitoring.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.23275v1"
    },
    {
        "title": "Risk Management with Feature-Enriched Generative Adversarial Networks\n  (FE-GAN)",
        "authors": [
            "Ling Chen"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  This paper investigates the application of Feature-Enriched Generative\nAdversarial Networks (FE-GAN) in financial risk management, with a focus on\nimproving the estimation of Value at Risk (VaR) and Expected Shortfall (ES).\nFE-GAN enhances existing GANs architectures by incorporating an additional\ninput sequence derived from preceding data to improve model performance. Two\nspecialized GANs models, the Wasserstein Generative Adversarial Network (WGAN)\nand the Tail Generative Adversarial Network (Tail-GAN), were evaluated under\nthe FE-GAN framework. The results demonstrate that FE-GAN significantly\noutperforms traditional architectures in both VaR and ES estimation. Tail-GAN,\nleveraging its task-specific loss function, consistently outperforms WGAN in ES\nestimation, while both models exhibit similar performance in VaR estimation.\nDespite these promising results, the study acknowledges limitations, including\nreliance on highly correlated temporal data and restricted applicability to\nother domains. Future research directions include exploring alternative input\ngeneration methods, dynamic forecasting models, and advanced neural network\narchitectures to further enhance GANs-based financial risk estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15519v1"
    },
    {
        "title": "Autoencoder Enhanced Realised GARCH on Volatility Forecasting",
        "authors": [
            "Qianli Zhao",
            "Chao Wang",
            "Richard Gerlach",
            "Giuseppe Storti",
            "Lingxiang Zhang"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Realised volatility has become increasingly prominent in volatility\nforecasting due to its ability to capture intraday price fluctuations. With a\ngrowing variety of realised volatility estimators, each with unique advantages\nand limitations, selecting an optimal estimator may introduce challenges. In\nthis thesis, aiming to synthesise the impact of various realised volatility\nmeasures on volatility forecasting, we propose an extension of the Realised\nGARCH model that incorporates an autoencoder-generated synthetic realised\nmeasure, combining the information from multiple realised measures in a\nnonlinear manner. Our proposed model extends existing linear methods, such as\nPrincipal Component Analysis and Independent Component Analysis, to reduce the\ndimensionality of realised measures. The empirical evaluation, conducted across\nfour major stock markets from January 2000 to June 2022 and including the\nperiod of COVID-19, demonstrates both the feasibility of applying an\nautoencoder to synthesise volatility measures and the superior effectiveness of\nthe proposed model in one-step-ahead rolling volatility forecasting. The model\nexhibits enhanced flexibility in parameter estimations across each rolling\nwindow, outperforming traditional linear approaches. These findings indicate\nthat nonlinear dimension reduction offers further adaptability and flexibility\nin improving the synthetic realised measure, with promising implications for\nfuture volatility forecasting applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.17136v1"
    },
    {
        "title": "Bayesian Analysis of Value-at-Risk with Product Partition Models",
        "authors": [
            "Giacomo Bormetti",
            "Maria Elena De Giuli",
            "Danilo Delpini",
            "Claudia Tarantola"
        ],
        "category": "q-fin.RM",
        "published_year": "2008",
        "summary": "  In this paper we propose a novel Bayesian methodology for Value-at-Risk\ncomputation based on parametric Product Partition Models. Value-at-Risk is a\nstandard tool to measure and control the market risk of an asset or a\nportfolio, and it is also required for regulatory purposes. Its popularity is\npartly due to the fact that it is an easily understood measure of risk. The use\nof Product Partition Models allows us to remain in a Normal setting even in\npresence of outlying points, and to obtain a closed-form expression for\nValue-at-Risk computation. We present and compare two different scenarios: a\nproduct partition structure on the vector of means and a product partition\nstructure on the vector of variances. We apply our methodology to an Italian\nstock market data set from Mib30. The numerical results clearly show that\nProduct Partition Models can be successfully exploited in order to quantify\nmarket risk exposure. The obtained Value-at-Risk estimates are in full\nagreement with Maximum Likelihood approaches, but our methodology provides\nricher information about the clustering structure of the data and the presence\nof outlying points.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.0241v2"
    },
    {
        "title": "Impact of Insurance for Operational Risk: Is it worthwhile to insure or\n  be insured for severe losses?",
        "authors": [
            "Gareth W. Peters",
            "Aaron D. Byrnes",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.RM",
        "published_year": "2010",
        "summary": "  Under the Basel II standards, the Operational Risk (OpRisk) advanced\nmeasurement approach allows a provision for reduction of capital as a result of\ninsurance mitigation of up to 20%. This paper studies the behaviour of\ndifferent insurance policies in the context of capital reduction for a range of\npossible extreme loss models and insurance policy scenarios in a multi-period,\nmultiple risk settings. A Loss Distributional Approach (LDA) for modelling of\nthe annual loss process, involving homogeneous compound Poisson processes for\nthe annual losses, with heavy tailed severity models comprised of alpha-stable\nseverities is considered. There has been little analysis of such models to date\nand it is believed, insurance models will play more of a role in OpRisk\nmitigation and capital reduction in future. The first question of interest is\nwhen would it be equitable for a bank or financial institution to purchase\ninsurance for heavy tailed OpRisk losses under different insurance policy\nscenarios? The second question then pertains to Solvency II and addresses what\nthe insurers capital would be for such operational risk scenarios under\ndifferent policy offerings. In addition we consider the insurers perspective\nwith respect to fair premium as a percentage above the expected annual claim\nfor each insurance policy. The intention being to address questions related to\nVaR reduction under Basel II, SCR under Solvency II and fair insurance premiums\nin OpRisk for different extreme loss scenarios. In the process we provide\nclosed form solutions for the distribution of loss process and claims process\nin an LDA structure as well as closed form analytic solutions for the Expected\nShortfall, SCR and MCR under Basel II and Solvency II. We also provide closed\nform analytic solutions for the annual loss distribution of multiple risks\nincluding insurance mitigation.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4406v2"
    },
    {
        "title": "Analytic Loss Distributional Approach Model for Operational Risk from\n  the alpha-Stable Doubly Stochastic Compound Processes and Implications for\n  Capital Allocation",
        "authors": [
            "Gareth W. Peters",
            "Pavel Shevchenko",
            "Mark Young",
            "Wendy Yip"
        ],
        "category": "q-fin.RM",
        "published_year": "2011",
        "summary": "  Under the Basel II standards, the Operational Risk (OpRisk) advanced\nmeasurement approach is not prescriptive regarding the class of statistical\nmodel utilised to undertake capital estimation. It has however become well\naccepted to utlise a Loss Distributional Approach (LDA) paradigm to model the\nindividual OpRisk loss process corresponding to the Basel II Business\nline/event type. In this paper we derive a novel class of doubly stochastic\nalpha-stable family LDA models. These models provide the ability to capture the\nheavy tailed loss process typical of OpRisk whilst also providing analytic\nexpressions for the compound process annual loss density and distributions as\nwell as the aggregated compound process annual loss models. In particular we\ndevelop models of the annual loss process in two scenarios. The first scenario\nconsiders the loss process with a stochastic intensity parameter, resulting in\nan inhomogeneous compound Poisson processes annually. The resulting arrival\nprocess of losses under such a model will have independent counts over\nincrements within the year. The second scenario considers discretization of the\nannual loss process into monthly increments with dependent time increments as\ncaptured by a Binomial process with a stochastic probability of success\nchanging annually. Each of these models will be coupled under an LDA framework\nwith heavy-tailed severity models comprised of $\\alpha$-stable severities for\nthe loss amounts per loss event. In this paper we will derive analytic results\nfor the annual loss distribution density and distribution under each of these\nmodels and study their properties.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3582v1"
    },
    {
        "title": "Financial density forecasts: A comprehensive comparison of risk-neutral\n  and historical schemes",
        "authors": [
            "Ricardo Crisostomo",
            "Lorena Couso"
        ],
        "category": "q-fin.RM",
        "published_year": "2018",
        "summary": "  We investigate the forecasting ability of the most commonly used benchmarks\nin financial economics. We approach the usual caveats of probabilistic\nforecasts studies -small samples, limited models and non-holistic validations-\nby performing a comprehensive comparison of 15 predictive schemes during a time\nperiod of over 21 years. All densities are evaluated in terms of their\nstatistical consistency, local accuracy and forecasting errors. Using a new\ncomposite indicator, the Integrated Forecast Score (IFS), we show that\nrisk-neutral densities outperform historical-based predictions in terms of\ninformation content. We find that the Variance Gamma model generates the\nhighest out-of-sample likelihood of observed prices and the lowest predictive\nerrors, whereas the ARCH-based GJR-FHS delivers the most consistent forecasts\nacross the entire density range. In contrast, lognormal densities, the Heston\nmodel or the Breeden-Litzenberger formula yield biased predictions and are\nrejected in statistical tests.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.08007v2"
    },
    {
        "title": "Estimating Quantile Families of Loss Distributions for Non-Life\n  Insurance Modelling via L-moments",
        "authors": [
            "Gareth W. Peters",
            "Wilson Y. Chen",
            "Richard H. Gerlach"
        ],
        "category": "q-fin.RM",
        "published_year": "2016",
        "summary": "  This paper discusses different classes of loss models in non-life insurance\nsettings. It then overviews the class Tukey transform loss models that have not\nyet been widely considered in non-life insurance modelling, but offer\nopportunities to produce flexible skewness and kurtosis features often required\nin loss modelling. In addition, these loss models admit explicit quantile\nspecifications which make them directly relevant for quantile based risk\nmeasure calculations. We detail various parameterizations and sub-families of\nthe Tukey transform based models, such as the g-and-h, g-and-k and g-and-j\nmodels, including their properties of relevance to loss modelling.\n  One of the challenges with such models is to perform robust estimation for\nthe loss model parameters that will be amenable to practitioners when fitting\nsuch models. In this paper we develop a novel, efficient and robust estimation\nprocedure for estimation of model parameters in this family Tukey transform\nmodels, based on L-moments. It is shown to be more robust and efficient than\ncurrent state of the art methods of estimation for such families of loss models\nand is simple to implement for practical purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01041v1"
    },
    {
        "title": "Efficient Black-Box Importance Sampling for VaR and CVaR Estimation",
        "authors": [
            "Anand Deo",
            "Karthyek Murthy"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  This paper considers Importance Sampling (IS) for the estimation of tail\nrisks of a loss defined in terms of a sophisticated object such as a machine\nlearning feature map or a mixed integer linear optimisation formulation.\nAssuming only black-box access to the loss and the distribution of the\nunderlying random vector, the paper presents an efficient IS algorithm for\nestimating the Value at Risk and Conditional Value at Risk. The key challenge\nin any IS procedure, namely, identifying an appropriate change-of-measure, is\nautomated with a self-structuring IS transformation that learns and replicates\nthe concentration properties of the conditional excess from less rare samples.\nThe resulting estimators enjoy asymptotically optimal variance reduction when\nviewed in the logarithmic scale. Simulation experiments highlight the efficacy\nand practicality of the proposed scheme\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10236v1"
    },
    {
        "title": "Approximations to ultimate ruin probabilities with a Wienner process\n  perturbation",
        "authors": [
            "Yacine Koucha",
            "Alfredo D. Egidio dos Reis"
        ],
        "category": "q-fin.RM",
        "published_year": "2021",
        "summary": "  In this paper, we adapt the classic Cram\\'er-Lundberg collective risk theory\nmodel to a perturbed model by adding a Wiener process to the compound Poisson\nprocess, which can be used to incorporate premium income uncertainty, interest\nrate fluctuations and changes in the number of policyholders. Our study is part\nof a Master dissertation, our aim is to make a short overview and present\nadditionally some new approximation methods for the infinite time ruin\nprobabilities for the perturbed risk model. We present four different\napproximation methods for the perturbed risk model. The first method is based\non iterative upper and lower approximations to the maximal aggregate loss\ndistribution. The second method relies on a four-moment exponential De Vylder\napproximation. The third method is based on the first-order Pad\\'e\napproximation of the Renyi and De Vylder approximations. The last method is the\nsecond order Pad\\'e-Ramsay approximation. These are generated by fitting one,\ntwo, three or four moments of the claim amount distribution, which greatly\ngeneralizes the approximations. We test the precision of approximations using a\ncombination of light and heavy tailed distributions for the individual claim\namount. We assess the ultimate ruin probability and present numerical results\nfor the exponential, gamma, and mixed exponential claim distributions,\ndemonstrating the high accuracy of these four methods. Analytical and numerical\nmethods are used to highlight the practical implications of our findings.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.02537v1"
    },
    {
        "title": "FinPT: Financial Risk Prediction with Profile Tuning on Pretrained\n  Foundation Models",
        "authors": [
            "Yuwei Yin",
            "Yazheng Yang",
            "Jian Yang",
            "Qi Liu"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  Financial risk prediction plays a crucial role in the financial sector.\nMachine learning methods have been widely applied for automatically detecting\npotential risks and thus saving the cost of labor. However, the development in\nthis field is lagging behind in recent years by the following two facts: 1) the\nalgorithms used are somewhat outdated, especially in the context of the fast\nadvance of generative AI and large language models (LLMs); 2) the lack of a\nunified and open-sourced financial benchmark has impeded the related research\nfor years. To tackle these issues, we propose FinPT and FinBench: the former is\na novel approach for financial risk prediction that conduct Profile Tuning on\nlarge pretrained foundation models, and the latter is a set of high-quality\ndatasets on financial risks such as default, fraud, and churn. In FinPT, we\nfill the financial tabular data into the pre-defined instruction template,\nobtain natural-language customer profiles by prompting LLMs, and fine-tune\nlarge foundation models with the profile text to make predictions. We\ndemonstrate the effectiveness of the proposed FinPT by experimenting with a\nrange of representative strong baselines on FinBench. The analytical studies\nfurther deepen the understanding of LLMs for financial risk prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.00065v1"
    },
    {
        "title": "Estimating Systemic Risk within Financial Networks: A Two-Step\n  Nonparametric Method",
        "authors": [
            "Weihuan Huang"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  CoVaR (conditional value-at-risk) is a crucial measure for assessing\nfinancial systemic risk, which is defined as a conditional quantile of a random\nvariable, conditioned on other random variables reaching specific quantiles. It\nenables the measurement of risk associated with a particular node in financial\nnetworks, taking into account the simultaneous influence of risks from multiple\ncorrelated nodes. However, estimating CoVaR presents challenges due to the\nunobservability of the multivariate-quantiles condition. To address the\nchallenges, we propose a two-step nonparametric estimation approach based on\nMonte-Carlo simulation data. In the first step, we estimate the unobservable\nmultivariate-quantiles using order statistics. In the second step, we employ a\nkernel method to estimate the conditional quantile conditional on the order\nstatistics. We establish the consistency and asymptotic normality of the\ntwo-step estimator, along with a bandwidth selection method. The results\ndemonstrate that, under a mild restriction on the bandwidth, the estimation\nerror arising from the first step can be ignored. Consequently, the asymptotic\nresults depend solely on the estimation error of the second step, as if the\nmultivariate-quantiles in the condition were observable. Numerical experiments\ndemonstrate the favorable performance of the two-step estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.18658v1"
    },
    {
        "title": "Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning",
        "authors": [
            "Javier Mancilla",
            "André Sequeira",
            "Tomas Tagliani",
            "Francisco Llaneza",
            "Claudio Beiza"
        ],
        "category": "q-fin.RM",
        "published_year": "2024",
        "summary": "  Quantum Kernels are projected to provide early-stage usefulness for quantum\nmachine learning. However, highly sophisticated classical models are hard to\nsurpass without losing interpretability, particularly when vast datasets can be\nexploited. Nonetheless, classical models struggle once data is scarce and\nskewed. Quantum feature spaces are projected to find better links between data\nfeatures and the target class to be predicted even in such challenging\nscenarios and most importantly, enhanced generalization capabilities. In this\nwork, we propose a novel approach called Systemic Quantum Score (SQS) and\nprovide preliminary results indicating potential advantage over purely\nclassical models in a production grade use case for the Finance sector. SQS\nshows in our specific study an increased capacity to extract patterns out of\nfewer data points as well as improved performance over data-hungry algorithms\nsuch as XGBoost, providing advantage in a competitive market as it is the\nFinTech and Neobank regime.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.00015v3"
    },
    {
        "title": "Decision-Making Frameworks for Network Resilience -- Managing and\n  Mitigating Systemic (Cyber) Risk",
        "authors": [
            "Gregor Svindland",
            "Alexander Voß"
        ],
        "category": "q-fin.RM",
        "published_year": "2023",
        "summary": "  We introduce a decision-making framework tailored for the management of\nsystemic risk in networks. This framework is constructed upon three fundamental\ncomponents: (1) a set of acceptable network configurations, (2) a set of\ninterventions aimed at risk mitigation, and (3) a cost function quantifying the\nexpenses associated with these interventions. While our discussion primarily\nrevolves around the management of systemic cyber risks in digital networks, we\nconcurrently draw parallels to risk management of other complex systems where\nanalogous approaches may be adequate.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.13884v3"
    }
]