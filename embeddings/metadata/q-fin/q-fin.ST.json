[
    {
        "title": "The Mirage of Triangular Arbitrage in the Spot Foreign Exchange Market",
        "authors": [
            "Daniel J. Fenn",
            "Sam D. Howison",
            "Mark McDonald",
            "Stacy Williams",
            "Neil F. Johnson"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We investigate triangular arbitrage within the spot foreign exchange market\nusing high-frequency executable prices. We show that triangular arbitrage\nopportunities do exist, but that most have short durations and small\nmagnitudes. We find intra-day variations in the number and length of arbitrage\nopportunities, with larger numbers of opportunities with shorter mean durations\noccurring during more liquid hours. We demonstrate further that the number of\narbitrage opportunities has decreased in recent years, implying a corresponding\nincrease in pricing efficiency. Using trading simulations, we show that a\ntrader would need to beat other market participants to an unfeasibly large\nproportion of arbitrage prices to profit from triangular arbitrage over a\nprolonged period of time. Our results suggest that the foreign exchange market\nis internally self-consistent and provide a limited verification of market\nefficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.0913v1"
    },
    {
        "title": "Preferred numbers and the distribution of trade sizes and trading\n  volumes in the Chinese stock market",
        "authors": [
            "Guo-Hua Mu",
            "Wei Chen",
            "János Kertész",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The distribution of trade sizes and trading volumes are investigated based on\nthe limit order book data of 22 liquid Chinese stocks listed on the Shenzhen\nStock Exchange in the whole year 2003. We observe that the size distribution of\ntrades for individual stocks exhibits jumps, which is caused by the number\npreference of traders when placing orders. We analyze the applicability of the\n\"$q$-Gamma\" function for fitting the distribution by the Cram\\'{e}r-von Mises\ncriterion. The empirical PDFs of trading volumes at different timescales\n$\\Delta{t}$ ranging from 1 min to 240 min can be well modeled. The\napplicability of the $q$-Gamma functions for multiple trades is restricted to\nthe transaction numbers $\\Delta{n}\\leqslant8$. We find that all the PDFs have\npower-law tails for large volumes. Using careful estimation of the average tail\nexponents $\\alpha$ of the distribution of trade sizes and trading volumes, we\nget $\\alpha>2$, well outside the L{\\'e}vy regime.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.1512v1"
    },
    {
        "title": "On properties of Continuous-Time Random Walks with Non-Poissonian\n  jump-times",
        "authors": [
            "Javier Villarroel",
            "Miquel Montero"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The usual development of the continuous-time random walk (CTRW) proceeds by\nassuming that the present is one of the jumping times. Under this restrictive\nassumption integral equations for the propagator and mean escape times have\nbeen derived. We generalize these results to the case when the present is an\narbitrary time by recourse to renewal theory. The case of Erlang distributed\ntimes is analyzed in detail. Several concrete examples are considered.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.2148v1"
    },
    {
        "title": "Probability of Large Movements in Financial Markets",
        "authors": [
            "Robert Kitt",
            "Maksim Sakki",
            "Jaan Kalda"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Based on empirical financial time-series, we show that the \"silence-breaking\"\nprobability follows a super-universal power law: the probability of observing a\nlarge movement is inversely proportional to the length of the on-going\nlow-variability period. Such a scaling law has been previously predicted\ntheoretically [R. Kitt, J. Kalda, Physica A 353 (2005) 480], assuming that the\nlength-distribution of the low-variability periods follows a multiscaling power\nlaw.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.4455v2"
    },
    {
        "title": "A long-range memory stochastic model of the return in financial markets",
        "authors": [
            "V. Gontis",
            "J. Ruseckas",
            "A. Kononovicius"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We present a nonlinear stochastic differential equation (SDE) which mimics\nthe probability density function (PDF) of the return and the power spectrum of\nthe absolute return in financial markets. Absolute return as a measure of\nmarket volatility is considered in the proposed model as a long-range memory\nstochastic variable. The SDE is obtained from the analogy with earlier proposed\nmodel of trading activity in the financial markets and generalized within the\nnonextensive statistical mechanics framework. The proposed stochastic model\ngenerates time series of the return with two power law statistics, i.e., the\nPDF and the power spectral density, reproducing the empirical data for the one\nminute trading return in the NYSE.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.0903v3"
    },
    {
        "title": "Correction to \"Leverage and volatility feedback effects in\n  high-frequency data\" [J. Financial Econometrics 4 (2006) 353--384]",
        "authors": [
            "Amparo Baillo"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Bollerslev et al. (2006) study the cross-covariances for squared returns\nunder the Heston (1993) stochastic volatility model. In order to obtain these\ncross-covariances the authors use an incorrect expression for the distribution\nof the squared returns. Here we will obtain the correct distribution of the\nsquared returns and check that, under this new distribution, the result in\nAppendix A.2 in Bollerslev et al. (2006) still holds.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.0713v1"
    },
    {
        "title": "Mechanical Model of Personal Income Distribution",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  A microeconomic model is developed, which accurately predicts the shape of\npersonal income distribution (PID) in the United States and the evolution of\nthe shape over time. The underlying concept is borrowed from geo-mechanics and\nthus can be considered as mechanics of income distribution. The model allows\nthe resolution of empirical and definitional problems associated with personal\nincome measurements. It also serves as a firm fundament for definitions of\nincome inequality as secondary derivatives from personal income distribution.\nIt is found that in relative terms the PID in the US has not been changing\nsince 1947. Effectively, the Gini coefficient has been almost constant during\nthe last 60 years, as reported by the Census Bureau.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.0203v1"
    },
    {
        "title": "What is the best firm size to invest?",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Significant differences in the evolution of firm size distribution for\nvarious industries in the United States have been revealed and documented. For\ntheoretical considerations, this finding puts major constraints on the\nmodelling of firm growth. For practical purposes, the observed differences\ncreate a solid basis for selective investment strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.0286v1"
    },
    {
        "title": "Statistical analysis of the overnight and daytime return",
        "authors": [
            "Fengzhong Wang",
            "Shwu-Jane Shieh",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We investigate the two components of the total daily return (close-to-close),\nthe overnight return (close-to-open) and the daytime return (open-to-close), as\nwell as the corresponding volatilities of the 2215 NYSE stocks from 1988 to\n2007. The tail distribution of the volatility, the long-term memory in the\nsequence, and the cross-correlation between different returns are analyzed. Our\nresults suggest that: (i) The two component returns and volatilities have\nsimilar features as that of the total return and volatility. The tail\ndistribution follows a power law for all volatilities, and long-term\ncorrelations exist in the volatility sequences but not in the return sequences.\n(ii) The daytime return contributes more to the total return. Both the tail\ndistribution and the long-term memory of the daytime volatility are more\nsimilar to that of the total volatility, compared to the overnight records. In\naddition, the cross-correlation between the daytime return and the total return\nis also stronger. (iii) The two component returns tend to be anti-correlated.\nMoreover, we find that the cross-correlations between the three different\nreturns (total, overnight, and daytime) are quite stable over the entire\n20-year period.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.0993v1"
    },
    {
        "title": "Statistical thermodynamics of economic systems",
        "authors": [
            "H. Quevedo",
            "M. N. Quevedo"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We formulate thermodynamics of economic systems in terms of an arbitrary\nprobability distribution for a conserved economic quantity. As in statistical\nphysics, thermodynamic macroeconomic variables emerge as the mean value of\nmicroeconomic variables and their determination is reduced to the computation\nof the partition function, starting from an arbitrary function. Explicit\nhypothetical examples are given which include linear and nonlinear economic\nsystems, as well as multiplicative systems such as those dominated by a Pareto\nlaw distribution. We propose to use the formalism of phase transitions to study\nsevere changes of macroeconomic variables.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.4216v2"
    },
    {
        "title": "Long-term correlations and multifractal analysis of trading volumes for\n  Chinese stocks",
        "authors": [
            "Guo-Hua Mu",
            "Wei Chen",
            "János Kertész",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We investigate the temporal correlations and multifractal nature of trading\nvolume of 22 liquid stocks traded on the Shenzhen Stock Exchange in 2003. We\nfind that the trading volume exhibit size-dependent non-universal long memory\nand multifractal nature. No crossover in the power-law dependence of the\ndetrended fluctuation functions is observed. Our results show that the intraday\npattern in the trading volume has negligible impact on the long memory and\nmultifractality.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1042v1"
    },
    {
        "title": "Scaling and memory in the return intervals of realized volatility",
        "authors": [
            "Fei Ren",
            "Gao-Feng Gu",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We perform return interval analysis of 1-min {\\em{realized volatility}}\ndefined by the sum of absolute high-frequency intraday returns for the Shanghai\nStock Exchange Composite Index (SSEC) and 22 constituent stocks of SSEC. The\nscaling behavior and memory effect of the return intervals between successive\nrealized volatilities above a certain threshold $q$ are carefully investigated.\nIn comparison with the volatility defined by the closest tick prices to the\nminute marks, the return interval distribution for the realized volatility\nshows a better scaling behavior since 20 stocks (out of 22 stocks) and the SSEC\npass the Kolmogorov-Smirnov (KS) test and exhibit scaling behaviors, among\nwhich the scaling function for 8 stocks could be approximated well by a\nstretched exponential distribution revealed by the KS goodness-of-fit test\nunder the significance level of 5%. The improved scaling behavior is further\nconfirmed by the relation between the fitted exponent $\\gamma$ and the\nthreshold $q$. In addition, the similarity of the return interval distributions\nfor different stocks is also observed for the realized volatility. The\ninvestigation of the conditional probability distribution and the detrended\nfluctuation analysis (DFA) show that both short-term and long-term memory\nexists in the return intervals of realized volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1107v2"
    },
    {
        "title": "Colloquium: Statistical mechanics of money, wealth, and income",
        "authors": [
            "Victor M. Yakovenko",
            "J. Barkley Rosser"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  This Colloquium reviews statistical models for money, wealth, and income\ndistributions developed in the econophysics literature since the late 1990s. By\nanalogy with the Boltzmann-Gibbs distribution of energy in physics, it is shown\nthat the probability distribution of money is exponential for certain classes\nof models with interacting economic agents. Alternative scenarios are also\nreviewed. Data analysis of the empirical distributions of wealth and income\nreveals a two-class distribution. The majority of the population belongs to the\nlower class, characterized by the exponential (\"thermal\") distribution, whereas\na small fraction of the population in the upper class is characterized by the\npower-law (\"superthermal\") distribution. The lower part is very stable,\nstationary in time, whereas the upper part is highly dynamical and out of\nequilibrium.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.1518v2"
    },
    {
        "title": "The effect of a market factor on information flow between stocks using\n  minimal spanning tree",
        "authors": [
            "Cheoljun Eom",
            "Okyu Kwon",
            "Woo-Sung Jung",
            "Seunghwan Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We empirically investigated the effects of market factors on the information\nflow created from N(N-1)/2 linkage relationships among stocks. We also examined\nthe possibility of employing the minimal spanning tree (MST) method, which is\ncapable of reducing the number of links to N-1. We determined that market\nfactors carry important information value regarding information flow among\nstocks. Moreover, the information flow among stocks evidenced time-varying\nproperties according to the changes in market status. In particular, we noted\nthat the information flow increased dramatically during periods of market\ncrises. Finally, we confirmed, via the MST method, that the information flow\namong stocks could be assessed effectively with the reduced linkage\nrelationships among all links between stocks from the perspective of the\noverall market.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.2043v1"
    },
    {
        "title": "On the short-term influence of oil price changes on stock markets in GCC\n  countries: linear and nonlinear analyses",
        "authors": [
            "Mohamed El Hedi Arouri",
            "Julien Fouquau"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  This paper examines the short-run relationships between oil prices and GCC\nstock markets. Since GCC countries are major world energy market players, their\nstock markets may be susceptible to oil price shocks. To account for the fact\nthat stock markets may respond nonlinearly to oil price shocks, we have\nexamined both linear and nonlinear relationships. Our findings show that there\nare significant links between the two variables in Qatar, Oman, and UAE. Thus,\nstock markets in these countries react positively to oil price increases. For\nBahrain, Kuwait, and Saudi Arabia we found that oil price changes do not affect\nstock market returns.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3870v1"
    },
    {
        "title": "Stock market integration in the Latin American markets: further evidence\n  from nonlinear modeling",
        "authors": [
            "Fredj Jawadi",
            "Nicolas Million",
            "Mohamed El Hedi Arouri"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  This article studies the financial integration between the six main Latin\nAmerican markets and the US market in a nonlinear framework. Using the\nthreshold cointegration techniques of Hansen and Seo (2002), we show\nsignificant threshold stock market linkages between Mexico, Chile and the US.\nThus, the dynamics of these markets depends simultaneously on local and global\nrisk factors. More importantly, our results show an on-off threshold financial\nintegration process that is activated only when the stock price adjustment\nexceeds some level.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.3874v1"
    },
    {
        "title": "Analysis of a network structure of the foreign currency exchange market",
        "authors": [
            "Jaroslaw Kwapien",
            "Sylwia Gworek",
            "Stanislaw Drozdz",
            "Andrzej Gorski"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We analyze structure of the world foreign currency exchange (FX) market\nviewed as a network of interacting currencies. We analyze daily time series of\nFX data for a set of 63 currencies, including gold, silver and platinum. We\ngroup together all the exchange rates with a common base currency and study\neach group separately. By applying the methods of filtered correlation matrix\nwe identify clusters of closely related currencies. The clusters are formed\ntypically according to the economical and geographical factors. We also study\ntopology of weighted minimal spanning trees for different network\nrepresentations (i.e., for different base currencies) and find that in a\nmajority of representations the network has a hierarchical scale-free\nstructure. In addition, we analyze the temporal evolution of the network and\ndetect that its structure is not stable over time. A medium-term trend can be\nidentified which affects the USD node by decreasing its centrality. Our\nanalysis shows also an increasing role of euro in the world's currency market.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.0480v1"
    },
    {
        "title": "Temporal structure and gain/loss asymmetry for real and artificial stock\n  indices",
        "authors": [
            "Johannes Vitalis Siven",
            "Jeffrey Todd Lins"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We demonstrate that the gain/loss asymmetry observed for stock indices\nvanishes if the temporal dependence structure is destroyed by scrambling the\ntime series. We also show that an artificial index constructed by a simple\naverage of a number of individual stocks display gain/loss asymmetry - this\nallows us to explicitly analyze the dependence between the index constituents.\nWe consider mutual information and correlation based measures and show that the\nstock returns indeed have a higher degree of dependence in times of market\ndownturns than upturns.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.0554v1"
    },
    {
        "title": "New procedures for testing whether stock price processes are martingales",
        "authors": [
            "Kei Takeuchi",
            "Akimichi Takemura",
            "Masayuki Kumon"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We propose procedures for testing whether stock price processes are\nmartingales based on limit order type betting strategies. We first show that\nthe null hypothesis of martingale property of a stock price process can be\ntested based on the capital process of a betting strategy. In particular with\nhigh frequency Markov type strategies we find that martingale null hypotheses\nare rejected for many stock price processes.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.3273v2"
    },
    {
        "title": "World stock market: more sizeable trend reversal likely in\n  February/March 2010",
        "authors": [
            "Stanislaw Drozdz",
            "Pawel Oswiecimka"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Based on our \"finance-prediction-oriented\" methodology which involves such\nelements as log-periodic self-similarity, the universal preferred scaling\nfactor lambda=2, and allows a phenomenon of the \"super-bubble\" we analyze the\n2009 world stock market (here represented by the SP500, Hang Seng and WIG)\ndevelopment. We identify elements that indicate the third decade of September\n2009 as a time limit for the present bull market phase which is thus to be\nfollowed by a significant correction. In this context we also interpret the\nChinese stock market index SSE.\n  The third decade of September 2009 was accompanied with a stock market\ncorrection typically within the range of 4-5% worldwide. Taking into account\nthe market patterns that followed the time of delivering the previous scenario\nwe present an updated scenario whose critical time corresponds to October 28,\n2009.\n  Assuming quite evident (as of November 12, 2009) termination of the\ncorrection due to the above critical time we extend - consistently with our\nmethodology - the stock market forecasting scenario. The corresponding expected\nSP500 future trend is shown in Fig. 5 and it supports a potential average\ncontinuation of increases to as far into the future as the turn of\nFebruary/March 2010. We also indicate the log-periodic patterns on the gold\nmarket and they point to the end of November 2009 as the time when the trend\nreversal - likely local however - is expected to begin.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.0418v3"
    },
    {
        "title": "Bubble Diagnosis and Prediction of the 2005-2007 and 2008-2009 Chinese\n  stock market bubbles",
        "authors": [
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou",
            "Didier Sornette",
            "Ryan Woodard",
            "Ken Bastiaensen",
            "Peter Cauwels"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  By combining (i) the economic theory of rational expectation bubbles, (ii)\nbehavioral finance on imitation and herding of investors and traders and (iii)\nthe mathematical and statistical physics of bifurcations and phase transitions,\nthe log-periodic power law model has been developed as a flexible tool to\ndetect bubbles. The LPPL model considers the faster-than-exponential (power law\nwith finite-time singularity) increase in asset prices decorated by\naccelerating oscillations as the main diagnostic of bubbles. It embodies a\npositive feedback loop of higher return anticipations competing with negative\nfeedback spirals of crash expectations. We use the LPPL model in one of its\nincarnations to analyze two bubbles and subsequent market crashes in two\nimportant indexes in the Chinese stock markets between May 2005 and July 2009.\nBoth the Shanghai Stock Exchange Composite and Shenzhen Stock Exchange\nComponent indexes exhibited such behavior in two distinct time periods: 1) from\nmid-2005, bursting in Oct. 2007 and 2) from Nov. 2008, bursting in the\nbeginning of Aug. 2009. We successfully predicted time windows for both crashes\nin advance with the same methods used to successfully predict the peak in\nmid-2006 of the US housing bubble and the peak in July 2008 of the global oil\nbubble. The more recent bubble in the Chinese indexes was detected and its end\nor change of regime was predicted independently by two groups with similar\nresults, showing that the model has been well-documented and can be replicated\nby industrial practitioners. Here we present more detailed analysis of the\nindividual Chinese index predictions and of the methods used to make and test\nthem.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.1007v2"
    },
    {
        "title": "Financial bubbles analysis with a cross-sectional estimator",
        "authors": [
            "Frederic Abergel",
            "Nicolas Huth",
            "Ioane Muni Toke"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We highlight a very simple statistical tool for the analysis of financial\nbubbles, which has already been studied in [1]. We provide extensive empirical\ntests of this statistical tool and investigate analytically its link with\nstocks correlation structure.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.2885v1"
    },
    {
        "title": "Universal and nonuniversal allometric scaling behaviors in the\n  visibility graphs of world stock market indices",
        "authors": [
            "Meng-Cen Qian",
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  The investigations of financial markets from a complex network perspective\nhave unveiled many phenomenological properties, in which the majority of these\nstudies map the financial markets into one complex network. In this work, we\ninvestigate 30 world stock market indices through their visibility graphs by\nadopting the visibility algorithm to convert each single stock index into one\nvisibility graph. A universal allometric scaling law is uncovered in the\nminimal spanning trees, whose scaling exponent is independent of the stock\nmarket and the length of the stock index. In contrast, the maximal spanning\ntrees and the random spanning trees do not exhibit universal allometric scaling\nbehaviors. There are marked discrepancies in the allometric scaling behaviors\nbetween the stock indices and the Brownian motions. Using surrogate time\nseries, we find that these discrepancies are caused by the fat-tailedness of\nthe return distribution, the nonlinear long-term correlation, and a coupling\neffect between these two influence factors.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.2524v1"
    },
    {
        "title": "Complex Systems: From Nuclear Physics to Financial Markets",
        "authors": [
            "J. Speth",
            "S. Drozdz",
            "F. Gruemmer"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We compare correlations and coherent structures in nuclei and financial\nmarkets. In the nuclear physics part we review giant resonances which can be\ninterpreted as a coherent structure embedded in chaos. With similar methods we\ninvestigate the financial empirical correlation matrix of the DAX and Dow\nJones. We will show, that if the time-zone delay is properly accounted for, the\ntwo distinct markets largely merge into one. This is reflected by the largest\neigenvalue that develops a gap relative to the remaining, chaotic eigenvalues.\nBy extending investigations of the specific character of financial collectivity\nwe also discuss the criticality-analog phenomenon of the financial\nlog-periodicity and show specific examples.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.4348v1"
    },
    {
        "title": "The Financial Bubble Experiment: advanced diagnostics and forecasts of\n  bubble terminations",
        "authors": [
            "Didier Sornette",
            "Ryan Woodard",
            "Maxim Fedorovsky",
            "Stefan Reimann",
            "Hilary Woodard",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  On 2 November 2009, the Financial Bubble Experiment was launched within the\nFinancial Crisis Observatory (FCO) at ETH Zurich\n(\\url{http://www.er.ethz.ch/fco/}). In that initial report, we diagnosed and\nannounced three bubbles on three different assets. In this latest release of 23\nDecember 2009 in this ongoing experiment, we add a diagnostic of a new bubble\ndeveloping on a fourth asset.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.0454v4"
    },
    {
        "title": "Sign and amplitude representation of the forex networks",
        "authors": [
            "Sylwia Gworek",
            "Jaroslaw Kwapien",
            "Stanislaw Drozdz"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We decompose the exchange rates returns of 41 currencies (incl. gold) into\ntheir sign and amplitude components. Then we group together all exchange rates\nwith a common base currency, construct Minimal Spanning Trees for each group\nindependently, and analyze properties of these trees. We show that both the\nsign and the amplitude time series have similar correlation properties as far\nas the core network structure is concerned. There exist however interesting\nperipheral differences that may open a new perspective to view the Forex\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.3045v1"
    },
    {
        "title": "Statistical Regularities of Equity Market Activity",
        "authors": [
            "Fengzhong Wang",
            "Kazuko Yamasaki",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Equity activity is an essential topic for financial market studies. To\nexplore its statistical regularities, we comprehensively examine the trading\nvalue, a measure of the equity activity, of the 3314 most-traded stocks in the\nU.S. equity market and find that (i) the trading values follow a log-normal\ndistribution; (ii) the standard deviation of the growth rate of the trading\nvalue obeys a power-law with the initial trading value, and the power-law\nexponent beta=0.14. Remarkably, both features hold for a wide range of sampling\nintervals, from 5 minutes to 20 trading days. Further, we show that all the\n3314 stocks have long-term correlations, and their Hurst exponents H follow a\nnormal distribution. Furthermore, we find that the Hurst exponent depends on\nthe size of the company. We also show that the relation between the scaling in\nthe growth rate and the long-term correlation is consistent with beta=1-H,\nsimilar to that found recently on human interaction activity by Rybski and\ncollaborators.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4258v1"
    },
    {
        "title": "Gain/loss asymmetry in time series of individual stock prices and its\n  relationship to the leverage effect",
        "authors": [
            "Johannes Vitalis Siven",
            "Jeffrey Todd Lins"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Previous research has shown that for stock indices, the most likely time\nuntil a return of a particular size has been observed is longer for gains than\nfor losses. We establish that this so-called gain/loss asymmetry is present\nalso for individual stocks and show that the phenomenon is closely linked to\nthe well-known leverage effect -- in the EGARCH model and a modified retarded\nvolatility model, the same parameter that governs the magnitude of the leverage\neffect also governs the gain/loss asymmetry.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4679v2"
    },
    {
        "title": "Superfamily classification of nonstationary time series based on DFA\n  scaling exponents",
        "authors": [
            "Chuang Liu",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  The superfamily phenomenon of time series with different dynamics can be\ncharacterized by the motif rank patterns observed in the nearest-neighbor\nnetworks of the time series in phase space. However, the determinants of\nsuperfamily classification are unclear. We attack this problem by studying the\ninfluence of linear temporal correlations and multifractality using fractional\nBrownian motions (FBMs) and multifractal random walks (MRWs). Numerical\ninvestigations unveil that the classification of superfamily phenomenon is\nuniquely determined by the detrended fluctuation analysis (DFA) scaling\nexponent $\\alpha$ of the time series. Only four motif patterns are observed in\nthe simulated data, which are delimited by three DFA scaling exponents $\\alpha\n\\simeq 0.25$, $\\alpha \\simeq 0.35$ and $\\alpha \\simeq 0.45$. The validity of\nthe result is confirmed by stock market indexes and turbulence velocity\nsignals.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.2016v1"
    },
    {
        "title": "Finite-size effect and the components of multifractality in financial\n  volatility",
        "authors": [
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Many financial variables are found to exhibit multifractal nature, which is\nusually attributed to the influence of temporal correlations and fat-tailedness\nin the probability distribution (PDF). Based on the partition function approach\nof multifractal analysis, we show that there is a marked finite-size effect in\nthe detection of multifractality, and the effective multifractality is the\napparent multifractality after removing the finite-size effect. We find that\nthe effective multifractality can be further decomposed into two components,\nthe PDF component and the nonlinearity component. Referring to the normal\ndistribution, we can determine the PDF component by comparing the effective\nmultifractality of the original time series and the surrogate data that have a\nnormal distribution and keep the same linear and nonlinear correlations as the\noriginal data. We demonstrate our method by taking the daily volatility data of\nDow Jones Industrial Average from 26 May 1896 to 27 April 2007 as an example.\nExtensive numerical experiments show that a time series exhibits effective\nmultifractality only if it possesses nonlinearity and the PDF has impact on the\neffective multifractality only when the time series possesses nonlinearity. Our\nmethod can also be applied to judge the presence of multifractality and\ndetermine its components of multifractal time series in other complex systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.4782v1"
    },
    {
        "title": "Universal patterns of inequality",
        "authors": [
            "Anand Banerjee",
            "Victor M. Yakovenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Probability distributions of money, income, and energy consumption per capita\nare studied for ensembles of economic agents. The principle of entropy\nmaximization for partitioning of a limited resource gives exponential\ndistributions for the investigated variables. A non-equilibrium difference of\nmoney temperatures between different systems generates net fluxes of money and\npopulation. To describe income distribution, a stochastic process with additive\nand multiplicative components is introduced. The resultant distribution\ninterpolates between exponential at the low end and power law at the high end,\nin agreement with the empirical data for USA. We show that the increase of\nincome inequality in USA originates primarily from the increase of the income\nfraction going to the upper tail, which now exceeds 20% of the total income.\nAnalyzing the data from the World Resources Institute, we find that the\ndistribution of energy consumption per capita around the world can be\napproximately described by the exponential function. Comparing the data for\n1990, 2000, and 2005, we discuss the effect of globalization on the inequality\nof energy consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.4898v4"
    },
    {
        "title": "Universal Behavior of Extreme Price Movements in Stock Markets",
        "authors": [
            "Miguel A. Fuentes",
            "Austin Gerig",
            "Javier Vicente"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Many studies assume stock prices follow a random process known as geometric\nBrownian motion. Although approximately correct, this model fails to explain\nthe frequent occurrence of extreme price movements, such as stock market\ncrashes. Using a large collection of data from three different stock markets,\nwe present evidence that a modification to the random model -- adding a slow,\nbut significant, fluctuation to the standard deviation of the process --\naccurately explains the probability of different-sized price changes, including\nthe relative high frequency of extreme movements. Furthermore, we show that\nthis process is similar across stocks so that their price fluctuations can be\ncharacterized by a single curve. Because the behavior of price fluctuations is\nrooted in the characteristics of volatility, we expect our results to bring\nincreased interest to stochastic volatility models, and especially to those\nthat can produce the properties of volatility reported here.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.5448v1"
    },
    {
        "title": "Utilisation des méthodes de Lee-Carter et Log-Poisson pour\n  l'ajustement de tables de mortalité dans le cas de petits échantillons",
        "authors": [
            "Frédéric Planchet",
            "Vincent Lelieur"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  The aim of this paper is to study the construction of prospective mortality\ntables from a low number of persons subjected to risk. The presented models are\nthe Lee-Carter and log-Poisson methods respectively. The low number of people\nsubjected to risk, particularly noticed for the persons who are getting on,\nimplies the use of an extrapolation method for the mortality rates. The\nLee-Carter and log-Poisson methods constitute twodimensional models, taking the\nyear and the age into account to calculate the mortality rates. The methods\nsuggested are applied to a real data set. The prospective tables, built in this\nway, allow to project the rates' evolution in the future, extrapolating the\ntemporal constituent. And then, it allows to compare this projection with the\nevolution predicted for the French population in its entirety. You determine\nthe best method through the nearness of the smoothed rates in comparison with\nthe raw rates and essentially through the caution of these models for the life\nannuities' calculation. The results stemed from these methods are too\nconfronted with the mortality rates obtained through a method of logistic fits.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1916v1"
    },
    {
        "title": "Analyzing the prices of the most expensive sheet iron all over the\n  world: Modeling, prediction and regime change",
        "authors": [
            "Fu-Tie Song",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  The private car license plates issued in Shanghai are bestowed the title of\n\"the most expensive sheet iron all over the world\", more expensive than gold. A\ncitizen has to bid in an monthly auction to obtain a license plate for his new\nprivate car. We perform statistical analysis to investigate the influence of\nthe minimal price $P_{\\min}$ of the bidding winners, the quota $N_{\\rm{quota}}$\nof private car license plates, the number $N_{\\rm{bidder}}$ of bidders, as well\nas two external shocks including the legality debate of the auction in 2004 and\nthe auction regime reform in January 2008 on the average price $P_{\\rm{mean}}$\nof all bidding winners. It is found that the legality debate of the auction had\nmarginal transient impact on the average price in a short time period. In\ncontrast, the change of the auction rules has significant permanent influence\non the average price, which reduces the price by about 3020 yuan Renminbi. It\nmeans that the average price exhibits nonlinear behaviors with a regime change.\nThe evolution of the average price is independent of the number\n$N_{\\rm{bidder}}$ of bidders in both regimes. In the early regime before\nJanuary 2008, the average price $P_{\\rm{mean}}$ was influenced only by the\nminimal price $P_{\\min}$ in the preceding month with a positive correlation. In\nthe current regime since January 2008, the average price is positively\ncorrelated with the minimal price and the quota in the preceding month and\nnegatively correlated with the quota in the same month. We test the predictive\npower of the two models using 2-year and 3-year moving windows and find that\nthe latter outperforms the former. It seems that the auction market becomes\nmore efficient after the auction reform since the prediction error increases.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3176v1"
    },
    {
        "title": "Cross-Correlation Dynamics in Financial Time Series",
        "authors": [
            "Thomas Conlon",
            "Heather J. Ruskin",
            "Martin Crane"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  The dynamics of the equal-time cross-correlation matrix of multivariate\nfinancial time series is explored by examination of the eigenvalue spectrum\nover sliding time windows. Empirical results for the S&P 500 and the Dow Jones\nEuro Stoxx 50 indices reveal that the dynamics of the small eigenvalues of the\ncross-correlation matrix, over these time windows, oppose those of the largest\neigenvalue. This behaviour is shown to be independent of the size of the time\nwindow and the number of stocks examined.\n  A basic one-factor model is then proposed, which captures the main dynamical\nfeatures of the eigenvalue spectrum of the empirical data. Through the addition\nof perturbations to the one-factor model, (leading to a 'market plus sectors'\nmodel), additional sectoral features are added, resulting in an Inverse\nParticipation Ratio comparable to that found for empirical data. By\npartitioning the eigenvalue time series, we then show that negative index\nreturns, (drawdowns), are associated with periods where the largest eigenvalue\nis greatest, while positive index returns, (drawups), are associated with\nperiods where the largest eigenvalue is smallest. The study of correlation\ndynamics provides some insight on the collective behaviour of traders with\nvarying strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.0321v1"
    },
    {
        "title": "A Random Matrix Approach to VARMA Processes",
        "authors": [
            "Zdzisław Burda",
            "Andrzej Jarosz",
            "Maciej A. Nowak",
            "Małgorzata Snarska"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We apply random matrix theory to derive spectral density of large sample\ncovariance matrices generated by multivariate VMA(q), VAR(q) and VARMA(q1,q2)\nprocesses. In particular, we consider a limit where the number of random\nvariables N and the number of consecutive time measurements T are large but the\nratio N/T is fixed. In this regime the underlying random matrices are\nasymptotically equivalent to Free Random Variables (FRV). We apply the FRV\ncalculus to calculate the eigenvalue density of the sample covariance for\nseveral VARMA-type processes. We explicitly solve the VARMA(1,1) case and\ndemonstrate a perfect agreement between the analytical result and the spectra\nobtained by Monte Carlo simulations. The proposed method is purely algebraic\nand can be easily generalized to q1>1 and q2>1.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.0934v1"
    },
    {
        "title": "Testing for financial crashes using the Log Periodic Power Law mode",
        "authors": [
            "David S. Bree",
            "Nathan Lael Joseph"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  A number of papers claim that a Log Periodic Power Law (LPPL) fitted to\nfinancial market bubbles that precede large market falls or 'crashes', contain\nparameters that are confined within certain ranges. The mechanism that has been\nclaimed as underlying the LPPL, is based on influence percolation and a\nmartingale condition. This paper examines these claims and the robustness of\nthe LPPL for capturing large falls in the Hang Seng stock market index, over a\n30-year period, including the current global downturn. We identify 11 crashes\non the Hang Seng market over the period 1970 to 2008. The fitted LPPLs have\nparameter values within the ranges specified post hoc by Johansen and Sornette\n(2001) for only seven of these crashes. Interestingly, the LPPL fit could have\npredicted the substantial fall in the Hang Seng index during the recent global\ndownturn. We also find that influence percolation combined with a martingale\ncondition holds for only half of the pre-crash bubbles previously reported.\nOverall, the mechanism posited as underlying the LPPL does not do so, and the\ndata used to support the fit of the LPPL to bubbles does so only partially.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1010v2"
    },
    {
        "title": "Complex stock trading network among investors",
        "authors": [
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We provide an empirical investigation aimed at uncovering the statistical\nproperties of intricate stock trading networks based on the order flow data of\na highly liquid stock (Shenzhen Development Bank) listed on Shenzhen Stock\nExchange during the whole year of 2003. By reconstructing the limit order book,\nwe can extract detailed information of each executed order for each trading day\nand demonstrate that the trade size distributions for different trading days\nexhibit power-law tails and that most of the estimated power-law exponents are\nwell within the L{\\'e}vy stable regime. Based on the records of order matching\namong investors, we can construct a stock trading network for each trading day,\nin which the investors are mapped into nodes and each transaction is translated\nas a direct edge from the seller to the buyer with the trade size as its\nweight. We find that all the trading networks comprise a giant component and\nhave power-law degree distributions and disassortative architectures. In\nparticular, the degrees are correlated with order sizes by a power-law\nfunction. By regarding the size executed order as its fitness, the fitness\nmodel can reproduce the empirical power-law degree distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.2459v2"
    },
    {
        "title": "Modeling share prices of banks and bankrupts",
        "authors": [
            "Ivan O. Kitov"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Share prices of financial companies from the S&P 500 list have been modeled\nby a linear function of consumer price indices in the USA. The Johansen and\nEngle-Granger tests for cointegration both demonstrated the presence of an\nequilibrium long-term relation between observed and predicted time series.\nEconometrically, the pricing concept is valid. For several companies, share\nprices are defined only by CPI readings in the past. Therefore, our empirical\npricing model is a deterministic one. For a few companies, including Lehman\nBrothers, AIG, Freddie Mac and Fannie Mae, negative share prices could be\nforeseen in May-September 2008. One might interpret the negative share prices\nas a sign of approaching bankruptcies.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.2692v1"
    },
    {
        "title": "Nonlinear Stochastic Model of Return matching to the data of New York\n  and Vilnius Stock Exchanges",
        "authors": [
            "V. Gontis",
            "A. Kononovicius"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We scale and analyze the empirical data of return from New York and Vilnius\nstock exchanges matching it to the same nonlinear double stochastic model of\nreturn in financial market.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.5356v1"
    },
    {
        "title": "Nonuniversal distributions of stock returns in an emerging market",
        "authors": [
            "Guo-Hua Mu",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  There is convincing evidence showing that the probability distributions of\nstock returns in mature markets exhibit power-law tails and both the positive\nand negative tails conform to the inverse cubic law. It supports the\npossibility that the tail exponents are universal at least for mature markets\nin the sense that they do not depend on stock market, industry sector, and\nmarket capitalization. We investigate the distributions of one-minute intraday\nreturns of all the A-share stocks traded in the Chinese stock market, which is\nthe largest emerging market in the world. We find that the returns can be well\nfitted by the $q$-Gaussian distribution and the tails have power-law\nrelaxations with the exponents fluctuating around $\\alpha=3$ and being well\noutside the L\\'evy stable regime for individual stocks. We provide\nstatistically significant evidence showing that the exponents logarithmically\ndecrease with the turnover rate and increase with the market capitalization,\nand find that the market capitalization has a greater impact on the tail\nexponent than the turnover rate. Our findings indicate that the intraday return\ndistributions are not universal in emerging stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.5984v1"
    },
    {
        "title": "S&P 500 returns revisited",
        "authors": [
            "Ivan O. Kitov",
            "Oleg I. Kitov"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  The predictions of the S&P 500 returns made in 2007 have been tested and the\nunderlying models amended. The period between 2003 and 2008 should be described\nby the dependence of the S&P 500 stock market index on real GDP because the\npopulation pyramid was highly inaccurate. The 2008 trough and 2009 rally are\nwell predicted by the original model, however. The rally will end in\nMarch/April 2010 and the S&P 500 level will be decreasing into 2011. This\nprediction should validate the model.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.0213v1"
    },
    {
        "title": "Universality in DAX index returns fluctuations",
        "authors": [
            "Rui Gonçalves",
            "Helena Ferreira",
            "Alberto Pinto"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  In terms of the stock exchange returns, we compute the analytic expression of\nthe probability distributions F{DAX,+} and F{DAX,-} of the normalized positive\nand negative DAX (Germany) index daily returns r(t). Furthermore, we define the\nalpha re-scaled DAX daily index positive returns r(t)^alpha and negative\nreturns (-r(t))^alpha that we call, after normalization, the alpha positive\nfluctuations and alpha negative fluctuations. We use the Kolmogorov-Smirnov\nstatistical test, as a method, to find the values of alpha that optimize the\ndata collapse of the histogram of the alpha fluctuations with the\nBramwell-Holdsworth-Pinton (BHP) probability density function. The optimal\nparameters that we found are alpha+=0.50 and alpha-=0.48. Since the BHP\nprobability density function appears in several other dissimilar phenomena, our\nresults reveal universality in the stock exchange markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1136v2"
    },
    {
        "title": "Universal Fluctuations of the FTSE100",
        "authors": [
            "Rui Gonçalves",
            "Helena Ferreira",
            "Alberto Pinto"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We compute the analytic expression of the probability distributions\nF{FTSE100,+} and F{FTSE100,-} of the normalized positive and negative FTSE100\n(UK) index daily returns r(t). Furthermore, we define the alpha re-scaled\nFTSE100 daily index positive returns r(t)^alpha and negative returns\n(-r(t))^alpha that we call, after normalization, the alpha positive\nfluctuations and alpha negative fluctuations. We use the Kolmogorov-Smirnov\nstatistical test, as a method, to find the values of alpha that optimize the\ndata collapse of the histogram of the alpha fluctuations with the\nBramwell-Holdsworth-Pinton (BHP) probability density function. The optimal\nparameters that we found are alpha+=0.55 and alpha-=0.55. Since the BHP\nprobability density function appears in several other dissimilar phenomena, our\nresults reveal universality in the stock exchange markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1138v2"
    },
    {
        "title": "Universal Fluctuations of AEX index",
        "authors": [
            "Rui Gonçalves",
            "Helena Ferreira",
            "Alberto Pinto"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We compute the analytic expression of the probability distributions F{AEX,+}\nand F{AEX,-} of the normalized positive and negative AEX (Netherlands) index\ndaily returns r(t). Furthermore, we define the \\alpha re-scaled AEX daily index\npositive returns r(t)^\\alpha and negative returns (-r(t))^\\alpha that we call,\nafter normalization, the \\alpha positive fluctuations and \\alpha negative\nfluctuations. We use the Kolmogorov-Smirnov statistical test, as a method, to\nfind the values of \\alpha that optimize the data collapse of the histogram of\nthe \\alpha fluctuations with the Bramwell-Holdsworth-Pinton (BHP) probability\ndensity function. The optimal parameters that we found are \\alpha+=0.46 and\n\\alpha-=0.43. Since the BHP probability density function appears in several\nother dissimilar phenomena, our results reveal universality in the stock\nexchange markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1210v2"
    },
    {
        "title": "Memory effect and multifractality of cross-correlations in financial\n  markets",
        "authors": [
            "Tian Qiu",
            "Guang Chen",
            "Li-Xin Zhong",
            "Xiao-Wei Lei"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  An average instantaneous cross-correlation function is introduced to quantify\nthe interaction of the financial market of a specific time. Based on the daily\ndata of the American and Chinese stock markets, memory effect of the average\ninstantaneous cross-correlations is investigated over different price return\ntime intervals. Long-range time-correlations are revealed, and are found to\npersist up to a month-order magnitude of the price return time interval.\nMultifractal nature is investigated by a multifractal detrended fluctuation\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.5547v1"
    },
    {
        "title": "The Financial Bubble Experiment: Advanced Diagnostics and Forecasts of\n  Bubble Terminations Volume II-Master Document",
        "authors": [
            "Didier Sornette",
            "Ryan Woodard",
            "Maxim Fedorovsky",
            "Stefan Reimann",
            "Hilary Woodard",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  This is the second installment of the Financial Bubble Experiment. Here we\nprovide the digital fingerprint of an electronic document in which we identify\n7 bubbles in 7 different global assets; for 4 of these assets, we present\nwindows of dates of the most likely ending time of each bubble. We will provide\nthat document of the original analysis on 1 November 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.5675v2"
    },
    {
        "title": "Statistical mechanics approach to the probability distribution of money",
        "authors": [
            "Victor M. Yakovenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  This Chapter reviews statistical models for the probability distribution of\nmoney developed in the econophysics literature since the late 1990s. In these\nmodels, economic transactions are modeled as random transfers of money between\nthe agents in payment for goods and services. Starting from the initially equal\ndistribution of money, the system spontaneously develops a highly unequal\ndistribution of money analogous to the Boltzmann-Gibbs distribution of energy\nin physics. Boundary conditions are crucial for achieving a stationary\ndistribution. When debt is permitted, it destabilizes the system, unless some\nsort of limit is imposed on maximal debt.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.5074v1"
    },
    {
        "title": "Volatilities That Change with Time: The Temporal Behavior of the\n  Distribution of Stock-Market Prices",
        "authors": [
            "Achilles D. Speliotopoulos"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  While the use of volatilities is pervasive throughout finance, our ability to\ndetermine the instantaneous volatility of stocks is nascent. Here, we present a\nmethod for measuring the temporal behavior of stocks, and show that stock\nprices for 24 DJIA stocks follow a stochastic process that describes an\nefficiently priced stock while using a volatility that changes\ndeterministically with time. We find that the often observed, abnormally large\nkurtoses are due to temporal variations in the volatility. Our method can\nresolve changes in volatility and drift of the stocks as fast as a single day\nusing daily close prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.5274v1"
    },
    {
        "title": "Optimization of Financial Instrument Parcels in Stochastic Wavelet Model",
        "authors": [
            "A. M. Avdeenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  To define oscillatory movements of securities market, we put in the non-local\nextension of Ito- equation for wavelet-images of random processes. It is\nproposed an algorithm of creation of evolutionary equation and a model of\nprediction of the most probable price movement path. It is carried out\nexperimental validation of findings.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.5413v1"
    },
    {
        "title": "Long-term correlations and multifractal nature in the intertrade\n  durations of a liquid Chinese stock and its warrant",
        "authors": [
            "Yong-Ping Ruan",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Intertrade duration of equities is an important financial measure\ncharacterizing the trading activities, which is defined as the waiting time\nbetween successive trades of an equity. Using the ultrahigh-frequency data of a\nliquid Chinese stock and its associated warrant, we perform a comparative\ninvestigation of the statistical properties of their intertrade duration time\nseries. The distributions of the two equities can be better described by the\nshifted power-law form than the Weibull and their scaled distributions do not\ncollapse onto a single curve. Although the intertrade durations of the two\nequities have very different magnitude, their intraday patterns exhibit very\nsimilar shapes. Both detrended fluctuation analysis (DFA) and detrending moving\naverage analysis (DMA) show that the 1-min intertrade duration time series of\nthe two equities are strongly correlated. In addition, both multifractal\ndetrended fluctuation analysis (MFDFA) and multifractal detrending moving\naverage analysis (MFDMA) unveil that the 1-min intertrade durations possess\nmultifractal nature. However, the difference between the two singularity\nspectra of the two equities obtained from the MFDMA is much smaller than that\nfrom the MFDFA.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.0160v1"
    },
    {
        "title": "Statistical mechanics of money, debt, and energy consumption",
        "authors": [
            "Victor M. Yakovenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We briefly review statistical models for the probability distribution of\nmoney developed in the econophysics literature since the late 1990s. In these\nmodels, economic transactions are modeled as random transfers of money between\nthe agents in payment for goods and services. We focus on conceptual\nfoundations for this approach, on the issues of money conservation and debt,\nand present new results for the energy consumption distribution around the\nworld.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.2179v1"
    },
    {
        "title": "Maximum penalized quasi-likelihood estimation of the diffusion function",
        "authors": [
            "Jeff Hamrick",
            "Yifei Huang",
            "Constantinos Kardaras",
            "Murad Taqqu"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We develop a maximum penalized quasi-likelihood estimator for estimating in a\nnonparametric way the diffusion function of a diffusion process, as an\nalternative to more traditional kernel-based estimators. After developing a\nnumerical scheme for computing the maximizer of the penalized maximum\nquasi-likelihood function, we study the asymptotic properties of our estimator\nby way of simulation. Under the assumption that overnight London Interbank\nOffered Rates (LIBOR); the USD/EUR, USD/GBP, JPY/USD, and EUR/USD nominal\nexchange rates; and 1-month, 3-month, and 30-year Treasury bond yields are\ngenerated by diffusion processes, we use our numerical scheme to estimate the\ndiffusion function.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.2421v2"
    },
    {
        "title": "The joint distribution of stock returns is not elliptical",
        "authors": [
            "Rémy Chicheportiche",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Using a large set of daily US and Japanese stock returns, we test in detail\nthe relevance of Student models, and of more general elliptical models, for\ndescribing the joint distribution of returns. We find that while Student\ncopulas provide a good approximation for strongly correlated pairs of stocks,\nsystematic discrepancies appear as the linear correlation between stocks\ndecreases, that rule out all elliptical models. Intuitively, the failure of\nelliptical models can be traced to the inadequacy of the assumption of a single\nvolatility mode for all stocks. We suggest several ideas of methodological\ninterest to efficiently visualise and compare different copulas. We identify\nthe rescaled difference with the Gaussian copula and the central value of the\ncopula as strongly discriminating observables. We insist on the need to shun\naway from formal choices of copulas with no financial interpretation.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1100v3"
    },
    {
        "title": "Tick size and price diffusion",
        "authors": [
            "Gabriele La Spada",
            "J. Doyne Farmer",
            "Fabrizio Lillo"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  A tick size is the smallest increment of a security price. It is clear that\nat the shortest time scale on which individual orders are placed the tick size\nhas a major role which affects where limit orders can be placed, the bid-ask\nspread, etc. This is the realm of market microstructure and there is a vast\nliterature on the role of tick size on market microstructure. However, tick\nsize can also affect price properties at longer time scales, and relatively\nless is known about the effect of tick size on the statistical properties of\nprices. The present paper is divided in two parts. In the first we review the\neffect of tick size change on the market microstructure and the diffusion\nproperties of prices. The second part presents original results obtained by\ninvestigating the tick size changes occurring at the New York Stock Exchange\n(NYSE). We show that tick size change has three effects on price diffusion.\nFirst, as already shown in the literature, tick size affects price return\ndistribution at an aggregate time scale. Second, reducing the tick size\ntypically leads to an increase of volatility clustering. We give a possible\nmechanistic explanation for this effect, but clearly more investigation is\nneeded to understand the origin of this relation. Third, we explicitly show\nthat the ability of the subordination hypothesis in explaining fat tails of\nreturns and volatility clustering is strongly dependent on tick size. While for\nlarge tick sizes the subordination hypothesis has significant explanatory\npower, for small tick sizes we show that subordination is not the main driver\nof these two important stylized facts of financial market.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.2329v2"
    },
    {
        "title": "Individual and collective stock dynamics: intra-day seasonalities",
        "authors": [
            "Romain Allez",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We establish several new stylised facts concerning the intra-day\nseasonalities of stock dynamics. Beyond the well known U-shaped pattern of the\nvolatility, we find that the average correlation between stocks increases\nthroughout the day, leading to a smaller relative dispersion between stocks.\nSomewhat paradoxically, the kurtosis (a measure of volatility surprises)\nreaches a minimum at the open of the market, when the volatility is at its\npeak. We confirm that the dispersion kurtosis is a markedly decreasing function\nof the index return. This means that during large market swings, the\nidiosyncratic component of the stock dynamics becomes sub-dominant. In a\nnutshell, early hours of trading are dominated by idiosyncratic or sector\nspecific effects with little surprises, whereas the influence of the market\nfactor increases throughout the day, and surprises become more frequent.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.4785v1"
    },
    {
        "title": "Financial LPPL Bubbles with Mean-Reverting Noise in the Frequency Domain",
        "authors": [
            "Vincenzo Liberatore"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  The log-periodic power law (LPPL) is a model of asset prices during\nendogenous bubbles. A major open issue is to verify the presence of LPPL in\nprice sequences and to estimate the LPPL parameters. Estimation is complicated\nby the fact that daily LPPL returns are typically orders of magnitude smaller\nthan measured price returns, suggesting that noise obscures the underlying LPPL\ndynamics. However, if noise is mean-reverting, it would quickly cancel out over\nsubsequent measurements. In this paper, we attempt to reject mean-reverting\nnoise from price sequences by exploiting frequency-domain properties of LPPL\nand of mean reversion. First, we calculate the spectrum of mean-reverting \\ou\nnoise and devise estimators for the noise's parameters. Then, we derive the\nLPPL spectrum by breaking it down into its two main characteristics of power\nlaw and of log-periodicity. We compare price spectra with noise spectra during\nhistorical bubbles. In general, noise was strong also at low frequencies and,\neven if LPPL underlied price dynamics, LPPL would be obscured by noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.4835v2"
    },
    {
        "title": "Statistical Properties of Cross-Correlation in the Korean Stock Market",
        "authors": [
            "Gabjin Oh",
            "Cheoljun Eom",
            "Fengzhong Wang",
            "Woo-Sung Jung",
            "H. Eugene Stanley",
            "Seunghwan Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We investigate the statistical properties of the correlation matrix between\nindividual stocks traded in the Korean stock market using the random matrix\ntheory (RMT) and observe how these affect the portfolio weights in the\nMarkowitz portfolio theory. We find that the distribution of the correlation\nmatrix is positively skewed and changes over time. We find that the eigenvalue\ndistribution of original correlation matrix deviates from the eigenvalues\npredicted by the RMT, and the largest eigenvalue is 52 times larger than the\nmaximum value among the eigenvalues predicted by the RMT. The $\\beta_{473}$\ncoefficient, which reflect the largest eigenvalue property, is 0.8, while one\nof the eigenvalues in the RMT is approximately zero. Notably, we show that the\nentropy function $E(\\sigma)$ with the portfolio risk $\\sigma$ for the original\nand filtered correlation matrices are consistent with a power-law function,\n$E(\\sigma) \\sim \\sigma^{-\\gamma}$, with the exponent $\\gamma \\sim 2.92$ and\nthose for Asian currency crisis decreases significantly.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.2048v1"
    },
    {
        "title": "Market panic on different time-scales",
        "authors": [
            "Lisa Borland",
            "Yoan Hassid"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Cross-sectional signatures of market panic were recently discussed on daily\ntime scales in [1], extended here to a study of cross-sectional properties of\nstocks on intra-day time scales. We confirm specific intra-day patterns of\ndispersion and kurtosis, and find that the correlation across stocks increases\nin times of panic yielding a bimodal distribution for the sum of signs of\nreturns. We also find that there is memory in correlations, decaying as a power\nlaw with exponent 0.05. During the Flash-Crash of May 6 2010, we find a drastic\nincrease in dispersion in conjunction with increased correlations. However, the\nkurtosis decreases only slightly in contrast to findings on daily time-scales\nwhere kurtosis drops drastically in times of panic. Our study indicates that\nthis difference in behavior is result of the origin of the panic-inducing\nvolatility shock: the more correlated across stocks the shock is, the more the\nkurtosis will decrease; the more idiosyncratic the shock, the lesser this\neffect and kurtosis is positively correlated with dispersion. We also find that\nthere is a leverage effect for correlations: negative returns tend to precede\nan increase in correlations. A stock price feed-back model with skew in\nconjunction with a correlation dynamics that follows market volatility explains\nour observations nicely.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4917v1"
    },
    {
        "title": "Statistical properties of derivatives: a journey in term structures",
        "authors": [
            "Delphine Lautier",
            "Franck Raynaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  This article presents an empirical study of thirteen derivative markets for\ncommodity and financial assets. It compares the statistical properties of\nfutures contracts's daily returns at different maturities, from 1998 to 2010\nand for delivery dates up to 120 months. The analysis of the fourth first\nmoments of the distribution shows that the mean and variance of the commodities\nfollow a scaling behavior in the maturity dimension. The comparison of the\ntails of the probability distribution according to the expiration dates also\nshows that there is a segmentation in the fat tails exponent term structure\nabove the L'evy stable region. Finally, the test of the robustness of the\ninverse cubic law in the maturity dimension shows that there are two regimes of\nextreme events for derivative markets, reminding of a phase diagram with a\ntransition value at the 18th delivery month.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.6026v1"
    },
    {
        "title": "The Financial Bubble Experiment: Advanced Diagnostics and Forecasts of\n  Bubble Terminations, Volume III",
        "authors": [
            "Ryan Woodard",
            "Didier Sornette",
            "Maxim Fedorovsky"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  This is the third installment of the Financial Bubble Experiment. Here we\nprovide the digital fingerprint of an electronic document in which we identify\n27 bubbles in 27 different global assets; for 25 of these assets, we present\nwindows of dates of the most likely ending time of each bubble. We will provide\nthat document of the original analysis on 2 May 2011.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.2882v2"
    },
    {
        "title": "Principal Regression Analysis and the index leverage effect",
        "authors": [
            "Pierre-Alain Reigneron",
            "Romain Allez",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We revisit the index leverage effect, that can be decomposed into a\nvolatility effect and a correlation effect. We investigate the latter using a\nmatrix regression analysis, that we call `Principal Regression Analysis' (PRA)\nand for which we provide some analytical (using Random Matrix Theory) and\nnumerical benchmarks. We find that downward index trends increase the average\ncorrelation between stocks (as measured by the most negative eigenvalue of the\nconditional correlation matrix), and makes the market mode more uniform. Upward\ntrends, on the other hand, also increase the average correlation between stocks\nbut rotates the corresponding market mode {\\it away} from uniformity. There are\ntwo time scales associated to these effects, a short one on the order of a\nmonth (20 trading days), and a longer time scale on the order of a year. We\nalso find indications of a leverage effect for sectorial correlations as well,\nwhich reveals itself in the second and third mode of the PRA.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.5810v2"
    },
    {
        "title": "Minimal model of financial stylized facts",
        "authors": [
            "Danilo Delpini",
            "Giacomo Bormetti"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  In this work we afford the statistical characterization of a linear\nStochastic Volatility Model featuring Inverse Gamma stationary distribution for\nthe instantaneous volatility. We detail the derivation of the moments of the\nreturn distribution, revealing the role of the Inverse Gamma law in the\nemergence of fat tails, and of the relevant correlation functions. We also\npropose a systematic methodology for estimating the parameters, and we describe\nthe empirical analysis of the Standard & Poor 500 index daily returns,\nconfirming the ability of the model to capture many of the established stylized\nfact as well as the scaling properties of empirical distributions over\ndifferent time horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.5983v2"
    },
    {
        "title": "Testing the Capital Asset Pricing Model (CAPM) on the Uganda Stock\n  Exchange",
        "authors": [
            "David Wakyiku"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  This paper examines the validity of the Capital Asset Pricing Model (CAPM) on\nthe Ugandan stock market using monthly stock returns from 10 of the 11\ncompanies listed on the Uganda Stock Exchange (USE), for the period 1st March\n2007 to 10th November 2009. Due to the absence of readily available Uganda\nStock Exchange(USE) data, and the placement of daily price lists in pdf only,\non the USE website: http://www.use.or.ug, the article also discusses the\nprocedures taken to mine the data needed. The securities were all put in one\nportfolio in order to diversify away the firm-specific part of returns thereby\nenhancing the precision of the beta estimates. This paper should be of interest\nto both Ugandan and non-Ugandan investors and market researchers. While many\ndeveloping countries have legal restrictions against foreign participation in\ncapital and money markets, this is not so in Uganda, where it has become part\nof government policy to encourage foreign capital in flow, inorder to stimulate\nthe development of the small and underdeveloped markets.\n  The Black, Jensen, and Scholes (1972) CAPM version is examined in this\narticle. This version predicts a non zero-beta rate, along with the relation of\nhigher returns to higher risk. The estimated zero-beta rate obtained is not\nstatistically different from zero, and the estimated portfolio beta coefficient\nis statistically significant, providing evidence that the traditional form of\nCAPM holds on the USE, albeit having a beta coefficient that is not good at\nexplaining the relationship between risk and return.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0184v1"
    },
    {
        "title": "Globalization and long-run co-movements in the stock market for the G7:\n  an application of VECM under structural breaks",
        "authors": [
            "Rui Menezes",
            "Andreia Dioniso"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  This paper analyzes the process of long-run co-movements and stock market\nglobalization on the basis of cointegration tests and vector error correction\n(VEC) models. The cointegration tests used here allow for structural breaks to\nbe explicitly modeled and breakpoints to be computed on a relative-time basis.\nThe data used in our empirical analysis were drawn from Datastream and comprise\nthe natural logarithms of relative stock market indexes since 1973 for the G7\ncountries. The main results point to the conclusion that significant causal\ncointegration effects occur in this context and that there is a long-run\nequilibrium relationship that governs the worldwide process of market\nintegration. Globalization, however, is a complex adjustment process and in\nmany cases there is only evidence of weak market integration which means that\nnon-proportional price transmission occurs in the market along with\nproportional changes. The worldwide markets, as expected, appear to be driven\nin general by the US stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.4093v1"
    },
    {
        "title": "A Copula Approach on the Dynamics of Statistical Dependencies in the US\n  Stock Market",
        "authors": [
            "Michael C. Münnix",
            "Rudi Schäfer"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We analyze the statistical dependency structure of the S&P 500 constituents\nin the 4-year period from 2007 to 2010 using intraday data from the New York\nStock Exchange's TAQ database. With a copula-based approach, we find that the\nstatistical dependencies are very strong in the tails of the marginal\ndistributions. This tail dependence is higher than in a bivariate Gaussian\ndistribution, which is implied in the calculation of many correlation\ncoefficients. We compare the tail dependence to the market's average\ncorrelation level as a commonly used quantity and disclose an nearly linear\nrelation.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.1099v2"
    },
    {
        "title": "Correlation of financial markets in times of crisis",
        "authors": [
            "Leonidas Sandoval Junior",
            "Italo De Paula Franca"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Using the eigenvalues and eigenvectors of correlations matrices of some of\nthe main financial market indices in the world, we show that high volatility of\nmarkets is directly linked with strong correlations between them. This means\nthat markets tend to behave as one during great crashes. In order to do so, we\ninvestigate several financial market crises that occurred in the years 1987\n(Black Monday), 1989 (Russian crisis), 2001 (Burst of the dot-com bubble and\nSeptember 11), and 2008 (Subprime Mortgage Crisis), which mark some of the\nlargest downturns of financial markets in the last three decades.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.1339v2"
    },
    {
        "title": "The fine structure of spectral properties for random correlation\n  matrices: an application to financial markets",
        "authors": [
            "G. Livan",
            "S. Alfarano",
            "E. Scalas"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We study some properties of eigenvalue spectra of financial correlation\nmatrices. In particular, we investigate the nature of the large eigenvalue\nbulks which are observed empirically, and which have often been regarded as a\nconsequence of the supposedly large amount of noise contained in financial\ndata. We challenge this common knowledge by acting on the empirical correlation\nmatrices of two data sets with a filtering procedure which highlights some of\nthe cluster structure they contain, and we analyze the consequences of such\nfiltering on eigenvalue spectra. We show that empirically observed eigenvalue\nbulks emerge as superpositions of smaller structures, which in turn emerge as a\nconsequence of cross-correlations between stocks. We interpret and corroborate\nthese findings in terms of factor models, and and we compare empirical spectra\nto those predicted by Random Matrix Theory for such models.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.4076v1"
    },
    {
        "title": "Modeling Long Memory in REITs",
        "authors": [
            "John Cotter",
            "Simon Stevenson"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  One stylized feature of financial volatility impacting the modeling process\nis long memory. This paper examines long memory for alternative risk measures,\nobserved absolute and squared returns for Daily REITs and compares the findings\nfor a non- REIT equity index. The paper utilizes a variety of tests for long\nmemory finding evidence that REIT volatility does display persistence, in\ncontrast to the actual return series. Trading volume is found to be strongly\nassociated with long memory. The results do however suggest differences in the\nfindings with regard to REITs in comparison to the broader equity sector which\nmay be due to relatively thin trading during the sample period.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5414v1"
    },
    {
        "title": "Uncovering Volatility Dynamics in Daily REIT Returns",
        "authors": [
            "John Cotter",
            "Simon Stevenson"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Using a time-varying approach, this paper examines the dynamics of volatility\nin the REIT sector. The results highlight the attractiveness and suitability of\nusing GARCH based approaches in the modeling of daily REIT volatility. The\npaper examines the influencing factors on REIT volatility, documenting the\nreturn and volatility linkages between REIT sub-sectors and also examines the\ninfluence of other US equity series. The results contrast with previous studies\nof monthly REIT volatility. Linkages within the REIT sector and with related\nsectors such as value stocks are diminished, while the general influence of\nmarket sentiment, coming through the large cap indices is enhanced. This would\nindicate that on a daily basis general market sentiment plays a more\nfundamental role than more intuitive relationships within the capital markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5417v1"
    },
    {
        "title": "Uncovering Long Memory in High Frequency UK Futures",
        "authors": [
            "John Cotter"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Accurate volatility modelling is paramount for optimal risk management\npractices. One stylized feature of financial volatility that impacts the\nmodelling process is long memory explored in this paper for alternative risk\nmeasures, observed absolute and squared returns for high frequency intraday UK\nfutures. Volatility series for three different asset types, using stock index,\ninterest rate and bond futures are analysed. Long memory is strongest for the\nbond contract. Long memory is always strongest for the absolute returns series\nand at a power transformation of k < 1. The long memory findings generally\nincorporate intraday periodicity. The APARCH model incorporating seven related\nGARCH processes generally models the futures series adequately documenting\nARCH, GARCH and leverage effects.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5651v1"
    },
    {
        "title": "U.S. Core Inflation: A Wavelet Analysis",
        "authors": [
            "kevin dowd",
            "john cotter"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  This paper proposes the use of wavelet methods to estimate U.S. core\ninflation. It explains wavelet methods and suggests they are ideally suited to\nthis task. Comparisons are made with traditional CPI-based and regression-based\nmeasures for their performance in following trend inflation and predicting\nfuture inflation. Results suggest that wavelet-based measures perform better,\nand sometimes much better, than the traditional approaches. These results\nsuggest that wavelet methods are a promising avenue for future research on core\ninflation.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5659v1"
    },
    {
        "title": "Multivariate Modeling of Daily REIT Volatility",
        "authors": [
            "John Cotter",
            "Simon Stevenson"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  This paper examines volatility in REITs using a multivariate GARCH based\nmodel. The Multivariate VAR-GARCH technique documents the return and volatility\nlinkages between REIT sub-sectors and also examines the influence of other US\nequity series. The motivation is for investors to incorporate time-varyng\nvolatility and correlations in their portfolio selection. The results\nillustrate the differences in results when higher frequency daily data is\ntested in comparison to the monthly data that has been commonly used in the\nexisting literature. The linkages both within the REIT sector and between REITs\nand related sectors such as value stocks are weaker than commonly found in\nmonthly studies. The broad market would appear to be more influential in the\ndaily case.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5660v1"
    },
    {
        "title": "The tail risks of FX return distributions: a comparison of the returns\n  associated with limit orders and market orders",
        "authors": [
            "john cotter",
            "kevin dowd"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  This paper measures and compares the tail risks of limit and market orders\nusing Extreme Value Theory. The analysis examines realised tail outcomes using\nthe Dealing 2000-2 electronic broking system based on completed transactions\nrather than the more common analysis of indicative quotes. In general, limit\nand market orders exhibit broadly similar tail behaviour, but limit orders have\nsignificantly heavier tails and larger tail quantiles than market orders.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5661v1"
    },
    {
        "title": "Absolute Return Volatility",
        "authors": [
            "John Cotter"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  The use of absolute return volatility has many modelling benefits says John\nCotter. An illustration is given for the market risk measure, minimum capital\nrequirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5976v1"
    },
    {
        "title": "The near-extreme density of intraday log-returns",
        "authors": [
            "Mauro Politi",
            "Nicolas Millot",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  The extreme event statistics plays a very important role in the theory and\npractice of time series analysis. The reassembly of classical theoretical\nresults is often undermined by non-stationarity and dependence between\nincrements. Furthermore, the convergence to the limit distributions can be\nslow, requiring a huge amount of records to obtain significant statistics, and\nthus limiting its practical applications. Focussing, instead, on the closely\nrelated density of \"near-extremes\" -- the distance between a record and the\nmaximal value -- can render the statistical methods to be more suitable in the\npractical applications and/or validations of models. We apply this recently\nproposed method in the empirical validation of an adapted financial market\nmodel of the intraday market fluctuations.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.0039v1"
    },
    {
        "title": "Agent based reasoning for the non-linear stochastic models of long-range\n  memory",
        "authors": [
            "Aleksejus Kononovicius",
            "Vygintas Gontis"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We extend Kirman's model by introducing variable event time scale. The\nproposed flexible time scale is equivalent to the variable trading activity\nobserved in financial markets. Stochastic version of the extended Kirman's\nagent based model is compared to the non-linear stochastic models of long-range\nmemory in financial markets. Agent based model providing matching macroscopic\ndescription serves as a microscopic reasoning of the earlier proposed\nstochastic model exhibiting power law statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.2685v2"
    },
    {
        "title": "The Second Wave of the Global Crisis? A Log-Periodic Oscillation\n  Analysis of Commodity Price Series",
        "authors": [
            "Askar Akaev",
            "Alexei Fomin",
            "Andrey Korotayev"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  This article continues our analysis of the gold price dynamics that was\npublished in December 2010 (abs/1012.4118) and forecasted the possibility of\nthe \"burst of the gold bubble\" in April - June 2011. Our recent analysis\nsuggests the possibility of one more substantial fluctuation before the final\ncollapse in July 2011. On the other hand, in early 2011 we detected a number of\nother commodity bubbles and forecasted the start of their collapse in May -\nJune 2011. We demonstrate that this collapse has actually begun, which in\nconjunction with the forthcoming burst of the gold bubble suggests that the\nWorld System is entering a bifurcation zone bearing rather high risks of the\nsecond wave of the global financial-economic crisis. Indeed, on the one hand,\nit is obvious that such a collapse may lead to huge losses or even bankruptcies\nof many of the major participants of exchange games and their dependent firms\nand banks. Therefore, the immediate market reaction is likely to be entirely\nnegative. Negative impact on the market could well be amplified by numerous\npublications in the media and business press, drawing analogies with the events\nof the early 1980s and earlier similar events, as well as by losses of\nshareholders of bankrupt companies. On the other hand, investments in gold also\nlead to the diversion of funds from stock market investments and to the\nreduction in the production of goods and services. If at the time of the\ncollapse some promising areas of investment appear in the developed and / or\ndeveloping countries, the investment can move to those markets, which, on the\ncontrary, could contribute to the production of new goods and services and\naccelerate the way out of the crisis. It is also obvious that the decline of\nthe oil (and other energy/mineral resources) prices may contribute to\nacceleration of world economic growth rates and world economic recovery.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0480v1"
    },
    {
        "title": "A Map of the Brazilian Stock Market",
        "authors": [
            "Leonidas Sandoval Junior"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We use the correlation matrix of stocks returns in order to create maps of\nthe S\\~ao Paulo Stock Exchange (BM&F-Bovespa), Brazil's main stock exchange.\nThe data reffer to the year 2010, and the correlations between stock returns\nlead to the construction of a minimum spanning tree and of asset graphs with a\nvariety of threshold values. The results are analised using techniques of\nnetwork theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4146v2"
    },
    {
        "title": "Statistical Methods for Estimating the non-random Content of Financial\n  Markets",
        "authors": [
            "Laurent Schoeffel"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  For the pedestrian observer, financial markets look completely random with\nerratic and uncontrollable behavior. To a large extend, this is correct. At\nfirst approximation the difference between real price changes and the random\nwalk model is too small to be detected using traditional time series analysis.\nHowever, we show in the following that this difference between real financial\ntime series and random walks, as small as it is, is detectable using modern\nstatistical multivariate analysis, with several triggers encoded in trading\nsystems. This kind of analysis are based on methods widely used in nuclear\nphysics, with large samples of data and advanced statistical inference.\nConsidering the movements of the Euro future contract at high frequency, we\nshow that a part of the non-random content of this series can be inferred,\nnamely the trend-following content depending on volatility ranges.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.2937v2"
    },
    {
        "title": "About the non-random Content of Financial Markets",
        "authors": [
            "Laurent Schoeffel"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  For the pedestrian observer, financial markets look completely random with\nerratic and uncontrollable behavior. To a large extend, this is correct. At\nfirst approximation the difference between real price changes and the random\nwalk model is too small to be detected using traditional time series analysis.\nHowever, we show in the following that this difference between real financial\ntime series and random walks, as small as it is, is detectable using modern\nstatistical multivariate analysis, with several triggers encoded in trading\nsystems. This kind of analysis are based on methods widely used in nuclear\nphysics, with large samples of data and advanced statistical inference.\nConsidering the movements of the Euro future contract at high frequency, we\nshow that a part of the non-random content of this series can be inferred,\nnamely the trend-following content depending on volatility ranges. Of course,\nthis is not a general proof of statistical inference, as we focus on one\nparticular example and the generality of the process can not be claimed.\nTherefore, we produce other examples on a completely different markets, largely\nuncorrelated to the Euro future, namely the DAX and Cacao future contracts. The\nsame procedure is followed using a trading system, based on the same\ningredients. We show that similar results can be obtained and we conclude that\nthis is an evidence that some invariants, as encoded in our system, have been\nidentified. They provide a kind of quantification of the non-random content of\nthe financial markets explored over a 10 years period of time.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.3155v2"
    },
    {
        "title": "Intermittency in Quantitative Finance",
        "authors": [
            "Laurent Schoeffel"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Factorial moments are convenient tools in nuclear physics to characterize the\nmultiplicity distributions when phase-space resolution ($\\Delta$) becomes\nsmall. For uncorrelated particle production within $\\Delta$, Gaussian\nstatistics holds and factorial moments $F_q$ are equal to unity for all orders\n$q$. Correlations between particles lead to a broadening of the multiplicity\ndistribution and to dynamical fluctuations. In this case, the factorial moments\nincrease above 1 with decreasing $\\Delta$. This corresponds to what can be\ncalled intermittency. In this letter, we show that a similar analysis can be\ndeveloped on financial price series, with an adequate definition of factorial\nmoments. An intermittent behavior can be extracted using moments of order 2\n($F_2$), illustrating a sensitivity to non-Gaussian fluctuations within time\nresolution below 4 hours. This confirms that correlations between price returns\nstart to play a role when the time resolution is below this threshold.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5596v1"
    },
    {
        "title": "Factorial Moments in Complex Systems",
        "authors": [
            "Laurent Schoeffel"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Factorial moments are convenient tools in particle physics to characterize\nthe multiplicity distributions when phase-space resolution ($\\Delta$) becomes\nsmall. They include all correlations within the system of particles and\nrepresent integral characteristics of any correlation between these particles.\nIn this letter, we show a direct comparison between high energy physics and\nquantitative finance results. Both for physics and finance, we illustrate that\ncorrelations between particles lead to a broadening of the multiplicity\ndistribution and to dynamical fluctuations when the resolution becomes small\nenough. From the generating function of factorial moments, we make a prediction\non the gap probability for sequences of returns of positive or negative signs.\nThe gap is defined as the number of consecutive positive returns after a\nnegative return, thus this is a gap in negative return. Inversely for a gap in\npositive return. Then, the gap probability is shown to be exponentially\nsuppressed within the gap size. We confirm this prediction with data.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.5946v1"
    },
    {
        "title": "Returns in futures markets and $ν=3$ t-distribution",
        "authors": [
            "Laurent Schoeffel"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  The probability distribution of log-returns of financial time series, sampled\nat high frequency, is the basis for any further developments in quantitative\nfinance. In this letter, we present experimental results based on a large set\nof time series on futures. Then, we show that the t-distribution with $\\nu\n\\simeq 3$ gives a nice description of almost all data series. This appears to\nbe a quite general result that stays robust on a large set of any financial\ndata as well as on a wide range of sampling frequency of these data, below one\nhour.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.1006v1"
    },
    {
        "title": "Time Scales in Futures Markets and Applications",
        "authors": [
            "Laurent Schoeffel"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  The probability distribution of log-returns for financial time series,\nsampled at high frequency, is the basis for any further developments in\nquantitative finance. In this letter, we present experimental results based on\na large set of time series on futures. We show that the t-distribution with\n$\\nu \\simeq 3$ gives a nice description of almost all data series considered\nfor a time scale $\\Delta t$ below 1 hour. For $\\Delta t \\ge 8$ hours, the\nGaussian regime is reached. A particular focus has been put on the DAX and Euro\nfutures. This appears to be a quite general result that stays robust on a large\nset of futures, but not on any data sets. In this sense, this is not universal.\nA technique using factorial moments defined on a sequence of returns is\ndescribed and similar results for time scales are obtained. Let us note that\nfrom a fundamental point of view, there is no clear reason why DAX and Euro\nfutures should present similar behavior with respect to their return\ndistributions. Both are complex markets where many internal and external\nfactors interact at each instant to determine the transaction price. These\nfactors are certainly different for an index on a change parity (Euro) and an\nindex on stocks (DAX). Thus, this is striking that we can identify universal\nstatistical features in price fluctuations of these markets. This is really the\nadvantage of micro-structure analysis to prompt unified approaches of different\nkinds of markets. Finally, we examine the relation of power law distribution of\nreturns with another scaling behavior of the data encoded into the Hurst\nexponent. We have obtained $H=0.54 \\pm 0.04$ for DAX and $H=0.51 \\pm 0.03$ for\nEuro futures.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.1727v1"
    },
    {
        "title": "Collective behavior of stock prices as a precursor to market crash",
        "authors": [
            "Jun-ichi Maskawa"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We study precursors to the global market crash that occurred on all main\nstock exchanges throughout the world in October 2008 about three weeks after\nthe bankruptcy of Lehman Brothers Holdings Inc. on 15 September. We examine the\ncollective behavior of stock returns and analyze the market mode, which is a\nmarket-wide collective mode, with constituent issues of the FTSE 100 index\nlisted on the London Stock Exchange. Before the market crash, a sharp rise in a\nmeasure of the collective behavior was observed. It was shown to be associated\nwith news including the words \"financial crisis.\" They did not impact stock\nprices severely alone, but they exacerbated the pessimistic mood that prevailed\namong stock market participants. Such news increased after the Lehman shock\npreceding the market crash. The variance increased along with the cumulative\namount of news according to a power law.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.4637v1"
    },
    {
        "title": "Cluster formation and evolution in networks of financial market indices",
        "authors": [
            "Leonidas Sandoval Junior"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Using data from world stock exchange indices prior to and during periods of\nglobal financial crises, clusters and networks of indices are built for\ndifferent thresholds and diverse periods of time, so that it is then possible\nto analyze how clusters are formed according to correlations among indices and\nhow they evolve in time, particularly during times of financial crises. Further\nanalysis is made on the eigenvectors corresponding to the second highest\neigenvalues of the correlation matrices, revealing a structure peculiar to\nmarkets that operate in different time zones.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.5069v1"
    },
    {
        "title": "Non-Gaussianity of the Intraday Returns Distribution: its evolution in\n  time",
        "authors": [
            "M. A. Virasoro"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We find a remarkable time persistence of various proxies for the kurtosis\n(p-kurtosis) of the intraday returns distribution for the S&P500 index and this\npermits a significant measure of their evolution from 1983 to 2004. There\nappears a long time scale dramatic variation of the p-kurtosis uncorrelated\nwith the variation of the volatility thus falsifying any hypothesis of a\nuniversal shape for the probability distribution of the returns. A large\nincrease in the kurtosis anticipates the October 87 crash. During the years\n1991-2003 it continuously decreases even when the volatility grows during the\ndot-com bubble. We propose some speculative interpretations of these results.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.0770v2"
    },
    {
        "title": "The topology of cross-border exposures: beyond the minimal spanning tree\n  approach",
        "authors": [
            "Alessandro Spelta",
            "Tanya Araújo"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  The recent financial crisis has stressed the need to understand financial\nsystems as networks of interdependent countries, where cross-border financial\nlinkages play the fundamental role. It has also been emphasized that the\nrelevance of these networks relies on the representation of changes follow-on\nthe occurrence of stress events. Adopting a topological approach we are able to\naddress the role that network structures play in the spread of shocks and\nconversely, the effectiveness of stress events and its impact on the structure\nof the networks. Here, from series of interbank liabilities and claims over\ndifferent time periods, we have developed networks of positions (net claims)\nbetween countries. Besides the Minimal Spanning Tree analysis of the\ntime-constrained networks, a coefficient of residuality is defined to capture\nthe structural evolution of the network of cross-border financial linkages.\nBecause some structural changes seem to be related to the role that countries\nplay in the financial context, networks of debtor and creditor countries are\nalso developed. Empirical results allows to relate the network structure that\nemerges in the last years to the globally turbulent period that has\ncharacterized financial systems since the latest nineties. The residuality\ncoefficient highlights an important modification acting in the financial\nlinkages across countries in the period 1997-2011, and situates the recent\nfinancial crises as replica of a larger structural change going on since 1997.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.5711v1"
    },
    {
        "title": "The Evolution of Stock Market Efficiency in the US: A Non-Bayesian\n  Time-Varying Model Approach",
        "authors": [
            "Mikio Ito",
            "Akihiko Noda",
            "Tatsuma Wada"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  A non-Bayesian time-varying model is developed by introducing the concept of\nthe degree of market efficiency that varies over time. This model may be seen\nas a reflection of the idea that continuous technological progress alters the\ntrading environment over time. With new methodologies and a new measure of the\ndegree of market efficiency, we examine whether the US stock market evolves\nover time. In particular, a time-varying autoregressive (TV-AR) model is\nemployed. Our main findings are: (i) the US stock market has evolved over time\nand the degree of market efficiency has cyclical fluctuations with a\nconsiderably long periodicity, from 30 to 40 years; and (ii) the US stock\nmarket has been efficient with the exception of four times in our sample\nperiod: during the long-recession of 1873-1879; the recession of 1902-1904; the\nNew Deal era; and the recession of 1957-1958 and soon after it. It is then\nshown that our results are partly consistent with the view of behavioral\nfinance.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.0100v13"
    },
    {
        "title": "A multifractal approach towards inference in finance",
        "authors": [
            "Ola Løvsletten",
            "Martin Rypdal"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We introduce tools for inference in the multifractal random walk introduced\nby Bacry et al. (2001). These tools include formulas for smoothing, filtering\nand volatility forecasting. In addition, we present methods for computing\nconditional densities for one- and multi-step returns. The inference techniques\npresented in this paper, including maximum likelihood estimation, are applied\nto data from the Oslo Stock Exchange, and it is observed that the volatility\nforecasts based on the multifractal random walk have a much richer structure\nthan the forecasts obtained from a basic stochastic volatility model.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.5376v1"
    },
    {
        "title": "International Stock Market Efficiency: A Non-Bayesian Time-Varying Model\n  Approach",
        "authors": [
            "Mikio Ito",
            "Akihiko Noda",
            "Tatsuma Wada"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This paper develops a non-Bayesian methodology to analyze the time-varying\nstructure of international linkages and market efficiency in G7 countries. We\nconsider a non-Bayesian time-varying vector autoregressive (TV-VAR) model, and\napply it to estimate the joint degree of market efficiency in the sense of Fama\n(1970, 1991). Our empirical results provide a new perspective that the\ninternational linkages and market efficiency change over time and that their\nbehaviors correspond well to historical events of the international financial\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.5176v14"
    },
    {
        "title": "Modeling and forecasting exchange rate volatility in time-frequency\n  domain",
        "authors": [
            "Jozef Barunik",
            "Tomas Krehlik",
            "Lukas Vacha"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This paper proposes an enhanced approach to modeling and forecasting\nvolatility using high frequency data. Using a forecasting model based on\nRealized GARCH with multiple time-frequency decomposed realized volatility\nmeasures, we study the influence of different timescales on volatility\nforecasts. The decomposition of volatility into several timescales approximates\nthe behaviour of traders at corresponding investment horizons. The proposed\nmethodology is moreover able to account for impact of jumps due to a recently\nproposed jump wavelet two scale realized volatility estimator. We propose a\nrealized Jump-GARCH models estimated in two versions using maximum likelihood\nas well as observation-driven estimation framework of generalized\nautoregressive score. We compare forecasts using several popular realized\nvolatility measures on foreign exchange rate futures data covering the recent\nfinancial crisis. Our results indicate that disentangling jump variation from\nthe integrated variation is important for forecasting performance. An\ninteresting insight into the volatility process is also provided by its\nmultiscale decomposition. We find that most of the information for future\nvolatility comes from high frequency part of the spectra representing very\nshort investment horizons. Our newly proposed models outperform statistically\nthe popular as well conventional models in both one-day and multi-period-ahead\nforecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.1452v4"
    },
    {
        "title": "Study of statistical correlations in intraday and daily financial return\n  time series",
        "authors": [
            "Gayatri Tilak",
            "Tamas Szell",
            "Remy Chicheportiche",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The aim of this article is to briefly review and make new studies of\ncorrelations and co-movements of stocks, so as to understand the\n\"seasonalities\" and market evolution. Using the intraday data of the CAC40, we\nbegin by reasserting the findings of Allez and Bouchaud [New J. Phys. 13,\n025010 (2011)]: the average correlation between stocks increases throughout the\nday. We then use multidimensional scaling (MDS) in generating maps and\nvisualizing the dynamic evolution of the stock market during the day. We do not\nfind any marked difference in the structure of the market during a day. Another\naim is to use daily data for MDS studies, and visualize or detect specific\nsectors in a market and periods of crisis. We suggest that this type of\nvisualization may be used in identifying potential pairs of stocks for \"pairs\ntrade\".\n",
        "pdf_link": "http://arxiv.org/pdf/1204.5103v1"
    },
    {
        "title": "Carbon-dioxide emissions trading and hierarchical structure in worldwide\n  finance and commodities markets",
        "authors": [
            "Zeyu Zheng",
            "Kazuko Yamasaki",
            "Joel N. Tenenbaum",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  In a highly interdependent economic world, the nature of relationships\nbetween financial entities is becoming an increasingly important area of study.\nRecently, many studies have shown the usefulness of minimal spanning trees\n(MST) in extracting interactions between financial entities. Here, we propose a\nmodified MST network whose metric distance is defined in terms of\ncross-correlation coefficient absolute values, enabling the connections between\nanticorrelated entities to manifest properly. We investigate 69 daily time\nseries, comprising three types of financial assets: 28 stock market indicators,\n21 currency futures, and 20 commodity futures. We show that though the\nresulting MST network evolves over time, the financial assets of similar type\ntend to have connections which are stable over time. In addition, we find a\ncharacteristic time lag between the volatility time series of the stock market\nindicators and those of the EU CO2 emission allowance (EUA) and crude oil\nfutures (WTI). This time lag is given by the peak of the cross-correlation\nfunction of the volatility time series EUA (or WTI) with that of the stock\nmarket indicators, and is markedly different (>20 days) from 0, showing that\nthe volatility of stock market indicators today can predict the volatility of\nEU emissions allowances and of crude oil in the near future.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1861v2"
    },
    {
        "title": "The fractional volatility model: No-arbitrage, leverage and completeness",
        "authors": [
            "R. Vilela Mendes",
            "M. J. Oliveira",
            "A. M. Rodrigues"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Based on a criterion of mathematical simplicity and consistency with\nempirical market data, a stochastic volatility model has been obtained with the\nvolatility process driven by fractional noise. Depending on whether the\nstochasticity generators of log-price and volatility are independent or are the\nsame, two versions of the model are obtained with different leverage behavior.\nHere, the no-arbitrage and completeness properties of the models are studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2866v1"
    },
    {
        "title": "A Multi-Level Lorentzian Analysis of the Basic Structures of the Daily\n  DJIA",
        "authors": [
            "Frank W. K. Firk"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  A quantitative analysis of the basic components of the daily DJIA. The\nparameters of the underlying Lorentzian states are obtained by fitting the\ndata. Statistical properties of the states are discussed. This is a practical\ndevelopment of the general method introduced in arXiv:1203.6021.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.5820v1"
    },
    {
        "title": "Stochastic Volatility with Heterogeneous Time Scales",
        "authors": [
            "Danilo Delpini",
            "Giacomo Bormetti"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Agents' heterogeneity is recognized as a driver mechanism for the persistence\nof financial volatility. We focus on the multiplicity of investment strategies'\nhorizons, we embed this concept in a continuous time stochastic volatility\nframework and prove that a parsimonious, two-scale version effectively captures\nthe long memory as measured from the real data. Since estimating parameters in\na stochastic volatility model is challenging, we introduce a robust methodology\nbased on the Generalized Method of Moments supported by a heuristic selection\nof the orthogonal conditions. In addition to the volatility clustering, the\nestimated model also captures other relevant stylized facts, emerging as a\nminimal but realistic and complete framework for modelling financial time\nseries.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.0026v3"
    },
    {
        "title": "A Test of the Adaptive Market Hypothesis using a Time-Varying AR Model\n  in Japan",
        "authors": [
            "Akihiko Noda"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This study examines the adaptive market hypothesis (AMH) in Japanese stock\nmarkets (TOPIX and TSE2). In particular, we measure the degree of market\nefficiency by using a time-varying model approach. The empirical results show\nthat (1) the degree of market efficiency changes over time in the two markets,\n(2) the level of market efficiency of the TSE2 is lower than that of the TOPIX\nin most periods, and (3) the market efficiency of the TOPIX has evolved, but\nthat of the TSE2 has not. We conclude that the results support the AMH for the\nmore qualified stock market in Japan.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.1842v4"
    },
    {
        "title": "How news affect the trading behavior of different categories of\n  investors in a financial market",
        "authors": [
            "Fabrizio Lillo",
            "Salvatore Miccichè",
            "Michele Tumminello",
            "Jyrki Piilo",
            "Rosario Nunzio Mantegna"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We investigate the trading behavior of a large set of single investors\ntrading the highly liquid Nokia stock over the period 2003-2008 with the aim of\ndetermining the relative role of endogenous and exogenous factors that may\naffect their behavior. As endogenous factors we consider returns and\nvolatility, whereas the exogenous factors we use are the total daily number of\nnews and a semantic variable based on a sentiment analysis of news. Linear\nregression and partial correlation analysis of data show that different\ncategories of investors are differently correlated to these factors.\nGovernmental and non profit organizations are weakly sensitive to news and\nreturns or volatility, and, typically, they are more correlated with the former\nthan with the latter. Households and companies, on the contrary, are very\nsensitive to both endogenous and exogenous factors, and volatility and returns\nare, on average, much more relevant than the number of news and sentiment,\nrespectively. Finally, financial institutions and foreign organizations are\nintermediate between these two cases, in terms of both the total explanatory\npower of these factors and their relative importance.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.3300v1"
    },
    {
        "title": "Global Inflation Dynamics: regularities & forecasts",
        "authors": [
            "Askar Akaev",
            "Andrey Korotayev",
            "Alexey Fomin"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The analysis of dollar inflation performed by the authors through the\napproximation of empirical data for 1913-2012 with a power-law function with an\naccelerating log-periodic oscillation superimposed over it has made it possible\nto detect a quasi-singularity point around the 17th of December, 2012. It is\ndemonstrated that, if adequate measures are not taken, one may expect a surge\nof inflation around the end of this year that may also mark the start of\nstagflation as there are no sufficient grounds to expect the re-start of the\ndynamic growth of the world economy by that time. On the other hand, as the\nexperience of the 1970s and the 1980s indicates, the stagflation consequences\ncan only be eliminated with great difficulties and at a rather high cost,\nbecause the combination of low levels of economic growth and employment with\nhigh inflation leads to a sharp decline in consumption, aggravating the\neconomic depression. In order to mitigate the inflationary consequences of the\nexplosive growth of money (and, first of all, US dollar) supply it is necessary\nto take urgently the world monetary emission under control. This issue should\nbecome central at the forthcoming G8 and G20 summits.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.4069v1"
    },
    {
        "title": "Modeling and Forecasting Persistent Financial Durations",
        "authors": [
            "Filip Zikes",
            "Jozef Barunik",
            "Nikhil Shenai"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This paper introduces the Markov-Switching Multifractal Duration (MSMD) model\nby adapting the MSM stochastic volatility model of Calvet and Fisher (2004) to\nthe duration setting. Although the MSMD process is exponential $\\beta$-mixing\nas we show in the paper, it is capable of generating highly persistent\nautocorrelation. We study analytically and by simulation how this feature of\ndurations generated by the MSMD process propagates to counts and realized\nvolatility. We employ a quasi-maximum likelihood estimator of the MSMD\nparameters based on the Whittle approximation and establish its strong\nconsistency and asymptotic normality for general MSMD specifications. We show\nthat the Whittle estimation is a computationally simple and fast alternative to\nmaximum likelihood. Finally, we compare the performance of the MSMD model with\ncompeting short- and long-memory duration models in an out-of-sample\nforecasting exercise based on price durations of three major foreign exchange\nfutures contracts. The results of the comparison show that the MSMD and LMSD\nperform similarly and are superior to the short-memory ACD models.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.3087v2"
    },
    {
        "title": "Revisiting the fractional cointegrating dynamics of implied-realized\n  volatility relation with wavelet band spectrum regression",
        "authors": [
            "Jozef Barunik",
            "Michaela Barunikova"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This paper revisits the fractional cointegrating relationship between ex-ante\nimplied volatility and ex-post realized volatility. We argue that the concept\nof corridor implied volatility (CIV) should be used instead of the popular\nmodel-free option-implied volatility (MFIV) when assessing the fractional\ncointegrating relation as the latter may introduce bias to the estimation. For\nthe realized volatility, we use recently proposed methods which are robust to\nnoise as well as jumps and interestingly we find that it does not affect the\nimplied-realized volatility relation. In addition, we develop a new tool for\nthe estimation of fractional cointegrating relation between implied and\nrealized volatility based on wavelets, a wavelet band least squares (WBLS). The\nmain advantage of WBLS in comparison to other frequency domain methods is that\nit allows us to work conveniently with potentially non-stationary volatility\ndue to the properties of wavelets. We study the dynamics of the relationship in\nthe time-frequency domain with the wavelet coherence confirming that the\ndependence comes solely from the lower frequencies of the spectra. Motivated by\nthis result we estimate the relationship only on this part of the spectra using\nWBLS and compare our results to the fully modified narrow-band least squares\n(FMNBLS) based on the Fourier frequencies. In the estimation, we use the S&P\n500 and DAX monthly and bi-weekly option prices covering the recent financial\ncrisis and we conclude that in the long-run, volatility inferred from the\noption prices using the corridor implied volatility (CIV) provides an unbiased\nforecast of the realized volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.4831v2"
    },
    {
        "title": "Analysis of short term price trends in daily stock-market index data",
        "authors": [
            "H. F. Coronel-Brizio",
            "A. R. Hernández Montoya",
            "H. R Olivares Sánchez",
            "E. Scalas"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  In financial time series there are periods in which the value increases or\ndecreases monotonically. We call those periods elemental trends and study the\nprobability distribution of their duration for the indices DJIA, NASDAQ and\nIPC. It is found that the trend duration distribution often differs from the\none expected under no memory. The expected and observed distributions are\ncompared by means of the Anderson-Darling test.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.3060v1"
    },
    {
        "title": "Testing the weak-form efficiency of the WTI crude oil futures market",
        "authors": [
            "Zhi-Qiang Jiang",
            "Wen-Jie Xie",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We perform detrending moving average analysis (DMA) and detrended fluctuation\nanalysis (DFA) of the WTI crude oil futures prices (1983-2012) to investigate\nits efficiency. We further put forward a strict statistical test in the spirit\nof bootstrapping to verify the weak-form market efficiency hypothesis by\nemploying the DMA (or DFA) exponent as the statistic. We verify the weak-form\nefficiency of the crude oil futures market when the whole period is considered.\nWhen we break the whole series into three sub-series separated by the outbreaks\nof the Gulf War and the Iraq War, our statistical tests uncover that only the\nGulf War has the impact of reducing the efficiency of the crude oil market. If\nwe split the whole time series into two sub-series based on the signing date of\nthe North American Free Trade Agreement, we find that the market is inefficient\nin the sub-periods during which the Gulf War broke out. We also perform the\nsame analysis on short time series in moving windows and find that the market\nis inefficient only when some turbulent events occur, such as the oil price\ncrash in 1985, the Gulf war, and the oil price crash in 2008. Our analysis may\noffer a new understanding of the efficiency of the crude oil futures market and\nshed new lights on the investigation of the efficiency in other financial\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.4686v1"
    },
    {
        "title": "Extreme value statistics and recurrence intervals of NYMEX energy\n  futures volatility",
        "authors": [
            "Wen-Jie Xie",
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Energy markets and the associated energy futures markets play a crucial role\nin global economies. We investigate the statistical properties of the\nrecurrence intervals of daily volatility time series of four NYMEX energy\nfutures, which are defined as the waiting times $\\tau$ between consecutive\nvolatilities exceeding a given threshold $q$. We find that the recurrence\nintervals are distributed as a stretched exponential $P_q(\\tau)\\sim\ne^{(a\\tau)^{-\\gamma}}$, where the exponent $\\gamma$ decreases with increasing\n$q$, and there is no scaling behavior in the distributions for different\nthresholds $q$ after the recurrence intervals are scaled with the mean\nrecurrence interval $\\bar\\tau$. These findings are significant under the\nKolmogorov-Smirnov test and the Cram{\\'e}r-von Mises test. We show that\nempirical estimations are in nice agreement with the numerical integration\nresults for the occurrence probability $W_q(\\Delta{t}|t)$ of a next event above\nthe threshold $q$ within a (short) time interval after an elapsed time $t$ from\nthe last event above $q$. We also investigate the memory effects of the\nrecurrence intervals. It is found that the conditional distributions of large\nand small recurrence intervals differ from each other and the conditional mean\nof the recurrence intervals scales as a power law of the preceding interval\n$\\bar\\tau(\\tau_0)/\\bar\\tau \\sim (\\tau_0/\\bar\\tau)^\\beta$, indicating that the\nrecurrence intervals have short-term correlations. Detrended fluctuation\nanalysis and detrending moving average analysis further uncover that the\nrecurrence intervals possess long-term correlations. We confirm that the\n\"clustering\" of the volatility recurrence intervals is caused by the long-term\ncorrelations well known to be present in the volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5502v1"
    },
    {
        "title": "Modeling non-stationarities in high-frequency financial time series",
        "authors": [
            "Linda Ponta",
            "Mailan Trinh",
            "Marco Raberto",
            "Enrico Scalas",
            "Silvano Cincotti"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We study tick-by-tick financial returns belonging to the FTSE MIB index of\nthe Italian Stock Exchange (Borsa Italiana). We can confirm previously detected\nnon-stationarities. However, scaling properties reported in the previous\nliterature for other high-frequency financial data are only approximately\nvalid. As a consequence of the empirical analyses, we propose a simple method\nfor describing non-stationary returns, based on a non-homogeneous normal\ncompound Poisson process. We test this model against the empirical findings and\nit turns out that the model can approximately reproduce several stylized facts\nof high-frequency financial time series. Moreover, using Monte Carlo\nsimulations, we analyze order selection for this model class using three\ninformation criteria: Akaike's information criterion (AIC), the Bayesian\ninformation criterion (BIC) and the Hannan-Quinn information criterion (HQ).\nFor comparison, we also perform a similar Monte Carlo experiment for the ACD\n(autoregressive conditional duration) model. Our results show that the\ninformation criteria work best for small parameter numbers for the compound\nPoisson type models, whereas for the ACD model the model selection procedure\ndoes not work well in certain cases.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.0479v4"
    },
    {
        "title": "Modelling systemic price cojumps with Hawkes factor models",
        "authors": [
            "Giacomo Bormetti",
            "Lucio Maria Calcagnile",
            "Michele Treccani",
            "Fulvio Corsi",
            "Stefano Marmi",
            "Fabrizio Lillo"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Instabilities in the price dynamics of a large number of financial assets are\na clear sign of systemic events. By investigating a set of 20 high cap stocks\ntraded at the Italian Stock Exchange, we find that there is a large number of\nhigh frequency cojumps. We show that the dynamics of these jumps is described\nneither by a multivariate Poisson nor by a multivariate Hawkes model. We\nintroduce a Hawkes one factor model which is able to capture simultaneously the\ntime clustering of jumps and the high synchronization of jumps across assets.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.6141v2"
    },
    {
        "title": "Volatility polarization of non-specialized investors' heterogeneous\n  activity",
        "authors": [
            "Mario Gutiérrez-Roig",
            "Josep Perelló"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Financial markets provide an ideal frame for studying decision making in\ncrowded environments. Both the amount and accuracy of the data allows to apply\ntools and concepts coming from physics that studies collective and emergent\nphenomena or self-organised and highly heterogeneous systems. We analyse the\nactivity of 29,930 non-expert individuals that represent a small portion of the\nwhole market trading volume. The very heterogeneous activity of individuals\nobeys a Zipf's law, while synchronization network properties unveil a community\nstructure. We thus correlate individual activity with the most eminent\nmacroscopic signal in financial markets, that is volatility, and quantify how\nindividuals are clearly polarized by volatility. The assortativity by\nattributes of our synchronization networks also indicates that individuals look\nat the volatility rather than imitate directly each other thus providing an\ninteresting interpretation of herding phenomena in human activity. The results\ncan also improve agent-based models since they provide direct estimation of the\nagent's parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.3169v1"
    },
    {
        "title": "Random Matrix Theory and Cross-correlations in Global Financial Indices\n  and Local Stock Market Indices",
        "authors": [
            "Ashadun Nobi",
            "Seong Eun Maeng",
            "Gyeong Gyun Ha",
            "Jae Woo Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We analyzed cross-correlations between price fluctuations of global financial\nindices (20 daily stock indices over the world) and local indices (daily\nindices of 200 companies in the Korean stock market) by using random matrix\ntheory (RMT). We compared eigenvalues and components of the largest and the\nsecond largest eigenvectors of the cross-correlation matrix before, during, and\nafter the global financial the crisis in the year 2008. We find that the\nmajority of its eigenvalues fall within the RMT bounds [{\\lambda}_,\n{\\lambda}+], where {\\lambda}_- and {\\lambda}_+ are the lower and the upper\nbounds of the eigenvalues of random correlation matrices. The components of the\neigenvectors for the largest positive eigenvalues indicate the identical\nfinancial market mode dominating the global and local indices. On the other\nhand, the components of the eigenvector corresponding to the second largest\neigenvalue are positive and negative values alternatively. The components\nbefore the crisis change sign during the crisis, and those during the crisis\nchange sign after the crisis. The largest inverse participation ratio (IPR)\ncorresponding to the smallest eigenvector is higher after the crisis than\nduring any other periods in the global and local indices. During the global\nfinancial the crisis, the correlations among the global indices and among the\nlocal stock indices are perturbed significantly. However, the correlations\nbetween indices quickly recover the trends before the crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.6305v1"
    },
    {
        "title": "Realizing stock market crashes: stochastic cusp catastrophe model of\n  returns under the time-varying volatility",
        "authors": [
            "Jozef Barunik",
            "Jiri Kukacka"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  This paper develops a two-step estimation methodology, which allows us to\napply catastrophe theory to stock market returns with time-varying volatility\nand model stock market crashes. Utilizing high frequency data, we estimate the\ndaily realized volatility from the returns in the first step and use stochastic\ncusp catastrophe on data normalized by the estimated volatility in the second\nstep to study possible discontinuities in markets. We support our methodology\nby simulations where we also discuss the importance of stochastic noise and\nvolatility in deterministic cusp catastrophe model. The methodology is\nempirically tested on almost 27 years of U.S. stock market evolution covering\nseveral important recessions and crisis periods. Due to the very long sample\nperiod we also develop a rolling estimation approach and we find that while in\nthe first half of the period stock markets showed marks of bifurcations, in the\nsecond half catastrophe theory was not able to confirm this behavior. Results\nsuggest that the proposed methodology provides an important shift in\napplication of catastrophe theory to stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.7036v2"
    },
    {
        "title": "Are random trading strategies more successful than technical ones?",
        "authors": [
            "A. E. Biondo",
            "A. Pluchino",
            "A. Rapisarda",
            "D. Helbing"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this paper we explore the specific role of randomness in financial\nmarkets, inspired by the beneficial role of noise in many physical systems and\nin previous applications to complex socio- economic systems. After a short\nintroduction, we study the performance of some of the most used trading\nstrategies in predicting the dynamics of financial markets for different\ninternational stock exchange indexes, with the goal of comparing them with the\nperformance of a completely random strategy. In this respect, historical data\nfor FTSE-UK, FTSE-MIB, DAX, and S&P500 indexes are taken into account for a\nperiod of about 15-20 years (since their creation until today).\n",
        "pdf_link": "http://arxiv.org/pdf/1303.4351v4"
    },
    {
        "title": "Non-Stationarity in Financial Time Series and Generic Features",
        "authors": [
            "Thilo A. Schmitt",
            "Desislava Chetalova",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Financial markets are prominent examples for highly non-stationary systems.\nSample averaged observables such as variances and correlation coefficients\nstrongly depend on the time window in which they are evaluated. This implies\nsevere limitations for approaches in the spirit of standard equilibrium\nstatistical mechanics and thermodynamics. Nevertheless, we show that there are\nsimilar generic features which we uncover in the empirical return distributions\nfor whole markets. We explain our findings by setting up a random matrix model.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.5130v2"
    },
    {
        "title": "Multifractality and long memory of a financial index",
        "authors": [
            "Pablo Suárez-García",
            "David Gómez-Ullate"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this paper we will try to assess the multifractality displayed by the\nhigh-frequency returns of Madrid's Stock Exchange IBEX35 index. A Multifractal\nDetrended Fluctuation Analysis shows that this index has a wide singularity\nspectrum which is most likely caused by its long memory. Our findings also show\nthat this long-memory can be considered as the superposition of a\nhigh-frequency component (related to the daily cycles of arrival of information\nto the market), over a slowly-varying component that reverberates for long\nperiods of time and which shows no apparent relation with human economic\ncycles. This later component is therefore postulated to be endogenous to\nmarket's dynamics and to be also the most probable source of some of the\nstylized facts commonly associated with financial time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0490v1"
    },
    {
        "title": "Market-wide price co-movement around crashes in the Tokyo Stock Exchange",
        "authors": [
            "Jun-ichi Maskawa",
            "Joshin Murai",
            "Koji Kuroda"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  As described in this paper, we study market-wide price co-movements around\ncrashes by analyzing a dataset of high-frequency stock returns of the\nconstituent issues of Nikkei 225 Index listed on the Tokyo Stock Exchange for\nthe three years during 2007--2009. Results of day-to-day principal component\nanalysis of the time series sampled at the 1 min time interval during the\ncontinuous auction of the daytime reveal the long range up to a couple of\nmonths significant auto-correlation of the maximum eigenvalue of the\ncorrelation matrix, which express the intensity of market-wide co-movement of\nstock prices. It also strongly correlates with the open-to-close intraday\nreturn and daily return of Nikkei 225 Index. We also study the market mode,\nwhich is the first principal component corresponding to the maximum eigenvalue,\nin the framework of Multi-fractal random walk model. The parameter of the model\nestimated in a sliding time window, which describes the covariance of the\nlogarithm of the stochastic volatility, grows before almost all large intraday\nprice declines of less than -5%. This phenomenon signifies the upwelling of the\nmarket-wide collective behavior before the crash, which might reflect a herding\nof market participants.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.2188v1"
    },
    {
        "title": "Effective Measure of Endogeneity for the Autoregressive Conditional\n  Duration Point Processes via Mapping to the Self-Excited Hawkes Process",
        "authors": [
            "Vladimir Filimonov",
            "Spencer Wheatley",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In order to disentangle the internal dynamics from exogenous factors within\nthe Autoregressive Conditional Duration (ACD) model, we present an effective\nmeasure of endogeneity. Inspired from the Hawkes model, this measure is defined\nas the average fraction of events that are triggered due to internal feedback\nmechanisms within the total population. We provide a direct comparison of the\nHawkes and ACD models based on numerical simulations and show that our\neffective measure of endogeneity for the ACD can be mapped onto the \"branching\nratio\" of the Hawkes model.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.2245v2"
    },
    {
        "title": "Systemic risk and spatiotemporal dynamics of the US housing market",
        "authors": [
            "Hao Meng",
            "Wen-Jie Xie",
            "Zhi-Qiang Jiang",
            "Boris Podobnik",
            "Wei-Xing Zhou",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Housing markets play a crucial role in economies and the collapse of a\nreal-estate bubble usually destabilizes the financial system and causes\neconomic recessions. We investigate the systemic risk and spatiotemporal\ndynamics of the US housing market (1975-2011) at the state level based on the\nRandom Matrix Theory (RMT). We identify rich economic information in the\nlargest eigenvalues deviating from RMT predictions and unveil that the\ncomponent signs of the eigenvectors contain either geographical information or\nthe extent of differences in house price growth rates or both. Our results show\nthat the US housing market experienced six different regimes, which is\nconsistent with the evolution of state clusters identified by the box\nclustering algorithm and the consensus clustering algorithm on the partial\ncorrelation matrices. Our analysis uncovers that dramatic increases in the\nsystemic risk are usually accompanied with regime shifts, which provides a\nmeans of early detection of housing bubbles.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.2831v1"
    },
    {
        "title": "Additive versus multiplicative parameters - applications in economics\n  and finance",
        "authors": [
            "Helena Jasiulewicz",
            "Wojciech Kordecki"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this paper, we pay our attention to geometric parameters and their\napplications in economics and finance. We discuss the multiplicative models in\nwhich a geometric mean and a geometric standard deviation are more natural than\narithmetic ones. We give two examples from Warsaw Stock Exchange in 1995--2009\nand from a bid of 52-week treasury bills in 1992--2009 in Poland as an\nillustrative example. For distributions having applications in finance and\ninsurance we give their multiplicative parameters as well as their estimations.\nWe consider, among others, heavy-tailed distributions such as lognormal and\nPareto distribution, applied to modelling of large losses.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.4994v2"
    },
    {
        "title": "Unveiling correlations between financial variables and topological\n  metrics of trading networks: Evidence from a stock and its warrant",
        "authors": [
            "Ming-Xia Li",
            "Zhi-Qiang Jiang",
            "Wen-Jie Xie",
            "Xiong Xiong",
            "Wei Zhang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Traders adopt different trading strategies to maximize their returns in\nfinancial markets. These trading strategies not only results in specific\ntopological structures in trading networks, which connect the traders with the\npairwise buy-sell relationships, but also have potential impacts on market\ndynamics. Here, we present a detailed analysis on how the market behaviors are\ncorrelated with the structures of traders in trading networks based on audit\ntrail data for the Baosteel stock and its warrant at the transaction level from\n22 August 2005 to 23 August 2006. In our investigation, we divide each trade\nday into 48 time windows with a length of five minutes, construct a trading\nnetwork within each window, and obtain a time series of over 1,100 trading\nnetworks. We find that there are strongly simultaneous correlations between the\ntopological metrics (including network centralization, assortative index, and\naverage path length) of trading networks that characterize the patterns of\norder execution and the financial variables (including return, volatility,\nintertrade duration, and trading volume) for the stock and its warrant. Our\nanalysis may shed new lights on how the microscopic interactions between\nelements within complex system affect the system's performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.0925v1"
    },
    {
        "title": "Dynamic evolution of cross-correlations in the Chinese stock market",
        "authors": [
            "Fei Ren",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We study the dynamic evolution of cross-correlations in the Chinese stock\nmarket mainly based on the random matrix theory (RMT). The correlation matrices\nconstructed from the return series of 367 A-share stocks traded on the Shanghai\nStock Exchange from January 4, 1999 to December 30, 2011 are calculated over a\nmoving window with a size of 400 days. The evolutions of the statistical\nproperties of the correlation coefficients, eigenvalues, and eigenvectors of\nthe correlation matrices are carefully analyzed. We find that the stock\ncorrelations are significantly increased in the periods of two market crashes\nin 2001 and 2008, during which only five eigenvalues significantly deviate from\nthe random correlation matrix, and the systemic risk is higher in these\nvolatile periods than calm periods. By investigating the significant\ncontributors of the deviating eigenvectors in different moving windows, we\nobserve a dynamic evolution behavior in business sectors such as IT,\nelectronics, and real estate, which lead the rise (drop) before (after) the\ncrashes.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1154v2"
    },
    {
        "title": "A relative information approach to financial time series analysis using\n  binary $N$-grams dictionaries",
        "authors": [
            "Igor Borovikov",
            "Michael Sadovsky"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Here we present a novel approach to statistical analysis of financial time\nseries. The approach is based on $n$-grams frequency dictionaries derived from\nthe quantized market data. Such dictionaries are studied by evaluating their\ninformation capacity using relative entropy. A specific quantization of\n(originally continuous) financial data is considered: so called binary\nquantization. Possible applications of the proposed technique include market\nevent study with the $n$-grams of higher information value. The finite length\nof the input data presents certain computational and theoretical challenges\ndiscussed in the paper. also, some other versions of a quantization are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.2732v1"
    },
    {
        "title": "Portfolio return distributions: Sample statistics with non-stationary\n  correlations",
        "authors": [
            "Desislava Chetalova",
            "Thilo A. Schmitt",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We consider random vectors drawn from a multivariate normal distribution and\ncompute the sample statistics in the presence of non-stationary correlations.\nFor this purpose, we construct an ensemble of random correlation matrices and\naverage the normal distribution over this ensemble. The resulting distribution\ncontains a modified Bessel function of the second kind whose behavior differs\nsignificantly from the multivariate normal distribution, in the central part as\nwell as in the tails. This result is then applied to asset returns. We compare\nwith empirical return distributions using daily data from the Nasdaq Composite\nIndex in the period from 1992 to 2012. The comparison reveals good agreement,\nthe average portfolio return distribution describes the data well especially in\nthe central part of the distribution. This in turn confirms our ansatz to model\nthe non-stationarity by an ensemble average.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.3961v2"
    },
    {
        "title": "Apparent criticality and calibration issues in the Hawkes self-excited\n  point process model: application to high-frequency financial data",
        "authors": [
            "Vladimir Filimonov",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We present a careful analysis of possible issues on the application of the\nself-excited Hawkes process to high-frequency financial data. We carefully\nanalyze a set of effects leading to significant biases in the estimation of the\n\"criticality index\" n that quantifies the degree of endogeneity of how much\npast events trigger future events. We report a number of model biases that are\nintrinsic to the estimation of brnaching ratio (n) when using power law memory\nkernels. We demonstrate that the calibration of the Hawkes process on mixtures\nof pure Poisson process with changes of regime leads to completely spurious\napparent critical values for the branching ratio (n~1) while the true value is\nactually n=0. More generally, regime shifts on the parameters of the Hawkes\nmodel and/or on the generating process itself are shown to systematically lead\nto a significant upward bias in the estimation of the branching ratio. We also\ndemonstrate the importance of the preparation of the high-frequency financial\ndata and give special care to the decrease of quality of the timestamps of tick\ndata due to latency and grouping of messages to packets by the stock exchange.\nAltogether, our careful exploration of the caveats of the calibration of the\nHawkes process stresses the need for considering all the above issues before\nany conclusion can be sustained. In this respect, because the above effects are\nplaguing their analyses, the claim by Hardiman, Bercot and Bouchaud (2013) that\nfinancial market have been continuously functioning at or close to criticality\n(n~1) cannot be supported. In contrast, our previous results on E-mini S&P 500\nFutures Contracts and on major commodity future contracts are upheld.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.6756v3"
    },
    {
        "title": "Exponential and power laws in public procurement markets",
        "authors": [
            "Ladislav Kristoufek",
            "Jiri Skuhrovec"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  For the first time ever, we analyze a unique public procurement database,\nwhich includes information about a number of bidders for a contract, a final\nprice, an identification of a winner and an identification of a contracting\nauthority for each of more than 40,000 public procurements in the Czech\nRepublic between 2006 and 2011, focusing on the distributional properties of\nthe variables of interest. We uncover several scaling laws -- the exponential\nlaw for the number of bidders, and the power laws for the total revenues and\ntotal spendings of the participating companies, which even follows the Zipf's\nlaw for the 100 most spending institutions. We propose an analogy between\nextensive and non-extensive systems in physics and the public procurement\nmarket situations. Through an entropy maximization, such the analogy yields\nsome interesting results and policy implications with respect to the\nMaxwell-Boltzmann and Pareto distributions in the analyzed quantities.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.0218v1"
    },
    {
        "title": "Learning from the past, predicting the statistics for the future,\n  learning an evolving system",
        "authors": [
            "Daniel Levin",
            "Terry Lyons",
            "Hao Ni"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We bring the theory of rough paths to the study of non-parametric statistics\non streamed data. We discuss the problem of regression where the input variable\nis a stream of information, and the dependent response is also (potentially) a\nstream.\n  A certain graded feature set of a stream, known in the rough path literature\nas the signature, has a universality that allows formally, linear regression to\nbe used to characterise the functional relationship between independent\nexplanatory variables and the conditional distribution of the dependent\nresponse.\n  This approach, via linear regression on the signature of the stream, is\nalmost totally general, and yet it still allows explicit computation. The\ngrading allows truncation of the feature set and so leads to an efficient local\ndescription for streams (rough paths). In the statistical context this method\noffers potentially significant, even transformational dimension reduction.\n  By way of illustration, our approach is applied to stationary time series\nincluding the familiar AR model and ARCH model. In the numerical examples we\nexamined, our predictions achieve similar accuracy to the Gaussian Process (GP)\napproach with much lower computational cost especially when the sample size is\nlarge.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.0260v6"
    },
    {
        "title": "Long-term memory in electricity prices: Czech market evidence",
        "authors": [
            "Ladislav Kristoufek",
            "Petra Lunackova"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We analyze long-term memory properties of hourly prices of electricity in the\nCzech Republic between 2009 and 2012. As the dynamics of the electricity prices\nis dominated by cycles -- mainly intraday and daily -- we opt for the detrended\nfluctuation analysis, which is well suited for such specific series. We find\nthat the electricity prices are non-stationary but strongly mean-reverting\nwhich distinguishes them from other financial assets which are usually\ncharacterized as unit root series. Such description is attributed to specific\nfeatures of electricity prices, mainly to non-storability. Additionally, we\nargue that the rapid mean-reversion is due to the principles of electricity\nspot prices. These properties are shown to be stable across all studied years.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.0582v1"
    },
    {
        "title": "Commodity futures and market efficiency",
        "authors": [
            "Ladislav Kristoufek",
            "Miloslav Vosvrda"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We analyze the market efficiency of 25 commodity futures across various\ngroups -- metals, energies, softs, grains and other agricultural commodities.\nTo do so, we utilize recently proposed Efficiency Index to find that the most\nefficient of all the analyzed commodities is heating oil, closely followed by\nWTI crude oil, cotton, wheat and coffee. On the other end of the ranking, we\ndetect live cattle and feeder cattle. The efficiency is also found to be\ncharacteristic for specific groups of commodities -- energy commodities being\nthe most efficient and the other agricultural commodities (formed mainly of\nlivestock) the least efficient groups. We also discuss contributions of the\nlong-term memory, fractal dimension and approximate entropy to the total\ninefficiency. Last but not least, we come across the nonstandard relationship\nbetween the fractal dimension and Hurst exponent. For the analyzed dataset, the\nrelationship between these two is positive meaning that local persistence\n(trending) is connected to global anti-persistence. We attribute this to\nspecifics of commodity futures which might be predictable in a short term and\nlocally but in a long term, they return to their fundamental price.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1492v1"
    },
    {
        "title": "Statistical inference of co-movements of stocks during a financial\n  crisis",
        "authors": [
            "Takero Ibuki",
            "Shunsuke Higano",
            "Sei Suzuki",
            "Jun-ichi Inoue",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In order to figure out and to forecast the emergence phenomena of social\nsystems, we propose several probabilistic models for the analysis of financial\nmarkets, especially around a crisis. We first attempt to visualize the\ncollective behaviour of markets during a financial crisis through\ncross-correlations between typical Japanese daily stocks by making use of\nmulti- dimensional scaling. We find that all the two-dimensional points\n(stocks) shrink into a single small region when a economic crisis takes place.\nBy using the properties of cross-correlations in financial markets especially\nduring a crisis, we next propose a theoretical framework to predict several\ntime-series simultaneously. Our model system is basically described by a\nvariant of the multi-layered Ising model with random fields as non-stationary\ntime series. Hyper-parameters appearing in the probabilistic model are\nestimated by means of minimizing the 'cumulative error' in the past market\nhistory. The justification and validity of our approaches are numerically\nexamined for several empirical data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1871v1"
    },
    {
        "title": "Dependency Structure and Scaling Properties of Financial Time Series Are\n  Related",
        "authors": [
            "Raffaello Morales",
            "T. Di Matteo",
            "Tomaso Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We report evidence of a deep interplay between cross-correlations\nhierarchical properties and multifractality of New York Stock Exchange daily\nstock returns. The degree of multifractality displayed by different stocks is\nfound to be positively correlated to their depth in the hierarchy of\ncross-correlations. We propose a dynamical model that reproduces this\nobservation along with an array of other empirical properties. The structure of\nthis model is such that the hierarchical structure of heterogeneous risks plays\na crucial role in the time evolution of the correlation matrix, providing an\ninterpretation to the mechanism behind the interplay between cross-correlation\nand multifractality in financial markets, where the degree of multifractality\nof stocks is associated to their hierarchical positioning in the\ncross-correlation structure. Empirical observations reported in this paper\npresent a new perspective towards the merging of univariate multi scaling and\nmultivariate cross-correlation properties of financial time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.2411v1"
    },
    {
        "title": "The Relationship Between Stock Market Parameters and Interbank Lending\n  Market: an Empirical Evidence",
        "authors": [
            "Magomet Yandiev",
            "Alexander Pakhalov"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  The article presents calculations that prove practical importance of the\nearlier derived theoretical relationship between the interest rate on the\ninterbank credit market, volume of investment and the quantity of securities\ntradable on the stock exchange.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.5703v1"
    },
    {
        "title": "Fractal Markets Hypothesis and the Global Financial Crisis: Wavelet\n  Power Evidence",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We analyze whether the prediction of the fractal markets hypothesis about a\ndominance of specific investment horizons during turbulent times holds. To do\nso, we utilize the continuous wavelet transform analysis and obtained wavelet\npower spectra which give the crucial information about the variance\ndistribution across scales and its evolution in time. We show that the most\nturbulent times of the Global Financial Crisis can be very well characterized\nby the dominance of short investment horizons which is in hand with the\nassertions of the fractal markets hypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.1446v1"
    },
    {
        "title": "Measuring correlations between non-stationary series with DCCA\n  coefficient",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this short report, we investigate the ability of the DCCA coefficient to\nmeasure correlation level between non-stationary series. Based on a wide Monte\nCarlo simulation study, we show that the DCCA coefficient can estimate the\ncorrelation coefficient accurately regardless the strength of non-stationarity\n(measured by the fractional differencing parameter $d$). For a comparison, we\nalso report the results for the standard Pearson's correlation coefficient. The\nDCCA coefficient dominates the Pearson's coefficient for non-stationary series.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.3984v1"
    },
    {
        "title": "Stock returns versus trading volume: is the correspondence more general?",
        "authors": [
            "Rafal Rak",
            "Stanislaw Drozdz",
            "Jaroslaw Kwapien",
            "Pawel Oswiecimka"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  This paper presents a quantitative analysis of the relationship between the\nstock market returns and corresponding trading volumes using high- frequency\ndata from the Polish stock market. First, for stocks that were traded for\nsuffciently long period of time, we study the return and volume distributions\nand identify their consistency with the power-law functions. We find that, for\nmajority of stocks, the scaling exponents of both distri- butions are\nsystematically related by about a factor of 2 with the ones for the returns\nbeing larger. Second, we study the empirical price impact of trades of a given\nvolume and find that this impact can be well described by a square-root\ndependence: r(V) V^(1/2). We conclude that the prop- erties of data from the\nPolish market resemble those reported in literature concerning certain mature\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.7018v1"
    },
    {
        "title": "Detrending moving-average cross-correlation coefficient: Measuring\n  cross-correlations between non-stationary series",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In the paper, we introduce a new measure of correlation between possibly\nnon-stationary series. As the measure is based on the detrending moving-average\ncross-correlation analysis (DMCA), we label it as the DMCA coefficient\n$\\rho_{DMCA}(\\lambda)$ with a moving average window length $\\lambda$. We\nanalytically show that the coefficient ranges between -1 and 1 as a standard\ncorrelation does. In the simulation study, we show that the values of\n$\\rho_{DMCA}(\\lambda)$ very well correspond to the true correlation between the\nanalyzed series regardless the (non-)stationarity level. Dependence of the\nnewly proposed measure on other parameters -- correlation level, moving average\nwindow length and time series length -- is discussed as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.0657v1"
    },
    {
        "title": "Uncertain growth and the value of the future",
        "authors": [
            "Jaume Masoliver",
            "Miquel Montero",
            "Josep Perelló",
            "John Geanakoplos",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  For environmental problems such as global warming future costs must be\nbalanced against present costs. This is traditionally done using an exponential\nfunction with a constant discount rate, which reduces the present value of\nfuture costs. The result is highly sensitive to the choice of discount rate and\nhas generated a major controversy as to the urgency for immediate action. We\nstudy analytically several standard interest rate models from finance and\ncompare their properties to empirical data. From historical time series for\nnominal interest rates and inflation covering 14 countries over hundreds of\nyears, we find that extended periods of negative real interest rates are\ncommon, occurring in many epochs in all countries. This leads us to choose the\nOrnstein-Uhlenbeck model, in which real short run interest rates fluctuate\nstochastically and can become negative, even if they revert to a positive mean\nvalue. We solve the model in closed form and prove that the long-run discount\nrate is always less than the mean; indeed it can be zero or even negative,\ndespite the fact that the mean short term interest rate is positive. We fit the\nparameters of the model to the data, and find that nine of the countries have\npositive long run discount rates while five have negative long-run discount\nrates. Even if one rejects the countries where hyperinflation has occurred, our\nresults support the low discounting rate used in the Stern report over higher\nrates advocated by others.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4068v1"
    },
    {
        "title": "Skew and implied leverage effect: smile dynamics revisited",
        "authors": [
            "Vincent Vargas",
            "Tung-Lam Dao",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We revisit the ``Smile Dynamics'' problem, which consists in relating the\nimplied leverage (i.e. the correlation of the at-the-money volatility with the\nreturns of the underlying) and the skew of the option smile. The ratio between\nthese two quantities, called ``Skew-Stickiness Ratio'' (SSR) by Bergomi (Smile\nDynamics IV, RISK, 94-100, December 2009), saturates to the value 2 for linear\nmodels in the limit of small maturities, and converges to 1 for long\nmaturities. We show that for more general, non-linear models (such as the\nasymmetric GARCH model), Bergomi's result must be modified, and can be larger\nthan 2 for small maturities. The discrepancy comes from the fact that the\nvolatility skew is, in general, different from the skewness of the underlying.\nWe compare our theory with empirical results, using data both from option\nmarkets and from the underlying price series, for the S&P500 and the DAX. We\nfind, among other things, that although both the implied leverage and the skew\nappear to be too strong on option markets, their ratio is well explained by the\ntheory. We observe that the SSR indeed becomes larger than 2 for small\nmaturities.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4078v1"
    },
    {
        "title": "Conditional correlation in asset return and GARCH intensity model",
        "authors": [
            "Geon Ho Choe",
            "Kyungsub Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In an asset return series there is a conditional asymmetric dependence\nbetween current return and past volatility depending on the current return's\nsign. To take into account the conditional asymmetry, we introduce new models\nfor asset return dynamics in which frequencies of the up and down movements of\nasset price have conditionally independent Poisson distributions with\nstochastic intensities. The intensities are assumed to be stochastic recurrence\nequations of the GARCH type in order to capture the volatility clustering and\nthe leverage effect. We provide an important linkage between our model and\nexisting GARCH, explain how to apply maximum likelihood estimation to determine\nthe parameters in the intensity model and show empirical results with the S&P\n500 index return series.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4977v1"
    },
    {
        "title": "Probabilistic and statistical properties of moment variations and their\n  use in inference and estimation based on high frequency return data",
        "authors": [
            "Kyungsub Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We discuss the probabilistic properties of the variation based third and\nfourth moments of financial returns as estimators of the actual moments of the\nreturn distributions. The moment variations are defined under non-parametric\nassumptions with quadratic variation method but for the computational\ntractability, we use a square root stochastic volatility model for the\nderivations of moment conditions for estimations. Using the S\\&P 500 index high\nfrequency data, the realized versions of the moment variations is used for the\nestimation of a stochastic volatility model. We propose a simple estimation\nmethod of a stochastic volatility model using the sample averages of the\nvariations and ARMA estimation. In addition, we compare the results with a\ngeneralized method of moments estimation based on the successive relation\nbetween realized moments and their lagged values.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.5036v2"
    },
    {
        "title": "Emergent quantum mechanics of finances",
        "authors": [
            "Vadim Nastasiuk"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  This paper is an attempt at understanding the quantum-like dynamics of\nfinancial markets in terms of non-differentiable price-time continuum having\nfractal properties. The main steps of this development are the statistical\nscaling, the non-differentiability hypothesis, and the equations of motion\nentailed by this hypothesis. From perspective of the proposed theory the\ndynamics of S&P500 index are analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.3247v1"
    },
    {
        "title": "Semi-Markov Models in High Frequency Finance: A Review",
        "authors": [
            "G. D'Amico",
            "F. Petroni",
            "F. Prattico"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this paper we describe three stochastic models based on a semi-Markov\nchains approach and its generalizations to study the high frequency price\ndynamics of traded stocks. The three models are: a simple semi-Markov chain\nmodel, an indexed semi-Markov chain model and a weighted indexed semi-Markov\nchain model. We show, through Monte Carlo simulations, that the models are able\nto reproduce important stylized facts of financial time series as the\npersistence of volatility. In particular, we analyzed high frequency data from\nthe Italian stock market from the first of January 2007 until end of December\n2010 and we apply to it the semi-Markov chain model and the indexed semi-Markov\nchain model. The last model, instead, is applied to data from Italian and\nGerman stock markets from January 1, 2007 until the end of December 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.3894v1"
    },
    {
        "title": "Crossing Stocks and the Positive Grassmannian I: The Geometry behind\n  Stock Market",
        "authors": [
            "Ovidiu Racorean"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  It seems to be very unlikely that all relevant information in the stock\nmarket could be fully encoded in a geometrical shape. Still,the present paper\nwill reveal the geometry behind the stock market transactions. The prices of\nmarket index (DJIA) stock components are arranged in ascending order from the\nsmallest one in the left to the highest in the right. In such arrangement, as\nstock prices changes due to daily market quotations, it could be noticed that\nthe price of a certain stock get over /under the price of a neighbor stock.\nThese stocks are crossing. Arranged this way, the diagram of successive stock\ncrossings is nothing else than a permutation diagram. From this point on the\nfinancial and combinatorial concepts are netted together to build a bridge\nconnecting the stock market to a beautiful geometrical object that will be\ncalled stock market polytope. The stock market polytope is associated with the\nremarkable structure of positive Grassmannian . This procedure makes all the\nrelevant information about the stock market encoded in the geometrical shape of\nthe stock market polytope more readable.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1281v2"
    },
    {
        "title": "Are European equity markets efficient? New evidence from fractal\n  analysis",
        "authors": [
            "Enrico Onali",
            "John Goddard"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Fractal analysis is carried out on the stock market indices of seven European\ncountries and the US. We find evidence of long range dependence in the log\nreturn series of the Mibtel (Italy) and the PX Glob (Czech Republic). Long\nrange dependence implies that predictable patterns in the log returns do not\ndissipate quickly, and may therefore produce potential arbitrage opportunities.\nTherefore, these results are in contravention of the Efficient Market\nHypothesis. We show that correcting for short range dependence, or\nprefiltering, may dispose of genuine long range dependence, suggesting that the\nmarket is efficient in cases when it is not. Prefiltering does not reduce\nsignificantly the power of the tests only for cases for which the Hurst\nexponent (a measure of the long range dependence) lies well outside the\nboundaries of no long range dependence. For borderline cases, the prefiltering\nprocedure reduces the power of the test. On the other hand, the absence of\nprefiltering does not result in a test that is significantly oversized.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1440v1"
    },
    {
        "title": "Information ratio analysis of momentum strategies",
        "authors": [
            "Fernando F. Ferreira",
            "A. Christian Silva",
            "Ju-Yi Yen"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In the past 20 years, momentum or trend following strategies have become an\nestablished part of the investor toolbox. We introduce a new way of analyzing\nmomentum strategies by looking at the information ratio (IR, average return\ndivided by standard deviation). We calculate the theoretical IR of a momentum\nstrategy, and show that if momentum is mainly due to the positive\nautocorrelation in returns, IR as a function of the portfolio formation period\n(look-back) is very different from momentum due to the drift (average return).\nThe IR shows that for look-back periods of a few months, the investor is more\nlikely to tap into autocorrelation. However, for look-back periods closer to 1\nyear, the investor is more likely to tap into the drift. We compare the\nhistorical data to the theoretical IR by constructing stationary periods. The\nempirical study finds that there are periods/regimes where the autocorrelation\nis more important than the drift in explaining the IR (particularly pre-1975)\nand others where the drift is more important (mostly after 1975). We conclude\nour study by applying our momentum strategy to 100 plus years of the Dow-Jones\nIndustrial Average. We report damped oscillations on the IR for look-back\nperiods of several years and model such oscilations as a reversal to the mean\ngrowth rate.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.3030v2"
    },
    {
        "title": "Information-theoretic approach to lead-lag effect on financial markets",
        "authors": [
            "Paweł Fiedor"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Recently the interest of researchers has shifted from the analysis of\nsynchronous relationships of financial instruments to the analysis of more\nmeaningful asynchronous relationships. Both of those analyses are concentrated\nonly on Pearson's correlation coefficient and thus intraday lead-lag\nrelationships associated with such. Under Efficient Market Hypothesis such\nrelationships are not possible as all information is embedded in the prices. In\nthis paper we analyse lead-lag relationships of financial instruments and\nextend known methodology by using mutual information instead of Pearson's\ncorrelation coefficient, which not only is a more general measure, sensitive to\nnon-linear dependencies, but also can lead to a simpler procedure of\nstatistical validation of links between financial instruments. We analyse\nlagged relationships using NYSE 100 data not only on intraday level but also\nfor daily stock returns, which has usually been ignored.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.3820v1"
    },
    {
        "title": "Empirical symptoms of catastrophic bifurcation transitions on financial\n  markets: A phenomenological approach",
        "authors": [
            "M. Kozłowska",
            "T. Gubiec",
            "T. R. Werner",
            "M. Denys",
            "A. Sienkiewicz",
            "R. Kutner",
            "Z. Struzik"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  The principal aim of this work is the evidence on empirical way that\ncatastrophic bifurcation breakdowns or transitions, proceeded by flickering\nphenomenon, are present on notoriously significant and unpredictable financial\nmarkets. Overall, in this work we developed various metrics associated with\ncatastrophic bifurcation transitions, in particular, the catastrophic slowing\ndown (analogous to the critical slowing down). All these things were considered\non a well-defined example of financial markets of small and middle to large\ncapitalization. The catastrophic bifurcation transition seems to be connected\nwith the question of whether the early-warning signals are present in financial\nmarkets. This question continues to fascinate both the research community and\nthe general public. Interestingly, such early-warning signals have recently\nbeen identified and explained to be a consequence of a catastrophic bifurcation\ntransition phenomenon observed in multiple physical systems, e.g. in\necosystems, climate dynamics and in medicine (epileptic seizure and asthma\nattack). In the present work we provide an analogical, positive identification\nof such phenomenon by examining its several different indicators in the context\nof a well-defined daily bubble; this bubble was induced by the recent worldwide\nfinancial crisis on typical financial markets of small and middle to large\ncapitalization.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.4047v1"
    },
    {
        "title": "Leverage effect in energy futures",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We propose a comprehensive treatment of the leverage effect, i.e. the\nrelationship between returns and volatility of a specific asset, focusing on\nenergy commodities futures, namely Brent and WTI crude oils, natural gas and\nheating oil. After estimating the volatility process without assuming any\nspecific form of its behavior, we find the volatility to be long-term dependent\nwith the Hurst exponent on a verge of stationarity and non-stationarity.\nBypassing this using by using the detrended cross-correlation and the\ndetrending moving-average cross-correlation coefficients, we find the standard\nleverage effect for both crude oil. For heating oil, the effect is not\nstatistically significant, and for natural gas, we find the inverse leverage\neffect. Finally, we also show that none of the effects between returns and\nvolatility is detected as the long-term cross-correlated one. These findings\ncan be further utilized to enhance forecasting models and mainly in the risk\nmanagement and portfolio diversification.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.0064v1"
    },
    {
        "title": "Exchange Rate Predictability in a Changing World",
        "authors": [
            "Joseph Byrne",
            "Dimitris Korobilis",
            "Pinho Ribeiro"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  An expanding literature articulates the view that Taylor rules are helpful in\npredicting exchange rates. In a changing world however, Taylor rule parameters\nmay be subject to structural instabilities, for example during the Global\nFinancial Crisis. This paper forecasts exchange rates using such Taylor rules\nwith Time Varying Parameters (TVP) estimated by Bayesian methods. In core\nout-of-sample results, we improve upon a random walk benchmark for at least\nhalf, and for as many as eight out of ten, of the currencies considered. This\ncontrasts with a constant parameter Taylor rule model that yields a more\nlimited improvement upon the benchmark. In further results, Purchasing Power\nParity and Uncovered Interest Rate Parity TVP models beat a random walk\nbenchmark, implying our methods have some generality in exchange rate\nprediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.0627v1"
    },
    {
        "title": "Partial Mutual Information Analysis of Financial Networks",
        "authors": [
            "Paweł Fiedor"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  The econophysics approach to socio-economic systems is based on the\nassumption of their complexity. Such assumption inevitably lead to another\nassumption, namely that underlying interconnections within socio-economic\nsystems, particularly financial markets, are nonlinear, which is shown to be\ntrue even in mainstream economic literature. Thus it is surprising to see that\nnetwork analysis of financial markets is based on linear correlation and its\nderivatives. An analysis based on partial correlation is of particular interest\nas it leading to the vicinity of causality detection in time series analysis.\nIn this paper we generalise the Planar Maximally Filtered Graphs and Partial\nCorrelation Planar Graphs to incorporate nonlinearity using partial mutual\ninformation.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.2050v1"
    },
    {
        "title": "Empirical properties of inter-cancellation durations in the Chinese\n  stock market",
        "authors": [
            "Gao-Feng Gu",
            "Xiong Xiong",
            "Wei Zhang",
            "Yong-Jie Zhang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Order cancellation process plays a crucial role in the dynamics of price\nformation in order-driven stock markets and is important in the construction\nand validation of computational finance models. Based on the order flow data of\n18 liquid stocks traded on the Shenzhen Stock Exchange in 2003, we investigate\nthe empirical statistical properties of inter-cancellation durations in units\nof events defined as the waiting times between two consecutive cancellations.\nThe inter-cancellation durations for both buy and sell orders of all the stocks\nfavor a $q$-exponential distribution when the maximum likelihood estimation\nmethod is adopted; In contrast, both cancelled buy orders of 6 stocks and\ncancelled sell orders of 3 stocks prefer Weibull distribution when the\nnonlinear least-square estimation is used. Applying detrended fluctuation\nanalysis (DFA), centered detrending moving average (CDMA) and multifractal\ndetrended fluctuation analysis (MF-DFA) methods, we unveil that the\ninter-cancellation duration time series process long memory and multifractal\nnature for both buy and sell cancellations of all the stocks. Our findings show\nthat order cancellation processes exhibit long-range correlated bursty\nbehaviors and are thus not Poissonian.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.3478v1"
    },
    {
        "title": "Testing for Detailed Balance in a Financial Market",
        "authors": [
            "Rudolf Fiebig",
            "David Musgrove"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We test a historical price time series in a financial market (the NASDAQ 100\nindex) for a statistical property known as detailed balance. The presence of\ndetailed balance would imply that the market can be modeled by a stochastic\nprocess based on a Markov chain, thus leading to equilibrium. In economic\nterms, a positive outcome of the test would support the efficient market\nhypothesis, a cornerstone of neo-classical economic theory. In contrast to the\nusage in prevalent economic theory the term equilibrium here is tied to the\nreturns, rather than the price time series. The test is based on an action\nfunctional $S$ constructed from the elements of the detailed balance condition\nand the historical data set, and then analyzing $S$ by means of simulated\nannealing. Checks are performed to verify the validity of the analysis method.\nWe discuss the outcome of this analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.3584v1"
    },
    {
        "title": "Predicting market instability: New dynamics between volume and\n  volatility",
        "authors": [
            "Zeyu Zheng",
            "Zhi Qiao",
            "Joel N. Tenenbaum",
            "H. Eugene Stanley",
            "Baowen Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Econophysics and econometrics agree that there is a correlation between\nvolume and volatility in a time series. Using empirical data and their\ndistributions, we further investigate this correlation and discover new ways\nthat volatility and volume interact, particularly when the levels of both are\nhigh. We find that the distribution of the volume-conditional volatility is\nwell fit by a power-law function with an exponential cutoff. We find that the\nvolume-conditional volatility distribution scales with volume, and collapses\nthese distributions to a single curve. We exploit the characteristics of the\nvolume-volatility scatter plot to find a strong correlation between logarithmic\nvolume and a quantity we define as local maximum volatility (LMV), which\nindicates the largest volatility observed in a given range of trading volumes.\nThis finding supports our empirical analysis showing that volume is an\nexcellent predictor of the maximum value of volatility for both same-day and\nnear-future time periods. We also use a joint conditional probability that\nincludes both volatility and volume to demonstrate that invoking both allows us\nto better predict the largest next-day volatility than invoking either one\nalone.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5193v1"
    },
    {
        "title": "The Implied Volatility Analysis: The South African Experience",
        "authors": [
            "Romuald N. Kenmoe S",
            "Carine D. Tafou"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In this paper, we analyse the South African implied volatility in various\nsetting. We assess the information content in SAVI implied volatility using\ndaily markets data. Our empirical application is focused on the FTSE/JSE Top 40\nindex and we emphasize our models performance in distinct sub-periods. Our\nresults are compared with VIX/VXN and S&P 500/NASDAQ 100 data in some points\nwhich are taken as our benchmark. We find a significant negative relationship\nbetween returns and volatility, in line with the results found in other\nmarkets. Finally, the link between SAVI, VIX and VXN are undertaken to examine\nthe equity market transmission with respect to uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5965v1"
    },
    {
        "title": "The limits of statistical significance of Hawkes processes fitted to\n  financial data",
        "authors": [
            "Mehdi Lallouache",
            "Damien Challet"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Many fits of Hawkes processes to financial data look rather good but most of\nthem are not statistically significant. This raises the question of what part\nof market dynamics this model is able to account for exactly. We document the\naccuracy of such processes as one varies the time interval of calibration and\ncompare the performance of various types of kernels made up of sums of\nexponentials. Because of their around-the-clock opening times, FX markets are\nideally suited to our aim as they allow us to avoid the complications of the\nlong daily overnight closures of equity markets. One can achieve statistical\nsignificance according to three simultaneous tests provided that one uses\nkernels with two exponentials for fitting an hour at a time, and two or three\nexponentials for full days, while longer periods could not be fitted within\nstatistical satisfaction because of the non-stationarity of the endogenous\nprocess. Fitted timescales are relatively short and endogeneity factor is high\nbut sub-critical at about 0.8.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.3967v2"
    },
    {
        "title": "Zooming into market states",
        "authors": [
            "Desislava Chetalova",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We analyze the daily stock data of the Nasdaq Composite index in the 22-year\nperiod 1992-2013 and identify market states as clusters of correlation matrices\nwith similar correlation structures. We investigate the stability of the\ncorrelation structure of each state by estimating the statistical fluctuations\nof correlations due to their non-stationarity. Our study is based on a random\nmatrix approach recently introduced to model the non-stationarity of\ncorrelations by an ensemble of random matrices. This approach reduces the\ncomplexity of the correlated market to a single parameter which characterizes\nthe fluctuations of the correlations and can be determined directly from the\nempirical return distributions. This parameter provides an insight into the\nstability of the correlation structure of each market state as well as into the\ncorrelation structure dynamics in the whole observation period. The analysis\nreveals an intriguing relationship between average correlation and correlation\nfluctuations. The strongest fluctuations occur during periods of high average\ncorrelation which is the case particularly in times of crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5386v1"
    },
    {
        "title": "Liquidity commonality does not imply liquidity resilience commonality: A\n  functional characterisation for ultra-high frequency cross-sectional LOB data",
        "authors": [
            "Efstathios Panayi",
            "Gareth Peters",
            "Ioannis Kosmidis"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We present a large-scale study of commonality in liquidity and resilience\nacross assets in an ultra high-frequency (millisecond-timestamped) Limit Order\nBook (LOB) dataset from a pan-European electronic equity trading facility. We\nfirst show that extant work in quantifying liquidity commonality through the\ndegree of explanatory power of the dominant modes of variation of liquidity\n(extracted through Principal Component Analysis) fails to account for heavy\ntailed features in the data, thus producing potentially misleading results. We\nemploy Independent Component Analysis, which both decorrelates the liquidity\nmeasures in the asset cross-section, but also reduces higher-order statistical\ndependencies.\n  To measure commonality in liquidity resilience, we utilise a novel\ncharacterisation as the time required for return to a threshold liquidity\nlevel. This reflects a dimension of liquidity that is not captured by the\nmajority of liquidity measures and has important ramifications for\nunderstanding supply and demand pressures for market makers in electronic\nexchanges, as well as regulators and HFTs. When the metric is mapped out across\na range of thresholds, it produces the daily Liquidity Resilience Profile (LRP)\nfor a given asset. This daily summary of liquidity resilience behaviour from\nthe vast LOB dataset is then amenable to a functional data representation. This\nenables the comparison of liquidity resilience in the asset cross-section via\nfunctional linear sub-space decompositions and functional regression. The\nfunctional regression results presented here suggest that market factors for\nliquidity resilience (as extracted through functional principal components\nanalysis) can explain between 10 and 40% of the variation in liquidity\nresilience at low liquidity thresholds, but are less explanatory at more\nextreme levels, where individual asset factors take effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5486v1"
    },
    {
        "title": "Survival Models for the Duration of Bid-Ask Spread Deviations",
        "authors": [
            "Efstathios Panayi",
            "Gareth Peters"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Many commonly used liquidity measures are based on snapshots of the state of\nthe limit order book (LOB) and can thus only provide information about\ninstantaneous liquidity, and not regarding the local liquidity regime. However,\ntrading in the LOB is characterised by many intra-day liquidity shocks, where\nthe LOB generally recovers after a short period of time. In this paper, we\ncapture this dynamic aspect of liquidity using a survival regression framework,\nwhere the variable of interest is the duration of the deviations of the spread\nfrom a pre-specified level. We explore a large number of model structures using\na branch-and-bound subset selection algorithm and illustrate the explanatory\nperformance of our model.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5487v1"
    },
    {
        "title": "Hierarchical structure of the European countries based on debts as a\n  percentage of GDP during the 2000-2011 period",
        "authors": [
            "Ersin Kantar",
            "Bayram Deviren",
            "Mustafa Keskin"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We investigate hierarchical structures of the European countries by using\ndebt as a percentage of Gross Domestic Product (GDP) of the countries as they\nchange over a certain period of time. We obtain the topological properties\namong the countries based on debt as a percentage of GDP of European countries\nover the period 2000-2011 by using the concept of hierarchical structure\nmethods (minimal spanning tree, (MST) and hierarchical tree, (HT)). This period\nis also divided into two sub-periods related to 2004 enlargement of the\nEuropean Union, namely 2000-2004 and 2005-2011, in order to test various\ntime-window and observe the temporal evolution. The bootstrap techniques is\napplied to see a value of statistical reliability of the links of the MSTs and\nHTs. The clustering linkage procedure is also used to observe the cluster\nstructure more clearly. From the structural topologies of these trees, we\nidentify different clusters of countries according to their level of debts and\neconomic ties. Our results show that by the debt crisis, the less and most\naffected Eurozones economies are formed as a cluster with each other in the\nMSTs and hierarchical trees.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.6559v2"
    },
    {
        "title": "Hierarchical structure of the countries based on electricity consumption\n  and economic growth",
        "authors": [
            "Ersin Kantar",
            "Alper Aslan",
            "Bayram Deviren",
            "Mustafa Keskin"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We investigate the hierarchical structures of countries based on electricity\nconsumption and economic growth by using the real amounts of their consumption\nover a certain time period. We use of electricity consumption data to detect\nthe topological properties of 60 countries from 1971 to 2008. These countries\nare divided into three subgroups: low income group, middle income group and\nhigh income group countries. Firstly, a relationship between electricity\nconsumption and economic growth is investigated by using the concept of\nhierarchical structure methods (minimal spanning tree (MST) and hierarchical\ntree (HT)). Secondly, we perform bootstrap techniques to investigate a value of\nthe statistical reliability to the links of the MST. Finally, we use a\nclustering linkage procedure in order to observe the cluster structure more\nclearly. The results of the structural topologies of these trees are as\nfollows: i) we identified different clusters of countries according to their\ngeographical location and economic growth, ii) we found a strong relation\nbetween energy consumption and economic growth for all the income groups\nconsidered in this study and iii) the results are in good agreement with the\ncausal relationship between electricity consumption and economic growth.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.6562v2"
    },
    {
        "title": "Hierarchical Structure of the Foreign Trade: The Case of the United\n  State",
        "authors": [
            "Ersin Kantar"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  This study uses hierarchical structure methods (minimal spanning tree, (MST)\nand hierarchical tree, (HT)) to examine the hierarchical structures of the\nUnited State (US) foreign trade by using the real prices of their commodity\nexport and import move together over time. We obtain the topological properties\namong the countries based on US foreign trade over the periods of 1985-2011. We\nalso perform the bootstrap techniques to investigate a value of the statistical\nreliability to the links of the MSTs. Finally, we use a clustering linkage\nprocedure in order to observe the cluster structure much better. The results of\nthe topologies structural of these trees are as follows: i) We identified\ndifferent clusters of countries according to their geographical location and\neconomic growth. ii) Our results show that the European Union and Asian\ncountries are more important within the network, due to a tighter connection\nwith other countries. The country's most important trading partners are the\nCanada, China, Mexico, Japan, Germany, United Kingdom, South Korea, France,\nTaiwan, India, Singapore and Netherlands iii) We have also found that these\ncountries play a significance role for US foreign trade and have important\nimplications for the design of portfolio and investment strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.7064v1"
    },
    {
        "title": "Dynamics in two networks based on stocks of the US stock market",
        "authors": [
            "Leonidas Sandoval Junior"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We follow the main stocks belonging to the New York Stock Exchange and to\nNasdaq from 2003 to 2012, through years of normality and of crisis, and study\nthe dynamics of networks built on two measures expressing relations between\nthose stocks: correlation, which is symmetric and measures how similar two\nstocks behave, and Transfer Entropy, which is non-symmetric and measures the\ninfluence of the time series of one stock onto another in terms of the\ninformation that the time series of one stock transmits to the time series of\nanother stock. The two measures are used in the creation of two networks that\nevolve in time, revealing how the relations between stocks and industrial\nsectors changed in times of crisis. The two networks are also used in\nconjunction with a dynamic model of the spreading of volatility in order to\ndetect which are the stocks that are most likely to spread crises, according to\nthe model. This information may be used in the building of policies aiming to\nreduce the effect of financial crises.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.1728v2"
    },
    {
        "title": "Recurrence plots of exchange rates of currencies",
        "authors": [
            "Amelia Carolina Sparavigna"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Used to investigate the presence of distinctive recurrent behaviours in\nnatural processes, the recurrence plots can be applied to the analysis of\neconomic data, and, in particular, to the characterization of exchange rates of\ncurrencies too. In this paper, we will show that these plots are able to\ncharacterize the periods of oscillation and random walk of currencies and\nenhance their reply to news and events, by means of texture transitions. The\nexamples of recurrence plots given here are obtained from time series of\nexchange rates of Euro.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.4746v1"
    },
    {
        "title": "Incorporating Views on Marginal Distributions in the Calibration of Risk\n  Models",
        "authors": [
            "Santanu Dey",
            "Sandeep Juneja",
            "Karthyek R. A. Murthy"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Entropy based ideas find wide-ranging applications in finance for calibrating\nmodels of portfolio risk as well as options pricing. The abstracted problem,\nextensively studied in the literature, corresponds to finding a probability\nmeasure that minimizes relative entropy with respect to a specified measure\nwhile satisfying constraints on moments of associated random variables. These\nmoments may correspond to views held by experts in the portfolio risk setting\nand to market prices of liquid options for options pricing models. However, it\nis reasonable that in the former settings, the experts may have views on tails\nof risks of some securities. Similarly, in options pricing, significant\nliterature focuses on arriving at the implied risk neutral density of benchmark\ninstruments through observed market prices. With the intent of calibrating\nmodels to these more general stipulations, we develop a unified entropy based\nmethodology to allow constraints on both moments as well as marginal\ndistributions of functions of underlying securities. This is applied to\nMarkowitz portfolio framework, where a view that a particular portfolio incurs\nheavy tailed losses is shown to lead to fatter and more reasonable tails for\nlosses of component securities. We also use this methodology to price\nnon-traded options using market information such as observed option prices and\nimplied risk neutral densities of benchmark instruments.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.0570v1"
    },
    {
        "title": "Trend and Fractality Assessment of Mexico's Stock Exchange",
        "authors": [
            "Javier Morales",
            "Víctor Tercero",
            "Fernando Camacho",
            "Eduardo Cordero",
            "Luis López",
            "F-Javier Almaguer"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  The total value of domestic market capitalization of the Mexican Stock\nExchange was calculated at 520 billion of dollars by the end of November 2013.\nTo manage this system and make optimum capital investments, its dynamics needs\nto be predicted. However, randomness within the stock indexes makes forecasting\na difficult task. To address this issue, in this work, trends and fractality\nwere studied using GNU-R over the opening and closing prices indexes over the\npast 23 years. Returns, Kernel density estimation, autocorrelation function and\nR/S analysis and the Hurst exponent were used in this research. As a result, it\nwas found that the Kernel estimation density and the autocorrelation function\nshown the presence of long-range memory effects. In a first approximation, the\nreturns of closing prices seems to behave according to a Markovian random walk\nwith a length of step size given by an alpha-stable random process. For extreme\nvalues, returns decay asymptotically as a power law with a characteristic\nexponent approximately equal to 2.5.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.3399v1"
    },
    {
        "title": "Modelling of dependence in high-dimensional financial time series by\n  cluster-derived canonical vines",
        "authors": [
            "David Walsh-Jones",
            "Daniel Jones",
            "Christoph Reisinger"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We extend existing models in the financial literature by introducing a\ncluster-derived canonical vine (CDCV) copula model for capturing high\ndimensional dependence between financial time series. This model utilises a\nsimplified market-sector vine copula framework similar to those introduced by\nHeinen and Valdesogo (2008) and Brechmann and Czado (2013), which can be\napplied by conditioning asset time series on a market-sector hierarchy of\nindexes. While this has been shown by the aforementioned authors to control the\nexcessive parameterisation of vine copulas in high dimensions, their models\nhave relied on the provision of externally sourced market and sector indexes,\nlimiting their wider applicability due to the imposition of restrictions on the\nnumber and composition of such sectors. By implementing the CDCV model, we\ndemonstrate that such reliance on external indexes is redundant as we can\nachieve equivalent or improved performance by deriving a hierarchy of indexes\ndirectly from a clustering of the asset time series, thus abstracting the\nmodelling process from the underlying data.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4970v1"
    },
    {
        "title": "Competition of Commodities for the Status of Money in an Agent-based\n  Model",
        "authors": [
            "Robert Gębarowski",
            "Stanisław Drożdż",
            "Andrzej Z. Górski",
            "Paweł Oświęcimka"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In this model study of the commodity market, we present some evidence of\ncompetition of commodities for the status of money in the regime of parameters,\nwhere emergence of money is possible. The competition reveals itself as a\nrivalry of a few (typically two) dominant commodities, which take the status of\nmoney in turn.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.2124v1"
    },
    {
        "title": "Financial Time Series: Stylized Facts for the Mexican Stock Exchange\n  Index Compared to Developed Markets",
        "authors": [
            "Omar Rojas",
            "Carlos Trejo-Pech"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We present some stylized facts exhibited by the time series of returns of the\nMexican Stock Exchange Index (IPC) and compare them to a sample of both\ndeveloped (USA, UK and Japan) and emerging markets (Brazil and India). The\nperiod of study is 1997-2011. The stylized facts are related mostly to the\nprobability distribution func- tion and the autocorrelation function (e.g. fat\ntails, non-normality, volatility cluster- ing, among others). We find that\npositive skewness for returns in Mexico and Brazil, but not in the rest,\nsuggest investment opportunities. Evidence of nonlinearity is also documented.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.3126v1"
    },
    {
        "title": "Adaptive Market Efficiency of Agricultural Commodity Futures Contracts",
        "authors": [
            "Semei Coronado-Ramírez",
            "Pedro Celso-Arellano",
            "Omar Rojas"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In this paper we investigate the adaptive market efficiency of the\nagricultural commodity futures market, using a sample of eight futures\ncontracts. Using a battery of nonlinear tests, we uncover the nonlinear serial\ndependence in the returns series. We run the Hinich portmanteau bicorrelation\ntest to uncover the moments in which the nonlinear serial dependence, and\ntherefore adaptive market efficiency, occurs for our sample.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.8017v3"
    },
    {
        "title": "Statistical Properties and Pre-hit Dynamics of Price Limit Hits in the\n  Chinese Stock Markets",
        "authors": [
            "Yu-Lei Wan",
            "Wen-Jie Xie",
            "Gao-Feng Gu",
            "Zhi-Qiang Jiang",
            "Wei Chen",
            "Xiong Xiong",
            "Wei Zhang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Price limit trading rules are adopted in some stock markets (especially\nemerging markets) trying to cool off traders' short-term trading mania on\nindividual stocks and increase market efficiency. Under such a microstructure,\nstocks may hit their up-limits and down-limits from time to time. However, the\nbehaviors of price limit hits are not well studied partially due to the fact\nthat main stock markets such as the US markets and most European markets do not\nset price limits. Here, we perform detailed analyses of the high-frequency data\nof all A-share common stocks traded on the Shanghai Stock Exchange and the\nShenzhen Stock Exchange from 2000 to 2011 to investigate the statistical\nproperties of price limit hits and the dynamical evolution of several important\nfinancial variables before stock price hits its limits. We compare the\nproperties of up-limit hits and down-limit hits. We also divide the whole\nperiod into three bullish periods and three bearish periods to unveil possible\ndifferences during bullish and bearish market states. To uncover the impacts of\nstock capitalization on price limit hits, we partition all stocks into six\nportfolios according to their capitalizations on different trading days. We\nfind that the price limit trading rule has a cooling-off effect (object to the\nmagnet effect), indicating that the rule takes effect in the Chinese stock\nmarkets. We find that price continuation is much more likely to occur than\nprice reversal on the next trading day after a limit-hitting day, especially\nfor down-limit hits, which has potential practical values for market\npractitioners.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.03548v1"
    },
    {
        "title": "A study of co-movements between USA and Latin American stock markets: a\n  cross-bicorrelations perspective",
        "authors": [
            "Semei Coronado",
            "Omar Rojas",
            "Rafael Romero-Meza",
            "Francisco Venegas-Martinez"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  In this paper we use the Brooks and Hinich cross-bicorrelation test in order\nto uncover nonlinear dependence periods between USA Standard and Poor 500\n(SP500), used as benchmark, and six Latin American stock markets indexes:\nMexico (BMV), Brazil (BOVESPA), Chile (IPSA), Colombia (COLCAP), Peru (IGBVL)\nand Argentina (MERVAL). We have found windows of nonlinear dependence and\nco-movement between the SP500 and the Latin American stock markets, some of\nwhich coincide with periods of crisis, giving way to a possible contagion or\ninterdependence interpretation.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.06926v1"
    },
    {
        "title": "Observability of Market Daily Volatility",
        "authors": [
            "Filippo Petroni",
            "Maurizio Serva"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We study the price dynamics of 65 stocks from the Dow Jones Composite Average\nfrom 1973 until 2014. We show that it is possible to define a Daily Market\nVolatility $\\sigma(t)$ which is directly observable from data. This quantity is\nusually indirectly defined by $r(t)=\\sigma(t) \\omega(t)$ where the $r(t)$ are\nthe daily returns of the market index and the $\\omega(t)$ are i.i.d. random\nvariables with vanishing average and unitary variance. The relation\n$r(t)=\\sigma(t) \\omega(t)$ alone is unable to give an operative definition of\nthe index volatility, which remains unobservable. On the contrary, we show that\nusing the whole information available in the market, the index volatility can\nbe operatively defined and detected.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08032v1"
    },
    {
        "title": "Dependence structure of market states",
        "authors": [
            "Desislava Chetalova",
            "Marcel Wollschläger",
            "Rudi Schäfer"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We study the dependence structure of market states by estimating empirical\npairwise copulas of daily stock returns. We consider both original returns,\nwhich exhibit time-varying trends and volatilities, as well as locally\nnormalized ones, where the non-stationarity has been removed. The empirical\npairwise copula for each state is compared with a bivariate K-copula. This\ncopula arises from a recently introduced random matrix model, in which\nnon-stationary correlations between returns are modeled by an ensemble of\nrandom matrices. The comparison reveals overall good agreement between\nempirical and analytical copulas, especially for locally normalized returns.\nStill, there are some deviations in the tails. Furthermore, we find an\nasymmetry in the dependence structure of market states. The empirical pairwise\ncopulas exhibit a stronger lower tail dependence, particularly in times of\ncrisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.09004v2"
    },
    {
        "title": "Fisher information and quantum mechanical models for finance",
        "authors": [
            "Vadim Nastasiuk"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The probability distribution function (PDF) for prices on financial markets\nis derived by extremization of Fisher information. It is shown how on that\nbasis the quantum-like description for financial markets arises and different\nfinancial market models are mapped by quantum mechanical ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.03822v1"
    },
    {
        "title": "Lead-Lag Relationship using a Stop-and-Reverse-MinMax Process",
        "authors": [
            "Stanislaus Maier-Paape",
            "Andreas Platen"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The intermarket analysis, in particular the lead-lag relationship, plays an\nimportant role within financial markets. Therefore a mathematical approach to\nbe able to find interrelations between the price development of two different\nfinancial underlyings is developed in this paper. Computing the differences of\nthe relative positions of relevant local extrema of two charts, i.e., the local\nphase shifts of these underlyings, gives us an empirical distribution on the\nunit circle. With the aid of directional statistics such angular distributions\nare studied for many pairs of markets. It is shown that there are several very\nstrongly correlated underlyings in the field of foreign exchange, commodities\nand indexes. In some cases one of the two underlyings is significantly ahead\nwith respect to the relevant local extrema, i.e., there is a phase shift\nunequal to zero between these two underlyings.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.06235v1"
    },
    {
        "title": "Collective synchronization and high frequency systemic instabilities in\n  financial markets",
        "authors": [
            "Lucio Maria Calcagnile",
            "Giacomo Bormetti",
            "Michele Treccani",
            "Stefano Marmi",
            "Fabrizio Lillo"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Recent years have seen an unprecedented rise of the role that technology\nplays in all aspects of human activities. Unavoidably, technology has heavily\nentered the Capital Markets trading space, to the extent that all major\nexchanges are now trading exclusively using electronic platforms. The ultra\nfast speed of information processing, order placement, and cancelling generates\nnew dynamics which is still not completely deciphered. Analyzing a large\ndataset of stocks traded on the US markets, our study evidences that since 2001\nthe level of synchronization of large price movements across assets has\nsignificantly increased. Even though the total number of over-threshold events\nhas diminished in recent years, when an event occurs, the average number of\nassets swinging together has increased. Quite unexpectedly, only a minor\nfraction of these events -- regularly less than 40% along all years -- can be\nconnected with the release of pre-announced macroeconomic news. We also\ndocument that the larger is the level of sistemicity of an event, the larger is\nthe probability -- and degree of sistemicity -- that a new event will occur in\nthe near future. This opens the way to the intriguing idea that systemic events\nemerge as an effect of a purely endogenous mechanism. Consistently, we present\na high-dimensional, yet parsimonious, model based on a class of self- and\ncross-exciting processes, termed Hawkes processes, which reconciles the\nmodeling effort with the empirical evidence.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.00704v1"
    },
    {
        "title": "Dynamic Multi-Factor Clustering of Financial Networks",
        "authors": [
            "Gordon J. Ross"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We investigate the tendency for financial instruments to form clusters when\nthere are multiple factors influencing the correlation structure. Specifically,\nwe consider a stock portfolio which contains companies from different\nindustrial sectors, located in several different countries. Both sector\nmembership and geography combine to create a complex clustering structure where\ncompanies seem to first be divided based on sector, with geographical\nsubclusters emerging within each industrial sector. We argue that standard\ntechniques for detecting overlapping clusters and communities are not able to\ncapture this type of structure, and show how robust regression techniques can\ninstead be used to remove the influence of both sector and geography from the\ncorrelation matrix separately. Our analysis reveals that prior to the 2008\nfinancial crisis, companies did not tend to form clusters based on geography.\nThis changed immediately following the crisis, with geography becoming a more\nimportant determinant of clustering.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01550v1"
    },
    {
        "title": "Variance Dynamics - An empirical journey",
        "authors": [
            "Florent Ségonne"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We investigate the joint dynamics of spot and implied volatility from an\nempirical perspective. We focus on the equity market with the SPX Index our\nunderlying of choice. Using only observable quantities, we extract the\ninstantaneous variance curves implied by the market and study their daily\nvariations jointly with spot returns. We analyze the characteristics of their\nindividual and joint densities, quantify the non-linear relationship between\nspot and volatility, and discuss the modeling implications on the implied\nleverage and the volatility clustering effects. We show that non-linearities\nhave little impact on the dynamics of at-the-money volatilities, but can have a\nsignificant effect on the pricing and hedging of volatility derivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.00846v1"
    },
    {
        "title": "Estimation of integrated quadratic covariation with endogenous sampling\n  times",
        "authors": [
            "Yoann Potiron",
            "Per Mykland"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  When estimating high-frequency covariance (quadratic covariation) of two\narbitrary assets observed asynchronously, simple assumptions, such as\nindependence, are usually imposed on the relationship between the prices\nprocess and the observation times. In this paper, we introduce a general\nendogenous two-dimensional nonparametric model. Because an observation is\ngenerated whenever an auxiliary process called observation time process hits\none of the two boundary processes, it is called the hitting boundary process\nwith time process (HBT) model. We establish a central limit theorem for the\nHayashi-Yoshida (HY) estimator under HBT in the case where the price process\nand the observation price process follow a continuous Ito process. We obtain an\nasymptotic bias. We provide an estimator of the latter as well as a\nbias-corrected HY estimator of the high-frequency covariance. In addition, we\ngive a consistent estimator of the associated standard error.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.01033v5"
    },
    {
        "title": "Bifurcation patterns of market regime transition",
        "authors": [
            "Sergey Kamenshchikov"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  In this paper mechanisms of reversion - momentum transition are considered.\nTwo basic nonlinear mechanisms are highlighted: a slow and fast bifurcation. A\nslow bifurcation leads to the equilibrium evolution, preceded by stability loss\ndelay of a control parameter. A single order parameter is introduced by\nMarkovian chain diffusion, which plays a role of a precursor. A fast\nbifurcation is formed by a singular fusion of unstable and stable equilibrium\nstates. The effect of a precatastrophic range compression is observed before\nthe discrete change of a system. A diffusion time scaling is presented as a\nprecursor of the fast bifurcation. The efficiency of both precursors in a\ncurrency market was illustrated by simulation of a prototype of a trading\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.03141v2"
    },
    {
        "title": "Analysis of cyclical behavior in time series of stock market returns",
        "authors": [
            "Djordje Stratimirovic",
            "Darko Sarvan",
            "Vladimir Miljkovic",
            "Suzana Blesic"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  In this paper we have analyzed scaling properties and cyclical behavior of\nthe three types of stock market indexes (SMI) time series: data belonging to\nstock markets of developed economies, emerging economies, and of the\nunderdeveloped or transitional economies. We have used two techniques of data\nanalysis to obtain and verify our findings: the wavelet spectral analysis to\nstudy SMI returns data, and the Hurst exponent formalism to study local\nbehavior around market cycles and trends. We have found cyclical behavior in\nall SMI data sets that we have analyzed. Moreover, the positions and the\nboundaries of cyclical intervals that we have found seam to be common for all\nmarkets in our dataset. We list and illustrate the presence of nine such\nperiods in our SMI data. We also report on the possibilities to differentiate\nbetween the level of growth of the analyzed markets by way of statistical\nanalysis of the properties of wavelet spectra that characterize particular peak\nbehaviors. Our results show that measures like the relative WT energy content\nand the relative WT amplitude for the peaks in the small scales region could be\nused for partial differentiation between market economies. Finally, we propose\na way to quantify the level of development of a stock market based on the Hurst\nscaling exponent approach. From the local scaling exponents calculated for our\nnine peak regions we have defined what we named the Development Index, which\nproved, at least in the case of our dataset, to be suitable to rank the SMI\nseries that we have analyzed in three distinct groups.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.03378v4"
    },
    {
        "title": "Statistical Emulators for Pricing and Hedging Longevity Risk Products",
        "authors": [
            "James Risk",
            "Michael Ludkovski"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We propose the use of statistical emulators for the purpose of valuing\nmortality-linked contracts in stochastic mortality models. Such models\ntypically require (nested) evaluation of expected values of nonlinear\nfunctionals of multi-dimensional stochastic processes. Except in the simplest\ncases, no closed-form expressions are available, necessitating numerical\napproximation. Rather than building ad hoc analytic approximations, we advocate\nthe use of modern statistical tools from machine learning to generate a\nflexible, non-parametric surrogate for the true mappings. This method allows\nperformance guarantees regarding approximation accuracy and removes the need\nfor nested simulation. We illustrate our approach with case studies involving\n(i) a Lee-Carter model with mortality shocks, (ii) index-based static hedging\nwith longevity basis risk; (iii) a Cairns-Blake-Dowd stochastic survival\nprobability model.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.00310v2"
    },
    {
        "title": "Designating market maker behaviour in Limit Order Book markets",
        "authors": [
            "Efstathios Panayi",
            "Gareth W. Peters",
            "Jon Danielsson",
            "Jean-Pierre Zigrand"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Financial exchanges provide incentives for limit order book (LOB) liquidity\nprovision to certain market participants, termed designated market makers or\ndesignated sponsors. While quoting requirements typically enforce the activity\nof these participants for a certain portion of the day, we argue that liquidity\ndemand throughout the trading day is far from uniformly distributed, and thus\nthis liquidity provision may not be calibrated to the demand. We propose that\nquoting obligations also include requirements about the speed of liquidity\nreplenishment, and we recommend use of the Threshold Exceedance Duration (TED)\nfor this purpose. We present a comprehensive regression modelling approach\nusing GLM and GAMLSS models to relate the TED to the state of the LOB and\nidentify the regression structures that are best suited to modelling the TED.\nSuch an approach can be used by exchanges to set target levels of liquidity\nreplenishment for designated market makers.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.04348v1"
    },
    {
        "title": "Currency target zone modeling: An interplay between physics and\n  economics",
        "authors": [
            "Sandro Claudio Lera",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We study the performance of the euro/Swiss franc exchange rate in the\nextraordinary period from September 6, 2011 and January 15, 2015 when the Swiss\nNational Bank enforced a minimum exchange rate of 1.20 Swiss francs per euro.\nBased on the analogy between Brownian motion in finance and physics, the\nfirst-order effect of such a steric constraint would enter a priori in the form\nof a repulsive entropic force associated with the paths crossing the barrier\nthat are forbidden. Non-parametric empirical estimates of drift and volatility\nshow that the predicted first-order analogy between economics and physics are\nincorrect. The clue is to realise that the random walk nature of financial\nprices results from the continuous anticipations of traders about future\nopportunities, whose aggregate actions translate into an approximate efficient\nmarket with almost no arbitrage opportunities. With the Swiss National Bank\nstated commitment to enforce the barrier, traders's anticipation of this action\nleads to a vanishing drift together with a volatility of the exchange rate that\ndepends on the distance to the barrier. We give direct quantitative empirical\nevidence that this effect is well described by Krugman's target zone model\n[P.R. Krugman. The Quarterly Journal of Economics, 106(3):669-682, 1991].\nMotivated by the insights from this economical model, we revise the initial\neconomics-physics analogy and show that, within the context of hindered\ndiffusion, the two systems can be described with the same mathematics after\nall. Using a recently proposed extended analogy in terms of a colloidal\nBrownian particle embedded in a fluid of molecules associated with the\nunderlying order book, we derive that, close to the restricting boundary, the\ndynamics of both systems is described by a stochastic differential equation\nwith a very small constant drift and a linear diffusion coefficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.04754v2"
    },
    {
        "title": "Time-dependent scaling patterns in high frequency financial data",
        "authors": [
            "Noemi Nava",
            "Tiziana Di Matteo",
            "Tomaso Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We measure the influence of different time-scales on the dynamics of\nfinancial market data. This is obtained by decomposing financial time series\ninto simple oscillations associated with distinct time-scales. We propose two\nnew time-varying measures: 1) an amplitude scaling exponent and 2) an\nentropy-like measure. We apply these measures to intraday, 30-second sampled\nprices of various stock indices. Our results reveal intraday trends where\ndifferent time-horizons contribute with variable relative amplitudes over the\ncourse of the trading day. Our findings indicate that the time series we\nanalysed have a non-stationary multifractal nature with predominantly\npersistent behaviour at the middle of the trading session and anti-persistent\nbehaviour at the open and close. We demonstrate that these deviations are\nstatistically significant and robust.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.07428v2"
    },
    {
        "title": "Forecasting Electricity Spot Prices using Lasso: On Capturing the\n  Autoregressive Intraday Structure",
        "authors": [
            "Florian Ziel"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  In this paper we present a regression based model for day-ahead electricity\nspot prices. We estimate the considered linear regression model by the lasso\nestimation method. The lasso approach allows for many possible parameters in\nthe model, but also shrinks and sparsifies the parameters automatically to\navoid overfitting. Thus, it is able to capture the autoregressive intraday\ndependency structure of the electricity price well. We discuss in detail the\nestimation results which provide insights to the intraday behavior of\nelectricity prices. We perform an out-of-sample forecasting study for several\nEuropean electricity markets. The results illustrate well that the efficient\nlasso based estimation technique can exhibit advantages from two popular model\napproaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.01966v2"
    },
    {
        "title": "Measuring multiscaling in financial time-series",
        "authors": [
            "Riccardo Junior Buonocore",
            "Tomaso Aste",
            "Tiziana Di Matteo"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We discuss the origin of multiscaling in financial time-series and\ninvestigate how to best quantify it. Our methodology consists in separating the\ndifferent sources of measured multifractality by analysing the\nmulti/uni-scaling behaviour of synthetic time-series with known properties. We\nuse the results from the synthetic time-series to interpret the measure of\nmultifractality of real log-returns time-series. The main finding is that the\naggregation horizon of the returns can introduce a strong bias effect on the\nmeasure of multifractality. This effect can become especially important when\nreturns distributions have power law tails with exponents in the range [2,5].\nWe discuss the right aggregation horizon to mitigate this bias.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.05471v2"
    },
    {
        "title": "Universality of market superstatistics",
        "authors": [
            "Mateusz Denys",
            "Maciej Jagielski",
            "Tomasz Gubiec",
            "Ryszard Kutner",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We use a continuous-time random walk (CTRW) to model market fluctuation data\nfrom times when traders experience excessive losses or excessive profits. We\nanalytically derive \"superstatistics\" that accurately model empirical market\nactivity data (supplied by Bogachev, Ludescher, Tsallis, and Bunde)that exhibit\ntransition thresholds. We measure the interevent times between excessive losses\nand excessive profits, and use the mean interevent time as a control variable\nto derive a universal description of empirical data collapse. Our\nsuperstatistic value is a weighted sum of two components, (i) a powerlaw\ncorrected by the lower incomplete gamma function, which asymptotically tends\ntoward robustness but initially gives an exponential, and (ii) a powerlaw\ndamped by the upper incomplete gamma function, which tends toward the power-law\nonly during short interevent times. We find that the scaling shape exponents\nthat drive both components subordinate themselves and a \"superscaling\"\nconfiguration emerges. We use superstatistics to describe the hierarchical\nactivity when component (i) reproduces the negative feedback and component (ii)\nreproduces the stylized fact of volatility clustering. Our results indicate\nthat there is a functional (but not literal) balance between excessive profits\nand excessive losses that can be described using the same body of\nsuperstatistics, but different calibration values and driving parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.06315v1"
    },
    {
        "title": "Asymmetry of cross correlations between intra-day and overnight\n  volatilities",
        "authors": [
            "Rubina Zadourian",
            "Peter Grassberger"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We point out a stunning time asymmetry in the short time cross correlations\nbetween intra-day and overnight volatilities (absolute values of log-returns of\nstock prices). While overnight volatility is significantly (and positively)\ncorrelated with the intra-day volatility during the \\textit{following} day\n(allowing thus non-trivial predictions), it is much less correlated with the\nintra-day volatility during the \\textit{preceding} day. While the effect is not\nunexpected in view of previous observations, its robustness and extreme\nsimplicity are remarkable.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.08079v1"
    },
    {
        "title": "Seasonalities and cycles in time series: A fresh look with computer\n  experiments",
        "authors": [
            "Michel Fliess",
            "Cédric Join"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Recent advances in the understanding of time series permit to clarify\nseasonalities and cycles, which might be rather obscure in today's literature.\nA theorem due to P. Cartier and Y. Perrin, which was published only recently,\nin 1995, and several time scales yield, perhaps for the first time, a clear-cut\ndefinition of seasonalities and cycles. Their detection and their extraction,\nmoreover, become easy to implement. Several computer experiments with concrete\ndata from various fields are presented and discussed. The conclusion suggests\nthe application of this approach to the debatable Kondriatev waves.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.00237v1"
    },
    {
        "title": "Analysis of the particle transfer between two systems under unification",
        "authors": [
            "I. A. Molotkov",
            "A. I. Osin"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We investigate unification of two systems of identical elements having\ndifferent dimensions which may be of interest for both physics and economics.\nCharacteristic parameters as well as explicit formulae for the temperature (in\neconomics - capital turnover) and dimension of the united system are obtained\nas functions of parameters of the initial systems. Expressions also found for\nthe entropies of initial and united system. The process of unification is\naccompanied by the transfer of particles (money) between the systems and\nexplicit expression is obtained for the transferred number of particles (size\nof the capital). A relation between parameters of the initial systems also\nfound which defines the regime with zero particle transfer.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.00876v1"
    },
    {
        "title": "Shortfall from Maximum Convexity",
        "authors": [
            "Matthew Ginley"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We review the dynamics of the returns of Leveraged Exchange Traded Funds\n(LETFs) and propose a new measure of realized volatility: Shortfall from\nMaximum Convexity. We show that SMC has a more intuitive interpretation and\nprovides more statistical information compared to the traditionally used sample\nstandard deviation when applied to LETF returns, a dataset where normality and\nindependence do not hold.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.00941v1"
    },
    {
        "title": "Is the Indian Stock Market efficient - A comprehensive study of Bombay\n  Stock Exchange Indices",
        "authors": [
            "Achal Awasthi",
            "Oleg Malafeyev"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  How an investor invests in the market is largely influenced by the market\nefficiency because if a market is efficient, it is extremely difficult to make\nexcessive returns because in an efficient market there will be no undervalued\nsecurities i.e. securities whose value is less than its assumed intrinsic\nvalue, which offer returns that are higher than the deserved expected returns,\ngiven their risk. However, there is a possibility of making excessive returns\nif the market is not efficient. This article analyses the five popular stock\nindices of BSE. This would not only test the efficiency of the Indian Stock\nMarket but also test the random walk nature of the stock market. The study\nundertaken in this paper has provided strong evidence in favor of the\ninefficient form of the Indian Stock Market. The series of stock indices in the\nIndian Stock Market are found to be biased random time series and the random\nwalk model can't be applied in the Indian Stock Market. This study confirms\nthat there is a drift in market efficiency and investors can capitalize on this\nby correctly choosing the securities that are undervalued.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.03704v1"
    },
    {
        "title": "On Capturing the Spreading Dynamics over Trading Prices in the Market",
        "authors": [
            "Hokky Situngkir"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  While market is a social field where information flows over the interacting\nagents, there have been not so many methods to observe the spreading\ninformation in the prices comprising the market. By incorporating the entropy\ntransfer in information theory in its relation to the Granger causality, the\npaper proposes a tree of weighted directed graph of market to detect the\nchanges of price might affect other price changes. We compare the proposed\nanalysis with the similar tree representation built from the correlation\ncoefficients of stock prices in order to have insight of possibility in seeing\nthe collective behavior of the market in general.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04690v1"
    },
    {
        "title": "Detrended cross-correlations between returns, volatility, trading\n  activity, and volume traded for the stock market companies",
        "authors": [
            "Rafal Rak",
            "Stanislaw Drozdz",
            "Jaroslaw Kwapien",
            "Pawel Oswiecimka"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We consider a few quantities that characterize trading on a stock market in a\nfixed time interval: logarithmic returns, volatility, trading activity (i.e.,\nthe number of transactions), and volume traded. We search for the power-law\ncross-correlations among these quantities aggregated over different time units\nfrom 1 min to 10 min. Our study is based on empirical data from the American\nstock market consisting of tick-by-tick recordings of 31 stocks listed in Dow\nJones Industrial Average during the years 2008-2011. Since all the considered\nquantities except the returns show strong daily patterns related to the\nvariable trading activity in different parts of a day, which are the best\nevident in the autocorrelation function, we remove these patterns by detrending\nbefore we proceed further with our study. We apply the multifractal detrended\ncross-correlation analysis with sign preserving (MFCCA) and show that the\nstrongest power-law cross-correlations exist between trading activity and\nvolume traded, while the weakest ones exist (or even do not exist) between the\nreturns and the remaining quantities. We also show that the strongest\ncross-correlations are carried by those parts of the signals that are\ncharacterized by large and medium variance. Our observation that the most\nconvincing power-law cross-correlations occur between trading activity and\nvolume traded reveals the existence of strong fractal-like coupling between\nthese quantities.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.04910v2"
    },
    {
        "title": "An empirical analysis of the relationships between crude oil, gold and\n  stock markets",
        "authors": [
            "Semei Coronado",
            "Rebeca Jiménez-Rodríguez",
            "Omar Rojas"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  This paper analyzes the direction of the causality between crude oil, gold\nand stock markets for the largest economy in the world with respect to such\nmarkets, the US. To do so, we apply non-linear Granger causality tests. We find\na nonlinear causal relationship among the three markets considered, with the\ncausality going in all directions, when the full sample and different\nsubsamples are considered. However, we find a unidirectional nonlinear causal\nrelationship between the crude oil and gold market (with the causality only\ngoing from oil price changes to gold price changes) when the subsample runs\nfrom the first date of any year between the mid-1990s and 2001 to last\navailable data (February 5, 2015). The latter result may explain the lack of\nconsensus existing in the literature about the direction of the causal link\nbetween the crude oil and gold markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07599v2"
    },
    {
        "title": "\"Speculative Influence Network\" during financial bubbles: application to\n  Chinese Stock Markets",
        "authors": [
            "Li Lin",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We introduce the Speculative Influence Network (SIN) to decipher the causal\nrelationships between sectors (and/or firms) during financial bubbles. The SIN\nis constructed in two steps. First, we develop a Hidden Markov Model (HMM) of\nregime-switching between a normal market phase represented by a geometric\nBrownian motion (GBM) and a bubble regime represented by the stochastic\nsuper-exponential Sornette-Andersen (2002) bubble model. The calibration of the\nHMM provides the probability at each time for a given security to be in the\nbubble regime. Conditional on two assets being qualified in the bubble regime,\nwe then use the transfer entropy to quantify the influence of the returns of\none asset $i$ onto another asset $j$, from which we introduce the adjacency\nmatrix of the SIN among securities. We apply our technology to the Chinese\nstock market during the period 2005-2008, during which a normal phase was\nfollowed by a spectacular bubble ending in a massive correction. We introduce\nthe Net Speculative Influence Intensity (NSII) variable as the difference\nbetween the transfer entropies from $i$ to $j$ and from $j$ to $i$, which is\nused in a series of rank ordered regressions to predict the maximum loss\n(\\%{MaxLoss}) endured during the crash. The sectors that influenced other\nsectors the most are found to have the largest losses. There is a clear\nprediction skill obtained by using the transfer entropy involving industrial\nsectors to explain the \\%{MaxLoss} of financial institutions but not vice\nversa. We also show that the bubble state variable calibrated on the Chinese\nmarket data corresponds well to the regimes when the market exhibits a strong\nprice acceleration followed by clear change of price regimes. Our results\nsuggest that SIN may contribute significant skill to the development of general\nlinkage-based systemic risks measures and early warning metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.08162v1"
    },
    {
        "title": "How to improve accuracy for DFA technique",
        "authors": [
            "Alessandro Stringhi",
            "Silvia Figini"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  This paper extends the existing literature on empirical estimation of the\nconfidence intervals associated to the Detrended Fluctuation Analysis (DFA). We\nused Montecarlo simulation to evaluate the confidence intervals. Varying the\nparameters in DFA technique, we point out the relationship between those and\nthe standard deviation of H. The parameters considered are the finite time\nlength L, the number of divisors d used and the values of those. We found that\nall these parameters play a crucial role, determining the accuracy of the\nestimation of H.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.00629v1"
    },
    {
        "title": "A study of co-movements between oil price, stock index and exchange rate\n  under a cross-bicorrelation perspective: the case of Mexico",
        "authors": [
            "Semei Coronado",
            "Omar Rojas"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  In this chapter we studied the nonlinear co-movements between the Mexican\nCrude Oil price, the Mexican Stock Market Index and the USD/MXN Exchange Rate,\nfor the sample period from 1994 to date. We used a battery of nonlinear tests,\ncf. (Patterson & Ashley, 2000) and one multivariate test, in order to determine\nthe dynamic co-movement exerted from the oil prices to the stock and exchange\nrate markets. Such co-movement and time windows are exposed using the Brooks &\nHinich (1999) cross- bicorrelation statistical test. The effects of oil spills\non other markets have been studied from different angles and on several\nfinancial assets. In this study, we focus our attention on the detection, not\nonly of the correlations amongst markets but on the epochs in which such\nnonlinear dependence might occur. This is important in order to understand\nbetter, how the markets that drive the economy interact with each other. We\nhope to contribute to the literature with such findings, filling a gap in the\nemerging markets context, in particular, for the Mexican case.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.03271v1"
    },
    {
        "title": "Filterbased Stochastic Volatility in Continuous-Time Hidden Markov\n  Models",
        "authors": [
            "Vikram Krishnamurthy",
            "Elisabeth Leoff",
            "Jörn Sass"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Regime-switching models, in particular Hidden Markov Models (HMMs) where the\nswitching is driven by an unobservable Markov chain, are widely-used in\nfinancial applications, due to their tractability and good econometric\nproperties. In this work we consider HMMs in continuous time with both constant\nand switching volatility. In the continuous-time model with switching\nvolatility the underlying Markov chain could be observed due to this stochastic\nvolatility, and no estimation (filtering) of it is needed (in theory), while in\nthe discretized model or the model with constant volatility one has to filter\nfor the underlying Markov chain. The motivations for continuous-time models are\nexplicit computations in finance. To have a realistic model with unobservable\nMarkov chain in continuous time and good econometric properties we introduce a\nregime-switching model where the volatility depends on the filter for the\nunderlying chain and state the filtering equations. We prove an approximation\nresult for a fixed information filtration and further motivate the model by\nconsidering social learning arguments. We analyze its relation to the switching\nvolatility model and present a convergence result for the discretized model. We\nthen illustrate its econometric properties by considering numerical\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05323v1"
    },
    {
        "title": "Do co-jumps impact correlations in currency markets?",
        "authors": [
            "Jozef Barunik",
            "Lukas Vacha"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We quantify how co-jumps impact correlations in currency markets. To\ndisentangle the continuous part of quadratic covariation from co-jumps, and\nstudy the influence of co-jumps on correlations, we propose a new wavelet-based\nestimator. The proposed estimation framework is able to localize the co-jumps\nvery precisely through wavelet coefficients and identify statistically\nsignificant co-jumps. Empirical findings reveal the different behaviors of\nco-jumps during Asian, European and U.S. trading sessions. Importantly, we\ndocument that co-jumps significantly influence correlation in currency markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05489v3"
    },
    {
        "title": "Noise Fit, Estimation Error and a Sharpe Information Criterion",
        "authors": [
            "Dirk Paulsen",
            "Jakob Söhl"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  When the in-sample Sharpe ratio is obtained by optimizing over a\nk-dimensional parameter space, it is a biased estimator for what can be\nexpected on unseen data (out-of-sample). We derive (1) an unbiased estimator\nadjusting for both sources of bias: noise fit and estimation error. We then\nshow (2) how to use the adjusted Sharpe ratio as model selection criterion\nanalogously to the Akaike Information Criterion (AIC). Selecting a model with\nthe highest adjusted Sharpe ratio selects the model with the highest estimated\nout-of-sample Sharpe ratio in the same way as selection by AIC does for the\nlog-likelihood as measure of fit.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.06186v5"
    },
    {
        "title": "Modified Profile Likelihood Inference and Interval Forecast of the Burst\n  of Financial Bubbles",
        "authors": [
            "Vladimir Filimonov",
            "Guilherme Demos",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We present a detailed methodological study of the application of the modified\nprofile likelihood method for the calibration of nonlinear financial models\ncharacterised by a large number of parameters. We apply the general approach to\nthe Log-Periodic Power Law Singularity (LPPLS) model of financial bubbles. This\nmodel is particularly relevant because one of its parameters, the critical time\n$t_c$ signalling the burst of the bubble, is arguably the target of choice for\ndynamical risk management. However, previous calibrations of the LPPLS model\nhave shown that the estimation of $t_c$ is in general quite unstable. Here, we\nprovide a rigorous likelihood inference approach to determine $t_c$, which\ntakes into account the impact of the other nonlinear (so-called \"nuisance\")\nparameters for the correct adjustment of the uncertainty on $t_c$. This\nprovides a rigorous interval estimation for the critical time, rather than a\npoint estimation in previous approaches. As a bonus, the interval estimations\ncan also be obtained for the nuisance parameters ($m,\\omega$, damping), which\ncan be used to improve filtering of the calibration results. We show that the\nuse of the modified profile likelihood method dramatically reduces the number\nof local extrema by constructing much simpler smoother log-likelihood\nlandscapes. The remaining distinct solutions can be interpreted as genuine\nscenarios that unfold as the time of the analysis flows, which can be compared\ndirectly via their likelihood ratio. Finally, we develop a multi-scale profile\nlikelihood analysis to visualize the structure of the financial data at\ndifferent scales (typically from 100 to 750 days). We test the methodology\nsuccessfully on synthetic price time series and on three well-known historical\nfinancial bubbles.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.08258v1"
    },
    {
        "title": "Copula--based Specification of vector MEMs",
        "authors": [
            "Fabrizio Cipollini",
            "Robert F. Engle",
            "Giampiero M. Gallo"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The Multiplicative Error Model (Engle (2002)) for nonnegative valued\nprocesses is specified as the product of a (conditionally autoregressive) scale\nfactor and an innovation process with nonnegative support. A multivariate\nextension allows for the innovations to be contemporaneously correlated. We\novercome the lack of sufficiently flexible probability density functions for\nsuch processes by suggesting a copula function approach to estimate the\nparameters of the scale factors and of the correlations of the innovation\nprocesses. We illustrate this vector MEM with an application to the\ninteractions between realized volatility, volume and the number of trades. We\nshow that significantly superior realized volatility forecasts are delivered in\nthe presence of other trading activity indicators and contemporaneous\ncorrelations.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01338v1"
    },
    {
        "title": "Evidence of Self-Organization in Time Series of Capital Markets",
        "authors": [
            "Leopoldo Sánchez-Cantú",
            "Carlos Arturo Soto-Campos",
            "Andriy Kryvko"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  A methodology is developed to identify, as units of study, each decrease in\nthe value of a stock from a given maximum price level. A critical level in the\namount of price declines is found to separate a segment operating under a\nrandom walk from a segment operating under a power law. This level is\ninterpreted as a point of phase transition into a self-organized system.\nEvidence of self-organization was found in all the stock market indices studied\nbut in none of the control synthetic random series. Findings partially explain\nthe fractal structure characteristic of financial time series and suggest that\nprice fluctuations adopt two different operating regimes. We propose to\nidentify downward movements larger than the critical level apparently subject\nto the power law, as self-organized states, and price decreases smaller than\nthe critical level, as a random walk with the Markov property.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.03996v2"
    },
    {
        "title": "Regime switching vine copula models for global equity and volatility\n  indices",
        "authors": [
            "Holger Fink",
            "Yulia Klimova",
            "Claudia Czado",
            "Jakob Stöber"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  For nearly every major stock market there exist equity and implied volatility\nindices. These play important roles within finance: be it as a benchmark, a\nmeasure of general uncertainty or a way of investing or hedging. It is well\nknown in the academic literature, that correlations and higher moments between\ndifferent indices tend to vary in time. However, to the best of our knowledge,\nno one has yet considered a global setup including both, equity and implied\nvolatility indices of various continents, and allowing for a changing\ndependence structure. We aim to close this gap by applying Markov-switching\n$R$-vine models to investigate the existence of different, global dependence\nregimes. In particular, we identify times of \"normal\" and \"abnormal\" states\nwithin a data set consisting of North-American, European and Asian indices. Our\nresults confirm the existence of joint points in time at which global regime\nswitching takes place.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.05598v1"
    },
    {
        "title": "An Explicit Formula for Likelihood Function for Gaussian Vector\n  Autoregressive Moving-Average Model Conditioned on Initial Observables with\n  Application to Model Calibration",
        "authors": [
            "Du Nguyen"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We derive an explicit formula for likelihood function for Gaussian VARMA\nmodel conditioned on initial observables where the moving-average (MA)\ncoefficients are scalar. For fixed MA coefficients the likelihood function is\noptimized in the autoregressive variables $\\Phi$'s by a closed form formula\ngeneralizing regression calculation of the VAR model with the introduction of\nan inner product defined by MA coefficients. We show the assumption of scalar\nMA coefficients is not restrictive and this formulation of the VARMA model\nshares many nice features of VAR and MA model. The gradient and Hessian could\nbe computed analytically. The likelihood function is preserved under the root\ninvertion maps of the MA coefficients. We discuss constraints on the gradient\nof the likelihood function with moving average unit roots. With the help of FFT\nthe likelihood function could be computed in $O((kp+1)^2T +ckT\\log(T))$ time.\nNumerical calibration is required for the scalar MA variables only. The\napproach can be generalized to include additional drifts as well as integrated\ncomponents. We discuss a relationship with the Borodin-Okounkov formula and the\ncase of infinite MA components.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.08677v1"
    },
    {
        "title": "World Financial 2014-2016 Market Bubbles: Oil Negative - US Dollar\n  Positive",
        "authors": [
            "Marcin Wątorek",
            "Stanisław Drożdż",
            "Paweł Oświęcimka"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Based on the Log-Periodic Power Law (LPPL) methodology, with the universal\npreferred scaling factor $\\lambda \\approx 2$, the negative bubble on the oil\nmarket in 2014-2016 has been detected. Over the same period a positive bubble\non the so called commodity currencies expressed in terms of the US dollar\nappears to take place with the oscillation pattern which largely is mirror\nreflected relative to oil price oscillation pattern. This documents recent\nstrong anti-correlation between the dynamics of the oil price and of the USD. A\nrelated forecast made at the time of FENS 2015 conference (beginning of\nNovember) turned out to be quite satisfactory. These findings provide also\nfurther indication that such a log-periodically accelerating down-trend signals\ntermination of the corresponding decreases.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.01218v1"
    },
    {
        "title": "The study of Thai stock market across the 2008 financial crisis",
        "authors": [
            "K. Kanjamapornkul",
            "Richard Pinčák",
            "Erik Bartoš"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The cohomology theory for financial market can allow us to deform Kolmogorov\nspace of time series data over time period with the explicit definition of\neight market states in grand unified theory. The anti-de Sitter space induced\nfrom a coupling behavior field among traders in case of a financial market\ncrash acts like gravitational field in financial market spacetime. Under this\nhybrid mathematical superstructure, we redefine a behavior matrix by using\nPauli matrix and modified Wilson loop for time series data. We use it to detect\nthe 2008 financial market crash by using a degree of cohomology group of sphere\nover tensor field in correlation matrix over all possible dominated stocks\nunderlying Thai SET50 Index Futures. The empirical analysis of financial tensor\nnetwork was performed with the help of empirical mode decomposition and\nintrinsic time scale decomposition of correlation matrix and the calculation of\ncloseness centrality of planar graph.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.02871v1"
    },
    {
        "title": "Deep Learning for Mortgage Risk",
        "authors": [
            "Justin Sirignano",
            "Apaar Sadhwani",
            "Kay Giesecke"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We develop a deep learning model of multi-period mortgage risk and use it to\nanalyze an unprecedented dataset of origination and monthly performance records\nfor over 120 million mortgages originated across the US between 1995 and 2014.\nOur estimators of term structures of conditional probabilities of prepayment,\nforeclosure and various states of delinquency incorporate the dynamics of a\nlarge number of loan-specific as well as macroeconomic variables down to the\nzip-code level. The estimators uncover the highly nonlinear nature of the\nrelationship between the variables and borrower behavior, especially\nprepayment. They also highlight the effects of local economic conditions on\nborrower behavior. State unemployment has the greatest explanatory power among\nall variables, offering strong evidence of the tight connection between housing\nfinance markets and the macroeconomy. The sensitivity of a borrower to changes\nin unemployment strongly depends upon current unemployment. It also\nsignificantly varies across the entire borrower population, which highlights\nthe interaction of unemployment and many other variables. These findings have\nimportant implications for mortgage-backed security investors, rating agencies,\nand housing finance policymakers.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02470v2"
    },
    {
        "title": "A Comparison of Nineteen Various Electricity Consumption Forecasting\n  Approaches and Practicing to Five Different Households in Turkey",
        "authors": [
            "T. O. Benli"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The accuracy of the household electricity consumption forecast is vital in\ntaking better cost effective and energy efficient decisions. In order to design\naccurate, proper and efficient forecasting model, characteristics of the series\nhave to been analyzed. The source of time series data comes from Online\nEnerjisa System, the system of electrical energy provider in capital of Turkey,\nwhich consumers can reach their latest two year period electricity\nconsumptions; in our study the period was May 2014 to May 2016. Various\ntechniques had been applied in order to analyze the data; classical\ndecomposition models; standard typed and also with the centering moving average\nmethod, regression equations, exponential smoothing models and ARIMA models. In\nour study, nine teen different approaches; all of these have at least\ndiversified aspects of methodology, had been compared and the best model for\nforecasting were decided by considering the smallest values of MAPE, MAD and\nMSD. As a first step we took the time period May 2014 to May 2016 and found\npredicted value for June 2016 with the best forecasting model. After finding\nthe best forecasting model and fitted value for June 2016, than validating\nprocess had been taken place; we made comparisons to see how well the real\nvalue of June 2016 and forecasted value for that specific period matched.\nAfterwards we made electrical consumption forecast for the following 3 months;\nJune-September 2016 for each of five households individually.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.05660v1"
    },
    {
        "title": "Dynamic structure of stock communities: A comparative study between\n  stock returns and turnover rates",
        "authors": [
            "Li-Ling Su",
            "Xiong-Fei Jiang",
            "Sai-Ping Li",
            "Li-Xin Zhong",
            "Fei Ren"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The detection of community structure in stock market is of theoretical and\npractical significance for the study of financial dynamics and portfolio risk\nestimation. We here study the community structures in Chinese stock markets\nfrom the aspects of both price returns and turnover rates, by using a\ncombination of the PMFG and infomap methods based on a distance matrix. We find\nthat a few of the largest communities are composed of certain specific industry\nor conceptional sectors and the correlation inside a sector is generally larger\nthan the correlation between different sectors. In comparison with returns, the\ncommunity structure for turnover rates is more complex and the sector effect is\nrelatively weaker. The financial dynamics is further studied by analyzing the\ncommunity structures over five sub-periods. Sectors like banks, real estate,\nhealth care and New Shanghai take turns to compose a few of the largest\ncommunities for both returns and turnover rates in different sub-periods.\nSeveral specific sectors appear in the communities with different rank orders\nfor the two time series even in the same sub-period. A comparison between the\nevolution of prices and turnover rates of stocks from these sectors is\nconducted to better understand their differences. We find that stock prices\nonly had large changes around some important events while turnover rates surged\nafter each of these events relevant to specific sectors, which may offer a\npossible explanation for the complexity of stock communities for turnover\nrates.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03053v1"
    },
    {
        "title": "Dynamic portfolio strategy using clustering approach",
        "authors": [
            "Fei Ren",
            "Ya-Nan Lu",
            "Sai-Ping Li",
            "Xiong-Fei Jiang",
            "Li-Xin Zhong",
            "Tian Qiu"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The problem of portfolio optimization is one of the most important issues in\nasset management. This paper proposes a new dynamic portfolio strategy based on\nthe time-varying structures of MST networks in Chinese stock markets, where the\nmarket condition is further considered when using the optimal portfolios for\ninvestment. A portfolio strategy comprises two stages: selecting the portfolios\nby choosing central and peripheral stocks in the selection horizon using five\ntopological parameters, i.e., degree, betweenness centrality, distance on\ndegree criterion, distance on correlation criterion and distance on distance\ncriterion, then using the portfolios for investment in the investment horizon.\nThe optimal portfolio is chosen by comparing central and peripheral portfolios\nunder different combinations of market conditions in the selection and\ninvestment horizons. Market conditions in our paper are identified by the\nratios of the number of trading days with rising index or the sum of the\namplitudes of the trading days with rising index to the total number of trading\ndays. We find that central portfolios outperform peripheral portfolios when the\nmarket is under a drawup condition, or when the market is stable or drawup in\nthe selection horizon and is under a stable condition in the investment\nhorizon. We also find that the peripheral portfolios gain more than central\nportfolios when the market is stable in the selection horizon and is drawdown\nin the investment horizon. Empirical tests are carried out based on the optimal\nportfolio strategy. Among all the possible optimal portfolio strategy based on\ndifferent parameters to select portfolios and different criteria to identify\nmarket conditions, $65\\%$ of our optimal portfolio strategies outperform the\nrandom strategy for the Shanghai A-Share market and the proportion is $70\\%$\nfor the Shenzhen A-Share market.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03058v1"
    },
    {
        "title": "Fractal approach towards power-law coherency to measure\n  cross-correlations between time series",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We focus on power-law coherency as an alternative approach towards studying\npower-law cross-correlations between simultaneously recorded time series. To be\nable to study empirical data, we introduce three estimators of the power-law\ncoherency parameter $H_{\\rho}$ based on popular techniques usually utilized for\nstudying power-law cross-correlations -- detrended cross-correlation analysis\n(DCCA), detrending moving-average cross-correlation analysis (DMCA) and height\ncross-correlation analysis (HXA). In the finite sample properties study, we\nfocus on the bias, variance and mean squared error of the estimators. We find\nthat the DMCA-based method is the safest choice among the three. The HXA method\nis reasonable for long time series with at least $10^4$ observations, which can\nbe easily attainable in some disciplines but problematic in others. The\nDCCA-based method does not provide favorable properties which even deteriorate\nwith an increasing time series length. The paper opens a new venue towards\nstudying cross-correlations between time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.06781v2"
    },
    {
        "title": "Foreign Exchange Market Performance: Evidence from Bivariate Time Series\n  Approach",
        "authors": [
            "Mansooreh Kazemilari",
            "Maman Abdurachman Djauhari",
            "Zuhaimy Ismail"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  There are many studies dealing with the analysis of similarity among\ncurrencies in foreign exchange market by using network analysis approach. In\nthose studies, each currency is represented by a univariate time series of\nexchange rate return. This is the standard practice to analyze the underlying\ninformation in the foreign exchange market. In this paper, Escoufier's RV\ncoefficient is applied to measure the similarity among currencies where each of\nthem is represented by bivariate time series. Based on that coefficient, we\nanalyze the topological structure of the currencies. An example of FOREX\nanalysis will be presented and discussed to illustrate the advantages of RV\ncoefficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.07694v1"
    },
    {
        "title": "Entropy and efficiency of the ETF market",
        "authors": [
            "Lucio Maria Calcagnile",
            "Fulvio Corsi",
            "Stefano Marmi"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We investigate the relative information efficiency of financial markets by\nmeasuring the entropy of the time series of high frequency data. Our tool to\nmeasure efficiency is the Shannon entropy, applied to 2-symbol and 3-symbol\ndiscretisations of the data. Analysing 1-minute and 5-minute price time series\nof 55 Exchange Traded Funds traded at the New York Stock Exchange, we develop a\nmethodology to isolate true inefficiencies from other sources of regularities,\nsuch as the intraday pattern, the volatility clustering and the microstructure\neffects. The first two are modelled as multiplicative factors, while the\nmicrostructure is modelled as an ARMA noise process. Following an analytical\nand empirical combined approach, we find a strong relationship between low\nentropy and high relative tick size and that volatility is responsible for the\nlargest amount of regularity, averaging 62% of the total regularity against 18%\nof the intraday pattern regularity and 20% of the microstructure.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04199v1"
    },
    {
        "title": "Hysteresis and Duration Dependence of Financial Crises in the US:\n  Evidence from 1871-2016",
        "authors": [
            "Rui Menezes",
            "Sonia Bentes"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  This study analyses the duration dependence of events that trigger volatility\npersistence in stock markets. Such events, in our context, are monthly spells\nof contiguous price decline or negative returns for the S&P500 stock market\nindex over the last 145 years. Factors known to affect the duration of these\nspells are the magnitude or intensity of the price decline, long-term interest\nrates and economic recessions, among others. The result of interest is the\nconditional probability of ending a spell of consecutive months over which\nstock market returns remain negative. In this study, we rely on continuous time\nsurvival models in order to investigate this question. Several specifications\nwere attempted, some of which under the proportional hazards assumption and\nothers under the accelerated failure time assumption. The best fit of the\nvarious models endeavored was obtained for the log-normal distribution. This\ndistribution yields a non-monotonic hazard function that increases up to a\nmaximum and then decreases. The peak is achieved 2-3 months after the spells\nonset with a hazard of around 0.9 or higher; this hazard then decays\nasymptotically to zero. Spells duration increase during recessions, when\ninterest rate rises and when price declines are more intense. The main\nconclusion is that short spells of negative returns appear to be mainly\nfrictional while long spells become structural and trigger hysteresis effects\nafter an initial period of adjustment. Although in line with our expectations,\nthese results may be of some importance for policy-makers.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00259v1"
    },
    {
        "title": "Time-Varying Comovement of Foreign Exchange Markets",
        "authors": [
            "Mikio Ito",
            "Akihiko Noda",
            "Tatsuma Wada"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  A time-varying cointegration model for foreign exchange rates is presented.\nUnlike previous studies, we allow the loading matrix in the vector error\ncorrection (VEC) model to be varying over time. Because the loading matrix in\nthe VEC model is associated with the speed at which deviations from the\nlong-run relationship disappear, we propose a new degree of market comovement\\\nbased on the time-varying loading matrix to measure the strength or robustness\nof the long-run relationship over time. Since exchange rates are determined by\nmacrovariables, cointegration among exchange rates implies those macroeconomic\nvariables share common stochastic trends. Therefore, the proposed degree\nmeasures the degree of market comovement. Our main finding is that the market\ncomovement has become stronger over the past quarter century, but the rate at\nwhich market comovement strengthens is decreasing with two major turning\npoints: one in 1995 and the other one in 2008.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.04334v1"
    },
    {
        "title": "Uncertainty Estimates in the Heston Model via Fisher Information",
        "authors": [
            "Oliver Pfante",
            "Nils Bertschinger"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We address the information content of European option prices about volatility\nin terms of the Fisher information matrix. We assume that observed option\nprices are centred on the theoretical price provided by Heston's model\ndisturbed by additive Gaussian noise. We fit the likelihood function on the\ncomponents of the VIX, i.e., near- and next-term put and call options on the\nS&P 500 with more than 23 days and less than 37 days to expiration and\nnon-vanishing bid, and compute their Fisher information matrices from the\nGreeks in the Heston model. We find that option prices allow reliable estimates\nof volatility with negligible uncertainty as long as volatility is large\nenough. Interestingly, if volatility drops below a critical value, inferences\nfrom option prices become impossible because Vega, the derivative of a European\noption w.r.t. volatility, nearly vanishes.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.04760v2"
    },
    {
        "title": "\"Butterfly Effect\" vs Chaos in Energy Futures Markets",
        "authors": [
            "Loretta Mastroeni",
            "Pierluigi Vellucci"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  In this paper we test for the sensitive dependence on initial conditions (the\nso called \"butterfly effect\") of energy futures time series (heating oil,\nnatural gas), and thus the determinism of those series. This paper is\ndistinguished from previous studies in the following points: first, we reread\nexistent works in the literature on energy markets, enlightening the role of\n\\emph{butterfly effect} in chaos definition (introduced by Devaney), using this\ndefinition to prevent us from misleading results about ostensible chaoticity of\nthe price series. Second, we test for the time series for sensitive dependence\non initial conditions, introducing a coefficient that describes the determinism\nrate of the series and that represents its reliability level (in percentage).\nThe introduction of this reliability level is motivated by the fact that time\nseries generated from stochastic systems also might show sensitive dependence\non initial conditions. According to this perspective, the maximum reliability\nlevel obtained here is too low to be able to ensure that there is strong\nevidence of sensitive The maximum reliability level obtained here was been\n$\\simeq 56\\% $, too low to ensure strong evidence of sensitive dependence on\ninitial conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.05697v1"
    },
    {
        "title": "Techniques for multifractal spectrum estimation in financial time series",
        "authors": [
            "Petr Jizba",
            "Jan Korbel"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Multifractal analysis is one of the important approaches that enables us to\nmeasure the complexity of various data via the scaling properties. We compare\nthe most common techniques used for multifractal exponents estimation from both\ntheoretical and practical point of view. Particularly, we discuss the methods\nbased on estimation of R\\'enyi entropy, which provide a powerful tool\nespecially in presence of heavy-tailed data. To put some flesh on bare bones,\nall methods are compared on various real financial datasets, including daily\nand high-frequency data.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.07028v1"
    },
    {
        "title": "The asset price bubbles in emerging financial markets: a new statistical\n  approach",
        "authors": [
            "Shu-Peng Chen",
            "Ling-Yun He"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The bubble is a controversial and important issue. Many methods which based\non the rational expectation have been proposed to detect the bubble. However,\nfor some developing countries, epically China, the asset markets are so young\nthat for many companies, there are no dividends and fundamental value, making\nit difficult (if not impossible) to measure the bubbles by existing methods.\nTherefore, we proposed a simple but effective statistical method and three\nstatistics (that is, C, U, V) to capture and quantify asset price bubbles,\nespecially in immature emerging markets. To present a clear example of the\napplication of this method to real world problems, we also applied our method\nto re-examine empirically the asset price bubble in some stock markets. Our\nmain contributions to current literature are as follows: firstly, this method\ndoes not rely on fundamental value, the discount rates and dividends, therefore\nis applicable to the immature markets without the sufficient data of such\nkinds, secondly, this new method allows us to examine different influences\n(herding behavior, abnormal fluctuation and composite influence) of bubble. Our\nnew statistical approach is, to the best of our knowledge, the only robust way\nin existing literature to to quantify the asset price bubble especially in\nemerging markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.07287v1"
    },
    {
        "title": "The Fellowship of LIBOR: A Study of Spurious Interbank Correlations by\n  the Method of Wigner-Ville Function",
        "authors": [
            "Peter B. Lerner"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The manipulation of LIBOR by a group of banks became one of the major blows\nto the remaining confidence in financial industry. Yet, despite an enormous\namount of popular literature on the subject, rigorous time-series studies are\nfew. In my paper, I discuss the following hypothesis. Namely, if we should\nassume for a statistical null, the quotes, which were submitted by the member\nbanks were true, the deviations from the LIBOR should have been entirely random\nbecause they were determined by idiosyncratic conditions by the member banks.\nThis hypothesis can be statistically verified. Serial correlations of the\nrates, which cannot be explained by the differences in credit qualities of the\nmember banks or the domicile Governments, were subjected to correlation tests.\nA new econometric method--the analysis of the Wigner-Ville function borrowed\nfrom quantum mechanics and signal processing--is used and explained for the\nstatistical interpretation of regression residuals.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08414v2"
    },
    {
        "title": "A Comparison of Various Electricity Tariff Price Forecasting Techniques\n  in Turkey and Identifying the Impact of Time Series Periods",
        "authors": [
            "T. O. Benli"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  It is very vital for suppliers and distributors to predict the deregulated\nelectricity prices for creating their bidding strategies in the competitive\nmarket area. Pre requirement of succeeding in this field, accurate and suitable\nelectricity tariff price forecasting tools are needed. In the presence of\neffective forecasting tools, taking the decisions of production, merchandising,\nmaintenance and investment with the aim of maximizing the profits and benefits\ncan be successively and effectively done. According to the electricity demand,\nthere are four various electricity tariffs pricing in Turkey; monochromic, day,\npeak and night. The objective is find the best suitable tool for predicting the\nfour pricing periods of electricity and produce short term forecasts (one year\nahead-monthly). Our approach based on finding the best model, which ensures the\nsmallest forecasting error measurements of: MAPE, MAD and MSD. We conduct a\ncomparison of various forecasting approaches in total accounts for nine teen,\nat least all of those have different aspects of methodology. Our beginning step\nwas doing forecasts for the year 2015. We validated and analyzed the\nperformance of our best model and made comparisons to see how well the\nhistorical values of 2015 and forecasted data for that specific period matched.\nResults show that given the time-series data, the recommended models provided\ngood forecasts. Second part of practice, we also include the year 2015, and\ncompute all the models with the time series of January 2011 to December 2015.\nAgain by choosing the best appropriate forecasting model, we conducted the\nforecast process and also analyze the impact of enhancing of time series\nperiods (January 2007 to December 2015) to model that we used for forecasting\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08415v1"
    },
    {
        "title": "Multifractal cross wavelet analysis",
        "authors": [
            "Zhi-Qiang Jiang",
            "Xing-Lu Gao",
            "Wei-Xing Zhou",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Complex systems are composed of mutually interacting components and the\noutput values of these components are usually long-range cross-correlated. We\npropose a method to characterize the joint multifractal nature of such\nlong-range cross correlations based on wavelet analysis, termed multifractal\ncross wavelet analysis (MFXWT). We assess the performance of the MFXWT method\nby performing extensive numerical experiments on the dual binomial measures\nwith multifractal cross correlations and the bivariate fractional Brownian\nmotions (bFBMs) with monofractal cross correlations. For binomial multifractal\nmeasures, the empirical joint multifractality of MFXWT is found to be in\napproximate agreement with the theoretical formula. For bFBMs, MFXWT may\nprovide spurious multifractality because of the wide spanning range of the\nmultifractal spectrum. We also apply the MFXWT method to stock market indexes\nand uncover intriguing joint multifractal nature in pairs of index returns and\nvolatilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09519v2"
    },
    {
        "title": "Long-range Correlation and Market Segmentation in Bond Market",
        "authors": [
            "Zhongxing Wang",
            "Yan Yan",
            "Xiaosong Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  This paper looks into the analysis of the long-range auto-correlations and\ncross-correlations in bond market. Based on Detrended Moving Average (DMA)\nmethod, empirical results present a clear evidence of long-range persistence\nthat exists in one year scale. The degree of long-range correlation related to\nmaturities has an upward tendency with a peak in short term. These findings\nconfirm the expectations of fractal market hypothesis (FMH). Furthermore, we\nhave developed a method based on a complex network to study the long-range\ncross-correlation structure and apply it to our data, and found a clear pattern\nof market segmentation in the long run. We also detected the nature of\nlong-range correlation in the sub-period 2007 to 2012 and 2011 to 2016. The\nresult from our research shows that long-range auto-correlations are decreasing\nin the recent years while long-range cross-correlations are strengthening.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.09812v1"
    },
    {
        "title": "A review of two decades of correlations, hierarchies, networks and\n  clustering in financial markets",
        "authors": [
            "Gautier Marti",
            "Frank Nielsen",
            "Mikołaj Bińkowski",
            "Philippe Donnat"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We review the state of the art of clustering financial time series and the\nstudy of their correlations alongside other interaction networks. The aim of\nthis review is to gather in one place the relevant material from different\nfields, e.g. machine learning, information geometry, econophysics, statistical\nphysics, econometrics, behavioral finance. We hope it will help researchers to\nuse more effectively this alternative modeling of the financial time series.\nDecision makers and quantitative researchers may also be able to leverage its\ninsights. Finally, we also hope that this review will form the basis of an open\ntoolbox to study correlations, hierarchies, networks and clustering in\nfinancial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00485v7"
    },
    {
        "title": "Wisdom of the institutional crowd",
        "authors": [
            "Kevin Primicerio",
            "Damien Challet",
            "Stanislao Gualdi"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The average portfolio structure of institutional investors is shown to have\nproperties which account for transaction costs in an optimal way. This implies\nthat financial institutions unknowingly display collective rationality, or\nWisdom of the Crowd. Individual deviations from the rational benchmark are\nample, which illustrates that system-wide rationality does not need nearly\nrational individuals. Finally we discuss the importance of accounting for\nconstraints when assessing the presence of Wisdom of the Crowd.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.01989v2"
    },
    {
        "title": "Joint News, Attention Spillover,and Market Returns",
        "authors": [
            "Li Guo",
            "Lin Peng",
            "Yubo Tao",
            "Jun Tu"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We analyze over 2.6 million news articles and propose a novel measure of\naggregate joint news coverage of firms. The measure strongly and negatively\npredicts market returns, both in sample and out of sample. The relation is\ncausal, robust to existing predictors, and is especially strong when market\nuncertainty is high or when market frictions are large. Using data on EDGAR\ndownloads by unique IPs, we provide direct evidence that joint news triggers\nattention spillover across firms. Our results are consistent with the\nexplanation that joint news generates a contagion in investor attention and\ncauses marketwide overvaluations and subsequent reversals.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.02715v4"
    },
    {
        "title": "New approaches in agent-based modeling of complex financial systems",
        "authors": [
            "T. T. Chen",
            "B. Zheng",
            "Y. Li",
            "X. F. Jiang"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Agent-based modeling is a powerful simulation technique to understand the\ncollective behavior and microscopic interaction in complex financial systems.\nRecently, the concept for determining the key parameters of the agent-based\nmodels from empirical data instead of setting them artificially was suggested.\nWe first review several agent-based models and the new approaches to determine\nthe key model parameters from historical market data. Based on the agents'\nbehaviors with heterogenous personal preferences and interactions, these models\nare successful to explain the microscopic origination of the temporal and\nspatial correlations of the financial markets. We then present a novel paradigm\ncombining the big-data analysis with the agent-based modeling. Specifically,\nfrom internet query and stock market data, we extract the information driving\nforces, and develop an agent-based model to simulate the dynamic behaviors of\nthe complex financial systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.06840v1"
    },
    {
        "title": "Cohort effects in mortality modelling: a Bayesian state-space approach",
        "authors": [
            "Man Chung Fung",
            "Gareth W. Peters",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Cohort effects are important factors in determining the evolution of human\nmortality for certain countries. Extensions of dynamic mortality models with\ncohort features have been proposed in the literature to account for these\nfactors under the generalised linear modelling framework. In this paper we\napproach the problem of mortality modelling with cohort factors incorporated\nthrough a novel formulation under a state-space methodology. In the process we\ndemonstrate that cohort factors can be formulated naturally under the\nstate-space framework, despite the fact that cohort factors are indexed\naccording to year-of-birth rather than year. Bayesian inference for cohort\nmodels in a state-space formulation is then developed based on an efficient\nMarkov chain Monte Carlo sampler, allowing for the quantification of parameter\nuncertainty in cohort models and resulting mortality forecasts that are used\nfor life expectancy and life table constructions. The effectiveness of our\napproach is examined through comprehensive empirical studies involving male and\nfemale populations from various countries. Our results show that cohort\npatterns are present for certain countries that we studied and the inclusion of\ncohort factors are crucial in capturing these phenomena, thus highlighting the\nbenefits of introducing cohort models in the state-space framework. Forecasting\nof cohort models is also discussed in light of the projection of cohort\nfactors.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.08282v1"
    },
    {
        "title": "High-Frequency Jump Analysis of the Bitcoin Market",
        "authors": [
            "Olivier Scaillet",
            "Adrien Treccani",
            "Christopher Trevisan"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We use the database leak of Mt. Gox exchange to analyze the dynamics of the\nprice of bitcoin from June 2011 to November 2013. This gives us a rare\nopportunity to study an emerging retail-focused, highly speculative and\nunregulated market with trader identifiers at a tick transaction level. Jumps\nare frequent events and they cluster in time. The order flow imbalance and the\npreponderance of aggressive traders, as well as a widening of the bid-ask\nspread predict them. Jumps have short-term positive impact on market activity\nand illiquidity and see a persistent change in the price.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.08175v2"
    },
    {
        "title": "A note on the Nelson Cao inequality constraints in the GJR-GARCH model:\n  Is there a leverage effect?",
        "authors": [
            "Stavros Stavroyiannis"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The majority of stylized facts of financial time series and several\nValue-at-Risk measures are modeled via univariate or multivariate GARCH\nprocesses. It is not rare that advanced GARCH models fail to converge for\ncomputational reasons, and a usual parsimonious approach is the GJR-GARCH\nmodel. There is a disagreement in the literature and the specialized\neconometric software, on which constraints should be used for the parameters,\nintroducing indirectly the distinction between asymmetry and leverage. We show\nthat the approach used by various software packages is not consistent with the\nNelson-Cao inequality constraints. Implementing Monte Carlo simulations,\ndespite of the results being empirically correct, the estimated parameters are\nnot theoretically coherent with the Nelson-Cao constraints for ensuring\npositivity of conditional variances. On the other hand ruling out the leverage\nhypothesis, the asymmetry term in the GJR model can take negative values when\ntypical constraints like the condition for the existence of the second and\nfourth moments, are imposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.00535v1"
    },
    {
        "title": "A Time Series Analysis-Based Forecasting Framework for the Indian\n  Healthcare Sector",
        "authors": [
            "Jaydip Sen",
            "Tamal Datta Chaudhuri"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Designing efficient and robust algorithms for accurate prediction of stock\nmarket prices is one of the most exciting challenges in the field of time\nseries analysis and forecasting. With the exponential rate of development and\nevolution of sophisticated algorithms and with the availability of fast\ncomputing platforms, it has now become possible to effectively and efficiently\nextract, store, process and analyze high volume of stock market data with\ndiversity in its contents. Availability of complex algorithms which can execute\nvery fast on parallel architecture over the cloud has made it possible to\nachieve higher accuracy in forecasting results while reducing the time required\nfor computation. In this paper, we use the time series data of the healthcare\nsector of India for the period January 2010 till December 2016. We first\ndemonstrate a decomposition approach of the time series and then illustrate how\nthe decomposition results provide us with useful insights into the behavior and\nproperties exhibited by the time series. Further, based on the structural\nanalysis of the time series, we propose six different methods of forecasting\nfor predicting the time series index of the healthcare sector. Extensive\nresults are provided on the performance of the forecasting methods to\ndemonstrate their effectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01144v1"
    },
    {
        "title": "The q-dependent detrended cross-correlation analysis of stock market",
        "authors": [
            "Longfeng Zhao",
            "Wei Li",
            "Andrea Fenu",
            "Boris Podobnik",
            "Yougui Wang",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The properties of q-dependent cross-correlation matrices of stock market have\nbeen analyzed by using the random matrix theory and complex network. The\ncorrelation structures of the fluctuations at different magnitudes have unique\nproperties. The cross-correlations among small fluctuations are much stronger\nthan those among large fluctuations. The large and small fluctuations are\ndominated by different groups of stocks. We use complex network representation\nto study these q-dependent matrices and discover some new identities. By\nutilizing those q-dependent correlation-based networks, we are able to\nconstruct some portfolio by those most independent stocks which consistently\nperform the best. The optimal multifractal order for portfolio optimization is\napproximately $q=2$. These results have deepened our understanding about the\ncollective behaviors of the complex financial system.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01406v2"
    },
    {
        "title": "Volatility Spillovers and Heavy Tails: A Large t-Vector AutoRegressive\n  Approach",
        "authors": [
            "Luca Barbaglia",
            "Christophe Croux",
            "Ines Wilms"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Volatility is a key measure of risk in financial analysis. The high\nvolatility of one financial asset today could affect the volatility of another\nasset tomorrow. These lagged effects among volatilities - which we call\nvolatility spillovers - are studied using the Vector AutoRegressive (VAR)\nmodel. We account for the possible fat-tailed distribution of the VAR model\nerrors using a VAR model with errors following a multivariate Student\nt-distribution with unknown degrees of freedom. Moreover, we study volatility\nspillovers among a large number of assets. To this end, we use penalized\nestimation of the VAR model with t-distributed errors. We study volatility\nspillovers among energy, biofuel and agricultural commodities and reveal\nbidirectional volatility spillovers between energy and biofuel, and between\nenergy and agricultural commodities.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.02073v1"
    },
    {
        "title": "Dynamic Conditional Correlation between Electricity and Stock markets\n  during the Financial Crisis in Greece",
        "authors": [
            "Panagiotis G. Papaioannou",
            "George P. Papaioannou",
            "Kostas Siettos",
            "Akylas Stratigakos",
            "Christos Dikaiakos"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Liberalization of electricity markets has increasingly created the need for\nunderstanding the volatility and correlation structure between electricity and\nfinancial markets. This work reveals the existence of structural changes in\ncorrelation patterns among these two markets and links the changes to both\nfundamentals and regulatory conditions prevailing in the markets, as well as\nthe current European financial crisis. We apply a Dynamic Conditional\nCorrelation (DCC) GARCH model to a set of market s fundamental variables and\nGreece s financial market and microeconomic indexes to study their interaction.\nEmphasis is given on the period of severe financial crisis of the Country to\nunderstand contagion and volatility spillover between these two markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07063v1"
    },
    {
        "title": "The stabilizing effect of volatility in financial markets",
        "authors": [
            "Davide Valenti",
            "Giorgio Fazio",
            "Bernardo Spagnolo"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  In financial markets, greater volatility is usually considered synonym of\ngreater risk and instability. However, large market downturns and upturns are\noften preceded by long periods where price returns exhibit only small\nfluctuations. To investigate this surprising feature, here we propose using the\nmean first hitting time, i.e. the average time a stock return takes to undergo\nfor the first time a large negative or positive variation, as an indicator of\nprice stability, and relate this to a standard measure of volatility. In an\nempirical analysis of daily returns for $1071$ stocks traded in the New York\nStock Exchange, we find that this measure of stability displays nonmonotonic\nbehavior, with a maximum, as a function of volatility. Also, we show that the\nstatistical properties of the empirical data can be reproduced by a nonlinear\nHeston model. This analysis implies that, contrary to conventional wisdom, not\nonly high, but also low volatility values can be associated with higher\ninstability in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.08695v1"
    },
    {
        "title": "Time-Varying Extreme Value Dependence with Application to Leading\n  European Stock Markets",
        "authors": [
            "Daniela Castro Camilo",
            "Miguel de Carvalho",
            "Jennifer Wadsworth"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Extremal dependence between international stock markets is of particular\ninterest in today's global financial landscape. However, previous studies have\nshown this dependence is not necessarily stationary over time. We concern\nourselves with modeling extreme value dependence when that dependence is\nchanging over time, or other suitable covariate. Working within a framework of\nasymptotic dependence, we introduce a regression model for the angular density\nof a bivariate extreme value distribution that allows us to assess how extremal\ndependence evolves over a covariate. We apply the proposed model to assess the\ndynamics governing extremal dependence of some leading European stock markets\nover the last three decades, and find evidence of an increase in extremal\ndependence over recent years.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.01198v1"
    },
    {
        "title": "Testing if the market microstructure noise is fully explained by the\n  informational content of some variables from the limit order book",
        "authors": [
            "Simon Clinet",
            "Yoann Potiron"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  In this paper, we build tests for the presence of residual noise in a model\nwhere the market microstructure noise is a known parametric function of some\nvariables from the limit order book. The tests compare two distinct\nquasi-maximum likelihood estimators of volatility, where the related model\nincludes a residual noise in the market microstructure noise or not. The limit\ntheory is investigated in a general nonparametric framework. In the presence of\nresidual noise, we examine the central limit theory of the related\nquasi-maximum likelihood estimation approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.02502v4"
    },
    {
        "title": "A new approach to the modeling of financial volumes",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  In this paper we study the high frequency dynamic of financial volumes of\ntraded stocks by using a semi-Markov approach. More precisely we assume that\nthe intraday logarithmic change of volume is described by a weighted-indexed\nsemi-Markov chain model. Based on this assumptions we show that this model is\nable to reproduce several empirical facts about volume evolution like time\nseries dependence, intra-daily periodicity and volume asymmetry. Results have\nbeen obtained from a real data application to high frequency data from the\nItalian stock market from first of January 2007 until end of December 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.05823v1"
    },
    {
        "title": "Universal Lévy's stable law of stock market and its characterization",
        "authors": [
            "Takumi Fukunaga",
            "Ken Umeno"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Price fluctuations in financial markets can be characterized by L\\'evy's\nstable distribution, which is supported by the generalized central limit\nsystem. When the stable parameters were estimated from four different stock\nmarkets in long term, they similarly indicated an unique value. On the other\nhand, when analyzed in short term, parameters and the stock prices fluctuated\nwith correlation, which shows that the stock markets are instable.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06279v2"
    },
    {
        "title": "Implied volatility smile dynamics in the presence of jumps",
        "authors": [
            "Martin Magris",
            "Perttu Barholm",
            "Juho Kanniainen"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The main purpose of this work is to examine the behavior of the implied\nvolatility smiles around jumps, contributing to the literature with a\nhigh-frequency analysis of the smile dynamics based on intra-day option data.\nFrom our high-frequency SPX S\\&P500 index option dataset, we utilize the first\nthree principal components to characterize the implied volatility smile and\nanalyze its dynamics by the distribution of the scores' means and variances and\nother statistics for the first hour of the day, in scenarios where jumps are\ndetected and not. Our analyses clearly suggest that changes in the volatility\nsmiles have abnormal properties around jumps compared with the absence of\njumps, regardless of maturity and type of the option.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02925v2"
    },
    {
        "title": "A cluster driven log-volatility factor model: a deepening on the source\n  of the volatility clustering",
        "authors": [
            "Anshul Verma",
            "Riccardo Junior Buonocore",
            "Tiziana di Matteo"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We introduce a new factor model for log volatilities that performs\ndimensionality reduction and considers contributions globally through the\nmarket, and locally through cluster structure and their interactions. We do not\nassume a-priori the number of clusters in the data, instead using the Directed\nBubble Hierarchical Tree (DBHT) algorithm to fix the number of factors. We use\nthe factor model and a new integrated non parametric proxy to study how\nvolatilities contribute to volatility clustering. Globally, only the market\ncontributes to the volatility clustering. Locally for some clusters, the\ncluster itself contributes statistically to volatility clustering. This is\nsignificantly advantageous over other factor models, since the factors can be\nchosen statistically, whilst also keeping economically relevant factors.\nFinally, we show that the log volatility factor model explains a similar amount\nof memory to a Principal Components Analysis (PCA) factor model and an\nexploratory factor model.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.02138v2"
    },
    {
        "title": "Stock market as temporal network",
        "authors": [
            "Longfeng Zhao",
            "Gang-Jin Wang",
            "Mingang Wang",
            "Weiqi Bao",
            "Wei Li",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Financial networks have become extremely useful in characterizing the\nstructure of complex financial systems. Meanwhile, the time evolution property\nof the stock markets can be described by temporal networks. We utilize the\ntemporal network framework to characterize the time-evolving correlation-based\nnetworks of stock markets. The market instability can be detected by the\nevolution of the topology structure of the financial networks. We employ the\ntemporal centrality as a portfolio selection tool. Those portfolios, which are\ncomposed of peripheral stocks with low temporal centrality scores, have\nconsistently better performance under different portfolio optimization schemes,\nsuggesting that the temporal centrality measure can be used as new portfolio\noptimization and risk management tools. Our results reveal the importance of\nthe temporal attributes of the stock markets, which should be taken serious\nconsideration in real life applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.04863v1"
    },
    {
        "title": "The relationship between trading volumes, number of transactions, and\n  stock volatility in GARCH models",
        "authors": [
            "Tetsuya Takaishi",
            "Ting Ting Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We examine the relationship between trading volumes, number of transactions,\nand volatility using daily stock data of the Tokyo Stock Exchange. Following\nthe mixture of distributions hypothesis, we use trading volumes and the number\nof transactions as proxy for the rate of information arrivals affecting stock\nvolatility. The impact of trading volumes or number of transactions on\nvolatility is measured using the generalized autoregressive conditional\nheteroscedasticity (GARCH) model. We find that the GARCH effects, that is,\npersistence of volatility, is not always removed by adding trading volumes or\nnumber of transactions, indicating that trading volumes and number of\ntransactions do not adequately represent the rate of information arrivals.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.06263v1"
    },
    {
        "title": "Short- to Mid-term Day-Ahead Electricity Price Forecasting Using Futures",
        "authors": [
            "Rick Steinert",
            "Florian Ziel"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Due to the liberalization of markets, the change in the energy mix and the\nsurrounding energy laws, electricity research is a dynamically altering field\nwith steadily changing challenges. One challenge especially for investment\ndecisions is to provide reliable short to mid-term forecasts despite high\nvariation in the time series of electricity prices. This paper tackles this\nissue in a promising and novel approach. By combining the precision of\neconometric autoregressive models in the short-run with the expectations of\nmarket participants reflected in future prices for the short- and mid-run we\nshow that the forecasting performance can be vastly increased while maintaining\nhourly precision. We investigate the day-ahead electricity price of the EPEX\nSpot for Germany and Austria and setup a model which incorporates the Phelix\nfuture of the EEX for Germany and Austria. The model can be considered as an\nAR24-X model with one distinct model for each hour of the day. We are able to\nshow that future data contains relevant price information for future time\nperiods of the day-ahead electricity price. We show that relying only on\ndeterministic external regressors can provide stability for forecast horizons\nof multiple weeks. By implementing a fast and efficient lasso estimation\napproach we demonstrate that our model can outperform several other models in\nthe literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.10583v1"
    },
    {
        "title": "On the interplay between multiscaling and stocks dependence",
        "authors": [
            "R. J. Buonocore",
            "G. Brandi",
            "R. N. Mantegna",
            "T. Di Matteo"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We find a nonlinear dependence between an indicator of the degree of\nmultiscaling of log-price time series of a stock and the average correlation of\nthe stock with respect to the other stocks traded in the same market. This\nresult is a robust stylized fact holding for different financial markets. We\ninvestigate this result conditional on the stocks' capitalization and on the\nkurtosis of stocks' log-returns in order to search for possible confounding\neffects. We show that a linear dependence with the logarithm of the\ncapitalization and the logarithm of kurtosis does not explain the observed\nstylized fact, which we interpret as being originated from a deeper\nrelationship.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.01113v2"
    },
    {
        "title": "Indexed Markov Chains for financial data: testing for the number of\n  states of the index process",
        "authors": [
            "Guglielmo D'Amico",
            "Ada Lika",
            "Filippo Petroni"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  A new branch based on Markov processes is developing in the recent literature\nof financial time series modeling. In this paper, an Indexed Markov Chain has\nbeen used to model high frequency price returns of quoted firms. The\npeculiarity of this type of model is that through the introduction of an Index\nprocess it is possible to consider the market volatility endogenously and two\nvery important stylized facts of financial time series can be taken into\naccount: long memory and volatility clustering. In this paper, first we propose\na method for the optimal determination of the state space of the Index process\nwhich is based on a change-point approach for Markov chains. Furthermore we\nprovide an explicit formula for the probability distribution function of the\nfirst change of state of the index process. Results are illustrated with an\napplication to intra-day prices of a quoted Italian firm from January $1^{st}$,\n2007 to December $31^{st}$ 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.01540v1"
    },
    {
        "title": "Return Optimization Securities and Other Remarkable Structured\n  Investment Products: Indicators of Future Outcomes for U.S. Treasuries?",
        "authors": [
            "Donald St. P. Richards"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We analyze four structured products that have caused severe losses to\ninvestors in recent years. These products are: return optimization securities,\nyield magnet notes, reverse exchangeable securities, and principal-protected\nnotes. We describe the basic structure of these products, analyze them\nprobabilistically using the Law of Total Expectation, and assess the practical\nimplications of buying them in the mid-2000s. By estimating expected rates of\nreturn under various scenarios, we conclude in each case that buyers were\nlikely to experience grave difficulties from the start. By inspecting various\nprospectuses, we detect that many structured products were designed to the\ndetriment of buyers and to the advantage of the issuing banks and\nbroker-dealers. Therefore, we find it difficult to understand why any\ninvestment advisor, in exercising fiduciary care of clients' funds, would have\nadvised a client to purchases these products in the mid-2000's.\n  In light of these results, we fear that the on-going worldwide financial\ncrisis will be lengthened because of these structured products and others even\nmore arcane than the ones considered here. We note that problems caused by\nstructured products have increased investors' fears about the economy and the\nfinancial markets, causing many of them to purchase U.S. Treasury securities at\nnegative real-interest rates. This has caused increases in the prices of U.S.\nTreasuries to record levels, and we fear for the day when this trend reverses.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.00820v1"
    },
    {
        "title": "Multifractal analysis of financial markets",
        "authors": [
            "Zhi-Qiang Jiang",
            "Wen-Jie Xie",
            "Wei-Xing Zhou",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Multifractality is ubiquitously observed in complex natural and socioeconomic\nsystems. Multifractal analysis provides powerful tools to understand the\ncomplex nonlinear nature of time series in diverse fields. Inspired by its\nstriking analogy with hydrodynamic turbulence, from which the idea of\nmultifractality originated, multifractal analysis of financial markets has\nbloomed, forming one of the main directions of econophysics. We review the\nmultifractal analysis methods and multifractal models adopted in or invented\nfor financial time series and their subtle properties, which are applicable to\ntime series in other disciplines. We survey the cumulating evidence for the\npresence of multifractality in financial time series in different markets and\nat different time periods and discuss the sources of multifractality. The\nusefulness of multifractal analysis in quantifying market inefficiency, in\nsupporting risk management and in developing other applications is presented.\nWe finally discuss open problems and further directions of multifractal\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04750v1"
    },
    {
        "title": "Deep Factor Model",
        "authors": [
            "Kei Nakagawa",
            "Takumi Uchida",
            "Tomohisa Aoshima"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We propose to represent a return model and risk model in a unified manner\nwith deep learning, which is a representative model that can express a\nnonlinear relationship. Although deep learning performs quite well, it has\nsignificant disadvantages such as a lack of transparency and limitations to the\ninterpretability of the prediction. This is prone to practical problems in\nterms of accountability. Thus, we construct a multifactor model by using\ninterpretable deep learning. We implement deep learning as a return model to\npredict stock returns with various factors. Then, we present the application of\nlayer-wise relevance propagation (LRP) to decompose attributes of the predicted\nreturn as a risk model. By applying LRP to an individual stock or a portfolio\nbasis, we can determine which factor contributes to prediction. We call this\nmodel a deep factor model. We then perform an empirical analysis on the\nJapanese stock market and show that our deep factor model has better predictive\ncapability than the traditional linear model or other machine learning methods.\nIn addition , we illustrate which factor contributes to prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.01278v1"
    },
    {
        "title": "A six-factor asset pricing model",
        "authors": [
            "Rahul Roy",
            "Santhakumar Shijin"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  The present study introduce the human capital component to the Fama and\nFrench five-factor model proposing an equilibrium six-factor asset pricing\nmodel. The study employs an aggregate of four sets of portfolios mimicking size\nand industry with varying dimensions. The first set consists of three set of\nsix portfolios each sorted on size to B/M, size to investment, and size to\nmomentum. The second set comprises of five index portfolios, third, a four-set\nof twenty-five portfolios each sorted on size to B/M, size to investment, size\nto profitability, and size to momentum, and the final set constitute thirty\nindustry portfolios. To estimate the parameters of six-factor asset pricing\nmodel for the four sets of variant portfolios, we use OLS and Generalized\nmethod of moments based robust instrumental variables technique (IVGMM). The\nresults obtained from the relevance, endogeneity, overidentifying restrictions,\nand the Hausman's specification, tests indicate that the parameter estimates of\nthe six-factor model using IVGMM are robust and performs better than the OLS\napproach. The human capital component shares equally the predictive power\nalongside the factors in the framework in explaining the variations in return\non portfolios. Furthermore, we assess the t-ratio of the human capital\ncomponent of each IVGMM estimates of the six-factor asset pricing model for the\nfour sets of variant portfolios. The t-ratio of the human capital of the\neighty-three IVGMM estimates are more than 3.00 with reference to the standard\nproposed by Harvey et al. (2016). This indicates the empirical success of the\nsix-factor asset-pricing model in explaining the variation in asset returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.07790v1"
    },
    {
        "title": "Intraday Seasonalities and Nonstationarity of Trading Volume in\n  Financial Markets: Individual and Cross-Sectional Features",
        "authors": [
            "Michelle B Graczyk",
            "Silvio M D Queirós"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We study the intraday behaviour of the statistical moments of the trading\nvolume of the blue chip equities that composed the Dow Jones Industrial Average\nindex between 2003 and 2014. By splitting that time interval into semesters, we\nprovide a quantitative account of the non-stationary nature of the intraday\nstatistical properties as well. Explicitly, we prove the well-known U-shape\nexhibited by the average trading volume-as well as the volatility of the price\nfluctuations-experienced a significant change from 2008 (the year of the\nsub-prime financial crisis) onwards. That has resulted in a faster relaxation\nafter the market opening and relates to a consistent decrease in the convexity\nof the average trading volume intraday profile. Simultaneously, the last part\nof the session has become steeper as well, a modification that is likely to\nhave been triggered by the new short-selling rules that were introduced in 2007\nby the Securities and Exchange Commission. The combination of both results\nreveals that the has been turning into a t. Additionally, the analysis of\nhigher-order cumulants namely the skewness and the kurtosis-shows that the\nmorning and the afternoon parts of the trading session are each clearly\nassociated with different statistical features and hence dynamical rules.\nConcretely, we claim that the large initial trading volume is due to wayward\nstocks whereas the large volume during the last part of the session hinges on a\ncohesive increase of the trading volume. That dissimilarity between the two\nparts of the trading session is stressed in periods of higher uproar in the\nmarket.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.12099v1"
    },
    {
        "title": "Predicting future stock market structure by combining social and\n  financial network information",
        "authors": [
            "Thársis T. P. Souza",
            "Tomaso Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We demonstrate that future market correlation structure can be predicted with\nhigh out-of-sample accuracy using a multiplex network approach that combines\ninformation from social media and financial data. Market structure is measured\nby quantifying the co-movement of asset prices returns, while social structure\nis measured as the co-movement of social media opinion on those same assets.\nPredictions are obtained with a simple model that uses link persistence and\nlink formation by triadic closure across both financial and social media\nlayers. Results demonstrate that the proposed model can predict future market\nstructure with up to a 40\\% out-of-sample performance improvement compared to a\nbenchmark model that assumes a time-invariant financial correlation structure.\nSocial media information leads to improved models for all settings tested,\nparticularly in the long-term prediction of financial market structure.\nSurprisingly, financial market structure exhibited higher predictability than\nsocial opinion structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.01103v1"
    },
    {
        "title": "Evaluating the Building Blocks of a Dynamically Adaptive Systematic\n  Trading Strategy",
        "authors": [
            "Sonam Srivastava",
            "Ritabratta Bhattacharya"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Financial markets change their behaviours abruptly. The mean, variance and\ncorrelation patterns of stocks can vary dramatically, triggered by fundamental\nchanges in macroeconomic variables, policies or regulations. A trader needs to\nadapt her trading style to make the best out of the different phases in the\nstock markets. Similarly, an investor might want to invest in different asset\nclasses in different market regimes for a stable risk adjusted return profile.\nHere, we explore the use of State Switching Markov Autoregressive models for\nidentifying and predicting different market regimes loosely modeled on the\nWyckoff Price Regimes of accumulation, distribution, advance and decline. We\nexplore the behaviour of various asset classes and market sectors in the\nidentified regimes. We look at the trading strategies like trend following,\nrange trading, retracement trading and breakout trading in the given market\nregimes and tailor them for the specific regimes. We tie together the best\ntrading strategy and asset allocation for the identified market regimes to come\nup with a robust dynamically adaptive trading system to outperform simple\ntraditional alphas.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.02527v1"
    },
    {
        "title": "Cross-shareholding networks and stock price synchronicity: Evidence from\n  China",
        "authors": [
            "Fenghua Wen",
            "Yujie Yuan",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This paper investigates the effect of cross-shareholding on stock price\nsynchronicity, as a measure of price informativeness, of the listed firms in\nthe Chinese stock market. We gauge firms' levels of cross-shareholdings in\nterms of centrality in the cross-shareholding network. It is confirmed that it\nis through a noise-reducing process that cross-shareholding promotes price\nsynchronicity and reduces price delay. More importantly, this effect on price\ninformativeness is pronounced for large firms and in the periods of market\ndownturns. Overall, our analyses provide insights into the relation between the\nownership structure and price informativeness.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01655v1"
    },
    {
        "title": "Uncovering networks amongst stocks returns by studying nonlinear\n  interactions in high frequency data of the Indian Stock Market using mutual\n  information",
        "authors": [
            "Charu Sharma",
            "Amber Habib"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this paper, we explore the detection of clusters of stocks that are in\nsynergy in the Indian Stock Market and understand their behaviour in different\ncircumstances. We have based our study on high frequency data for the year\n2014. This was a year when general elections were held in India, keeping this\nin mind our data set was divided into 3 subsets, pre-election period: Jan-Feb\n2014; election period: Mar-May 2014 and :post-election period: Jun-Dec 2014. On\nanalysing the spectrum of the correlation matrix, quite a few deviations were\nobserved from RMT indicating a correlation across all the stocks. We then used\nmutual information to capture the non-linearity of the data and compared our\nresults with widely used correlation technique using minimum spanning tree\nmethod. With a larger value of power law exponent {\\alpha}, corresponding to\ndistribution of degrees in a network, the nonlinear method of mutual\ninformation succeeds in establishing effective network in comparison to the\ncorrelation method. Of the two prominent clusters detected by our analysis, one\ncorresponds to the financial sector and another to the energy sector. The\nfinancial sector emerged as an isolated, standalone cluster, which remain\nunaffected even during the election periods.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.03407v1"
    },
    {
        "title": "Stylized facts of the Indian Stock Market",
        "authors": [
            "Rituparna Sen",
            "Manavthi S"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Historical daily data for eleven years of the fifty constituent stocks of the\nNIFTY index traded on the National Stock Exchange have been analyzed to check\nfor the stylized facts in the Indian market. It is observed that while some\nstylized facts of other markets are also observed in Indian market, there are\nsignificant deviations in three main aspects, namely leverage, asymmetry and\nautocorrelation. Leverage and asymmetry are both reversed making this a more\npromising market to invest in. While significant autocorrelation observed in\nthe returns points towards market inefficiency, the increased predictive power\nis better for investors.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.05322v1"
    },
    {
        "title": "Dynamic Hurst Exponent in Time Series",
        "authors": [
            "Carlos Arturo Soto Campos",
            "Leopoldo Sánchez Cantú",
            "Zeus Hernández Veleros"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The market efficiency hypothesis has been proposed to explain the behavior of\ntime series of stock markets. The Black-Scholes model (B-S) for example, is\nbased on the assumption that markets are efficient. As a consequence, it is\nimpossible, at least in principle, to \"predict\" how a market behaves, whatever\nthe circumstances. Recently we have found evidence which shows that it is\npossible to find self-organized behavior in the prices of assets in financial\nmarkets during deep falls of those prices. Through a kurtosis analysis we have\nidentified a critical point that separates time series from stock markets in\ntwo different regimes: the mesokurtic segment compatible with a random walk\nregime and the leptokurtic one that allegedly follows a power law behavior. In\nthis paper we provide some evidence, showing that the Hurst exponent is a good\nestimator of the regime in which the market is operating. Finally, we propose\nthat the Hurst exponent can be considered as a critical variable in just the\nsame way as magnetization, for example, can be used to distinguish the phase of\na magnetic system in physics.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.07809v1"
    },
    {
        "title": "Transaction Cost Analytics for Corporate Bonds",
        "authors": [
            "Xin Guo",
            "Charles-Albert Lehalle",
            "Renyuan Xu"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The electronic platform has been increasingly popular for executing large\ncorporate bond orders by asset managers, who in turn have to assess the quality\nof their executions via Transaction Cost Analysis (TCA). One of the challenges\nin TCA is to build a realistic benchmark for the expected transaction cost and\nto characterize the price impact of each individual trade with given bond\ncharacteristics and market conditions.\n  Taking the viewpoint of retail investors, this paper presents an analytical\nmethodology for TCA of corporate bond trading. Our analysis is based on the\nTRACE Enhanced dataset; and starts with estimating the initiator of a bond\ntransaction, followed by estimating the bid-ask spread and the mid-price\ndynamics. With these estimations, the first part of our study is to identify\nkey features for corporate bonds and to compute the expected average trading\ncost. This part is on the time scale of weekly transactions, and is by applying\nand comparing several regularized regression models. The second part of our\nstudy is using the estimated mid-price dynamics to investigate the amplitude of\nits price impact and the decay pattern of individual bond transaction. This\npart is on the time scale of each transaction of liquid corporate bonds, and is\nby applying a transient impact model to estimate the price impact kernel using\na non-parametric method.\n  Our benchmark model allows for identifying abnormal transactions and for\nenhancing counter-party selections. A key discovery of our study is the price\nimpact asymmetry between customer-buy orders and consumer-sell orders.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09140v4"
    },
    {
        "title": "Sutte Indicator: an approach to predict the direction of stock market\n  movements",
        "authors": [
            "Ansari Saleh Ahmar"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The purpose of this research is to apply technical analysis of Sutte\nIndicator in stock trading which will assist in the investment decision making\nprocess i.e. buying or selling shares. This research takes data of \"A\" on the\nIndonesia Stock Exchange(IDX or BEI) 29 November 2006 until 20 September 2016\nperiod. To see the performance of Sutte Indicator, other technical analysis are\nused as a comparison, Simple Moving Average (SMA) and Moving Average\nConvergence/Divergence (MACD). To see a comparison of the level of reliability\nprediction, the stock data were compared using the mean absolute deviation\n(MAD), mean of square error (MSE), and mean absolute percentage error (MAPE).\nThe result of this research is that Sutte Indicator can be used as a reference\nin predicting stock movements, and if it is compared to other indicator methods\n(SMA and MACD) via MAD, MSE, and MAPE, the Sutte Indicator has a better level\nof reliability.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11642v1"
    },
    {
        "title": "The Time Function of Stock Price",
        "authors": [
            "Shengfeng Mei",
            "Hong Gao"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This paper tends to define the quantitative relationship between the stock\nprice and time as a time function. Based on the empirical evidence that the\nlog-return of a stock is the series of white noise, a mathematical model of the\nintegral white noise is established to describe the phenomenon of stock price\nmovement. A deductive approach is used to derive the auto-correlation function,\ndisplacement formula and power spectral density of the stock price movement,\nwhich reveals not only the characteristics and rules of the movement but also\nthe predictability of the stock price. The deductive fundamental is provided\nfor the price analysis, prediction and risk management of portfolio investment.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.11806v2"
    },
    {
        "title": "Approximation of eigenvalues of spot cross volatility matrix with a view\n  toward principal component analysis",
        "authors": [
            "Nien-Lin Liu",
            "Hoang-Long Ngo"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In order to study the geometry of interest rates market dynamics, Malliavin,\nMancino and Recchioni [A non-parametric calibration of the HJM geometry: an\napplication of It\\^o calculus to financial statistics, {\\it Japanese Journal of\nMathematics}, 2, pp.55--77, 2007] introduced a scheme, which is based on the\nFourier Series method, to estimate eigenvalues of a spot cross volatility\nmatrix. In this paper, we present another estimation scheme based on the\nQuadratic Variation method. We first establish limit theorems for each scheme\nand then we use a stochastic volatility model of Heston's type to compare the\neffectiveness of these two schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.2214v1"
    },
    {
        "title": "Empirical Study of the 1-2-3 Trend Indicator",
        "authors": [
            "Yasemin Hafizogullari",
            "Stanislaus Maier-Paape",
            "Andreas Platen"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In this paper we study automatically recognized trends and investigate their\nstatistics. To do that we introduce the notion of a wavelength for time series\nvia cross correlation and use this wavelength to calibrate the 1-2-3 trend\nindicator of Maier-Paape [Automatic One Two Three, Quantitative Finance, 2013]\nto automatically find trends. Extensive statistics are reported for EUR-USD,\nDAX-Future, Gold and Crude Oil regarding e.g. the dynamic, duration and\nextension of trends on different time scales.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.5321v1"
    },
    {
        "title": "Signal Diffusion Mapping: Optimal Forecasting with Time Varying Lags",
        "authors": [
            "Paul Gaskell",
            "Frank McGroarty",
            "Thanassis Tiropanis"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We introduce a new methodology for forecasting which we call Signal Diffusion\nMapping. Our approach accommodates features of real world financial data which\nhave been ignored historically in existing forecasting methodologies. Our\nmethod builds upon well-established and accepted methods from other areas of\nstatistical analysis. We develop and adapt those models for use in forecasting.\nWe also present tests of our model on data in which we demonstrate the efficacy\nof our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.6443v1"
    },
    {
        "title": "Entropy and Optimization of Portfolios",
        "authors": [
            "Krzysztof Urbanowicz"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We briefly review the approach to optimization of portfolios according to the\ntheory of Markowitz and propose a further modification that can improve the\noutcome of the optimization process. The modification takes account of the\nentropic contribution from the time series used to compute the parameters in\nthe Markowitz method.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.7002v1"
    },
    {
        "title": "Time Evolution of Non-linear Currency Networks",
        "authors": [
            "Paweł Fiedor",
            "Artur Hołda"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Financial markets are complex adaptive systems, and are commonly studied as\ncomplex networks. Most of such studies fall short in two respects: they do not\naccount for non-linearity of the studied relationships, and they create one\nnetwork for the whole studied time series, providing an average picture of a\nvery long, economically non-homogeneous, period. In this study we look at the\ncurrency markets by creating networks which can account for non-linearity in\nthe underlying relationships, and are based on short time horizons with the use\nof running window approach. Since information--theoretic measures are slow to\nconverge, we use Hirschfeld-Gebelein-Renyi Maximum Correlation Coefficient as a\nmeasure of the relationships between currencies. We use the Randomized\nDependence Coefficient (RDC) as an estimator of the above. It measures the\ndependence between random samples as the largest canonical correlation between\nk randomly chosen non-linear projections of their copula transformations. On\nthis basis we create full graphs, and further filter them into minimally\nspanning trees. We create such networks for each window moving along the\nstudied time series, and analyse the time evolution of various network\ncharacteristics, particularly the degree distributions, and their economic\nsignificance. We apply this procedure to a dataset describing logarithmic\nchanges in exchange rates in relation to silver for 27 world currencies for the\nyears between 2002 and 2013.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.8609v1"
    },
    {
        "title": "Plunges in the Bombay stock exchange: Characteristics and indicators",
        "authors": [
            "Kinjal Banerjee",
            "Chandradew Sharma",
            "N. Bittu"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We study the various sectors of the Bombay Stock Exchange (BSE) for a period\nof eight years from January 2006 to March 2014. Using the data of the daily\nreturns of a period of eight years we investigate the financial cross\ncorrelation co-efficients among the sectors of BSE and Price by Earning (PE)\nratio of BSE Sensex. We show that the behavior of these quantities during\nnormal periods and during crisis is very different. We show that the PE ratio\nshows a particular distinctive trend in the approach to a crash of the\nfinancial market and can therefore be used as an indicator of an impending\ncatastrophe. We propose that a model of analysis of crashes in a financial\nmarket can be built using two parameters: (i) the PE ratio and (ii) the largest\neigenvalue of the cross correlation matrix.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05508v1"
    },
    {
        "title": "Power-law tails in the distribution of order imbalance",
        "authors": [
            "T. Zhang",
            "G. -F. Gu",
            "H. -C. Xu",
            "X. Xiong",
            "W. Chen",
            "W. -X. Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We investigate the probability distribution of order imbalance calculated\nfrom the order flow data of 43 Chinese stocks traded on the Shenzhen Stock\nExchange. Two definitions of order imbalance are considered based on the order\nnumber and the order size. We find that the order imbalance distributions of\nindividual stocks have power-law tails. However, the tail index fluctuates\nremarkably from stock to stock. We also investigate the distributions of\naggregated order imbalance of all stocks at different timescales $\\Delta{t}$.\nWe find no clear trend in the tail index with respect $\\Delta{t}$. All the\nanalyses suggest that the distributions of order imbalance are asymmetric.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05550v1"
    },
    {
        "title": "Lagrange regularisation approach to compare nested data sets and\n  determine objectively financial bubbles' inceptions",
        "authors": [
            "Guilherme Demos",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Inspired by the question of identifying the start time $\\tau$ of financial\nbubbles, we address the calibration of time series in which the inception of\nthe latest regime of interest is unknown. By taking into account the tendency\nof a given model to overfit data, we introduce the Lagrange regularisation of\nthe normalised sum of the squared residuals, $\\chi^{2}_{np}(\\Phi)$, to\nendogenously detect the optimal fitting window size := $w^* \\in\n[\\tau:\\bar{t}_2]$ that should be used for calibration purposes for a fixed\npseudo present time $\\bar{t}_2$. The performance of the Lagrange regularisation\nof $\\chi^{2}_{np}(\\Phi)$ defined as $\\chi^{2}_{\\lambda (\\Phi)}$ is exemplified\non a simple Linear Regression problem with a change point and compared against\nthe Residual Sum of Squares (RSS) := $\\chi^{2}(\\Phi)$ and RSS/(N-p):=\n$\\chi^{2}_{np}(\\Phi)$, where $N$ is the sample size and p is the number of\ndegrees of freedom. Applied to synthetic models of financial bubbles with a\nwell-defined transition regime and to a number of financial time series (US\nS\\&P500, Brazil IBovespa and China SSEC Indices), the Lagrange regularisation\nof $\\chi^{2}_{\\lambda}(\\Phi)$ is found to provide well-defined reasonable\ndeterminations of the starting times for major bubbles such as the bubbles\nending with the 1987 Black-Monday, the 2008 Sub-prime crisis and minor\nspeculative bubbles on other Indexes, without any further exogenous\ninformation. It thus allows one to endogenise the determination of the\nbeginning time of bubbles, a problem that had not received previously a\nsystematic objective solution.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07162v1"
    },
    {
        "title": "Statistical properties and multifractality of Bitcoin",
        "authors": [
            "Tetsuya Takaishi"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Using 1-min returns of Bitcoin prices, we investigate statistical properties\nand multifractality of a Bitcoin time series. We find that the 1-min return\ndistribution is fat-tailed, and kurtosis largely deviates from the Gaussian\nexpectation. Although for large sampling periods, kurtosis is anticipated to\napproach the Gaussian expectation, we find that convergence to that is very\nslow. Skewness is found to be negative at time scales shorter than one day and\nbecomes consistent with zero at time scales longer than about one week. We also\ninvestigate daily volatility-asymmetry by using GARCH, GJR, and RGARCH models,\nand find no evidence of it. On exploring multifractality using multifractal\ndetrended fluctuation analysis, we find that the Bitcoin time series exhibits\nmultifractality. The sources of multifractality are investigated, confirming\nthat both temporal correlation and the fat-tailed distribution contribute to\nit. The influence of \"Brexit\" on June 23, 2016 to GBP--USD exchange rate and\nBitcoin is examined in multifractal properties. We find that, while Brexit\ninfluenced the GBP--USD exchange rate, Bitcoin was robust to Brexit.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07618v3"
    },
    {
        "title": "PCA for Implied Volatility Surfaces",
        "authors": [
            "Marco Avellaneda",
            "Brian Healy",
            "Andrew Papanicolaou",
            "George Papanicolaou"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Principal component analysis (PCA) is a useful tool when trying to construct\nfactor models from historical asset returns. For the implied volatilities of\nU.S. equities there is a PCA-based model with a principal eigenportfolio whose\nreturn time series lies close to that of an overarching market factor. The\nauthors show that this market factor is the index resulting from the daily\ncompounding of a weighted average of implied-volatility returns, with weights\nbased on the options' open interest (OI) and Vega. The authors also analyze the\nsingular vectors derived from the tensor structure of the implied volatilities\nof S&P500 constituents, and find evidence indicating that some type of OI and\nVega-weighted index should be one of at least two significant factors in this\nmarket.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.00085v1"
    },
    {
        "title": "Crude oil price forecasting incorporating news text",
        "authors": [
            "Yun Bai",
            "Xixi Li",
            "Hao Yu",
            "Suling Jia"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Sparse and short news headlines can be arbitrary, noisy, and ambiguous,\nmaking it difficult for classic topic model LDA (latent Dirichlet allocation)\ndesigned for accommodating long text to discover knowledge from them.\nNonetheless, some of the existing research about text-based crude oil\nforecasting employs LDA to explore topics from news headlines, resulting in a\nmismatch between the short text and the topic model and further affecting the\nforecasting performance. Exploiting advanced and appropriate methods to\nconstruct high-quality features from news headlines becomes crucial in crude\noil forecasting. To tackle this issue, this paper introduces two novel\nindicators of topic and sentiment for the short and sparse text data. Empirical\nexperiments show that AdaBoost.RT with our proposed text indicators, with a\nmore comprehensive view and characterization of the short and sparse text data,\noutperforms the other benchmarks. Another significant merit is that our method\nalso yields good forecasting performance when applied to other futures\ncommodities.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.02010v4"
    },
    {
        "title": "Analysis of intra-day fluctuations in the Mexican financial market index",
        "authors": [
            "Léster Alfonso",
            "Danahe E. Garcia-Ramirez",
            "Ricardo Mansilla",
            "César A. Terrero-Escalante"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper, a statistical analysis of high frequency fluctuations of the\nIPC, the Mexican Stock Market Index, is presented. A sample of tick-to-tick\ndata covering the period from January 1999 to December 2002 was analyzed, as\nwell as several other sets obtained using temporal aggregation. Our results\nindicates that the highest frequency is not useful to understand the Mexican\nmarket because almost two thirds of the information corresponds to inactivity.\nFor the frequency where fluctuations start to be relevant, the IPC data does\nnot follows any alpha-stable distribution, including the Gaussian, perhaps\nbecause of the presence of autocorrelations. For a long range of\nlower-frequencies, but still in the intra-day regime, fluctuations can be\ndescribed as a truncated L\\'evy flight, while for frequencies above two-days, a\nGaussian distribution yields the best fit. Thought these results are consistent\nwith other previously reported for several markets, there are significant\ndifferences in the details of the corresponding descriptions.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05697v1"
    },
    {
        "title": "Trimming the Sail: A Second-order Learning Paradigm for Stock Prediction",
        "authors": [
            "Chi Chen",
            "Li Zhao",
            "Wei Cao",
            "Jiang Bian",
            "Chunxiao Xing"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Nowadays, machine learning methods have been widely used in stock prediction.\nTraditional approaches assume an identical data distribution, under which a\nlearned model on the training data is fixed and applied directly in the test\ndata. Although such assumption has made traditional machine learning techniques\nsucceed in many real-world tasks, the highly dynamic nature of the stock market\ninvalidates the strict assumption in stock prediction. To address this\nchallenge, we propose the second-order identical distribution assumption, where\nthe data distribution is assumed to be fluctuating over time with certain\npatterns. Based on such assumption, we develop a second-order learning paradigm\nwith multi-scale patterns. Extensive experiments on real-world Chinese stock\ndata demonstrate the effectiveness of our second-order learning paradigm in\nstock prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.06878v1"
    },
    {
        "title": "Measuring capital market efficiency: Long-term memory, fractal dimension\n  and approximate entropy",
        "authors": [
            "Ladislav Kristoufek",
            "Miloslav Vosvrda"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We utilize long-term memory, fractal dimension and approximate entropy as\ninput variables for the Efficiency Index [Kristoufek & Vosvrda (2013), Physica\nA 392]. This way, we are able to comment on stock market efficiency after\ncontrolling for different types of inefficiencies. Applying the methodology on\n38 stock market indices across the world, we find that the most efficient\nmarkets are situated in the Eurozone (the Netherlands, France and Germany) and\nthe least efficient ones in the Latin America (Venezuela and Chile).\n",
        "pdf_link": "http://arxiv.org/pdf/1307.3060v3"
    },
    {
        "title": "Predicting financial markets with Google Trends and not so random\n  keywords",
        "authors": [
            "Damien Challet",
            "Ahmed Bel Hadj Ayed"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We check the claims that data from Google Trends contain enough data to\npredict future financial index returns. We first discuss the many subtle (and\nless subtle) biases that may affect the backtest of a trading strategy,\nparticularly when based on such data. Expectedly, the choice of keywords is\ncrucial: by using an industry-grade backtesting system, we verify that random\nfinance-related keywords do not to contain more exploitable predictive\ninformation than random keywords related to illnesses, classic cars and arcade\ngames. We however show that other keywords applied on suitable assets yield\nrobustly profitable strategies, thereby confirming the intuition of Preis et\nal. (2013)\n",
        "pdf_link": "http://arxiv.org/pdf/1307.4643v3"
    },
    {
        "title": "Testing power-law cross-correlations: Rescaled covariance test",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We introduce a new test for detection of power-law cross-correlations among a\npair of time series - the rescaled covariance test. The test is based on a\npower-law divergence of the covariance of the partial sums of the long-range\ncross-correlated processes. Utilizing a heteroskedasticity and auto-correlation\nrobust estimator of the long-term covariance, we develop a test with desirable\nstatistical properties which is well able to distinguish between short- and\nlong-range cross-correlations. Such test should be used as a starting point in\nthe analysis of long-range cross-correlations prior to an estimation of\nbivariate long-term memory parameters. As an application, we show that the\nrelationship between volatility and traded volume, and volatility and returns\nin the financial markets can be labeled as the one with power-law\ncross-correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.4727v2"
    },
    {
        "title": "Mixed-correlated ARFIMA processes for power-law cross-correlations",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We introduce a general framework of the Mixed-correlated ARFIMA (MC-ARFIMA)\nprocesses which allows for various specifications of univariate and bivariate\nlong-term memory. Apart from a standard case when $H_{xy}={1}{2}(H_x+H_y)$,\nMC-ARFIMA also allows for processes with $H_{xy}<{1}{2}(H_x+H_y)$ but also for\nlong-range correlated processes which are either short-range cross-correlated\nor simply correlated. The major contribution of MC-ARFIMA lays in the fact that\nthe processes have well-defined asymptotic properties for $H_x$, $H_y$ and\n$H_{xy}$, which are derived in the paper, so that the processes can be used in\nsimulation studies comparing various estimators of the bivariate Hurst exponent\n$H_{xy}$. Moreover, the framework allows for modeling of processes which are\nfound to have $H_{xy}<{1}{2}(H_x+H_y)$.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.6046v2"
    },
    {
        "title": "Extracting information from the signature of a financial data stream",
        "authors": [
            "Lajos Gergely Gyurkó",
            "Terry Lyons",
            "Mark Kontkowski",
            "Jonathan Field"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Market events such as order placement and order cancellation are examples of\nthe complex and substantial flow of data that surrounds a modern financial\nengineer. New mathematical techniques, developed to describe the interactions\nof complex oscillatory systems (known as the theory of rough paths) provides\nnew tools for analysing and describing these data streams and extracting the\nvital information. In this paper we illustrate how a very small number of\ncoefficients obtained from the signature of financial data can be sufficient to\nclassify this data for subtle underlying features and make useful predictions.\n  This paper presents financial examples in which we learn from data and then\nproceed to classify fresh streams. The classification is based on features of\nstreams that are specified through the coordinates of the signature of the\npath. At a mathematical level the signature is a faithful transform of a\nmultidimensional time series. (Ben Hambly and Terry Lyons \\cite{uniqueSig}),\nHao Ni and Terry Lyons \\cite{NiLyons} introduced the possibility of its use to\nunderstand financial data and pointed to the potential this approach has for\nmachine learning and prediction.\n  We evaluate and refine these theoretical suggestions against practical\nexamples of interest and present a few motivating experiments which demonstrate\ninformation the signature can easily capture in a non-parametric way avoiding\ntraditional statistical modelling of the data. In the first experiment we\nidentify atypical market behaviour across standard 30-minute time buckets\nsampled from the WTI crude oil future market (NYMEX). The second and third\nexperiments aim to characterise the market \"impact\" of and distinguish between\nparent orders generated by two different trade execution algorithms on the FTSE\n100 Index futures market listed on NYSE Liffe.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.7244v2"
    },
    {
        "title": "Regularizing Portfolio Risk Analysis: A Bayesian Approach",
        "authors": [
            "Sourish Das",
            "Aritra Halder",
            "Dipak K. Dey"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  It is important for a portfolio manager to estimate and analyze recent\nportfolio volatility to keep the portfolio's risk within limit. Though the\nnumber of financial instruments in the portfolio can be very large, sometimes\nmore than thousands, daily returns considered for analysis are only for a month\nor even less. In this case rank of portfolio covariance matrix is less than\nfull, hence solution is not unique. It is typically known as the ``ill-posed\"\nproblem. In this paper we discuss a Bayesian approach to regularize the\nproblem. One of the additional advantages of this approach is to analyze the\nsource of risk by estimating the probability of positive `conditional\ncontribution to total risk' (CCTR). Each source's CCTR would sum up to the\nportfolio's total volatility risk. Existing methods only estimate CCTR of a\nsource, and does not estimate the probability of CCTR to be significantly\ngreater (or less) than zero. This paper presents Bayesian methodology to do so.\nWe use a parallelizable and easy to use Monte Carlo (MC) approach to achieve\nour objective. Estimation of various risk measures, such as Value at Risk and\nExpected Shortfall, becomes a by-product of this Monte-Carlo approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.3258v2"
    },
    {
        "title": "Braided and Knotted Stocks in the Stock Market: Anticipating the flash\n  crashes",
        "authors": [
            "Ovidiu Racorean"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  A simple and elegant arrangement of stock components of a portfolio (market\nindex-DJIA) in a recent paper [1], has led to the construction of crossing of\nstocks diagram. The crossing stocks method revealed hidden remarkable algebraic\nand geometrical aspects of stock market. The present paper continues to uncover\nnew mathematical structures residing from crossings of stocks diagram by\nintroducing topological properties stock market is endowed with. The crossings\nof stocks are categorized as overcrossings and undercrossings and interpreted\nas generators of braid that stocks form in the process of prices quotations in\nthe market. Topological structure of the stock market is even richer if the\nclosure of stocks braid is considered, such that it forms a knot. To\ndistinguish the kind of knot that stock market forms, Alexander-Conway\npolynomial and the Jones polynomials are calculated for some knotted stocks.\nThese invariants of knots are important for the future practical applications\ntopological stock market might have. Such application may account of the\nrelation between Jones polynomial and phase transition statistical models to\nprovide a clear way to anticipate the transition of financial markets to the\nphase that leads to crisis. The resemblance between braided stocks and logic\ngates of topological quantum computers could quantum encode the stock market\nbehavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.6637v2"
    },
    {
        "title": "Impact of non-stationarity on estimating and modeling empirical copulas\n  of daily stock returns",
        "authors": [
            "Marcel Wollschläger",
            "Rudi Schäfer"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  All too often measuring statistical dependencies between financial time\nseries is reduced to a linear correlation coefficient. However this may not\ncapture all facets of reality. We study empirical dependencies of daily stock\nreturns by their pairwise copulas. Here we investigate particularly to which\nextent the non-stationarity of financial time series affects both the\nestimation and the modeling of empirical copulas. We estimate empirical copulas\nfrom the non-stationary, original return time series and stationary, locally\nnormalized ones. Thereby we are able to explore the empirical dependence\nstructure on two different scales: a global and a local one. Additionally the\nasymmetry of the empirical copulas is emphasized as a fundamental\ncharacteristic. We compare our empirical findings with a single Gaussian\ncopula, with a correlation-weighted average of Gaussian copulas, with the\nK-copula directly addressing the non-stationarity of dependencies as a model\nparameter, and with the skewed Student's t-copula. The K-copula covers the\nempirical dependence structure on the local scale most adequately, whereas the\nskewed Student's t-copula best captures the asymmetry of the empirical copula\non the global scale.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08054v1"
    },
    {
        "title": "Sparse Mean-Variance Portfolios: A Penalized Utility Approach",
        "authors": [
            "David Puelz",
            "P. Richard Hahn",
            "Carlos M. Carvalho"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  This paper considers mean-variance optimization under uncertainty,\nspecifically when one desires a sparsified set of optimal portfolio weights.\nFrom the standpoint of a Bayesian investor, our approach produces a small\nportfolio from many potential assets while acknowledging uncertainty in asset\nreturns and parameter estimates. We demonstrate the procedure using static and\ndynamic models for asset returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.02310v4"
    },
    {
        "title": "Hawkes process model with a time-dependent background rate and its\n  application to high-frequency financial data",
        "authors": [
            "Takahiro Omi",
            "Yoshito Hirata",
            "Kazuyuki Aihara"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  A Hawkes process model with a time-varying background rate is developed for\nanalyzing the high-frequency financial data. In our model, the logarithm of the\nbackground rate is modeled by a linear model with a relatively large number of\nvariable-width basis functions, and the parameters are estimated by a Bayesian\nmethod. Our model can capture not only the slow time-variation, such as in the\nintraday seasonality, but also the rapid one, which follows a macroeconomic\nnews announcement. By analyzing the tick data of the Nikkei 225 mini, we find\nthat (i) our model is better fitted to the data than the Hawkes models with a\nconstant background rate or a slowly varying background rate, which have been\ncommonly used in the field of quantitative finance; (ii) the improvement in the\ngoodness-of-fit to the data by our model is significant especially for sessions\nwhere considerable fluctuation of the background rate is present; and (iii) our\nmodel is statistically consistent with the data. The branching ratio, which\nquantifies the level of the endogeneity of markets, estimated by our model is\n0.41, suggesting the relative importance of exogenous factors in the market\ndynamics. We also demonstrate that it is critically important to appropriately\nmodel the time-dependent background rate for the branching ratio estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.04443v4"
    },
    {
        "title": "Relation between regional uncertainty spillovers in the global banking\n  system",
        "authors": [
            "Sachapon Tungsong",
            "Fabio Caccioli",
            "Tomaso Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We report on time-varying network connectedness within three banking systems:\nNorth America, the EU, and ASEAN. The original method by Diebold and Yilmaz is\nimproved by using exponentially weighted daily returns and ridge regularization\non vector autoregression (VAR) and forecast error variance decomposition\n(FEVD). We compute the total network connectedness for each of the three\nbanking systems, which quantifies regional uncertainty. Results over rolling\nwindows of 300 days during the period between 2005 and 2015 reveal changing\nuncertainty patterns which are similar across regions, with common peaks\nassociated with identifiable exogenous events. Lead-lag relationships among\nchanges of total network connectedness of the three systems, quantified by\ntransfer entropy, reveal that uncertainties in the three regional systems are\nsignificantly causally related, with the North American system having the\nlargest influence on EU and ASEAN.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.05944v1"
    },
    {
        "title": "Evidence for criticality in financial data",
        "authors": [
            "G. Ruiz López",
            "A. Fernández de Marcos"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We provide evidence that cumulative distributions of absolute normalized\nreturns for the $100$ American companies with the highest market\ncapitalization, uncover a critical behavior for different time scales $\\Delta\nt$. Such cumulative distributions, in accordance with a variety of complex\n--and financial-- systems, can be modeled by the cumulative distribution\nfunctions of $q$-Gaussians, the distribution function that, in the context of\nnonextensive statistical mechanics, maximizes a non-Boltzmannian entropy. These\n$q$-Gaussians are characterized by two parameters, namely $(q,\\beta)$, that are\nuniquely defined by $\\Delta t$. From these dependencies, we find a monotonic\nrelationship between $q$ and $\\beta$, which can be seen as evidence of\ncriticality. We numerically determine the various exponents which characterize\nthis criticality.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.06191v1"
    },
    {
        "title": "Information measure for financial time series: quantifying short-term\n  market heterogeneity",
        "authors": [
            "Linda Ponta",
            "Anna Carbone"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  A well-interpretable measure of information has been recently proposed based\non a partition obtained by intersecting a random sequence with its moving\naverage. The partition yields disjoint sets of the sequence, which are then\nranked according to their size to form a probability distribution function and\nfinally fed in the expression of the Shannon entropy. In this work, such\nentropy measure is implemented on the time series of prices and volatilities of\nsix financial markets. The analysis has been performed, on tick-by-tick data\nsampled every minute for six years of data from 1999 to 2004, for a broad range\nof moving average windows and volatility horizons. The study shows that the\nentropy of the volatility series depends on the individual market, while the\nentropy of the price series is practically a market-invariant for the six\nmarkets. Finally, a cumulative information measure - the `Market Heterogeneity\nIndex'- is derived from the integral of the proposed entropy measure. The\nvalues of the Market Heterogeneity Index are discussed as possible tools for\noptimal portfolio construction and compared with those obtained by using the\nSharpe ratio a traditional risk diversity measure.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.07331v2"
    },
    {
        "title": "Profitability of simple stationary technical trading rules with\n  high-frequency data of Chinese Index Futures",
        "authors": [
            "Jing-Chao Chen",
            "Yu Zhou",
            "Xi Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Technical trading rules have been widely used by practitioners in financial\nmarkets for a long time. The profitability remains controversial and few\nconsider the stationarity of technical indicators used in trading rules. We\nconvert MA, KDJ and Bollinger bands into stationary processes and investigate\nthe profitability of these trading rules by using 3 high-frequency data(15s,30s\nand 60s) of CSI300 Stock Index Futures from January 4th 2012 to December 31st\n2016. Several performance and risk measures are adopted to assess the practical\nvalue of all trading rules directly while ADF-test is used to verify the\nstationarity and SPA test to check whether trading rules perform well due to\nintrinsic superiority or pure luck. The results show that there are several\nsignificant combinations of parameters for each indicator when transaction\ncosts are not taken into consideration. Once transaction costs are included,\ntrading profits will be eliminated completely. We also propose a method to\nreduce the risk of technical trading rules.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.07470v1"
    },
    {
        "title": "Multi-Likelihood Methods for Developing Stock Relationship Networks\n  Using Financial Big Data",
        "authors": [
            "Xue Guo",
            "Hu Zhang",
            "Tianhai Tian"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Development of stock networks is an important approach to explore the\nrelationship between different stocks in the era of big-data. Although a number\nof methods have been designed to construct the stock correlation networks, it\nis still a challenge to balance the selection of prominent correlations and\nconnectivity of networks. To address this issue, we propose a new approach to\nselect essential edges in stock networks and also maintain the connectivity of\nestablished networks. This approach uses different threshold values for\nchoosing the edges connecting to a particular stock, rather than employing a\nsingle threshold value in the existing asset-value method. The innovation of\nour algorithm includes the multiple distributions in a maximum likelihood\nestimator for selecting the threshold value rather than the single distribution\nestimator in the existing methods. Using the Chinese Shanghai security market\ndata of 151 stocks, we develop a stock relationship network and analyze the\ntopological properties of the developed network. Our results suggest that the\nproposed method is able to develop networks that maintain appropriate\nconnectivities in the type of assets threshold methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.08088v1"
    },
    {
        "title": "Is Factor Momentum More than Stock Momentum?",
        "authors": [
            "Antoine Falck",
            "Adam Rej",
            "David Thesmar"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Yes, but only at short lags. In this paper we investigate the relationship\nbetween factor momentum and stock momentum. Using a sample of 72 factors\ndocumented in the literature, we first replicate earlier findings that factor\nmomentum exists and works both directionally and cross-sectionally. We then ask\nif factor momentum is spanned by stock momentum. A simple spanning test reveals\nthat after controlling for stock momentum and factor exposure, statistically\nsignificant Sharpe ratios only belong to implementations which include the last\nmonth of returns. We conclude this study with a simple theoretical model that\ncaptures these forces: (1) there is stock-level mean reversion at short lags\nand momentum at longer lags, (2) there is stock and factor momentum at all lags\nand (3) there is natural comovement between the PNLs of stock and factor\nmomentums at all horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.04824v1"
    },
    {
        "title": "Forecasting the Leading Indicator of a Recession: The 10-Year minus\n  3-Month Treasury Yield Spread",
        "authors": [
            "Sudiksha Joshi"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this research paper, I have applied various econometric time series and\ntwo machine learning models to forecast the daily data on the yield spread.\nFirst, I decomposed the yield curve into its principal components, then\nsimulated various paths of the yield spread using the Vasicek model. After\nconstructing univariate ARIMA models, and multivariate models such as ARIMAX,\nVAR, and Long Short Term Memory, I calibrated the root mean squared error to\nmeasure how far the results deviate from the current values. Through impulse\nresponse functions, I measured the impact of various shocks on the difference\nyield spread. The results indicate that the parsimonious univariate ARIMA model\noutperforms the richly parameterized VAR method, and the complex LSTM with\nmultivariate data performs equally well as the simple ARIMA model.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.05507v1"
    },
    {
        "title": "Covid-19 impact on cryptocurrencies: evidence from a wavelet-based Hurst\n  exponent",
        "authors": [
            "M. Belén Arouxet",
            "Aurelio F. Bariviera",
            "Verónica E. Pastor",
            "Victoria Vampa"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Cryptocurrency history begins in 2008 as a means of payment proposal.\nHowever, cryptocurrencies evolved into complex, high yield speculative assets.\nContrary to traditional financial instruments, they are not (mostly) traded in\norganized, law-abiding venues, but on online platforms, where anonymity reigns.\nThis paper examines the long term memory in return and volatility, using high\nfrequency time series of eleven important coins. Our study covers the\npre-Covid-19 and the subsequent pandemia period. We use a recently developed\nmethod, based on the wavelet transform, which provides more robust estimators\nof the Hurst exponent. We detect that, during the peak of Covid-19 pandemic\n(around March 2020), the long memory of returns was only mildly affected.\nHowever, volatility suffered a temporary impact in its long range correlation\nstructure. Our results could be of interest for both academics and\npractitioners.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.05652v1"
    },
    {
        "title": "Recent scaling properties of Bitcoin price returns",
        "authors": [
            "Tetsuya Takaishi"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  While relevant stylized facts are observed for Bitcoin markets, we find a\ndistinct property for the scaling behavior of the cumulative return\ndistribution. For various assets, the tail index $\\mu$ of the cumulative return\ndistribution exhibits $\\mu \\approx 3$, which is referred to as \"the inverse\ncubic law.\" On the other hand, that of the Bitcoin return is claimed to be $\\mu\n\\approx 2$, which is known as \"the inverse square law.\" We investigate the\nscaling properties using recent Bitcoin data and find that the tail index\nchanges to $\\mu \\approx 3$, which is consistent with the inverse cubic law.\nThis suggests that some properties of the Bitcoin market could vary over time.\nWe also investigate the autocorrelation of absolute returns and find that it is\ndescribed by a power-law with two scaling exponents. By analyzing the absolute\nreturns standardized by the realized volatility, we verify that the Bitcoin\nreturn time series is consistent with normal random variables with time-varying\nvolatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06874v1"
    },
    {
        "title": "Data driven value-at-risk forecasting using a SVR-GARCH-KDE hybrid",
        "authors": [
            "Marius Lux",
            "Wolfgang Karl Härdle",
            "Stefan Lessmann"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Appropriate risk management is crucial to ensure the competitiveness of\nfinancial institutions and the stability of the economy. One widely used\nfinancial risk measure is Value-at-Risk (VaR). VaR estimates based on linear\nand parametric models can lead to biased results or even underestimation of\nrisk due to time varying volatility, skewness and leptokurtosis of financial\nreturn series. The paper proposes a nonlinear and nonparametric framework to\nforecast VaR that is motivated by overcoming the disadvantages of parametric\nmodels with a purely data driven approach. Mean and volatility are modeled via\nsupport vector regression (SVR) where the volatility model is motivated by the\nstandard generalized autoregressive conditional heteroscedasticity (GARCH)\nformulation. Based on this, VaR is derived by applying kernel density\nestimation (KDE). This approach allows for flexible tail shapes of the profit\nand loss distribution, adapts for a wide class of tail events and is able to\ncapture complex structures regarding mean and volatility.\n  The SVR-GARCH-KDE hybrid is compared to standard, exponential and threshold\nGARCH models coupled with different error distributions. To examine the\nperformance in different markets, one-day-ahead and ten-days-ahead forecasts\nare produced for different financial indices. Model evaluation using a\nlikelihood ratio based test framework for interval forecasts and a test for\nsuperior predictive ability indicates that the SVR-GARCH-KDE hybrid performs\ncompetitive to benchmark models and reduces potential losses especially for\nten-days-ahead forecasts significantly. Especially models that are coupled with\na normal distribution are systematically outperformed.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06910v1"
    },
    {
        "title": "Model-driven statistical arbitrage on LETF option markets",
        "authors": [
            "Sergey Nasekin",
            "Wolfgang Karl Härdle"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper, we study the statistical properties of the moneyness scaling\ntransformation by Leung and Sircar (2015). This transformation adjusts the\nmoneyness coordinate of the implied volatility smile in an attempt to remove\nthe discrepancy between the IV smiles for levered and unlevered ETF options. We\nconstruct bootstrap uniform confidence bands which indicate that the implied\nvolatility smiles are statistically different after moneyness scaling has been\nperformed. An empirical application shows that there are trading opportunities\npossible on the LETF market. A statistical arbitrage type strategy based on a\ndynamic semiparametric factor model is presented. This strategy presents a\nstatistical decision algorithm which generates trade recommendations based on\ncomparison of model and observed LETF implied volatility surface. It is shown\nto generate positive returns with a high probability. Extensive econometric\nanalysis of LETF implied volatility process is performed including\nout-of-sample forecasting based on a semiparametric factor model and uniform\nconfidence bands' study. It provides new insights into the latent dynamics of\nthe implied volatility surface. We also incorporate Heston stochastic\nvolatility into the moneyness scaling method for better tractability of the\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09713v1"
    },
    {
        "title": "Regularization Approach for Network Modeling of German Power Derivative\n  Market",
        "authors": [
            "Shi Chen",
            "Wolfgang Karl Härdle",
            "Brenda López Cabrera"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper we propose a regularization approach for network modeling of\nGerman power derivative market. To deal with the large portfolio, we combine\nhigh-dimensional variable selection techniques with dynamic network analysis.\nThe estimated sparse interconnectedness of the full German power derivative\nmarket, clearly identify the significant channels of relevant potential risk\nspillovers. Our empirical findings show the importance of interdependence\nbetween different contract types, and identify the main risk contributors. We\nfurther observe strong pairwise interconnections between the neighboring\ncontracts especially for the spot contracts trading in the peak hours, its\nimplications for regulators and investors are also discussed. The network\nanalysis of the full German power derivative market helps us to complement a\nfull picture of system risk, and have a better understanding of the German\npower market functioning and environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09739v1"
    },
    {
        "title": "Implied Basket Correlation Dynamics",
        "authors": [
            "Wolfgang Karl Härdle",
            "Elena Silyakova"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Equity basket correlation can be estimated both using the physical measure\nfrom stock prices, and also using the risk neutral measure from option prices.\nThe difference between the two estimates motivates a so-called \"dispersion\nstrategy''. We study the performance of this strategy on the German market and\npropose several profitability improvement schemes based on implied correlation\n(IC) forecasts. Modelling IC conceals several challenges. Firstly the number of\ncorrelation coefficients would grow with the size of the basket. Secondly, IC\nis not constant over maturities and strikes. Finally, IC changes over time. We\nreduce the dimensionality of the problem by assuming equicorrelation. The IC\nsurface (ICS) is then approximated from the implied volatilities of stocks and\nthe implied volatility of the basket. To analyze the dynamics of the ICS we\nemploy a dynamic semiparametric factor model.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09770v1"
    },
    {
        "title": "CRIX an index for cryptocurrencies",
        "authors": [
            "Simon Trimborn",
            "Wolfgang Karl Härdle"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The cryptocurrency market is unique on many levels: Very volatile, frequently\nchanging market structure, emerging and vanishing of cryptocurrencies on a\ndaily level. Following its development became a difficult task with the success\nof cryptocurrencies (CCs) other than Bitcoin. For fiat currency markets, the\nIMF offers the index SDR and, prior to the EUR, the ECU existed, which was an\nindex representing the development of European currencies. Index providers\ndecide on a fixed number of index constituents which will represent the market\nsegment. It is a challenge to fix a number and develop rules for the\nconstituents in view of the market changes. In the frequently changing CC\nmarket, this challenge is even more severe. A method relying on the AIC is\nproposed to quickly react to market changes and therefore enable us to create\nan index, referred to as CRIX, for the cryptocurrency market. CRIX is chosen by\nmodel selection such that it represents the market well to enable each\ninterested party studying economic questions in this market and to invest into\nthe market. The diversified nature of the CC market makes the inclusion of\naltcoins in the index product critical to improve tracking performance. We have\nshown that assigning optimal weights to altcoins helps to reduce the tracking\nerrors of a CC portfolio, despite the fact that their market cap is much\nsmaller relative to Bitcoin. The codes used here are available via\nwww.quantlet.de.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09782v1"
    },
    {
        "title": "Complexity in economic and social systems: cryptocurrency market at\n  around COVID-19",
        "authors": [
            "Stanisław Drożdż",
            "Jarosław Kwapień",
            "Paweł Oświęcimka",
            "Tomasz Stanisz",
            "Marcin Wątorek"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Social systems are characterized by an enormous network of connections and\nfactors that can influence the structure and dynamics of these systems. All\nfinancial markets, including the cryptocurrency market, belong to the\neconomical sphere of human activity that seems to be the most interrelated and\ncomplex. The cryptocurrency market complexity can be studied from different\nperspectives. First, the dynamics of the cryptocurrency exchange rates to other\ncryptocurrencies and fiat currencies can be studied and quantified by means of\nmultifractal formalism. Second, coupling and decoupling of the cryptocurrencies\nand the conventional assets can be investigated with the advanced\ncross-correlation analyses based on fractal analysis. Third, an internal\nstructure of the cryptocurrency market can also be a subject of analysis that\nexploits, for example, a network representation of the market. We approach this\nsubject from all three perspectives based on data recorded between January 2019\nand June 2020. This period includes the Covid-19 pandemic and we pay particular\nattention to this event and investigate how strong its impact on the structure\nand dynamics of the market was. Besides, the studied data covers a few other\nsignificant events like double bull and bear phases in 2019. We show that,\nthroughout the considered interval, the exchange rate returns were multifractal\nwith intermittent signatures of bifractality that can be associated with the\nmost volatile periods of the market dynamics like a bull market onset in April\n2019 and the Covid-19 outburst in March 2020. The topology of a minimal\nspanning tree representation of the market also used to alter during these\nevents from a distributed type without any dominant node to a highly\ncentralized type with a dominating hub of USDT. However, the MST topology\nduring the pandemic differs in some details from other volatile periods.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.10030v1"
    },
    {
        "title": "Distillation of News Flow into Analysis of Stock Reactions",
        "authors": [
            "Junni L. Zhang",
            "Wolfgang Karl Härdle",
            "Cathy Y. Chen",
            "Elisabeth Bommes"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The gargantuan plethora of opinions, facts and tweets on financial business\noffers the opportunity to test and analyze the influence of such text sources\non future directions of stocks. It also creates though the necessity to distill\nvia statistical technology the informative elements of this prodigious and\nindeed colossal data source. Using mixed text sources from professional\nplatforms, blog fora and stock message boards we distill via different lexica\nsentiment variables. These are employed for an analysis of stock reactions:\nvolatility, volume and returns. An increased sentiment, especially for those\nwith negative prospection, will influence volatility as well as volume. This\ninfluence is contingent on the lexical projection and different across Global\nIndustry Classification Standard (GICS) sectors. Based on review articles on\n100 S&P 500 constituents for the period of October 20, 2009, to October 13,\n2014, we project into BL, MPQA, LM lexica and use the distilled sentiment\nvariables to forecast individual stock indicators in a panel context.\nExploiting different lexical projections to test different stock reaction\nindicators we aim at answering the following research questions: (i) Are the\nlexica consistent in their analytic ability? (ii) To which degree is there an\nasymmetric response given the sentiment scales (positive v.s. negative)? (iii)\nAre the news of high attention firms diffusing faster and result in more timely\nand efficient stock reaction? (iv) Is there a sector-specific reaction from the\ndistilled sentiment measures? We find there is significant incremental\ninformation in the distilled news flow and the sentiment effect is\ncharacterized as an asymmetric, attention-specific and sector-specific response\nof stock reactions.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.10392v1"
    },
    {
        "title": "Pricing Cryptocurrency Options",
        "authors": [
            "Ai Jun Hou",
            "Weining Wang",
            "Cathy Y. H. Chen",
            "Wolfgang Karl Härdle"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Cryptocurrencies, especially Bitcoin (BTC), which comprise a new digital\nasset class, have drawn extraordinary worldwide attention. The characteristics\nof the cryptocurrency/BTC include a high level of speculation, extreme\nvolatility and price discontinuity. We propose a pricing mechanism based on a\nstochastic volatility with a correlated jump (SVCJ) model and compare it to a\nflexible co-jump model by Bandi and Ren\\`o (2016). The estimation results of\nboth models confirm the impact of jumps and co-jumps on options obtained via\nsimulation and an analysis of the implied volatility curve. We show that a\nsizeable proportion of price jumps are significantly and contemporaneously\nanti-correlated with jumps in volatility. Our study comprises pioneering\nresearch on pricing BTC options. We show how the proposed pricing mechanism\nunderlines the importance of jumps in cryptocurrency markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.11007v1"
    },
    {
        "title": "A Decade of Evidence of Trend Following Investing in Cryptocurrencies",
        "authors": [
            "Evans Rozario",
            "Samuel Holt",
            "James West",
            "Shaun Ng"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Cryptocurrency markets have many of the characteristics of 20th century\ncommodities markets, making them an attractive candidate for trend following\nstrategies. We present a decade of evidence from the infancy of bitcoin,\nshowcasing the potential investor returns in cryptocurrency trend following,\n255% walkforward annualised returns. We find that cryptocurrencies offer\nsimilar returns characteristics to commodities with similar risk-adjusted\nreturns, and strong bear market diversification against traditional equities.\nCode available at https://github.com/Globe-Research/bittrends.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.12155v1"
    },
    {
        "title": "lCARE -- localizing Conditional AutoRegressive Expectiles",
        "authors": [
            "Xiu Xu",
            "Andrija Mihoci",
            "Wolfgang Karl Härdle"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We account for time-varying parameters in the conditional expectile-based\nvalue at risk (EVaR) model. The EVaR downside risk is more sensitive to the\nmagnitude of portfolio losses compared to the quantile-based value at risk\n(QVaR). Rather than fitting the expectile models over ad-hoc fixed data\nwindows, this study focuses on parameter instability of tail risk dynamics by\nutilising a local parametric approach. Our framework yields a data-driven\noptimal interval length at each time point by a sequential test. Empirical\nevidence at three stock markets from 2005-2016 shows that the selected lengths\naccount for approximately 3-6 months of daily observations. This method\nperforms favorable compared to the models with one-year fixed intervals, as\nwell as quantile based candidates while employing a time invariant portfolio\nprotection (TIPP) strategy for the DAX, FTSE 100 and S&P 500 portfolios. The\ntail risk measure implied by our model finally provides valuable insights for\nasset allocation and portfolio insurance.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13215v1"
    },
    {
        "title": "An analysis of network filtering methods to sovereign bond yields during\n  COVID-19",
        "authors": [
            "Raymond Ka-Kay Pang",
            "Oscar Granados",
            "Harsh Chhajer",
            "Erika Fille Legara"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this work, we investigate the impact of the COVID-19 pandemic on sovereign\nbond yields. We consider the temporal changes from financial correlations using\nnetwork filtering methods. These methods consider a subset of links within the\ncorrelation matrix, which gives rise to a network structure. We use sovereign\nbond yield data from 17 European countries between the 2010 and 2020 period. We\nfind the mean correlation to decrease across all filtering methods during the\nCOVID-19 period. We also observe a distinctive trend between filtering methods\nunder multiple network centrality measures. We then relate the significance of\neconomic and health variables towards filtered networks within the COVID-19\nperiod. Under an exponential random graph model, we are able to identify key\nrelations between economic groups across different filtering methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13390v2"
    },
    {
        "title": "Direct Evidence for Synchronization in Japanese Business Cycle",
        "authors": [
            "Yuichi Ikeda",
            "Hideaki Aoyama",
            "Hiroshi Iyetomi",
            "Hiroshi Yoshikawa"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We have analyzed the Indices of Industrial Production (Seasonal Adjustment\nIndex) for a long period of 240 months (January 1988 to December 2007) to\ndevelop a deeper understanding of the economic shocks. The angular frequencies\nestimated using the Hilbert transformation, are almost identical for the 16\nindustrial sectors. Moreover, the partial phase locking was observed for the 16\nsectors. These are the direct evidence of the synchronization in the Japanese\nbusiness cycle. We also showed that the information of the economic shock is\ncarried by the phase time-series. The common shock and individual shocks are\nseparated using phase time-series. The former dominates the economic shock in\nall of 1992, 1998 and 2001. The obtained results suggest that the business\ncycle may be described as a dynamics of the coupled limit-cycle oscillators\nexposed to the common shocks and random individual shocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.2263v1"
    },
    {
        "title": "An Exactly Solvable Discrete Stochastic Process with Correlated\n  Properties",
        "authors": [
            "Jongwook Kim",
            "Junghyo Jo"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We propose a correlated stochastic process of which the novel non-Gaussian\nprobability mass function is constructed by exactly solving moment generating\nfunction. The calculation of cumulants and auto-correlation shows that the\nprocess is convergent and scale invariant in the large but finite number limit.\nWe demonstrate that the model consistently explains both the distribution and\nthe correlation of discrete financial time-series data, and predicts the data\ndistribution with high precision in the small number regime.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.2655v1"
    },
    {
        "title": "A Model for Stock Returns and Volatility",
        "authors": [
            "Tao Ma",
            "R. A. Serota"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We prove that Student's t-distribution provides one of the better fits to\nreturns of S&P component stocks and the generalized inverse gamma distribution\nbest fits VIX and VXO volatility data. We further argue that a more accurate\nmeasure of the volatility may be possible based on the fact that stock returns\ncan be understood as the product distribution of the volatility and normal\ndistributions. We find Brown noise in VIX and VXO time series and explain the\nmean and the variance of the relaxation times on approach to the steady-state\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.4173v1"
    },
    {
        "title": "Joint multifractal analysis based on wavelet leaders",
        "authors": [
            "Zhi-Qiang Jiang",
            "Yan-Hong Yang",
            "Gang-Jin Wang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Mutually interacting components form complex systems and the outputs of these\ncomponents are usually long-range cross-correlated. Using wavelet leaders, we\npropose a method of characterizing the joint multifractal nature of these\nlong-range cross correlations, a method we call joint multifractal analysis\nbased on wavelet leaders (MF-X-WL). We test the validity of the MF-X-WL method\nby performing extensive numerical experiments on the dual binomial measures\nwith multifractal cross correlations and the bivariate fractional Brownian\nmotions (bFBMs) with monofractal cross correlations. Both experiments indicate\nthat MF-X-WL is capable to detect the cross correlations in synthetic data with\nacceptable estimating errors. We also apply the MF-X-WL method to the pairs of\nseries from financial markets (returns and volatilities) and online worlds\n(online numbers of different genders and different societies) and find an\nintriguing joint multifractal behavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.00897v1"
    },
    {
        "title": "Emerging interdependence between stock values during financial crashes",
        "authors": [
            "Jacopo Rocchi",
            "Enoch Yan Lok Tsui",
            "David Saad"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  To identify emerging interdependencies between traded stocks we investigate\nthe behavior of the stocks of FTSE 100 companies in the period 2000-2015, by\nlooking at daily stock values. Exploiting the power of information theoretical\nmeasures to extract direct influences between multiple time series, we compute\nthe information flow across stock values to identify several different regimes.\nWhile small information flows is detected in most of the period, a dramatically\ndifferent situation occurs in the proximity of global financial crises, where\nstock values exhibit strong and substantial interdependence for a prolonged\nperiod. This behavior is consistent with what one would generally expect from a\ncomplex system near criticality in physical systems, showing the long lasting\neffects of crashes on stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.02549v1"
    },
    {
        "title": "Application of the Generalized Linear Models in Actuarial Framework",
        "authors": [
            "Murwan H. M. A. Siddig"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  This paper aims to review the methodology behind the generalized linear\nmodels which are used in analyzing the actuarial situations instead of the\nordinary multiple linear regression. We introduce how to assess the adequacy of\nthe model which includes comparing nested models using the deviance and the\nscaled deviance. The Akiake information criterion is proposed as a\ncomprehensive tool for selecting the adequate model. We model a simple\nautomobile portfolio using the generalized linear models, and use the best\nchosen model to predict the number of claims made by the policyholders in the\nportfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.02556v1"
    },
    {
        "title": "Asynchronous ADRs: Overnight vs Intraday Returns and Trading Strategies",
        "authors": [
            "Tim Leung",
            "Jamie Kang"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  American Depositary Receipts (ADRs) are exchange-traded certificates that\nrep- resent shares of non-U.S. company securities. They are major financial\ninstruments for investing in foreign companies. Focusing on Asian ADRs in the\ncontext of asyn- chronous markets, we present methodologies and results of\nempirical analysis of their returns. In particular, we dissect their returns\ninto intraday and overnight com- ponents with respect to the U.S. market hours.\nThe return difference between the S&P500 index, traded through the SPDR S&P500\nETF (SPY), and each ADR is found to be a mean-reverting time series, and is\nfitted to an Ornstein-Uhlenbeck process via maximum-likelihood estimation\n(MLE). Our empirical observations also lead us to develop and backtest pairs\ntrading strategies to exploit the mean-reverting ADR-SPY spreads. We find\nconsistent positive payoffs when long position in ADR and short position in SPY\nare simultaneously executed at selected entry and exit levels.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03110v1"
    },
    {
        "title": "Time-varying return predictability in the Chinese stock market",
        "authors": [
            "Huai-Long Shi",
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  China's stock market is the largest emerging market all over the world. It is\nwidely accepted that the Chinese stock market is far from efficiency and it\npossesses possible linear and nonlinear dependence. We study the predictability\nof returns in the Chinese stock market by employing the wild bootstrap\nautomatic variance ratio test and the generalized spectral test. We find that\nthe return predictability vary over time and significant return predictability\nis observed around market turmoils. Our findings are consistent with the\nAdaptive Markets Hypothesis and have practical implications for market\nparticipants.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.04090v1"
    },
    {
        "title": "Empirical analysis of daily cash flow time series and its implications\n  for forecasting",
        "authors": [
            "Francisco Salas-Molina",
            "Juan A. Rodríguez-Aguilar",
            "Joan Serrà",
            "Montserrat Guillen",
            "Francisco J. Martin"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Cash managers make daily decisions based on predicted monetary inflows from\ndebtors and outflows to creditors. Usual assumptions on the statistical\nproperties of daily net cash flow include normality, absence of correlation and\nstationarity. We provide a comprehensive study based on a real-world cash flow\ndata set from small and medium companies, which is the most common type of\ncompanies in Europe. We also propose a new cross-validated test for time-series\nnon-linearity showing that: (i) the usual assumption of normality, absence of\ncorrelation and stationarity hardly appear; (ii) non-linearity is often\nrelevant for forecasting; and (iii) typical data transformations have little\nimpact on linearity and normality. Our results provide a forecasting strategy\nfor cash flow management which performs better than classical methods. This\nevidence may lead to consider a more data-driven approach such as time-series\nforecasting in an attempt to provide cash managers with expert systems in cash\nmanagement.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.04941v4"
    },
    {
        "title": "\"Chaos\" in energy and commodity markets: a controversial matter",
        "authors": [
            "Loretta Mastroeni",
            "Pierluigi Vellucci"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We test whether the futures prices of some commodity and energy markets are\ndetermined by stochastic rules or exhibit nonlinear deterministic endogenous\nfluctuations. As for the methodologies, we use the maximal Lyapunov exponents\n(MLE) and a determinism test, both based on the reconstruction of the phase\nspace. In particular, employing a recent methodology, we estimate a coefficient\n$\\kappa$ that describes the determinism rate of the analyzed time series. We\nfind that the underlying system for futures prices shows a reliability level\n$\\kappa$ near to $1$ while the MLE is positive for all commodity futures\nseries. Thus, the empirical evidence suggests that commodity and energy futures\nprices are the measured footprint of a nonlinear deterministic, rather than a\nstochastic, system.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.07432v2"
    },
    {
        "title": "Order statistics of horse racing and the randomly broken stick",
        "authors": [
            "Peter A. Bebbington",
            "Julius Bonart"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We find a remarkable agreement between the statistics of a randomly divided\ninterval and the observed statistical patterns and distributions found in horse\nracing betting markets. We compare the distribution of implied winning odds,\nthe average true winning probabilities, the implied odds conditional on a win,\nand the average implied odds of the winning horse with the corresponding\nquantities from the \"randomly broken stick problem\". We observe that the market\nis at least to some degree informationally efficient. From the mapping between\nexponential random variables and the statistics of the random division we\nconclude that horses' true winning abilities are exponentially distributed.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02567v1"
    },
    {
        "title": "Speculation and Power Law",
        "authors": [
            "Sabiou Inoua"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  It is now well established empirically that financial price changes are\ndistributed according to a power law, with cubic exponent. This is a\nfascinating regularity, as it holds for various classes of securities, on\nvarious markets, and on various time scales. The universality of this law\nsuggests that there must be some basic, general and stable mechanism behind it.\nThe standard (neoclassical) paradigm implies no such mechanism. Agent-based\nmodels of financial markets, on the other hand, exhibit realistic price\nchanges, but they involve relatively complicated, and often mathematically\nintractable, mechanisms. This paper identifies a simple principle behind the\npower law: the feedback intrinsic to the very idea of speculation, namely\nbuying when one expects a price rise (and selling when one expects a price\nfall). By this feedback, price changes follow a random coefficient\nautoregressive process, and therefore they have a power law by Kesten theorem.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.08705v1"
    },
    {
        "title": "Global economic dynamics of the forthcoming years. A forecast",
        "authors": [
            "Askar Akaev",
            "Andrey Korotayev"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The paper analyzes the current state of the world economy and offers a\nshort-term forecast of its development. Our analysis of log-periodic\noscillations in the DJIA dynamics suggests that in the second half of 2017 the\nUnited States and other more developed countries could experience a new\nrecession, due to the third phase of the global financial crisis. The economies\nof developing countries will continue their slowdown due to lower prices of raw\ncommodities and the increased pressure of dollar debt load. The bottom of the\nslowdown in global economic growth is likely to be achieved in 2017-2018. Then\nwe expect the start of a new acceleration of global economic growth at the\nupswing phase of the 6th Kondratieff cycle (2018-2050). A speedy and steady\nwithdrawal from the third phase of the global financial crisis requires\ncooperative action between developed and developing countries within G20 to\nstimulate global demand, world trade and a fair solution of the debt problem of\ndeveloping countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.09189v1"
    },
    {
        "title": "The Random Walk behind Volatility Clustering",
        "authors": [
            "Sabiou Inoua"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Financial price changes obey two universal properties: they follow a power\nlaw and they tend to be clustered in time. The second regularity, known as\nvolatility clustering, entails some predictability in the price changes: while\ntheir sign is uncorrelated in time, their amplitude (or volatility) is\nlong-range correlated. Many models have been proposed to account for these\nregularities, notably agent-based models; but these models often invoke\nrelatively complicated mechanisms. This paper identifies a basic reason behind\nvolatility clustering: the impact of exogenous news on expectations. Indeed the\nexpectations of financial agents clearly vary with the advent of news; the\nsimplest way of modeling this idea is to assume the expectations follow a\nrandom walk. We show that this random walk implies volatility clustering in a\ngeneric way.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.09344v1"
    },
    {
        "title": "Power-law cross-correlations: Issues, solutions and future challenges",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Analysis of long-range dependence in financial time series was one of the\ninitial steps of econophysics into the domain of mainstream finance and\nfinancial economics in the 1990s. Since then, many different financial series\nhave been analyzed using the methods standardly used outside of finance to\ndeliver some important stylized facts of the financial markets. In the late\n2000s, these methods have started being generalized to bivariate settings so\nthat the relationship between two series could be examined in more detail. It\nwas then only a single step from bivariate long-range dependence towards\nscale-specific correlations and regressions as well as power-law coherency as a\nunique relationship between power-law correlated series. Such rapid development\nin the field has brought some issues and challenges that need further\ndiscussion and attention. We shortly review the development and historical\nsteps from long-range dependence to bivariate generalizations and connected\nmethods, focus on its technical aspects and discuss problematic parts and\nchallenges for future directions in this specific subfield of econophysics.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01616v1"
    },
    {
        "title": "Multifractal characteristics and return predictability in the Chinese\n  stock markets",
        "authors": [
            "Xin-Lan Fu",
            "Xing-Lu Gao",
            "Zheng Shan",
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  By adopting Multifractal detrended fluctuation (MF-DFA) analysis methods, the\nmultifractal nature is revealed in the high-frequency data of two typical\nindexes, the Shanghai Stock Exchange Composite 180 Index (SH180) and the\nShenzhen Stock Exchange Composite Index (SZCI). The characteristics of the\ncorresponding multifractal spectra are defined as a measurement of market\nvolatility. It is found that there is a statistically significant relationship\nbetween the stock index returns and the spectral characteristics, which can be\napplied to forecast the future market return. The in-sample and out-of-sample\ntests on the return predictability of multifractal characteristics indicate the\nspectral width $\\Delta {\\alpha}$ is a significant and positive excess return\npredictor. Our results shed new lights on the application of multifractal\nnature in asset pricing.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07604v1"
    },
    {
        "title": "Critical slowing down associated with critical transition and risk of\n  collapse in cryptocurrency",
        "authors": [
            "Chengyi Tu",
            "Paolo DOdorico",
            "Samir Suweis"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  The year 2017 saw the rise and fall of the crypto-currency market, followed\nby high variability in the price of all crypto-currencies. In this work, we\nstudy the abrupt transition in crypto-currency residuals, which is associated\nwith the critical transition (the phenomenon of critical slowing down) or the\nstochastic transition phenomena. We find that, regardless of the specific\ncrypto-currency or rolling window size, the autocorrelation always fluctuates\naround a high value, while the standard deviation increases monotonically.\nTherefore, while the autocorrelation does not display signals of critical\nslowing down, the standard deviation can be used to anticipate critical or\nstochastic transitions. In particular, we have detected two sudden jumps in the\nstandard deviation, in the second quarter of 2017 and at the beginning of 2018,\nwhich could have served as early warning signals of two majors price collapses\nthat have happened in the following periods. We finally propose a mean-field\nphenomenological model for the price of crypto-currency to show how the use of\nthe standard deviation of the residuals is a better leading indicator of the\ncollapse in price than the time series' autocorrelation. Our findings represent\na first step towards a better diagnostic of the risk of critical transition in\nthe price and/or volume of crypto-currencies.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08386v2"
    },
    {
        "title": "What Makes An Asset Useful?",
        "authors": [
            "Yves-Laurent Kom Samo",
            "Dieter Hendricks"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Given a new candidate asset represented as a time series of returns, how\nshould a quantitative investment manager be thinking about assessing its\nusefulness? This is a key qualitative question inherent to the investment\nprocess which we aim to make precise. We argue that the usefulness of an asset\ncan only be determined relative to a reference universe of assets and/or\nbenchmarks the investment manager already has access to or would like to\ndiversify away from, for instance, standard risk factors, common trading styles\nand other assets. We identify four features that the time series of returns of\nan asset should exhibit for the asset to be useful to an investment manager,\ntwo primary and two secondary. As primary criteria, we propose that the new\nasset should provide sufficient incremental diversification to the reference\nuniverse of assets/benchmarks, and its returns time series should be\nsufficiently predictable. As secondary criteria, we propose that the new asset\nshould mitigate tail risk, and the new asset should be suitable for passive\ninvestment (e.g. buy-and-hold or short-and-hold). We discuss how to quantify\nincremental diversification, returns predictability, impact on tail risk, and\nsuitability for passive investment, and for each criterion, we provide a\nscalable algorithmic test of usefulness.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08444v1"
    },
    {
        "title": "Herding behavior in cryptocurrency markets",
        "authors": [
            "Obryan Poyser"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  There are no solid arguments to sustain that digital currencies are the\nfuture of online payments or the disruptive technology that some of its former\nparticipants declared when used to face critiques. This paper aims to solve the\ncryptocurrency puzzle from a behavioral finance perspective by finding the\nparallelism between biases present in financial markets that could be applied\nto cryptomarkets. Moreover, it is suggested that cryptocurrencies' prices are\ndriven by herding, hence this study test herding behavior under asymmetric and\nsymmetric conditions and the existence of different herding regimes by\nemploying the Markov-Switching approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.11348v2"
    },
    {
        "title": "How should you discount your backtest PnL?",
        "authors": [
            "Adam Rej",
            "Philip Seager",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In-sample overfitting is a drawback of any backtest-based investment\nstrategy. It is thus of paramount importance to have an understanding of why\nand how the in-sample overfitting occurs. In this article we propose a simple\nframework that allows one to model and quantify in-sample PnL overfitting. This\nallows us to compute the factor appropriate for discounting PnLs of in-sample\ninvestment strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01802v1"
    },
    {
        "title": "Development of an agent-based speculation game for higher\n  reproducibility of financial stylized facts",
        "authors": [
            "Kei Katahira",
            "Yu Chen",
            "Gaku Hashimoto",
            "Hiroshi Okuda"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Simultaneous reproduction of all financial stylized facts is so difficult\nthat most existing stochastic process-based and agent-based models are unable\nto achieve the goal. In this study, by extending the decision-making structure\nof Minority Game, we propose a novel agent-based model called \"Speculation\nGame,\" for a better reproducibility of the stylized facts. The new model has\nthree distinct characteristics comparing with preceding agent-based adaptive\nmodels for the financial market: the enabling of nonuniform holding and idling\nperiods, the inclusion of magnitude information of price change in history, and\nthe implementation of a cognitive world for the evaluation of investment\nstrategies with capital gains and losses. With these features, Speculation Game\nsucceeds in reproducing 10 out of the currently well studied 11 stylized facts\nunder a single parameter setting.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.02040v1"
    },
    {
        "title": "Phase transition in the Bayesian estimation of the default portfolio",
        "authors": [
            "Masato Hisakado",
            "Shintaro Mori"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The probability of default (PD) estimation is an important process for\nfinancial institutions. The difficulty of the estimation depends on the\ncorrelations between borrowers. In this paper, we introduce a hierarchical\nBayesian estimation method using the beta binomial distribution and consider a\nmulti-year case with a temporal correlation. A phase transition occurs when the\ntemporal correlation decays by power decay. When the power index is less than\none, the PD estimator does not converge. It is difficult to estimate the PD\nwith limited historical data. Conversely, when the power index is greater than\none, the convergence is the same as that of the binomial distribution. We\nprovide a condition for the estimation of the PD and discuss the universality\nclass of the phase transition. We investigate the empirical default data\nhistory of rating agencies and their Fourier transformations to confirm the\nform of the correlation decay. The power spectrum of the decay history seems to\nbe 1/f, which corresponds to a long memory. But the estimated power index is\nmuch greater than one. If we collect adequate historical data,the parameters\ncan be estimated correctly.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.03797v2"
    },
    {
        "title": "Unified Bayesian Conditional Autoregressive Risk Measures using the Skew\n  Exponential Power Distribution",
        "authors": [
            "Marco Bottone",
            "Mauro Bernardi",
            "Lea Petrella"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Conditional Autoregressive Value-at-Risk and Conditional Autoregressive\nExpectile have become two popular approaches for direct measurement of market\nrisk. Since their introduction several improvements both in the Bayesian and in\nthe classical framework have been proposed to better account for asymmetry and\nlocal non-linearity. Here we propose a unified Bayesian Conditional\nAutoregressive Risk Measures approach by using the Skew Exponential Power\ndistribution. Further, we extend the proposed models using a semiparametric\nP-spline approximation answering for a flexible way to consider the presence of\nnon-linearity. To make the statistical inference we adapt the MCMC algorithm\nproposed in Bernardi et al. (2018) to our case. The effectiveness of the whole\napproach is demonstrated using real data on daily return of five stock market\nindices.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.03982v2"
    },
    {
        "title": "Direct determination approach for the multifractal detrending moving\n  average analysis",
        "authors": [
            "Hai-Chuan Xu",
            "Gao-Feng Gu",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In the canonical framework, we propose an alternative approach for the\nmultifractal analysis based on the detrending moving average method (MF-DMA).\nWe define a canonical measure such that the multifractal mass exponent\n$\\tau(q)$ is related to the partition function and the multifractal spectrum\n$f(\\alpha)$ can be directly determined. The performances of the direct\ndetermination approach and the traditional approach of the MF-DMA are compared\nbased on three synthetic multifractal and monofractal measures generated from\nthe one-dimensional $p$-model, the two-dimensional $p$-model and the fractional\nBrownian motions. We find that both approaches have comparable performances to\nunveil the fractal and multifractal nature. In other words, without loss of\naccuracy, the multifractal spectrum $f(\\alpha)$ can be directly determined\nusing the new approach with less computation cost. We also apply the new MF-DMA\napproach to the volatility time series of stock prices and confirm the presence\nof multifractality.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.04437v1"
    },
    {
        "title": "Correlation Patterns in Foreign Exchange Markets",
        "authors": [
            "Lasko Basnarkov",
            "Viktor Stojkoski",
            "Zoran Utkovski",
            "Ljupco Kocarev"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The value of an asset in a financial market is given in terms of another\nasset known as numeraire. The dynamics of the value is non-stationary and\nhence, to quantify the relationships between different assets, one requires\nconvenient measures such as the means and covariances of the respective log\nreturns. Here, we develop transformation equations for these means and\ncovariances when one changes the numeraire. The results are verified by a\nthorough empirical analysis capturing the dynamics of numerous assets in a\nforeign exchange market. We show that the partial correlations between pairs of\nassets are invariant under the change of the numeraire. This observable\nquantifies the relationship between two assets, while the influence of the rest\nis removed. As such the partial correlations uncover intriguing observations\nwhich may not be easily noticed in the ordinary correlation analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.06483v2"
    },
    {
        "title": "Q-Gaussian diffusion in stock markets",
        "authors": [
            "Alonso-Marroquin Fernando",
            "Arias-Calluari Karina",
            "Harre Michael",
            "Najafi Morteza N.",
            "Herrmann Hans J"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We analyze the Standard & Poor's 500 stock market index from the last 22\nyears. The probability density function of price returns exhibits two\nwell-distinguished regimes with self-similar structure: the first one displays\nstrong super-diffusion together with short-time correlations, and the second\none corresponds to weak super-diffusion with weak time correlations. Both\nregimes are well-described by q-Gaussian distributions. The porous media\nequation is used to derive the governing equation for these regimes, and the\nBlack-Scholes diffusion coefficient is explicitly obtained from the governing\nequation.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.10500v1"
    },
    {
        "title": "Determining the number of factors in a forecast model by a random matrix\n  test: cryptocurrencies",
        "authors": [
            "Andrés García Medina",
            "Graciela González-Farías"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We determine the number of statistically significant factors in a forecast\nmodel using a random matrices test. The applied forecast model is of the type\nof Reduced Rank Regression (RRR), in particular, we chose a flavor which can be\nseen as the Canonical Correlation Analysis (CCA). As empirical data, we use\ncryptocurrencies at hour frequency, where the variable selection was made by a\ncriterion from information theory. The results are consistent with the usual\nvisual inspection, with the advantage that the subjective element is avoided.\nFurthermore, the computational cost is minimal compared to the cross-validation\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.00545v1"
    },
    {
        "title": "Relevant Stylized Facts About Bitcoin: Fluctuations, First Return\n  Probability, and Natural Phenomena",
        "authors": [
            "C. R. da Cunha",
            "R. da Silva"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Bitcoin is a digital financial asset that is devoid of a central authority.\nThis makes it distinct from traditional financial assets in a number of ways.\nFor instance, the total number of tokens is limited and it has not explicit use\nvalue. Nonetheless, little is know whether it obeys the same stylized facts\nfound in traditional financial assets. Here we test bitcoin for a set of these\nstylized facts and conclude that it behaves statistically as most of other\nassets. For instance, it exhibits aggregational Gaussianity and fluctuation\nscaling. Moreover, we show by an analogy with natural occurring quakes that\nbitcoin obeys both the Omori and Gutenberg-Richter laws. Finally, we show that\nthe global persistence, originally defined for spin systems, presents a power\nlaw behavior with exponent similar to that found in stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03211v1"
    },
    {
        "title": "A New Stock Market Valuation Measure with Applications to Retirement\n  Planning",
        "authors": [
            "Andrey Sarantsev"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We generalize the classic Shiller cyclically adjusted price-earnings ratio\n(CAPE) used for prediction of future total returns of the stock market. We\ntreat earnings growth as exogenous. The difference between log wealth and log\nearnings is modeled as an autoregression of order 1 with linear trend 4.5\\% and\nGaussian innovations. Detrending gives us a new valuation measure. This\nautoregression is significantly different from the random walk. Therefore, our\nresults disprove the Efficient Market Hypothesis. Therefore, long-run total\nreturns equal long-run earnings growth plus 4.5\\%. We apply results to\nretirement planning. A withdrawal process governs how a retired capital owner\nwithdraws a certain fraction of wealth annually. The fraction can vary from\nyear to year. We study the long-term behavior of such processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.04603v14"
    },
    {
        "title": "Predicting and Forecasting the Price of Constituents and Index of\n  Cryptocurrency Using Machine Learning",
        "authors": [
            "Reaz Chowdhury",
            "M. Arifur Rahman",
            "M. Sohel Rahman",
            "M. R. C. Mahdy"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  At present, cryptocurrencies have become a global phenomenon in financial\nsectors as it is one of the most traded financial instruments worldwide.\nCryptocurrency is not only one of the most complicated and abstruse fields\namong financial instruments, but it is also deemed as a perplexing problem in\nfinance due to its high volatility. This paper makes an attempt to apply\nmachine learning techniques on the index and constituents of cryptocurrency\nwith a goal to predict and forecast prices thereof. In particular, the purpose\nof this paper is to predict and forecast the close (closing) price of the\ncryptocurrency index 30 and nine constituents of cryptocurrencies using machine\nlearning algorithms and models so that, it becomes easier for people to trade\nthese currencies. We have used several machine learning techniques and\nalgorithms and compared the models with each other to get the best output. We\nbelieve that our work will help reduce the challenges and difficulties faced by\npeople, who invest in cryptocurrencies. Moreover, the obtained results can play\na major role in cryptocurrency portfolio management and in observing the\nfluctuations in the prices of constituents of cryptocurrency market. We have\nalso compared our approach with similar state of the art works from the\nliterature, where machine learning approaches are considered for predicting and\nforecasting the prices of these currencies. In the sequel, we have found that\nour best approach presents better and competitive results than the best works\nfrom the literature thereby advancing the state of the art. Using such\nprediction and forecasting methods, people can easily understand the trend and\nit would be even easier for them to trade in a difficult and challenging\nfinancial instrument like cryptocurrency.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08444v1"
    },
    {
        "title": "BitMEX Funding Correlation with Bitcoin Exchange Rate",
        "authors": [
            "Sai Srikar Nimmagadda",
            "Pawan Sasanka Ammanamanchi"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This paper examines the relationship between Inverse Perpetual Swap\ncontracts, a Bitcoin derivative akin to futures and the margin funding interest\nrates levied on BitMEX. This paper proves the Heteroskedastic nature of funding\nrates and goes onto establish a causal relationship between the funding rates\nand the Bitcoin inverse Perpetual swap contracts based on Granger causality.\nThe paper further dwells into developing a predictive model for funding rates\nusing best-fitted GARCH models. Implications of the results are presented, and\nfunding rates as a predictive tool for gauging the market trend is discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.03270v1"
    },
    {
        "title": "Evolutionary Dynamics of Investors Expectations and Market Price\n  Movement",
        "authors": [
            "Inga Ivanova"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The paper presents a step forward into the development of the theory of\nmeaning. Stock and financial markets are examined from\ncommunication-theoretical perspective on the dynamics of information and\nmeaning. This study focuses on the link between the dynamics of investors'\nexpectations and market price movement. The model for market asset price\ndynamiscs, based on non-linear evolutionary equation linking investors'\nexpectations and market asset price movement, is provided. Model predictions\nare tested on various FX, energy, food, and indices markets along different\ntime frames. The results suggest that model predicted time series is\nco-integrated with asset time series which implies that the prop[osed model can\nbe used to forecast future price movement.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11216v6"
    },
    {
        "title": "The Generalisation of the DMCA Coefficient to Serve Distinguishing\n  Between Hedge and Safe Haven Capabilities of the Gold",
        "authors": [
            "Mohamed Arbi Madani",
            "Zied Ftiti"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This paper aims to investigate the role of gold as a hedge and/or safe haven\nagainst oil price and currency market movements for medium (calm period) and\nlarge (extreme movement) fluctuations. In revisiting the role of gold, our\nstudy proposes new insights into the literature. First, our empirical design\nrelaxes the assumption of homogeneous investors in favour of agents with\ndifferent horizons. Second, we develop a new measure of correlation based on\nthe fractal approach, called the q-detrending moving average cross-correlation\ncoefficient. This allows us to measure the dependence for calm and extreme\nmovements. The proposed measure is both time-varying and time-scale varying,\ntaking into account the complex pattern of commodities and financial time\nseries (chaotic, non-stationary, etc.). Using intraday data from May 2017 to\nMarch 2019, including 35608 observations for each variable, our results are as\nfollows. First, we show a negative and significant average and tail dependence\nfor all time scales between gold and USD exchange rates that is consistent with\nthe gold's role as an effective hedge and safe-haven asset. Second, this study\nputs out average independence and positive and significant tail independence\nbetween gold and oil indicating that gold can be used by investors as a weak\nhedge but cannot be used as an effective safe-haven asset under exceptional\nmarket circumstances for all time scales. Third, we examine the hedging and\nstabilising benefits of gold over calm and turmoil periods for gold-oil futures\nand gold-currency portfolios by estimation of the optimal portfolio weights and\nthe optimal hedge ratio. We confirm the usefulness of gold for hedging and safe\nhavens at different investment horizons, which favors the inclusion of gold\nfutures in oil futures and currency portfolios for risk management purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12590v1"
    },
    {
        "title": "Disentangling shock diffusion on complex networks: Identification\n  through graph planarity",
        "authors": [
            "Sudarshan Kumar",
            "Tiziana Di Matteo",
            "Anindya S. Chakrabarti"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Large scale networks delineating collective dynamics often exhibit cascading\nfailures across nodes leading to a system-wide collapse. Prominent examples of\nsuch phenomena would include collapse on financial and economic networks.\nIntertwined nature of the dynamics of nodes in such network makes it difficult\nto disentangle the source and destination of a shock that percolates through\nthe network, a property known as reflexivity. In this article, a novel\nmethodology is proposed which combines vector autoregression model with an\nunique identification restrictions obtained from the topological structure of\nthe network to uniquely characterize cascades. In particular, we show that\nplanarity of the network allows us to statistically estimate a dynamical\nprocess consistent with the observed network and thereby uniquely identify a\npath for shock propagation from any chosen epicenter to all other nodes in the\nnetwork. We analyze the distress propagation mechanism in closed loops giving\nrise to a detailed picture of the effect of feedback loops in transmitting\nshocks. We show usefulness and applications of the algorithm in two networks\nwith dynamics at different time-scales: worldwide GDP growth network and stock\nnetwork. In both cases, we observe that the model predicts the impact of the\nshocks emanating from the US would be concentrated within the cluster of\ndeveloped countries and the developing countries show very muted response,\nwhich is consistent with empirical observations over the past decade.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.01518v1"
    },
    {
        "title": "Uncovering the hierarchical structure of the international FOREX market\n  by using similarity metric between the fluctuation distributions of\n  currencies",
        "authors": [
            "Abhijit Chakraborty",
            "Soumya Easwaran",
            "Sitabhra Sinha"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The decentralized international market of currency trading is a prototypical\ncomplex system having a highly heterogeneous composition. To understand the\nhierarchical structure relating the price movement of different currencies in\nthe market, we have focused on quantifying the degree of similarity between the\ndistributions of exchange rate fluctuations. For this purpose we use a metric\nconstructed using the Jensen-Shannon divergence between the normalized\nlogarithmic return distributions of the different currencies. This provides a\nnovel method for revealing associations between currencies in terms of the\nstatistical nature of their rate fluctuations, which is distinct from the\nconventional correlation-based methods. The resulting clusters are consistent\nwith the nature of the underlying economies but also show striking divergences\nduring periods of major international crises.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.02482v1"
    },
    {
        "title": "Non-Extensive Value-at-Risk Estimation During Times of Crisis",
        "authors": [
            "Ahmad Hajihasani",
            "Ali Namaki",
            "Nazanin Asadi",
            "Reza Tehrani"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Value-at-risk is one of the important subjects that extensively used by\nresearchers and practitioners for measuring and managing uncertainty in\nfinancial markets. Although value-at-risk is a common risk control instrument,\nbut there are criticisms about its performance. One of these cases, which has\nbeen studied in this research, is the value-at-risk underestimation during\ntimes of crisis. In these periods, the non-Gaussian behavior of markets\nintensifies and the estimated value-at-risks by normal models are lower than\nthe real values. In fact, during times of crisis, the probability density of\nextreme values in financial return series increases and this heavy-tailed\nbehavior of return series reduces the accuracy of the normal value-at-risk\nestimation models. A potential approach that can be used to describe\nnon-Gaussian behavior of return series, is Tsallis entropy framework and\nnon-extensive statistical methods. In this paper, we have used non-extensive\nvalue at risk model for analyzing the behavior of financial markets during\ntimes of crisis. By applying q-Gaussian probability density function, we can\nsee a better value-at-risk estimation in comparison with the normal models,\nespecially during times of crisis. We showed that q-Gaussian model estimates\nvalue-at-risk better than normal model. Also we saw in the mature markets, it\nis obvious that the difference of value-at-risk between normal condition and\nnon-extensive approach increase more than one standard deviation during times\nof crisis, but in the emerging markets we cannot see a specific pattern.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09036v2"
    },
    {
        "title": "Dynamical Hurst exponent as a tool to monitor unstable periods in\n  financial time series",
        "authors": [
            "Raffaello Morales",
            "T. Di Matteo",
            "Ruggero Gramatica",
            "Tomaso Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We investigate the use of the Hurst exponent, dynamically computed over a\nmoving time-window, to evaluate the level of stability/instability of financial\nfirms. Financial firms bailed-out as a consequence of the 2007-2010 credit\ncrisis show a neat increase with time of the generalized Hurst exponent in the\nperiod preceding the unfolding of the crisis. Conversely, firms belonging to\nother market sectors, which suffered the least throughout the crisis, show\nopposite behaviors. These findings suggest the possibility of using the scaling\nbehavior as a tool to track the level of stability of a firm. In this paper, we\nintroduce a method to compute the generalized Hurst exponent which assigns\nlarger weights to more recent events with respect to older ones. In this way\nlarge fluctuations in the remote past are less likely to influence the recent\npast. We also investigate the scaling associated with the tails of the\nlog-returns distributions and compare this scaling with the scaling associated\nwith the Hurst exponent, observing that the processes underlying the price\ndynamics of these firms are truly multi-scaling.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0465v1"
    },
    {
        "title": "Pruning a Minimum Spanning Tree",
        "authors": [
            "Leonidas Sandoval Junior"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  This work employs some techniques in order to filter random noise from the\ninformation provided by minimum spanning trees obtained from the correlation\nmatrices of international stock market indices prior to and during times of\ncrisis. The first technique establishes a threshold above which connections are\nconsidered affected by noise, based on the study of random networks with the\nsame probability density distribution of the original data. The second\ntechnique is to judge the strengh of a connection by its survival rate, which\nis the amount of time a connection between two stock market indices endure. The\nidea is that true connections will survive for longer periods of time, and that\nrandom connections will not. That information is then combined with the\ninformation obtained from the first technique in order to create a smaller\nnetwork, where most of the connections are either strong or enduring in time.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0642v1"
    },
    {
        "title": "Analysis of the trends in the index of the Dow Jones Industrial Average\n  (DJIA) of the New York Stock Exchange (NYSE)",
        "authors": [
            "Caglar Tuncay"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  It is hypothesized that price charts can be empirically decomposed into two\ncomponents as random and non random. The non random component, which can be\ntreated as approximately regular behavior of the prices (trend) in an epoch, is\na geometric line. Thus, the random component fluctuates around the non random\ncomponent with various amplitudes. Moreover, the shape of a trend in an epoch\nmay be different in another epoch. It is further hypothesized that statistical\nevidence can be found for various relations between several types of trends and\nthe direction of the next movements of the prices. These hypotheses are tested\non the historical data of the DJIA (Dow) and confirmed. Moreover, it is\nstatistically showed that a number of trends that have occurred in the near\npast course of the Dow can be utilized to presage the near future of the index.\nAs a result, upcoming of a recession in the DJIA, which may portend a worldwide\neconomic crisis, is predicted.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4372v1"
    },
    {
        "title": "Super-exponential endogenous bubbles in an equilibrium model of rational\n  and noise traders",
        "authors": [
            "T. Kaizoji",
            "M. Leiss",
            "A. Saichev",
            "D. Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We introduce a model of super-exponential financial bubbles with two assets\n(risky and risk-free), in which rational investors and noise traders co-exist.\nRational investors form expectations on the return and risk of a risky asset\nand maximize their constant relative risk aversion expected utility with\nrespect to their allocation on the risky asset versus the risk-free asset.\nNoise traders are subjected to social imitation and follow momentum trading.\nAllowing for random time-varying herding propensity, we are able to reproduce\nseveral well-known stylized facts of financial markets such as a fat-tail\ndistribution of returns and volatility clustering. In particular, we observe\ntransient faster-than-exponential bubble growth with approximate log-periodic\nbehavior and give analytical arguments why this follows from our framework. The\nmodel accounts well for the behavior of traders and for the price dynamics that\ndeveloped during the dotcom bubble in 1995-2000. Momentum strategies are shown\nto be transiently profitable, supporting these strategies as enhancing herding\nbehavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4726v3"
    },
    {
        "title": "Time-Frequency Dynamics of Biofuels-Fuels-Food System",
        "authors": [
            "Lukas Vacha",
            "Karel Janda",
            "Ladislav Kristoufek",
            "David Zilberman"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  For the first time, we apply the wavelet coherence methodology on biofuels\n(ethanol and biodiesel) and a wide range of related commodities (gasoline,\ndiesel, crude oil, corn, wheat, soybeans, sugarcane and rapeseed oil). This\nway, we are able to investigate dynamics of correlations in time and across\nscales (frequencies) with a model-free approach. We show that correlations\nindeed vary in time and across frequencies. We find two highly correlated pairs\nwhich are strongly connected at low frequencies - ethanol with corn and\nbiodiesel with German diesel - during almost the whole analyzed period\n(2003-2011). Structure of correlations remarkably changes during the food\ncrisis - higher frequencies become important for both mentioned pairs. This\nimplies that during stable periods, ethanol is correlated with corn and\nbiodiesel is correlated with German diesel mainly at low frequencies so that\nthey follow a common long-term trend. However, in the crisis periods, ethanol\n(biodiesel) is lead by corn (German diesel) even at high frequencies (low\nscales), which implies that the biofuels prices react more rapidly to the\nchanges in their producing factors.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.0900v1"
    },
    {
        "title": "Hierarchical structure of stock price fluctuations in financial markets",
        "authors": [
            "Ya-Chun Gao",
            "Shi-Min Cai",
            "Bing-Hong Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The financial market and turbulence have been broadly compared on account of\nthe same quantitative methods and several common stylized facts they shared. In\nthis paper, the She-Leveque (SL) hierarchy, proposed to explain the anomalous\nscaling exponents deviated from Kolmogorov monofractal scaling of the velocity\nfluctuation in fluid turbulence, is applied to study and quantify the\nhierarchical structure of stock price fluctuations in financial markets. We\ntherefore observed certain interesting results: (i) The hierarchical structure\nrelated to multifractal scaling generally presents in all the stock price\nfluctuations we investigated. (ii) The quantitatively statistical parameters\nthat describes SL hierarchy are different between developed financial markets\nand emerging ones, distinctively. (iii) For the high-frequency stock price\nfluctuation, the hierarchical structure varies with different time period. All\nthese results provide a novelty analogy in turbulence and financial market\ndynamics and a insight to deeply understand the multifractality in financial\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.4175v1"
    },
    {
        "title": "With string model to time series forecasting",
        "authors": [
            "Richard Pinčák",
            "Erik Bartoš"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Overwhelming majority of econometric models applied on a long term basis in\nthe financial forex market do not work sufficiently well. The reason is that\ntransaction costs and arbitrage opportunity are not included, as this does not\nsimulate the real financial markets. Analyses are not conducted on the non\nequidistant date but rather on the aggregate date, which is also not a real\nfinancial case. In this paper, we would like to show a new way how to analyze\nand, moreover, forecast financial market. We utilize the projections of the\nreal exchange rate dynamics onto the string-like topology in the OANDA market.\nThe latter approach allows us to build the stable prediction models in trading\nin the financial forex market. The real application of the multi-string\nstructures is provided to demonstrate our ideas for the solution of the problem\nof the robust portfolio selection. The comparison with the trend following\nstrategies was performed, the stability of the algorithm on the transaction\ncosts for long trade periods was confirmed.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.00483v1"
    },
    {
        "title": "Risk-return relationship: An empirical study of different statistical\n  methods for estimating the Capital Asset Pricing Models (CAPM) and the\n  Fama-French model for large cap stocks",
        "authors": [
            "Linh Nghiem"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The Capital Asset Pricing Model (CAPM) is one of the original models in\nexplaining risk-return relationship in the financial market. However, when\napplying the CAPM into reality, it demonstrates a lot of shortcomings. While\nimproving the performance of the model, many studies, on one hand, have\nattempted to apply different statistical methods to estimate the model, on the\nother hand, have added more predictors to the model. First, the thesis focuses\non reviewing the CAPM and comparing popular statistical methods used to\nestimate it, and then, the thesis compares predictive power of the CAPM and the\nFama-French model, which is an important extension of the CAPM. Through an\nempirical study on the data set of large cap stocks, we have demonstrated that\nthere is no statistical method that would recover the expected relationship\nbetween systematic risk (represented by beta) and return from the CAPM, and\nthat the Fama-French model does not have a better predictive performance than\nthe CAPM on individual stocks. Therefore, the thesis provides more evidence to\nsupport the incorrectness of the CAPM and the limitation of the Fama-French\nmodel in explaining risk-return relationship.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.07101v1"
    },
    {
        "title": "Large large-trader activity weakens the long memory of limit order\n  markets",
        "authors": [
            "Kevin Primicerio",
            "Damien Challet"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Using more than 6.7 billions of trades, we explore how the tick-by-tick\ndynamics of limit order books depends on the aggregate actions of large\ninvestment funds on a much larger (quarterly) timescale. In particular, we find\nthat the well-established long memory of market order signs is markedly weaker\nwhen large investment funds trade either in a directional way and even weaker\nwhen their aggregate participation ratio is large. Conversely, we investigate\nto what respect a weaker memory of market order signs predicts that an asset is\nbeing actively traded by large funds. Theoretical arguments suggest two simple\nmechanisms that contribute to the observed effect: a larger number of active\nmeta-orders and a modification of the distribution of size of meta-orders.\nEmpirical evidence suggests that the number of active meta-orders is the most\nimportant contributor to the loss of market order sign memory.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08390v1"
    },
    {
        "title": "Scaling properties of extreme price fluctuations in Bitcoin markets",
        "authors": [
            "Stjepan Begušić",
            "Zvonko Kostanjčar",
            "H. Eugene Stanley",
            "Boris Podobnik"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Detection of power-law behavior and studies of scaling exponents uncover the\ncharacteristics of complexity in many real world phenomena. The complexity of\nfinancial markets has always presented challenging issues and provided\ninteresting findings, such as the inverse cubic law in the tails of stock price\nfluctuation distributions. Motivated by the rise of novel digital assets based\non blockchain technology, we study the distributions of cryptocurrency price\nfluctuations. We consider Bitcoin returns over various time intervals and from\nmultiple digital exchanges, in order to investigate the existence of universal\nscaling behavior in the tails, and ascertain whether the scaling exponent\nsupports the presence of a finite second moment. We provide empirical evidence\non slowly decaying tails in the distributions of returns over multiple time\nintervals and different exchanges, corresponding to a power-law. We estimate\nthe scaling exponent and find an asymptotic power-law behavior with 2 <\n{\\alpha} < 2.5 suggesting that Bitcoin returns, in addition to being more\nvolatile, also exhibit heavier tails than stocks, which are known to be around\n3. Our results also imply the existence of a finite second moment, thus\nproviding a fundamental basis for the usage of standard financial theories and\ncovariance-based techniques in risk management and portfolio optimization\nscenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08405v1"
    },
    {
        "title": "Time-dependent lead-lag relationship between the onshore and offshore\n  Renminbi exchange rates",
        "authors": [
            "Hai-Chuan Xu",
            "Wei-Xing Zhou",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We employ the thermal optimal path method to explore both the long-term and\nshort-term interaction patterns between the onshore CNY and offshore CNH\nexchange rates (2012-2015). For the daily data, the CNY and CNH exchange rates\nshow a weak alternate lead-lag structure in most of the time periods. When CNY\nand CNH display a large disparity, the lead-lag relationship is uncertain and\ndepends on the prevailing market factors. The minute-scale interaction pattern\nbetween the CNY and CNH exchange rates change over time according to different\nmarket situations. We find that US dollar appreciation is associated with a\nlead-lag relationship running from offshore to onshore, while a (contrarian)\nRenminbi appreciation is associated with a lead-lag relationship running from\nonshore to offshore. These results are robust with respect to different\nsub-sample analyses and variations of the key smoothing parameter of the TOP\nmethod.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09432v1"
    },
    {
        "title": "Strategic behaviour and indicative price diffusion in Paris Stock\n  Exchange auctions",
        "authors": [
            "Damien Challet"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We report statistical regularities of the opening and closing auctions of\nFrench equities, focusing on the diffusive properties of the indicative auction\nprice. Two mechanisms are at play as the auction end time nears: the typical\nprice change magnitude decreases, favoring underdiffusion, while the rate of\nthese events increases, potentially leading to overdiffusion. A third\nmechanism, caused by the strategic behavior of traders, is needed to produce\nnearly diffusive prices: waiting to submit buy orders until sell orders have\ndecreased the indicative price and vice-versa.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00573v1"
    },
    {
        "title": "Entropy Analysis of Financial Time Series",
        "authors": [
            "Stephan Schwill"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  This thesis applies entropy as a model independent measure to address three\nresearch questions concerning financial time series. In the first study we\napply transfer entropy to drawdowns and drawups in foreign exchange rates, to\nstudy their correlation and cross correlation. When applied to daily and hourly\nEUR/USD and GBP/USD exchange rates, we find evidence of dependence among the\nlargest draws (i.e. 5% and 95% quantiles), but not as strong as the correlation\nbetween the daily returns of the same pair of FX rates. In the second study we\nuse state space models (Hidden Markov Models) of volatility to investigate\nvolatility spill overs between exchange rates. Among the currency pairs, the\nco-movement of EUR/USD and CHF/USD volatility states show the strongest\nobserved relationship. With the use of transfer entropy, we find evidence for\ninformation flows between the volatility state series of AUD, CAD and BRL. The\nthird study uses the entropy of S&P realised volatility in detecting changes of\nvolatility regime in order to re-examine the theme of market volatility timing\nof hedge funds. A one-factor model is used, conditioned on information about\nthe entropy of market volatility, to measure the dynamic of hedge funds equity\nexposure. On a cross section of around 2500 hedge funds with a focus on the US\nequity markets we find that, over the period from 2000 to 2014, hedge funds\nadjust their exposure dynamically in response to changes in volatility regime.\nThis adds to the literature on the volatility timing behaviour of hedge fund\nmanager, but using entropy as a model independent measure of volatility regime.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.09423v1"
    },
    {
        "title": "Co-existence of Trend and Value in Financial Markets: Estimating an\n  Extended Chiarella Model",
        "authors": [
            "Adam Majewski",
            "Stefano Ciliberti",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Trend and Value are pervasive anomalies, common to all financial markets. We\naddress the problem of their co-existence and interaction within the framework\nof Heterogeneous Agent Based Models (HABM). More specifically, we extend the\nChiarella (1992) model by adding noise traders and a non-linear demand of\nfundamentalists. We use Bayesian filtering techniques to calibrate the model on\ntime series of prices across a variety of asset classes since 1800. The\nfundamental value is an output of the calibration, and does not require the use\nof an external pricing model. Our extended model reproduces many empirical\nobservations, including the non-monotonic relation between past trends and\nfuture returns. The destabilizing activity of trend-followers leads to a\nqualitative change of mispricing distribution, from unimodal to bimodal,\nmeaning that some markets tend to be over- (or under-) valued for long periods\nof time.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.11751v1"
    },
    {
        "title": "Blindfolded monkeys or financial analysts: who is worth your money? New\n  evidence on informational inefficiencies in the U.S. stock market",
        "authors": [
            "Giuseppe Pernagallo",
            "Benedetto Torrisi"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The efficient market hypothesis has been considered one of the most\ncontroversial arguments in finance, with the academia divided between who\nclaims the impossibility of beating the market and who believes that it is\npossible to gain over the average profits. If the hypothesis holds, it means,\nas suggested by Burton Malkiel, that a blindfolded monkey selecting stocks by\nthrowing darts at a newspaper's financial pages could perform as well as a\nfinancial analyst, or even better. In this paper we use a novel approach, based\non confidence intervals for proportions, to assess the degree of inefficiency\nin the S&P 500 Index components concluding that several stocks are inefficient:\nwe estimated the proportion of inefficient stocks in the index to be between\n12.13% and 27.87%. This supports other studies proving that a financial\nanalyst, probably, is a better investor than a blindfolded monkey.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.03488v2"
    },
    {
        "title": "A Theory of Information overload applied to perfectly efficient\n  financial markets",
        "authors": [
            "Giuseppe Pernagallo",
            "Benedetto Torrisi"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Before the massive spread of computer technology, information was far from\ncomplex. The development of technology shifted the paradigm: from individuals\nwho faced scarce and costly information to individuals who face massive amounts\nof information accessible at low costs. Nowadays we are living in the era of\nbig data and investors deal every day with a huge flow of information. In the\nspirit of the modern idea that economic agents have limited computational\ncapacity, we propose an original model using information overload to show how\ntoo much information could cause financial markets to depart from the\ntraditional assumption of informational efficiency. We show that when\ninformation tends to infinite, the efficient market hypothesis ceases to be\ntrue. This happens also for lower levels of information, when the use of the\nmaximum amount of information is not optimal for investors. The present work\ncan be a stimulus to consider more realistic economic models and it can be\nfurther deepened including other realistic features present in financial\nmarkets, such as information asymmetry or noise in the transmission of\ninformation.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.03726v1"
    },
    {
        "title": "On the Co-movement of Crude, Gold Prices and Stock Index in Indian\n  Market",
        "authors": [
            "Abhibasu Sen",
            "Karabi Dutta Chaudhury"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This non-linear relationship in the joint time-frequency domain has been\nstudied for the Indian National Stock Exchange (NSE) with the international\nGold price and WTI Crude Price being converted from Dollar to Indian National\nRupee based on that week's closing exchange rate. Though a good correlation was\nobtained during some period, but as a whole no such cointegration relation can\nbe found out. Using the \\textit{Discrete Wavelet Analysis}, the data was\ndecomposed and the presence of Granger Causal relations was tested.\nUnfortunately no significant relationships are being found. We then studied the\n\\textit{Wavelet Coherence} of the two pairs viz. NSE-Nifty \\& Gold and\nNSE-Nifty \\& Crude. For different frequencies, the coherence between the pairs\nhave been studied. At lower frequencies, some relatively good coherence have\nbeen found. In this paper, we report for the first time the co-movements\nbetween Crude Oil, Gold and Indian Stock Market Index using Wavelet Analysis\n(both Discrete and Continuous), a technique which is most sophisticated and\nrecent in market analysis. Thus for long term traders they can include gold\nand/or crude in their portfolio along with NSE-Nifty index in order to decrease\nthe risk(volatility) of the portfolio for Indian Market. But for short term\ntraders, it will not be effective, not to include all the three in their\nportfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05317v1"
    },
    {
        "title": "A memory-based method to select the number of relevant components in\n  Principal Component Analysis",
        "authors": [
            "Anshul Verma",
            "Pierpaolo Vivo",
            "Tiziana Di Matteo"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We propose a new data-driven method to select the optimal number of relevant\ncomponents in Principal Component Analysis (PCA). This new method applies to\ncorrelation matrices whose time autocorrelation function decays more slowly\nthan an exponential, giving rise to long memory effects. In comparison with\nother available methods present in the literature, our procedure does not rely\non subjective evaluations and is computationally inexpensive. The underlying\nbasic idea is to use a suitable factor model to analyse the residual memory\nafter sequentially removing more and more components, and stopping the process\nwhen the maximum amount of memory has been accounted for by the retained\ncomponents. We validate our methodology on both synthetic and real financial\ndata, and find in all cases a clear and computationally superior answer\nentirely compatible with available heuristic criteria, such as cumulative\nvariance and cross-validation.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05931v6"
    },
    {
        "title": "Rough volatility of Bitcoin",
        "authors": [
            "Tetsuya Takaishi"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Recent studies have found that the log-volatility of asset returns exhibit\nroughness. This study investigates roughness or the anti-persistence of Bitcoin\nvolatility. Using the multifractal detrended fluctuation analysis, we obtain\nthe generalized Hurst exponent of the log-volatility increments and find that\nthe generalized Hurst exponent is less than $1/2$, which indicates\nlog-volatility increments that are rough. Furthermore, we find that the\ngeneralized Hurst exponent is not constant. This observation indicates that the\nlog-volatility has multifractal property. Using shuffled time series of the\nlog-volatility increments, we infer that the source of multifractality partly\ncomes from the distributional property.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.12346v1"
    },
    {
        "title": "Empirical facts characterizing banking crises: an analysis via binary\n  time series",
        "authors": [
            "Paolo Di Caro",
            "Giuseppe Pernagallo",
            "Antonino Damiano Rossello",
            "Benedetto Torrisi"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Various works have already showed that common shocks and cross-country\nfinancial linkages caused the banking systems of several countries to be highly\ninterconnected with the result that during bad times, banking crises may arise\nsimultaneously in different countries. Our aim is to provide further evidence\non the topic using a dataset made by dichotomous banking crises time series for\n66 countries from 1800 to 2014. Via the use of heatmap matrices we show that\nseveral countries exhibit pairwise correlation, which means that banking crises\ntend to occur in the same year. Clustering analysis suggests that developed\ncountries (for the most European ones) are highly similar in terms of the path\nof events. An analysis of the events that followed the Great Depression and the\nGreat Recession shows that after the crisis of 2008, banking crises tend to\ncharacterize countries tied by financial links whereas before 2008 contagion\nseems to affect countries in the same geographical area. Clustering analysis\nshows also that after financial liberalization, crises affected countries with\nsimilar economic structures and growth. Further researches should enlighten the\norigin of these linkages investigating how the process of contagion eventually\nhappens.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.12526v1"
    },
    {
        "title": "On the Compound Beta-Binomial Risk Model with Delayed Claims and\n  Randomized Dividends",
        "authors": [
            "Aparna B. S",
            "Neelesh S Upadhye"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this paper, we propose the discrete time Compound Beta-Binomial Risk Model\nwith by-claims, delayed by-claims and randomized dividends. We then analyze the\nGerber-Shiu function for the cases where the dividend threshold $d=0$ and $d>0$\nunder the assumption that the constant discount rate $\\nu \\in (0,1)$. More\nspecifically, we study the discrete time compound binomial risk model subject\nto the assumption that the probabilities with which the claims, by-claims occur\nand the dividends are issued are not fixed(constant), instead the probabilities\nare random and follow a Beta distribution with parameters $a_{i}$ and $b_{i}$,\n$i = 1, 2, 3$. Recursive expressions for the Gerber-Shiu function corresponding\nto the proposed model are obtained. The recursive relations are further\nutilized to obtain significant ruin related quantities of interest. Recursive\nrelations for probability of ruin, the probability of the deficit at ruin, the\ngenerating function of the deficit at ruin and the probability of surplus at\nruin and for the probability of the claim causing ruin are obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.03407v1"
    },
    {
        "title": "Christmas Jump in LIBOR",
        "authors": [
            "Vikenty Mikheev",
            "Serge E. Miheev"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  A short-term pattern in LIBOR dynamics was discovered. Namely, 2-month LIBOR\nexperiences a jump after Xmas. The sign and size of the jump depend on the data\ntrend on 21 days before Xmas.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.10014v1"
    },
    {
        "title": "An extended Speculation Game for the recovery of Hurst exponent of\n  financial time series",
        "authors": [
            "Kei Katahira",
            "Yu Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The speculation game is an agent-based toy model to investigate the dynamics\nof the financial market. Our model has achieved the reproduction of 10 of the\nwell-known stylized facts for financial time series. However, there is also a\ndivergence from the behavior of real market. The market price of the model\ntends to be anti-persistent to the initial price, resulting in the quite small\nvalue of Hurst exponent of price change. To overcome this problem, we extend\nthe speculation game by introducing a perturbative part to the price change\nwith the consideration of other effects besides pure speculative behaviors.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.02899v1"
    },
    {
        "title": "Heterogeneous wealth distribution, round-trip trading and the emergence\n  of volatility clustering in Speculation Game",
        "authors": [
            "Kei Katahira",
            "Yu Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This study is a detailed analysis of Speculation Game, a minimal agent-based\nmodel of financial markets, in which the round-trip trading and the dynamic\nwealth evolution with variable trading volumes are implemented. Instead of\nherding behavior, we find that the emergence of volatility clustering can be\ninduced by the heterogeneous wealth distribution among traders. In particular,\nthe spontaneous redistribution of market wealth through repetitions of\nround-trip trades can widen the wealth disparity and establish the Pareto\ndistribution of the capital size. In the meantime, large fluctuations in price\nreturn are brought on by the intermittent placements of the relatively big\norders from rich traders. Empirical data are used to support the scenario\nderived from the model.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03185v1"
    },
    {
        "title": "Application of Principal Component Analysis in Chinese Sovereign Bond\n  Market and Principal Component-Based Fixed Income Immunization",
        "authors": [
            "Lim Tze Yee",
            "Tony She",
            "Kezia Irene"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This paper analyses the Chinese Sovereign bond yield to find out the\nprincipal factors affecting the term structure of interest rate changes. We\napply Principal Component Analysis (PCA) on our data consisting of the Chinese\nSovereign bond from January 2002 till May 2018 with the different yield to\nmaturity. Then we will discuss the multi-factor immunization model (method on\nhedging market risk) on a bond portfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.07288v1"
    },
    {
        "title": "Financial ratios and stock returns reappraised through a topological\n  data analysis lens",
        "authors": [
            "Pawel Dlotko",
            "Wanling Qiu",
            "Simon Rudkin"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Firm financials are well established as return predictors, being the\ninspiration for a large set of anomalies in the asset pricing literature.\nEmploying topological data analysis we revisit the question of association\nbetween seven of the most commonly studied financial ratios and stock returns.\nSpecifically the TDA Ball Mapper algorithm is applied to visualise the point\ncloud of financial ratios as an abstract two-dimensional graph readily allowing\nfor identification of interdependencies between factors. These relationships\nare seldom monotonic, opportunities for investors to profitably exploit this\nknowledge provided by TDA abound. Clear potential offered by the tools of TDA\nto shed new light on asset pricing models is demonstrated. Scope for benefit is\nlimited only by the availability of information to the analyst.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.10297v1"
    },
    {
        "title": "Reaction Asymmetries to Social Responsibility Index Recomposition: A\n  Matching Portfolio Approach",
        "authors": [
            "Wanling Rudkin",
            "Charlie X Cai"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Listing on the Dow Jones Sustainability Index is seen as a gold-standard,\nverifying to the market that a firm is fully engaged with a corporate social\nresponsibility agenda. Robustly quantifying the impact of listing, and\nde-listing, against any industry level shocks, as well as evolution in the\ncompetitive relationship between firms within the industry, provides a strength\nabsent in existing works. It is shown that cumulative abnormal returns on\nstocks added to the index are significantly positive in the three trading weeks\nprior to the official announcement. The post-listing correction result posited\nto date is also demonstrated to hold; the proportion of periods with\nsignificant negative returns is low, however. Announcement, rather than\neffective dates are critical to returns. Differentials between these stages in\nthe chronology is an important contribution of this paper. Most effects end\nbefore the membership changes become effective. Whilst there are considerable\ngains to be made, they come pre-announcement date and require foresight to\nexploit. Investors must research likely new members to gain maximum return.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.12582v1"
    },
    {
        "title": "Long-range memory test by the burst and inter-burst duration\n  distribution",
        "authors": [
            "Vygintas Gontis"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  It is empirically established that order flow in the financial markets is\npositively auto-correlated and can serve as an example of a social system with\nlong-range memory. Nevertheless, widely used long-range memory estimators give\nvarying values of the Hurst exponent. We propose the burst and inter-burst\nduration statistical analysis as one more test of long-range memory and\nimplement it with the limit order book data comparing it with other widely used\nestimators. This method gives a more reliable evaluation of the Hurst exponent\nindependent of the stock in consideration or time definition used. Results\nstrengthen the expectation that burst and inter-burst duration analysis can\nserve as a better method to investigate the property of long-range memory.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.00596v2"
    },
    {
        "title": "Doubly Multiplicative Error Models with Long- and Short-run Components",
        "authors": [
            "Alessandra Amendola",
            "Vincenzo Candila",
            "Fabrizio Cipollini",
            "Giampiero M. Gallo"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We suggest the Doubly Multiplicative Error class of models (DMEM) for\nmodeling and forecasting realized volatility, which combines two components\naccommodating low-, respectively, high-frequency features in the data. We\nderive the theoretical properties of the Maximum Likelihood and Generalized\nMethod of Moments estimators. Two such models are then proposed, the\nComponent-MEM, which uses daily data for both components, and the MEM-MIDAS,\nwhich exploits the logic of MIxed-DAta Sampling (MIDAS). The empirical\napplication involves the S&P 500, NASDAQ, FTSE 100 and Hang Seng indices:\nirrespective of the market, both DMEM's outperform the HAR and other relevant\nGARCH-type models.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.03458v1"
    },
    {
        "title": "Manifold Feature Index: A novel index based on high-dimensional data\n  simplification",
        "authors": [
            "Chenkai Xu",
            "Hongwei Lin",
            "Xuansu Fang"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper, we propose a novel stock index model, namely the manifold\nfeature(MF) index, to reflect the overall price activity of the entire stock\nmarket. Based on the theory of manifold learning, the researched stock dataset\nis assumed to be a low-dimensional manifold embedded in a higher-dimensional\nEuclidean space. After data preprocessing, its manifold structure and discrete\nLaplace-Beltrami operator(LBO) matrix are constructed. We propose a\nhigh-dimensional data feature detection method to detect feature points on the\neigenvectors of LBO, and the stocks corresponding to these feature points are\nconsidered as the constituent stocks of the MF index. Finally, the MF index is\ngenerated by a weighted formula using the price and market capitalization of\nthese constituents. The stock market studied in this research is the Shanghai\nStock Exchange(SSE). We propose four metrics to compare the MF index series and\nthe SSE index series (SSE 50, SSE 100, SSE 150, SSE 180 and SSE 380). From the\nperspective of data approximation, the results demonstrate that our indexes are\ncloser to the stock market than the SSE index series. From the perspective of\nrisk premium, MF indexes have higher stability and lower risk.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.11119v1"
    },
    {
        "title": "Examining the Effect of COVID-19 on Foreign Exchange Rate and Stock\n  Market -- An Applied Insight into the Variable Effects of Lockdown on Indian\n  Economy",
        "authors": [
            "Indrajit Banerjee",
            "Atul Kumar",
            "Rupam Bhattacharyya"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Since March 25, 2020, India had been under a nation-wide lockdown announced\nas a response to the spread of SARS-CoV-2 and COVID-19 and has resorted to a\nprocess of 'unlocking' the lockdown over the past couple of months. This work\nattempts to examine the effect of novel coronavirus 2019 (COVID-19) and its\nresulting disease, the COVID-19, on the foreign exchange rates and stock market\nperformances of India using secondary data over a span of 112 days spanning\nbetween March 11 and June 30, 2020. The study explores whether the causal\nrelationships and directions among the growth rate of confirmed cases\n(GROWTHC), exchange rate (GEX) and SENSEX value (GSENSEX) are remaining the\nsame across different pre and post-lockdown phases, attempting to capture any\npotential changes over time via the vector autoregressive (VAR) models. A\npositive correlation is found between the growth rate of confirmed cases and\nthe growth rate of exchange rate, and a negative correlation between the growth\nrate of confirmed cases and the growth rate of SENSEX value. However, on\napplying a vector autoregressive (VAR) model, it is observed that an increase\nin the confirmed COVID-19 cases causes no significant change in the values of\nthe exchange rate and SENSEX index. The result varies if the analysis is split\nacross different time periods - before lockdown, the four phases of lockdown,\nand the first phase of unlock. Nuanced and sensible interpretations of the\nnumeric results indicate significant variability across time in terms of the\nrelation between the variables of interest. The detailed knowledge about the\nvarying patterns of dependence could potentially help the policy makers and\ninvestors of India in order to develop their strategies to cope up with the\nsituation.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.14499v4"
    },
    {
        "title": "Statistical properties of the aftershocks of stock market crashes\n  revisited: Analysis based on the 1987 crash, financial-crisis-2008 and\n  COVID-19 pandemic",
        "authors": [
            "Anish Rai",
            "Ajit Mahata",
            "Md Nurujjaman",
            "Om Prakash"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  During any unique crisis, panic sell-off leads to a massive stock market\ncrash that may continue for more than a day, termed as mainshock. The effect of\na mainshock in the form of aftershocks can be felt throughout the recovery\nphase of stock price. As the market remains in stress during recovery, any\nsmall perturbation leads to a relatively smaller aftershock. The duration of\nthe recovery phase has been estimated using structural break analysis. We have\ncarried out statistical analyses of the 1987 stock market crash, 2008 financial\ncrisis and 2020 COVID-19 pandemic considering the actual crash-times of the\nmainshock and aftershocks. Earlier, such analyses were done considering an\nabsolute one-day return, which cannot capture a crash properly. The results\nshow that the mainshock and aftershock in the stock market follow the\nGutenberg-Richter (GR) power law. Further, we obtained a higher $\\beta$ value\nfor the COVID-19 crash compared to the financial-crisis-2008 from the GR law.\nThis implies that the recovery of stock price during COVID-19 may be faster\nthan the financial-crisis-2008. The result is consistent with the present\nrecovery of the market from the COVID-19 pandemic. The analysis shows that the\nhigh magnitude aftershocks are rare, and low magnitude aftershocks are frequent\nduring the recovery phase. The analysis also shows that the distribution\n$P(\\tau_i)$ follows the generalized Pareto distribution, i.e.,\n$\\displaystyle~P(\\tau_i)\\propto\\frac{1}{\\{1+\\lambda(q-1)\\tau_i\\}^{\\frac{1}{(q-1)}}}$,\nwhere $\\lambda$ and $q$ are constants and $\\tau_i$ is the inter-occurrence\ntime. This analysis may help investors to restructure their portfolios during a\nmarket crash.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.03012v2"
    },
    {
        "title": "Dynamical Characteristics of Global Stock Markets Based on Time\n  Dependent Tsallis Non-Extensive Statistics and Generalized Hurst Exponents",
        "authors": [
            "Ioannis P. Antoniades",
            "Leonidas P. Karakatsanis",
            "Evgenios G. Pavlos"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We perform non-linear analysis on stock market indices using time-dependent\nextended Tsallis statistics. Specifically, we evaluate the q-triplet for\nparticular time periods with the purpose of demonstrating the temporal\ndependence of the extended characteristics of the underlying market dynamics.\nWe apply the analysis on daily close price timeseries of four major global\nmarkets (S&P 500, Tokyo-NIKKEI, Frankfurt-DAX, London-LSE). For comparison, we\nalso compute time-dependent Generalized Hurst Exponents (GHE) Hq using the GHE\nmethod, thus estimating the temporal evolution of the multiscaling\ncharacteristics of the index dynamics. We focus on periods before and after\ncritical market events such as stock market bubbles (2000 dot.com bubble,\nJapanese 1990 bubble, 2008 US real estate crisis) and find that the temporal\ntrends of q-triplet values significantly differ among these periods indicating\nthat in the rising period before a bubble break, the underlying extended\nstatistics of the market dynamics strongly deviates from purely stochastic\nbehavior, whereas, after the breakdown, it gradually converges to the\nGaussian-like behavior which is a characteristic of an efficient market. We\nalso conclude that relative temporal variation patterns of the Tsallis\nq-triplet can be connected to different aspects of market dynamics and reveals\nuseful information about market conditions especially those underlying the\ndevelopment of a stock market bubble. We found specific temporal patterns and\ntrends in the relative variation of the indices in the q-triplet that\ndistinguish periods just before and just after a stock-market bubble break.\nDifferences between endogenous and exogenous stock market crises are also\ncaptured by the temporal changes in the Tsallis q-triplet. Finally, we\nintroduce two new time-dependent empirical metrics (Q-metrics) that are\nfunctions of the Tsallis q-triplet.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.06856v2"
    },
    {
        "title": "Censored EM algorithm for Weibull mixtures: application to arrival times\n  of market orders",
        "authors": [
            "Markus Kreer",
            "Ayse Kizilersu",
            "Anthony W. Thomas"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In a previous analysis the problem of \"zero-inflated\" time data (caused by\nhigh frequency trading in the electronic order book) was handled by\nleft-truncating the inter-arrival times. We demonstrated, using rigorous\nstatistical methods, that the Weibull distribution describes the corresponding\nstochastic dynamics for all inter-arrival time differences except in the region\nnear zero. However, since the truncated Weibull distribution was not able to\ndescribe the huge \"zero-inflated\" probability mass in the neighbourhood of zero\n(making up approximately 50\\% of the data for limit orders), it became clear\nthat the entire probability distribution is a mixture distribution of which the\nWeibull distribution is a significant part. Here we use a censored EM algorithm\nto analyse data for the difference of the arrival times of market orders, which\nusually have a much lower percentage of zero inflation, for four selected\nstocks trading on the London Stock Exchange.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.10601v1"
    },
    {
        "title": "Simple approaches on how to discover promising strategies for efficient\n  enterprise performance, at time of crisis in the case of SMEs : Voronoi\n  clustering and outlier effects perspective",
        "authors": [
            "Marcel Ausloos",
            "Francesca Bartolacci",
            "Nicola G. Castellano",
            "Roy Cerqueti"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This paper analyzes the connection between innovation activities of companies\n-- implemented before a financial crisis -- and their performance -- measured\nafter such a time of crisis. Pertinent data about companies listed in the STAR\nMarket Segment of the Italian Stock Exchange is analyzed. Innovation is\nmeasured through the level of investments in total tangible and intangible\nfixed assets in 2006-2007, while performance is captured through growth --\nexpressed by variations of sales or of total assets, -- profitability --\nthrough ROI or ROS evolution, - and productivity -- through asset turnover or\nsales/employee in the period 2008-2010. The variables of interest are analyzed\nand compared through statistical techniques and by adopting a cluster analysis.\nIn particular, a Voronoi tessellation is implemented in a varying centroids\nframework. In accord with a large part of the literature, we find that the\nbehavior of the performance of the companies is not univocal when they\ninnovate. The statistical outliers are the best cases in order to suggest\nefficient strategies. In brief, it is found that a positive rate of investments\nis preferable.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.14297v1"
    },
    {
        "title": "Development and similarity of insurance markets of European Union\n  countries after the enlargement in 2004",
        "authors": [
            "Anna Denkowska",
            "Stanisław Wanat"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The enlargement of the European Union to new countries in 2004 launched\nmechanisms supporting the development of various social and economic areas, as\nwell as levelling the differences between the Community members in these areas.\nThis article focuses on the insurance sector. Its main purpose is to analyze\nthe development and similarity of the insurance markets of old and new members\nof the European Union after the enlargement in 2004.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.15078v1"
    },
    {
        "title": "Event-Driven LSTM For Forex Price Prediction",
        "authors": [
            "Ling Qi",
            "Matloob Khushi",
            "Josiah Poon"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The majority of studies in the field of AI guided financial trading focus on\npurely applying machine learning algorithms to continuous historical price and\ntechnical analysis data. However, due to non-stationary and high volatile\nnature of Forex market most algorithms fail when put into real practice. We\ndeveloped novel event-driven features which indicate a change of trend in\ndirection. We then build long deep learning models to predict a retracement\npoint providing a perfect entry point to gain maximum profit. We use a simple\nrecurrent neural network (RNN) as our baseline model and compared with\nshort-term memory (LSTM), bidirectional long short-term memory (BiLSTM) and\ngated recurrent unit (GRU). Our experiment results show that the proposed\nevent-driven feature selection together with the proposed models can form a\nrobust prediction system which supports accurate trading strategies with\nminimal risk. Our best model on 15-minutes interval data for the EUR/GBP\ncurrency achieved RME 0.006x10^(-3) , RMSE 2.407x10^(-3), MAE 1.708x10^(-3),\nMAPE 0.194% outperforming previous studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.01499v1"
    },
    {
        "title": "Exploring asymmetric multifractal cross-correlations of price-volatility\n  and asymmetric volatility dynamics in cryptocurrency markets",
        "authors": [
            "Shinji Kakinaka",
            "Ken Umeno"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Asymmetric relationship between price and volatility is a prominent feature\nof the financial market time series. This paper explores the price-volatility\nnexus in cryptocurrency markets and investigates the presence of asymmetric\nvolatility effect between uptrend (bull) and downtrend (bear) regimes. The\nconventional GARCH-class models have shown that in cryptocurrency markets,\nasymmetric reactions of volatility to returns differ from those of other\ntraditional financial assets. We address this issue from a viewpoint of fractal\nanalysis, which can cover the nonlinear interactions and the self-similarity\nproperties widely acknowledged in the field of econophysics. The asymmetric\ncross-correlations between price and volatility for Bitcoin (BTC), Ethereum\n(ETH), Ripple (XRP), and Litecoin (LTC) during the period from June 1, 2016 to\nDecember 28, 2020 are investigated using the MF-ADCCA method and quantified via\nthe asymmetric DCCA coefficient. The approaches take into account the\nnonlinearity and asymmetric multifractal scaling properties, providing new\ninsights in investigating the relationships in a dynamical way. We find that\ncross-correlations are stronger in downtrend markets than in uptrend markets\nfor maturing BTC and ETH. In contrast, for XRP and LTC, inverted reactions are\npresent where cross-correlations are stronger in uptrend markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.02865v2"
    },
    {
        "title": "Asymmetric Tsallis distributions for modelling financial market dynamics",
        "authors": [
            "Sandhya Devi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Financial markets are highly non-linear and non-equilibrium systems. Earlier\nworks have suggested that the behavior of market returns can be well described\nwithin the framework of non-extensive Tsallis statistics or superstatistics.\nFor small time scales (delays), a good fit to the distributions of stock\nreturns is obtained with q-Gaussian distributions, which can be derived either\nfrom Tsallis statistics or superstatistics. These distributions are symmetric.\nHowever, as the time lag increases, the distributions become increasingly\nnon-symmetric. In this work, we address this problem by considering the data\ndistribution as a linear combination of two independent normalized\ndistributions - one for negative returns and one for positive returns. Each of\nthese two independent distributions are half q-Gaussians with different\nnon-extensivity parameter q and temperature parameter beta. Using this model,\nwe investigate the behavior of stock market returns over time scales from 1 to\n80 days. The data covers both the .com bubble and the 2008 crash periods. These\ninvestigations show that for all the time lags, the fits to the data\ndistributions are better using asymmetric distributions than symmetric\nq-Gaussian distributions. The behaviors of the q parameter are quite different\nfor positive and negative returns. For positive returns, q approaches a\nconstant value of 1 after a certain lag, indicating the distributions have\nreached equilibrium. On the other hand, for negative returns, the q values do\nnot reach a stationary value over the time scales studied. In the present\nmodel, the markets show a transition from normal to superdiffusive behavior (a\npossible phase transition) during the 2008 crash period. Such behavior is not\nobserved with a symmetric q-Gaussian distribution model with q independent of\ntime lag.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04532v1"
    },
    {
        "title": "Wavelet Denoised-ResNet CNN and LightGBM Method to Predict Forex Rate of\n  Change",
        "authors": [
            "Yiqi Zhao",
            "Matloob Khushi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Foreign Exchange (Forex) is the largest financial market in the world. The\ndaily trading volume of the Forex market is much higher than that of stock and\nfutures markets. Therefore, it is of great significance for investors to\nestablish a foreign exchange forecast model. In this paper, we propose a\nWavelet Denoised-ResNet with LightGBM model to predict the rate of change of\nForex price after five time intervals to allow enough time to execute trades.\nAll the prices are denoised by wavelet transform, and a matrix of 30 time\nintervals is formed by calculating technical indicators. Image features are\nobtained by feeding the maxtrix into a ResNet. Finally, the technical\nindicators and image features are fed to LightGBM. Our experiments on 5-minutes\nUSDJPY demonstrate that the model outperforms baseline modles with MAE:\n0.240977x10EXP-3 MSE: 0.156x10EXP-6 and RMSE: 0.395185x10EXP-3. An accurate\nprice prediction after 25 minutes in future provides a window of opportunity\nfor hedge funds algorithm trading. The code is available from\nhttps://mkhushi.github.io/\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04861v1"
    },
    {
        "title": "Combination of window-sliding and prediction range method based on LSTM\n  model for predicting cryptocurrency",
        "authors": [
            "Yifan Yao",
            "Lina Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The present study aims to establish the model of the cryptocurrency price\ntrend based on financial theory using the LSTM model with multiple combinations\nbetween the window length and the predicting horizons, the random walk model is\nalso applied with different parameter settings.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.05448v1"
    },
    {
        "title": "Time-varying properties of asymmetric volatility and multifractality in\n  Bitcoin",
        "authors": [
            "Tetsuya Takaishi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This study investigates the volatility of daily Bitcoin returns and\nmultifractal properties of the Bitcoin market by employing the rolling window\nmethod and examines relationships between the volatility asymmetry and market\nefficiency. Whilst we find an inverted asymmetry in the volatility of Bitcoin,\nits magnitude changes over time, and recently, it has become small. This\nasymmetric pattern of volatility also exists in higher frequency returns. Other\nmeasurements, such as kurtosis, skewness, average, serial correlation, and\nmultifractal degree, also change over time. Thus, we argue that properties of\nthe Bitcoin market are mostly time dependent. We examine efficiency-related\nmeasures: the Hurst exponent, multifractal degree, and kurtosis. We find that\nwhen these measures represent that the market is more efficient, the volatility\nasymmetry weakens. For the recent Bitcoin market, both efficiency-related\nmeasures and the volatility asymmetry prove that the market becomes more\nefficient.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.07425v1"
    },
    {
        "title": "Power-Law Return-Volatility Cross Correlations of Bitcoin",
        "authors": [
            "T. Takaishi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper investigates the return-volatility asymmetry of Bitcoin. We find\nthat the cross correlations between return and volatility (squared return) are\nmostly insignificant on a daily level. In the high-frequency region, we find\nthata power-law appears in negative cross correlation between returns and\nfuture volatilities, which suggests that the cross correlation is\n\\revision{long ranged}. We also calculate a cross correlation between returns\nand the power of absolute returns, and we find that the strength of\n\\revision{the cross correlations} depends on the value of the power.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.08187v1"
    },
    {
        "title": "Modeling Price Clustering in High-Frequency Prices",
        "authors": [
            "Vladimír Holý",
            "Petra Tomanová"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The price clustering phenomenon manifesting itself as an increased occurrence\nof specific prices is widely observed and well-documented for various financial\ninstruments and markets. In the literature, however, it is rarely incorporated\ninto price models. We consider that there are several types of agents trading\nonly in specific multiples of the tick size resulting in an increased\noccurrence of these multiples in prices. For example, stocks on the NYSE and\nNASDAQ exchanges are traded with precision to one cent but multiples of five\ncents and ten cents occur much more often in prices. To capture this behavior,\nwe propose a discrete price model based on a mixture of double Poisson\ndistributions with dynamic volatility and dynamic proportions of agent types.\nThe model is estimated by the maximum likelihood method. In an empirical study\nof DJIA stocks, we find that higher instantaneous volatility leads to weaker\nprice clustering at the ultra-high frequency. This is in sharp contrast with\nresults at low frequencies which show that daily realized volatility has a\npositive impact on price clustering.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12112v2"
    },
    {
        "title": "Statistical mechanics and Bayesian Inference addressed to the Osborne\n  Paradox",
        "authors": [
            "Geoffrey Ducournau"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  One of the greatest contributors of the 20th century among all academician in\nthe field of statistical finance, M. F. M. Osborne published in 1956 [6] an\nessential paper and proposed to treat the question of stock market motion\nthrough the prism of both the Law of Weber-Fechner [1, 4] and the branch of\nphysics developed by James Clerk Maxwell, Ludwig Boltzmann and Josiah Willard\nGibbs [3, 5] namely the statistical mechanics. He proposed an improvement of\nthe known research made by his predecessor Louis Jean-Baptiste Alphonse\nBachelier, by not considering the arithmetic changes of stock prices as means\nof statistical measurement, but by drawing on the Weber-Fechner Law, to treat\nthe changes of prices. Osborne emphasized that as in statistical mechanics, the\nprobability distribution of the steady-state of subjective change in prices is\ndetermined by the condition of maximum probability, a statement close to the\nGibbs distribution conditions. However, Osborne also admitted that the\nempirical observation of the probability distribution of logarithmic changes of\nstock prices was emphasizing obvious asymmetries and consequently could not\nperfectly confirm his prior theory. The purpose of this paper is to propose an\nexplanation to what we could call the Osborne paradox and then address an\nalternative approach via Bayesian inference regarding the description of the\nprobability distribution of changes in logarithms of prices that was\nthenceforth under the prism of frequentist inference. We show that the stock\nmarket returns are locally described by equilibrium statistical mechanics with\nconserved statistics variables, whereas globally there is yet other statistics\nwith persistent flowing variables that can be effectively described by a\nsuperposition of several statistics on different time scales, namely, a\nsuperstatistics.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.00788v1"
    },
    {
        "title": "How good is good? Probabilistic benchmarks and nanofinance+",
        "authors": [
            "Rolando Gonzales Martinez"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Benchmarks are standards that allow to identify opportunities for improvement\namong comparable units. This study suggests a 2-step methodology for\ncalculating probabilistic benchmarks in noisy data sets: (i) double-hyperbolic\nundersampling filters the noise of key performance indicators (KPIs), and (ii)\na relevance vector machine estimates probabilistic benchmarks with denoised\nKPIs. The usefulness of the methods is illustrated with an application to a\ndatabase of nano-finance+. The results indicate that-in the case of\nnano-finance groups-a higher discrimination power is obtained with variables\nthat capture the macro-economic environment of the country where a group\noperates. Also, the estimates show that groups operating in rural regions have\ndifferent probabilistic benchmarks, compared to groups in urban and peri-urban\nareas.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.01669v1"
    },
    {
        "title": "Risk-dependent centrality in the Brazilian stock market",
        "authors": [
            "Michel Alexandre",
            "Kauê Lopes de Moraes",
            "Francisco Aparecido Rodrigues"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The purpose of this paper is to calculate the risk-dependent centrality (RDC)\nof the Brazilian stock market. We computed the RDC for assets traded on the\nBrazilian stock market between January 2008 to June 2020 at different levels of\nexternal risk. We observed that the ranking of assets based on the RDC depends\non the external risk. Rankings' volatility is related to crisis events,\ncapturing the recent Brazilian economic-political crisis. Moreover, we have\nfound a negative correlation between the average volatility of assets' ranking\nbased on the RDC and the average daily returns on the stock market. It goes in\nhand with the hypothesis that the rankings' volatility is higher in periods of\ncrisis.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09059v1"
    },
    {
        "title": "A Survey of Forex and Stock Price Prediction Using Deep Learning",
        "authors": [
            "Zexin Hu",
            "Yiqi Zhao",
            "Matloob Khushi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The prediction of stock and foreign exchange (Forex) had always been a hot\nand profitable area of study. Deep learning application had proven to yields\nbetter accuracy and return in the field of financial prediction and\nforecasting. In this survey we selected papers from the DBLP database for\ncomparison and analysis. We classified papers according to different deep\nlearning methods, which included: Convolutional neural network (CNN), Long\nShort-Term Memory (LSTM), Deep neural network (DNN), Recurrent Neural Network\n(RNN), Reinforcement Learning, and other deep learning methods such as HAN,\nNLP, and Wavenet. Furthermore, this paper reviewed the dataset, variable,\nmodel, and results of each article. The survey presented the results through\nthe most used performance metrics: RMSE, MAPE, MAE, MSE, accuracy, Sharpe\nratio, and return rate. We identified that recent models that combined LSTM\nwith other methods, for example, DNN, are widely researched. Reinforcement\nlearning and other deep learning method yielded great returns and performances.\nWe conclude that in recent years the trend of using deep-learning based method\nfor financial modeling is exponentially rising.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09750v1"
    },
    {
        "title": "Cryptocurrency Dynamics: Rodeo or Ascot?",
        "authors": [
            "Konstantin Häusler",
            "Wolfgang Karl Härdle"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We model the dynamics of the cryptocurrency (CC) asset class via a stochastic\nvolatility with correlated jumps (SVCJ) model with rolling-window parameter\nestimates. By analyzing the time-series of parameters, stylized patterns are\nobservable which are robust to changes of the window size and supported by\ncluster analysis. During bullish periods, volatility stabilizes at low levels\nand the size and volatility of jumps in mean decreases. In bearish periods\nthough, volatility increases and takes longer to return to its long-run trend.\nFurthermore, jumps in mean and jumps in volatility are independent. With the\nrise of the CC market in 2017, a level shift of the volatility of volatility\noccurred. All codes are available on Quantlet.com.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.12461v2"
    },
    {
        "title": "Utilizing Technical Data to Discover Similar Companies in Dhaka Stock\n  Exchange",
        "authors": [
            "Tashreef Muhammad",
            "Tahsin Aziz",
            "Mohammad Shafiul Alam"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Stock market investment have been an ideal form of investment for many years.\nInvesting capitals smartly in stock market yields high profit returns. But\nthere are many companies available in a market. Currently there are more than\n$345$ active companies who have stocks in Dhaka Stock Exchange (DSE). Analyzing\nall these companies is quite impossible. However, many companies tend to move\ntogether. This study aims at finding which companies in DSE have a close\nconnection and move alongside each other. By analyzing this relation, the\ninvestors and traders will be able to analyze a lot of companies' statistics\nfrom a calculating just a handful number of companies. The conducted experiment\nyielded promising results. It was found that though the system was not given\nanything other than technical data, it was able to identify companies that show\ndomain specific outcomes. In other words, a relation between technical data and\nfundamental data was discovered from the conducted experiment.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.04455v1"
    },
    {
        "title": "Stock market forecasting using DRAGAN and feature matching",
        "authors": [
            "Fateme Shahabi Nejad",
            "Mohammad Mehdi Ebadzadeh"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Applying machine learning methods to forecast stock prices has been one of\nthe research topics of interest in recent years. Almost few studies have been\nreported based on generative adversarial networks (GANs) in this area, but\ntheir results are promising. GANs are powerful generative models successfully\napplied in different areas but suffer from inherent challenges such as training\ninstability and mode collapse. Also, a primary concern is capturing\ncorrelations in stock prices. Therefore, our challenges fall into two main\ncategories: capturing correlations and inherent problems of GANs. In this\npaper, we have introduced a novel framework based on DRAGAN and feature\nmatching for stock price forecasting, which improves training stability and\nalleviates mode collapse. We have employed windowing to acquire temporal\ncorrelations by the generator. Also, we have exploited conditioning on\ndiscriminator inputs to capture temporal correlations and correlations between\nprices and features. Experimental results on data from several stocks indicate\nthat our proposed method outperformed long short-term memory (LSTM) as a\nbaseline method, also basic GANs and WGAN-GP as two different variants of GANs.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.05693v1"
    },
    {
        "title": "Wavelet Analysis for Time Series Financial Signals via Element Analysis",
        "authors": [
            "Nathan Zavanelli"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The method of element analysis is proposed here as an alternative to\ntraditional wavelet-based approaches to analyzing perturbations in financial\nsignals by scale. In this method, the processes that generate oscillations in\nfinancial signals are modelled as scaled, shifted, and isolated events that\nproduce ripples of various frequencies across a sea of noise as opposed to a\nsimple sinusoidal or mixed frequency oscillation or an impulse. This allows one\nto directly estimate the wavelet parameters derived only from the generating\nfunctions, rejecting spurious perturbations driven by noise or extraneous\nfactors. Financial signals may then be reconstructed based on a finite set of\ngenerators localized in time and frequency. This method offers a marked\nadvantage compared to traditional econometric tools because it directly targets\nthe generators of oscillations. Furthermore, the choice of the Morse wavelet\nallows for wide latitude in capturing a broad set of diverse generators. In\nthis work, the basic mathematical principles underlying element analysis are\npresented, and the method is applied to the study of variance in financial\ndata, where the advantages of element analysis over traditional wavelet\ntechniques is demonstrated. Specifically, in the example analysis of inflation\nexpectations, element analysis shows a clear ability to distinguish between\noscillations formed by noise and those formed by generators logically matched\nto historical events.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.13255v1"
    },
    {
        "title": "Random matrix approach to the dynamics of stock inventory variations",
        "authors": [
            "W. -X. Zhou",
            "G. -H. Mu",
            "J. Kertész"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We study the cross-correlation matrix $C_{ij}$ of inventory variations of the\nmost active individual and institutional investors in an emerging market to\nunderstand the dynamics of inventory variations. We find that the distribution\nof cross-correlation coefficient $C_{ij}$ has a power-law form in the bulk\nfollowed by exponential tails and there are more positive coefficients than\nnegative ones. In addition, it is more possible that two individuals or two\ninstitutions have stronger inventory variation correlation than one individual\nand one institution. We find that the largest and the second largest\neigenvalues ($\\lambda_1$ and $\\lambda_2$) of the correlation matrix cannot be\nexplained by the random matrix theory and the projection of inventory\nvariations on the first eigenvector $u(\\lambda_1)$ are linearly correlated with\nstock returns, where individual investors play a dominating role. The investors\nare classified into three categories based on the cross-correlation\ncoefficients $C_{VR}$ between inventory variations and stock returns. Half\nindividuals are reversing investors who exhibit evident buy and sell herding\nbehaviors, while 6% individuals are trending investors. For institutions, only\n10% and 8% investors are trending and reversing investors. A strong Granger\ncausality is unveiled from stock returns to inventory variations, which means\nthat a large proportion of individuals hold the reversing trading strategy and\na small part of individuals hold the trending strategy. Comparing with the case\nof Spanish market, Chinese investors exhibit common and market-specific\nbehaviors. Our empirical findings have scientific significance in the\nunderstanding of investors' trading behaviors and in the construction of\nagent-based models for stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.0433v1"
    },
    {
        "title": "Understanding the source of multifractality in financial markets",
        "authors": [
            "Jozef Barunik",
            "Tomaso Aste",
            "Tiziana Di Matteo",
            "Ruipeng Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  In this paper, we use the generalized Hurst exponent approach to study the\nmulti- scaling behavior of different financial time series. We show that this\napproach is robust and powerful in detecting different types of multiscaling.\nWe observe a puzzling phenomenon where an apparent increase in multifractality\nis measured in time series generated from shuffled returns, where all\ntime-correlations are destroyed, while the return distributions are conserved.\nThis effect is robust and it is reproduced in several real financial data\nincluding stock market indices, exchange rates and interest rates. In order to\nunderstand the origin of this effect we investigate different simulated time\nseries by means of the Markov switching multifractal (MSM) model,\nautoregressive fractionally integrated moving average (ARFIMA) processes with\nstable innovations, fractional Brownian motion and Levy flights. Overall we\nconclude that the multifractality observed in financial time series is mainly a\nconsequence of the characteristic fat-tailed distribution of the returns and\ntime-correlations have the effect to decrease the measured multifractality.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.1535v2"
    },
    {
        "title": "The class of nonlinear stochastic models as a background for the bursty\n  behavior in financial markets",
        "authors": [
            "Vygintas Gontis",
            "Aleksejus Kononovicius",
            "Stefan Reimann"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We investigate large changes, bursts, of the continuous stochastic signals,\nwhen the exponent of multiplicativity is higher than one. Earlier we have\nproposed a general nonlinear stochastic model which can be transformed into\nBessel process with known first hitting (first passage) time statistics. Using\nthese results we derive PDF of burst duration for the proposed model. We\nconfirm analytical expressions by numerical evaluation and discuss bursty\nbehavior of return in financial markets in the framework of modeling by\nnonlinear SDE.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3083v3"
    },
    {
        "title": "Survivability and centrality measures for networks of financial market\n  indices",
        "authors": [
            "Leonidas Sandoval Junior"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Using data from 92 indices of stock exchanges worldwide, I analize the\ncluster formation and evolution from 2007 to 2010, which includes the Subprime\nMortgage Crisis of 2008, using asset graphs based on distance thresholds. I\nalso study the survivability of connections and of clusters through time and\nthe influence of noise in centrality measures applied to the networks of\nfinancial indices.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4490v1"
    },
    {
        "title": "Co-movement of energy commodities revisited: Evidence from wavelet\n  coherence analysis",
        "authors": [
            "Lukas Vacha",
            "Jozef Barunik"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  In this paper, we contribute to the literature on energy market co-movement\nby studying its dynamics in the time-frequency domain. The novelty of our\napproach lies in the application of wavelet tools to commodity market data. A\nmajor part of economic time series analysis is done in the time or frequency\ndomain separately. Wavelet analysis combines these two fundamental approaches\nallowing study of the time series in the time- frequency domain. Using this\nframework, we propose a new, model-free way of estimating time-varying cor-\nrelations. In the empirical analysis, we connect our approach to the dynamic\nconditional correlation approach of Engle (2002) on the main components of the\nenergy sector. Namely, we use crude oil, gasoline, heating oil, and natural gas\non a nearest-future basis over a period of approximately 16 and 1/2 years\nbeginning on November 1, 1993 and ending on July 21, 2010. Using wavelet\ncoherence, we uncover interesting dynamics of correlations between energy\ncommodities in the time-frequency space.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4776v1"
    },
    {
        "title": "On Hurst exponent estimation under heavy-tailed distributions",
        "authors": [
            "Jozef Barunik",
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  In this paper, we show how the sampling properties of the Hurst exponent\nmethods of estimation change with the presence of heavy tails. We run extensive\nMonte Carlo simulations to find out how rescaled range analysis (R/S),\nmultifractal detrended fluctuation analysis (MF-DFA), detrending moving average\n(DMA) and generalized Hurst exponent approach (GHE) estimate Hurst exponent on\nindependent series with different heavy tails. For this purpose, we generate\nindependent random series from stable distribution with stability exponent\n{\\alpha} changing from 1.1 (heaviest tails) to 2 (Gaussian normal distribution)\nand we estimate the Hurst exponent using the different methods. R/S and GHE\nprove to be robust to heavy tails in the underlying process. GHE provides the\nlowest variance and bias in comparison to the other methods regardless the\npresence of heavy tails in data and sample size. Utilizing this result, we\napply a novel approach of the intraday time-dependent Hurst exponent and we\nestimate the Hurst exponent on high frequency data for each trading day\nseparately. We obtain Hurst exponents for S&P500 index for the period beginning\nwith year 1983 and ending by November 2009 and we discuss the surprising result\nwhich uncovers how the market's behavior changed over this long period.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4786v1"
    },
    {
        "title": "Asymmetric correlation matrices: an analysis of financial data",
        "authors": [
            "Giacomo Livan",
            "Luca Rebecchi"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We analyze the spectral properties of correlation matrices between distinct\nstatistical systems. Such matrices are intrinsically non symmetric, and lend\nthemselves to extend the spectral analyses usually performed on standard\nPearson correlation matrices to the realm of complex eigenvalues. We employ\nsome recent random matrix theory results on the average eigenvalue density of\nthis type of matrices to distinguish between noise and non trivial correlation\nstructures, and we focus on financial data as a case study. Namely, we employ\ndaily prices of stocks belonging to the American and British stock exchanges,\nand look for the emergence of correlations between two such markets in the\neigenvalue spectrum of their non symmetric correlation matrix. We find several\nnon trivial results, also when considering time-lagged correlations over short\nlags, and we corroborate our findings by additionally studying the asymmetric\ncorrelation matrix of the principal components of our datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.6535v2"
    },
    {
        "title": "Emergence of statistically validated financial intraday lead-lag\n  relationships",
        "authors": [
            "Chester Curme",
            "Michele Tumminello",
            "Rosario N. Mantegna",
            "H. Eugene Stanley",
            "Dror Y. Kenett"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  According to the leading models in modern finance, the presence of intraday\nlead-lag relationships between financial assets is negligible in efficient\nmarkets. With the advance of technology, however, markets have become more\nsophisticated. To determine whether this has resulted in an improved market\nefficiency, we investigate whether statistically significant lagged correlation\nrelationships exist in financial markets. We introduce a numerical method to\nstatistically validate links in correlation-based networks, and employ our\nmethod to study lagged correlation networks of equity returns in financial\nmarkets. Crucially, our statistical validation of lead-lag relationships\naccounts for multiple hypothesis testing over all stock pairs. In an analysis\nof intraday transaction data from the periods 2002--2003 and 2011--2012, we\nfind a striking growth in the networks as we increase the frequency with which\nwe sample returns. We compute how the number of validated links and the\nmagnitude of correlations change with increasing sampling frequency, and\ncompare the results between the two data sets. Finally, we compare topological\nproperties of the directed correlation-based networks from the two periods\nusing the in-degree and out-degree distributions and an analysis of three-node\nmotifs. Our analysis suggests a growth in both the efficiency and instability\nof financial markets over the past decade.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.0462v1"
    },
    {
        "title": "Modeling the stylized facts of wholesale system marginal price (SMP) and\n  the impacts of regulatory reforms on the Greek Electricity Market",
        "authors": [
            "G. Papaioannou",
            "P. Papaioannou",
            "N. Parliaris"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  This work presents the results of an empirical research with the target of\nmodeling the stylized facts of the daily expost System Marginal Price (SMP) of\nthe Greek wholesale electricity market, using data from January 2004 to\nDecember of 2011. SMP is considered here as the footprint of an underline\nstochastic and nonlinear process that bears all the information reflecting not\nonly the effects of changes in endogenous or fundamental factors of the market\nbut also the impacts of a series of regulatory reforms that have continuously\nchanged the market's microstructure. To capture the dynamics of the conditional\nmean and volatility of SMP that generate the stylized facts(mean reversion,\nprice spikes, fat tails price distribution etc), a number of ARMAX GARCH models\nhave been estimated using as regressors an extensive set of fundamental factors\nin the Greek electricity market as well as dummy variables that mimic the\nhistory of Regulator's interventions. The findings show that changes in the\nmicrostructure of the market caused by the reforms have strongly affected the\ndynamic evolution of SMP and that the best found model captures adequately the\nstylized facts of the series that other electricity and financial markets\nshare. The dynamics of the conditional volatility generated by the model can be\nextremely useful in the efforts that are under way towards market restructuring\nso the Greek market to be more compatible with the requirements of the European\nTarget Model.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.5452v1"
    },
    {
        "title": "Modeling Credit Spreads Using Nonlinear Regression",
        "authors": [
            "Radoslava Mirkov",
            "Thomas Maul",
            "Ronald Hochreiter",
            "Holger Thomae"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  The term structure of credit spreads is studied with an aim to predict its\nfuture movements. A completely new approach to tackle this problem is\npresented, which utilizes nonlinear parametric models. The Brain-Cousens\nregression model with five parameters is chosen to describe the term structure\nof credit spreads. Further, we investigate the dependence of the parameter\nchanges over time and the determinants of credit spreads.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.6955v1"
    },
    {
        "title": "Self-affinity in financial asset returns",
        "authors": [
            "John Goddard",
            "Enrico Onali"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We test for departures from normal and independent and identically\ndistributed (NIID) returns, when returns under the alternative hypothesis are\nself-affine. Self-affine returns are either fractionally integrated and\nlong-range dependent, or drawn randomly from an L-stable distribution with\ninfinite higher-order moments. The finite sample performance of estimators of\nthe two forms of self-affinity is explored in a simulation study which\ndemonstrates that, unlike rescaled range analysis and other conventional\nestimation methods, the variant of fluctuation analysis that considers finite\nsample moments only is able to identify either form of self-affinity. However,\nwhen returns are self-affine and long-range dependent under the alternative\nhypothesis, rescaled range analysis has greater power than fluctuation\nanalysis. The finite-sample properties of the estimators when returns exhibit\neither form of self-affinity can be exploited to determine the source of\nself-affinity in empirical returns data. The techniques are illustrated by\nmeans of an analysis of the fractal properties of the daily logarithmic returns\nfor the indices of 11 stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.7170v1"
    },
    {
        "title": "Cross-correlation asymmetries and causal relationships between stock and\n  market risk",
        "authors": [
            "Stanislav S. Borysov",
            "Alexander V. Balatsky"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We study historical correlations and lead-lag relationships between\nindividual stock risk (volatility of daily stock returns) and market risk\n(volatility of daily returns of a market-representative portfolio) in the US\nstock market. We consider the cross-correlation functions averaged over all\nstocks, using 71 stock prices from the Standard \\& Poor's 500 index for\n1994--2013. We focus on the behavior of the cross-correlations at the times of\nfinancial crises with significant jumps of market volatility. The observed\nhistorical dynamics showed that the dependence between the risks was almost\nlinear during the US stock market downturn of 2002 and after the US housing\nbubble in 2007, remaining on that level until 2013. Moreover, the averaged\ncross-correlation function often had an asymmetric shape with respect to zero\nlag in the periods of high correlation. We develop the analysis by the\napplication of the linear response formalism to study underlying causal\nrelations. The calculated response functions suggest the presence of\ncharacteristic regimes near financial crashes, when the volatility of an\nindividual stock follows the market volatility and vice versa.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.8106v3"
    },
    {
        "title": "How does bad and good volatility spill over across petroleum markets?",
        "authors": [
            "Jozef Barunik",
            "Evzen Kocenda",
            "Lukas Vacha"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We detect and quantify asymmetries in volatility spillovers using the\nrealized semivariances of petroleum commodities: crude oil, gasoline, and\nheating oil. During the 1987--2014 period we document increasing spillovers\nfrom volatility among petroleum commodities that substantially change after the\n2008 financial crisis. The increase in volatility spillovers correlates with\nthe progressive financialization of the commodities. In terms of asymmetries in\nspillovers we show that periods of increasing crude oil prices strongly\ncorrelate with dominating spillovers due to bad volatility. Overall, bad\nvolatility due to negative returns spills over among petroleum commodities to a\nmuch larger extent than good volatility due to positive returns. After the 2008\nfinancial crisis the asymmetries in spillovers markedly declined in terms of\ntotal as well as directional spillovers. An analysis of directional spillovers\nfurther reveals that no commodity dominates other commodities in terms of\nspillover transmission in general.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.2445v1"
    },
    {
        "title": "Transport catastrophe analysis as an alternative to a fractal\n  description: theory and application to financial crisis time series",
        "authors": [
            "Sergey A. Kamenshchikov"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  The goal of this investigation was to overcome limitations of a persistency\nanalysis, introduced by Benoit Mandelbrot for fractal Brownian processes:\nnondifferentiability, Brownian nature of process and a linear memory measure.\nWe have extended a sense of a Hurst factor by consideration of a phase\ndiffusion power law. It was shown that pre-catastrophic stabilization as an\nindicator of bifurcation leads to a new minimum of momentary phase diffusion,\nwhile bifurcation causes an increase of the momentary transport. Basic\nconclusions of a diffusive analysis have been compared to the Lyapunov\nstability model. An extended Reynolds parameter has been introduces as an\nindicator of phase transition. A combination of diffusive and Reynolds analysis\nhas been applied for a description of a time series of Dow Jones Industrial\nweekly prices for a world financial crisis of 2007-2009. Diffusive and Reynolds\nparameters shown an extreme values in October 2008 when a mortgage crisis was\nfixed. A combined R/D description allowed distinguishing of short-memory and\nlong memory shifts of a market evolution. It was stated that a systematic large\nscale failure of a financial system has begun in October 2008 and started\nfading in February 2009.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.6990v2"
    },
    {
        "title": "Mixed Tempered Stable distribution",
        "authors": [
            "Edit Rroji",
            "Lorenzo Mercuri"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In this paper we introduce a new parametric distribution, the Mixed Tempered\nStable. It has the same structure of the Normal Variance Mean Mixtures but the\nnormality assumption leaves place to a semi-heavy tailed distribution. We show\nthat, by choosing appropriately the parameters of the distribution and under\nthe concrete specification of the mixing random variable, it is possible to\nobtain some well-known distributions as special cases.\n  We employ the Mixed Tempered Stable distribution which has many attractive\nfeatures for modeling univariate returns. Our results suggest that it is enough\nflexible to accomodate different density shapes. Furthermore, the analysis\napplied to statistical time series shows that our approach provides a better\nfit than competing distributions that are common in the practice of finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7603v1"
    },
    {
        "title": "Rockets and feathers meet Joseph: Reinvestigating the oil-gasoline\n  asymmetry on the international markets",
        "authors": [
            "Ladislav Kristoufek",
            "Petra Lunackova"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We reinvestigate the \"rockets and feathers\" effect between retail gasoline\nand crude oil prices in a new framework of fractional integration, long-term\nmemory and borderline (non-)stationarity. The most frequently used\nerror-correction model is examined in detail and we find that the prices return\nto their equilibrium value much more slowly than would be typical for the\nerror-correction model. Such dynamics is usually referred to as \"the Joseph\neffect\". The standard procedure is shown to be troublesome and we introduce\nthree new tests to investigate possible asymmetry in the price adjustment to\nequilibrium under these complicated time series characteristics. On the dataset\nof seven national gasoline prices, we report that apart from Belgium, there is\nno asymmetry found. The proposed methodology is not limited to the gasoline and\ncrude oil case but it can be utilized for any asymmetric adjustment to\nequilibrium analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.5466v1"
    },
    {
        "title": "Stochastic simulation framework for the Limit Order Book using liquidity\n  motivated agents",
        "authors": [
            "Efstathios Panayi",
            "Gareth Peters"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  In this paper we develop a new form of agent-based model for limit order\nbooks based on heterogeneous trading agents, whose motivations are liquidity\ndriven. These agents are abstractions of real market participants, expressed in\na stochastic model framework. We develop an efficient way to perform\nstatistical calibration of the model parameters on Level 2 limit order book\ndata from Chi-X, based on a combination of indirect inference and\nmulti-objective optimisation. We then demonstrate how such an agent-based\nmodelling framework can be of use in testing exchange regulations, as well as\ninforming brokerage decisions and other trading based scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.02447v3"
    },
    {
        "title": "Data manipulation detection via permutation information theory\n  quantifiers",
        "authors": [
            "Aurelio Fernandez Bariviera",
            "M. Belén Guercio",
            "Lisana B. Martinez"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Recent news cast doubts on London Interbank Offered Rate (LIBOR) integrity.\nGiven its economic importance and the delay with which authorities realize\nabout this situation, we aim to find an objective method in order to detect\ndepartures in the LIBOR rate that from the expected behavior. We analyze\nseveral interest rates time series and we detect an anomalous behavior in\nLIBOR, specially during the period of the financial crisis of 2008. Our\nfindings could be consistent with data manipulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.04123v1"
    },
    {
        "title": "Bin Size Independence in Intra-day Seasonalities for Relative Prices",
        "authors": [
            "Esteban Guevara Hidalgo"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  In this paper we perform a statistical analysis over the returns and relative\nprices of the CAC $40$ and the S\\&P $500$ with the purpose of analyzing the\nintra-day seasonalities of single and cross-sectional stock dynamics. In order\nto do that, we characterized the dynamics of a stock (or a set of stocks) by\nthe evolution of the moments of its returns (and relative prices) during a\ntypical day. We show that these intra-day seasonalities are independent of the\nsize of the bin, and the index we consider, (but characteristic for each index)\nfor the case of the relative prices but not for the case of the returns.\nFinally, we suggest how this bin size independence could be used to\ncharacterize \"atypical days\" for indexes and \"anomalous behaviours\" in stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.05176v2"
    },
    {
        "title": "Time and Frequency Structure of Causal Correlation Network in China Bond\n  Market",
        "authors": [
            "Zhongxing Wang",
            "Yan Yan",
            "Xiaosong Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  There are more than eight hundred interest rates published in China bond\nmarket every day. Which are the benchmark interest rates that have broad\ninfluences on most interest rates is a major concern for economists. In this\npaper, multi-variable Granger causality test is developed and applied to\nconstruct a directed network of interest rates, whose important nodes, regarded\nas key interest rates, are evaluated with inverse Page Rank scores. The results\nindicate that some short-term interest rates have larger influences on the most\nkey interest rates, while repo rates are the benchmark of short-term rates. It\nis also found that central bank bills'rates are in the core position of\nmid-term interest rates'network, and treasury bond rates are leading the\nlong-term bonds rates. The evolution of benchmark interest rates is also\nstudied from 2008 to 2014, and it's found that SHIBOR has generally become the\nbenchmark interest rate in China. In the frequency domain we detect the\nproperties of information flows between interest rates and the result confirms\nthe existence of market segmentation in China bond market.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00263v1"
    },
    {
        "title": "No Stable Distributions in Finance, please!",
        "authors": [
            "Lev B Klebanov"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Failure of the main argument for the use of heavy tailed distribution in\nFinance is given. More precisely, one cannot observe so many outliers for\nCauchy or for symmetric stable distributions as we have in reality.\nkeywords:outliers; financial indexes; heavy tails; Cauchy distribution; stable\ndistributions\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00566v2"
    },
    {
        "title": "Critical value of the total debt in view of the debts durations",
        "authors": [
            "I. A. Molotkov",
            "N. A. Ryabova"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Parastatistic distribution of a total debt owed to a large number of\ncreditors considered in relation to the duration of these debts. The process of\ndebt calculation depends on the fractal dimension of economic system in which\nthis process takes place. Two actual variants of these dimensions are\ninvestigated. Critical values for these variants are determined. These critical\nvalues represent the levels after that borrower bankruptcy occurs. The\ncalculation of the critical value is performed by two independent methods: as\nthe point where the entropy of the system reaches its maximum value, and as the\npoint where the chemical potential is zero, which corresponds to the\ntermination of payments on the debt. Both methods lead to the same critical\nvalue. When the velocity of money circulation decrease, it is found for what\ndimensions critical debt value is increased and for what it is decreased in the\ncase when the velocity of money circulation is increased.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.07900v2"
    },
    {
        "title": "Regularities and Discrepancies of Credit Default Swaps: a Data Science\n  approach through Benford's Law",
        "authors": [
            "Marcel Ausloos",
            "Rosella Castellano",
            "Roy Cerqueti"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  In this paper, we search whether the Benford's law is applicable to monitor\ndaily changes in sovereign Credit Default Swaps (CDS) quotes, which are\nacknowledged to be complex systems of economic content. This test is of\nparamount importance since the CDS of a country proxy its health and\nprobability to default, being associated to an insurance against the event of\nits default. We fit the Benford's law to the daily changes in sovereign CDS\nspreads for 13 European countries, - both inside and outside the European Union\nand European Monetary Union. Two different tenors for the sovereign CDS\ncontracts are considered: 5 yrs and 10 yrs, - the former being the reference\nand most liquid one. The time period under investigation is 2008-2015 which\nincludes the period of distress caused by the European sovereign debt crisis.\nMoreover, (i) an analysis over relevant sub-periods is carried out, (ii)\nseveral insights are provided also by implementing the tracking of the\nBenford's law over moving windows. The main test for checking the conformance\nto Benford's law is - as usual - the $\\chi^{2}$ test, whose values are\npresented and discussed for all cases. The analysis is further completed by\nelaborations based on Chebyshev's distance and Kullback and Leibler's\ndivergence. The results highlight differences by countries and tenors. In\nparticular, these results suggest that liquidity seems to be associated to\nhigher levels of distortion. Greece - representing a peculiar case - shows a\nvery different path with respect to the other European countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01103v1"
    },
    {
        "title": "Forecasting time series with structural breaks with Singular Spectrum\n  Analysis, using a general form of recurrent formula",
        "authors": [
            "Donya Rahmani",
            "Saeed Heravi",
            "Hossein Hassani",
            "Mansi Ghodsi"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  This study extends and evaluates the forecasting performance of the Singular\nSpectrum Analysis (SSA) technique using a general non-linear form for the re-\ncurrent formula. In this study, we consider 24 series measuring the monthly\nseasonally adjusted industrial production of important sectors of the German,\nFrench and UK economies. This is tested by comparing the performance of the new\nproposed model with basic SSA and the SSA bootstrap forecasting, especially\nwhen there is evidence of structural breaks in both in-sample and out-of-sample\nperiods. According to root mean-square error (RMSE), SSA using the general\nrecursive formula outperforms both the SSA and the bootstrap forecasting at\nhorizons of up to a year. We found no significant difference in predicting the\ndirection of change between these methods. Therefore, it is suggested that the\nSSA model with the general recurrent formula should be chosen by users in the\ncase of structural breaks in the series.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.02188v1"
    },
    {
        "title": "Survey on log-normally distributed market-technical trend data",
        "authors": [
            "René Kempen",
            "Stanislaus Maier-Paape"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  In this survey, a short introduction in the recent discovery of log-normally\ndistributed market-technical trend data will be given. The results of the\nstatistical evaluation of typical market-technical trend variables will be\npresented. It will be shown that the log-normal assumption fits better to\nempirical trend data than to daily returns of stock prices. This enables to\nmathematically evaluate trading systems depending on such variables. In this\nmanner, a basic approach to an anti cyclic trading system will be given as an\nexample.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.03559v1"
    },
    {
        "title": "Discrete Wavelet Transform-Based Prediction of Stock Index: A Study on\n  National Stock Exchange Fifty Index",
        "authors": [
            "Dhanya Jothimani",
            "Ravi Shankar",
            "Surendra S. Yadav"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Financial Times Series such as stock price and exchange rates are, often,\nnon-linear and non-stationary. Use of decomposition models has been found to\nimprove the accuracy of predictive models. The paper proposes a hybrid approach\nintegrating the advantages of both decomposition model (namely, Maximal Overlap\nDiscrete Wavelet Transform (MODWT)) and machine learning models (ANN and SVR)\nto predict the National Stock Exchange Fifty Index. In first phase, the data is\ndecomposed into a smaller number of subseries using MODWT. In next phase, each\nsubseries is predicted using machine learning models (i.e., ANN and SVR). The\npredicted subseries are aggregated to obtain the final forecasts. In final\nstage, the effectiveness of the proposed approach is evaluated using error\nmeasures and statistical test. The proposed methods (MODWT-ANN and MODWT-SVR)\nare compared with ANN and SVR models and, it was observed that the return on\ninvestment obtained based on trading rules using predicted values of MODWT-SVR\nmodel was higher than that of Buy-and-hold strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.07278v1"
    },
    {
        "title": "Fluctuation analysis of electric power loads in Europe: Correlation\n  multifractality vs. Distribution function multifractality",
        "authors": [
            "Hynek Lavicka",
            "Jiri Kracik"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We analyze the time series of the power loads of the 35 separated countries\npublicly sharing hourly data through ENTSO-E platform for more than 5 years. We\napply the Multifractal Detrended Fluctuation Analysis for the demonstration of\nthe multifractal nature, autocorrelation and the distribution function\nfundamentals. Additionally, we improved the basic method described by\nKanterhardt, et al using uniform shuffling and surrogate the datasets to prove\nthe robustness of the results with respect to the non-linear effects of the\nprocesses. All the datasets exhibit multifractality in the distribution\nfunction as well as in the autocorrelation function. The basic differences\nbetween individual states are manifested in the width of the multifractal\nspectra and in the location of the maximum. We present the hypothesis about the\nproduction portfolio and the export/import dependences.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00467v1"
    },
    {
        "title": "Complex Correlation Approach for High Frequency Financial Data",
        "authors": [
            "Mateusz Wilinski",
            "Yuichi Ikeda",
            "Hideaki Aoyama"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We propose a novel approach that allows to calculate Hilbert transform based\ncomplex correlation for unevenly spaced data. This method is especially\nsuitable for high frequency trading data, which are of a particular interest in\nfinance. Its most important feature is the ability to take into account\nlead-lag relations on different scales, without knowing them in advance. We\nalso present results obtained with this approach while working on Tokyo Stock\nExchange intraday quotations. We show that individual sectors and subsectors\ntend to form important market components which may follow each other with small\nbut significant delays. These components may be recognized by analysing\neigenvectors of complex correlation matrix for Nikkei 225 stocks.\nInterestingly, sectorial components are also found in eigenvectors\ncorresponding to the bulk eigenvalues, traditionally treated as noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.06355v2"
    },
    {
        "title": "An Investigation of the Structural Characteristics of the Indian IT\n  Sector and the Capital Goods Sector: An Application of the R Programming in\n  Time Series Decomposition and Forecasting",
        "authors": [
            "Jaydip Sen",
            "Tamal Datta Chaudhuri"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Time series analysis and forecasting of stock market prices has been a very\nactive area of research over the last two decades. Availability of extremely\nfast and parallel architecture of computing and sophisticated algorithms has\nmade it possible to extract, store, process and analyze high volume stock\nmarket time series data very efficiently. In this paper, we have used time\nseries data of the two sectors of the Indian economy: Information Technology\nand Capital Goods for the period January 2009 till April 2016 and have studied\nthe relationships of these two time series with the time series of DJIA index,\nNIFTY index and the US Dollar to Indian Rupee exchange rate. We establish by\ngraphical and statistical tests that while the IT sector of India has a strong\nassociation with DJIA index and the Dollar to Rupee exchange rate, the Indian\nCG sector exhibits a strong association with the NIFTY index. We contend that\nthese observations corroborate our hypotheses that the Indian IT sector is\nstrongly coupled with the world economy whereas the CG sector of India reflects\ninternal economic growth of India. We also present several models of regression\nbetween the time series which exhibit strong association among them. The\neffectiveness of these models have been demonstrated by very low values of\ntheir forecasting errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07821v1"
    },
    {
        "title": "Decomposition of Time Series Data to Check Consistency between Fund\n  Style and Actual Fund Composition of Mutual Funds",
        "authors": [
            "Jaydip Sen",
            "Tamal Datta Chaudhuri"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We propose a novel approach for analysis of the composition of an equity\nmutual fund based on the time series decomposition of the price movements of\nthe individual stocks of the fund. The proposed scheme can be applied to check\nwhether the style proclaimed for a mutual fund actually matches with the fund\ncomposition. We have applied our proposed framework on eight well known mutual\nfunds of varying styles in the Indian financial market to check the consistency\nbetween their fund style and actual fund composition, and have obtained\nextensive results from our experiments. A detailed analysis of the results has\nshown that while in majority of the cases the actual allocations of funds are\nconsistent with the corresponding fund styles, there have been some notable\ndeviations too.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.08361v1"
    },
    {
        "title": "Some Statistical Problems with High Dimensional Financial data",
        "authors": [
            "Arnab Chakrabarti",
            "Rituparna Sen"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  For high dimensional data, some of the standard statistical techniques do not\nwork well. So modification or further development of statistical methods are\nnecessary. In this paper, we explore these modifications. We start with the\nimportant problem of estimating high dimensional covariance matrix. Then we\nexplore some of the important statistical techniques such as high dimensional\nregression, principal component analysis, multiple testing problems and\nclassification. We describe some of the fast algorithms that can be readily\napplied in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02953v1"
    },
    {
        "title": "GARCH(1,1) model of the financial market with the Minkowski metric",
        "authors": [
            "Richard Pincak",
            "Kabin Kanjamapornkul"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We solved a stylized fact on a long memory process of volatility cluster\nphenomena by using Minkowski metric for GARCH(1,1) under assumption that price\nand time can not be separated. We provide a Yang-Mills equation in financial\nmarket and anomaly on superspace of time series data as a consequence of the\nproof from the general relativity theory. We used an original idea in Minkowski\nspacetime embedded in Kolmogorov space in time series data with behavior of\ntraders.The result of this work is equivalent to the dark volatility or the\nhidden risk fear field induced by the interaction of the behavior of the trader\nin the financial market panic when the market crashed.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.04231v1"
    },
    {
        "title": "Exploring how innovation strategies at time of crisis influence\n  performance: a cluster analysis perspective",
        "authors": [
            "Marcel Ausloos",
            "Francesca Bartolacci",
            "Nicola G. Castellano",
            "Roy Cerqueti"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  This paper analyzes the connection between innovation activities of companies\n-- implemented before crisis -- and their performance -- measured at time of\ncrisis. The companies listed in the STAR Market Segment of the Italian Stock\nExchange are analyzed. Innovation is measured through the level of investments\nin total tangible and intangible fixed assets in 2006-2007, while performance\nis captured through growth -- expressed by variations of sales, total assets\nand employees -- profitability -- through ROI or ROS -- and productivity --\nthrough asset turnover or sales per employee in the period 2008-2010. The\nvariables of interest are analyzed and compared through statistical techniques\nand by adopting cluster analysis. In particular, a Voronoi tessellation is also\nimplemented in a varying centroids framework. In accord with a large part of\nthe literature, we find that the behavior of the performance of the companies\nis not univocal when they innovate.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.05893v1"
    },
    {
        "title": "Emergence of Turbulent Epochs in Oil Prices",
        "authors": [
            "Josselin Garnier",
            "Knut Solna"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Oil price data have a complicated multi-scale structure that may vary with\ntime. We use time-frequency analysis to identify the main features of these\nvariations and, in particular, the regime shifts. The analysis is based on a\nwavelet-based decomposition and analysis of the associated scale spectrum. The\njoint estimation of the local Hurst exponent and volatility is the key to\ndetect and identify regime shifting and switching of the oil price. The\nframework involves in particular modeling in terms of a process of\n`multi-fractional' type so that both the roughness and the volatility of the\nprice process may vary with time. Special epochs then emerge as a result of\nthese degrees of freedom, moreover, as a result of the special type of spectral\nestimator used. These special epochs are discussed and related to historical\nevents. Some of them are not detected by standard analysis based on maximum\nlikelihood estimation. The paper presents a novel algorithm for robust\ndetection of such special epochs and multi-fractional behavior in financial or\nother types of data. In the financial context insight about such behavior of\nthe asset price is important to evaluate financial contracts involving the\nasset.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09382v2"
    },
    {
        "title": "Analytic Moments for GARCH Processes",
        "authors": [
            "Carol Alexander",
            "Emese Lazar",
            "Silvia Stanescu"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  For a GJR-GARCH specification with a generic innovation distribution we\nderive analytic expressions for the first four conditional moments of the\nforward and aggregated returns and variances. Moment for the most commonly used\nGARCH models are stated as special cases. We also the limits of these moments\nas the time horizon increases, establishing regularity conditions for the\nmoments of aggregated returns to converge to normal moments. Our empirical\nstudy yields excellent approximate predictive distributions from these analytic\nmoments, thus precluding the need for time-consuming simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.09666v2"
    },
    {
        "title": "Multiplicative random cascades with additional stochastic process in\n  financial markets",
        "authors": [
            "Jun-ichi Maskawa",
            "Koji Kuroda",
            "Joshin Murai"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Multiplicative random cascade model naturally reproduces the intermittency or\nmultifractality, which is frequently shown among hierarchical complex systems\nsuch as turbulence and financial markets. As described herein, we investigate\nthe validity of a multiplicative hierarchical random cascade model through an\nempirical study using financial data. Although the intermittency and\nmultifractality of the time series are verified, random multiplicative factors\nlinking successive hierarchical layers show strongly negative correlation. We\nextend the multiplicative model to incorporate an additional stochastic term.\nResults show that the proposed model is consistent with all the empirical\nresults presented here.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.00820v1"
    },
    {
        "title": "The Zumbach effect under rough Heston",
        "authors": [
            "Omar El Euch",
            "Jim Gatheral",
            "Radoš Radoičić",
            "Mathieu Rosenbaum"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Previous literature has identified an effect, dubbed the Zumbach effect, that\nis nonzero empirically but conjectured to be zero in any conventional\nstochastic volatility model. Essentially this effect corresponds to the\nproperty that past squared returns forecast future volatilities better than\npast volatilities forecast future squared returns. We provide explicit\ncomputations of the Zumbach effect under rough Heston and show that they are\nconsistent with empirical estimates. In agreement with previous conjectures\nhowever, the Zumbach effect is found to be negligible in the classical Heston\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.02098v1"
    },
    {
        "title": "The new face of multifractality: Multi-branchedness and the phase\n  transitions in time series of mean inter-event times",
        "authors": [
            "Jarosław Klamut",
            "Ryszard Kutner",
            "Tomasz Gubiec",
            "Zbigniew R. Struzik"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Empirical time series of inter-event or waiting times are investigated using\na modified Multifractal Detrended Fluctuation Analysis operating on\nfluctuations of mean detrended dynamics. The core of the extended multifractal\nanalysis is the non-monotonic behavior of the generalized Hurst exponent $h(q)$\n-- the fundamental exponent in the study of multifractals. The consequence of\nthis behavior is the non-monotonic behavior of the coarse H\\\"older exponent\n$\\alpha (q)$ leading to multi-branchedness of the spectrum of dimensions. The\nLegendre-Fenchel transform is used instead of the routinely used canonical\nLegendre (single-branched) contact transform. Thermodynamic consequences of the\nmulti-branched multifractality are revealed. These are directly expressed in\nthe language of phase transitions between thermally stable, metastable, and\nunstable phases. These phase transitions are of the first and second orders\naccording to Mandelbrot's modified Ehrenfest classification. The discovery of\nmulti-branchedness is tantamount in significance to extending multifractal\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.02674v4"
    },
    {
        "title": "Modeling Nelson-Siegel Yield Curve using Bayesian Approach",
        "authors": [
            "Sourish Das"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Yield curve modeling is an essential problem in finance. In this work, we\nexplore the use of Bayesian statistical methods in conjunction with\nNelson-Siegel model. We present the hierarchical Bayesian model for the\nparameters of the Nelson-Siegel yield function. We implement the MAP estimates\nvia BFGS algorithm in rstan. The Bayesian analysis relies on the Monte Carlo\nsimulation method. We perform the Hamiltonian Monte Carlo (HMC), using the\nrstan package. As a by-product of the HMC, we can simulate the Monte Carlo\nprice of a Bond, and it helps us to identify if the bond is over-valued or\nunder-valued. We demonstrate the process with an experiment and US Treasury's\nyield curve data. One of the interesting observation of the experiment is that\nthere is a strong negative correlation between the price and long-term effect\nof yield. However, the relationship between the short-term interest rate effect\nand the value of the bond is weakly positive. This is because posterior\nanalysis shows that the short-term effect and the long-term effect are\nnegatively correlated.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.06077v2"
    },
    {
        "title": "Chaos and Order in the Bitcoin Market",
        "authors": [
            "Josselin Garnier",
            "Knut Solna"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  The bitcoin price has surged in recent years and it has also exhibited phases\nof rapid decay. In this paper we address the question to what extent this novel\ncryptocurrency market can be viewed as a classic or semi-efficient market.\nNovel and robust tools for estimation of multi-fractal properties are used to\nshow that the bitcoin price exhibits a very interesting multi-scale correlation\nstructure. This structure can be described by a power-law behavior of the\nvariances of the returns as functions of time increments and it can be\ncharacterized by two parameters, the volatility and the Hurst exponent. These\npower-law parameters, however, vary in time. A new notion of generalized Hurst\nexponent is introduced which allows us to check if the multi-fractal character\nof the underlying signal is well captured. It is moreover shown how the\nmonitoring of the power-law parameters can be used to identify regime shifts\nfor the bitcoin price. A novel technique for identifying the regimes switches\nbased on a goodness of fit of the local power-law parameters is presented. It\nautomatically detects dates associated with some known events in the bitcoin\nmarket place. A very surprising result is moreover that, despite the wild ride\nof the bitcoin price in recent years and its multi-fractal and non-stationary\ncharacter, this price has both local power-law behaviors and a very orderly\ncorrelation structure when it is observed on its entire period of existence.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08403v2"
    },
    {
        "title": "Tail probabilities for short-term returns on stocks",
        "authors": [
            "Henrik O. Rasmussen",
            "Paul Wilmott"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We consider the tail probabilities of stock returns for a general class of\nstochastic volatility models. In these models, the stochastic differential\nequation for volatility is autonomous, time-homogeneous and dependent on only a\nfinite number of dimensional parameters. Three bounds on the high-volatility\nlimits of the drift and diffusion coefficients of volatility ensure that\nvolatility is mean-reverting, has long memory and is as volatile as the stock\nprice. Dimensional analysis then provides leading-order approximations to the\ndrift and diffusion coefficients of volatility for the high-volatility limit.\nThereby, using the Kolmogorov forward equation for the transition probability\nof volatility, we find that the tail probability for short-term returns falls\noff like an inverse cubic. Our analysis then provides a possible explanation\nfor the inverse cubic fall off that Gopikrishnan et al. (1998) report for\nreturns over 5-120 minutes intervals. We find, moreover, that the tail\nprobability scales like the length of the interval, over which the return is\nmeasured, to the power 3/2. There do not seem to be any empirical results in\nthe literature with which to compare this last prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08416v2"
    },
    {
        "title": "An Empirical Study of the Behaviour of the Sample Kurtosis in Samples\n  from Symmetric Stable Distributions",
        "authors": [
            "J. Martin van Zyl"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Kurtosis is seen as a measure of the discrepancy between the observed data\nand a Gaussian distribution and is defined when the 4th moment is finite. In\nthis work an empirical study is conducted to investigate the behaviour of the\nsample estimate of kurtosis with respect to sample size and the tail index when\napplied to heavy-tailed data where the 4th moment does not exist. The study\nwill focus on samples from the symmetric stable distributions. It was found\nthat the expected value of excess kurtosis divided by the sample size is finite\nfor any value of the tail index and the sample estimate of kurtosis increases\nas a linear function of sample size and tail index. It is very sensitive to\nchanges in the tail-index.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.00476v2"
    },
    {
        "title": "Reframing the S\\&P500 Network of Stocks along the \\nth{21} Century",
        "authors": [
            "Tanya Araújo",
            "Maximilian Göbel"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Since the beginning of the new millennium, stock markets went through every\nstate from long-time troughs, trade suspensions to all-time highs. The\nliterature on asset pricing hence assumes random processes to be underlying the\nmovement of stock returns. Observed procyclicality and time-varying correlation\nof stock returns tried to give the apparently random behavior some sort of\nstructure. However, common misperceptions about the co-movement of asset prices\nin the years preceding the \\emph{Great Recession} and the \\emph{Global\nCommodity Crisis}, is said to have even fueled the crisis' economic impact.\nHere we show how a varying macroeconomic environment influences stocks'\nclustering into communities. From a sample of 296 stocks of the S\\&P 500 index,\ndistinct periods in between 2004 and 2011 are used to develop networks of\nstocks. The Minimal Spanning Tree analysis of those time-varying networks of\nstocks demonstrates that the crises of 2007-2008 and 2010-2011 drove the market\nto clustered community structures in both periods, helping to restore the stock\nmarket's ceased order of the pre-crises era. However, a comparison of the\nemergent clusters with the \\textit{General Industry Classification Standard}\nconveys the impression that industry sectors do not play a major role in that\norder.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03092v2"
    },
    {
        "title": "New fat-tail normality test based on conditional second moments with\n  applications to finance",
        "authors": [
            "Damian Jelito",
            "Marcin Pitera"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In this paper we introduce an efficient fat-tail measurement framework that\nis based on the conditional second moments. We construct a goodness-of-fit\nstatistic that has a direct interpretation and can be used to assess the impact\nof fat-tails on central data conditional dispersion. Next, we show how to use\nthis framework to construct a powerful normality test. In particular, we\ncompare our methodology to various popular normality tests, including the\nJarque--Bera test that is based on third and fourth moments, and show that in\nmany cases our framework outperforms all others, both on simulated and market\nstock data. Finally, we derive asymptotic distributions for conditional mean\nand variance estimators, and use this to show asymptotic normality of the\nproposed test statistic.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05464v4"
    },
    {
        "title": "Portfolio Theory, Information Theory and Tsallis Statistics",
        "authors": [
            "Marco A. S. Trindade",
            "Sergio Floquet",
            "Lourival M. S. Filho"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We developed a strategic of optimal portfolio based on information theory and\nTsallis statistics. The growth rate of a stock market is defined by using\n$q$-deformed functions and we find that the wealth after n days with the\noptimal portfolio is given by a $q$-exponential function. In this context, the\nasymptotic optimality is investigated on causal portfolios, showing advantages\nof the optimal portfolio over an arbitrary choice of causal portfolios.\nFinally, we apply the formulation in a small number of stocks in brazilian\nstock market $[B]^{3}$ and analyzed the results.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.07237v2"
    },
    {
        "title": "Estimation of Ornstein-Uhlenbeck Process Using Ultra-High-Frequency Data\n  with Application to Intraday Pairs Trading Strategy",
        "authors": [
            "Vladimír Holý",
            "Petra Tomanová"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  When stock prices are observed at high frequencies, more information can be\nutilized in estimation of parameters of the price process. However,\nhigh-frequency data are contaminated by the market microstructure noise which\ncauses significant bias in parameter estimation when not taken into account. We\npropose an estimator of the Ornstein-Uhlenbeck process based on the maximum\nlikelihood which is robust to the noise and utilizes irregularly spaced data.\nWe also show that the Ornstein-Uhlenbeck process contaminated by the\nindependent Gaussian white noise and observed at discrete equidistant times\nfollows an ARMA(1,1) process. To illustrate benefits of the proposed\nnoise-robust approach, we introduce a novel intraday pairs trading strategy\nbased on the mean-variance optimization. In an empirical study of 7 Big Oil\ncompanies, we show that the use of the proposed estimator of the\nOrnstein-Uhlenbeck process leads to an increase in profitability of the pairs\ntrading strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.09312v3"
    },
    {
        "title": "Empirical forward price distribution from Bitcoin option prices",
        "authors": [
            "Nikolai Zaitsev"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Report presents analysis of empirical distribution of future returns of\nbitcoin (BTC) from BTUSD inverse option prices. Logistic pdf is chosen as\nunderlying distribution to fit option prices. The result is satisfactory and\nsuggests that these prices can be described with just three or even one\nparameter. Fitted Logistic pdf matches forward price movements upto a scaling\nfactor. Nevertheless, this observation stands alone and does not allow\nstochastic description of underlying prices with logistic pdf in similar\nfashion as it is done within Black-Scholes modelling framework. Put-call parity\nrelationship is derived connecting prices of vanilla inverse options and\nfutures.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04770v1"
    },
    {
        "title": "Financial Portfolios based on Tsallis Relative Entropy as the Risk\n  Measure",
        "authors": [
            "Sandhya Devi"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Earlier studies have shown that stock market distributions can be well\ndescribed by distributions derived from Tsallis entropy, which is a\ngeneralization of Shannon entropy to non-extensive systems. In this paper,\nTsallis relative entropy (TRE), which is the generalization of Kullback-Leibler\nrelative entropy (KLRE) to non-extensive systems, is investigated as a possible\nrisk measure in constructing risk optimal portfolios whose returns beat market\nreturns. Portfolios are constructed by binning the risk values and allocating\nthe stocks to bins according to their risk values. The average return in excess\nof market returns for each bin is calculated to get the risk-return patterns of\nthe portfolios. The results are compared with those from three other risk\nmeasures: 1) the commonly used 'beta' of the Capital Asset Pricing Model\n(CAPM), 2) Kullback-Leibler relative entropy, and 3) the relative standard\ndeviation. Tests carried out for both long (~18 years) and shorter terms (~9\nyears), which include the dot-com bubble and the 2008 crash periods, show that\na linear fit can be obtained for the risk-excess return profiles of all four\nrisk measures. However, in all cases, the profiles from Tsallis relative\nentropy show a more consistent behavior in terms of both goodness of fit and\nthe variation of returns with risk, than the other three risk measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04945v3"
    },
    {
        "title": "Nonextensive triplets in stock market indices",
        "authors": [
            "Dusan Stosic",
            "Darko Stosic",
            "Tatijana Stosic"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Stock market indices are one of the most investigated complex systems in\neconophysics. Here we extend the existing literature on stock markets in\nconnection with nonextensive statistical mechanics. We explore the\nnonextensivity of price volatilities for 34 major stock market indices between\n2010 and 2019. We discover that stock markets follow nonextensive statistics\nregarding equilibrium, relaxation and sensitivity. We find nonextensive\nbehavior in stock markets for developed countries, but not for developing\ncountries. Distances between nonextensive triplets suggest that some stock\nmarkets might share similar nonextensive dynamics, while others are widely\ndifferent. The current findings strongly indicate that the stock market\nrepresents a system whose physics is properly described by nonextensive\nstatistical mechanics. Our results shed light on the complex nature of stock\nmarket indices, and establish another formal link with the nonextensive theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.07721v1"
    },
    {
        "title": "A Study on Neural Network Architecture Applied to the Prediction of\n  Brazilian Stock Returns",
        "authors": [
            "Leonardo Felizardo",
            "Afonso Pinto"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this paper we present a statistical analysis about the characteristics\nthat we intend to influence in the performance of the neural networks in terms\nof assertiveness in the prediction of Brazilian stock returns. We created a\npopulation of architectures for analysis and extracted the sample that had the\nbest assertive performance. It was verified how the characteristics of this\nsample stand out and affect the neural networks. In addition, we make\ninferences about what kind of influence the different architectures have on the\nperformance of neural networks. In the study, the prediction of the return of a\nBrazilian stock traded on the stock exchange of S\\~ao Paulo to measure the\nerror committed by the different architectures of constructed neural networks.\nThe results are promising and indicate that some aspects of the neural network\narchitecture have a significant impact on the assertiveness of the model.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.09143v1"
    },
    {
        "title": "Detailed study of a moving average trading rule",
        "authors": [
            "Fernando F. Ferreira",
            "A. Christian Silva",
            "Ju-Yi Yen"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We present a detailed study of the performance of a trading rule that uses\nmoving average of past returns to predict future returns on stock indexes. Our\nmain goal is to link performance and the stochastic process of the traded\nasset. Our study reports short, medium and long term effects by looking at the\nSharpe ratio (SR). We calculate the Sharpe ratio of our trading rule as a\nfunction of the probability distribution function of the underlying traded\nasset and compare it with data. We show that if the performance is mainly due\nto presence of autocorrelation in the returns of the traded assets, the SR as a\nfunction of the portfolio formation period (look-back) is very different from\nperformance due to the drift (average return). The SR shows that for look-back\nperiods of a few months the investor is more likely to tap into\nautocorrelation. However, for look-back larger than few months, the drift of\nthe asset becomes progressively more important. Finally, our empirical work\nreports a new long-term effect, namely oscillation of the SR and propose a\nnon-stationary model to account for such oscillations.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.00212v1"
    },
    {
        "title": "Comparative analysis of layered structures in empirical investor\n  networks and cellphone communication networks",
        "authors": [
            "Peng Wang",
            "Jun-Chao Ma",
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Empirical investor networks (EIN) proposed by\n\\cite{Ozsoylev-Walden-Yavuz-Bildik-2014-RFS} are assumed to capture the\ninformation spreading path among investors. Here, we perform a comparative\nanalysis between the EIN and the cellphone communication networks (CN) to test\nwhether EIN is an information exchanging network from the perspective of the\nlayer structures of ego networks. We employ two clustering algorithms\n($k$-means algorithm and $H/T$ break algorithm) to detect the layer structures\nfor each node in both networks. We find that the nodes in both networks can be\nclustered into two groups, one that has a layer structure similar to the\ntheoretical Dunbar Circle corresponding to that the alters in ego networks\nexhibit a four-layer hierarchical structure with the cumulative number of 5,\n15, 50 and 150 from the inner layer to the outer layer, and the other one\nhaving an additional inner layer with about 2 alters compared with the Dunbar\nCircle. We also find that the scale ratios, which are estimated based on the\nunique parameters in the theoretical model of layer structures\n\\citep{Tamarit-Cuesta-Dunbar-Sanchez-2018-PNAS}, conform to a log-normal\ndistribution for both networks. Our results not only deepen our understanding\non the topological structures of EIN, but also provide empirical evidence of\nthe channels of information diffusion among investors.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.01119v1"
    },
    {
        "title": "Optimistic Bull or Pessimistic Bear: Adaptive Deep Reinforcement\n  Learning for Stock Portfolio Allocation",
        "authors": [
            "Xinyi Li",
            "Yinchuan Li",
            "Yuancheng Zhan",
            "Xiao-Yang Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Portfolio allocation is crucial for investment companies. However, getting\nthe best strategy in a complex and dynamic stock market is challenging. In this\npaper, we propose a novel Adaptive Deep Deterministic Reinforcement Learning\nscheme (Adaptive DDPG) for the portfolio allocation task, which incorporates\noptimistic or pessimistic deep reinforcement learning that is reflected in the\ninfluence from prediction errors. Dow Jones 30 component stocks are selected as\nour trading stocks and their daily prices are used as the training and testing\ndata. We train the Adaptive DDPG agent and obtain a trading strategy. The\nAdaptive DDPG's performance is compared with the vanilla DDPG, Dow Jones\nIndustrial Average index and the traditional min-variance and mean-variance\nportfolio allocation strategies. Adaptive DDPG outperforms the baselines in\nterms of the investment return and the Sharpe ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.01503v1"
    },
    {
        "title": "Identification of short-term and long-term time scales in stock markets\n  and effect of structural break",
        "authors": [
            "Ajit Mahata",
            "Debi Prasad Bal",
            "Md Nurujjaman"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The paper presents the comparative study of the nature of stock markets in\nshort-term and long-term time scales with and without structural break in the\nstock data. Structural break point has been identified by applying Zivot and\nAndrews structural trend break model to break the original time series (TSO)\ninto time series before structural break (TSB) and time series after structural\nbreak (TSA). The empirical mode decomposition based Hurst exponent and variance\ntechniques have been applied to the TSO, TSB and TSA to identify the time\nscales in short-term and long-term from the decomposed intrinsic mode\nfunctions. We found that for TSO, TSB and TSA the short-term time scales and\nlong-term time scales are within the range of few days to 3 months and greater\nthan 5 months respectively, which indicates that the short-term and long-term\ntime scales are present in the stock market. The Hurst exponent is $\\sim 0.5$\nand $\\geq 0.75$ for TSO, TSB and TSA in short-term and long-term respectively,\nwhich indicates that the market is random in short-term and strongly correlated\nin long-term. The identification of time scales at short-term and long-term\ninvestment horizon will be useful for investors to design investment and\ntrading strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.03009v1"
    },
    {
        "title": "A Stock Market Model Based on CAPM and Market Size",
        "authors": [
            "Andrey Sarantsev",
            "Blessing Ofori-Atta",
            "Brandon Flores"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We introduce a new system of stochastic differential equations which models\ndependence of market beta and unsystematic risk upon size, measured by market\ncapitalization. We fit our model using size deciles data from Kenneth French's\ndata library. This model is somewhat similar to generalized\nvolatility-stabilized models in (Pal, 2011; Pickova, 2013). The novelty of our\nwork is twofold. First, we take into account the difference between price and\ntotal returns (in other words, between market size and wealth processes).\nSecond, we work with actual market data. We study the long-term properties of\nthis system of equations, and reproduce observed linearity of the capital\ndistribution curve. Our model has two modifications: for price returns and for\nequity premium. Somewhat surprisingly, they exhibit the same fit, with very\nsimilar coefficients. In the Appendix, we analyze size-based real-world index\nfunds.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.08911v8"
    },
    {
        "title": "Stationarity of the detrended price return in stock markets",
        "authors": [
            "Karina Arias-Calluari",
            "Morteza. N. Najafi",
            "Michael S. Harré",
            "Fernando Alonso-Marroquin"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This paper proposes a governing equation for stock market indexes that\naccounts for non-stationary effects. This is a linear Fokker-Planck equation\n(FPE) that describes the time evolution of the probability distribution\nfunction (PDF) of the price return. By applying Ito's lemma, this FPE is\nassociated with a stochastic differential equation (SDE) that models the time\nevolution of the price return in a fashion different from the classical\nBlack-Scholes equation. Both FPE and SDE equations account for a deterministic\npart or trend, and a stationary, stochastic part as a q-Gaussian noise. The\nmodel is validated using the S\\&P500 index's data. After removing the trend\nfrom the index, we show that the detrended part is stationary by evaluating the\nHurst exponent of the multifractal time series, its power spectrum, and its\nautocorrelation.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01034v2"
    },
    {
        "title": "Homogeneity and heterogeneity of cryptocurrencies",
        "authors": [
            "Xiao Fan Liu",
            "Zeng-Xian Lin",
            "Xiao-Pu Han"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Thousands of cryptocurrencies have been issued and publicly exchanged since\nBitcoin was invented in 2008. The total cryptocurrency market value exceeds 300\nbillion US dollars as of 2019. This paper analyzes the prices, volumes,\nblockchain transactions, coin difficulties and public opinion popularities of\n3607 actively exchanged cryptocurrencies. We aim to reveal and explain the\nhomogeneity, i.e., the strong correlation of market performance, and the\nheterogeneity, i.e., the imbalance of popularities and sophistications, of the\ncryptocurrencies.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01330v2"
    },
    {
        "title": "On the quantum behavior and clustering properties of correlated\n  financial portfolios",
        "authors": [
            "Carlo Requião da Cunha",
            "Roberto da Silva"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We investigate 17 digital currencies making an analogy with quantum systems\nand develop the concept of eigenportfolios. We show that the density of states\nof the correlation matrix of these assets shows a behavior between that of the\nWishart ensemble and one whose elements are Cauchy distributed. A metric for\nthe participation matrix based on superposition of Gaussian functions is\nproposed and we show that small eigenvalues correspond to localized states.\nNonetheless, some level of localization is also present for bigger eigenvalues\nprobably caused by the fat tails of the distribution of returns of these\nassets. We also show through a clustering study that the digital currencies\ntend to stagger together. We conclude the paper showing that this correlation\nstructure leads to an Epps effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.08627v2"
    },
    {
        "title": "Order patterns, their variation and change points in financial time\n  series and Brownian motion",
        "authors": [
            "Christoph Bandt"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Order patterns and permutation entropy have become useful tools for studying\nbiomedical, geophysical or climate time series. Here we study day-to-day market\ndata, and Brownian motion which is a good model for their order patterns. A\ncrucial point is that for small lags (1 up to 6 days), pattern frequencies in\nfinancial data remain essentially constant. The two most important order\nparameters of a time series are turning rate and up-down balance. For change\npoints in EEG brain data, turning rate is excellent while for financial data,\nup-down balance seems the best. The fit of Brownian motion with respect to\nthese parameters is tested, providing a new version of a forgotten test by\nBienaym'e.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09978v1"
    },
    {
        "title": "Horse race of weekly idiosyncratic momentum strategies with respect to\n  various risk metrics: Evidence from the Chinese stock market",
        "authors": [
            "Huai-Long Shi",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This paper focuses on the horse race of weekly idiosyncratic momentum (IMOM)\nwith respect to various idiosyncratic risk metrics. Using the A-share\nindividual stocks in the Chinese market from January 1997 to December 2017, we\nfirst evaluate the performance of the weekly momentum based on raw returns and\nidiosyncratic returns, respectively. After that the univariate portfolio\nanalysis is conducted to investigate the return predictability with respect to\nvarious idiosyncratic risk metrics. Further, we perform a comparative study on\nthe performance of the IMOM portfolios with respect to various risk metrics. At\nlast, we explore the possible explanations to IMOM as well as risk based IMOM\nportfolios. We find that 1) there are prevailing contrarian effect and IMOM\neffect for the whole sample; 2) the negative relations exist between most of\nthe idiosyncratic risk metrics and the cross-sectional stock returns, and\nbetter performance is linked to idiosyncratic volatility (IVol) and maximum\ndrawdowns (IMDs); 3) additionally, the IVol-based and IMD-based IMOM portfolios\nexhibit better explanatory power to the IMOM portfolios with respect to other\nrisk metrics; 4) finally, higher profitability of IMOM as well as IVol-based\nand IMD-based IMOM portfolios is found to be related to upside market states,\nhigh levels of liquidity and high levels of investor sentiment.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.13115v2"
    },
    {
        "title": "A Self-Exciting Modelling Framework for Forward Prices in Power Markets",
        "authors": [
            "Giorgia Callegaro",
            "Andrea Mazzoran",
            "Carlo Sgarra"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We propose and investigate two model classes for forward power price\ndynamics, based on continuous branching processes with immigration, and on\nHawkes processes with exponential kernel, respectively. The models proposed\nexhibit jumps clustering features. Models of this kind have been already\nproposed for the spot price dynamics, but the main purpose of the present work\nis to investigate the performances of such models in describing the forward\ndynamics. We adopt a Heath-Jarrow-Morton approach in order to capture the whole\nforward curve evolution. By examining daily data in the French power market, we\nperform a goodness-of-fit test and we present our conclusions about the\nadequacy of these models in describing the forward prices evolution.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.13286v1"
    },
    {
        "title": "Multidimensional Analysis of Monthly Stock Market Returns",
        "authors": [
            "Osman Gulseven"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This study examines the monthly returns in Turkish and American stock market\nindices to investigate whether these markets experience abnormal returns during\nsome months of the calendar year. The data used in this research includes 212\nobservations between January 1996 and August 2014. I apply statistical summary\nanalysis, decomposition technique, dummy variable estimation, and binary\nlogistic regression to check for the monthly market anomalies. The\nmultidimensional methods used in this article suggest weak evidence against the\nefficient market hypothesis on monthly returns. While some months tend to show\nabnormal returns, there is no absolute unanimity in the applied approaches.\nNevertheless, there is a strikingly negative May effect on the Turkish stocks\nfollowing a positive return in April. Stocks tend to be bullish in December in\nboth markets, yet we do not observe anya significant January effect is not\nobserved.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05750v1"
    },
    {
        "title": "Turn-of-the Year Affect in Gold Prices: Decomposition Analysis",
        "authors": [
            "Osman Gulseven"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this article, we examine whether the gold market returns show abnormally\npositive or negative returns in some months of the calendar year. The\nstatistical analysis and the decomposition techniques suggest that gold prices\nshow some seasonal behavior during the turn of the year. We observe a strong\ncyclical behavior in gold markets during the turn-of-the-year period. January\nis likely to offer the highest return whereas significant negative returns are\nexpected in July.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11027v1"
    },
    {
        "title": "Coupled criticality analysis of inflation and unemployment",
        "authors": [
            "Z. Koohi Lai",
            "A. Namaki",
            "A. Hosseiny",
            "G. R. Jafari",
            "M. Ausloos"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper, we are interested to focus on the critical periods in the\neconomy which are characterized by large fluctuations in macroeconomic\nindicators.\n  To capture unusual and large fluctuations of inflation and unemployment, we\nconcentrate on the non-Gaussianity of their distributions.\n  To this aim, by using the coupled multifractal approach, we analyze US data\nfor a period of 70 years from 1948 until 2018 and measure the non-Gausianity of\nthe distributions. Then, we investigate how the non-Gaussianity of the\nvariables affects the coupling structure of them. By applying the multifractal\nmethod, one can see that the non-Gaussianity depends on the scales. While the\nnon-Gaussianity of unemployment is noticeable only for periods smaller than 1\nyear and for longer periods tends to Gaussian behavior, the non-Gaussianities\nof inflation persist for all time scales. Also, it is observed that the\ncoupling structure of these variables tends to a Gaussian behavior after $2$\nyears.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.12655v1"
    },
    {
        "title": "Autocorrelation of returns in major cryptocurrency markets",
        "authors": [
            "Eugene Tartakovsky",
            "Ksenia Plesovskikh",
            "Anastasiia Sarmakeeva",
            "Alexander Bibik"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This paper is the first of a series of short articles that explore the\nefficiency of major cryptocurrency markets. A number of statistical tests and\nproperties of statistical distributions will be used to assess if\ncryptocurrency markets are efficient, and how their efficiency changes over\ntime. In this paper, we analyze autocorrelation of returns in major\ncryptocurrency markets using the following methods: Pearson's autocorrelation\ncoefficient of different orders, Ljung-Box test, and first-order Pearson's\nautocorrelation coefficient in a rolling window. All experiments are conducted\non the BTC/USD, ETH/USD, ETH/BTC markets on Bitfinex exchange, and the XBT/USD\nmarket on Bitmex exchange, each on 5-minute, 1-hour, 1-day, and 1-week time\nframes. The results are represented visually on charts. Statistically\nsignificant autocorrelation is persistently present on the 5m and 1H time\nframes on all markets. The tests disagree on the 1D and 1W time frames. The\nresults of this article are fully reproducible. Used datasets, source code, and\na runnable Jupyter Notebook are available on GitHub.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13517v2"
    },
    {
        "title": "Time-varying volatility in Bitcoin market and information flow at\n  minute-level frequency",
        "authors": [
            "Irena Barjašić",
            "Nino Antulov-Fantulin"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper, we analyze the time-series of minute price returns on the\nBitcoin market through the statistical models of generalized autoregressive\nconditional heteroskedasticity (GARCH) family. Several mathematical models have\nbeen proposed in finance, to model the dynamics of price returns, each of them\nintroducing a different perspective on the problem, but none without\nshortcomings. We combine an approach that uses historical values of returns and\ntheir volatilities - GARCH family of models, with a so-called \"Mixture of\nDistribution Hypothesis\", which states that the dynamics of price returns are\ngoverned by the information flow about the market. Using time-series of\nBitcoin-related tweets and volume of transactions as external information, we\ntest for improvement in volatility prediction of several GARCH model variants\non a minute level Bitcoin price time series. Statistical tests show that the\nsimplest GARCH(1,1) reacts the best to the addition of external signal to model\nvolatility process on out-of-sample data.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.00550v2"
    },
    {
        "title": "Regression Approach for Modeling COVID-19 Spread and its Impact On Stock\n  Market",
        "authors": [
            "Bohdan M. Pavlyshenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The paper studies different regression approaches for modeling COVID-19\nspread and its impact on the stock market. The logistic curve model was used\nwith Bayesian regression for predictive analytics of the coronavirus spread.\nThe impact of COVID-19 was studied using regression approach and compared to\nother crises influence. In practical analytics, it is important to find the\nmaximum of coronavirus cases per day, this point means the estimated half time\nof coronavirus spread in the region under investigation. The obtained results\nshow that different crises with different reasons have different impact on the\nsame stocks. It is important to analyze their impact separately. Bayesian\ninference makes it possible to analyze the uncertainty of crisis impacts.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01489v1"
    },
    {
        "title": "Evolving efficiency and robustness of global oil trade networks",
        "authors": [
            "Wen-Jie Xie",
            "Na Wei",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  As a vital strategic resource, oil has an essential influence on the world\neconomy, diplomacy and military development. Using oil trade data to\ndynamically monitor and warn about international trade risks is an urgent need.\nBased on the UN Comtrade data from 1988 to 2017, we construct unweighted and\nweighted global oil trade networks (OTNs). Complex network theories have some\nadvantages in analyzing global oil trade as a system with numerous economies\nand complicated relationships. This paper establishes a trading-based network\nmodel for global oil trade to study the evolving efficiency, criticality and\nrobustness of economies and the relationships between oil trade partners. The\nresults show that for unweighted OTNs, the efficiency of oil flows gradually\nincreases with growing complexity of the OTNs, and the weighted efficiency\nindicators are more capable of highlighting the impact of major events on the\nOTNs. The identified critical economies and trade relationships have more\nimportant strategic significance in the real market. The simulated deliberate\nattacks corresponding to national bankruptcy, trade blockade, and economic\nsanctions have a more significant impact on the robustness than random attacks.\nWhen the economies are promoting high-quality economic development, and\ncontinuously enhancing positions in the OTN, more attention needs be paid to\nthe identified critical economies and trade relationships. To conclude, some\nsuggestions for application are given according to the results.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.05325v1"
    },
    {
        "title": "Information transfer between stock market sectors: A comparison between\n  the USA and China",
        "authors": [
            "Peng Yue",
            "Yaodong Fan",
            "Jonathan A. Batten",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Information diffusion within financial markets plays a crucial role in the\nprocess of price formation and the propagation of sentiment and risk. We\nperform a comparative analysis of information transfer between industry sectors\nof the Chinese and the USA stock markets, using daily sector indices for the\nperiod from 2000 to 2017. The information flow from one sector to another is\nmeasured by the transfer entropy of the daily returns of the two sector\nindices. We find that the most active sector in information exchange (i.e., the\nlargest total information inflow and outflow) is the {\\textit{non-bank\nfinancial}} sector in the Chinese market and the {\\textit{technology}} sector\nin the USA market. This is consistent with the role of the non-bank sector in\ncorporate financing in China and the impact of technological innovation in the\nUSA. In each market, the most active sector is also the largest information\nsink that has the largest information inflow (i.e., inflow minus outflow). In\ncontrast, we identify that the main information source is the {\\textit{bank}}\nsector in the Chinese market and the {\\textit{energy}} sector in the USA\nmarket. In the case of China, this is due to the importance of net bank lending\nas a signal of corporate activity and the role of energy pricing in affecting\ncorporate profitability. There are sectors such as the {\\textit{real estate}}\nsector that could be an information sink in one market but an information\nsource in the other, showing the complex behavior of different markets.\nOverall, these findings show that stock markets are more synchronized, or\nordered, during periods of turmoil than during periods of stability.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.07612v1"
    },
    {
        "title": "Information flow networks of Chinese stock market sectors",
        "authors": [
            "Peng Yue",
            "Qing Cai",
            "Wanfeng Yan",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Transfer entropy measures the strength and direction of information flow\nbetween different time series. We study the information flow networks of the\nChinese stock market and identify important sectors and information flow paths.\nThis paper uses the daily closing price data of the 28 level-1 sectors from\nShenyin \\& Wanguo Securities ranging from 2000 to 2017 to study the information\ntransmission between different sectors. We construct information flow networks\nwith the sectors as the nodes and the transfer entropy between them as the\ncorresponding edges. Then we adopt the maximum spanning arborescence (MSA) to\nextracting important information flows and the hierarchical structure of the\nnetworks. We find that, during the whole sample period, the \\textit{composite}\nsector is an information source of the whole stock market, while the\n\\textit{non-bank financial} sector is the information sink. We also find that\nthe \\textit{non-bank finance}, \\textit{bank}, \\textit{computer},\n\\textit{media}, \\textit{real estate}, \\textit{medical biology} and\n\\textit{non-ferrous metals} sectors appear as high-degree root nodes in the\noutgoing and incoming information flow MSAs. Especially, the \\textit{non-bank\nfinance} and \\textit{bank} sectors have significantly high degrees after 2008\nin the outgoing information flow networks. We uncover how stock market turmoils\naffect the structure of the MSAs. Finally, we reveal the specificity of\ninformation source and sink sectors and make a conclusion that the root node\nsector as the information sink of the incoming information flow networks.\nOverall, our analyses show that the structure of information flow networks\nchanges with time and the market exhibits a sector rotation phenomenon. Our\nwork has important implications for market participants and policy makers in\nmanaging market risks and controlling the contagion of risks.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.08759v1"
    },
    {
        "title": "Skewed non-Gaussian GARCH models for cryptocurrencies volatility\n  modelling",
        "authors": [
            "Roy Cerqueti",
            "Massimiliano Giacalone",
            "Raffaele Mattera"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Recently, cryptocurrencies have attracted a growing interest from investors,\npractitioners and researchers. Nevertheless, few studies have focused on the\npredictability of them. In this paper we propose a new and comprehensive study\nabout cryptocurrency market, evaluating the forecasting performance for three\nof the most important cryptocurrencies (Bitcoin, Ethereum and Litecoin) in\nterms of market capitalization. At this aim, we consider non-Gaussian GARCH\nvolatility models, which form a class of stochastic recursive systems commonly\nadopted for financial predictions. Results show that the best specification and\nforecasting accuracy are achieved under the Skewed Generalized Error\nDistribution when Bitcoin/USD and Litecoin/USD exchange rates are considered,\nwhile the best performances are obtained for skewed Distribution in the case of\nEthereum/USD exchange rate. The obtain findings state the effectiveness -- in\nterms of prediction performance -- of relaxing the normality assumption and\nconsidering skewed distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11674v1"
    },
    {
        "title": "Uncovering the Dynamics of Correlation Structures Relative to the\n  Collective Market Motion",
        "authors": [
            "Anton J. Heckens",
            "Sebastian M. Krause",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The measured correlations of financial time series in subsequent epochs\nchange considerably as a function of time. When studying the whole correlation\nmatrices, quasi-stationary patterns, referred to as market states, are seen by\napplying clustering methods. They emerge, disappear or reemerge, but they are\ndominated by the collective motion of all stocks. In the jargon, one speaks of\nthe market motion, it is always associated with the largest eigenvalue of the\ncorrelation matrices. Thus the question arises, if one can extract more refined\ninformation on the system by subtracting the dominating market motion in a\nproper way. To this end we introduce a new approach by clustering reduced-rank\ncorrelation matrices which are obtained by subtracting the dyadic matrix\nbelonging to the largest eigenvalue from the standard correlation matrices. We\nanalyze daily data of 262 companies of the S&P 500 index over a period of\nalmost 15 years from 2002 to 2016. The resulting dynamics is remarkably\ndifferent, and the corresponding market states are quasi-stationary over a long\nperiod of time. Our approach adds to the attempts to separate endogenous from\nexogenous effects.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.12336v2"
    },
    {
        "title": "Long-Range Dependence in Financial Markets: a Moving Average Cluster\n  Entropy Approach",
        "authors": [
            "Pietro Murialdo",
            "Linda Ponta",
            "Anna Carbone"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  A perspective is taken on the intangible complexity of economic and social\nsystems by investigating the underlying dynamical processes that produce, store\nand transmit information in financial time series in terms of the\n\\textit{moving average cluster entropy}. An extensive analysis has evidenced\nmarket and horizon dependence of the \\textit{moving average cluster entropy} in\nreal world financial assets. The origin of the behavior is scrutinized by\napplying the \\textit{moving average cluster entropy} approach to long-range\ncorrelated stochastic processes as the Autoregressive Fractionally Integrated\nMoving Average (ARFIMA) and Fractional Brownian motion (FBM). To that end, an\nextensive set of series is generated with a broad range of values of the Hurst\nexponent $H$ and of the autoregressive, differencing and moving average\nparameters $p,d,q$. A systematic relation between \\textit{moving average\ncluster entropy}, \\textit{Market Dynamic Index} and long-range correlation\nparameters $H$, $d$ is observed. This study shows that the characteristic\nbehaviour exhibited by the horizon dependence of the cluster entropy is related\nto long-range positive correlation in financial markets. Specifically, long\nrange positively correlated ARFIMA processes with differencing parameter $\nd\\simeq 0.05$, $d\\simeq 0.15$ and $ d\\simeq 0.25$ are consistent with\n\\textit{moving average cluster entropy} results obtained in time series of\nDJIA, S\\&P500 and NASDAQ.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14736v1"
    },
    {
        "title": "A Research on Cross-sectional Return Dispersion and Volatility of US\n  Stock Market during COVID-19",
        "authors": [
            "Jiawei Du"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We studied the volatility and cross-sectional return dispersion effect of S&P\nHealth Care Sector under the covid-19 epidemic. We innovatively used the Google\nindex to proxy the impact of the epidemic and modeled the volatility. We also\nstudied the influencing factors of the log-return of S&P Energy Sector and S&P\nHealth Care Sector. We found that volatility is significantly affected by both\nthe epidemic and cross-sectional return dispersion, and the coefficients in\nfront of them are all positive, which means that the herding behaviour did not\nexist and as the cross-sectional return dispersion increases and the epidemic\nbecomes more severe, the volatility of stock returns is also increasing. We\nalso found that the epidemic has a significant negative impact on the return of\nthe energy sector, and finally we provided our suggestions to investors.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11546v2"
    },
    {
        "title": "Market laws",
        "authors": [
            "Caglar Tuncay"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  More than one billion data sampled with different frequencies from several\nfinancial instruments were investigated with the aim of testing whether they\ninvolve power law. As a result, a known power law with the power exponent\naround -4 was detected in the empirical distributions of the relative returns.\nMoreover, a number of new power law behaviors with various power exponents were\nexplored in the same data. Further on, a model based on finite sums over\nnumerous Maxwell-Boltzmann type distribution functions with random\n(pseudorandom) multipliers in the exponent were proposed to deal with the\nempirical distributions involving power laws. The results indicate that the\nproposed model may be universal.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.01199v1"
    },
    {
        "title": "Deep Learning for Digital Asset Limit Order Books",
        "authors": [
            "Rakshit Jha",
            "Mattijs De Paepe",
            "Samuel Holt",
            "James West",
            "Shaun Ng"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This paper shows that temporal CNNs accurately predict bitcoin spot price\nmovements from limit order book data. On a 2 second prediction time horizon we\nachieve 71\\% walk-forward accuracy on the popular cryptocurrency exchange\ncoinbase. Our model can be trained in less than a day on commodity GPUs which\ncould be installed into colocation centers allowing for model sync with\nexisting faster orderbook prediction models. We provide source code and data at\nhttps://github.com/Globe-Research/deep-orderbook.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.01241v1"
    },
    {
        "title": "A Horserace of Volatility Models for Cryptocurrency: Evidence from\n  Bitcoin Spot and Option Markets",
        "authors": [
            "Yeguang Chi",
            "Wenyan Hao"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We test various volatility models using the Bitcoin spot price series. Our\nmodels include HIST, EMA ARCH, GARCH, and EGARCH, models. Both of our\nin-sample-fit and out-of-sample-forecast results suggest that GARCH and EGARCH\nmodels perform much better than other models. Moreover, the EGARCH model's\nasymmetric term is positive and insignificant, which suggests that Bitcoin\nprices lack the asymmetric volatility response to past returns. Finally, we\nformulate an option trading strategy by exploiting the volatility spread\nbetween the GARCH volatility forecast and the option's implied volatility. We\nshow that a simple volatility-spread trading strategy with delta-hedging can\nyield robust profits.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.07402v1"
    },
    {
        "title": "The measure of model risk in credit capital requirements",
        "authors": [
            "Roberto Baviera"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Credit capital requirements in Internal Rating Based approaches require the\ncalibration of two key parameters: the probability of default and the\nloss-given-default. This letter considers the uncertainty about these two\nparameters and models this uncertainty in an elementary way: it shows how this\nestimation risk can be computed and properly taken into account in regulatory\ncapital.\n  We analyse two standard real datasets: one composed by all corporates rated\nby Moody's and one limited only to the speculative grade ones. We statistically\ntest model hypotheses on both marginal distributions and parameter dependency.\nWe compute the estimation risk impact and observe that parameter dependency\nraises substantially the tail risk in capital requirements. The results are\nstriking with a required increase in regulatory capital in the range\n$38\\%$-$66\\%$.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08028v1"
    },
    {
        "title": "Model of continuous random cascade processes in financial markets",
        "authors": [
            "Jun-ichi Maskawa",
            "Koji Kuroda"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This article present a continuous cascade model of volatility formulated as a\nstochastic differential equation. Two independent Brownian motions are\nintroduced as random sources triggering the volatility cascade. One\nmultiplicatively combines with volatility; the other does so additively.\nAssuming that the latter acts perturbatively on the system, then the model\nparameters are estimated by application to an actual stock price time series.\nNumerical calculation of the Fokker--Planck equation derived from the\nstochastic differential equation is conducted using the estimated values of\nparameters. The results reproduce the pdf of the empirical volatility, the\nmultifractality of the time series, and other empirical facts.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12270v1"
    },
    {
        "title": "Endogenous Representation of Asset Returns",
        "authors": [
            "Zhipu Zhou",
            "Alexander Shkolnik",
            "Sang-Yun Oh"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Factor modeling of asset returns has been a dominant practice in investment\nscience since the introduction of the Capital Asset Pricing Model (CAPM) and\nthe Arbitrage Pricing Theory (APT). The factors, which account for the\nsystematic risk, are either specified or interpreted to be exogenous. They\nexplain a significant portion of the risk in large portfolios. We propose a\nframework that asks how much of the risk, that we see in equity markets, may be\nexplained by the asset returns themselves. To answer this question, we\ndecompose the asset returns into an endogenous component and the remainder, and\nanalyze the properties of the resulting risk decomposition. Statistical methods\nto estimate this decomposition from data are provided along with empirical\ntests. Our results point to the possibility that most of the risk in equity\nmarkets may be explained by a sparse network of interacting assets (or their\nissuing firms). This sparse network can give the appearance of a set exogenous\nfactors where, in fact, there may be none. We illustrate our results with\nseveral case studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.13245v2"
    },
    {
        "title": "Fear and Volatility in Digital Assets",
        "authors": [
            "Faizaan Pervaiz",
            "Christopher Goh",
            "Ashley Pennington",
            "Samuel Holt",
            "James West",
            "Shaun Ng"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We show Bitcoin implied volatility on a 5 minute time horizon is modestly\npredictable from price, volatility momentum and alternative data including\nsentiment and engagement. Lagged Bitcoin index price and volatility movements\ncontribute to the model alongside Google Trends with markets responding often\nseveral hours later. The code and datasets used in this paper can be found at\nhttps://github.com/Globe-Research/bitfear.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15611v1"
    },
    {
        "title": "Augmenting transferred representations for stock classification",
        "authors": [
            "Elizabeth Fons",
            "Paula Dawson",
            "Xiao-jun Zeng",
            "John Keane",
            "Alexandros Iosifidis"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Stock classification is a challenging task due to high levels of noise and\nvolatility of stocks returns. In this paper we show that using transfer\nlearning can help with this task, by pre-training a model to extract universal\nfeatures on the full universe of stocks of the S$\\&$P500 index and then\ntransferring it to another model to directly learn a trading rule. Transferred\nmodels present more than double the risk-adjusted returns than their\ncounterparts trained from zero. In addition, we propose the use of data\naugmentation on the feature space defined as the output of a pre-trained model\n(i.e. augmenting the aggregated time-series representation). We compare this\naugmentation approach with the standard one, i.e. augmenting the time-series in\nthe input space. We show that augmentation methods on the feature space leads\nto $20\\%$ increase in risk-adjusted return compared to a model trained with\ntransfer learning but without augmentation.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.04545v1"
    },
    {
        "title": "Dynamics of market states and risk assessment",
        "authors": [
            "Hirdesh K. Pharasi",
            "Eduard Seligman",
            "Suchetana Sadhukhan",
            "Parisa Majari",
            "Thomas H. Seligman"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Previous research explored various conditions of financial markets based on\nthe similarity of correlation structures and classified as market states. We\nintroduce modifications to previous selection criteria for these market states,\nmainly due to increased attention to the transition matrix between the states.\nClustering and thus market states are fixed by the optimization of two\nparameters -- number of clusters and noise suppression, but in similar\nconditions, we give preference to the clustering which avoids large jumps in\nthe transition matrix. We found statistically significant results applying this\nmodel to the SP 500 and Nikkei 225 markets for the pre-COVID-19 pandemic era\n(2006-2019). Retaining the epoch length of 20 trading days but reducing the\nshift of the epoch to a single trading day we are led to the concept of a\ntrajectory of the market in the space of correlation matrices. We may visualize\nthese states after dimensional scaling to two or three dimensions. This\napproach, using dynamics, improves the options of risk assessment, opens the\ndoor to dynamical treatments of markets (e.g. hedging), and shows noise\nsuppression in a new light.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.05984v2"
    },
    {
        "title": "Aplicação do Movimento Browniano Geométrico para Simulação\n  de Preços de Ações do Índice Brasileiro de Small Caps",
        "authors": [
            "Marcos Vinícius dos Santos Araújo"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This work addressed the use of the geometric Brownian motion to simulate the\nprices of shares listed in the Small Caps index of the Brazilian stock exchange\nB3 (Brazil, Bolsa, Balc\\~ao). The data used refer to the price history from\nJanuary 2016 to December 2018. The price history of 2019 was used to be\ncompared with the simulated prices. The data was imported from the Yahoo\nFinance database using the Python programming language, and the simulations\nwere performed for each stock individually, and for portfolios formed based on\nexpected returns, risk and the Sharpe Index. The results were better for\nportfolios with higher returns, lower risks and higher Sharpe Indexes.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.08128v1"
    },
    {
        "title": "Principal Component Analysis and Factor Analysis for Feature Selection\n  in Credit Rating",
        "authors": [
            "Shenghuan Yang",
            "lonut Florescu",
            "Md Tariqul Islam"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The credit rating is an evaluation of a company's credit risk that values the\nability to pay back the debt and predict the likelihood of the debtor\ndefaulting. There are various features influencing credit rating. Therefore, it\nis essential to select substantive features to explore the main reason for\ncredit rating change. To address this issue, this paper exploited Principal\nComponent Analysis and Factor Analysis as feature selection algorithms to\nselect important features, summarized the similar features together, and\nobtained a minimum set of features for four sectors, Financial Sector, Energy\nSector, Health Care Sector, Consumer Discretionary Sector. This paper used two\ndata sets, Financial Ratio and Balance Sheet, with two mappings, Detailed\nMapping, and Coarse Mapping, converting the target variable(credit rating) into\ncategorical variable. To test the accuracy of credit rating prediction, Random\nForest Classifier was used to test and train feature sets. The results showed\nthat the accuracy of Financial Ratio feature sets was higher than that of\nBalance Sheet feature sets. In addition, Factor Analysis can reduce the number\nof features significantly to obtain almost the same accuracy that can decrease\ndramatically the time spent on analyzing data; we also summarized seven\ndominant factors and ten dominant factors affecting credit rating change in\nFinancial Ratio and Balance Sheet by utilizing Factor Analysis, respectively,\nwhich can explain the reason of credit rating change better.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.09137v2"
    },
    {
        "title": "Visualizing the Financial Impact of Presidential Tweets on Stock Markets",
        "authors": [
            "Ujwal Kandi",
            "Sasikanth Gujjula",
            "Venkatesh Buddha",
            "V S Bhagavan"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  As more and more data being created every day, all of it can help take better\ndecisions with data analysis. It is not different from data generated in\nfinancial markets. Here we examine the process of how the global economy is\naffected by the market sentiment influenced by the micro-blogging data (tweets)\nof American President Donald Trump. The news feed is gathered from The Guardian\nand Bloomberg from the period between December 2016 and October 2019, which are\nused to further identify the potential tweets that influenced the markets as\nmeasured by changes in equity indices.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03205v1"
    },
    {
        "title": "Evidence and Behaviour of Support and Resistance Levels in Financial\n  Time Series",
        "authors": [
            "Ken Chung",
            "Anthony Bellotti"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper investigates the phenomenon of support and resistance levels (SR\nlevels) in financial time series, which act as temporary price barriers that\nreverses price trends. We develop a heuristic discovery algorithm for this\npurpose, to discover and evaluate SR levels for intraday price series. Our\nsimple approach discovers SR levels which are able to reverse price trends\nstatistically significantly. Asset price entering SR levels with higher number\nof price bounces before are more likely to bounce on such SR levels again. We\nalso show that the decay aspect of the discovered SR levels as decreasing\nprobability of price bounce over time. We conclude SR levels are features in\nfinancial time series are not explained simply by AR(1) processes, stationary\nor otherwise; and that they contribute to the temporary predictability and\nstationarity of the investigated price series.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.07410v1"
    },
    {
        "title": "Market Regime Detection via Realized Covariances: A Comparison between\n  Unsupervised Learning and Nonlinear Models",
        "authors": [
            "Andrea Bucci",
            "Vito Ciciretti"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  There is broad empirical evidence of regime switching in financial markets.\nThe transition between different market regimes is mirrored in correlation\nmatrices, whose time-varying coefficients usually jump higher in highly\nvolatile regimes, leading to the failure of common diversification methods. In\nthis article, we aim to identify market regimes from covariance matrices and\ndetect transitions towards highly volatile regimes, hence improving tail-risk\nhedging. Starting from the time series of fractionally differentiated\nsentiment-like future values, two models are applied on monthly realized\ncovariance matrices to detect market regimes. Specifically, the regime\ndetection is implemented via vector logistic smooth transition autoregressive\nmodel (VLSTAR) and through an unsupervised learning methodology, the\nagglomerative hierarchical clustering. Since market regime switches are\nunobservable processes that describe the latent change of market behaviour, the\nability of correctly detecting market regimes is validated in two ways:\nfirstly, randomly generated data are used to assess a correct classification\nwhen regimes are known; secondly, a na\\\"{i}ve trading strategy filtered with\nthe detected regime switches is used to understand whether an improvement is\nshowed when accounting for regime switches. The results point to the VLSTAR as\nthe best performing model for labelling market regimes.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.03667v1"
    },
    {
        "title": "A Bayesian analysis of gain-loss asymmetry",
        "authors": [
            "Andrea Giuseppe Di Iura",
            "Giulia Terenzi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We perform a quantitative analysis of the gain/loss asymmetry for financial\ntime series by using a Bayesian approach. In particular, we focus on some\nselected indices and analyze the statistical significance of the asymmetry\namount through a Bayesian generalization of the t-Test, which relaxes the\nnormality assumption on the underlying distribution. We propose two different\nmodels for data distribution, we study the convergence of our method and we\nprovide several graphical representations of our numerical results. Finally, we\nperform a sensitivity analysis with respect to model parameters in order to\nstudy the reliability and robustness of our results.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.06044v1"
    },
    {
        "title": "Foreign exchange markets: price response and spread impact",
        "authors": [
            "Juan Camilo Henao Londono",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We carry out a detailed large-scale data analysis of price response functions\nin the spot foreign exchange market for different years and different time\nscales. Such response functions provide quantitative information on the\ndeviation from Markovian behavior. The price response functions show an\nincrease to a maximum followed by a slow decrease as the time lag grows, in\ntrade time scale and in physical time scale, for all analyzed years.\nFurthermore, we use a price increment point (pip) bid-ask spread definition to\ngroup different foreign exchange pairs and analyze the impact of the spread in\nthe price response functions. We find that large pip spreads have a stronger\nimpact on the response. This is similar to what has been found in stock\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.09309v2"
    },
    {
        "title": "Modelling Net Loan Loss with Bayesian and Frequentist Regression\n  Analysis",
        "authors": [
            "Nathan Thomas Provost"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We create two distinct nonlinear regression models relating net loan loss (as\nan outcome) to several other financial and sociological quantities. We consider\nthese data for the time interval between April 1st 2011 and April 1st 2020. We\nalso include temporal quantities (month and year) in our model to improve\naccuracy. One model follows the frequentist paradigm for nonlinear regression,\nwhile the other follows the Bayesian paradigm. By using the two methods, we\nobtain a rounded understanding of the relationship between net loan losses and\nour given financial, sociological, and temporal variables, improving our\nability to make financial predictions regarding the profitability of loan\nallocation.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.13947v1"
    },
    {
        "title": "Why and how systematic strategies decay",
        "authors": [
            "Antoine Falck",
            "Adam Rej",
            "David Thesmar"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In this paper, we propose ex-ante characteristics that predict the drop in\nrisk-adjusted performance out-of-sample for a large set of stock anomalies\npublished in finance and accounting academic journals. Our set of predictors is\ngenerated by hypotheses of OOS decay put forward by McLean and Pontiff (2016):\narbitrage capital flowing into newly published strategies and in-sample\noverfitting linked to multiple hypothesis testing. The year of publication\nalone - compatible with both hypotheses - explains 30% of the variance of\nSharpe decay across factors: Every year, the Sharpe decay of newly-published\nfactors increases by 5ppt. The other important variables are directly related\nto overfitting: the number of operations required to calculate the signal and\ntwo measures of sensitivity of in-sample Sharpe to outliers together add\nanother 15% of explanatory power. Some arbitrage-related variables are\nstatistically significant, but their predictive power is marginal.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01380v1"
    },
    {
        "title": "Bayesian inference and superstatistics to describe long memory processes\n  of financial time series",
        "authors": [
            "Geoffrey Ducournau"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  One of the standardized features of financial data is that log-returns are\nuncorrelated, but absolute log-returns or their squares namely the fluctuating\nvolatility are correlated and is characterized by heavy tailed in the sense\nthat some moment of the absolute log-returns is infinite and typically\nnon-Gaussian [20]. And this last characteristic change accordantly to different\ntimescales. We propose to model this long-memory phenomenon by superstatistical\ndynamics and provide a Bayesian Inference methodology drawing on\nMetropolis-Hasting random walk sampling to determine which superstatistics\namong inverse-Gamma and log-Normal describe the best log-returns complexity on\ndifferent timescales, from high to low frequency. We show that on smaller\ntimescales (minutes) even though the Inverse-Gamma superstatistics works the\nbest, the log-Normal model remains very reliable and suitable to fit the\nabsolute log-returns probability density distribution with strong capacity of\ndescribing heavy tails and power law decays. On larger timescales (daily), we\nshow in terms of Bayes factor that the inverse-Gamma superstatistics is\npreferred to the log-Normal model. We also show evidence of a transition of\nstatistics from power law decay on small timescales to exponential decay on\nlarge scale with less heavy tails meaning that on larger time scales the\nfluctuating volatility tend to be memoryless, consequently superstatistics\nbecomes less relevant.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.04171v1"
    },
    {
        "title": "Fat Tails and Black Swans: Exact Results for Multiplicative Processes\n  with Resets",
        "authors": [
            "Damián H. Zanette",
            "Susanna Manrubia"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We consider a class of multiplicative processes which, added with stochastic\nreset events, give origin to stationary distributions with power-law tails --\nubiquitous in the statistics of social, economic, and ecological systems. Our\nmain goal is to provide a series of exact results on the dynamics and\nasymptotic behaviour of increasingly complex versions of a basic multiplicative\nprocess with resets, including discrete and continuous-time variants and\nseveral degrees of randomness in the parameters that control the process. In\nparticular, we show how the power-law distributions are built up as time\nelapses, how their moments behave with time, and how their stationary profiles\nbecome quantitatively determined by those parameters. Our discussion emphasizes\nthe connection with financial systems, but these stochastic processes are also\nexpected to be fruitful in modeling a wide variety of social and biological\nphenomena.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.11679v1"
    },
    {
        "title": "Time-dependent relations between gaps and returns in a Bitcoin order\n  book",
        "authors": [
            "Roberto Mota Navarro",
            "Paulino Monroy Castillero",
            "Francois Leyvraz"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Several studies have shown that large changes in the returns of an asset are\nassociated with the sized of the gaps present in the order book In general,\nthese associations have been studied without explicitly considering the\ndynamics of either gaps or returns. Here we present a study of these\nrelationships. Our results suggest that the causal relationship between gaps\nand returns is limited to instantaneous causation.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.02187v2"
    },
    {
        "title": "Modeling premiums of non-life insurance companies in India",
        "authors": [
            "Kartik Sethi",
            "Siddhartha P. Chakrabarty"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We undertake an empirical analysis for the premium data of non-life insurance\ncompanies operating in India, in the paradigm of fitting the data for the\nparametric distribution of Lognormal and the extreme value based distributions\nof Generalized Extreme Value and Generalized Pareto. The best fit to the data\nfor ten companies considered, is obtained for the Generalized Extreme Value\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.02446v1"
    },
    {
        "title": "A new look at calendar anomalies: Multifractality and day of the week\n  effect",
        "authors": [
            "Darko Stosic",
            "Dusan Stosic",
            "Irena Vodenska",
            "H. Eugene Stanley",
            "Tatijana Stosic"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Stock markets can become inefficient due to calendar anomalies known as\nday-of-the-week effect. Calendar anomalies are well-known in financial\nliterature, but the phenomena remain to be explored in econophysics. In this\npaper we use multifractal analysis to evaluate if the temporal dynamics of\nmarket returns also exhibits calendar anomalies such as day-of-the-week\neffects. We apply the multifractal detrended fluctuation analysis (MF-DFA) to\ndaily returns of market indices around the world for each day of the week. Our\nresults indicate that individual days of the week are characterized by distinct\nmultifractal properties. Monday returns tend to exhibit more persistent\nbehavior and richer multifractal structures than other day-resolved returns.\nShuffling the series reveals that multifractality arises both from a broad\nprobability density function and from long-term correlations. From the\ntime-dependent multifractal analysis we find that multifractal spectra for\nMonday returns are much wider than for other days during periods of financial\ncrises. The presence of day-of-the-week effects in multifractal dynamics of\nmarket returns motivates further research on calendar anomalies from an\neconophysics perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.06164v1"
    },
    {
        "title": "The link between Bitcoin and Google Trends attention",
        "authors": [
            "Nektarios Aslanidis",
            "Aurelio F. Bariviera",
            "Óscar G. López"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper shows that Bitcoin is not correlated to a general uncertainty\nindex as measured by the Google Trends data of Castelnuovo and Tran (2017).\nInstead, Bitcoin is linked to a Google Trends attention measure specific for\nthe cryptocurrency market. First, we find a bidirectional relationship between\nGoogle Trends attention and Bitcoin returns up to six days. Second, information\nflows from Bitcoin volatility to Google Trends attention seem to be larger than\ninformation flows in the other direction. These relations hold across different\nsub-periods and different compositions of the proposed Google Trends\nCryptocurrency index.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.07104v1"
    },
    {
        "title": "A Two-Step Framework for Arbitrage-Free Prediction of the Implied\n  Volatility Surface",
        "authors": [
            "Wenyong Zhang",
            "Lingfei Li",
            "Gongqiu Zhang"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We propose a two-step framework for predicting the implied volatility surface\nover time without static arbitrage. In the first step, we select features to\nrepresent the surface and predict them over time. In the second step, we use\nthe predicted features to construct the implied volatility surface using a deep\nneural network (DNN) model by incorporating constraints that prevent static\narbitrage. We consider three methods to extract features from the implied\nvolatility data: principal component analysis, variational autoencoder and\nsampling the surface, and we predict these features using LSTM. Using a long\ntime series of implied volatility data for S\\&P500 index options to train our\nmodels, we find two feature construction methods, sampling the surface and\nvariational autoencoders combined with DNN for surface construction, are the\nbest performers in out-of-sample prediction. In particular, they outperform a\nclassical method substantially. Furthermore, the DNN model for surface\nconstruction not only removes static arbitrage, but also significantly reduces\nthe prediction error compared with a standard interpolation method. Our\nframework can also be used to simulate the dynamics of the implied volatility\nsurface without static arbitrage.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.07177v2"
    },
    {
        "title": "Trend-Following Strategies via Dynamic Momentum Learning",
        "authors": [
            "Bruno P. C. Levy",
            "Hedibert F. Lopes"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Time series momentum strategies are widely applied in the quantitative\nfinancial industry and its academic research has grown rapidly since the work\nof Moskowitz, Ooi and Pedersen (2012). However, trading signals are usually\nobtained via simple observation of past return measurements. In this article we\nstudy the benefits of incorporating dynamic econometric models to sequentially\nlearn the time-varying importance of different look-back periods for individual\nassets. By the use of a dynamic binary classifier model, the investor is able\nto switch between time-varying or constant relations between past momentum and\nfuture returns, dynamically combining or selecting different momentum speeds\nduring turning points, improving trading signals accuracy and portfolio\nperformance. Using data from 56 future contracts we show that a mean-variance\ninvestor will be willing to pay a considerable management fee to switch from\nthe traditional naive time series momentum strategy to the dynamic classifier\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.08420v4"
    },
    {
        "title": "Effectiveness of Artificial Intelligence in Stock Market Prediction\n  based on Machine Learning",
        "authors": [
            "Sohrab Mokhtari",
            "Kang K. Yen",
            "Jin Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper tries to address the problem of stock market prediction leveraging\nartificial intelligence (AI) strategies. The stock market prediction can be\nmodeled based on two principal analyses called technical and fundamental. In\nthe technical analysis approach, the regression machine learning (ML)\nalgorithms are employed to predict the stock price trend at the end of a\nbusiness day based on the historical price data. In contrast, in the\nfundamental analysis, the classification ML algorithms are applied to classify\nthe public sentiment based on news and social media. In the technical analysis,\nthe historical price data is exploited from Yahoo Finance, and in fundamental\nanalysis, public tweets on Twitter associated with the stock market are\ninvestigated to assess the impact of sentiments on the stock market's forecast.\nThe results show a median performance, implying that with the current\ntechnology of AI, it is too soon to claim AI can beat the stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.01031v1"
    },
    {
        "title": "On the short term stability of financial ARCH price processes",
        "authors": [
            "Gilles Zumbach"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  For many financial applications, it is important to have reliable and\ntractable models for the behavior of assets and indexes, for example in risk\nevaluation. A successful approach is based on ARCH processes, which strike the\nright balance between statistical properties and ease of computation. This\nstudy focuses on quadratic ARCH processes and the theoretical conditions to\nhave a stable long-term behavior. In particular, the weights for the variance\nestimators should sum to 1, and the variance of the innovations should be 1.\nUsing historical data, the realized empirical innovations can be computed, and\ntheir statistical properties assessed. Using samples of 3 to 5 decades, the\nvariance of the empirical innovations are always significantly above 1, for a\nsample of stock indexes, commodity indexes and FX rates. This departure points\nto a short term instability, or to a fast adaptability due to changing\nconditions. Another theoretical condition on the innovations is to have a zero\nmean. This condition is also investigated empirically, with some time series\nshowing significant departure from zero.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06758v1"
    },
    {
        "title": "Market risk factors analysis for an international mining company.\n  Multi-dimensional, heavy-tailed-based modelling",
        "authors": [
            "Łukasz Bielak",
            "Aleksandra Grzesiek",
            "Joanna Janczura",
            "Agnieszka Wyłomańska"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Mining companies to properly manage their operations and be ready to make\nbusiness decisions, are required to analyze potential scenarios for main market\nrisk factors. The most important risk factors for KGHM, one of the biggest\ncompanies active in the metals and mining industry, are the price of copper\n(Cu), traded in US dollars, and the Polish zloty (PLN) exchange rate (USDPLN).\nThe main scope of the paper is to understand the mid- and long-term dynamics of\nthese two risk factors. For a mining company it might help to properly evaluate\npotential downside market risk and optimise hedging instruments. From the\nmarket risk management perspective, it is also important to analyze the\ndynamics of these two factors combined with the price of copper in Polish zloty\n(Cu in PLN), which jointly drive the revenues, cash flows, and financial\nresults of the company. Based on the relation between analyzed risk factors and\ndistribution analysis, we propose to use two-dimensional vector autoregressive\n(VAR) model with the $\\alpha-$stable distribution. The non-homogeneity of the\ndata is reflected in two identified regimes: first - corresponding to the 2008\ncrisis and second - to the stable market situation. As a natural implication of\nthe model fitted to market assets, we derive the dynamics of the copper price\nin PLN, which is not a traded asset but is crucial for the KGHM company risk\nexposure. A comparative study is performed to demonstrate the effect of\nincluding dependencies of the assets and the implications of the regime change.\nSince for various international companies, risk factors are given rather in the\nnational than the market currency, the approach is universal and can be used in\ndifferent market contexts, like mining or oil companies, but also other\ncommodities involved in the global trading system.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.07142v1"
    },
    {
        "title": "Collective correlations, dynamics, and behavioural inconsistencies of\n  the cryptocurrency market over time",
        "authors": [
            "Nick James",
            "Max Menzies"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper introduces new methods to study behaviours among the 52 largest\ncryptocurrencies between 01-01-2019 and 30-06-2021. First, we explore\nevolutionary correlation behaviours and apply a recently proposed turning point\nalgorithm to identify regimes in market correlation. Next, we inspect the\nrelationship between collective dynamics and the cryptocurrency market size -\nrevealing an inverse relationship between the size of the market and the\nstrength of collective dynamics. We then explore the time-varying consistency\nof the relationships between cryptocurrencies' size and their returns and\nvolatility. There, we demonstrate that there is greater consistency between\nsize and volatility than size and returns. Finally, we study the spread of\nvolatility behaviours across the market changing with time by examining the\nstructure of Wasserstein distances between probability density functions of\nrolling volatility. We demonstrate a new phenomenon of increased uniformity in\nvolatility during market crashes, which we term \\emph{volatility dispersion}.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.13926v2"
    },
    {
        "title": "Feature importance recap and stacking models for forex price prediction",
        "authors": [
            "Yunze Li",
            "Yanan Xie",
            "Chen Yu",
            "Fangxing Yu",
            "Bo Jiang",
            "Matloob Khushi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Forex trading is the largest market in terms of qutantitative trading.\nTraditionally, traders refer to technical analysis based on the historical data\nto make decisions and trade. With the development of artificial intelligent,\ndeep learning plays a more and more important role in forex forecasting. How to\nuse deep learning models to predict future price is the primary purpose of most\nresearchers. Such prediction not only helps investors and traders make\ndecisions, but also can be used for auto-trading system. In this article, we\nhave proposed a novel approach of feature selection called 'feature importance\nrecap' which combines the feature importance score from tree-based model with\nthe performance of deep learning model. A stacking model is also developed to\nfurther improve the performance. Our results shows that proper feature\nselection approach could significantly improve the model performance, and for\nfinancial data, some features have high importance score in many models. The\nresults of stacking model indicate that combining the predictions of some\nmodels and feed into a neural network can further improve the performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14092v1"
    },
    {
        "title": "On the Parameter Estimation in the Schwartz-Smiths Two-Factor Model",
        "authors": [
            "Karol Binkowski",
            "Peilun He",
            "Nino Kordzakhia",
            "Pavel Shevchenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The two unobservable state variables representing the short and long term\nfactors introduced by Schwartz and Smith in [16] for risk-neutral pricing of\nfutures contracts are modelled as two correlated Ornstein-Uhlenbeck processes.\nThe Kalman Filter (KF) method has been implemented to estimate the short and\nlong term factors jointly with un- known model parameters. The parameter\nidentification problem arising within the likelihood function in the KF has\nbeen addressed by introduc- ing an additional constraint. The obtained model\nparameter estimates are the conditional Maximum Likelihood Estimators (MLEs)\nevaluated within the KF. Consistency of the conditional MLEs is studied. The\nmethodology has been tested on simulated data.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.01881v1"
    },
    {
        "title": "On Modelling of Crude Oil Futures in a Bivariate State-Space Framework",
        "authors": [
            "Peilun He",
            "Karol Binkowski",
            "Nino Kordzakhia",
            "Pavel Shevchenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We study a bivariate latent factor model for the pricing of commodity fu-\ntures. The two unobservable state variables representing the short and long\nterm fac- tors are modelled as Ornstein-Uhlenbeck (OU) processes. The Kalman\nFilter (KF) algorithm has been implemented to estimate the unobservable factors\nas well as unknown model parameters. The estimates of model parameters were\nobtained by maximising a Gaussian likelihood function. The algorithm has been\napplied to WTI Crude Oil NYMEX futures data.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.01886v1"
    },
    {
        "title": "Previsão dos preços de abertura, mínima e máxima de índices\n  de mercados financeiros usando a associação de redes neurais LSTM",
        "authors": [
            "Gabriel de Oliveira Guedes Nogueira",
            "Marcel Otoboni de Lima"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In order to make good investment decisions, it is vitally important for an\ninvestor to know how to make good analysis of financial time series. Within\nthis context, studies on the forecast of the values and trends of stock prices\nhave become more relevant. Currently, there are different approaches to dealing\nwith the task. The two main ones are the historical analysis of stock prices\nand technical indicators and the analysis of sentiments in news, blogs and\ntweets about the market. Some of the most used statistical and artificial\nintelligence techniques are genetic algorithms, Support Vector Machines (SVM)\nand different architectures of artificial neural networks. This work proposes\nthe improvement of a model based on the association of three distinct LSTM\nneural networks, each acting in parallel to predict the opening, minimum and\nmaximum prices of stock exchange indices on the day following the analysis. The\ndataset is composed of historical data from more than 10 indices from the\nworld's largest stock exchanges. The results demonstrate that the model is able\nto predict trends and stock prices with reasonable accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.10065v1"
    },
    {
        "title": "A Time-Varying Network for Cryptocurrencies",
        "authors": [
            "Li Guo",
            "Wolfgang Karl Härdle",
            "Yubo Tao"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Cryptocurrencies return cross-predictability and technological similarity\nyield information on risk propagation and market segmentation. To investigate\nthese effects, we build a time-varying network for cryptocurrencies, based on\nthe evolution of return cross-predictability and technological similarities. We\ndevelop a dynamic covariate-assisted spectral clustering method to consistently\nestimate the latent community structure of cryptocurrencies network that\naccounts for both sets of information. We demonstrate that investors can\nachieve better risk diversification by investing in cryptocurrencies from\ndifferent communities. A cross-sectional portfolio that implements an\ninter-crypto momentum trading strategy earns a 1.08% daily return. By\ndissecting the portfolio returns on behavioral factors, we confirm that our\nresults are not driven by behavioral mechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.11921v1"
    },
    {
        "title": "Iterated and exponentially weighted moving principal component analysis",
        "authors": [
            "Paul Bilokon",
            "David Finkelstein"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The principal component analysis (PCA) is a staple statistical and\nunsupervised machine learning technique in finance. The application of PCA in a\nfinancial setting is associated with several technical difficulties, such as\nnumerical instability and nonstationarity. We attempt to resolve them by\nproposing two new variants of PCA: an iterated principal component analysis\n(IPCA) and an exponentially weighted moving principal component analysis\n(EWMPCA). Both variants rely on the Ogita-Aishima iteration as a crucial step.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.13072v1"
    },
    {
        "title": "What drives bitcoin? An approach from continuous local transfer entropy\n  and deep learning classification models",
        "authors": [
            "Andrés García-Medina",
            "Toan Luu Duc Huynh3"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Bitcoin has attracted attention from different market participants due to\nunpredictable price patterns. Sometimes, the price has exhibited big jumps.\nBitcoin prices have also had extreme, unexpected crashes. We test the\npredictive power of a wide range of determinants on bitcoins' price direction\nunder the continuous transfer entropy approach as a feature selection\ncriterion. Accordingly, the statistically significant assets in the sense of\npermutation test on the nearest neighbour estimation of local transfer entropy\nare used as features or explanatory variables in a deep learning classification\nmodel to predict the price direction of bitcoin. The proposed variable\nselection methodology excludes the NASDAQ index and Tesla as drivers. Under\ndifferent scenarios and metrics, the best results are obtained using the\nsignificant drivers during the pandemic as validation. In the test, the\naccuracy increased in the post-pandemic scenario of July 2020 to January 2021\nwithout drivers. In other words, our results indicate that in times of high\nvolatility, Bitcoin seems to autoregulate and does not need additional drivers\nto improve the accuracy of the price direction.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.01214v1"
    },
    {
        "title": "Bitcoin Volatility and Intrinsic Time Using Double Subordinated Levy\n  Processes",
        "authors": [
            "Abootaleb Shirvani",
            "Stefan Mittnik",
            "W. Brent Lindquist",
            "Svetlozar T. Rachev"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We propose a doubly subordinated Levy process, NDIG, to model the time series\nproperties of the cryptocurrency bitcoin. NDIG captures the skew and fat-tailed\nproperties of bitcoin prices and gives rise to an arbitrage free, option\npricing model. In this framework we derive two bitcoin volatility measures. The\nfirst combines NDIG option pricing with the Cboe VIX model to compute an\nimplied volatility; the second uses the volatility of the unit time increment\nof the NDIG model. Both are compared to a volatility based upon historical\nstandard deviation. With appropriate linear scaling, the NDIG process perfectly\ncaptures observed, in-sample, volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.15051v2"
    },
    {
        "title": "Causal effect of regulated Bitcoin futures on volatility and volume",
        "authors": [
            "Fiammetta Menchetti",
            "Fabrizio Cipollini",
            "Fabrizia Mealli"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In December 2017, two leading derivative exchanges, CBOE and CME, introduced\nthe first regulated Bitcoin futures. Our aim is estimating their causal impact\non Bitcoin volatility and trading volume. Employing a new causal approach,\nC-ARIMA, we find that the CME future triggered an increase in both outcomes.\nThere is also evidence of a positive volume-volatility relationship and that\nthe effect on volatility was partially due to the higher trading volumes\ninduced by the launch of the contract. After controlling for the effect on\nvolumes, we find that the CME instrument caused Bitcoin volatility to increase\nby more than double.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.15052v1"
    },
    {
        "title": "Application of DEA in International Market Selection for the export of\n  products from Spain",
        "authors": [
            "Safa El Kefi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This article presents a Benchmarking methodology to support decision-making\nfor international market selection (IMS). In order to do so, we will be using\nan output-oriented Data Envelopment Analysis (DEA) model. This methodology\nconsiders multiple variables validated with a correlation analysis. The\nmethodology is applied to all of the products directly exported from Spain, it\ntakes into consideration different Inputs variables and returns us the\nefficient and regions generating higher benefits to access international\nmarkets with the lowest costs possible.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.03512v1"
    },
    {
        "title": "A sentiment-based modeling and analysis of stock price during the\n  COVID-19: U- and Swoosh-shaped recovery",
        "authors": [
            "Anish Rai",
            "Ajit Mahata",
            "Md. Nurujjaman",
            "Sushovan Majhi",
            "Kanish debnath"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Recently, a stock price model is proposed by A. Mahata et al. [Physica A,\n574, 126008 (2021)] to understand the effect of COVID-19 on stock market. It\ndescribes V- and L-shaped recovery of the stocks and indices, but fails to\nsimulate the U- and Swoosh-shaped recovery that arises due to sharp crisis and\nprolong drop followed by quick recovery (U-shaped) or slow recovery for longer\nperiod (Swoosh-shaped recovery). We propose a modified model by introducing a\nnew variable $\\theta$ that quantifies the sentiment of the investors.\n$\\theta=+1,~0,~-1$ for positive, neutral and negative sentiment, respectively.\nThe model explains the movement of sectoral indices with positive $\\phi$\nshowing U- and Swoosh-shaped recovery. The simulation using synthetic fund-flow\n($\\Psi_{st}$) with different shock lengths ($T_S$), $\\phi$, negative sentiment\nperiod ($T_N$) and portion of fund-flow ($\\lambda$) during recovery period show\nU- and Swoosh-shaped recovery. The results show that the recovery of the\nindices with positive $\\phi$ becomes very weak with the extended $T_S$ and\n$T_N$. The stocks with higher $\\phi$ and $\\lambda$ recover quickly. The\nsimulation of the Nifty Bank, Nifty Financial and Nifty Realty show U-shaped\nrecovery and Nifty IT shows Swoosh-shaped recovery. The simulation result is\nconsistent with the real stock price movement. The time-scale ($\\tau$) of the\nshock and recovery of these indices during the COVID-19 are consistent with the\ntime duration of the change of negative sentiment from the onset of the\nCOVID-19. This study may help the investors to plan their investment during\ndifferent crises.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.03986v1"
    },
    {
        "title": "Graph Auto-Encoders for Financial Clustering",
        "authors": [
            "Edward Turner"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Deep learning has shown remarkable results on Euclidean data (e.g. audio,\nimages, text) however this type of data is limited in the amount of relational\ninformation it can hold. In mathematics we can model more general relational\ndata in a graph structure while retaining Euclidean data as associated node or\nedge features. Due to the ubiquity of graph data, and its ability to hold\nmultiple dimensions of information, graph deep learning has become a fast\nemerging field. We look at applying and optimising graph deep learning on a\nfinance graph to produce more informed clusters of companies. Having clusters\nproduced from multiple streams of data can be highly useful in quantitative\nfinance; not only does it allow clusters to be tailored to the specific task\nbut the culmination of multiple streams allows for cross source pattern\nrecognition that would have otherwise gone unnoticed. This can provide\nfinancial institutions with an edge over competitors which is crucial in the\nheavily optimised world of trading. In this paper we use news co-occurrence and\nstock price for our data combination. We optimise our model to achieve an\naverage testing precision of 78% and find a clear improvement in clustering\ncapabilities when dual data sources are used; cluster purity rises from 32% for\njust vertex data and 42% for just edge data to 64% when both are used in\ncomparisons to ground-truth Bloomberg clusters. The framework we provide\nutilises unsupervised learning which we view as key for future work due to the\nvolume of unlabelled data in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.13519v2"
    },
    {
        "title": "Is Bitcoin really a currency? A viewpoint of a stochastic volatility\n  model",
        "authors": [
            "Noriyuki Kunimoto",
            "Kazuhiko Kakamu"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Using the asymmetric stochastic volatility model, this study investigates the\nday-of-the-week and holiday effects on the returns and volatility of Bitcoin\nfrom January 1, 2013 to August 31, 2019; in this context, we also discuss the\ncharacteristics of Bitcoin as a financial asset. The results of the estimation\nare threefold. First, the finding shows a small day-of-the week effect in\nvolatility on Saturday and Sunday than in the rest of the week. Second,\nalthough the holiday effects are examined in active trading countries, namely\nJapan, China, Germany, and the United States, the positive post-holiday effect\non the returns and weak positive pre-holiday effect on the volatility are only\nobserved in the United States. Finally, the asymmetry effect is not observed. A\ncomparison of Bitcoin to several assets such as stock, currency, and gold shows\nBitcoin's positioning between stock, currency, and gold in relation to the week\nand holiday effects, its reaction to federal funds and medium of exchange\ncharacteristics, and the lack of asymmetry effect.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.15351v1"
    },
    {
        "title": "An Improved Reinforcement Learning Model Based on Sentiment Analysis",
        "authors": [
            "Yizhuo Li",
            "Peng Zhou",
            "Fangyi Li",
            "Xiao Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  With the development of artificial intelligence technology, quantitative\ntrading systems represented by reinforcement learning have emerged in the stock\ntrading market. The authors combined the deep Q network in reinforcement\nlearning with the sentiment quantitative indicator ARBR to build a\nhigh-frequency stock trading model for the share market. To improve the\nperformance of the model, the PCA algorithm is used to reduce the\ndimensionality feature vector while incorporating the influence of market\nsentiment on the long-short power into the spatial state of the trading model\nand uses the LSTM layer to replace the fully connected layer to solve the\ntraditional DQN model due to limited empirical data storage. Through the use of\ncumulative income, Sharpe ratio to evaluate the performance of the model and\nthe use of double moving averages and other strategies for comparison. The\nresults show that the improved model proposed by authors is far superior to the\ncomparison model in terms of income, achieving a maximum annualized rate of\nreturn of 54.5%, which is proven to be able to increase reinforcement learning\nperformance significantly in stock trading.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.15354v1"
    },
    {
        "title": "Prediction of Fund Net Value Based on ARIMA-LSTM Hybrid Model",
        "authors": [
            "Peng Zhou",
            "Fangyi Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The net value of the fund is affected by performance and market, and the\nresearchers try to quantify these effects to predict the future net value by\nestablishing different models. The current prediction models usually can only\nreflect the linear variation law, poorly handled or selectively ignore their\nnonlinear characteristics, so the prediction results are usually less accurate.\nThis paper uses a fund prediction method based on the ARIMA-LSTM hybrid model.\nAfter preprocessing the historical data, the first filter out the linear data\ncharacteristics with the ARIMA model, then pass the data to the LSTM model to\nextract the nonlinear characteristic by residual, and finally superposition the\nrespective prediction values of the two models to obtain the prediction results\nof the hybrid model. Empirically shows that the methods in the paper are more\naccurate and applicable than traditional fund prediction methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.15355v1"
    },
    {
        "title": "Improved Method of Stock Trading under Reinforcement Learning Based on\n  DRQN and Sentiment Indicators ARBR",
        "authors": [
            "Peng Zhou",
            "Jingling Tang"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  With the application of artificial intelligence in the financial field,\nquantitative trading is considered to be profitable. Based on this, this paper\nproposes an improved deep recurrent DRQN-ARBR model because the existing\nquantitative trading model ignores the impact of irrational investor behavior\non the market, making the application effect poor in an environment where the\nstock market in China is non-efficiency. By changing the fully connected layer\nin the original model to the LSTM layer and using the emotion indicator ARBR to\nconstruct a trading strategy, this model solves the problems of the traditional\nDQN model with limited memory for empirical data storage and the impact of\nobservable Markov properties on performance. At the same time, this paper also\nimproved the shortcomings of the original model with fewer stock states and\nchose more technical indicators as the input values of the model. The\nexperimental results show that the DRQN-ARBR algorithm proposed in this paper\ncan significantly improve the performance of reinforcement learning in stock\ntrading.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.15356v1"
    },
    {
        "title": "Optimal Income Crossover for Two-Class Model Using Particle Swarm\n  Optimization",
        "authors": [
            "Paulo H. dos Santos",
            "Igor D. S. Siciliani",
            "M. H. R. Tragtenberg"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Personal income distribution may exhibit a two-class structure, such that the\nlower income class of the population (85-98%) is described by exponential\nBoltzmann-Gibbs distribution, whereas the upper income class (15-2%) has a\nPareto power-law distribution. We propose a method, based on a theoretical and\nnumerical optimization scheme, which allows us to determine the crossover\nincome between the distributions, the temperature of the Boltzmann-Gibbs\ndistribution and the Pareto index. Using this method, the Brazilian income\ndistribution data provided by the National Household Sample Survey was studied.\nThe data was stratified into two dichotomies (sex/gender and color/race), so\nthe model was tested using different subsets along with accessing the economic\ndifferences between these groups. Lastly, we analyse the temporal evolution of\nthe parameters of our model and the Gini coefficient discussing the implication\non the Brazilian income inequality. To our knowledge, for the first time an\noptimization method is proposed in order to find a continuous two-class income\ndistribution, which is able to delimit the boundaries of the two distributions.\nIt also gives a measure of inequality which is a function that depends only on\nthe Pareto index and the percentage of people in the high income region. It was\nfound a temporal dynamics relation, that may be general, between the Pareto and\nthe percentage of people described by the Pareto tail.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.02449v1"
    },
    {
        "title": "Stationarity analysis of the stock market data and its application to\n  mechanical trading",
        "authors": [
            "Kazuki Kanehira",
            "Norikazu Todoroki"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This study proposes a scheme for stationarity analysis of stock price\nfluctuations based on KM$_2$O-Langevin theory. Using this scheme, we classify\nthe time-series data of stock price fluctuations into three periods:\nstationary, non-stationary, and intermediate. We then suggest an example of a\nlow-risk stock trading strategy to demonstrate the usefulness of our scheme by\nusing actual stock index data. Our strategy uses a trend-based indicator,\nmoving averages, for stationary periods and an oscillator-based indicator,\npsychological lines, for non-stationary periods to make trading decisions.\nFinally, we confirm that our strategy is a safe trading strategy with small\nmaximum drawdown by back testing on the Nikkei Stock Average. Our study, the\nfirst to apply the stationarity analysis of KM$_2$O-Langevin theory to actual\nmechanical trading, opens up new avenues for stock price prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.12459v3"
    },
    {
        "title": "Evolutionary correlation, regime switching, spectral dynamics and\n  optimal trading strategies for cryptocurrencies and equities",
        "authors": [
            "Nick James"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper uses new and recently established methodologies to study the\nevolutionary dynamics of the cryptocurrency market, and compares the findings\nwith that of the equity market. We begin by applying random matrix theory and\nprincipal components analysis (PCA) to correlation matrices of both\ncollections, highlighting clear differences in the eigenspectra exhibited. We\nthen explore the heterogeneity of both asset classes, studying the time-varying\ndynamics of underlying sector behaviours, and determine the collective\nsimilarity within each collection. We then turn to a study of structural break\ndynamics and evolutionary power spectra, where we quantify the collective\naffinity in structural breaks and evolutionary behaviours of underlying sector\ntime series. Finally, we implement two algorithms simulating `portfolio choice'\ndynamics to compare the effectiveness of stock selection and sector allocation\nin cryptocurrency portfolios. There, we highlight the importance of both\nendeavours and comment on noteworthy implications for cryptocurrency portfolio\nmanagement.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.15321v3"
    },
    {
        "title": "Limiting Spectral Distribution of High-dimensional Hayashi-Yoshida\n  Estimator of Integrated Covariance Matrix",
        "authors": [
            "Arnab Chakrabarti",
            "Rituparna Sen"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In this paper, the estimation of the Integrated Covariance matrix from\nhigh-frequency data, for high dimensional stock price process, is considered.\nThe Hayashi-Yoshida covolatility estimator is an improvement over Realized\ncovolatility for asynchronous data and works well in low dimensions. However it\nbecomes inconsistent and unreliable in the high dimensional situation. We study\nthe bulk spectrum of this matrix and establish its connection to the spectrum\nof the true covariance matrix in the limiting case where the dimension goes to\ninfinity. The results are illustrated with simulation studies in finite, but\nhigh, dimensional cases. An application to real data with tick-by-tick data on\n50 stocks is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.00119v1"
    },
    {
        "title": "From Rough to Multifractal volatility: the log S-fBM model",
        "authors": [
            "Peng Wu",
            "Jean-François Muzy",
            "Emmanuel Bacry"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We introduce a family of random measures $M_{H,T} (d t)$, namely log S-fBM,\nsuch that, for $H>0$, $M_{H,T}(d t) = e^{\\omega_{H,T}(t)} d t$ where\n$\\omega_{H,T}(t)$ is a Gaussian process that can be considered as a stationary\nversion of an $H$-fractional Brownian motion. Moreover, when $H \\to 0$, one has\n$M_{H,T}(d t) \\rightarrow {\\widetilde M}_{T}(d t)$ (in the weak sense) where\n${\\widetilde M}_{T}(d t)$ is the celebrated log-normal multifractal random\nmeasure (MRM). Thus, this model allows us to consider, within the same\nframework, the two popular classes of multifractal ($H = 0$) and rough\nvolatility ($0<H < 1/2$) models. The main properties of the log S-fBM are\ndiscussed and their estimation issues are addressed. We notably show that the\ndirect estimation of $H$ from the scaling properties of $\\ln(M_{H,T}([t,\nt+\\tau]))$, at fixed $\\tau$, can lead to strongly over-estimating the value of\n$H$. We propose a better GMM estimation method which is shown to be valid in\nthe high-frequency asymptotic regime. When applied to a large set of empirical\nvolatility data, we observe that stock indices have values around $H=0.1$ while\nindividual stocks are characterized by values of $H$ that can be very close to\n$0$ and thus well described by a MRM. We also bring evidence that unlike the\nlog-volatility variance $\\nu^2$ whose estimation appears to be poorly reliable\n(though used widely in the rough volatility literature), the estimation of the\nso-called \"intermittency coefficient\" $\\lambda^2$, which is the product of\n$\\nu^2$ and the Hurst exponent $H$, appears to be far more reliable leading to\nvalues that seem to be universal for respectively all individual stocks and all\nstock indices.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.09516v2"
    },
    {
        "title": "Stock exchange shares ranking and binary-ternary compressive coding",
        "authors": [
            "Igor Nesiolovskiy"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper proposes a method for ranking the investment attractiveness of\nexchange-traded stocks where investment risk is not related to the volatility\nindicator but instead is related to the indicator of compression of the time\nseries of price changes. The article describes in detail the ranking algorithm,\nprovides an example of ranking the shares of all companies included in the Dow\nJones stock index. The paper additionally compares the results of ranking these\nstocks by volatility and compression and also shows the strengths of the second\nindicator, which is formed using the method of binary-ternary compression of\nhistorical financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.11507v1"
    },
    {
        "title": "New Collectivity Measures for Financial Covariances and Correlations",
        "authors": [
            "Anton J. Heckens",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Complex systems are usually non-stationary and their dynamics is often\ndominated by collective effects. Collectivity, defined as coherent motion of\nthe whole system or of some of its parts, manifests itself in the\ntime-dependent structures of covariance and correlation matrices. The largest\neigenvalue corresponds to the collective motion of the system as a whole, while\nthe other large, isolated, eigenvalues indicate collectivity in parts of the\nsystem. In the case of finance, these are industrial sectors. By removing the\ncollective motion of the system as a whole, the latter effects are much better\nrevealed. We measure a remaining collectivity to which we refer as average\nsector collectivity. We identify collective signals around the Lehman Brothers\ncrash and after the dot-com bubble burst. For the Lehman Brothers crash, we\nfind a potential precursor. We analyze 213 US stocks over a period of more than\n30 years from 1990 to 2021. We plot the average sector collectivity versus the\ncollectivity corresponding to the largest eigenvalue to study the whole market\ntrajectory in a two dimensional space spanned by both collectivities.\nTherefore, we capture the average sector collectivity in a much more precise\nway. Additionally, we observe that larger values in the average sector\ncollectivity are often accompanied by trend shifts in the mean covariances and\nmean correlations. As of 2015/2016 the collectivity in the US stock markets\nchanged fundamentally.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.00297v2"
    },
    {
        "title": "2T-POT Hawkes model for left- and right-tail conditional quantile\n  forecasts of financial log-returns: out-of-sample comparison of conditional\n  EVT models",
        "authors": [
            "Matthew F. Tomlinson",
            "David Greenwood",
            "Marcin Mucha-Kruczynski"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Conditional extreme value theory (EVT) methods promise enhanced forecasting\nof the extreme tail events that often dominate systemic risk. We present an\nimproved two-tailed peaks-over-threshold (2T-POT) Hawkes model that is adapted\nfor conditional quantile forecasting in both the left and right tails of a\nunivariate time series. This is applied to the daily log-returns of six large\ncap indices. We also take the unique step of fitting the model at multiple\nexceedance thresholds (from the 1.25% to 25.00% mirrored quantiles).\nQuantitatively similar asymmetries in Hawkes parameters are found across all\nsix indices, adding further empirical support to a temporal leverage effect in\nfinancial price time series in which the impact of losses is not only larger\nbut also more immediate. Out-of-sample backtests find that our 2T-POT Hawkes\nmodel is more reliably accurate than the GARCH-EVT model when forecasting\n(mirrored) value-at-risk and expected shortfall at the 5% coverage level and\nbelow. This suggests that asymmetric Hawkes-type arrival dynamics are a better\napproximation of the true data generating process for extreme daily log-returns\nthan GARCH-type conditional volatility; our 2T-POT Hawkes model therefore\npresents a better performing alternative for financial risk modelling.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.01043v2"
    },
    {
        "title": "Time-Series K-means in Causal Inference and Mechanism Clustering for\n  Financial Data",
        "authors": [
            "Minheng Xiao"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  This paper investigates the application of Time Series K-means (TS-K-means)\nwithin the context of causal inference and mechanism clustering of financial\ntime series data. Traditional clustering approaches like K-means often rely on\nstatic distance metrics, such as Euclidean distance, which inadequately capture\nthe temporal dependencies intrinsic to financial returns. By incorporating\nDynamic Time Warping (DTW) as a distance metric, TS-K-means addresses this\nlimitation, improving the robustness of clustering in time-dependent financial\ndata. This study extends the Additive Noise Model Mixture Model (ANM-MM)\nframework by integrating TS-K-means, facilitating more accurate causal\ninference and mechanism clustering. The approach is validated through\nsimulations and applied to real-world financial data, demonstrating its\neffectiveness in enhancing the analysis of complex financial time series,\nparticularly in identifying causal relationships and clustering data based on\nunderlying generative mechanisms. The results show that TS-K-means outperforms\ntraditional K-means, especially with smaller datasets, while maintaining robust\ncausal direction detection as the dataset size changes.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03146v5"
    },
    {
        "title": "Financial Crisis in the Framework of Non-zero Temperature Balance Theory",
        "authors": [
            "MohammadReza Zahedian",
            "Mahsa Bagherikalhor",
            "Andrey Trufanov",
            "G. Reza Jafari"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Financial crises are known as crashes that result in a sudden loss of value\nof financial assets in large part and they continue to occur from time to time\nsurprisingly. In order to discover features of the financial network, the\npairwise interaction of stocks has been considered in many research, but the\nexistence of the strong correlation of stocks and their collective behavior in\ncrisis made us address higher-order interactions. Hence, in this study, we\ninvestigate financial networks by triplet interaction in the framework of\nbalance theory. Due to detecting the contribution of higher-order interactions\nin understanding the complex behavior of stocks we take the advantage of the\norders parameters of the higher-order interactions. Looking at real data of\nfinancial market obtained from $S\\&P500$ through the lens of balance theory for\nthe quest of network structure in different periods of time near and far from\ncrisis reveals the existence of a structural difference of the network that\ncorresponds to different periods of time. Here, we address two well-known\ncrises the Great regression (2008) and the Covid-19 recession (2020). Results\nshow an ordered structure forms on-crisis in the financial network while stocks\nbehave independently far from a crisis. The formation of the ordered structure\nof stocks in crisis makes the network resistant against disorder. The\nresistance of the ordered structure against applying a disorder (temperature)\ncan measure the crisis strength and determine the temperature at which the\nnetwork transits. There is a critical temperature, $T_{c}$, in the language of\nstatistical mechanics and mean-field approach which above, the ordered\nstructure destroys abruptly and a first-order phase transition occurs. The\nstronger the crisis, the higher the critical temperature.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03198v1"
    },
    {
        "title": "A Dual Generalized Long Memory Modelling for Forecasting Electricity\n  Spot Price: Neural Network and Wavelet Estimate",
        "authors": [
            "Souhir Ben Amor",
            "Heni Boubaker",
            "Lotfi Belkacem"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In this paper, dual generalized long memory modelling has been proposed to\npredict the electricity spot price. First, we focus on modelling the\nconditional mean of the series so we adopt a generalized fractional k-factor\nGegenbauer process ( k-factor GARMA). Secondly, the residual from the k-factor\nGARMA model has been used as a proxy for the conditional variance; these\nresiduals were predicted using two different approaches. In the first approach,\na local linear wavelet neural network model (LLWNN) has developed to predict\nthe conditional variance using two different learning algorithms, so we\nestimate the hybrid k- factor GARMA-LLWNN based backpropagation (BP) algorithm\nand based particle swarm optimization (PSO) algorithm. In the second approach,\nthe Gegenbauer generalized autoregressive conditional heteroscedasticity\nprocess (G-GARCH) has been adopted, and the parameters of the k-factor GARMAG-\nGARCH model have been estimated using the wavelet methodology based on the\ndiscrete wavelet packet transform (DWPT) approach. To illustrate the usefulness\nof our methodology, we carry out an empirical application using the hourly\nreturns of electricity prices from the Nord Pool market. The empirical results\nhave shown that the k-factor GARMA-G-GARCH model has the best prediction\naccuracy in terms of forecasting criteria, and find that this is more\nappropriate for forecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.08289v1"
    },
    {
        "title": "A portfolio management of a small RES utility with a Structural Vector\n  Autoregressive model of German electricity markets",
        "authors": [
            "Katarzyna Maciejowska"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The changes in electricity markets expose RES producers and electricity\ntraders to various risks, among which the price and the volume risk play a very\nimportant role. In this research, a portfolio building strategies are\npresented, which allow to dynamically choose a proportion of electricity traded\nin different electricity markets (day-ahead and intraday) and hence to optimize\nthe behavior of an utility. Two types of approaches are considered: simple,\nassuming that the proportions are fixed, and data driven, which allows for\nthier fluctuation. In order to explore the market information, Structural\nVector Autoregressive (SVAR) model is applied, which allows to estimate the\nrelationship between variables of interest and to simulate their future\ndistribution. The presented methods are evaluated with data coming from German\nelectricity market. The results indicate that data driven trading strategies\nallow to increase the utility revenue and at the same time reduce the trading\nrisk, measured by the predictability of the next day income and the revenue\nValue at Risk. It turns out that the approach based on Sharp Ratio provides the\nmost robust results.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00975v1"
    },
    {
        "title": "Collective behavior of stock prices in the time of crisis as a response\n  to the external stimulus",
        "authors": [
            "Maryam Zamani",
            "Sander Paekivi",
            "Philipp Meyer",
            "Holger Kantz"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We analyze the interaction between stock prices of big companies in the USA\nand Germany using Granger Causality. We claim that the increase in pair-wise\nGranger causality interaction between prices in the times of crisis is the\nconsequence of simultaneous response of the markets to the outside events or\nexternal stimulus that is considered as a common driver to all the stocks, not\na result of real causal predictability between the prices themselves. An\nalternative approach through recurrence analysis in single stock price series\nsupports this claim. The observed patterns in the price of stocks are modelled\nby adding a multiplicative exogenous term as the representative for external\nfactors to the geometric Brownian motion model for stock prices. Altogether, we\ncan detect and model the effects of the Great Recession as a consequence of the\nmortgage crisis in 2007/2008 as well as the impacts of the Covid out-break in\nearly 2020\n",
        "pdf_link": "http://arxiv.org/pdf/2205.06677v1"
    },
    {
        "title": "Optimizing Returns Using the Hurst Exponent and Q Learning on Momentum\n  and Mean Reversion Strategies",
        "authors": [
            "Y. Chang",
            "C. Lizardi",
            "R. Shah"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Momentum and mean reversion trading strategies have opposite characteristics.\nThe former is generally better with trending assets, and the latter is\ngenerally better with mean reverting assets. Using the Hurst exponent, which\nclassifies time series as trending or mean reverting, we attempt to trade with\neach strategy when it is advantageous to generate higher returns on average. We\nultimately find that trading with the Hurst exponent can achieve higher\nreturns, but it also comes at a higher risk. Finally, we consider limitations\nof our study and propose a method using Q-learning to improve our strategy and\nimplementation of individual algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.11122v1"
    },
    {
        "title": "Analysis of inter-transaction time fluctuations in the cryptocurrency\n  market",
        "authors": [
            "Jarosław Kwapień",
            "Marcin Wątorek",
            "Marija Bezbradica",
            "Martin Crane",
            "Tai Tan Mai",
            "Stanisław Drożdż"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We analyse tick-by-tick data representing major cryptocurrencies traded on\nsome different cryptocurrency trading platforms. We focus on such quantities\nlike the inter-transaction times, the number of transactions in time unit, the\ntraded volume, and volatility. We show that the inter-transaction times show\nlong-range power-law autocorrelations. These lead to multifractality expressed\nby the right-side asymmetry of the singularity spectra $f(\\alpha)$ indicating\nthat the periods of increased market activity are characterised by richer\nmultifractality compared to the periods of quiet market. We also show that\nneither the stretched exponential distribution nor the power-law-tail\ndistribution are able to model universally the cumulative distribution\nfunctions of the quantities considered in this work. For each quantity, some\ndata sets can be modeled by the former, some data sets by the latter, while\nboth fail in other cases. An interesting, yet difficult to account for,\nobservation is that parallel data sets from different trading platforms can\nshow disparate statistical properties.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.07831v2"
    },
    {
        "title": "Detection and Forecasting of Extreme event in Stock Price Triggered by\n  Fundamental, Technical, and External Factors",
        "authors": [
            "Anish Rai",
            "Salam Rabindrajit Luwang",
            "Md Nurujjaman",
            "Chittaranjan Hens",
            "Pratyay Kuila",
            "Kanish Debnath"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The sporadic large fluctuations are seen in the stock market due to changes\nin fundamental parameters, technical setups, and external factors. These large\nfluctuations are termed as Extreme Events (EE). The EEs may be positive or\nnegative depending on the impact of these factors. During such events, the\nstock price time series is found to be nonstationary. Hence, the Hilbert-Huang\ntransformation (HHT) is used to identify EEs based on their high instantaneous\nenergy ($IE$) concentration. The analysis shows that the $IE$ concentration in\nthe stock price is very high during both positive and negative EE with\n$IE>E_{\\mu}+4\\sigma,$ where $E_{\\mu}$ and $\\sigma$ are the mean energy and\nstandard deviation of energy, respectively. Further, support vector regression\nis used to predict the stock price during an EE, with the close price being the\nmost helpful input than the open-high-low-close (OHLC) inputs. The maximum\nprediction accuracy for one step using close price and OHLC prices are 95.98\\%\nand 95.64\\% respectively. Whereas, for the two steps prediction, the accuracies\nare 94.09\\% and 93.58\\% respectively. The EEs found from the predicted time\nseries shows similar statistical characteristics that were obtained from the\noriginal data. The analysis emphasizes the importance of monitoring factors\nthat lead to EEs for a compelling entry or exit strategy as investors can gain\nor lose significant amounts of capital due to these events.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13860v2"
    },
    {
        "title": "StockBot: Using LSTMs to Predict Stock Prices",
        "authors": [
            "Shaswat Mohanty",
            "Anirudh Vijay",
            "Nandagopan Gopakumar"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The evaluation of the financial markets to predict their behaviour have been\nattempted using a number of approaches, to make smart and profitable investment\ndecisions. Owing to the highly non-linear trends and inter-dependencies, it is\noften difficult to develop a statistical approach that elucidates the market\nbehaviour entirely. To this end, we present a long-short term memory (LSTM)\nbased model that leverages the sequential structure of the time-series data to\nprovide an accurate market forecast. We then develop a decision making StockBot\nthat buys/sells stocks at the end of the day with the goal of maximizing\nprofits. We successfully demonstrate an accurate prediction model, as a result\nof which our StockBot can outpace the market and can strategize for gains that\nare ~15 times higher than the most aggressive ETFs in the market.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.06605v2"
    },
    {
        "title": "Efficiency of the Moscow Stock Exchange before 2022",
        "authors": [
            "Andrey Shternshis",
            "Piero Mazzarisi",
            "Stefano Marmi"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  This paper investigates the degree of efficiency for the Moscow Stock\nExchange. A market is called efficient if prices of its assets fully reflect\nall available information. We show that the degree of market efficiency is\nsignificantly low for most of the months from 2012 to 2021. We calculate the\ndegree of market efficiency by (i) filtering out regularities in financial data\nand (ii) computing the Shannon entropy of the filtered return time series. We\nhave developed a simple method for estimating volatility and price staleness in\nempirical data, in order to filter out such regularity patterns from return\ntime series. The resulting financial time series of stocks' returns are then\nclustered into different groups according to some entropy measures. In\nparticular, we use the Kullback-Leibler distance and a novel entropy metric\ncapturing the co-movements between pairs of stocks. By using Monte Carlo\nsimulations, we are then able to identify the time periods of market\ninefficiency for a group of 18 stocks. The inefficiency of the Moscow Stock\nExchange that we have detected is a signal of the possibility of devising\nprofitable strategies, net of transaction costs. The deviation from the\nefficient behavior for a stock strongly depends on the industrial sector it\nbelongs.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.10476v2"
    },
    {
        "title": "Exploring Financial Networks Using Quantile Regression and Granger\n  Causality",
        "authors": [
            "Kara Karpman",
            "Samriddha Lahiry",
            "Diganta Mukherjee",
            "Sumanta Basu"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In the post-crisis era, financial regulators and policymakers are\nincreasingly interested in data-driven tools to measure systemic risk and to\nidentify systemically important firms. Granger Causality (GC) based techniques\nto build networks among financial firms using time series of their stock\nreturns have received significant attention in recent years. Existing GC\nnetwork methods model conditional means, and do not distinguish between\nconnectivity in lower and upper tails of the return distribution - an aspect\ncrucial for systemic risk analysis. We propose statistical methods that measure\nconnectivity in the financial sector using system-wide tail-based analysis and\nis able to distinguish between connectivity in lower and upper tails of the\nreturn distribution. This is achieved using bivariate and multivariate GC\nanalysis based on regular and Lasso penalized quantile regressions, an approach\nwe call quantile Granger causality (QGC). By considering centrality measures of\nthese financial networks, we can assess the build-up of systemic risk and\nidentify risk propagation channels. We provide an asymptotic theory of QGC\nestimators under a quantile vector autoregressive model, and show its benefit\nover regular GC analysis on simulated data. We apply our method to the monthly\nstock returns of large U.S. firms and demonstrate that lower tail based\nnetworks can detect systemically risky periods in historical data with higher\naccuracy than mean-based networks. In a similar analysis of large Indian banks,\nwe find that upper and lower tail networks convey different information and\nhave the potential to distinguish between periods of high connectivity that are\ngoverned by positive vs negative news in the market.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.10705v2"
    },
    {
        "title": "Identifying Dominant Industrial Sectors in Market States of the S&P 500\n  Financial Data",
        "authors": [
            "Tobias Wand",
            "Martin Heßler",
            "Oliver Kamps"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Understanding and forecasting changing market conditions in complex economic\nsystems like the financial market is of great importance to various\nstakeholders such as financial institutions and regulatory agencies. Based on\nthe finding that the dynamics of sector correlation matrices of the S&P 500\nstock market can be described by a sequence of distinct states via a clustering\nalgorithm, we try to identify the industrial sectors dominating the correlation\nstructure of each state. For this purpose, we use a method from Explainable\nArtificial Intelligence (XAI) on daily S&P 500 stock market data from 1992 to\n2012 to assign relevance scores to every feature of each data point. To compare\nthe significance of the features for the entire data set we develop an\naggregation procedure and apply a Bayesian change point analysis to identify\nthe most significant sector correlations. We show that the correlation matrix\nof each state is dominated only by a few sector correlations. Especially the\nenergy and IT sector are identified as key factors in determining the state of\nthe economy. Additionally we show that a reduced surrogate model, using only\nthe eight sector correlations with the highest XAI-relevance, can replicate 90%\nof the cluster assignments. In general our findings imply an additional\ndimension reduction of the dynamics of the financial market.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.14106v3"
    },
    {
        "title": "Asymptotic Normality for the Fourier spot volatility estimator in the\n  presence of microstructure noise",
        "authors": [
            "Maria Elvira Mancino",
            "Tommaso Mariotti",
            "Giacomo Toscano"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The main contribution of the paper is proving that the Fourier spot\nvolatility estimator introduced in [Malliavin and Mancino, 2002] is consistent\nand asymptotically efficient if the price process is contaminated by\nmicrostructure noise. Specifically, in the presence of additive microstructure\nnoise we prove a Central Limit Theorem with the optimal rate of convergence\n$n^{1/8}$. The result is obtained without the need for any manipulation of the\noriginal data or bias correction. Moreover, we complete the asymptotic theory\nfor the Fourier spot volatility estimator in the absence of noise, originally\npresented in [Mancino and Recchioni, 2015], by deriving a Central Limit Theorem\nwith the optimal convergence rate $n^{1/4}$. Finally, we propose a novel\nfeasible adaptive method for the optimal selection of the parameters involved\nin the implementation of the Fourier spot volatility estimator with noisy\nhigh-frequency data and provide support to its accuracy both numerically and\nempirically.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.08967v1"
    },
    {
        "title": "A Real Data-Driven Analytical Model to Predict Information Technology\n  Sector Index Price of S&P 500",
        "authors": [
            "Jayanta K. Pokharel",
            "Erasmus Tetteh-Bator",
            "Chris P. Tsokos"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  S&P 500 Index is one of the most sought after stock indices in the world. In\nparticular, Information Technology Sector of S&P 500 is the number one business\nsegment of the S&P 500 in terms of market capital, annual revenue and the\nnumber of companies (75) associated with it, and is one of the most attracting\nareas for many investors due to high percentage annual returns on investment\nover the years. A non-linear real data-driven analytical model is built to\npredict the Weekly Closing Price (WCP) of the Information Technology Sector\nIndex of S&P 500 using six financial, four economic indicators and their two\nway interactions as the attributable entities that drive the stock returns. We\nrank the statistically significant indicators and their interactions based on\nthe percentage of contribution to the $WCP$ of the Information Technology\nSector Index of the S&P 500 that provides significant information for the\nbeneficiary of the proposed predictive model. The model has the predictive\naccuracy of 99.4%, and the paper presents some intriguing findings and the\nmodel's usefulness.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.10720v1"
    },
    {
        "title": "Variance of entropy for testing time-varying regimes with an application\n  to meme stocks",
        "authors": [
            "Andrey Shternshis",
            "Piero Mazzarisi"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Shannon entropy is the most common metric to measure the degree of randomness\nof time series in many fields, ranging from physics and finance to medicine and\nbiology. Real-world systems may be in general non stationary, with an entropy\nvalue that is not constant in time. The goal of this paper is to propose a\nhypothesis testing procedure to test the null hypothesis of constant Shannon\nentropy for time series, against the alternative of a significant variation of\nthe entropy between two subsequent periods. To this end, we find an unbiased\napproximation of the variance of the Shannon entropy's estimator, up to the\norder O(n^(-4)) with n the sample size. In order to characterize the variance\nof the estimator, we first obtain the explicit formulas of the central moments\nfor both the binomial and the multinomial distributions, which describe the\ndistribution of the Shannon entropy. Second, we find the optimal length of the\nrolling window used for estimating the time-varying Shannon entropy by\noptimizing a novel self-consistent criterion based on the counting of\nsignificant variations of entropy within a time window. We corroborate our\nfindings by using the novel methodology to test for time-varying regimes of\nentropy for stock price dynamics, in particular considering the case of meme\nstocks in 2020 and 2021. We empirically show the existence of periods of market\ninefficiency for meme stocks. In particular, sharp increases of prices and\ntrading volumes correspond to statistically significant drops of Shannon\nentropy.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.05415v3"
    },
    {
        "title": "A Multimodal Embedding-Based Approach to Industry Classification in\n  Financial Markets",
        "authors": [
            "Rian Dolphin",
            "Barry Smyth",
            "Ruihai Dong"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Industry classification schemes provide a taxonomy for segmenting companies\nbased on their business activities. They are relied upon in industry and\nacademia as an integral component of many types of financial and economic\nanalysis. However, even modern classification schemes have failed to embrace\nthe era of big data and remain a largely subjective undertaking prone to\ninconsistency and misclassification. To address this, we propose a multimodal\nneural model for training company embeddings, which harnesses the dynamics of\nboth historical pricing data and financial news to learn objective company\nrepresentations that capture nuanced relationships. We explain our approach in\ndetail and highlight the utility of the embeddings through several case studies\nand application to the downstream task of industry classification.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.06378v1"
    },
    {
        "title": "Early Warning Signals for Cryptocurrency Market States",
        "authors": [
            "Vishwas Kukreti"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Being archetypal complex systems, financial markets exhibit rich set of\ndynamics in their interactions. In this paper, we focus on the recently evolved\ncryptocurrency market as an example of a complex system and analyse the\nevolution of cross correlation structure of cryptocurrencies in the 5 year\nperiod from 2017 to 2022. We observe characteristic correlation structures in\nthe observation time window duration and use these specific structures to\ncluster the cryptocurrency market in 4 market states.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.12356v1"
    },
    {
        "title": "An Intraday GARCH Model for Discrete Price Changes and Irregularly\n  Spaced Observations",
        "authors": [
            "Vladimír Holý"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We develop a novel observation-driven model for high-frequency prices. We\naccount for irregularly spaced observations, simultaneous transactions,\ndiscreteness of prices, and market microstructure noise. The relation between\ntrade durations and price volatility, as well as intraday patterns of trade\ndurations and price volatility, is captured using smoothing splines. The\ndynamic model is based on the zero-inflated Skellam distribution with\ntime-varying volatility in a score-driven framework. Market microstructure\nnoise is filtered by including a moving average component. The model is\nestimated by the maximum likelihood method. In an empirical study of the IBM\nstock, we demonstrate that the model provides a good fit to the data. Besides\nmodeling intraday volatility, it can also be used to measure daily realized\nvolatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.12376v4"
    },
    {
        "title": "Mechanism of information transmission from a spot rate market to\n  crypto-asset markets",
        "authors": [
            "Takeshi Yoshihara",
            "Taisei Kaizoji"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We applied the SVAR-LiNGAM to illustrate the causal relationships between the\nspot exchange rate, and three crypto-asset exchange rates, Bitcoin, Ethereum,\nand Ripple. It was notable that the causal order, the EUR_USD spot\nrate->Bitcoin->Ethereum->Ripple, was obtained by this approach. All the\ninstantaneous effects were strongly positive. Moreover, it was notable that\nBitcoin can influence the EUR_USD spot rate positively with a one-day time lag.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.16176v1"
    },
    {
        "title": "Using Intermarket Data to Evaluate the Efficient Market Hypothesis with\n  Machine Learning",
        "authors": [
            "N'yoma Diamond",
            "Grant Perkins"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In its semi-strong form, the Efficient Market Hypothesis (EMH) implies that\ntechnical analysis will not reveal any hidden statistical trends via\nintermarket data analysis. If technical analysis on intermarket data reveals\ntrends which can be leveraged to significantly outperform the stock market,\nthen the semi-strong EMH does not hold. In this work, we utilize a variety of\nmachine learning techniques to empirically evaluate the EMH using stock market,\nforeign currency (Forex), international government bond, index future, and\ncommodities future assets. We train five machine learning models on each\ndataset and analyze the average performance of these models for predicting the\ndirection of future S&P 500 movement as approximated by the SPDR S&P 500 Trust\nETF (SPY). From our analysis, the datasets containing bonds, index futures,\nand/or commodities futures data notably outperform baselines by substantial\nmargins. Further, we find that the usage of intermarket data induce\nstatistically significant positive impacts on the accuracy, macro F1 score,\nweighted F1 score, and area under receiver operating characteristic curve for a\nvariety of models at the 95% confidence level. This provides strong empirical\nevidence contradicting the semi-strong EMH.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.08734v2"
    },
    {
        "title": "Forecasting the Turkish Lira Exchange Rates through Univariate\n  Techniques: Can the Simple Models Outperform the Sophisticated Ones?",
        "authors": [
            "Mostafa R. Sarkandiz"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Throughout the past year, Turkey's central bank policy to decrease the\nnominal interest rate has caused episodes of severe fluctuations in Turkish\nlira exchange rates. According to these conditions, the daily return of the\nUSD/TRY have attracted the risk-taker investors' attention. Therefore, the\nuncertainty about the rates has pushed algorithmic traders toward finding the\nbest forecasting model. While there is a growing tendency to employ\nsophisticated models to forecast financial time series, in most cases, simple\nmodels can provide more precise forecasts. To examine that claim, present study\nhas utilized several models to predict daily exchange rates for a short\nhorizon. Interestingly, the simple exponential smoothing model outperformed all\nother alternatives. Besides, in contrast to the initial inferences, the time\nseries neither had structural break nor exhibited signs of the ARCH and\nleverage effects. Despite that behavior, there was undeniable evidence of a\nlong-memory trend. That means the series tends to keep a movement, at least for\na short period. Finally, the study concluded the simple models provide better\nforecasts for exchange rates than the complicated approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.08897v1"
    },
    {
        "title": "Revealing production networks from firm growth dynamics",
        "authors": [
            "Luca Mungo",
            "José Moran"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We study the correlation structure of firm growth rates. We show that most\nfirms are correlated because of their exposure to a common factor but that\nfirms linked through the supply chain exhibit a stronger correlation on average\nthan firms that are not. Removing this common factor significantly reduces the\naverage correlation between two firms with no relationship in the supply chain\nwhile maintaining a significant correlation between two firms that are linked.\nWe then investigate if this observation can be used to reconstruct the topology\nof a supply chain network using Gaussian Markov Models.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.09906v4"
    },
    {
        "title": "Monetary Policy, Digital Assets, and DeFi Activity",
        "authors": [
            "Antzelos Kyriazis",
            "Iason Ofeidis",
            "Georgios Palaiokrassas",
            "Leandros Tassiulas"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper studies the effects of unexpected changes in US monetary policy on\ndigital asset returns. We use event study regressions and find that monetary\npolicy surprises negatively affect BTC and ETH, the two largest digital assets,\nbut do not significantly affect the rest of the market. Second, we use\nhigh-frequency price data to examine the effect of the FOMC statements release\nand Minutes release on the prices of the assets with the higher collateral\nusage on the Ethereum Blockchain Decentralized Finance (DeFi) ecosystem. The\nFOMC statement release strongly affects the volatility of digital asset\nreturns, while the effect of the Minutes release is weaker. The volatility\neffect strengthened after December 2021, when the Federal Reserve changed its\npolicy to fight inflation. We also show that some borrowing interest rates in\nthe Ethereum DeFi ecosystem are affected positively by unexpected changes in\nmonetary policy. In contrast, the debt outstanding and the total value locked\nare negatively affected. Finally, we utilize a local Ethereum Blockchain node\nto record the activity history of primary DeFi functions, such as depositing,\nborrowing, and liquidating, and study how these are influenced by the FOMC\nannouncements over time.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.10252v1"
    },
    {
        "title": "Portfolio Volatility Estimation Relative to Stock Market Cross-Sectional\n  Intrinsic Entropy",
        "authors": [
            "Claudiu Vinte",
            "Marcel Ausloos"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Selecting stock portfolios and assessing their relative volatility risk\ncompared to the market as a whole, market indices, or other portfolios is of\ngreat importance to professional fund managers and individual investors alike.\nOur research uses the cross-sectional intrinsic entropy (CSIE) model to\nestimate the cross-sectional volatility of the stock groups that can be\nconsidered together as portfolio constituents. In our study, we benchmark\nportfolio volatility risks against the volatility of the entire market provided\nby the CSIE and the volatility of market indices computed using longitudinal\ndata. This article introduces CSIE-based betas to characterise the relative\nvolatility risk of the portfolio against market indices and the market as a\nwhole. We empirically prove that, through CSIE-based betas, multiple sets of\nsymbols that outperform the market indices in terms of rate of return while\nmaintaining the same level of risk or even lower than the one exhibited by the\nmarket index can be discovered, for any given time interval. These sets of\nsymbols can be used as constituent stock portfolios and, in connection with the\nperspective provided by the CSIE volatility estimates, to hierarchically assess\ntheir relative volatility risk within the broader context of the overall\nvolatility of the stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09330v1"
    },
    {
        "title": "A Deep Dive into NFT Whales: A Longitudinal Study of the NFT Trading\n  Ecosystem",
        "authors": [
            "Na Hyeon Park",
            "Hanna Kim",
            "Chanhee Lee",
            "Changhoon Yoon",
            "Seunghyeon Lee",
            "Youngjin jin",
            "Seungwon Shin"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  NFT (Non-fungible Token) has drastically increased in its size, accounting\nfor over \\$16.9B of total market capitalization. Despite the rapid growth of\nNFTs, this market has not been examined thoroughly from a financial\nperspective. In this paper, we conduct methodical analyses to identify NFT\nmarket movers who play a significant role in potentially manipulating and\noscillating NFT values. We collect over 3.8M NFT transaction data from the\nEthereum Blockchain from January 2021 to February 2022 to extract trading\ninformation in line with the NFT lifecycle: (i) mint, (ii) transfer/sale, and\n(iii) burn. Based on the size of held NFT values, we classify NFT traders into\nthree groups (whales, dolphins, and minnows). In total, we analyze 430K traders\nfrom 91 different NFT collection sources. We find that the top 0.1\\% of NFT\ntraders (i.e., whales) drive the NFT market with consistent, high returns. We\nthen identify and characterize the NFT whales' unique investment strategies\n(e.g., mint/sale patterns, wash trading) to empirically understand the whales\nin the NFT market for the first time.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09393v1"
    },
    {
        "title": "Predictive Optimized Model on Money Markets Instruments With Capital\n  Market and Bank Rates Ratio",
        "authors": [
            "Bilal Hungund",
            "Shilpa Rastogi"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The money market and the capital market of the Indian financial markets have\na symbiotic relationship in the development of the Indian economy. The nature\nand the characteristics of the markets differ to a large extent as the money\nmarket ensures liquidity in the system through the monetary policy by the\nregulators; capital markets propel and act as the engine driver for the economy\nin the long term. Therefore, the final throughput of the economy is the\naggregation of the output of both the markets. Does that imply that the\ndevelopment of both markets is parallel in nature or is any one superior to the\nother or are they competitors? To understand the influence of one over the\nother the research was undertaken through a correlation matrix and time series\nmodel. A predictive model was further constructed for predicting the volume of\nmoney market instrument on the basis of fourteen days historical.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10481v1"
    },
    {
        "title": "High-Frequency Volatility Estimation with Fast Multiple Change Points\n  Detection",
        "authors": [
            "Greeshma Balabhadra",
            "El Mehdi Ainasse",
            "Pawel Polak"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We propose a method for constructing sparse high-frequency volatility\nestimators that are robust against change points in the spot volatility\nprocess. The estimators we propose are $\\ell_1$-regularized versions of\nexisting volatility estimators. We focus on power variation estimators as they\nrepresent a fundamental class of volatility estimators. We establish\nconsistency of these estimators for the true unobserved volatility and the\nchange points locations, showing that minimax rates can be achieved for\nparticular volatility estimators. The new estimators utilize the\ncomputationally efficient least angle regression algorithm for estimation\npurposes, followed by a reduced dynamic programming step to refine the final\nnumber of change points. In terms of numerical performance, these estimators\nare not only computationally fast but also accurately identify breakpoints near\nthe end of the sample, both features highly desirable in today's electronic\ntrading environment. In terms of out-of-sample volatility prediction, our new\nestimators provide more realistic and smoother volatility forecasts,\noutperforming a broad range of classical and recent volatility estimators\nacross various frequencies and forecasting horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10550v3"
    },
    {
        "title": "On the Connection between Temperature and Volatility in Ideal Agent\n  Systems",
        "authors": [
            "Christoph J. Börner",
            "Ingo Hoffmann",
            "John H. Stiebel"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Models for spin systems known from statistical physics are applied by analogy\nin econometrics in the form of agent-based models. Researchers suggest that the\nstate variable temperature $T$ corresponds to volatility $\\sigma$ in capital\nmarket theory problems. To the best of our knowledge, this has not yet been\ntheoretically derived, for example, for an ideal agent system. In the present\npaper, we derive the exact algebraic relation between $T$ and $\\sigma$ for an\nideal agent system and discuss implications and limitations.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.15164v1"
    },
    {
        "title": "Dynamical properties of volume at the spread in the Bitcoin/USD market",
        "authors": [
            "Roberto Mota Navarro",
            "Francois Leyvraz",
            "Hernán Larralde"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The study of order volumes in financial markets has shown that these display\nseveral non-trivial statistical properties. Most studies have been focused on\nthe bulk properties of volume of incoming orders or of realized transactions\nrather than the dynamical aspects. The present work is a study of the dynamical\nproperties of volume. Unlike previous works, we studied the volume available at\nthe spread rather than the volume of incoming orders or of realized\ntransactions. We found evidence that suggests mean reverting volume changes and\nstrong asymmetries in the equilibrium of sell and buy orders as well as the\npresence of clustering.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.01907v4"
    },
    {
        "title": "Structured Multifractal Scaling of the Principal Cryptocurrencies:\n  Examination using a Self-Explainable Machine Learning",
        "authors": [
            "Foued Saâdaoui"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Multifractal analysis is a forecasting technique used to study the scaling\nregularity properties of financial returns, to analyze the long-term memory and\npredictability of financial markets. In this paper, we propose a novel\nstructural detrended multifractal fluctuation analysis (S-MF-DFA) to\ninvestigate the efficiency of the main cryptocurrencies. The new methodology\ngeneralizes the conventional approach by allowing it to proceed on the\ndifferent fluctuation regimes previously determined using a change-points\ndetection test. In this framework, the characterization of the various\nexogenous factors influencing the scaling behavior is performed on the basis of\na single-factor model, thus creating a kind of self-explainable machine\nlearning for price forecasting. The proposal is tested on the daily data of the\nthree among the main cryptocurrencies in order to examine whether the digital\nmarket has experienced upheavals in recent years and whether this has in some\nways led to a structured multifractal behavior. The sampled period ranges from\nApril 2017 to December 2022. We especially detect common periods of local\nscaling for the three prices with a decreasing multifractality after 2018.\nComplementary tests on shuffled and surrogate data prove that the distribution,\nlinear correlation, and nonlinear structure also explain at some level the\nstructural multifractality. Finally, prediction experiments based on neural\nnetworks fed with multi-fractionally differentiated data show the interest of\nthis new self-explained algorithm, thus giving decision-makers and investors\nthe ability to use it for more accurate and interpretable forecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.08440v1"
    },
    {
        "title": "Collective dynamics, diversification and optimal portfolio construction\n  for cryptocurrencies",
        "authors": [
            "Nick James",
            "Max Menzies"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Since its conception, the cryptocurrency market has been frequently described\nas an immature market, characterized by significant swings in volatility and\noccasionally described as lacking rhyme or reason. There has been great\nspeculation as to what role it plays in a diversified portfolio. For instance,\nis cryptocurrency exposure an inflationary hedge or a speculative investment\nthat follows broad market sentiment with amplified beta? We have recently\nexplored similar questions with a clear focus on the equity market. There, our\nresearch revealed several noteworthy dynamics such as: an increase in the\nmarket's collective strength and uniformity during crises, greater\ndiversification benefits across equity sectors (rather than within them), and\nthe existence of a \"best value\" portfolio of equities. In essence, we can now\ncontrast any potential signatures of maturity we identify in the cryptocurrency\nmarket and contrast these with the substantially larger, older and better\nestablished equity market. This paper aims to investigate whether the\ncryptocurrency market has recently exhibited similar mathematical properties as\nthe equity market. Instead of relying on traditional portfolio theory, which is\ngrounded in the financial dynamics of equity securities, we adjust our\nexperimental focus to capture the presumed behavioral purchasing patterns of\nretail cryptocurrency investors. Our focus is on collective dynamics and\nportfolio diversification in the cryptocurrency market, and examining whether\npreviously established results in the equity market hold in the cryptocurrency\nmarket, and to what extent. Results reveal nuanced signatures of maturity\nrelated to the equity market, including the fact that correlations collectively\nspike around exchange collapses, and identify an ideal portfolio size and\nspread across different groups of cryptocurrencies.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.08902v2"
    },
    {
        "title": "Study on the Identification of Financial Risk Path Under the Digital\n  Transformation of Enterprise Based on DEMATEL-ISM-MICMAC",
        "authors": [
            "Jie Dong"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Digital transformation challenges financial management while reducing costs\nand increasing efficiency for enterprises in various countries. Identifying the\ntransmission paths of enterprise financial risks in the context of digital\ntransformation is an urgent problem to be solved. This paper constructs a\nsystem of influencing factors of corporate financial risks in the new era\nthrough literature research. It proposes a path identification method of\nfinancial risks in the context of the digital transformation of enterprises\nbased on DEMATEL-ISM-MICMAC. This paper explores the intrinsic association\namong the influencing factors of corporate financial risks, identifies the key\ninfluencing factors, sorts out the hierarchical structure of the influencing\nfactor system, and analyses the dependency and driving relationships among the\nfactors in this system. The results show that: (1) The political and economic\nenvironment being not optimistic will limit the enterprise's operating ability,\nthus directly leading to the change of the enterprise's asset and liability\nstructure and working capital stock. (2) The enterprise's unreasonable talent\ntraining and incentive mechanism will limit the enterprise's technological\ninnovation ability and cause a shortage of digitally literate financial\ntalents, which eventually leads to the vulnerability of the enterprise's\nfinancial management. This study provides a theoretical reference for\nenterprises to develop risk management strategies and ideas for future academic\nresearch in digital finance.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04216v1"
    },
    {
        "title": "An Empirical Study of Capital Asset Pricing Model based on Chinese\n  A-share Trading Data",
        "authors": [
            "Kai Ren"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper presents an empirical analysis of the capital asset pricing model\nusing trading data for the Chinese A-share market from 2000 to 2019. Firstly,\nthe standard CAPM is tested using a Fama-MacBetch regression and although the\nresults successfully test the three core hypotheses, the resulting beta risk\ndoes not have a significant impact on returns. Secondly, the Fama-French\nthree-factor model, which uses a combination of market, size and value factors\nto price capital assets, is analysed, showing that it is able to capture most\nof the variation in A-share market returns, with an adjusted R-squared greater\nthan 0.88 for the 25 portfolios constructed. Finally, the paper takes into\naccount the \"shell value contamination\" problem caused by IPO regulation in the\nChinese stock market, and minimises its impact by excluding stocks in the\nlowest 30% of the market capitalisation, which allows some of the anomalous\nresults in the three-factor model to be effectively corrected. Although this\npaper does not present a more innovative approach, it is unique in its data\nselection and presents a detailed presentation of the data processing and\nregression analysis process, which 1) illustrates the applicability of capital\nasset pricing models in the Chinese market; and 2) provides a set of open\nsource materials for the basic learning of capital asset pricing models.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04838v1"
    },
    {
        "title": "Estimating the impact of supply chain network contagion on financial\n  stability",
        "authors": [
            "Zlata Tabachová",
            "Christian Diem",
            "András Borsos",
            "Csaba Burger",
            "Stefan Thurner"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Realistic credit risk assessment, the estimation of losses from\ncounterparty's failure, is central for the financial stability. Credit risk\nmodels focus on the financial conditions of borrowers and only marginally\nconsider other risks from the real economy, supply chains in particular. Recent\npandemics, geopolitical instabilities, and natural disasters demonstrated that\nsupply chain shocks do contribute to large financial losses. Based on a unique\nnation-wide micro-dataset, containing practically all supply chain relations of\nall Hungarian firms, together with their bank loans, we estimate how\nfirm-failures affect the supply chain network, leading to potentially\nadditional firm defaults and additional financial losses. Within a multi-layer\nnetwork framework we define a financial systemic risk index (FSRI) for every\nfirm, quantifying these expected financial losses caused by its own- and all\nthe secondary defaulting loans caused by supply chain network (SCN) shock\npropagation. We find a small fraction of firms carrying substantial financial\nsystemic risk, affecting up to 16% of the banking system's overall equity.\nThese losses are predominantly caused by SCN contagion. For every bank we\ncalculate the expected loss (EL), value at risk (VaR) and expected shortfall\n(ES), with and without accounting for SCN contagion. We find that SCN contagion\namplifies the EL, VaR, and ES by a factor of 4.3, 4.5, and 3.2, respectively.\nThese findings indicate that for a more complete picture of financial stability\nand realistic credit risk assessment, SCN contagion needs to be considered.\nThis newly quantified contagion channel is of potential relevance for\nregulators' future systemic risk assessments.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04865v1"
    },
    {
        "title": "Financial sentiment analysis using FinBERT with application in\n  predicting stock movement",
        "authors": [
            "Tingsong Jiang",
            "Andy Zeng"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We apply sentiment analysis in financial context using FinBERT, and build a\ndeep neural network model based on LSTM to predict the movement of financial\nmarket movement. We apply this model on stock news dataset, and compare its\neffectiveness to BERT, LSTM and classical ARIMA model. We find that sentiment\nis an effective factor in predicting market movement. We also propose several\nmethod to improve the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.02136v1"
    },
    {
        "title": "Deep Attentive Survival Analysis in Limit Order Books: Estimating Fill\n  Probabilities with Convolutional-Transformers",
        "authors": [
            "Alvaro Arroyo",
            "Alvaro Cartea",
            "Fernando Moreno-Pino",
            "Stefan Zohren"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  One of the key decisions in execution strategies is the choice between a\npassive (liquidity providing) or an aggressive (liquidity taking) order to\nexecute a trade in a limit order book (LOB). Essential to this choice is the\nfill probability of a passive limit order placed in the LOB. This paper\nproposes a deep learning method to estimate the filltimes of limit orders\nposted in different levels of the LOB. We develop a novel model for survival\nanalysis that maps time-varying features of the LOB to the distribution of\nfilltimes of limit orders. Our method is based on a convolutional-Transformer\nencoder and a monotonic neural network decoder. We use proper scoring rules to\ncompare our method with other approaches in survival analysis, and perform an\ninterpretability analysis to understand the informativeness of features used to\ncompute fill probabilities. Our method significantly outperforms those\ntypically used in survival analysis literature. Finally, we carry out a\nstatistical analysis of the fill probability of orders placed in the order book\n(e.g., within the bid-ask spread) for assets with different queue dynamics and\ntrading activity.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05479v1"
    },
    {
        "title": "Testing for intrinsic multifractality in the global grain spot market\n  indices: A multifractal detrended fluctuation analysis",
        "authors": [
            "Li Wang",
            "Xing-Lu Gao",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Grains account for more than 50% of the calories consumed by people\nworldwide, and military conflicts, pandemics, climate change, and soaring grain\nprices all have vital impacts on food security. However, the complex price\nbehavior of the global grain spot markets has not been well understood. A\nrecent study performed multifractal moving average analysis (MF-DMA) of the\nGrains & Oilseeds Index (GOI) and its sub-indices of wheat, maize, soyabeans,\nrice, and barley and found that only the maize and barley sub-indices exhibit\nan intrinsic multifractal nature with convincing evidence. Here, we utilize\nmultifractal fluctuation analysis (MF-DFA) to investigate the same problem.\nExtensive statistical tests confirm the presence of intrinsic multifractality\nin the maize and barley sub-indices and the absence of intrinsic\nmultifractality in the wheat and rice sub-indices. Different from the MF-DMA\nresults, the MF-DFA results suggest that there is also intrinsic\nmultifractality in the GOI and soyabeans sub-indices. Our comparative analysis\ndoes not provide conclusive information about the GOI and soyabeans and\nhighlights the high complexity of the global grain spot markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.10496v1"
    },
    {
        "title": "Successive one-sided Hodrick-Prescott filter with incremental filtering\n  algorithm for nonlinear economic time series",
        "authors": [
            "Yuxia Liu",
            "Qi Zhang",
            "Wei Xiao",
            "Tianguang Chu"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We propose a successive one-sided Hodrick-Prescott (SOHP) filter from\nmultiple time scale decomposition perspective to derive trend estimate for a\ntime series. The idea is to apply the one-sided HP (OHP) filter recursively on\nthe updated cyclical component to extract the trend residual on multiple time\nscales, thereby to improve the trend estimate. To address the issue of\noptimization with a moving horizon as that of the SOHP filter, we present an\nincremental HP filtering algorithm, which greatly simplifies the involved\ninverse matrix operation and reduces the computational demand of the basic HP\nfiltering. Actually, the new algorithm also applies effectively to other\nHP-type filters, especially for large-size or expanding data scenario.\nNumerical examples on real economic data show the better performance of the\nSOHP filter in comparison with other known HP-type filters.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12439v1"
    },
    {
        "title": "Analysis of Indian foreign exchange markets: A Multifractal Detrended\n  Fluctuation Analysis (MFDFA) approach",
        "authors": [
            "R. P. Datta"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The multifractal spectra of daily foreign exchange rates for US dollar (USD),\nthe British Pound (GBP), the Euro (Euro) and the Japanese Yen (Yen) with\nrespect to the Indian Rupee are analysed for the period 6th January 1999 to\n24th July 2018. We observe that the time series of logarithmic returns of all\nthe four exchange rates exhibit features of multifractality. Next, we research\nthe source of the observed multifractality. For this, we transform the return\nseries in two ways: a) We randomly shuffle the original time series of\nlogarithmic returns and b) We apply the process of phase randomisation on the\nunchanged series. Our results indicate in the case of the US dollar the source\nof multifractality is mainly the fat tail. For the British Pound and the Euro,\nwe see the long-range correlations between the observations and the thick tails\nof the probability distribution give rise to the observed multifractal\nfeatures, while in the case of the Japanese Yen, the origin of the multifractal\nnature of the return series is mostly due to the broad tail.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.16162v1"
    },
    {
        "title": "The Chebyshev Polynomials Of The First Kind For Analysis Rates Shares Of\n  Enterprises",
        "authors": [
            "Sergey Yekimov"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Chebyshev polynomials of the first kind have long been used to approximate\nexperimental data in solving various technical problems. Within the framework\nof this study, the dynamics of shares of eight Czech enterprises was analyzed\nby the Chebyshev polynomial decomposition: CEZ A.S. (CEZP), Colt CZ Group SE\n(CZG), Erste Bank (ERST), Komercni Banka (BKOM), Moneta Money Bank A.S.\n(MONET), Photon (PENP), Vienna insurance group (VIGR) in 2021. An investor,\nwhen making a decision to purchase a security , is guided largely by an\nheuristic approach . And variance and correlation are not observed by human\nsenses. The vectors of decomposition of time series of exchange values of\nsecurities allow analyzing the dynamics of exchange values of securities more\neffectively if their dynamics does not correspond to the normal distribution\nlaw. The proposed model allows analyzing the dynamics of the exchange value of\na securities portfolio without calculating variance and correlation. This model\ncan be useful if the dynamics of the exchange values of securities does not\nobey, due to certain circumstances, the normal law of distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08465v1"
    },
    {
        "title": "Bayesian Forecasting of Stock Returns on the JSE using Simultaneous\n  Graphical Dynamic Linear Models",
        "authors": [
            "Nelson Kyakutwika",
            "Bruce Bartlett"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Cross-series dependencies are crucial in obtaining accurate forecasts when\nforecasting a multivariate time series. Simultaneous Graphical Dynamic Linear\nModels (SGDLMs) are Bayesian models that elegantly capture cross-series\ndependencies. This study forecasts returns of a 40-dimensional time series of\nstock data from the Johannesburg Stock Exchange (JSE) using SGDLMs. The SGDLM\napproach involves constructing a customised dynamic linear model (DLM) for each\nunivariate time series. At each time point, the DLMs are recoupled using\nimportance sampling and decoupled using mean-field variational Bayes. Our\nresults suggest that SGDLMs forecast stock data on the JSE accurately and\nrespond to market gyrations effectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08665v1"
    },
    {
        "title": "An exploration of the mathematical structure and behavioural biases of\n  21st century financial crises",
        "authors": [
            "Nick James",
            "Max Menzies"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In this paper we contrast the dynamics of the 2022 Ukraine invasion financial\ncrisis with notable financial crises of the 21st century - the dot-com bubble,\nglobal financial crisis and COVID-19. We study the similarity in market\ndynamics and associated implications for equity investors between various\nfinancial market crises and we introduce new mathematical techniques to do so.\nFirst, we study the strength of collective dynamics during different market\ncrises, and compare suitable portfolio diversification strategies with respect\nto the unique number of sectors and stocks for optimal systematic risk\nreduction. Next, we introduce a new linear operator method to quantify\ndistributional distance between equity returns during various crises. Our\nmethod allows us to fairly compare underlying stock and sector performance\nduring different time periods, normalising for those collective dynamics driven\nby the overall market. Finally, we introduce a new combinatorial portfolio\noptimisation framework driven by random sampling to investigate whether\nparticular equities and equity sectors are more effective in maximising\ninvestor risk-adjusted returns during market crises.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.15402v2"
    },
    {
        "title": "Shifting Cryptocurrency Influence: A High-Resolution Network Analysis of\n  Market Leaders",
        "authors": [
            "Arnav Hiray",
            "Pratvi Shah",
            "Vishwa Shah",
            "Agam Shah",
            "Sudheer Chava",
            "Mukesh Tiwari"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Over the last decade, the cryptocurrency market has experienced unprecedented\ngrowth, emerging as a prominent financial market. As this market rapidly\nevolves, it necessitates re-evaluating which cryptocurrencies command the\nmarket and steer the direction of blockchain technology. We implement a\nnetwork-based cryptocurrency market analysis to investigate this changing\nlandscape. We use novel hourly-resolution data and Kendall's Tau correlation to\nexplore the interconnectedness of the cryptocurrency market. We observed\ncritical differences in the hierarchy of cryptocurrencies determined by our\nmethod compared to rankings derived from daily data and Pearson's correlation.\nThis divergence emphasizes the potential information loss stemming from daily\ndata aggregation and highlights the limitations of Pearson's correlation. Our\nfindings show that in the early stages of this growth, Bitcoin held a leading\nrole. However, during the 2021 bull run, the landscape changed drastically. We\nsee that while Ethereum has emerged as the overall leader, it was FTT and its\nassociated exchange, FTX, that greatly led to the increase at the beginning of\nthe bull run. We also find that highly-influential cryptocurrencies are\nincreasingly gaining a commanding influence over the market as time progresses,\ndespite the growing number of cryptocurrencies making up the market.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.16874v4"
    },
    {
        "title": "Regularity in forex returns during financial distress: Evidence from\n  India",
        "authors": [
            "Radhika Prosad Datta"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper uses the concepts of entropy to study the regularity/irregularity\nof the returns from the Indian Foreign exchange (forex) markets. The\nApproximate Entropy and Sample Entropy statistics which measure the level of\nrepeatability in the data are used to quantify the randomness in the forex\nreturns from the time period 2006 to 2021. The main objective of the research\nis to see how the randomness of the foreign exchange returns evolve over the\ngiven time period particularly during periods of high financial instability or\nturbulence in the global financial market. With this objective we look at 2\nmajor financial upheavals, the subprime crisis also known as the Global\nFinancial Crisis (GFC) during 2006-2007 and the recent Covid-19 pandemic during\n2020-2021. Our empirical results overwhelmingly confirm our working hypothesis\nthat regularity in the returns of the major Indian foreign exchange rates\nincreases during times of financial crisis. This is evidenced by a decrease in\nthe values of the sample entropy and approximate entropy before and\nafter/during the financial crisis period for the majority of the exchange\nrates. Our empirical results also show that Sample Entropy is a better measure\nof regularity than Approximate Entropy for the Indian forex rates which is in\nagreement with the theoretical predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04181v1"
    },
    {
        "title": "Chance or Chaos? Fractal geometry aimed to inspect the nature of Bitcoin",
        "authors": [
            "Esther Cabezas-Rivas",
            "Felipe Sánchez-Coll",
            "Isaac Tormo-Xaixo"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The aim of this paper is to analyse the Bitcoin in order to shed some light\non its nature and behaviour. We select 9 cryptocurrencies that account for\nalmost 75\\% of total market capitalisation and compare their evolution with\nthat of a wide variety of traditional assets: commodities with spot and futures\ncontracts, treasury bonds, stock indices, growth and value stocks. Fractal\ngeometry will be applied to carry out a careful statistical analysis of the\nperformance of the Bitcoin returns. As a main conclusion, we have detected a\nhigh degree of persistence in its prices, which decreases the efficiency but\nincreases its predictability. Moreover, we observe that the underlying\ntechnology influences price dynamics, with fully decentralised cryptocurrencies\nbeing the only ones to exhibit self-similarity features at any time scale.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.00390v1"
    },
    {
        "title": "On statistical arbitrage under a conditional factor model of equity\n  returns",
        "authors": [
            "Trent Spears",
            "Stefan Zohren",
            "Stephen Roberts"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We consider a conditional factor model for a multivariate portfolio of United\nStates equities in the context of analysing a statistical arbitrage trading\nstrategy. A state space framework underlies the factor model whereby asset\nreturns are assumed to be a noisy observation of a linear combination of factor\nvalues and latent factor risk premia. Filter and state prediction estimates for\nthe risk premia are retrieved in an online way. Such estimates induce filtered\nasset returns that can be compared to measurement observations, with large\ndeviations representing candidate mean reversion trades. Further, in that the\nrisk premia are modelled as time-varying quantities, non-stationarity in\nreturns is de facto captured. We study an empirical trading strategy respectful\nof transaction costs, and demonstrate performance over a long history of 29\nyears, for both a linear and a non-linear state space model. Our results show\nthat the model is competitive relative to the results of other methods,\nincluding simple benchmarks and other cutting-edge approaches as published in\nthe literature. Also of note, while strategy performance degradation is noticed\nthrough time -- especially for the most recent years -- the strategy continues\nto offer compelling economics, and has scope for further advancement.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.02205v1"
    },
    {
        "title": "Dynamic Time Warping for Lead-Lag Relationships in Lagged Multi-Factor\n  Models",
        "authors": [
            "Yichi Zhang",
            "Mihai Cucuringu",
            "Alexander Y. Shestopaloff",
            "Stefan Zohren"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In multivariate time series systems, lead-lag relationships reveal\ndependencies between time series when they are shifted in time relative to each\nother. Uncovering such relationships is valuable in downstream tasks, such as\ncontrol, forecasting, and clustering. By understanding the temporal\ndependencies between different time series, one can better comprehend the\ncomplex interactions and patterns within the system. We develop a\ncluster-driven methodology based on dynamic time warping for robust detection\nof lead-lag relationships in lagged multi-factor models. We establish\nconnections to the multireference alignment problem for both the homogeneous\nand heterogeneous settings. Since multivariate time series are ubiquitous in a\nwide range of domains, we demonstrate that our algorithm is able to robustly\ndetect lead-lag relationships in financial markets, which can be subsequently\nleveraged in trading strategies with significant economic benefits.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08800v1"
    },
    {
        "title": "Econometric Model Using Arbitrage Pricing Theory and Quantile Regression\n  to Estimate the Risk Factors Driving Crude Oil Returns",
        "authors": [
            "Sarit Maitra",
            "Vivek Mishra",
            "Sukanya Kundu",
            "Manav Chopra"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This work adopts a novel approach to determine the risk and return of crude\noil stocks by employing Arbitrage Pricing Theory (APT) and Quantile Regression\n(QR).The APT identifies the underlying risk factors likely to impact crude oil\nreturns.Subsequently, QR estimates the relationship between the factors and the\nreturns across different quantiles of the distribution. The West Texas\nIntermediate (WTI) crude oil price is used in this study as a benchmark for\ncrude oil prices. WTI price fluctuations can have a significant impact on the\nperformance of crude oil stocks and, subsequently, the global economy.To\ndetermine the proposed models stability, various statistical measures are used\nin this study.The results show that changes in WTI returns can have varying\neffects depending on market conditions and levels of volatility. The study\nhighlights the impact of structural discontinuities on returns, which can be\ncaused by changes in the global economy and the demand for crude oil.The\ninclusion of pandemic, geopolitical, and inflation-related explanatory\nvariables add uniqueness to this study as it considers current global events\nthat can affect crude oil returns.Findings show that the key factors that pose\nmajor risks to returns are industrial production, inflation, the global price\nof energy, the shape of the yield curve, and global economic policy\nuncertainty.This implies that while making investing decisions in WTI futures,\ninvestors should pay particular attention to these elements\n",
        "pdf_link": "http://arxiv.org/pdf/2309.13096v2"
    },
    {
        "title": "Stock Volatility Prediction Based on Transformer Model Using\n  Mixed-Frequency Data",
        "authors": [
            "Wenting Liu",
            "Zhaozhong Gui",
            "Guilin Jiang",
            "Lihua Tang",
            "Lichun Zhou",
            "Wan Leng",
            "Xulong Zhang",
            "Yujiang Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  With the increasing volume of high-frequency data in the information age,\nboth challenges and opportunities arise in the prediction of stock volatility.\nOn one hand, the outcome of prediction using tradition method combining stock\ntechnical and macroeconomic indicators still leaves room for improvement; on\nthe other hand, macroeconomic indicators and peoples' search record on those\nsearch engines affecting their interested topics will intuitively have an\nimpact on the stock volatility. For the convenience of assessment of the\ninfluence of these indicators, macroeconomic indicators and stock technical\nindicators are then grouped into objective factors, while Baidu search indices\nimplying people's interested topics are defined as subjective factors. To align\ndifferent frequency data, we introduce GARCH-MIDAS model. After mixing all the\nabove data, we then feed them into Transformer model as part of the training\ndata. Our experiments show that this model outperforms the baselines in terms\nof mean square error. The adaption of both types of data under Transformer\nmodel significantly reduces the mean square error from 1.00 to 0.86.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.16196v1"
    },
    {
        "title": "Covariance matrix filtering and portfolio optimisation: the Average\n  Oracle vs Non-Linear Shrinkage and all the variants of DCC-NLS",
        "authors": [
            "Christian Bongiorno",
            "Damien Challet"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The Average Oracle, a simple and very fast covariance filtering method, is\nshown to yield superior Sharpe ratios than the current state-of-the-art (and\ncomplex) methods, Dynamic Conditional Covariance coupled to Non-Linear\nShrinkage (DCC+NLS). We pit all the known variants of DCC+NLS (quadratic\nshrinkage, gross-leverage or turnover limitations, and factor-augmented NLS)\nagainst the Average Oracle in large-scale randomized experiments. We find\ngenerically that while some variants of DCC+NLS sometimes yield the lowest\naverage realized volatility, albeit with a small improvement, their excessive\ngross leverage and investment concentration, and their 10-time larger turnover\ncontribute to smaller average portfolio returns, which mechanically result in\nsmaller realized Sharpe ratios than the Average Oracle. We also provide simple\nanalytical arguments about the origin of the advantage of the Average Oracle\nover NLS in a changing world.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.17219v1"
    },
    {
        "title": "Study of Stylized Facts in Stock Market Data",
        "authors": [
            "Vaibhav Sherkar",
            "Rituparna Sen"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  A property of data which is common across a wide range of instruments,\nmarkets and time periods is known as stylized empirical fact in the financial\nstatistics literature. This paper first presents a wide range of stylized facts\nstudied in literature which include some univariate distributional properties,\nmultivariate properties and time series related properties of the financial\ntime series data. In the next part of the paper, price data from several stocks\nlisted on 10 stock exchanges spread across different continents has been\nanalysed and data analysis has been presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00753v1"
    },
    {
        "title": "Bitcoin versus S&P 500 Index: Return and Risk Analysis",
        "authors": [
            "A. H. Nzokem"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The S&P 500 index is considered the most popular trading instrument in\nfinancial markets. With the rise of cryptocurrencies over the past years,\nBitcoin has also grown in popularity and adoption. The paper aims to analyze\nthe daily return distribution of the Bitcoin and S&P 500 index and assess their\ntail probabilities through two financial risk measures. As a methodology, We\nuse Bitcoin and S&P 500 Index daily return data to fit The seven-parameter\nGeneral Tempered Stable (GTS) distribution using the advanced Fast Fractional\nFourier transform (FRFT) scheme developed by combining the Fast Fractional\nFourier (FRFT) algorithm and the 12-point rule Composite Newton-Cotes\nQuadrature. The findings show that peakedness is the main characteristic of the\nS&P 500 return distribution, whereas heavy-tailedness is the main\ncharacteristic of the Bitcoin return distribution. The GTS distribution shows\nthat $80.05\\%$ of S&P 500 returns are within $-1.06\\%$ and $1.23\\%$ against\nonly $40.32\\%$ of Bitcoin returns. At a risk level ($\\alpha$), the severity of\nthe loss ($AVaR_{\\alpha}(X)$) on the left side of the distribution is larger\nthan the severity of the profit ($AVaR_{1-\\alpha}(X)$) on the right side of the\ndistribution. Compared to the S&P 500 index, Bitcoin has $39.73\\%$ more\nprevalence to produce high daily returns (more than $1.23\\%$ or less than\n$-1.06\\%$). The severity analysis shows that at a risk level ($\\alpha$) the\naverage value-at-risk ($AVaR(X)$) of the bitcoin returns at one significant\nfigure is four times larger than that of the S&P 500 index returns at the same\nrisk.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02436v1"
    },
    {
        "title": "Linkages among the Foreign Exchange, Stock, and Bond Markets in Japan\n  and the United States",
        "authors": [
            "Yi Jiang",
            "Shohei Shimizu"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  While economic theory explains the linkages among the financial markets of\ndifferent countries, empirical studies mainly verify the linkages through\nGranger causality, without considering latent variables or instantaneous\neffects. Their findings are inconsistent regarding the existence of causal\nlinkages among financial markets, which might be attributed to differences in\nthe focused markets, data periods, and methods applied. Our study adopts causal\ndiscovery methods including VAR-LiNGAM and LPCMCI with domain knowledge to\nexplore the linkages among financial markets in Japan and the United States\n(US) for the post Covid-19 pandemic period under divergent monetary policy\ndirections. The VAR-LiNGAM results reveal that the previous day's US market\ninfluences the following day's Japanese market for both stocks and bonds, and\nthe bond markets of the previous day impact the following day's foreign\nexchange (FX) market directly and the following day's Japanese stock market\nindirectly. The LPCMCI results indicate the existence of potential latent\nconfounders. Our results demonstrate that VAR-LiNGAM uniquely identifies the\ndirected acyclic graph (DAG), and thus provides informative insight into the\ncausal relationship when the assumptions are considered valid. Our study\ncontributes to a better understanding of the linkages among financial markets\nin the analyzed data period by supporting the existence of linkages between\nJapan and the US for the same financial markets and among FX, stock, and bond\nmarkets, thus highlighting the importance of leveraging causal discovery\nmethods in the financial domain.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.16841v1"
    },
    {
        "title": "Stock Market Directional Bias Prediction Using ML Algorithms",
        "authors": [
            "Ryan Chipwanya"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The stock market has been established since the 13th century, but in the\ncurrent epoch of time, it is substantially more practicable to anticipate the\nstock market than it was at any other point in time due to the tools and data\nthat are available for both traditional and algorithmic trading. There are many\ndifferent machine learning models that can do time-series forecasting in the\ncontext of machine learning. These models can be used to anticipate the future\nprices of assets and/or the directional bias of assets. In this study, we\nexamine and contrast the effectiveness of three different machine learning\nalgorithms, namely, logistic regression, decision tree, and random forest to\nforecast the movement of the assets traded on the Japanese stock market. In\naddition, the models are compared to a feed forward deep neural network, and it\nis found that all of the models consistently reach above 50% in directional\nbias forecasting for the stock market. The results of our study contribute to a\nbetter understanding of the complexity involved in stock market forecasting and\ngive insight on the possible role that machine learning could play in this\ncontext.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.16855v1"
    },
    {
        "title": "Potential of ChatGPT in predicting stock market trends based on Twitter\n  Sentiment Analysis",
        "authors": [
            "Ummara Mumtaz",
            "Summaya Mumtaz"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The rise of ChatGPT has brought a notable shift to the AI sector, with its\nexceptional conversational skills and deep grasp of language. Recognizing its\nvalue across different areas, our study investigates ChatGPT's capacity to\npredict stock market movements using only social media tweets and sentiment\nanalysis. We aim to see if ChatGPT can tap into the vast sentiment data on\nplatforms like Twitter to offer insightful predictions about stock trends. We\nfocus on determining if a tweet has a positive, negative, or neutral effect on\ntwo big tech giants Microsoft and Google's stock value. Our findings highlight\na positive link between ChatGPT's evaluations and the following days stock\nresults for both tech companies. This research enriches our view on ChatGPT's\nadaptability and emphasizes the growing importance of AI in shaping financial\nmarket forecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06273v1"
    },
    {
        "title": "A Modeling Approach of Return and Volatility of Structured Investment\n  Products with Caps and Floors",
        "authors": [
            "Jiaer He",
            "Roberto Rivera"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Popular investment structured products in Puerto Rico are stock market tied\nIndividual Retirement Accounts (IRA), which offer some stock market growth\nwhile protecting the principal. The performance of these retirement strategies\nhas not been studied. This work examines the expected return and risk of Puerto\nRico stock market IRA (PRIRAs) and compares their statistical properties with\nother investment instruments before and after tax. We propose a parametric\nmodeling approach for structured products and apply it to PRIRAs. Our method\nfirst estimates the conditional expected return (and variance) of PRIRA assets\nfrom which we extract marginal moments through the Law of Iterated Expectation.\nOur results indicate that PRIRAs underperform against investing directly in the\nstock market while still carrying substantial risk. The expected return of the\nstock market IRA from Popular Bank (PRIRA1) after tax is slightly greater than\nthat of investing in U.S. bonds, while PRIRA1 has almost two times the risk.\nThe stock market IRA from Universal (PRIRA2) performs similarly to PRIRA1,\nwhile PRIRA2 has a lower risk than PRIRA1. PRIRAs may be reasonable for some\nrisk-averse investors due to their principal protection and tax deferral.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06282v1"
    },
    {
        "title": "Application Research of Spline Interpolation and ARIMA in the Field of\n  Stock Market Forecasting",
        "authors": [
            "Xitai Yu"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The ARIMA (Autoregressive Integrated Moving Average model) has extensive\napplications in the field of time series forecasting. However, the predictive\nperformance of the ARIMA model is limited when dealing with data gaps or\nsignificant noise. Based on previous research, we have found that cubic spline\ninterpolation performs well in capturing the smooth changes of stock price\ncurves, especially when the market trends are relatively stable. Therefore,\nthis paper integrates the two approaches by taking the time series data in\nstock trading as an example, establishes a time series forecasting model based\non cubic spline interpolation and ARIMA. Through validation, the model has\ndemonstrated certain guidance and reference value for short-term time series\nforecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.10759v1"
    },
    {
        "title": "Measure of Dependence for Financial Time-Series",
        "authors": [
            "Martin Winistörfer",
            "Ivan Zhdankin"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Assessing the predictive power of both data and models holds paramount\nsignificance in time-series machine learning applications. Yet, preparing time\nseries data accurately and employing an appropriate measure for predictive\npower seems to be a non-trivial task. This work involves reviewing and\nestablishing the groundwork for a comprehensive analysis of shaping time-series\ndata and evaluating various measures of dependence. Lastly, we present a\nmethod, framework, and a concrete example for selecting and evaluating a\nsuitable measure of dependence.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.12129v1"
    },
    {
        "title": "The Impact Of Interest Rates On Firms Financial Decisions",
        "authors": [
            " Efendi",
            "Rahmadani Srifitri",
            "Septriza Berliana"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Financial decisions are the decisions that managers take with regard to the\nfinances of a company. This article aims to examine and explain the effect of\ninterest rates on economic and financial decisions such as investment, funding,\nand dividend in a firm. This research uses the correlation coefficient analysis\nmethods and descriptive methods to illustrate the relationship between interest\nrates and financial decisions. The data used in this research was obtained from\nseveral government reports and leading economic sources. The results of this\nresearch show that interest rates have a negatively insignificant effect on\ninvestment and funding decisions, but positively moderate effect on dividend\ndecisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14738v1"
    },
    {
        "title": "Rough volatility: evidence from range volatility estimators",
        "authors": [
            "Saad Mouti"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In Gatheral et al. 2018, first posted in 2014, volatility is characterized by\nfractional behavior with a Hurst exponent $H < 0.5$, challenging traditional\nviews of volatility dynamics. Gatheral et al. demonstrated this using realized\nvolatility measurements. Our study extends this analysis by employing\nrange-based proxies to confirm their findings across a broader dataset and\nnon-standard assets. Notably, we address the concern that rough volatility\nmight be an artifact of microstructure noise in high-frequency return data. Our\nresults reveal that log-volatility, estimated via range-based methods, behaves\nakin to fractional Brownian motion with an even lower $H$, below $0.1$. We also\naffirm the efficacy of the rough fractional stochastic volatility model (RFSV),\nfinding that its predictive capability surpasses that of AR, HAR, and GARCH\nmodels in most scenarios. This work substantiates the intrinsic nature of rough\nvolatility, independent of the microstructure noise often present in\nhigh-frequency financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.01426v2"
    },
    {
        "title": "An explanation for the distribution characteristics of stock returns",
        "authors": [
            "Bo Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Observations indicate that the distributions of stock returns in financial\nmarkets usually do not conform to normal distributions, but rather exhibit\ncharacteristics of high peaks, fat tails and biases. In this work, we assume\nthat the effects of events or information on prices obey normal distribution,\nwhile financial markets often overreact or underreact to events or information,\nresulting in non normal distributions of stock returns. Based on the above\nassumptions, we propose a reaction function for a financial market reacting to\nevents or information, and a model based on it to describe the distribution of\nreal stock returns. Our analysis of the returns of China Securities Index 300\n(CSI 300), the Standard & Poor's 500 Index (SPX or S&P 500) and the Nikkei 225\nIndex (N225) at different time scales shows that financial markets often\nunderreact to events or information with minor impacts, overreact to events or\ninformation with relatively significant impacts, and react slightly stronger to\npositive events or information than to negative ones. In addition, differences\nin financial markets and time scales of returns can also affect the shapes of\nthe reaction functions.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.02472v1"
    },
    {
        "title": "A Decadal Analysis of the Lead-Lag Effect in the NYSE",
        "authors": [
            "Aarush Pratik Sheth",
            "Jonah Riley Weinbaum",
            "Kevin Javier Zvonarek"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  As is widely known, the stock market is a complex system in which a multitude\nof factors influence the performance of individual stocks and the market as a\nwhole. One method for comprehending -- and potentially predicting -- stock\nmarket behavior is through network analysis, which can offer insights into the\nrelationships between stocks and the overall market structure. In this paper,\nwe seek to address the question: Can network analysis of the stock market,\nspecifically in observation of the lead-lag effect, provide valuable insights\nfor investors and market analysts?\n",
        "pdf_link": "http://arxiv.org/pdf/2312.10084v1"
    },
    {
        "title": "Causal Discovery in Financial Markets: A Framework for Nonstationary\n  Time-Series Data",
        "authors": [
            "Agathe Sadeghi",
            "Achintya Gopal",
            "Mohammad Fesanghary"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper introduces a new causal structure learning method for\nnonstationary time series data, a common data type found in fields such as\nfinance, economics, healthcare, and environmental science. Our work builds upon\nthe constraint-based causal discovery from nonstationary data algorithm\n(CD-NOD). We introduce a refined version (CD-NOTS) which is designed\nspecifically to account for lagged dependencies in time series data. We compare\nthe performance of different algorithmic choices, such as the type of\nconditional independence test and the significance level, to help select the\nbest hyperparameters given various scenarios of sample size, problem\ndimensionality, and availability of computational resources. Using the results\nfrom the simulated data, we apply CD-NOTS to a broad range of real-world\nfinancial applications in order to identify causal connections among\nnonstationary time series data, thereby illustrating applications in\nfactor-based investing, portfolio diversification, and comprehension of market\ndynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.17375v3"
    },
    {
        "title": "Multiple-bubble testing in the cryptocurrency market: a case study of\n  bitcoin",
        "authors": [
            "Sanaz Behzadi",
            "Mahmonir Bayanati",
            "Hamed Nozari"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Economic periods and financial crises have highlighted the importance of\nevaluating financial markets to investors and researchers in recent decades.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05417v1"
    },
    {
        "title": "Introduction of L0 norm and application of L1 and C1 norm in the study\n  of time-series",
        "authors": [
            "Victor Ujaldon Garcia"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Four markets are considered: Cryptocurrencies / South American exchange rate\n/ Spanish Banking indices and European Indices and studied using TDA\n(Topological Data Analysis) tools. These tools are used to predict and showcase\nboth strengths and weakness of the current TDA tools. In this paper a new tool\n$L0$ norm is defined and complemented with the already existing $C1$ norm.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05423v1"
    },
    {
        "title": "A closer look at the chemical potential of an ideal agent system",
        "authors": [
            "Christoph J. Börner",
            "Ingo Hoffmann",
            "John H. Stiebel"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Models for spin systems known from statistical physics are used in\neconometrics in the form of agent-based models. Econophysics research in\neconometrics is increasingly developing general market models that describe\nexchange phenomena and use the chemical potential $\\mu$ known from physics in\nthe context of particle number changes. In statistical physics, equations of\nstate are known for the chemical potential, which take into account the\nrespective model framework and the corresponding state variables. A simple\ntransfer of these equations of state to problems in econophysics appears\ndifficult. To the best of our knowledge, the equation of state for the chemical\npotential is currently missing even for the simplest conceivable model of an\nideal agent system. In this paper, this research gap is closed and the equation\nof state for the chemical potential is derived from the econophysical model\nassumptions of the ideal agent system. An interpretation of the equation of\nstate leads to fundamental relationships that could also have been guessed, but\nare shown here by the theory.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.09233v1"
    },
    {
        "title": "FNSPID: A Comprehensive Financial News Dataset in Time Series",
        "authors": [
            "Zihan Dong",
            "Xinyu Fan",
            "Zhiyuan Peng"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Financial market predictions utilize historical data to anticipate future\nstock prices and market trends. Traditionally, these predictions have focused\non the statistical analysis of quantitative factors, such as stock prices,\ntrading volumes, inflation rates, and changes in industrial production. Recent\nadvancements in large language models motivate the integrated financial\nanalysis of both sentiment data, particularly market news, and numerical\nfactors. Nonetheless, this methodology frequently encounters constraints due to\nthe paucity of extensive datasets that amalgamate both quantitative and\nqualitative sentiment analyses. To address this challenge, we introduce a\nlarge-scale financial dataset, namely, Financial News and Stock Price\nIntegration Dataset (FNSPID). It comprises 29.7 million stock prices and 15.7\nmillion time-aligned financial news records for 4,775 S&P500 companies,\ncovering the period from 1999 to 2023, sourced from 4 stock market news\nwebsites. We demonstrate that FNSPID excels existing stock market datasets in\nscale and diversity while uniquely incorporating sentiment information. Through\nfinancial analysis experiments on FNSPID, we propose: (1) the dataset's size\nand quality significantly boost market prediction accuracy; (2) adding\nsentiment scores modestly enhances performance on the transformer-based model;\n(3) a reproducible procedure that can update the dataset. Completed work, code,\ndocumentation, and examples are available at github.com/Zdong104/FNSPID. FNSPID\noffers unprecedented opportunities for the financial research community to\nadvance predictive modeling and analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06698v1"
    },
    {
        "title": "Contagion on Financial Networks: An Introduction",
        "authors": [
            "Sunday Akukodi Ugwu"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This mini-project models propagation of shocks, in time point, through links\nin connected banks. In particular, financial network of 100 banks out of which\n15 are shocked to default (that is, 85.00% of the banks are solvent) is\nmodelled using Erdos and Renyi network -- directed, weighted and randomly\ngenerated network. Shocking some banks in a financial network implies removing\ntheir assets and redistributing their liabilities to other connected ones in\nthe network. The banks are nodes and two ranges of probability values determine\ntendency of having a link between a pair of banks. Our major finding shows that\nthe ranges of probability values and banks' percentage solvency have positive\ncorrelation.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.08071v2"
    },
    {
        "title": "Quantifying neural network uncertainty under volatility clustering",
        "authors": [
            "Steven Y. K. Wong",
            "Jennifer S. K. Chan",
            "Lamiae Azizi"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Time-series with volatility clustering pose a unique challenge to uncertainty\nquantification (UQ) for returns forecasts. Methods for UQ such as Deep\nEvidential regression offer a simple way of quantifying return forecast\nuncertainty without the costs of a full Bayesian treatment. However, the\nNormal-Inverse-Gamma (NIG) prior adopted by Deep Evidential regression is prone\nto miscalibration as the NIG prior is assigned to latent mean and variance\nparameters in a hierarchical structure. Moreover, it also overparameterizes the\nmarginal data distribution. These limitations may affect the accurate\ndelineation of epistemic (model) and aleatoric (data) uncertainties. We propose\na Scale Mixture Distribution as a simpler alternative which can provide\nfavorable complexity-accuracy trade-off and assign separate subnetworks to each\nmodel parameter. To illustrate the performance of our proposed method, we apply\nit to two sets of financial time-series exhibiting volatility clustering:\ncryptocurrencies and U.S. equities and test the performance in some ablation\nstudies.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.14476v2"
    },
    {
        "title": "Dimensionality reduction techniques to support insider trading detection",
        "authors": [
            "Adele Ravagnani",
            "Fabrizio Lillo",
            "Paola Deriu",
            "Piero Mazzarisi",
            "Francesca Medda",
            "Antonio Russo"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Identification of market abuse is an extremely complicated activity that\nrequires the analysis of large and complex datasets. We propose an unsupervised\nmachine learning method for contextual anomaly detection, which allows to\nsupport market surveillance aimed at identifying potential insider trading\nactivities. This method lies in the reconstruction-based paradigm and employs\nprincipal component analysis and autoencoders as dimensionality reduction\ntechniques. The only input of this method is the trading position of each\ninvestor active on the asset for which we have a price sensitive event (PSE).\nAfter determining reconstruction errors related to the trading profiles,\nseveral conditions are imposed in order to identify investors whose behavior\ncould be suspicious of insider trading related to the PSE. As a case study, we\napply our method to investor resolved data of Italian stocks around takeover\nbids.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00707v2"
    },
    {
        "title": "From Factor Models to Deep Learning: Machine Learning in Reshaping\n  Empirical Asset Pricing",
        "authors": [
            "Junyi Ye",
            "Bhaskar Goswami",
            "Jingyi Gu",
            "Ajim Uddin",
            "Guiling Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper comprehensively reviews the application of machine learning (ML)\nand AI in finance, specifically in the context of asset pricing. It starts by\nsummarizing the traditional asset pricing models and examining their\nlimitations in capturing the complexities of financial markets. It explores how\n1) ML models, including supervised, unsupervised, semi-supervised, and\nreinforcement learning, provide versatile frameworks to address these\ncomplexities, and 2) the incorporation of advanced ML algorithms into\ntraditional financial models enhances return prediction and portfolio\noptimization. These methods can adapt to changing market dynamics by modeling\nstructural changes and incorporating heterogeneous data sources, such as text\nand images. In addition, this paper explores challenges in applying ML in asset\npricing, addressing the growing demand for explainability in decision-making\nand mitigating overfitting in complex models. This paper aims to provide\ninsights into novel methodologies showcasing the potential of ML to reshape the\nfuture of quantitative finance.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06779v1"
    },
    {
        "title": "Advanced Statistical Arbitrage with Reinforcement Learning",
        "authors": [
            "Boming Ning",
            "Kiseop Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Statistical arbitrage is a prevalent trading strategy which takes advantage\nof mean reverse property of spread of paired stocks. Studies on this strategy\noften rely heavily on model assumption. In this study, we introduce an\ninnovative model-free and reinforcement learning based framework for\nstatistical arbitrage. For the construction of mean reversion spreads, we\nestablish an empirical reversion time metric and optimize asset coefficients by\nminimizing this empirical mean reversion time. In the trading phase, we employ\na reinforcement learning framework to identify the optimal mean reversion\nstrategy. Diverging from traditional mean reversion strategies that primarily\nfocus on price deviations from a long-term mean, our methodology creatively\nconstructs the state space to encapsulate the recent trends in price movements.\nAdditionally, the reward function is carefully tailored to reflect the unique\ncharacteristics of mean reversion trading.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.12180v1"
    },
    {
        "title": "Nonlinear shifts and dislocations in financial market structure and\n  composition",
        "authors": [
            "Nick James",
            "Max Menzies"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper develops new mathematical techniques to identify temporal shifts\namong a collection of US equities partitioned into a new and more detailed set\nof market sectors. Although conceptually related, our three analyses reveal\ndistinct insights about financial markets, with meaningful implications for\ninvestment managers. First, we explore a variety of methods to identify\nnonlinear shifts in market sector structure and describe the mathematical\nconnection between the measure used and the captured phenomena. Second, we\nstudy network structure with respect to our new market sectors and identify\nmeaningfully connected sector-to-sector mappings. Finally, we conduct a series\nof sampling experiments over different sample spaces and contrast the\ndistribution of Sharpe ratios produced by long-only, long-short and short-only\ninvestment portfolios. In addition, we examine the sector composition of the\ntop-performing portfolios for each of these portfolio styles. In practice, the\nmethods proposed in this paper could be used to identify regime shifts,\noptimally structured portfolios, and better communities of equities.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.15163v2"
    },
    {
        "title": "Revisiting Elastic String Models of Forward Interest Rates",
        "authors": [
            "Victor Le Coz",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Twenty five years ago, several authors proposed to describe the forward\ninterest rate curve (FRC) as an elastic string along which idiosyncratic shocks\npropagate, accounting for the peculiar structure of the return correlation\nacross different maturities. In this paper, we revisit the specific \"stiff''\nelastic string field theory of Baaquie and Bouchaud (2004) in a way that makes\nits micro-foundation more transparent. Our model can be interpreted as\ncapturing the effect of market forces that set the rates of nearby tenors in a\nself-referential fashion. The model is parsimonious and accurately reproduces\nthe whole correlation structure of the FRC over the time period 1994-2023, with\nan error around 1% and with only one adjustable parameter, the value of which\nbeing very stable across the last three decades. The dependence of correlation\non time resolution (also called the Epps effect) is also faithfully reproduced\nwithin the model and leads to a cross-tenor information propagation time on the\norder of 30 minutes. Finally, we confirm that the perceived time in interest\nrate markets is a strongly sub-linear function of real time, as surmised by\nBaaquie and Bouchaud (2004). In fact, our results are fully compatible with\nhyperbolic discounting, in line with the recent behavioral Finance literature\n(Farmer and Geanakoplos, 2009).\n",
        "pdf_link": "http://arxiv.org/pdf/2403.18126v2"
    },
    {
        "title": "Non-stationary Financial Risk Factors and Macroeconomic Vulnerability\n  for the UK",
        "authors": [
            "Katalin Varga",
            "Tibor Szendrei"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Tracking the build-up of financial vulnerabilities is a key component of\nfinancial stability policy. Due to the complexity of the financial system, this\ntask is daunting, and there have been several proposals on how to manage this\ngoal. One way to do this is by the creation of indices that act as a signal for\nthe policy maker. While factor modelling in finance and economics has a rich\nhistory, most of the applications tend to focus on stationary factors.\nNevertheless, financial stress (and in particular tail events) can exhibit a\nhigh degree of inertia. This paper advocates moving away from the stationary\nparadigm and instead proposes non-stationary factor models as measures of\nfinancial stress. Key advantage of a non-stationary factor model is that while\nsome popular measures of financial stress describe the variance-covariance\nstructure of the financial stress indicators, the new index can capture the\ntails of the distribution. To showcase this, we use the obtained factors as\nvariables in a growth-at-risk exercise. This paper offers an overview of how to\nconstruct non-stationary dynamic factors of financial stress using the UK\nfinancial market as an example.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.01451v1"
    },
    {
        "title": "A theoretical framework for dynamical fee choice in AMMs",
        "authors": [
            "Abe Alexander",
            "Lars Fritz"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In the ever evolving landscape of decentralized finance automated market\nmakers (AMMs) play a key role: they provide a market place for trading assets\nin a decentralized manner. For so-called bluechip pairs, arbitrage activity\nprovides a major part of the revenue generation of AMMs but also a major source\nof loss due to the so-called informed orderflow. Finding ways to minimize those\nlosses while still keeping uninformed trading activity alive is a major problem\nin the field. In this paper we will investigate the mechanics of said arbitrage\nand try to understand how AMMs can maximize the revenue creation or in other\nwords minimize the losses. To that end, we model the dynamics of arbitrage\nactivity for a concrete implementation of a pool and study its sensitivity to\nthe choice of fee aiming to maximize the value retention. We manage to map the\nensuing dynamics to that of a random walk with a specific reward scheme that\nprovides a convenient starting point for further studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.03976v3"
    },
    {
        "title": "Generalization of the Alpha-Stable Distribution with the Degree of\n  Freedom",
        "authors": [
            "Stephen H. Lihn"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  A Wright function based framework is proposed to combine and extend several\ndistribution families. The $\\alpha$-stable distribution is generalized by\nadding the degree of freedom parameter. The PDF of this two-sided super\ndistribution family subsumes those of the original $\\alpha$-stable, Student's t\ndistributions, as well as the exponential power distribution and the modified\nBessel function of the second kind. Its CDF leads to a fractional extension of\nthe Gauss hypergeometric function. The degree of freedom makes possible for\nvalid variance, skewness, and kurtosis, just like Student's t. The original\n$\\alpha$-stable distribution is viewed as having one degree of freedom, that\nexplains why it lacks most of the moments. A skew-Gaussian kernel is derived\nfrom the characteristic function of the $\\alpha$-stable law, which maximally\npreserves the law in the new framework. To facilitate such framework, the\nstable count distribution is generalized as the fractional extension of the\ngeneralized gamma distribution. It provides rich subordination capabilities,\none of which is the fractional $\\chi$ distribution that supplies the needed\n'degree of freedom' parameter. Hence, the \"new\" $\\alpha$-stable distribution is\na \"ratio distribution\" of the skew-Gaussian kernel and the fractional $\\chi$\ndistribution. Mathematically, it is a new form of higher transcendental\nfunction under the Wright function family. Last, the new univariate symmetric\ndistribution is extended to the multivariate elliptical distribution\nsuccessfully.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.04693v1"
    },
    {
        "title": "High-Frequency Stock Market Order Transitions during the US-China Trade\n  War 2018: A Discrete-Time Markov Chain Analysis",
        "authors": [
            "Salam Rabindrajit Luwang",
            "Anish Rai",
            "Md. Nurujjaman",
            "Om Prakash",
            "Chittaranjan Hens"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Statistical analysis of high-frequency stock market order transaction data is\nconducted to understand order transition dynamics. We employ a first-order\ntime-homogeneous discrete-time Markov chain model to the sequence of orders of\nstocks belonging to six different sectors during the USA-China trade war of\n2018. The Markov property of the order sequence is validated by the Chi-square\ntest. We estimate the transition probability matrix of the sequence using\nmaximum likelihood estimation. From the heat-map of these matrices, we found\nthe presence of active participation by different types of traders during high\nvolatility days. On such days, these traders place limit orders primarily with\nthe intention of deleting the majority of them to influence the market. These\nfindings are supported by high stationary distribution and low mean recurrence\nvalues of add and delete orders. Further, we found similar spectral gap and\nentropy rate values, which indicates that similar trading strategies are\nemployed on both high and low volatility days during the trade war. Among all\nthe sectors considered in this study, we observe that there is a recurring\npattern of full execution orders in Finance & Banking sector. This shows that\nthe banking stocks are resilient during the trade war. Hence, this study may be\nuseful in understanding stock market order dynamics and devise trading\nstrategies accordingly on high and low volatility days during extreme\nmacroeconomic events.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.05634v1"
    },
    {
        "title": "Complex network analysis of cryptocurrency market during crashes",
        "authors": [
            "Kundan Mukhia",
            "Anish Rai",
            "SR Luwang",
            "Md Nurujjaman",
            "Sushovan Majhi",
            "Chittaranjan Hens"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper identifies the cryptocurrency market crashes and analyses its\ndynamics using the complex network. We identify three distinct crashes during\n2017-20, and the analysis is carried out by dividing the time series into\npre-crash, crash, and post-crash periods. Partial correlation based complex\nnetwork analysis is carried out to study the crashes. Degree density\n($\\rho_D$), average path length ($\\bar{l}$), and average clustering coefficient\n($\\overline{cc}$) are estimated from these networks. We find that both $\\rho_D$\nand $\\overline{cc}$ are smallest during the pre-crash period, and spike during\nthe crash suggesting the network is dense during a crash. Although $\\rho_D$ and\n$\\overline{cc}$ decrease in the post-crash period, they remain higher than\npre-crash levels for the 2017-18 and 2018-19 crashes suggesting a market\nattempt to return to normalcy. We get $\\bar{l}$ is minimal during the crash\nperiod, suggesting a rapid flow of information. A dense network and rapid\ninformation flow suggest that during a crash uninformed synchronized panic\nsell-off happens. However, during the 2019-20 crash, the values of $\\rho_D$,\n$\\overline{cc}$, and $\\bar{l}$ did not vary significantly, indicating minimal\nchange in dynamics compared to other crashes. The findings of this study may\nguide investors in making decisions during market crashes.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.05642v1"
    },
    {
        "title": "\"Microstructure Modes\" -- Disentangling the Joint Dynamics of Prices &\n  Order Flow",
        "authors": [
            "Salma Elomari-Kessab",
            "Guillaume Maitrier",
            "Julius Bonart",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Understanding the micro-dynamics of asset prices in modern electronic order\nbooks is crucial for investors and regulators. In this paper, we use an order\nby order Eurostoxx database spanning over 3 years to analyze the joint dynamics\nof prices and order flow. In order to alleviate various problems caused by\nhigh-frequency noise, we propose a double coarse-graining procedure that allows\nus to extract meaningful information at the minute time scale. We use Principal\nComponent Analysis to construct \"microstructure modes\" that describe the most\ncommon flow/return patterns and allow one to separate them into bid-ask\nsymmetric and bid-ask anti-symmetric. We define and calibrate a Vector\nAuto-Regressive (VAR) model that encodes the dynamical evolution of these\nmodes. The parameters of the VAR model are found to be extremely stable in\ntime, and lead to relatively high $R^2$ prediction scores, especially for\nsymmetric liquidity modes. The VAR model becomes marginally unstable as more\nlags are included, reflecting the long-memory nature of flows and giving some\nfurther credence to the possibility of \"endogenous liquidity crises\". Although\nvery satisfactory on several counts, we show that our VAR framework does not\naccount for the well known square-root law of price impact.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10654v1"
    },
    {
        "title": "The puzzle of Carbon Allowance spread",
        "authors": [
            "Michele Azzone",
            "Roberto Baviera",
            "Pietro Manzoni"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  A growing number of contributions in the literature have identified a puzzle\nin the European carbon allowance (EUA) market. Specifically, a persistent\ncost-of-carry spread (C-spread) over the risk-free rate has been observed. We\nare the first to explain the anomalous C-spread with the credit spread of the\ncorporates involved in the emission trading scheme. We obtain statistical\nevidence that the C-spread is cointegrated with both this credit spread and the\nrisk-free interest rate. This finding has a relevant policy implication: the\nmost effective solution to solve the market anomaly is including the EUA in the\nlist of European Central Bank eligible collateral for refinancing operations.\nThis change in the ECB monetary policy operations would greatly benefit the\ncarbon market and the EU green transition.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12982v1"
    },
    {
        "title": "A novel portfolio construction strategy based on the core-periphery\n  profile of stocks",
        "authors": [
            "Imran Ansari",
            "Charu Sharma",
            "Akshay Agrawal",
            "Niteesh Sahni"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper highlights the significance of mesoscale structures, particularly\nthe core-periphery structure, in financial networks for portfolio optimization.\nWe build portfolios of stocks belonging to the periphery part of the Planar\nmaximally filtered subgraphs of the underlying network of stocks created from\nPearson correlations between pairs of stocks and compare its performance with\nsome well-known strategies of Pozzi et. al. hinging around the local indices of\ncentrality in terms of the Sharpe ratio, returns and standard deviation. Our\nfindings reveal that these portfolios consistently outperform traditional\nstrategies and further the core-periphery profile obtained is statistically\nsignificant across time periods. These empirical findings substantiate the\nefficacy of using the core-periphery profile of the stock market network for\nboth inter-day and intraday trading and provide valuable insights for investors\nseeking better returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12993v1"
    },
    {
        "title": "Decision Trees for Intuitive Intraday Trading Strategies",
        "authors": [
            "Prajwal Naga",
            "Dinesh Balivada",
            "Sharath Chandra Nirmala",
            "Poornoday Tiruveedi"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This research paper aims to investigate the efficacy of decision trees in\nconstructing intraday trading strategies using existing technical indicators\nfor individual equities in the NIFTY50 index. Unlike conventional methods that\nrely on a fixed set of rules based on combinations of technical indicators\ndeveloped by a human trader through their analysis, the proposed approach\nleverages decision trees to create unique trading rules for each stock,\npotentially enhancing trading performance and saving time. By extensively\nbacktesting the strategy for each stock, a trader can determine whether to\nemploy the rules generated by the decision tree for that specific stock. While\nthis method does not guarantee success for every stock, decision treebased\nstrategies outperform the simple buy-and-hold strategy for many stocks. The\nresults highlight the proficiency of decision trees as a valuable tool for\nenhancing intraday trading performance on a stock-by-stock basis and could be\nof interest to traders seeking to improve their trading strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.13959v1"
    },
    {
        "title": "Investigating the price determinants of the European Emission Trading\n  System: a non-parametric approach",
        "authors": [
            "Cristiano Salvagnin",
            "Aldo Glielmo",
            "Maria Elena De Giuli",
            "Antonietta Mira"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The European carbon market plays a pivotal role in the European Union's\nambitious target of achieving carbon neutrality by 2050. Understanding the\nintricacies of factors influencing European Union Emission Trading System (EU\nETS) market prices is paramount for effective policy making and strategy\nimplementation. We propose the use of the Information Imbalance, a recently\nintroduced non-parametric measure quantifying the degree to which a set of\nvariables is informative with respect to another one, to study the\nrelationships among macroeconomic, economic, uncertainty, and energy variables\nconcerning EU ETS prices. Our analysis shows that in Phase 3 commodity related\nvariables such as the ERIX index are the most informative to explain the\nbehaviour of the EU ETS market price. Transitioning to Phase 4, financial\nfluctuations take centre stage, with the uncertainty in the EUR/CHF exchange\nrate emerging as a crucial determinant. These results reflect the disruptive\nimpacts of the COVID-19 pandemic and the energy crisis in reshaping the\nimportance of the different variables. Beyond variable analysis, we also\npropose to leverage the Information Imbalance to address the problem of\nmixed-frequency forecasting, and we identify the weekly time scale as the most\ninformative for predicting the EU ETS price. Finally, we show how the\nInformation Imbalance can be effectively combined with Gaussian Process\nregression for efficient nowcasting and forecasting using very small sets of\nhighly informative predictors.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.05094v1"
    },
    {
        "title": "The Theory of Intrinsic Time: A Primer",
        "authors": [
            "James B. Glattfelder",
            "Richard B. Olsen"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The concept of time mostly plays a subordinate role in finance and economics.\nThe assumption is that time flows continuously and that time series data should\nbe analyzed at regular, equidistant intervals. Nonetheless, already nearly 60\nyears ago, the concept of an event-based measure of time was first introduced.\nThis paper expands on this theme by discussing the paradigm of intrinsic time,\nits origins, history, and modern applications. Departing from traditional,\ncontinuous measures of time, intrinsic time proposes an event-based,\nalgorithmic framework that captures the dynamic and fluctuating nature of\nreal-world phenomena more accurately. Unsuspected implications arise in general\nfor complex systems and specifically for financial markets. For instance, novel\nstructures and regularities are revealed, otherwise obscured by any analysis\nutilizing equidistant time intervals. Of particular interest is the emergence\nof a multiplicity of scaling laws, a hallmark signature of an underlying\norganizational principle in complex systems. Moreover, a central insight from\nthis novel paradigm is the realization that universal time does not exist;\ninstead, time is observer-dependent, shaped by the intrinsic activity unfolding\nwithin complex systems. This research opens up new avenues for economic\nmodeling and forecasting, paving the way for a deeper understanding of the\ninvisible forces that guide the evolution and emergence of market dynamics and\nfinancial systems. An exciting and rich landscape of possibilities emerges\nwithin the paradigm of intrinsic time.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.07354v1"
    },
    {
        "title": "Dissecting Multifractal detrended cross-correlation analysis",
        "authors": [
            "Borko Stosic",
            "Tatijana Stosic"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In this work we address the question of the Multifractal detrended\ncross-correlation analysis method that has been subject to some controversies\nsince its inception almost two decades ago. To this end we propose several new\noptions to deal with negative cross-covariance among two time series, that may\nserve to construct a more robust view of the multifractal spectrum among the\nseries. We compare these novel options with the proposals already existing in\nthe literature, and we provide fast code in C, R and Python for both new and\nthe already existing proposals. We test different algorithms on synthetic\nseries with an exact analytical solution, as well as on daily price series of\nethanol and sugar in Brazil from 2010 to 2023.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19406v1"
    },
    {
        "title": "Modeling a Financial System with Memory via Fractional Calculus and\n  Fractional Brownian Motion",
        "authors": [
            "Patrick Geraghty"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Financial markets have long since been modeled using stochastic methods such\nas Brownian motion, and more recently, rough volatility models have been built\nusing fractional Brownian motion. This fractional aspect brings memory into the\nsystem. In this project, we describe and analyze a financial model based on the\nfractional Langevin equation with colored noise generated by fractional\nBrownian motion. Physics-based methods of analysis are used to examine the\nphase behavior and dispersion relations of the system upon varying input\nparameters. A type of anomalous marginal glass phase is potentially seen in\nsome regions, which motivates further exploration of this model and expanded\nuse of phase behavior and dispersion relation methods to analyze financial\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19408v1"
    },
    {
        "title": "Risk Analysis of Passive Portfolios",
        "authors": [
            "Sourish Das"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In this work, we present an alternative passive investment strategy. The\npassive investment philosophy comes from the Efficient Market Hypothesis (EMH),\nand its adoption is widespread. If EMH is true, one cannot outperform market by\nactively managing their portfolio for a long time. Also, it requires little to\nno intervention. People can buy an exchange-traded fund (ETF) with a long-term\nperspective. As the economy grows over time, one expects the ETF to grow. For\nexample, in India, one can invest in NETF, which suppose to mimic the Nifty50\nreturn. However, the weights of the Nifty 50 index are based on market\ncapitalisation. These weights are not necessarily optimal for the investor. In\nthis work, we present that volatility risk and extreme risk measures of the\nNifty50 portfolio are uniformly larger than Markowitz's optimal portfolio.\nHowever, common people can't create an optimised portfolio. So we proposed an\nalternative passive investment strategy of an equal-weight portfolio. We show\nthat if one pushes the maximum weight of the portfolio towards equal weight,\nthe idiosyncratic risk of the portfolio would be minimal. The empirical\nevidence indicates that the risk profile of an equal-weight portfolio is\nsimilar to that of Markowitz's optimal portfolio. Hence instead of buying\nNifty50 ETFs, one should equally invest in the stocks of Nifty50 to achieve a\nuniformly better risk profile than the Nifty 50 ETF portfolio. We also present\nan analysis of how portfolios perform to idiosyncratic events like the Russian\ninvasion of Ukraine. We found that the equal weight portfolio has a uniformly\nlower risk than the Nifty 50 portfolio before and during the Russia-Ukraine\nwar. All codes are available on GitHub\n(\\url{https://github.com/sourish-cmi/quant/tree/main/Chap_Risk_Anal_of_Passive_Portfolio}).\n",
        "pdf_link": "http://arxiv.org/pdf/2407.08332v1"
    },
    {
        "title": "Sentiment Analysis of State Bank of Pakistan's Monetary Policy Documents\n  and its Impact on Stock Market",
        "authors": [
            "Aabid Karim",
            "Heman Das Lohano"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This research examines whether sentiments conveyed in the State Bank of\nPakistan's (SBP) communications impact financial market expectations and can\nact as a monetary policy tool. To achieve our goal, we first use sentiment\nanalysis techniques to quantify the tone of SBP monetary policy documents and\nsecond, we use short time window, high frequency methodology to approximate the\nimpact of tone on stock market returns. Our results show that positive\n(negative) change in the tone positively (negatively) impacts stock returns in\nKarachi Stock Exchange. Further extension shows that the communication of SBP\nstill has a statistically significant impact on stock returns when controlling\nfor different variables and monetary policy tool. Also, the communication of\nSBP does not have a long term constant effect on stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.03328v1"
    },
    {
        "title": "A GCN-LSTM Approach for ES-mini and VX Futures Forecasting",
        "authors": [
            "Nikolas Michael",
            "Mihai Cucuringu",
            "Sam Howison"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We propose a novel data-driven network framework for forecasting problems\nrelated to E-mini S\\&P 500 and CBOE Volatility Index futures, in which products\nwith different expirations act as distinct nodes. We provide visual\ndemonstrations of the correlation structures of these products in terms of\ntheir returns, realized volatility, and trading volume. The resulting networks\noffer insights into the contemporaneous movements across the different\nproducts, illustrating how inherently connected the movements of the future\nproducts belonging to these two classes are. These networks are further\nutilized by a multi-channel Graph Convolutional Network to enhance the\npredictive power of a Long Short-Term Memory network, allowing for the\npropagation of forecasts of highly correlated quantities, combining the\ntemporal with the spatial aspect of the term structure.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.05659v1"
    },
    {
        "title": "The Efficient Tail Hypothesis: An Extreme Value Perspective on Market\n  Efficiency",
        "authors": [
            "Junshu Jiang",
            "Jordan Richards",
            "Raphaël Huser",
            "David Bolin"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In econometrics, the Efficient Market Hypothesis posits that asset prices\nreflect all available information in the market. Several empirical\ninvestigations show that market efficiency drops when it undergoes extreme\nevents. Many models for multivariate extremes focus on positive dependence,\nmaking them unsuitable for studying extremal dependence in financial markets\nwhere data often exhibit both positive and negative extremal dependence. To\nthis end, we construct regular variation models on the entirety of\n$\\mathbb{R}^d$ and develop a bivariate measure for asymmetry in the strength of\nextremal dependence between adjacent orthants. Our directional tail dependence\n(DTD) measure allows us to define the Efficient Tail Hypothesis (ETH) -- an\nanalogue of the Efficient Market Hypothesis -- for the extremal behaviour of\nthe market. Asymptotic results for estimators of DTD are described, and we\ndiscuss testing of the ETH via permutation-based methods and present novel\ntools for visualization. Empirical study of China's futures market leads to a\nrejection of the ETH and we identify potential profitable investment\nopportunities. To promote the research of microstructure in China's derivatives\nmarket, we open-source our high-frequency data, which are being collected\ncontinuously from multiple derivative exchanges.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.06661v1"
    },
    {
        "title": "Stylized facts in Web3",
        "authors": [
            "Wei-Ru Chen",
            "A. Christian Silva",
            "Shen-Ning Tung"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper presents a comprehensive statistical analysis of the Web3\necosystem, comparing various Web3 tokens with traditional financial assets\nacross multiple time scales. We examine probability distributions, tail\nbehaviors, and other key stylized facts of the returns for a diverse range of\ntokens, including decentralized exchanges, liquidity pools, and centralized\nexchanges. Despite functional differences, most tokens exhibit well-established\nempirical facts, including unconditional probability density of returns with\nheavy tails gradually becoming Gaussian and volatility clustering. Furthermore,\nwe compare assets traded on centralized (CEX) and decentralized (DEX)\nexchanges, finding that DEXs exhibit similar stylized facts despite different\ntrading mechanisms and often divergent long-term performance. We propose that\nthis similarity is attributable to arbitrageurs striving to maintain similar\ncentralized and decentralized prices. Our study contributes to a better\nunderstanding of the dynamics of Web3 tokens and the relationship between CEX\nand DEX markets, with important implications for risk management, pricing\nmodels, and portfolio construction in the rapidly evolving DeFi landscape.\nThese results add to the growing body of literature on cryptocurrency markets\nand provide insights that can guide the development of more accurate models for\nDeFi markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.07653v3"
    },
    {
        "title": "A new measure of risk using Fourier analysis",
        "authors": [
            "Michael Grabinski",
            "Galiya Klinkova"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We use Fourier analysis to access risk in financial products. With it we\nanalyze price changes of e.g. stocks. Via Fourier analysis we scrutinize\nquantitatively whether the frequency of change is higher than a change in\n(conserved) company value would allow. If it is the case, it would be a clear\nindicator of speculation and with it risk. The entire methods or better its\napplication is fairly new. However, there were severe flaws in previous\nattempts; making the results (not the method) doubtful. We corrected all these\nmistakes by e.g. using Fourier transformation instead of discrete Fourier\nanalysis. Our analysis is reliable in the entire frequency band, even for\nfre-quency of 1/1d or higher if the prices are noted accordingly. For the\nstocks scrutinized we found that the price of stocks changes disproportionally\nwithin one week which clearly indicates spec-ulation. It would be an\ninteresting extension to apply the method to crypto currencies as these\ncurrencies have no conserved value which makes normal considerations of\nvolatility difficult.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.10279v1"
    },
    {
        "title": "State-Space Dynamic Functional Regression for Multicurve Fixed Income\n  Spread Analysis and Stress Testing",
        "authors": [
            "Peilun He",
            "Gareth W. Peters",
            "Nino Kordzakhia",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The Nelson-Siegel model is widely used in fixed income markets to produce\nyield curve dynamics. The multiple time-dependent parameter model conveniently\naddresses the level, slope, and curvature dynamics of the yield curves. In this\nstudy, we present a novel state-space functional regression model that\nincorporates a dynamic Nelson-Siegel model and functional regression\nformulations applied to multi-economy setting. This framework offers distinct\nadvantages in explaining the relative spreads in yields between a reference\neconomy and a response economy. To address the inherent challenges of model\ncalibration, a kernel principal component analysis is employed to transform the\nrepresentation of functional regression into a finite-dimensional, tractable\nestimation problem. A comprehensive empirical analysis is conducted to assess\nthe efficacy of the functional regression approach, including an in-sample\nperformance comparison with the dynamic Nelson-Siegel model. We conducted the\nstress testing analysis of yield curves term-structure within a dual economy\nframework. The bond ladder portfolio was examined through a case study focused\non spread modelling using historical data for US Treasury and UK bonds.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.00348v2"
    },
    {
        "title": "Review of the EU ETS Literature: A Bibliometric Perspective",
        "authors": [
            "Cristiano Salvagnin"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This study conducts a bibliometric review of scientific literature on the\nEuropean Union Emissions Trading System (EU ETS) from 2004 to 2024, using\nresearch articles from the Scopus database. Using the Bibliometrix R package,\nwe analyze publication trends, key themes, influential authors, and prominent\njournals related to the EU ETS. Our results indicate a notable increase in\nresearch activity over the past two decades, particularly during significant\npolicy changes and economic events affecting carbon markets. Key research\nfocuses include carbon pricing, market volatility, and economic impacts,\nhighlighting a shift toward financial analysis and policy implications.\nThematic mapping shows cap-and-trade systems, and carbon leakage as central\ntopics linking various research areas. Additionally, we observe key areas where\nfurther research could be beneficial, such as expanding non-parametric\nmethodologies, deepening the exploration of macroeconomic factors, and\nenhancing the examination of financial market connections. Moreover, we\nhighlight recent and innovative papers that contribute new insights, showcasing\nemerging trends and cutting-edge approaches within the field. This review\nprovides insights for researchers and policymakers, highlighting the evolving\nlandscape of EU ETS research and its relevance to global climate strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.01739v3"
    },
    {
        "title": "Predicting Foreign Exchange EUR/USD direction using machine learning",
        "authors": [
            "Kevin Cedric Guyard",
            "Michel Deriaz"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The Foreign Exchange market is a significant market for speculators,\ncharacterized by substantial transaction volumes and high volatility.\nAccurately predicting the directional movement of currency pairs is essential\nfor formulating a sound financial investment strategy. This paper conducts a\ncomparative analysis of various machine learning models for predicting the\ndaily directional movement of the EUR/USD currency pair in the Foreign Exchange\nmarket. The analysis includes both decorrelated and non-decorrelated feature\nsets using Principal Component Analysis. Additionally, this study explores\nmeta-estimators, which involve stacking multiple estimators as input for\nanother estimator, aiming to achieve improved predictive performance.\nUltimately, our approach yielded a prediction accuracy of 58.52% for one-day\nahead forecasts, coupled with an annual return of 32.48% for the year 2022.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.04471v2"
    },
    {
        "title": "On the macroeconomic fundamentals of long-term volatilities and dynamic\n  correlations in COMEX copper futures",
        "authors": [
            "Zian Wang",
            "Xinshu Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper examines the influence of low-frequency macroeconomic variables on\nthe high-frequency returns of copper futures and the long-term correlation with\nthe S&P 500 index, employing GARCH-MIDAS and DCC-MIDAS modeling frameworks. The\nestimated results of GARCH-MIDAS show that realized volatility (RV), level of\ninterest rates (IR), industrial production (IP) and producer price index (PPI),\nvolatility of Slope, PPI, consumer sentiment index (CSI), and dollar index (DI)\nhave significant impacts on Copper futures returns, among which PPI is the most\nefficient macroeconomic variable. From comparison among DCC-GARCH and DCC-MIDAS\nmodel, the added MIDAS filter of PPI improves the model fitness and have better\nperformance than RV in effecting the long-run relationship between Copper\nfutures and S&P 500.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08355v1"
    },
    {
        "title": "Macroscopic properties of equity markets: stylized facts and portfolio\n  performance",
        "authors": [
            "Steven Campbell",
            "Qien Song",
            "Ting-Kam Leonard Wong"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Macroscopic properties of equity markets affect the performance of active\nequity strategies but many are not adequately captured by conventional models\nof financial mathematics and econometrics. Using the CRSP Database of the US\nequity market, we study empirically several macroscopic properties defined in\nterms of market capitalizations and returns, and highlight a list of stylized\nfacts and open questions motivated in part by stochastic portfolio theory.\nAdditionally, we present a systematic backtest of the diversity-weighted\nportfolio under various configurations and study its performance in relation to\nmacroscopic quantities. All of our results can be replicated using codes made\navailable on our GitHub repository.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.10859v2"
    },
    {
        "title": "A Comparison between Financial and Gambling Markets",
        "authors": [
            "Haoyu Liu",
            "Carl Donovan",
            "Valentin Popov"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Financial and gambling markets are ostensibly similar and hence strategies\nfrom one could potentially be applied to the other. Financial markets have been\nextensively studied, resulting in numerous theorems and models, while gambling\nmarkets have received comparatively less attention and remain relatively\nundocumented. This study conducts a comprehensive comparison of both markets,\nfocusing on trading rather than regulation. Five key aspects are examined:\nplatform, product, procedure, participant and strategy. The findings reveal\nnumerous similarities between these two markets. Financial exchanges resemble\nonline betting platforms, such as Betfair, and some financial products,\nincluding stocks and options, share speculative traits with sports betting. We\nexamine whether well-established models and strategies from financial markets\ncould be applied to the gambling industry, which lacks comparable frameworks.\nFor example, statistical arbitrage from financial markets has been effectively\napplied to gambling markets, particularly in peer-to-peer betting exchanges,\nwhere bettors exploit odds discrepancies for risk-free profits using\nquantitative models. Therefore, exploring the strategies and approaches used in\nboth markets could lead to new opportunities for innovation and optimization in\ntrading and betting activities.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13528v1"
    },
    {
        "title": "PDSim: A Shiny App for Polynomial Diffusion Model Simulation and\n  Estimation",
        "authors": [
            "Peilun He",
            "Nino Kordzakhia",
            "Gareth W. Peters",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  PDSim is an R package that enables users to simulate commodity futures prices\nusing the polynomial diffusion model introduced in Filipovic and Larsson (2016)\nthrough both a Shiny web application and R scripts. It also provides state\nvariables and contract estimations via the Extended Kalman Filter (EKF) or\nUnscented Kalman Filter (UKF). With its user-friendly interface, PDSim makes\nthe features of simulations and estimations accessible to all users. To date,\nit is the only package specifically designed for the simulation and estimation\nof the polynomial diffusion model. Additionally, the package integrates the\nSchwartz and Smith two-factor model (Schwartz & Smith, 2000) as an alternative\napproach. PDSim offers versatile deployment options, including running locally,\nvia the Shiny server, or through Docker.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19385v1"
    },
    {
        "title": "Multi-Factor Polynomial Diffusion Models and Inter-Temporal Futures\n  Dynamics",
        "authors": [
            "Peilun He",
            "Nino Kordzakhia",
            "Gareth W. Peters",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In stochastic multi-factor commodity models, it is often the case that\nfutures prices are explained by two latent state variables which represent the\nshort and long term stochastic factors. In this work, we develop the family of\nstochastic models using polynomial diffusion to obtain the unobservable spot\nprice to be used for modelling futures curve dynamics. The polynomial family of\ndiffusion models allows one to incorporate a variety of non-linear,\nhigher-order effects, into a multi-factor stochastic model, which is a\ngeneralisation of Schwartz and Smith (2000) two-factor model. Two filtering\nmethods are used for the parameter and the latent factor estimation to address\nthe non-linearity. We provide a comparative analysis of the performance of the\nestimation procedures. We discuss the parameter identification problem present\nin the polynomial diffusion case, regardless, the futures prices can still be\nestimated accurately. Moreover, we study the effects of different methods of\ncalculating matrix exponential in the polynomial diffusion model. As the\npolynomial order increases, accurately and efficiently approximating the\nhigh-dimensional matrix exponential becomes essential in the polynomial\ndiffusion model.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19386v1"
    },
    {
        "title": "Impermanent loss and loss-vs-rebalancing I: some statistical properties",
        "authors": [
            "Abe Alexander",
            "Lars Fritz"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  There are two predominant metrics to assess the performance of automated\nmarket makers and their profitability for liquidity providers: 'impermanent\nloss' (IL) and 'loss-versus-rebalance' (LVR). In this short paper we shed light\non the statistical aspects of both concepts and show that they are more similar\nthan conventionally appreciated. Our analysis uses the properties of a random\nwalk and some analytical properties of the statistical integral combined with\nthe mechanics of a constant function market maker (CFMM). We consider non-toxic\nor rather unspecific trading in this paper. Our main finding can be summarized\nin one sentence: For Brownian motion with a given volatility, IL and LVR have\nidentical expectation values but vastly differing distribution functions.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00854v1"
    },
    {
        "title": "Value of Information in the Mean-Square Case and its Application to the\n  Analysis of Financial Time-Series Forecast",
        "authors": [
            "Roman Belavkin",
            "Panos Pardalos",
            "Jose Principe"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The advances and development of various machine learning techniques has lead\nto practical solutions in various areas of science, engineering, medicine and\nfinance. The great choice of algorithms, their implementations and libraries\nhas resulted in another challenge of selecting the right algorithm and tuning\ntheir parameters in order to achieve optimal or satisfactory performance in\nspecific applications. Here we show how the value of information (V(I)) can be\nused in this task to guide the algorithm choice and parameter tuning process.\nAfter estimating the amount of Shannon's mutual information between the\npredictor and response variables, V(I) can define theoretical upper bound of\nperformance of any algorithm. The inverse function I(V) defines the lower\nfrontier of the minimum amount of information required to achieve the desired\nperformance. In this paper, we illustrate the value of information for the\nmean-square error minimization and apply it to forecasts of cryptocurrency\nlog-returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.01831v1"
    },
    {
        "title": "Optimizing Time Series Forecasting: A Comparative Study of Adam and\n  Nesterov Accelerated Gradient on LSTM and GRU networks Using Stock Market\n  data",
        "authors": [
            "Ahmad Makinde"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Several studies have discussed the impact different optimization techniques\nin the context of time series forecasting across different Neural network\narchitectures. This paper examines the effectiveness of Adam and Nesterov's\nAccelerated Gradient (NAG) optimization techniques on LSTM and GRU neural\nnetworks for time series prediction, specifically stock market time-series. Our\nstudy was done by training LSTM and GRU models with two different optimization\ntechniques - Adam and Nesterov Accelerated Gradient (NAG), comparing and\nevaluating their performance on Apple Inc's closing price data over the last\ndecade. The GRU model optimized with Adam produced the lowest RMSE,\noutperforming the other model-optimizer combinations in both accuracy and\nconvergence speed. The GRU models with both optimizers outperformed the LSTM\nmodels, whilst the Adam optimizer outperformed the NAG optimizer for both model\narchitectures. The results suggest that GRU models optimized with Adam are\nwell-suited for practitioners in time-series prediction, more specifically\nstock price time series prediction producing accurate and computationally\nefficient models. The code for the experiments in this project can be found at\nhttps://github.com/AhmadMak/Time-Series-Optimization-Research Keywords:\nTime-series Forecasting, Neural Network, LSTM, GRU, Adam Optimizer, Nesterov\nAccelerated Gradient (NAG) Optimizer\n",
        "pdf_link": "http://arxiv.org/pdf/2410.01843v1"
    },
    {
        "title": "Joint multifractality in the cross-correlations between grains \\&\n  oilseeds indices and external uncertainties",
        "authors": [
            "Ying-Hui Shao",
            "Xing-Lu Gao",
            "Yan-Hong Yang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This study investigates the relationships between agricultural spot markets\nand external uncertainties via the multifractal detrending moving-average\ncross-correlation analysis (MF-X-DMA). The dataset contains the Grains \\&\nOilseeds Index (GOI) and its five sub-indices of wheat, maize, soyabeans, rice,\nand barley. Moreover, we use three uncertainty proxies, namely, economic policy\nuncertainty (EPU), geopolitical risk (GPR), and volatility Index (VIX). We\nobserve the presence of multifractal cross-correlations between agricultural\nmarkets and uncertainties. Further, statistical tests show that maize has\nintrinsic joint multifractality with all the uncertainty proxies, exhibiting a\nhigh degree of sensitivity. Additionally, intrinsic multifractality among\nGOI-GPR, wheat-GPR and soyabeans-VIX is illustrated. However, other series have\napparent multifractal cross-correlations with high possibilities. Moreover, our\nanalysis suggests that among the three kinds of external uncertainties,\ngeopolitical risk has a relatively stronger association with grain prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.02798v1"
    },
    {
        "title": "Detecting Structural breakpoints in natural gas and electricity\n  wholesale prices via Bayesian ensemble approach, in the era of energy prices\n  turmoil of 2022 period: the cases of ten European markets",
        "authors": [
            "Panayotis G. Papaioannou",
            "George P. Papaioannou",
            "George Evangelidis",
            "George Gavalakis"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We investigate the impact of several critical events associated with the\nRusso Ukrainian war, started officially on 24 February 2022 with the Russian\ninvasion of Ukraine, on ten European electricity markets, two natural gas\nmarkets (the European reference trading hub TTF and N.Y. NGNMX market) and how\nthese markets interact to each other and with USDRUB exchange rate, a financial\nmarket. We analyze the reactions of these markets, manifested as breakpoints\nattributed to these critical events, and their interaction, by using a set of\nthree tools that can shed light on different aspects of this complex situation.\nWe combine the concepts of market efficiency, measured by quantifying the\nEfficient market hypothesis (EMH) via rolling Hurst exponent, with structural\nbreakpoints occurred in the time series of gas, electricity and financial\nmarkets, the detection of which is possible by using a Bayesian ensemble\napproach, the Bayesian Estimator of Abrupt change, Seasonal change and Trend\n(BEAST), a powerful tool that can effectively detect structural breakpoints,\ntrends, seasonalities and sudden abrupt changes in time series. The results\nshow that the analyzed markets have exhibited different modes of reactions to\nthe critical events, both in respect of number, nature, and time of occurrence\n(leading, lagging, concurrent with dates of critical events) of breakpoints as\nwell as of the dynamic behavior of their trend components.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.07224v1"
    },
    {
        "title": "Corporate Fundamentals and Stock Price Co-Movement",
        "authors": [
            "Lyuhong Wang",
            "Jiawei Jiang",
            "Yang Zhao"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We introduce an innovative framework that leverages advanced big data\ntechniques to analyze dynamic co-movement between stocks and their underlying\nfundamentals using high-frequency stock market data. Our method identifies\nleading co-movement stocks through four distinct regression models: Forecast\nError Variance Decomposition, transaction volume-normalized FEVD, Granger\ncausality test frequency, and Granger causality test days. Validated using\nChinese banking sector stocks, our framework uncovers complex relationships\nbetween stock price co-movements and fundamental characteristics, demonstrating\nits robustness and wide applicability across various sectors and markets. This\napproach not only enhances our understanding of market dynamics but also\nprovides actionable insights for investors and policymakers, helping to\nmitigate broader market volatilities and improve financial stability. Our model\nindicates that banks' influence on their peers is significantly affected by\ntheir wealth management business, interbank activities, equity multiplier,\nnon-performing loans, regulatory requirements, and reserve requirement ratios.\nThis aids in mitigating the impact of broader market volatilities and provides\ndeep insights into the unique influence of banks within the financial\necosystem.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.03922v1"
    },
    {
        "title": "New approaches of the DCC-GARCH residual: Application to foreign\n  exchange rates",
        "authors": [
            "Kenichiro Shiraya",
            "Kanji Suzuki",
            "Tomohisa Yamakami"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Two formulations are proposed to filter out correlations in the residuals of\nthe multivariate GARCH model. The first approach is to estimate the correlation\nmatrix as a parameter and transform any joint distribution to have an arbitrary\ncorrelation matrix. The second approach transforms time series data into an\nuncorrelated residual based on the eigenvalue decomposition of a correlation\nmatrix. The empirical performance of these methods is examined through a\nprediction task for foreign exchange rates and compared with other\nmethodologies in terms of the out-of-sample likelihood. By using these\napproaches, the DCC-GARCH residual can be almost independent.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.08246v1"
    },
    {
        "title": "Deep Learning in Long-Short Stock Portfolio Allocation: An Empirical\n  Study",
        "authors": [
            "Junjie Guo"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper provides an empirical study explores the application of deep\nlearning algorithms-Multilayer Perceptron (MLP), Convolutional Neural Networks\n(CNN), Long Short-Term Memory (LSTM), and Transformer-in constructing\nlong-short stock portfolios. Two datasets comprising randomly selected stocks\nfrom the S&P500 and NASDAQ indices, each spanning a decade of daily data, are\nutilized. The models predict daily stock returns based on historical features\nsuch as past returns,Relative Strength Index (RSI), trading volume, and\nvolatility. Portfolios are dynamically adjusted by longing stocks with positive\npredicted returns and shorting those with negative predictions, with equal\nasset weights. Performance is evaluated over a two-year testing period,\nfocusing on return, Sharpe ratio, and maximum drawdown metrics. The results\ndemonstrate the efficacy of deep learning models in enhancing long-short stock\nportfolio performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13555v3"
    },
    {
        "title": "Forecasting the Price of Rice in Banda Aceh after Covid-19",
        "authors": [
            "Fadhlul Mubarak",
            "Vinny Yuliani Sundara",
            " Nurniswah"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This research aims to predict the price of rice in Banda Aceh after the\noccurrence of Covid-19. The last observation carried forward (LOCF) imputation\ntechnique has been used to solve the problem of missing values from this\nresearch data. Furthermore, the technique used to forecast rice prices in Banda\nAceh is auto-ARIMA which is the best ARIMA model based on AIC, AICC, or BIC\nvalues. The results of this research show that the ARIMA model (0,0,5) is the\nbest model to predict the prices of lower quality rice I (BKB1), lower quality\nrice II (BKB2), medium quality rice I (BKM1), medium quality rice II (BKM2),\nsuper quality rice I (BKS1), and super quality rice II (BKS2). Based on this\nmodel, the results of forecasting rice prices for all qualities show that there\nwas a decline for some time (between September 1, 2023 and September 6, 2023)\nand then remained constant (between September 6, 2023 and December 31, 2023).\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15228v1"
    },
    {
        "title": "A Decision Support System for Stock Selection and Asset Allocation Based\n  on Fundamental Data Analysis",
        "authors": [
            "Ali Abrishami",
            "Jafar Habibi",
            "AmirAli Jarrahi",
            "Dariush Amiri",
            "MohammadAmin Fazli"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Financial markets are integral to a country's economic success, yet their\ncomplex nature raises challenging issues for predicting their behaviors. There\nis a growing demand for an integrated system that explores the vast and diverse\ndata in financial reports with powerful machine-learning models to analyze\nfinancial markets and suggest appropriate investment strategies. This research\nprovides an end-to-end decision support system (DSS) that pervasively covers\nthe stages of gathering, cleaning, and modeling the stock's financial and\nfundamental data alongside the country's macroeconomic conditions. Analyzing\nand modeling the fundamental data of securities is a noteworthy method that,\ndespite its greater power, has been used by fewer researchers due to its more\ncomplex and challenging issues. By precisely analyzing securities' fundamental\ndata, the proposed system assists investors in predicting stock future prices\nand allocating assets in major financial markets: stock, bond, and commodity.\nThe most notable contributions and innovations of this research are: (1)\nDeveloping a robust predictive model for mid- to long-term stock returns,\ntailored for investors rather than traders, (2) The proposed DSS considers a\ndiverse set of features relating to the economic conditions of the company,\nincluding fundamental data, stock trading characteristics, and macro-economic\nattributes to enhance predictive accuracy, (3) Evaluating the DSS performance\non the Tehran Stock Exchange that has specific characteristics of small to\nmedium-sized economies with high inflation rates and showing the superiority to\nnovel researches, and (4) Empowering the DSS to generate different asset\nallocation strategies in various economic situations by simulating expert\ninvestor decision-making.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.05297v1"
    },
    {
        "title": "Multi-Factor Function-on-Function Regression of Bond Yields on WTI\n  Commodity Futures Term Structure Dynamics",
        "authors": [
            "Peilun He",
            "Gareth W. Peters",
            "Nino Kordzakhia",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In the analysis of commodity futures, it is commonly assumed that futures\nprices are driven by two latent factors: short-term fluctuations and long-term\nequilibrium price levels. In this study, we extend this framework by\nintroducing a novel state-space functional regression model that incorporates\nyield curve dynamics. Our model offers a distinct advantage in capturing the\ninterdependencies between commodity futures and the yield curve. Through a\ncomprehensive empirical analysis of WTI crude oil futures, using US Treasury\nyields as a functional predictor, we demonstrate the superior accuracy of the\nfunctional regression model compared to the Schwartz-Smith two-factor model,\nparticularly in estimating the short-end of the futures curve. Additionally, we\nconduct a stress testing analysis to examine the impact of both temporary and\npermanent shocks to US Treasury yields on futures price estimation.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.05889v1"
    },
    {
        "title": "Limit Order Book Event Stream Prediction with Diffusion Model",
        "authors": [
            "Zetao Zheng",
            "Guoan Li",
            "Deqiang Ouyang",
            "Decui Liang",
            "Jie Shao"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Limit order book (LOB) is a dynamic, event-driven system that records\nreal-time market demand and supply for a financial asset in a stream flow.\nEvent stream prediction in LOB refers to forecasting both the timing and the\ntype of events. The challenge lies in modeling the time-event distribution to\ncapture the interdependence between time and event type, which has\ntraditionally relied on stochastic point processes. However, modeling complex\nmarket dynamics using stochastic processes, e.g., Hawke stochastic process, can\nbe simplistic and struggle to capture the evolution of market dynamics. In this\nstudy, we present LOBDIF (LOB event stream prediction with diffusion model),\nwhich offers a new paradigm for event stream prediction within the LOB system.\nLOBDIF learns the complex time-event distribution by leveraging a diffusion\nmodel, which decomposes the time-event distribution into sequential steps, with\neach step represented by a Gaussian distribution. Additionally, we propose a\ndenoising network and a skip-step sampling strategy. The former facilitates\neffective learning of time-event interdependence, while the latter accelerates\nthe sampling process during inference. By introducing a diffusion model, our\napproach breaks away from traditional modeling paradigms, offering novel\ninsights and providing an effective and efficient solution for learning the\ntime-event distribution in order streams within the LOB system. Extensive\nexperiments using real-world data from the limit order books of three widely\ntraded assets confirm that LOBDIF significantly outperforms current\nstate-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.09631v1"
    },
    {
        "title": "A Deep Learning Approach for Trading Factor Residuals",
        "authors": [
            "Wo Long",
            "Victor Xiao"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The residuals in factor models prevalent in asset pricing presents\nopportunities to exploit the mis-pricing from unexplained cross-sectional\nvariation for arbitrage. We performed a replication of the methodology of\nGuijarro-Ordonez et al. (2019) (G-P-Z) on Deep Learning Statistical Arbitrage\n(DLSA), originally applied to U.S. equity data from 1998 to 2016, using a more\nrecent out-of-sample period from 2016 to 2024. Adhering strictly to\npoint-in-time (PIT) principles and ensuring no information leakage, we follow\nthe same data pre-processing, factor modeling, and deep learning architectures\n(CNNs and Transformers) as outlined by G-P-Z. Our replication yields unusually\nstrong performance metrics in certain tests, with out-of-sample Sharpe ratios\noccasionally exceeding 10. While such results are intriguing, they may indicate\nmodel overfitting, highly specific market conditions, or insufficient\naccounting for transaction costs and market impact. Further examination and\nrobustness checks are needed to align these findings with the more modest\nimprovements reported in the original study. (This work was conducted as the\nfinal project for IEOR 4576: Data-Driven Methods in Finance at Columbia\nUniversity.)\n",
        "pdf_link": "http://arxiv.org/pdf/2412.11432v2"
    },
    {
        "title": "Multivariate Distributions in Non-Stationary Complex Systems I: Random\n  Matrix Model and Formulae for Data Analysis",
        "authors": [
            "Efstratios Manolakis",
            "Anton J. Heckens",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Risk assessment for rare events is essential for understanding systemic\nstability in complex systems. As rare events are typically highly correlated,\nit is important to study heavy-tailed multivariate distributions of the\nrelevant variables, i.e. their joint probability density functions. Only for\nfew systems, such investigation have been performed. Statistical models are\ndesirable that describe heavy-tailed multivariate distributions, in particular\nwhen non-stationarity is present as is typically the case in complex systems.\nRecently, we put forward such a model based on a separation of time scales. By\nutilizing random matrices, we showed that the fluctuations of the correlations\nlift the tails. Here, we present formulae and methods to carry out a data\ncomparisons for complex systems. There are only few fit parameters. Compared to\nour previous results, we manage to remove in the algebraic cases one out of the\ntwo, respectively three, fit parameters which considerably facilitates\napplications. Furthermore, we explicitly work out the moments of our model\ndistributions. In a forthcoming paper we will apply our model to financial\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.11601v1"
    },
    {
        "title": "Multivariate Distributions in Non-Stationary Complex Systems II:\n  Empirical Results for Correlated Stock Markets",
        "authors": [
            "Anton J. Heckens",
            "Efstratios Manolakis",
            "Cedric Schuhmann",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Multivariate Distributions are needed to capture the correlation structure of\ncomplex systems. In previous works, we developed a Random Matrix Model for such\ncorrelated multivariate joint probability density functions that accounts for\nthe non-stationarity typically found in complex systems. Here, we apply these\nresults to the returns measured in correlated stock markets. Only the knowledge\nof the multivariate return distributions allows for a full-fledged risk\nassessment. We analyze intraday data of 479 US stocks included in the S&P500\nindex during the trading year of 2014. We focus particularly on the tails which\nare algebraic and heavy. The non-stationary fluctuations of the correlations\nmake the tails heavier. With the few-parameter formulae of our Random Matrix\nModel we can describe and quantify how the empirical distributions change for\nvarying time resolution and in the presence of non-stationarity.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.11602v1"
    },
    {
        "title": "AI-Enhanced Factor Analysis for Predicting S&P 500 Stock Dynamics",
        "authors": [
            "Jiajun Gu",
            "Zichen Yang",
            "Xintong Lin",
            "Sixun Chen",
            "YuTing Lu"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This project investigates the interplay of technical, market, and statistical\nfactors in predicting stock market performance, with a primary focus on S&P 500\ncompanies. Utilizing a comprehensive dataset spanning multiple years, the\nanalysis constructs advanced financial metrics, such as momentum indicators,\nvolatility measures, and liquidity adjustments. The machine learning framework\nis employed to identify patterns, relationships, and predictive capabilities of\nthese factors. The integration of traditional financial analytics with machine\nlearning enables enhanced predictive accuracy, offering valuable insights into\nmarket behavior and guiding investment strategies. This research highlights the\npotential of combining domain-specific financial expertise with modern\ncomputational tools to address complex market dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.12438v1"
    },
    {
        "title": "Multivariate Rough Volatility",
        "authors": [
            "Ranieri Dugo",
            "Giacomo Giorgio",
            "Paolo Pigato"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Motivated by empirical evidence from the joint behavior of realized\nvolatility time series, we propose to model the joint dynamics of\nlog-volatilities using a multivariate fractional Ornstein-Uhlenbeck process.\nThis model is a multivariate version of the Rough Fractional Stochastic\nVolatility model proposed in Gatheral, Jaisson, and Rosenbaum, Quant. Finance,\n2018. It allows for different Hurst exponents in the different marginal\ncomponents and non trivial interdependencies.\n  We discuss the main features of the model and propose an estimator that\njointly identifies its parameters. We derive the asymptotic theory of the\nestimator and perform a simulation study that confirms the asymptotic theory in\nfinite sample.\n  We carry out an extensive empirical investigation on all realized volatility\ntime series covering the entire span of about two decades in the Oxford-Man\nrealized library. Our analysis shows that these time series are strongly\ncorrelated and can exhibit asymmetries in their cross-covariance structure,\naccurately captured by our model. These asymmetries lead to spillover effects\nthat we analyse theoretically within the model and then using our empirical\nestimates. Moreover, in accordance with the existing literature, we observe\nbehaviors close to non-stationarity and rough trajectories.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.14353v1"
    },
    {
        "title": "Evaluating the resilience of ESG investments in European Markets during\n  turmoil periods",
        "authors": [
            "Barbara Iannone",
            "Pierdomenico Duttilo",
            "Stefano Antonio Gattone"
        ],
        "category": "q-fin.ST",
        "published_year": "2025",
        "summary": "  This study investigates the resilience of Environmental, Social, and\nGovernance (ESG) investments during periods of financial instability, comparing\nthem with traditional equity indices across major European markets-Germany,\nFrance, and Italy. Using daily returns from October 2021 to February 2024, the\nanalysis explores the effects of key global disruptions such as the Covid-19\npandemic and the Russia-Ukraine conflict on market performance. A mixture of\ntwo generalised normal distributions (MGND) and EGARCH-in-mean models are used\nto identify periods of market turmoil and assess volatility dynamics. The\nfindings indicate that during crises, ESG investments present higher volatility\nin Germany and Italy than in France. Despite some regional variations, ESG\nportfolios demonstrate greater resilience compared to traditional ones,\noffering potential risk mitigation during market shocks. These results\nunderscore the importance of integrating ESG factors into long-term investment\nstrategies, particularly in the face of unpredictable financial turmoil.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.03269v1"
    },
    {
        "title": "True and Apparent Scaling: The Proximity of the Markov-Switching\n  Multifractal Model to Long-Range Dependence",
        "authors": [
            "Ruipeng Liu",
            "T. Di Matteo",
            "Thomas Lux"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  In this paper, we consider daily financial data of a collection of different\nstock market indices, exchange rates, and interest rates, and we analyze their\nmulti-scaling properties by estimating a simple specification of the\nMarkov-switching multifractal model (MSM). In order to see how well the\nestimated models capture the temporal dependence of the data, we estimate and\ncompare the scaling exponents $H(q)$ (for $q = 1, 2$) for both empirical data\nand simulated data of the estimated MSM models. In most cases the multifractal\nmodel appears to generate `apparent' long memory in agreement with the\nempirical scaling laws.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.1338v1"
    },
    {
        "title": "Financial time-series analysis: A brief overview",
        "authors": [
            "A. Chakraborti",
            "M. Patriarca",
            "M. S. Santhanam"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Prices of commodities or assets produce what is called time-series. Different\nkinds of financial time-series have been recorded and studied for decades.\nNowadays, all transactions on a financial market are recorded, leading to a\nhuge amount of data available, either for free in the Internet or commercially.\nFinancial time-series analysis is of great interest to practitioners as well as\nto theoreticians, for making inferences and predictions. Furthermore, the\nstochastic uncertainties inherent in financial time-series and the theory\nneeded to deal with them make the subject especially interesting not only to\neconomists, but also to statisticians and physicists. While it would be a\nformidable task to make an exhaustive review on the topic, with this review we\ntry to give a flavor of some of its aspects.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.1738v1"
    },
    {
        "title": "Scaling laws of strategic behaviour and size heterogeneity in agent\n  dynamics",
        "authors": [
            "Gabriella Vaglica",
            "Fabrizio Lillo",
            "Esteban Moro",
            "Rosario N. Mantegna"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The dynamics of many socioeconomic systems is determined by the decision\nmaking process of agents. The decision process depends on agent's\ncharacteristics, such as preferences, risk aversion, behavioral biases, etc..\nIn addition, in some systems the size of agents can be highly heterogeneous\nleading to very different impacts of agents on the system dynamics. The large\nsize of some agents poses challenging problems to agents who want to control\ntheir impact, either by forcing the system in a given direction or by hiding\ntheir intentionality. Here we consider the financial market as a model system,\nand we study empirically how agents strategically adjust the properties of\nlarge orders in order to meet their preference and minimize their impact. We\nquantify this strategic behavior by detecting scaling relations of allometric\nnature between the variables characterizing the trading activity of different\ninstitutions. We observe power law distributions in the investment time\nhorizon, in the number of transactions needed to execute a large order and in\nthe traded value exchanged by large institutions and we show that heterogeneity\nof agents is a key ingredient for the emergence of some aggregate properties\ncharacterizing this complex system.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.2003v1"
    },
    {
        "title": "Deterministic Factors of Stock Networks based on Cross-correlation in\n  Financial Market",
        "authors": [
            "Cheoljun Eom",
            "Gabjin Oh",
            "Seunghwan Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The stock market has been known to form homogeneous stock groups with a\nhigher correlation among different stocks according to common economic factors\nthat influence individual stocks. We investigate the role of common economic\nfactors in the market in the formation of stock networks, using the arbitrage\npricing model reflecting essential properties of common economic factors. We\nfind that the degree of consistency between real and model stock networks\nincreases as additional common economic factors are incorporated into our\nmodel. Furthermore, we find that individual stocks with a large number of links\nto other stocks in a network are more highly correlated with common economic\nfactors than those with a small number of links. This suggests that common\neconomic factors in the stock market can be understood in terms of\ndeterministic factors.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.0076v1"
    },
    {
        "title": "Network Topology of an Experimental Futures Exchange",
        "authors": [
            "S. C. Wang",
            "J. J. Tseng",
            "C. C. Tai",
            "K. H. Lai",
            "W. S. Wu",
            "S. H. Chen",
            "S. P. Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Many systems of different nature exhibit scale free behaviors. Economic\nsystems with power law distribution in the wealth is one of the examples. To\nbetter understand the working behind the complexity, we undertook an empirical\nstudy measuring the interactions between market participants. A Web server was\nsetup to administer the exchange of futures contracts whose liquidation prices\nwere coupled to event outcomes. After free registration, participants started\ntrading to compete for the money prizes upon maturity of the futures contracts\nat the end of the experiment. The evolving `cash' flow network was\nreconstructed from the transactions between players. We show that the network\ntopology is hierarchical, disassortative and scale-free with a power law\nexponent of 1.02+-0.09 in the degree distribution. The small-world property\nemerged early in the experiment while the number of participants was still\nsmall. We also show power law distributions of the net incomes and\ninter-transaction time intervals. Big winners and losers are associated with\nhigh degree, high betweenness centrality, low clustering coefficient and low\ndegree-correlation. We identify communities in the network as groups of the\nlike-minded. The distribution of the community sizes is shown to be power-law\ndistributed with an exponent of 1.19+-0.16.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.2551v1"
    },
    {
        "title": "Multifractality in stock indexes: Fact or fiction?",
        "authors": [
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Multifractal analysis and extensive statistical tests are performed upon\nintraday minutely data within individual trading days for four stock market\nindexes (including HSI, SZSC, S&P500, and NASDAQ) to check whether the indexes\n(instead of the returns) possess multifractality. We find that the mass\nexponent $\\tau(q)$ is linear and the singularity $\\alpha(q)$ is close to 1 for\nall trading days and all indexes. Furthermore, we find strong evidence showing\nthat the scaling behaviors of the original data sets cannot be distinguished\nfrom those of the shuffled time series. Hence, the so-called multifractality in\nthe intraday stock market indexes is merely an illusion.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.2140v1"
    },
    {
        "title": "Effects of payoff functions and preference distributions in an adaptive\n  population",
        "authors": [
            "H. M. Yang",
            "Y. S. Ting",
            "K. Y. Michael Wong"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Adaptive populations such as those in financial markets and distributed\ncontrol can be modeled by the Minority Game. We consider how their dynamics\ndepends on the agents' initial preferences of strategies, when the agents use\nlinear or quadratic payoff functions to evaluate their strategies. We find that\nthe fluctuations of the population making certain decisions (the volatility)\ndepends on the diversity of the distribution of the initial preferences of\nstrategies. When the diversity decreases, more agents tend to adapt their\nstrategies together. In systems with linear payoffs, this results in dynamical\ntransitions from vanishing volatility to a non-vanishing one. For low signal\ndimensions, the dynamical transitions for the different signals do not take\nplace at the same critical diversity. Rather, a cascade of dynamical\ntransitions takes place when the diversity is reduced. In contrast, no phase\ntransitions are found in systems with the quadratic payoffs. Instead, a basin\nboundary of attraction separates two groups of samples in the space of the\nagents' decisions. Initial states inside this boundary converge to small\nvolatility, while those outside diverge to a large one. Furthermore, when the\npreference distribution becomes more polarized, the dynamics becomes more\nerratic. All the above results are supported by good agreement between\nsimulations and theory.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.3122v2"
    },
    {
        "title": "Information flow between composite stock index and individual stocks",
        "authors": [
            "Okyu Kwon",
            "Jae-Suk Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We investigate the strength and the direction of information transfer in the\nU.S. stock market between the composite stock price index of stock market and\nprices of individual stocks using the transfer entropy. Through the\ndirectionality of the information transfer, we find that individual stocks are\ninfluenced by the index of the market.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0063v1"
    },
    {
        "title": "Group dynamics of the Japanese market",
        "authors": [
            "Woo-Sung Jung",
            "Okyu Kwon",
            "Fengzhong Wang",
            "Taisei Kaizoji",
            "Hie-Tae Moon",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We investigated the network structures of the Japanese stock market through\nthe minimum spanning tree. We defined grouping coefficient to test the validity\nof conventional grouping by industrial categories, and found a decreasing in\ntrend for the coefficient. This phenomenon supports the increasing external\ninfluences on the market due to the globalization. To reduce this influence, we\nused S&P500 index as the international market and removed its correlation with\nevery stock. We found stronger grouping in this measurement, compared to the\noriginal analysis, which agrees with our assumption that the international\nmarket influences to the Japanese market.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0562v1"
    },
    {
        "title": "Empirical distributions of Chinese stock returns at different\n  microscopic timescales",
        "authors": [
            "Gao-Feng Gu",
            "Wei Chen",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We study the distributions of event-time returns and clock-time returns at\ndifferent microscopic timescales using ultra-high-frequency data extracted from\nthe limit-order books of 23 stocks traded in the Chinese stock market in 2003.\nWe find that the returns at the one-trade timescale obey the inverse cubic law.\nFor larger timescales (2-32 trades and 1-5 minutes), the returns follow the\nStudent distribution with power-law tails. With the decrease of timescale, the\ntail becomes fatter, which is consistent with the vibrational theory.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.3472v1"
    },
    {
        "title": "Relationship between degree of efficiency and prediction in stock price\n  changes",
        "authors": [
            "Cheoljun Eom",
            "Gabjin Oh",
            "Woo-Sung Jung"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  This study investigates empirically whether the degree of stock market\nefficiency is related to the prediction power of future price change using the\nindices of twenty seven stock markets. Efficiency refers to weak-form efficient\nmarket hypothesis (EMH) in terms of the information of past price changes. The\nprediction power corresponds to the hit-rate, which is the rate of the\nconsistency between the direction of actual price change and that of predicted\none, calculated by the nearest neighbor prediction method (NN method) using the\nout-of-sample. In this manuscript, the Hurst exponent and the approximate\nentropy (ApEn) are used as the quantitative measurements of the degree of\nefficiency. The relationship between the Hurst exponent, reflecting the various\ntime correlation property, and the ApEn value, reflecting the randomness in the\ntime series, shows negative correlation. However, the average prediction power\non the direction of future price change has the strongly positive correlation\nwith the Hurst exponent, and the negative correlation with the ApEn. Therefore,\nthe market index with less market efficiency has higher prediction power for\nfuture price change than one with higher market efficiency when we analyze the\nmarket using the past price change pattern. Furthermore, we show that the Hurst\nexponent, a measurement of the long-term memory property, provides more\nsignificant information in terms of prediction of future price changes than the\nApEn and the NN method.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.4178v1"
    },
    {
        "title": "World currency exchange rate cross-correlations",
        "authors": [
            "S. Drozdz",
            "A. Z. Gorski",
            "J. Kwapien"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  World currency network constitutes one of the most complex structures that is\nassociated with the contemporary civilization. On a way towards quantifying its\ncharacteristics we study the cross correlations in changes of the daily foreign\nexchange rates within the basket of 60 currencies in the period December 1998\n-- May 2005. Such a dynamics turns out to predominantly involve one outstanding\neigenvalue of the correlation matrix. The magnitude of this eigenvalue depends\nhowever crucially on which currency is used as a base currency for the\nremaining ones. Most prominent it looks from the perspective of a peripheral\ncurrency. This largest eigenvalue is seen to systematically decrease and thus\nthe structure of correlations becomes more heterogeneous, when more significant\ncurrencies are used as reference. An extreme case in this later respect is the\nUSD in the period considered. Besides providing further insight into subtle\nnature of complexity, these observations point to a formal procedure that in\ngeneral can be used for practical purposes of measuring the relative currencies\nsignificance on various time horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.4347v1"
    },
    {
        "title": "An empirical behavioral model of liquidity and volatility",
        "authors": [
            "Szabolcs Mike",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We develop a behavioral model for liquidity and volatility based on empirical\nregularities in trading order flow in the London Stock Exchange. This can be\nviewed as a very simple agent based model in which all components of the model\nare validated against real data. Our empirical studies of order flow uncover\nseveral interesting regularities in the way trading orders are placed and\ncancelled. The resulting simple model of order flow is used to simulate price\nformation under a continuous double auction, and the statistical properties of\nthe resulting simulated sequence of prices are compared to those of real data.\nThe model is constructed using one stock (AZN) and tested on 24 other stocks.\nFor low volatility, small tick size stocks (called Group I) the predictions are\nvery good, but for stocks outside Group I they are not good. For Group I, the\nmodel predicts the correct magnitude and functional form of the distribution of\nthe volatility and the bid-ask spread, without adjusting any parameters based\non prices. This suggests that at least for Group I stocks, the volatility and\nheavy tails of prices are related to market microstructure effects, and\nsupports the hypothesis that, at least on short time scales, the large\nfluctuations of absolute returns are well described by a power law with an\nexponent that varies from stock to stock.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0159v1"
    },
    {
        "title": "Detrended Cross-Correlation Analysis: A New Method for Analyzing Two\n  Non-stationary Time Series",
        "authors": [
            "Boris Podobnik",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Here we propose a method, based on detrended covariance which we call\ndetrended cross-correlation analysis (DXA), to investigate power-law\ncross-correlations between different simultaneously-recorded time series in the\npresence of non-stationarity. We illustrate the method by selected examples\nfrom physics, physiology, and finance.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0281v1"
    },
    {
        "title": "Utility function estimation: the entropy approach",
        "authors": [
            "Andreia Dionisio",
            "A. Heitor Reis"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The maximum entropy principle can be used to assign utility values when only\npartial information is available about the decision maker's preferences. In\norder to obtain such utility values it is necessary to establish an analogy\nbetween probability and utility through the notion of a utility density\nfunction. According to some authors [Soofi (1990), Abbas (2006a) (2006b),\nSandow et al. (2006), Friedman and Sandow (2006), Darooneh (2006)] the maximum\nentropy utility solution embeds a large family of utility functions. In this\npaper we explore the maximum entropy principle to estimate the utility function\nof a risk averse decision maker.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0591v1"
    },
    {
        "title": "Entropy and Uncertainty Analysis in Financial Markets",
        "authors": [
            "Andreia Dionisio",
            "Rui Menezes",
            "Diana A. Mendes"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The investor is interested in the expected return and he is also concerned\nabout the risk and the uncertainty assumed by the investment. One of the most\npopular concepts used to measure the risk and the uncertainty is the variance\nand/or the standard-deviation. In this paper we explore the following issues:\nIs the standard-deviation a good measure of risk and uncertainty? What are the\npotentialities of the entropy in this context? Can entropy present some\nadvantages as a measure of uncertainty and simultaneously verify some basic\nassumptions of the portfolio management theory, namely the effect of\ndiversification?\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0668v1"
    },
    {
        "title": "A Comparative Study of Stochastic Volatility Models",
        "authors": [
            "E. Cisana",
            "L. Fermi",
            "G. Montagna",
            "O. Nicrosini"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The correlated stochastic volatility models constitute a natural extension of\nthe Black and Scholes-Merton framework: here the volatility is not a constant,\nbut a stochastic process correlated with the price log-return one. At present,\nseveral stochastic volatility models are discussed in the literature, differing\nin the dynamics attached to the volatility. The aim of the present work is to\ncompare the most recent results about three popular models: the Vasicek, Heston\nand exponential Ornstein-Uhlenbeck models. We analyzed for each of them the\ntheoretical results known in the literature (volatility and return\ndistribution, higher-order moments and different-time correlations) in order to\ntest their predictive effectiveness on the outcomes of original numerical\nsimulations, paying particular attention to their ability to reproduce\nempirical statistical properties of prices. The numerical results demonstrate\nthat these models can be implemented maintaining all their features, especially\nin view of financial applications like market risk management or option\npricing. In order to critically compare the models, we also perform an\nempirical analysis of financial time series from the Italian stock market,\nshowing the exponential Ornstein-Uhlenbeck model's ability to capture the\nstylized facts of volatility and log-return probability distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0810v1"
    },
    {
        "title": "Modeling long-range cross-correlations in two-component ARFIMA and\n  FIARCH processes",
        "authors": [
            "Boris Podobnik",
            "Davor Horvatic",
            "Alfonso Lam Ng",
            "H. Eugene Stanley",
            "Plamen Ch. Ivanov"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We investigate how simultaneously recorded long-range power-law correlated\nmulti-variate signals cross-correlate. To this end we introduce a two-component\nARFIMA stochastic process and a two-component FIARCH process to generate\ncoupled fractal signals with long-range power-law correlations which are at the\nsame time long-range cross-correlated. We study how the degree of\ncross-correlations between these signals depends on the scaling exponents\ncharacterizing the fractal correlations in each signal and on the coupling\nbetween the signals. Our findings have relevance when studying parallel outputs\nof multiple-component of physical, physiological and social systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0838v1"
    },
    {
        "title": "Relaxation dynamics of aftershocks after large volatility shocks in the\n  SSEC index",
        "authors": [
            "Guo-Hua Mu",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The relaxation dynamics of aftershocks after large volatility shocks are\ninvestigated based on two high-frequency data sets of the Shanghai Stock\nExchange Composite (SSEC) index. Compared with previous relevant work, we have\ndefined main financial shocks based on large volatilities rather than large\ncrashes. We find that the occurrence rate of aftershocks with the magnitude\nexceeding a given threshold for both daily volatility (constructed using\n1-minute data) and minutely volatility (using intra-minute data) decays as a\npower law. The power-law relaxation exponent increases with the volatility\nthreshold and is significantly greater than 1. Taking financial volatility as\nthe counterpart of seismic activity, the power-law relaxation in financial\nvolatility deviates remarkably from the Omori law in Geophysics.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.1219v1"
    },
    {
        "title": "Influence of deterministic trend on the estimated parameters of\n  GARCH(1,1) model",
        "authors": [
            "Calin Vamos",
            "Maria Craciun"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The log returns of financial time series are usually modeled by means of the\nstationary GARCH(1,1) stochastic process or its generalizations which can not\nproperly describe the nonstationary deterministic components of the original\nseries. We analyze the influence of deterministic trends on the GARCH(1,1)\nparameters using Monte Carlo simulations. The statistical ensembles contain\nnumerically generated time series composed by GARCH(1,1) noise superposed on\ndeterministic trends. The GARCH(1,1) parameters characteristic for financial\ntime series longer than one year are not affected by the detrending errors. We\nalso show that if the ARCH coefficient is greater than the GARCH coefficient,\nthen the estimated GARCH(1,1) parameters depend on the number of monotonic\nparts of the trend and on the ratio between the trend and the noise amplitudes.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.1536v2"
    },
    {
        "title": "Economic dynamics with financial fragility and mean-field interaction: a\n  model",
        "authors": [
            "Corrado Di Guilmi",
            "Mauro Gallegati",
            "Simone Landini"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Following the statistical mechanics methodology, firstly introduced in\nmacroeconomics by Aoki [1996,2002], we provide some insights to the well known\nworks of Greenwald and Stiglitz [1990, 1993]. Specifically, we reach\nanalytically a closed form solution of their models overcoming the aggregation\nproblem. The key idea is to represent the economy as an evolving complex\nsystem, composed by heterogeneous interacting agents, that can partitioned into\na space of macroscopic states. This meso level of aggregation permits to adopt\nmean field interaction modeling and master equation techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2083v1"
    },
    {
        "title": "Long Memory and Volatility Clustering: is the empirical evidence\n  consistent across stock markets?",
        "authors": [
            "Sonia R. Bentes",
            "Rui Menezes",
            "Diana A. Mendes"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Long memory and volatility clustering are two stylized facts frequently\nrelated to financial markets. Traditionally, these phenomena have been studied\nbased on conditionally heteroscedastic models like ARCH, GARCH, IGARCH and\nFIGARCH, inter alia. One advantage of these models is their ability to capture\nnonlinear dynamics. Another interesting manner to study the volatility\nphenomena is by using measures based on the concept of entropy. In this paper\nwe investigate the long memory and volatility clustering for the SP 500, NASDAQ\n100 and Stoxx 50 indexes in order to compare the US and European Markets.\nAdditionally, we compare the results from conditionally heteroscedastic models\nwith those from the entropy measures. In the latter, we examine Shannon\nentropy, Renyi entropy and Tsallis entropy. The results corroborate the\nprevious evidence of nonlinear dynamics in the time series considered.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2178v3"
    },
    {
        "title": "Statistical Investigation of Connected Structures of Stock Networks in\n  Financial Time Series",
        "authors": [
            "Cheoljun Eom",
            "Gabjin Oh",
            "Seunghwan Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  In this study, we have investigated factors of determination which can affect\nthe connected structure of a stock network. The representative index for\ntopological properties of a stock network is the number of links with other\nstocks. We used the multi-factor model, extensively acknowledged in financial\nliterature. In the multi-factor model, common factors act as independent\nvariables while returns of individual stocks act as dependent variables. We\ncalculated the coefficient of determination, which represents the measurement\nvalue of the degree in which dependent variables are explained by independent\nvariables. Therefore, we investigated the relationship between the number of\nlinks in the stock network and the coefficient of determination in the\nmulti-factor model. We used individual stocks traded on the market indices of\nKorea, Japan, Canada, Italy and the UK. The results are as follows. We found\nthat the mean coefficient of determination of stocks with a large number of\nlinks have higher values than those with a small number of links with other\nstocks. These results suggest that common factors are significantly\ndeterministic factors to be taken into account when making a stock network.\nFurthermore, stocks with a large number of links to other stocks can be more\naffected by common factors.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2200v1"
    },
    {
        "title": "Topological Properties of Stock Networks Based on Random Matrix Theory\n  in Financial Time Series",
        "authors": [
            "Cheoljun Eom",
            "Gapjin Oh",
            "Hawoong Jeong",
            "Seunghwan Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We investigated the topological properties of stock networks through a\ncomparison of the original stock network with the estimated stock network from\nthe correlation matrix created by the random matrix theory (RMT). We used\nindividual stocks traded on the market indices of Korea, Japan, Canada, the\nUSA, Italy, and the UK. The results are as follows. As the correlation matrix\nreflects the more eigenvalue property, the estimated stock network from the\ncorrelation matrix gradually increases the degree of consistency with the\noriginal stock network. Each stock with a different number of links to other\nstocks in the original stock network shows a different response. In particular,\nthe largest eigenvalue is a significant deterministic factor in terms of the\nformation of a stock network.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2209v1"
    },
    {
        "title": "Measuring Volatility Clustering in Stock Markets",
        "authors": [
            "Gabjin Oh",
            "Seunghwan Kim",
            "Cheoljun Eom",
            "Taehyuk Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We propose a novel method to quantify the clustering behavior in a complex\ntime series and apply it to a high-frequency data of the financial markets. We\nfind that regardless of used data sets, all data exhibits the volatility\nclustering properties, whereas those which filtered the volatility clustering\neffect by using the GARCH model reduce volatility clustering significantly. The\nresult confirms that our method can measure the volatility clustering effect in\nfinancial market.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2416v1"
    },
    {
        "title": "Correlations and clustering in the trading of members of the London\n  Stock Exchange",
        "authors": [
            "Ilija I. Zovko",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  This paper analyzes correlations in patterns of trading of different members\nof the London Stock Exchange. The collection of strategies associated with a\nmember institution is defined by the sequence of signs of net volume traded by\nthat institution in hour intervals. Using several methods we show that there\nare significant and persistent correlations between institutions. In addition,\nthe correlations are structured into correlated and anti-correlated groups.\nClustering techniques using the correlations as a distance metric reveal a\nmeaningful clustering structure with two groups of institutions trading in\nopposite directions.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.3261v1"
    },
    {
        "title": "Interest rates mapping",
        "authors": [
            "M. Kanevski",
            "M. Maignan",
            "A. Pozdnoukhov",
            "V. Timonin"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The present study deals with the analysis and mapping of Swiss franc interest\nrates. Interest rates depend on time and maturity, defining term structure of\nthe interest rate curves (IRC). In the present study IRC are considered in a\ntwo-dimensional feature space - time and maturity. Geostatistical models and\nmachine learning algorithms (multilayer perceptron and Support Vector Machines)\nwere applied to produce interest rate maps. IR maps can be used for the\nvisualisation and patterns perception purposes, to develop and to explore\neconomical hypotheses, to produce dynamic asses-liability simulations and for\nthe financial risk assessments. The feasibility of an application of interest\nrates mapping approach for the IRC forecasting is considered as well.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.4361v1"
    },
    {
        "title": "Quasistatically varying log-normal distribution in the middle scale\n  region of Japanese land prices",
        "authors": [
            "Atushi Ishikawa"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Employing data on the assessed value of land in 1974--2007 Japan, we exhibit\na quasistatically varying log-normal distribution in the middle scale region.\nIn the derivation, a Non-Gibrat's law under the detailed quasi-balance is\nadopted together with two approximations. The resultant distribution is\npower-law with the varying exponent in the large scale region and the\nquasistatic log-normal distribution with the varying standard deviation in the\nmiddle scale region. In the distribution, not only the change of the exponent\nbut also the change of the standard deviation depends on the parameter of the\ndetailed quasi-balance. These results are consistently confirmed by the\nempirical data.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.1893v1"
    },
    {
        "title": "Intraday pattern in bid-ask spreads and its power-law relaxation for\n  Chinese A-share stocks",
        "authors": [
            "Xiao-Hui Ni",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We use high-frequency data of 1364 Chinese A-share stocks traded on the\nShanghai Stock Exchange and Shenzhen Stock Exchange to investigate the intraday\npatterns in the bid-ask spreads. The daily periodicity in the spread time\nseries is confirmed by Lomb analysis and the intraday bid-ask spreads are found\nto exhibit $L$-shaped pattern with idiosyncratic fine structure. The intraday\nspread of individual stocks relaxes as a power law within the first hour of the\ncontinuous double auction from 9:30AM to 10:30AM with exponents\n$\\beta_{\\rm{SHSE}}=0.19\\pm0.069$ for the Shanghai market and\n$\\beta_{\\rm{SZSE}}=0.18\\pm0.067$ for the Shenzhen market. The power-law\nrelaxation exponent $\\beta$ of individual stocks is roughly normally\ndistributed. There is evidence showing that the accumulation of information\nwidening the spread is an endogenous process.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.2402v1"
    },
    {
        "title": "Multifractality in the Random Parameters Model",
        "authors": [
            "Camilo Rodrigues Neto",
            "Andr\\' e C. R. Martins"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The Random Parameters model was proposed to explain the structure of the\ncovariance matrix in problems where most, but not all, of the eigenvalues of\nthe covariance matrix can be explained by Random Matrix Theory. In this\narticle, we explore other properties of the model, like the scaling of its PDF\nas one take larger scales. Special attention is given to the multifractal\nstructure of the model time series, which revealed a scaling structure\ncompatible with the known stylized facts for a reasonable choice of the\nparameter values.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.5497v1"
    },
    {
        "title": "Empirics versus RMT in financial cross-correlations",
        "authors": [
            "S. Drozdz",
            "J. Kwapien",
            "P. Oswiecimka"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  In order to pursue the issue of the relation between the financial\ncross-correlations and the conventional Random Matrix Theory we analyse several\ncharacteristics of the stock market correlation matrices like the distribution\nof eigenvalues, the cross-correlations among signs of the returns, the\nvolatility cross-correlations, and the multifractal characteristics of the\nprincipal values. The results indicate that the stock market dynamics is not\nsimply decomposable into 'market', 'sectors', and the Wishart random bulk. This\nclearly is seen when the time series used to construct the correlation matrices\nare sufficiently long and thus the measurement noise suppressed. Instead, a\nhierarchically convoluted and highly nonlinear organization of the market\nemerges and indicates that the relevant information about the whole market is\nencoded already in its constituents.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.0644v1"
    },
    {
        "title": "Effective multifractal features and l-variability diagrams of\n  high-frequency price fluctuations time series",
        "authors": [
            "Jeferson de Souza",
            "Silvio M. Duarte Queiros"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  In this manuscript we present a comprehensive study on the multifractal\nproperties of high-frequency price fluctuations and instantaneous volatility of\nthe equities that compose Dow Jones Industrial Average. The analysis consists\nabout quantification of dependence and non-Gaussianity on the multifractal\ncharacter of financial quantities. Our results point out an equivalent\ninfluence of dependence and non-Gaussianity on the multifractality of time\nseries. Moreover, we analyse l-diagrams of price fluctuations. In the latter\ncase, we show that the fractal dimension of these maps is basically independent\nof the lag between price fluctuations that we assume.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.2550v1"
    },
    {
        "title": "A threshold model of financial markets",
        "authors": [
            "Paweł Sieczka",
            "Janusz A. Hołyst"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We proposed a model of interacting market agents based on the Ising spin\nmodel. The agents can take three actions: \"buy,\" \"sell,\" or \"stay inactive.\" We\ndefined a price evolution in terms of the system magnetization. The model\nreproduces main stylized facts of real markets such as: fat-tailed distribution\nof returns and volatility clustering.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.3106v3"
    },
    {
        "title": "The non-random walk of stock prices: The long-term correlation between\n  signs and sizes",
        "authors": [
            "Gabriele La Spada",
            "J. Doyne Farmer",
            "Fabrizio Lillo"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We investigate the random walk of prices by developing a simple model\nrelating the properties of the signs and absolute values of individual price\nchanges to the diffusion rate (volatility) of prices at longer time scales. We\nshow that this benchmark model is unable to reproduce the diffusion properties\nof real prices. Specifically, we find that for one hour intervals this model\nconsistently over-predicts the volatility of real price series by about 70%,\nand that this effect becomes stronger as the length of the intervals increases.\nBy selectively shuffling some components of the data while preserving others we\nare able to show that this discrepancy is caused by a subtle but long-range\nnon-contemporaneous correlation between the signs and sizes of individual\nreturns. We conjecture that this is related to the long-memory of transaction\nsigns and the need to enforce market efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.4596v2"
    },
    {
        "title": "Hurst exponent and prediction based on weak-form efficient market\n  hypothesis of stock markets",
        "authors": [
            "Cheoljun Eom",
            "Sunghoon Choi",
            "Gabjin Oh",
            "Woo-Sung Jung"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We empirically investigated the relationships between the degree of\nefficiency and the predictability in financial time-series data. The Hurst\nexponent was used as the measurement of the degree of efficiency, and the hit\nrate calculated from the nearest-neighbor prediction method was used for the\nprediction of the directions of future price changes. We used 60 market indexes\nof various countries. We empirically discovered that the relationship between\nthe degree of efficiency (the Hurst exponent) and the predictability (the hit\nrate) is strongly positive. That is, a market index with a higher Hurst\nexponent tends to have a higher hit rate. These results suggested that the\nHurst exponent is useful for predicting future price changes. Furthermore, we\nalso discovered that the Hurst exponent and the hit rate are useful as\nstandards that can distinguish emerging capital markets from mature capital\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.1624v1"
    },
    {
        "title": "Moving Mini-Max - a new indicator for technical analysis",
        "authors": [
            "Z. K. Silagadze"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We propose a new indicator for technical analysis. The indicator emphasizes\nmaximums and minimums in price series with inherent smoothing and has a\npotential to be useful in both mechanical trading rules and chart pattern\nanalysis.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.0984v2"
    },
    {
        "title": "Effects of time dependency and efficiency on information flow in\n  financial markets",
        "authors": [
            "Cheoljun Eom",
            "Woo-Sung Jung",
            "Sunghoon Choi",
            "Gabjin Oh",
            "Seunghwan Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We investigated financial market data to determine which factors affect\ninformation flow between stocks. Two factors, the time dependency and the\ndegree of efficiency, were considered in the analysis of Korean, the Japanese,\nthe Taiwanese, the Canadian, and US market data. We found that the frequency of\nthe significant information decreases as the time interval increases. However,\nno significant information flow was observed in the time series from which the\ntemporal time correlation was removed. These results indicated that the\ninformation flow between stocks evidences time-dependency properties.\nFurthermore, we discovered that the difference in the degree of efficiency\nperforms a crucial function in determining the direction of the significant\ninformation flow.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.1500v1"
    },
    {
        "title": "Information flow between stock indices",
        "authors": [
            "Okyu Kwon",
            "Jae-Suk Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Using transfer entropy, we observed the strength and direction of information\nflow between stock indices. We uncovered that the biggest source of information\nflow is America. In contrast, the Asia/Pacific region the biggest is receives\nthe most information. According to the minimum spanning tree, the GSPC is\nlocated at the focal point of the information source for world stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.1747v1"
    },
    {
        "title": "Current log-periodic view on future world market development",
        "authors": [
            "Stanislaw Drozdz",
            "Jaroslaw Kwapien",
            "Pawel Oswiecimka",
            "Josef Speth"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Applicability of the concept of financial log-periodicity is discussed and\nencouragingly verified for various phases of the world stock markets\ndevelopment in the period 2000-2010. In particular, a speculative forecasting\nscenario designed in the end of 2004, that properly predicted the world stock\nmarket increases in 2007, is updated by setting some more precise constraints\non the time of duration of the present long-term equity market bullish phase. A\ntermination of this phase is evaluated to occur in around November 2009. In\nparticular, on the way towards this dead-line, a Spring-Summer 2008 increase is\nexpected. On the precious metals market a forthcoming critical time signal is\ndetected at the turn of March/April 2008 which marks a tendency for at least a\nserious correction to begin.\n  In the present extended version some predictions for the future oil price are\nincorporated. In particular a serious correction on this market is expected to\nstart in the coming days.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.4043v2"
    },
    {
        "title": "Modified Holder Exponents Approach to Prediction of the USA Stock Market\n  Critical Points and Crashes",
        "authors": [
            "Yu. A Kuperin",
            "R. R. Schastlivtsev"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The paper is devoted to elaboration of a novel specific indicator based on\nthe modified Holder exponents. This indicator has been used for forecasting\ncritical points of financial time series and crashes of the USA stock market.\nThe proposed approach is based on the hypothesis, which claims that before\nmarket critical points occur the dynamics of financial time series radically\nchanges, namely time series become smoother. The approach has been tested on\nthe stylized data and real USA stock market data. It has been shown that it is\npossible to forecast such critical points of financial time series as large\nupward and downward movements and trend changes. On this basis a new trading\nstrategy has been elaborated and tested.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.4460v1"
    },
    {
        "title": "Different fractal properties of positive and negative returns",
        "authors": [
            "P. Oswiecimka",
            "J. Kwapien",
            "S. Drozdz",
            "A. Z. Gorski",
            "R. Rak"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We perform an analysis of fractal properties of the positive and the negative\nchanges of the German DAX30 index separately using Multifractal Detrended\nFluctuation Analysis (MFDFA). By calculating the singularity spectra\n$f(\\alpha)$ we show that returns of both signs reveal multiscaling. Curiously,\nthese spectra display a significant difference in the scaling properties of\nreturns with opposite sign. The negative price changes are ruled by stronger\ntemporal correlations than the positive ones, what is manifested by larger\nvalues of the corresponding H\\\"{o}lder exponents. As regards the properties of\ndominant trends, a bear market is more persistent than the bull market\nirrespective of the sign of fluctuations.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.1374v1"
    },
    {
        "title": "Return interval distribution of extreme events and long term memory",
        "authors": [
            "M. S. Santhanam",
            "Holger Kantz"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The distribution of recurrence times or return intervals between extreme\nevents is important to characterize and understand the behavior of physical\nsystems and phenomena in many disciplines. It is well known that many physical\nprocesses in nature and society display long range correlations. Hence, in the\nlast few years, considerable research effort has been directed towards studying\nthe distribution of return intervals for long range correlated time series.\nBased on numerical simulations, it was shown that the return interval\ndistributions are of stretched exponential type. In this paper, we obtain an\nanalytical expression for the distribution of return intervals in long range\ncorrelated time series which holds good when the average return intervals are\nlarge. We show that the distribution is actually a product of power law and a\nstretched exponential form. We also discuss the regimes of validity and perform\ndetailed studies on how the return interval distribution depends on the\nthreshold used to define extreme events.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.1706v1"
    },
    {
        "title": "Comment on ``Tests of scaling and universality of the distributions of\n  trade size and share volume: Evidence from three distinct markets\" by Plerou\n  and Stanley, Phys. Rev. E 76, 046109 (2007)",
        "authors": [
            "Éva Rácz",
            "Zoltán Eisler",
            "János Kertész"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Comment on ``Tests of scaling and universality of the distributions of trade\nsize and share volume: Evidence from three distinct markets\" by Plerou and\nStanley, Phys. Rev. E 76, 046109 (2007)\n",
        "pdf_link": "http://arxiv.org/pdf/0803.3733v1"
    },
    {
        "title": "Estimating correlation from high, low, opening and closing prices",
        "authors": [
            "L. C. G. Rogers",
            "Fanyin Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  In earlier studies, the estimation of the volatility of a stock using\ninformation on the daily opening, closing, high and low prices has been\ndeveloped; the additional information in the high and low prices can be\nincorporated to produce unbiased (or near-unbiased) estimators with\nsubstantially lower variance than the simple open--close estimator. This paper\ntackles the more difficult task of estimating the correlation of two stocks\nbased on the daily opening, closing, high and low prices of each. If we had\naccess to the high and low values of some linear combination of the two log\nprices, then we could use the univariate results via polarization, but this is\nnot data that is available. The actual problem is more challenging; we present\nan unbiased estimator which halves the variance.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0162v1"
    },
    {
        "title": "Scaling in the distribution of intertrade durations of Chinese stocks",
        "authors": [
            "Zhi-Qiang Jiang",
            "Wei Chen",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The distribution of intertrade durations, defined as the waiting times\nbetween two consecutive transactions, is investigated based upon the limit\norder book data of 23 liquid Chinese stocks listed on the Shenzhen Stock\nExchange in the whole year 2003. A scaling pattern is observed in the\ndistributions of intertrade durations, where the empirical density functions of\nthe normalized intertrade durations of all 23 stocks collapse onto a single\ncurve. The scaling pattern is also observed in the intertrade duration\ndistributions for filled and partially filled trades and in the conditional\ndistributions. The ensemble distributions for all stocks are modeled by the\nWeibull and the Tsallis $q$-exponential distributions. Maximum likelihood\nestimation shows that the Weibull distribution outperforms the $q$-exponential\nfor not-too-large intertrade durations which account for more than 98.5% of the\ndata. Alternatively, nonlinear least-squares estimation selects the\n$q$-exponential as a better model, in which the optimization is conducted on\nthe distance between empirical and theoretical values of the logarithmic\nprobability densities. The distribution of intertrade durations is Weibull\nfollowed by a power-law tail with an asymptotic tail exponent close to 3.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.3431v2"
    },
    {
        "title": "A Theory for Market Impact: How Order Flow Affects Stock Price",
        "authors": [
            "Austin Gerig"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  It is known that the impact of transactions on stock price (market impact) is\na concave function of the size of the order, but there exists little\nquantitative theory that suggests why this is so. I develop a quantitative\ntheory for the market impact of hidden orders (orders that reflect the true\nintention of buying and selling) that matches the empirically measured result\nand that reproduces some of the non-trivial and universal properties of stock\nreturns (returns are percent changes in stock price). The theory is based on a\nsimple premise, that the stock market can be modeled in a mechanical way - as a\ndevice that translates order flow into an uncorrelated price stream. Given that\norder flow is highly autocorrelated, this premise requires that market impact\n(1) depends on past order flow and (2) is asymmetric for buying and selling. I\nderive the specific form for the dependence in (1) by assuming that current\nliquidity responds to information about all currently active hidden orders\n(liquidity is a measure of the price response to a transaction of a given\nsize). This produces an equation that suggests market impact should scale\nlogarithmically with total order size. Using data from the London Stock\nExchange I empirically measure market impact and show that the result matches\nthe theory. Also using empirical data, I qualitatively specify the asymmetry of\n(2). Putting all results together, I form a model for market impact that\nreproduces three universal properties of stock returns - that returns are\nuncorrelated, that returns are distributed with a power law tail, and that the\nmagnitude of returns is highly autocorrelated (also known as clustered\nvolatility).\n",
        "pdf_link": "http://arxiv.org/pdf/0804.3818v2"
    },
    {
        "title": "Theory of market fluctuations",
        "authors": [
            "S. V. Panyukov"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We propose coalescent mechanism of economic grow because of redistribution of\nexternal resources. It leads to Zipf distribution of firms over their sizes,\nturning to stretched exponent because of size-dependent effects, and predicts\nexponential distribution of income between individuals. We also present new\napproach to describe fluctuations on the market, based on separation of hot\n(short-time) and cold (long-time) degrees of freedoms, which predicts tent-like\ndistribution of fluctuations with stable tail exponent mu=3 (mu=2 for news).\nThe theory predicts observable asymmetry of the distribution, and its size\ndependence. In the case of financial markets the theory explains first time\nmarket mill patterns, conditional distribution, D-smile, z-shaped response,\nconditional double dynamics, the skewness and so on. We derive the set of\nLangeven equations, which predicts logarithmic dependence of price shift on\ntrading volume and volatility patterns after jumps. We calculate parameters of\nprice distributions, correlation functions and Hurst exponents at different\ntime scales. At large times the price experiences fractional Brownian motion\nwith chaotically switching of long-time persistent and anti-persistent\nbehavior, and we calculate corresponding probabilities, response functions, and\nrisks.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.4191v3"
    },
    {
        "title": "Scaling and Memory Effect in Volatility Return Interval of the Chinese\n  Stock Market",
        "authors": [
            "Tian Qiu",
            "Liang Guo",
            "Guang Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We investigate the probability distribution of the volatility return\nintervals $\\tau$ for the Chinese stock market. We rescale both the probability\ndistribution $P_{q}(\\tau)$ and the volatility return intervals $\\tau$ as\n$P_{q}(\\tau)=1/\\bar{\\tau} f(\\tau/\\bar{\\tau})$ to obtain a uniform scaling curve\nfor different threshold value $q$. The scaling curve can be well fitted by the\nstretched exponential function $f(x) \\sim e^{-\\alpha x^{\\gamma}}$, which\nsuggests memory exists in $\\tau$. To demonstrate the memory effect, we\ninvestigate the conditional probability distribution $P_{q} (\\tau|\\tau_{0})$,\nthe mean conditional interval $<\\tau|\\tau_{0}>$ and the cumulative probability\ndistribution of the cluster size of $\\tau$. The results show clear clustering\neffect. We further investigate the persistence probability distribution\n$P_{\\pm}(t)$ and find that $P_{-}(t)$ decays by a power law with the exponent\nfar different from the value 0.5 for the random walk, which further confirms\nlong memory exists in $\\tau$. The scaling and long memory effect of $\\tau$ for\nthe Chinese stock market are similar to those obtained from the United States\nand the Japanese financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.2194v1"
    },
    {
        "title": "The structural role of weak and strong links in a financial market\n  network",
        "authors": [
            "Antonios Garas",
            "Panos Argyrakis",
            "Shlomo Havlin"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We investigate the properties of correlation based networks originating from\neconomic complex systems, such as the network of stocks traded at the New York\nStock Exchange (NYSE). The weaker links (low correlation) of the system are\nfound to contribute to the overall connectivity of the network significantly\nmore than the strong links (high correlation). We find that nodes connected\nthrough strong links form well defined communities. These communities are\nclustered together in more complex ways compared to the widely used\nclassification according to the economic activity. We find that some companies,\nsuch as General Electric (GE), Coca Cola (KO), and others, can be involved in\ndifferent communities. The communities are found to be quite stable over time.\nSimilar results were obtained by investigating markets completely different in\nsize and properties, such as the Athens Stock Exchange (ASE). The present\nmethod may be also useful for other networks generated through correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.2477v1"
    },
    {
        "title": "Coherence-based multivariate analysis of high frequency stock market\n  values",
        "authors": [
            "Donatello Materassi",
            "Giacomo Innocenti"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The paper tackles the problem of deriving a topological structure among stock\nprices from high frequency historical values. Similar studies using low\nfrequency data have already provided valuable insights. However, in those cases\ndata need to be collected for a longer period and then they have to be\ndetrended. An effective technique based on averaging a metric function on short\nsubperiods of the observation horizon is suggested. Since a standard\ncorrelation-based metric is not capable of catching dependencies at different\ntime instants, it is not expected to perform the best when dealing with high\nfrequency data. Hence, the choice of a more suitable metric is discussed. In\nparticular, a coherence-based metric is proposed, for it is able to detect any\npossible linear relation between two times series, even at different time\ninstants. The averaging technique is employed to analyze a set of 100 high\nvolume stocks of the New York Stock Exchange, observed during March 2008.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.2713v1"
    },
    {
        "title": "On the probability distribution of stock returns in the Mike-Farmer\n  model",
        "authors": [
            "Gao-Feng Gu",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Recently, Mike and Farmer have constructed a very powerful and realistic\nbehavioral model to mimick the dynamic process of stock price formation based\non the empirical regularities of order placement and cancelation in a purely\norder-driven market, which can successfully reproduce the whole distribution of\nreturns, not only the well-known power-law tails, together with several other\nimportant stylized facts. There are three key ingredients in the Mike-Farmer\n(MF) model: the long memory of order signs characterized by the Hurst index\n$H_s$, the distribution of relative order prices $x$ in reference to the same\nbest price described by a Student distribution (or Tsallis' $q$-Gaussian), and\nthe dynamics of order cancelation. They showed that different values of the\nHurst index $H_s$ and the freedom degree $\\alpha_x$ of the Student distribution\ncan always produce power-law tails in the return distribution $f(r)$ with\ndifferent tail exponent $\\alpha_r$. In this paper, we study the origin of the\npower-law tails of the return distribution $f(r)$ in the MF model, based on\nextensive simulations with different combinations of the left part $f_L(x)$ for\n$x<0$ and the right part $f_R(x)$ for $x>0$ of $f(x)$. We find that power-law\ntails appear only when $f_L(x)$ has a power-law tail, no matter $f_R(x)$ has a\npower-law tail or not. In addition, we find that the distributions of returns\nin the MF model at different timescales can be well modeled by the Student\ndistributions, whose tail exponents are close to the well-known cubic law and\nincrease with the timescale.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.3593v1"
    },
    {
        "title": "Inconsistency of the judgment matrix in the AHP method and the decision\n  maker's knowledge",
        "authors": [
            "Anna Szczypinska",
            "Edward W. Piotrowski"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  In this paper we propose a method for a quantitative estimation of the\ndecision maker's knowledge in the context of the Analytic Hierarchy Process\n(AHP) in cases, where the judgment matrix is inconsistent. We show that the\nmatrix of deviation from the transitivity condition corresponds to the rate\nmatrix for transaction costs in the financial market. For the quantitative\nestimation of the decision maker's professionalism, we apply the Ising model\nand thermodynamics tools.\n",
        "pdf_link": "http://arxiv.org/pdf/0806.4876v3"
    },
    {
        "title": "Random matrix theory and the evolution of business cycle synchronisation\n  1886-2006",
        "authors": [
            "Paul Ormerod"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The major study by Bordo and Helbing (2003) analyses the business cycle in\nWestern economies 1881-2001. They examine four distinct periods in economic\nhistory, and conclude that there is a secular trend towards greater\nsynchronisation for much of the 20th century. Their analysis, in common with\nthe standard economic literature on business cycle synchronisation, relies upon\nthe estimation of an empirical correlation matrix of time series data of\nmacroeconomic aggregates. However because of the small number of observations\nand economies, the empirical correlation matrix may contain considerable noise.\nRandom matrix theory was developed to overcome this problem. I use random\nmatrix theory, and the associated technique of agglomerative hierarchical\nclustering, to examine the evolution of business cycle synchronisation between\nthe capitalist economies in the long-run. Contrary to the findings of Bordo and\nHelbing, it is not possible to speak of a 'secular trend' towards greater\nsynchronisation over the period as a whole. During the pre-First World War\nperiod, the cross-country correlations of annual real GDP growth are\nindistinguishable from those which could be generated by a purely random\nmatrix. The periods 1920-38 and 1948-72 do show a certain degree of\nsynchronisation, but it is very weak. In particular, the cycles of the major\neconomies cannot be said to be synchronised. Such synchronisation as exists in\nthe overall data is due to meaningful co-movements in sub-groups. So the degree\nof synchronisation has evolved fitfully. It is only in the most recent\n1973-2006 period that we can speak meaningfully of anything resembling an\ninternational business cycle.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1771v1"
    },
    {
        "title": "The evolution of EU business cycle synchronisation 1981-2007",
        "authors": [
            "Paul Ormerod"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Most of the analytical techniques used in the business cycle synchronisation\nliterature rely upon the estimation of an empirical correlation matrix of time\nseries data of macroeconomic aggregates, real GDP usually being the key\nvariable. But the small number of available observations and small number of\neconomies mean that the empirical correlation matrix may contain considerable\nnoise. Random matrix theory was developed in physics to overcome this problem.\nThe largest eigenvalue of the correlation matrix informs us directly about the\ndegree to which movements of the economies are genuinely correlated. The\nevolution of business cycle synchronisation can be analysed with the temporal\nevolution of the largest eigenvalue over a fixed window of data. I analyse\nquarterly real GDP data 1981Q1-2008Q1 for the core EU economies - Germany,\nFrance, Italy, Spain, Netherlands, Belgium - along with the UK, which is a\nmember of the EU but not the Euro, and the US as a comparator. The core EU\neconomies have shown varying but strong synchronisation over the whole period.\nIn contrast, the UK and the US are much more synchronised with each other than\nthey are with the core EU economies.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1831v1"
    },
    {
        "title": "Scaling and efficiency determine the irreversible evolution of a market",
        "authors": [
            "Fulvio Baldovin",
            "Attilio L. Stella"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  In setting up a stochastic description of the time evolution of a financial\nindex, the challenge consists in devising a model compatible with all stylized\nfacts emerging from the analysis of financial time series and providing a\nreliable basis for simulating such series. Based on constraints imposed by\nmarket efficiency and on an inhomogeneous-time generalization of standard\nsimple scaling, we propose an analytical model which accounts simultaneously\nfor empirical results like the linear decorrelation of successive returns, the\npower law dependence on time of the volatility autocorrelation function, and\nthe multiscaling associated to this dependence. In addition, our approach gives\na justification and a quantitative assessment of the irreversible character of\nthe index dynamics. This irreversibility enters as a key ingredient in a novel\nsimulation strategy of index evolution which demonstrates the predictive\npotential of the model.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.2583v1"
    },
    {
        "title": "What drives mutual fund asset concentration?",
        "authors": [
            "Yonathan Schwarzkopf",
            "J. Doyne Farmer"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Is the large influence that mutual funds assert on the U.S. financial system\nspread across many funds, or is it is concentrated in only a few? We argue that\nthe dominant economic factor that determines this is market efficiency, which\ndictates that fund performance is size independent and fund growth is\nessentially random. The random process is characterized by entry, exit and\ngrowth. We present a new time-dependent solution for the standard equations\nused in the industrial organization literature and show that relaxation to the\nsteady-state solution is extremely slow. Thus, even if these processes were\nstationary (which they are not), the steady-state solution, which is a very\nheavy-tailed power law, is not relevant. The distribution is instead\nwell-approximated by a less heavy-tailed log-normal. We perform an empirical\nanalysis of the growth of mutual funds, propose a new, more accurate\nsize-dependent model, and show that it makes a good prediction of the\nempirically observed size distribution. While mutual funds are in many respects\nlike other firms, market efficiency introduces effects that make their growth\nprocess distinctly different. Our work shows that a simple model based on\nmarket efficiency provides a good explanation of the concentration of assets,\nsuggesting that other effects, such as transaction costs or the behavioral\naspects of investor choice, play a smaller role.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.3800v3"
    },
    {
        "title": "Emergence of long memory in stock volatility from a modified Mike-Farmer\n  model",
        "authors": [
            "Gao-Feng Gu",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The Mike-Farmer (MF) model was constructed empirically based on the\ncontinuous double auction mechanism in an order-driven market, which can\nsuccessfully reproduce the cubic law of returns and the diffusive behavior of\nstock prices at the transaction level. However, the volatility (defined by\nabsolute return) in the MF model does not show sound long memory. We propose a\nmodified version of the MF model by including a new ingredient, that is, long\nmemory in the aggressiveness (quantified by the relative prices) of incoming\norders, which is an important stylized fact identified by analyzing the order\nflows of 23 liquid Chinese stocks. Long memory emerges in the volatility\nsynthesized from the modified MF model with the DFA scaling exponent close to\n0.76, and the cubic law of returns and the diffusive behavior of prices are\nalso produced at the same time. We also find that the long memory of order\nsigns has no impact on the long memory property of volatility, and the memory\neffect of order aggressiveness has little impact on the diffusiveness of stock\nprices.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.4639v3"
    },
    {
        "title": "Changes in the Distribution of Income Volatility",
        "authors": [
            "Shane T. Jensen",
            "Stephen H. Shore"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Recent research has documented a significant rise in the volatility (e.g.,\nexpected squared change) of individual incomes in the U.S. since the 1970s.\nExisting measures of this trend abstract from individual heterogeneity,\neffectively estimating an increase in average volatility. We decompose this\nincrease in average volatility and find that it is far from representative of\nthe experience of most people: there has been no systematic rise in volatility\nfor the vast majority of individuals. The rise in average volatility has been\ndriven almost entirely by a sharp rise in the income volatility of those\nexpected to have the most volatile incomes, identified ex-ante by large income\nchanges in the past. We document that the self-employed and those who\nself-identify as risk-tolerant are much more likely to have such volatile\nincomes; these groups have experienced much larger increases in income\nvolatility than the population at large. These results color the policy\nimplications one might draw from the rise in average volatility. While the\nbasic results are apparent from PSID summary statistics, providing a complete\ncharacterization of the dynamics of the volatility distribution is a\nmethodological challenge. We resolve these difficulties with a Markovian\nhierarchical Dirichlet process that builds on work from the non-parametric\nBayesian statistics literature.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.1090v1"
    },
    {
        "title": "Heterogeneous expectations and long range correlation of the volatility\n  of asset returns",
        "authors": [
            "Jerome Coulon",
            "Yannick Malevergne"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Inspired by the recent literature on aggregation theory, we aim at relating\nthe long range correlation of the stocks return volatility to the heterogeneity\nof the investors' expectations about the level of the future volatility. Based\non a semi-parametric model of investors' anticipations, we make the connection\nbetween the distributional properties of the heterogeneity parameters and the\nauto-covariance/auto-correlation functions of the realized volatility. We\nreport different behaviors, or change of convention, whose observation depends\non the market phase under consideration. In particular, we report and justify\nthe fact that the volatility exhibits significantly longer memory during the\nphases of speculative bubble than during the phase of recovery following the\ncollapse of a speculative bubble.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.1538v1"
    },
    {
        "title": "Multifactor Analysis of Multiscaling in Volatility Return Intervals",
        "authors": [
            "Fengzhong Wang",
            "Kazuko Yamasaki",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We study the volatility time series of 1137 most traded stocks in the US\nstock markets for the two-year period 2001-02 and analyze their return\nintervals $\\tau$, which are time intervals between volatilities above a given\nthreshold $q$. We explore the probability density function of $\\tau$,\n$P_q(\\tau)$, assuming a stretched exponential function, $P_q(\\tau) \\sim\ne^{-\\tau^\\gamma}$. We find that the exponent $\\gamma$ depends on the threshold\nin the range between $q=1$ and 6 standard deviations of the volatility. This\nfinding supports the multiscaling nature of the return interval distribution.\nTo better understand the multiscaling origin, we study how $\\gamma$ depends on\nfour essential factors, capitalization, risk, number of trades and return. We\nshow that $\\gamma$ depends on the capitalization, risk and return but almost\ndoes not depend on the number of trades. This suggests that $\\gamma$ relates to\nthe portfolio selection but not on the market activity. To further characterize\nthe multiscaling of individual stocks, we fit the moments of $\\tau$, $\\mu_m\n\\equiv <(\\tau/<\\tau>)^m>^{1/m}$, in the range of $10 < <\\tau> \\le 100$ by a\npower-law, $\\mu_m \\sim <\\tau>^\\delta$. The exponent $\\delta$ is found also to\ndepend on the capitalization, risk and return but not on the number of trades,\nand its tendency is opposite to that of $\\gamma$. Moreover, we show that\n$\\delta$ decreases with $\\gamma$ approximately by a linear relation. The return\nintervals demonstrate the temporal structure of volatilities and our findings\nsuggest that their multiscaling features may be helpful for portfolio\noptimization.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.3200v1"
    },
    {
        "title": "Criticality Characteristics of Current Oil Price Dynamics",
        "authors": [
            "Stanislaw Drozdz",
            "Jaroslaw Kwapien",
            "Pawel Oswiecimka"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Methodology that recently lead us to predict to an amazing accuracy the date\n(July 11, 2008) of reverse of the oil price up trend is briefly summarized and\nsome further aspects of the related oil price dynamics elaborated. This\nmethodology is based on the concept of discrete scale invariance whose\nfinance-prediction-oriented variant involves such elements as log-periodic\nself-similarity, the universal preferred scaling factor lambda=2, and allows a\nphenomenon of the \"super-bubble\". From this perspective the present (as of\nAugust 22, 2008) violent - but still log-periodically decelerating - decrease\nof the oil prices is associated with the decay of such a \"super- bubble\" that\nhas started developing about one year ago on top of the longer-term oil price\nincreasing phase (normal bubble) whose ultimate termination is evaluated to\noccur in around mid 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.3360v1"
    },
    {
        "title": "Multiscaling behavior in the volatility return intervals of Chinese\n  indices",
        "authors": [
            "Fei Ren",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We investigate the probability distribution of the return intervals $\\tau$\nbetween successive 1-min volatilities of two Chinese indices exceeding a\ncertain threshold $q$. The Kolmogorov-Smirnov (KS) tests show that the two\nindices exhibit multiscaling behavior in the distribution of $\\tau$, which\nfollows a stretched exponential form $f_q(\\tau/< \\tau >)\\sim e^{- a(\\tau/ <\n\\tau >)^{\\gamma}}$ with different correlation exponent $\\gamma$ for different\nthreshold $q$, where $<\\tau>$ is the mean return interval corresponding to a\ncertain value of $q$. An extended self-similarity analysis of the moments\nprovides further evidence of multiscaling in the return intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.0250v1"
    },
    {
        "title": "Minimal Spanning Tree graphs and power like scaling in FOREX networks",
        "authors": [
            "A Z Gorski",
            "S. Drozdz",
            "J. Kwapien"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Correlation matrices of foreign exchange rate time series are investigated\nfor 60 world currencies. Minimal Spanning Tree (MST) graphs for the gold,\nsilver and platinum are presented. Inverse power like scaling is discussed for\nthese graphs as well as for four distinct currency groups (major, liquid, less\nliquid and non-tradable). The worst scaling has been found for USD and related\ncurrencies.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.0437v1"
    },
    {
        "title": "Patterns in high-frequency FX data: Discovery of 12 empirical scaling\n  laws",
        "authors": [
            "J. B. Glattfelder",
            "A. Dupuis",
            "R. B. Olsen"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We have discovered 12 independent new empirical scaling laws in foreign\nexchange data-series that hold for close to three orders of magnitude and\nacross 13 currency exchange rates. Our statistical analysis crucially depends\non an event-based approach that measures the relationship between different\ntypes of events. The scaling laws give an accurate estimation of the length of\nthe price-curve coastline, which turns out to be surprisingly long. The new\nlaws substantially extend the catalogue of stylised facts and sharply constrain\nthe space of possible theoretical explanations of the market mechanisms.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.1040v2"
    },
    {
        "title": "Stock market volatility: An approach based on Tsallis entropy",
        "authors": [
            "Sonia R. Bentes",
            "Rui Menezes",
            "Diana A. Mendes"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  One of the major issues studied in finance that has always intrigued, both\nscholars and practitioners, and to which no unified theory has yet been\ndiscovered, is the reason why prices move over time. Since there are several\nwell-known traditional techniques in the literature to measure stock market\nvolatility, a central point in this debate that constitutes the actual scope of\nthis paper is to compare this common approach in which we discuss such popular\ntechniques as the standard deviation and an innovative methodology based on\nEconophysics. In our study, we use the concept of Tsallis entropy to capture\nthe nature of volatility. More precisely, what we want to find out is if\nTsallis entropy is able to detect volatility in stock market indexes and to\ncompare its values with the ones obtained from the standard deviation. Also, we\nshall mention that one of the advantages of this new methodology is its ability\nto capture nonlinear dynamics. For our purpose, we shall basically focus on the\nbehaviour of stock market indexes and consider the CAC 40, MIB 30, NIKKEI 225,\nPSI 20, IBEX 35, FTSE 100 and SP 500 for a comparative analysis between the\napproaches mentioned above.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.4570v1"
    },
    {
        "title": "Scale free effects in world currency exchange network",
        "authors": [
            "A. Z. Gorski",
            "S. Drozdz",
            "J. Kwapien"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  A large collection of daily time series for 60 world currencies' exchange\nrates is considered. The correlation matrices are calculated and the\ncorresponding Minimal Spanning Tree (MST) graphs are constructed for each of\nthose currencies used as reference for the remaining ones. It is shown that\nmultiplicity of the MST graphs' nodes to a good approximation develops a power\nlike, scale free distribution with the scaling exponent similar as for several\nother complex systems studied so far. Furthermore, quantitative arguments in\nfavor of the hierarchical organization of the world currency exchange network\nare provided by relating the structure of the above MST graphs and their\nscaling exponents to those that are derived from an exactly solvable\nhierarchical network model. A special status of the USD during the period\nconsidered can be attributed to some departures of the MST features, when this\ncurrency (or some other tied to it) is used as reference, from characteristics\ntypical to such a hierarchical clustering of nodes towards those that\ncorrespond to the random graphs. Even though in general the basic structure of\nthe MST is robust with respect to changing the reference currency some trace of\na systematic transition from somewhat dispersed -- like the USD case -- towards\nmore compact MST topology can be observed when correlations increase.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.1215v1"
    },
    {
        "title": "Volatility Effects on the Escape Time in Financial Market Models",
        "authors": [
            "Bernardo Spagnolo",
            "Davide Valenti"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We shortly review the statistical properties of the escape times, or hitting\ntimes, for stock price returns by using different models which describe the\nstock market evolution. We compare the probability function (PF) of these\nescape times with that obtained from real market data. Afterwards we analyze in\ndetail the effect both of noise and different initial conditions on the escape\ntime in a market model with stochastic volatility and a cubic nonlinearity. For\nthis model we compare the PF of the stock price returns, the PF of the\nvolatility and the return correlation with the same statistical characteristics\nobtained from real market data.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.1625v1"
    },
    {
        "title": "Market Mill Dependence Pattern in the Stock Market: Multiscale\n  Conditional Dynamics",
        "authors": [
            "Sergey Zaitsev",
            "Alexander Zaitsev",
            "Andrei Leonidov",
            "Vladimir Trainin"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Market Mill is a complex dependence pattern leading to nonlinear correlations\nand predictability in intraday dynamics of stock prices. The present paper puts\ntogether previous efforts to build a dynamical model reflecting the market mill\nasymmetries. We show that certain properties of the conditional dynamics at a\nsingle time scale such as a characteristic shape of an asymmetry generating\ncomponent of the conditional probability distribution result in the\n\"elementary\" market mill pattern. This asymmetry generating component matches\nthe empirical distribution obtained from the market data. We discuss these\nproperties as a mixture of trend-preserving and contrarian strategies used by\nmarket agents. Three basic types of asymmetry patterns characterizing\nindividual stocks are outlined. Multiple time scale considerations make the\nresulting \"composite\" mill similar to the empirical market mill patterns.\nMultiscale model also reflects a multi-agent nature of the market.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.4409v2"
    },
    {
        "title": "Serial correlation and heterogeneous volatility in financial markets:\n  beyond the LeBaron effect",
        "authors": [
            "Simone Bianco",
            "Fulvio Corsi",
            "Roberto Reno'"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We study the relation between serial correlation of financial returns and\nvolatility at intraday level for the S&P500 stock index. At daily and weekly\nlevel, serial correlation and volatility are known to be negatively correlated\n(LeBaron effect). While confirming that the LeBaron effect holds also at\nintraday level, we go beyond it and, complementing the efficient market\nhyphotesis (for returns) with the heterogenous market hyphotesis (for\nvolatility), we test the impact of unexpected volatility, defined as the part\nof volatility which cannot be forecasted, on the presence of serial\ncorrelations in the time series. We show that unexpected volatility is instead\npositively correlated with intraday serial correlation.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.4912v1"
    },
    {
        "title": "A multiscale view on inverse statistics and gain/loss asymmetry in\n  financial time series",
        "authors": [
            "Johannes Vitalis Siven",
            "Jeffrey Todd Lins",
            "Jonas Lundbek Hansen"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Researchers have studied the first passage time of financial time series and\nobserved that the smallest time interval needed for a stock index to move a\ngiven distance is typically shorter for negative than for positive price\nmovements. The same is not observed for the index constituents, the individual\nstocks. We use the discrete wavelet transform to illustrate that this is a long\nrather than short time scale phenomenon -- if enough low frequency content of\nthe price process is removed, the asymmetry disappears. We also propose a new\nmodel, which explain the asymmetry by prolonged, correlated down movements of\nindividual stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.3122v1"
    },
    {
        "title": "Effect of changing data size on eigenvalues in the Korean and Japanese\n  stock markets",
        "authors": [
            "Cheoljun Eom",
            "Woo-Sung Jung",
            "Taisei Kaizoji",
            "Seunghwan Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  In this study, we attempted to determine how eigenvalues change, according to\nrandom matrix theory (RMT), in stock market data as the number of stocks\ncomprising the correlation matrix changes. Specifically, we tested for changes\nin the eigenvalue properties as a function of the number and type of stocks in\nthe correlation matrix. We determined that the value of the eigenvalue\nincreases in proportion with the number of stocks. Furthermore, we noted that\nthe largest eigenvalue maintains its identical properties, regardless of the\nnumber and type, whereas other eigenvalues evidence different features.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.4021v2"
    },
    {
        "title": "The Spread of the Credit Crisis: View from a Stock Correlation Network",
        "authors": [
            "Reginald D. Smith"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  The credit crisis roiling the world's financial markets will likely take\nyears and entire careers to fully understand and analyze. A short empirical\ninvestigation of the current trends, however, demonstrates that the losses in\ncertain markets, in this case the US equity markets, follow a cascade or\nepidemic flow like model along the correlations of various stocks. This\nphenomenon will be shown by the graphical display of stock returns across the\nnetwork as well as the dependence of stock returns on topological measures.\nFinally, whether the idea of \"epidemic\" or a \"cascade\" is a metaphor or model\nfor this crisis will be discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.1392v5"
    },
    {
        "title": "Structure and evolution of the foreign exchange networks",
        "authors": [
            "Jaroslaw Kwapien",
            "Sylwia Gworek",
            "Stanislaw Drozdz"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We investigate topology and temporal evolution of the foreign currency\nexchange market viewed from a weighted network perspective. Based on exchange\nrates for a set of 46 currencies (including precious metals), we construct\ndifferent representations of the FX network depending on a choice of the base\ncurrency. Our results show that the network structure is not stable in time,\nbut there are main clusters of currencies, which persist for a long period of\ntime despite the fact that their size and content are variable. We find a\nlong-term trend in the network's evolution which affects the USD and EUR nodes.\nIn all the network representations, the USD node gradually loses its\ncentrality, while, on contrary, the EUR node has become slightly more central\nthan it used to be in its early years. Despite this directional trend, the\noverall evolution of the network is noisy.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.4793v1"
    },
    {
        "title": "Threshold levels in Economics",
        "authors": [
            "V. P. Maslov"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  In this paper, we present theorems specifying the critical values for series\nassociated with debts arranged in the order of their duration.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.4783v2"
    },
    {
        "title": "Stock Market and Motion of a Variable Mass Spring",
        "authors": [
            "Enrique Canessa"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We establish an analogy between the motion of spring whose mass increases\nlinearly with time and volatile stock markets dynamics within an economic model\nbased on simple temporal demand and supply functions [J. Phys. A: Math. Gen.\n33, 3637 (2000)]. The total system energy E_t is shown to be proportional to a\ndecreasing time dependent spring constant k_t. This model allows to derive\nlog-periodicity cos[log (t-t_{c})] on commodity prices and oscillations\n(surplus and shortages) in the level of stocks. We also made an attempt to\nconnect these results to the Tsallis statistics parameter q based on a possible\nforce-entropy correlation [Physica A 341, 165 (2004)] and find that the Tsallis\nsecond entropic term \\sum_{i=1}^{W} p_i^{q}/(q-1) relates to the square of the\ndemand (or supply) function.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4450v1"
    },
    {
        "title": "The components of empirical multifractality in financial returns",
        "authors": [
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We perform a systematic investigation on the components of the empirical\nmultifractality of financial returns using the daily data of Dow Jones\nIndustrial Average from 26 May 1896 to 27 April 2007 as an example. The\ntemporal structure and fat-tailed distribution of the returns are considered as\npossible influence factors. The multifractal spectrum of the original return\nseries is compared with those of four kinds of surrogate data: (1) shuffled\ndata that contain no temporal correlation but have the same distribution, (2)\nsurrogate data in which any nonlinear correlation is removed but the\ndistribution and linear correlation are preserved, (3) surrogate data in which\nlarge positive and negative returns are replaced with small values, and (4)\nsurrogate data generated from alternative fat-tailed distributions with the\ntemporal correlation preserved. We find that all these factors have influence\non the multifractal spectrum. We also find that the temporal structure (linear\nor nonlinear) has minor impact on the singularity width $\\Delta\\alpha$ of the\nmultifractal spectrum while the fat tails have major impact on $\\Delta\\alpha$,\nwhich confirms the earlier results. In addition, the linear correlation is\nfound to have only a horizontal translation effect on the multifractal spectrum\nin which the distance is approximately equal to the difference between its DFA\nscaling exponent and 0.5. Our method can also be applied to other financial or\nphysical variables and other multifractal formalisms.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.1089v2"
    },
    {
        "title": "Modeling the non-Markovian, non-stationary scaling dynamics of financial\n  markets",
        "authors": [
            "Fulvio Baldovin",
            "Dario Bovina",
            "Francesco Camana",
            "Attilio L. Stella"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  A central problem of Quantitative Finance is that of formulating a\nprobabilistic model of the time evolution of asset prices allowing reliable\npredictions on their future volatility. As in several natural phenomena, the\npredictions of such a model must be compared with the data of a single process\nrealization in our records. In order to give statistical significance to such a\ncomparison, assumptions of stationarity for some quantities extracted from the\nsingle historical time series, like the distribution of the returns over a\ngiven time interval, cannot be avoided. Such assumptions entail the risk of\nmasking or misrepresenting non-stationarities of the underlying process, and of\ngiving an incorrect account of its correlations. Here we overcome this\ndifficulty by showing that five years of daily Euro/US-Dollar trading records\nin the about three hours following the New York market opening, provide a rich\nenough ensemble of histories. The statistics of this ensemble allows to propose\nand test an adequate model of the stochastic process driving the exchange rate.\nThis turns out to be a non-Markovian, self-similar process with non-stationary\nreturns. The empirical ensemble correlators are in agreement with the\npredictions of this model, which is constructed on the basis of the\ntime-inhomogeneous, anomalous scaling obeyed by the return distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.3244v2"
    },
    {
        "title": "Wavelet Based Volatility Clustering Estimation of Foreign Exchange Rates",
        "authors": [
            "A. N. Sekar Iyengar"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We have presented a novel technique of detecting intermittencies in a\nfinancial time series of the foreign exchange rate data of U.S.- Euro\ndollar(US/EUR) using a combination of both statistical and spectral techniques.\nThis has been possible due to Continuous Wavelet Transform (CWT) analysis which\nhas been popularly applied to fluctuating data in various fields science and\nengineering and is also being tried out in finance and economics. We have been\nable to qualitatively identify the presence of nonlinearity and chaos in the\ntime series of the foreign exchange rates for US/EURO (United States dollar to\nEuro Dollar) and US/UK (United States dollar to United Kingdom Pound)\ncurrencies. Interestingly we find that for the US-INDIA(United States dollar to\nIndian Rupee) foreign exchange rates, no such chaotic dynamics is observed.\nThis could be a result of the government control over the foreign exchange\nrates, instead of the market controlling them.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.0087v1"
    },
    {
        "title": "Multifractal analysis and instability index of prior-to-crash market\n  situations",
        "authors": [
            "M. Piacquadio",
            "F. O. Redelico"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We take prior-to-crash market prices (NASDAQ, Dow Jones Industrial Average)\nas a signal, a function of time, we project these discrete values onto a\nvertical axis, thus obtaining a Cantordust. We study said cantordust with the\ntools of multifractal analysis, obtaining spectra by definition and by\nlagrangian coordinates. These spectra have properties that typify the\nprior-to-crash market situation. Any of these spectra entail elaborate\nprocessing of the raw signal data. With the unprocessed raw data we obtain an\ninstability index, also with properties that typify the prior-to-crisis market\nsituation. Both spectra and the instability index agree in characterizing such\ncrashes, and in giving an early warning of them.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.2474v1"
    },
    {
        "title": "Compensating asynchrony effects in the calculation of financial\n  correlations",
        "authors": [
            "Michael C. Münnix",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We present a method to compensate statistical errors in the calculation of\ncorrelations on asynchronous time series. The method is based on the assumption\nof an underlying time series. We set up a model and apply it to financial data\nto examine the decrease of calculated correlations towards smaller return\nintervals (Epps effect). We show that this statistical effect is a major cause\nof the Epps effect. Hence, we are able to quantify and to compensate it using\nonly trading prices and trading times.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.2909v3"
    },
    {
        "title": "Multifractal dynamics of stock markets",
        "authors": [
            "Dariusz Grech",
            "Lukasz Czarnecki"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We present a comparative analysis of multifractal properties of financial\ntime series built on stock indices from developing (WIG) and developed (S&P500)\nfinancial markets. It is shown how the multifractal image of the market is\naltered with the change of the length of time series and with the economic\nsituation on the market. We emphasize that the proper adjustment of scaling\nrange for multiscaling power laws is essential to obtain the multifractal image\nof time series. We analyze in this paper multifractal properties of real\nfinancial time series using H\\\"older $f(\\alpha)$ representation and\nmultifractal-DFA method. It is also investigated how multifractal properties of\nstocks change with variety of \"surgeries\" done on the initial real financial\ntime series. This way we reveal main phenomena on the market influencing its\nmultifractal dynamics. In particular, we focus on examining how multifractal\npicture of real time series changes when one cuts off extreme events like\ncrashes or rupture points, and how fluctuations around the main trend in time\nseries influence the multifractal behavior of financial series in the long-time\nhorizon for both developed and developing markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.3390v1"
    },
    {
        "title": "The level crossing and inverse statistic analysis of German stock market\n  index (DAX) and daily oil price time series",
        "authors": [
            "F. Shayeganfar",
            "M. Holling",
            "J. Peinke",
            "M. Reza Rahimi Tabar"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  The level crossing and inverse statistics analysis of DAX and oil price time\nseries are given. We determine the average frequency of positive-slope\ncrossings, $\\nu_{\\alpha}^+$, where $T_{\\alpha} =1/\\nu_{\\alpha}^+ $ is the\naverage waiting time for observing the level $\\alpha$ again. We estimate the\nprobability $P(K, \\alpha)$, which provides us the probability of observing $K$\ntimes of the level $\\alpha$ with positive slope, in time scale $T_{\\alpha}$.\nFor analyzed time series we found that maximum $K$ is about 6. We show that by\nusing the level crossing analysis one can estimate how the DAX and oil time\nseries will develop. We carry out same analysis for the increments of DAX and\noil price log-returns,(which is known as inverse statistics) and provide the\ndistribution of waiting times to observe some level for the increments.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.4401v3"
    },
    {
        "title": "Impact of the tick-size on financial returns and correlations",
        "authors": [
            "Michael C. Münnix",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We demonstrate that the lowest possible price change (tick-size) has a large\nimpact on the structure of financial return distributions. It induces a\nmicrostructure as well as it can alter the tail behavior. On small return\nintervals, the tick-size can distort the calculation of correlations. This\nespecially occurs on small return intervals and thus contributes to the decay\nof the correlation coefficient towards smaller return intervals (Epps effect).\nWe study this behavior within a model and identify the effect in market data.\nFurthermore, we present a method to compensate this purely statistical error.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.5124v4"
    },
    {
        "title": "Asset returns and volatility clustering in financial time series",
        "authors": [
            "Jie-Jun Tseng",
            "Sai-Ping Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  An analysis of the stylized facts in financial time series is carried out. We\nfind that, instead of the heavy tails in asset return distributions, the slow\ndecay behaviour in autocorrelation functions of absolute returns is actually\ndirectly related to the degree of clustering of large fluctuations within the\nfinancial time series. We also introduce an index to quantitatively measure the\nclustering behaviour of fluctuations in these time series and show that big\nlosses in financial markets usually lump more severely than big gains. We\nfurther give examples to demonstrate that comparing to conventional methods,\nour index enables one to extract more information from the financial time\nseries.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.0284v2"
    },
    {
        "title": "A new space-time model for volatility clustering in the financial market",
        "authors": [
            "Maria Boguta",
            "Eric Järpe"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  A new space-time model for interacting agents on the financial market is\npresented. It is a combination of the Curie-Weiss model and a space-time model\nintroduced by J\\\"arpe 2005. Properties of the model are derived with focus on\nthe critical temperature and magnetization. It turns out that the Hamiltonian\nis a sufficient statistic for the temperature parameter and thus statistical\ninference about this parameter can be performed. Thus e.g. statements about how\nfar the current financial situation is from a financial crisis can be made, and\nfinancial trading stability be monitored for detection of malicious risk\nindicating signals.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.0609v1"
    },
    {
        "title": "Recurrence interval analysis of trading volumes",
        "authors": [
            "Fei Ren",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We study the statistical properties of the recurrence intervals $\\tau$\nbetween successive trading volumes exceeding a certain threshold $q$. The\nrecurrence interval analysis is carried out for the 20 liquid Chinese stocks\ncovering a period from January 2000 to May 2009, and two Chinese indices from\nJanuary 2003 to April 2009. Similar to the recurrence interval distribution of\nthe price returns, the tail of the recurrence interval distribution of the\ntrading volumes follows a power-law scaling, and the results are verified by\nthe goodness-of-fit tests using the Kolmogorov-Smirnov (KS) statistic, the\nweighted KS statistic and the Cram{\\'{e}}r-von Mises criterion. The\nmeasurements of the conditional probability distribution and the detrended\nfluctuation function show that both short-term and long-term memory effects\nexist in the recurrence intervals between trading volumes. We further study the\nrelationship between trading volumes and price returns based on the recurrence\ninterval analysis method. It is found that large trading volumes are more\nlikely to occur following large price returns, and the comovement between\ntrading volumes and price returns is more pronounced for large trading volumes.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1653v1"
    },
    {
        "title": "Adaptive financial networks with static and dynamic thresholds",
        "authors": [
            "Tian Qiu",
            "Bo Zheng",
            "Guang Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Based on the daily data of American and Chinese stock markets, the dynamic\nbehavior of a financial network with static and dynamic thresholds is\ninvestigated. Compared with the static threshold, the dynamic threshold\nsuppresses the large fluctuation induced by the cross-correlation of individual\nstock prices, and leads to a stable topological structure in the dynamic\nevolution. Long-range time-correlations are revealed for the average clustering\ncoefficient, average degree and cross-correlation of degrees. The dynamic\nnetwork shows a two-peak behavior in the degree distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.3432v1"
    },
    {
        "title": "Large-volatility dynamics in financial markets",
        "authors": [
            "X. F. Jiang",
            "B. Zheng",
            "J. Shen"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We investigate the large-volatility dynamics in financial markets, based on\nthe minute-to-minute and daily data of the Chinese Indices and German DAX. The\ndynamic relaxation both before and after large volatilities is characterized by\na power law, and the exponents $p_\\pm$ usually vary with the strength of the\nlarge volatilities. The large-volatility dynamics is time-reversal symmetric at\nthe time scale in minutes, while asymmetric at the daily time scale. Careful\nanalysis reveals that the time-reversal asymmetry is mainly induced by\nexogenous events. It is also the exogenous events which drive the financial\ndynamics to a non-stationary state. Different characteristics of the Chinese\nand German stock markets are uncovered.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.3747v2"
    },
    {
        "title": "Characterizing Multi-Scale Self-Similar Behavior and Non-Statistical\n  Properties of Financial Time Series",
        "authors": [
            "Sayantan Ghosh",
            "P. Manimaran",
            "Prasanta K. Panigrahi"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We make use of wavelet transform to study the multi-scale, self similar\nbehavior and deviations thereof, in the stock prices of large companies,\nbelonging to different economic sectors. The stock market returns exhibit\nmulti-fractal characteristics, with some of the companies showing deviations at\nsmall and large scales. The fact that, the wavelets belonging to the\nDaubechies' (Db) basis enables one to isolate local polynomial trends of\ndifferent degrees, plays the key role in isolating fluctuations at different\nscales. One of the primary motivations of this work is to study the emergence\nof the $k^{-3}$ behavior \\cite{hes5} of the fluctuations starting with high\nfrequency fluctuations. We make use of Db4 and Db6 basis sets to respectively\nisolate local linear and quadratic trends at different scales in order to study\nthe statistical characteristics of these financial time series. The\nfluctuations reveal fat tail non-Gaussian behavior, unstable periodic\nmodulations, at finer scales, from which the characteristic $k^{-3}$ power law\nbehavior emerges at sufficiently large scales. We further identify stable\nperiodic behavior through the continuous Morlet wavelet.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.2539v4"
    },
    {
        "title": "Characteristics of Real Futures Trading Networks",
        "authors": [
            "Junjie Wang",
            "Shuigeng Zhou",
            "Jihong Guan"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Futures trading is the core of futures business, and it is considered as one\nof the typical complex systems. To investigate the complexity of futures\ntrading, we employ the analytical method of complex networks. First, we use\nreal trading records from the Shanghai Futures Exchange to construct futures\ntrading networks, in which nodes are trading participants, and two nodes have a\ncommon edge if the two corresponding investors appear simultaneously in at\nleast one trading record as a purchaser and a seller respectively. Then, we\nconduct a comprehensive statistical analysis on the constructed futures trading\nnetworks. Empirical results show that the futures trading networks exhibit\nfeatures such as scale-free behavior with interesting odd-even-degree\ndivergence in low-degree regions, small-world effect, hierarchical\norganization, power-law betweenness distribution, disassortative mixing, and\nshrinkage of both the average path length and the diameter as network size\nincreases. To the best of our knowledge, this is the first work that uses real\ndata to study futures trading networks, and we argue that the research results\ncan shed light on the nature of real futures business.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4402v2"
    },
    {
        "title": "Persistent collective trend in stock markets",
        "authors": [
            "Emeric Balogh",
            "Ingve Simonsen",
            "Balint Zs. Nagy",
            "Zoltan Neda"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Empirical evidence is given for a significant difference in the collective\ntrend of the share prices during the stock index rising and falling periods.\nData on the Dow Jones Industrial Average and its stock components are studied\nbetween 1991 and 2008. Pearson-type correlations are computed between the\nstocks and averaged over stock-pairs and time. The results indicate a general\ntrend: whenever the stock index is falling the stock prices are changing in a\nmore correlated manner than in case the stock index is ascending. A thorough\nstatistical analysis of the data shows that the observed difference is\nsignificant, suggesting a constant-fear factor among stockholders.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0378v1"
    },
    {
        "title": "Note on log-periodic description of 2008 financial crash",
        "authors": [
            "Katarzyna Bolonek-Lason",
            "Piotr Kosinski"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We analyze the financial crash in 2008 for different financial markets from\nthe point of view of log-periodic function model. In particular, we consider\nDow Jones index, DAX index and Hang Seng index. We shortly discuss the possible\nrelation of the theory of critical phenomena in physics to financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.2044v2"
    },
    {
        "title": "Scaling and multiscaling in financial series: a simple model",
        "authors": [
            "Alessandro Andreoli",
            "Francesco Caravenna",
            "Paolo Dai Pra",
            "Gustavo Posta"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We propose a simple stochastic volatility model which is analytically\ntractable, very easy to simulate and which captures some relevant stylized\nfacts of financial assets, including scaling properties. In particular, the\nmodel displays a crossover in the log-return distribution from power-law tails\n(small time) to a Gaussian behavior (large time), slow decay in the volatility\nautocorrelation and multiscaling of moments. Despite its few parameters, the\nmodel is able to fit several key features of the time series of financial\nindexes, such as the Dow Jones Industrial Average, with a remarkable accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.0155v2"
    },
    {
        "title": "On-line trading as a renewal process: Waiting time and inspection\n  paradox",
        "authors": [
            "Jun-ichi Inoue",
            "Naoya Sazuka",
            "Enrico Scalas"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We briefly review our recent studies on stochastic processes modelling\ninternet on-line trading. We present a way to evaluate the average waiting time\nbetween the observation of the price in financial markets and the next price\nchange, especially in an on-line foreign exchange trading service for\nindividual customers via the internet. The basic method of our approach depends\non the so-called renewal-reward theorem. Assuming that the stochastic process\nmodelling the price change is a renewal process, we use the theorem to\ncalculate the average waiting time of the process. The so-called ``inspection\nparadox'' is discussed, which, in general, means that the average durations is\nshorter than the average waiting time.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.3347v1"
    },
    {
        "title": "A contribution to the systematics of stochastic volatility models",
        "authors": [
            "Frantisek Slanina"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We compare systematically several classes of stochastic volatility models of\nstock market fluctuations. We show that the long-time return distribution is\neither Gaussian or develops a power-law tail, while the short-time return\ndistribution has generically a stretched-exponential form, but can assume also\nan algebraic decay, in the family of models which we call ``GARCH''-type. The\nintermediate regime is found in the exponential Ornstein-Uhlenbeck process. We\ncalculate also the decay of the autocorrelation function of volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.2696v1"
    },
    {
        "title": "The endogenous dynamics of markets: price impact and feedback loops",
        "authors": [
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We review the evidence that the erratic dynamics of markets is to a large\nextent of endogenous origin, i.e. determined by the trading activity itself and\nnot due to the rational processing of exogenous news. In order to understand\nwhy and how prices move, the joint fluctuations of order flow and liquidity -\nand the way these impact prices - become the key ingredients. Impact is\nnecessary for private information to be reflected in prices, but by the same\ntoken, random fluctuations in order flow necessarily contribute to the\nvolatility of markets. Our thesis is that the latter contribution is in fact\ndominant, resulting in a decoupling between prices and fundamental values, at\nleast on short to medium time scales. We argue that markets operate in a regime\nof vanishing revealed liquidity, but large latent liquidity, which would\nexplain their hyper-sensitivity to fluctuations. More precisely, we identify a\ndangerous feedback loop between bid-ask spread and volatility that may lead to\nmicro-liquidity crises and price jumps. We discuss several other unstable\nfeedback loops that should be relevant to account for market crises: imitation,\nunwarranted quantitative models, pro-cyclical regulation, etc.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.2928v1"
    },
    {
        "title": "Statistical causes for the Epps effect in microstructure noise",
        "authors": [
            "Michael C. Münnix",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We present two statistical causes for the distortion of correlations on\nhigh-frequency financial data. We demonstrate that the asynchrony of trades as\nwell as the decimalization of stock prices has a large impact on the decline of\nthe correlation coefficients towards smaller return intervals (Epps effect).\nThese distortions depend on the properties of the time series and are of purely\nstatistical origin. We are able to present parameter-free compensation methods,\nwhich we validate in a model setup. Furthermore, the compensation methods are\napplied to high-frequency empirical data from the NYSE's TAQ database. A major\nfraction of the Epps effect can be compensated. The contribution of the\npresented causes is particularly high for stocks that are traded at low prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.6157v1"
    },
    {
        "title": "Brownian markets",
        "authors": [
            "R. Tsekov"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Financial market dynamics is rigorously studied via the exact generalized\nLangevin equation. Assuming market Brownian self-similarity, the market return\nrate memory and autocorrelation functions are derived, which exhibit an\noscillatory-decaying behavior with a long-time tail, similar to empirical\nobservations. Individual stocks are also described via the generalized Langevin\nequation. They are classified by their relation to the market memory as heavy,\nneutral and light stocks, possessing different kinds of autocorrelation\nfunctions.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.2061v7"
    },
    {
        "title": "On detecting the dependence of time series",
        "authors": [
            "Nikolai Dokuchaev"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  This short note suggests a heuristic method for detecting the dependence of\nrandom time series that can be used in the case when this dependence is\nrelatively weak and such that the traditional methods are not effective. The\nmethod requires to compare some special functionals on the sample\ncharacteristic functions with the same functionals computed for the benchmark\ntime series with a known degree of correlation. Some experiments for financial\ntime series are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.2576v1"
    },
    {
        "title": "Do price and volatility jump together?",
        "authors": [
            "Jean Jacod",
            "Viktor Todorov"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We consider a process $X_t$, which is observed on a finite time interval\n$[0,T]$, at discrete times $0,\\Delta_n,2\\Delta_n,\\ldots.$ This process is an\nIt\\^{o} semimartingale with stochastic volatility $\\sigma_t^2$. Assuming that\n$X$ has jumps on $[0,T]$, we derive tests to decide whether the volatility\nprocess has jumps occurring simultaneously with the jumps of $X_t$. There are\ntwo different families of tests for the two possible null hypotheses (common\njumps or disjoint jumps). They have a prescribed asymptotic level as the mesh\n$\\Delta_n$ goes to $0$. We show on some simulations that these tests perform\nreasonably well even in the finite sample case, and we also put them in use on\nS&P 500 index data.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.4990v1"
    },
    {
        "title": "The foreign exchange market: return distributions, multifractality,\n  anomalous multifractality and Epps effect",
        "authors": [
            "Stanislaw Drozdz",
            "Jaroslaw Kwapien",
            "Pawel Oswiecimka",
            "Rafal Rak"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We present a systematic study of various statistical characteristics of\nhigh-frequency returns from the foreign exchange market. This study is based on\nsix exchange rates forming two triangles: EUR-GBP-USD and GBP-CHF-JPY. It is\nshown that the exchange rate return fluctuations for all the pairs considered\nare well described by the nonextensive statistics in terms of q-Gaussians.\nThere exist some small quantitative variations in the nonextensivity\nq-parameter values for different exchange rates and this can be related to the\nimportance of a given exchange rate in the world's currency trade. Temporal\ncorrelations organize the series of returns such that they develop the\nmultifractal characteristics for all the exchange rates with a varying degree\nof symmetry of the singularity spectrum f(alpha) however. The most symmetric\nspectrum is identified for the GBP/USD. We also form time series of triangular\nresidual returns and find that the distributions of their fluctuations develop\ndisproportionately heavier tails as compared to small fluctuations which\nexcludes description in terms of q-Gaussians. The multifractal characteristics\nfor these residual returns reveal such anomalous properties like negative\nsingularity exponents and even negative singularity spectra. Such anomalous\nmultifractal measures have so far been considered in the literature in\nconnection with the diffusion limited aggregation and with turbulence. We find\nthat market inefficiency on short time scales leads to the occurrence of the\nEpps effect on much longer time scales. Although the currency market is much\nmore liquid than the stock markets and it has much larger transaction\nfrequency, the building-up of correlations takes up to several hours - time\nthat does not differ much from what is observed in the stock markets. This may\nsuggest that non-synchronicity of transactions is not the unique source of the\nobserved effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.2385v1"
    },
    {
        "title": "Cross-correlations between volume change and price change",
        "authors": [
            "Boris Podobnik",
            "Davor Horvatic",
            "Alexander M. Petersen",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  In finance, one usually deals not with prices but with growth rates $R$,\ndefined as the difference in logarithm between two consecutive prices. Here we\nconsider not the trading volume, but rather the volume growth rate $\\tilde R$,\nthe difference in logarithm between two consecutive values of trading volume.\nTo this end, we use several methods to analyze the properties of volume changes\n$|\\tilde R|$, and their relationship to price changes $|R|$. We analyze\n$14,981$ daily recordings of the S\\&P 500 index over the 59-year period\n1950--2009, and find power-law {\\it cross-correlations\\/} between $|R|$ and\n$|\\tilde R|$ using detrended cross-correlation analysis (DCCA). We introduce a\njoint stochastic process that models these cross-correlations. Motivated by the\nrelationship between $| R|$ and $|\\tilde R|$, we estimate the tail exponent\n${\\tilde\\alpha}$ of the probability density function $P(|\\tilde R|) \\sim\n|\\tilde R|^{-1 -\\tilde\\alpha}$ for both the S\\&P 500 index as well as the\ncollection of 1819 constituents of the New York Stock Exchange Composite index\non 17 July 2009. As a new method to estimate $\\tilde\\alpha$, we calculate the\ntime intervals $\\tau_q$ between events where $\\tilde R>q$. We demonstrate that\n$\\bar\\tau_q$, the average of $\\tau_q$, obeys $\\bar \\tau_q \\sim\nq^{\\tilde\\alpha}$. We find $\\tilde \\alpha \\approx 3$. Furthermore, by\naggregating all $\\tau_q$ values of 28 global financial indices, we also observe\nan approximate inverse cubic law.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.2674v1"
    },
    {
        "title": "Temporal Evolution of Financial Market Correlations",
        "authors": [
            "Daniel J. Fenn",
            "Mason A. Porter",
            "Stacy Williams",
            "Mark McDonald",
            "Neil F. Johnson",
            "Nick S. Jones"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We investigate financial market correlations using random matrix theory and\nprincipal component analysis. We use random matrix theory to demonstrate that\ncorrelation matrices of asset price changes contain structure that is\nincompatible with uncorrelated random price changes. We then identify the\nprincipal components of these correlation matrices and demonstrate that a small\nnumber of components accounts for a large proportion of the variability of the\nmarkets that we consider. We then characterize the time-evolving relationships\nbetween the different assets by investigating the correlations between the\nasset price time series and principal components. Using this approach, we\nuncover notable changes that occurred in financial markets and identify the\nassets that were significantly affected by these changes. We show in particular\nthat there was an increase in the strength of the relationships between several\ndifferent markets following the 2007--2008 credit and liquidity crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.3225v2"
    },
    {
        "title": "Networks of Economic Market Interdependence and Systemic Risk",
        "authors": [
            "Dion Harmon",
            "Blake Stacey",
            "Yavni Bar-Yam",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  The dynamic network of relationships among corporations underlies cascading\neconomic failures including the current economic crisis, and can be inferred\nfrom correlations in market value fluctuations. We analyze the time dependence\nof the network of correlations to reveal the changing relationships among the\nfinancial, technology, and basic materials sectors with rising and falling\nmarkets and resource constraints. The financial sector links otherwise weakly\ncoupled economic sectors, particularly during economic declines. Such links\nincrease economic risk and the extent of cascading failures. Our results\nsuggest that firewalls between financial services for different sectors would\nreduce systemic risk without hampering economic growth.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.3707v2"
    },
    {
        "title": "The US stock market leads the Federal funds rate and Treasury bond\n  yields",
        "authors": [
            "Kun Guo",
            "Wei-Xing Zhou",
            "Si-Wei Cheng",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Using a recently introduced method to quantify the time varying lead-lag\ndependencies between pairs of economic time series (the thermal optimal path\nmethod), we test two fundamental tenets of the theory of fixed income: (i) the\nstock market variations and the yield changes should be anti-correlated; (ii)\nthe change in central bank rates, as a proxy of the monetary policy of the\ncentral bank, should be a predictor of the future stock market direction. Using\nboth monthly and weekly data, we found very similar lead-lag dependence between\nthe S&P500 stock market index and the yields of bonds inside two groups: bond\nyields of short-term maturities (Federal funds rate (FFR), 3M, 6M, 1Y, 2Y, and\n3Y) and bond yields of long-term maturities (5Y, 7Y, 10Y, and 20Y). In all\ncases, we observe the opposite of (i) and (ii). First, the stock market and\nyields move in the same direction. Second, the stock market leads the yields,\nincluding and especially the FFR. Moreover, we find that the short-term yields\nin the first group lead the long-term yields in the second group before the\nfinancial crisis that started mid-2007 and the inverse relationship holds\nafterwards. These results suggest that the Federal Reserve is increasingly\nmindful of the stock market behavior, seen at key to the recovery and health of\nthe economy. Long-term investors seem also to have been more reactive and\nmindful of the signals provided by the financial stock markets than the Federal\nReserve itself after the start of the financial crisis. The lead of the S&P500\nstock market index over the bond yields of all maturities is confirmed by the\ntraditional lagged cross-correlation analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.2138v1"
    },
    {
        "title": "Quantifying and Modeling Long-Range Cross-Correlations in Multiple Time\n  Series with Applications to World Stock Indices",
        "authors": [
            "Duan Wang",
            "Boris Podobnik",
            "Davor Horvatić",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We propose a modified time lag random matrix theory in order to study time\nlag cross-correlations in multiple time series. We apply the method to 48 world\nindices, one for each of 48 different countries. We find long-range power-law\ncross-correlations in the absolute values of returns that quantify risk, and\nfind that they decay much more slowly than cross-correlations between the\nreturns. The magnitude of the cross-correlations constitute \"bad news\" for\ninternational investment managers who may believe that risk is reduced by\ndiversifying across countries. We find that when a market shock is transmitted\naround the world, the risk decays very slowly. We explain these time lag\ncross-correlations by introducing a global factor model (GFM) in which all\nindex returns fluctuate in response to a single global factor. For each pair of\nindividual time series of returns, the cross-correlations between returns (or\nmagnitudes) can be modeled with the auto-correlations of the global factor\nreturns (or magnitudes). We estimate the global factor using principal\ncomponent analysis, which minimizes the variance of the residuals after\nremoving the global trend. Using random matrix theory, a significant fraction\nof the world index cross-correlations can be explained by the global factor,\nwhich supports the utility of the GFM. We demonstrate applications of the GFM\nin forecasting risks at the world level, and in finding uncorrelated individual\nindices. We find 10 indices are practically uncorrelated with the global factor\nand with the remainder of the world indices, which is relevant information for\nworld managers in reducing their portfolio risk. Finally, we argue that this\ngeneral method can be applied to a wide range of phenomena in which time series\nare measured, ranging from seismology and physiology to atmospheric geophysics.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.2240v1"
    },
    {
        "title": "Multifractal detrending moving average cross-correlation analysis",
        "authors": [
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  There are a number of situations in which several signals are simultaneously\nrecorded in complex systems, which exhibit long-term power-law\ncross-correlations. The multifractal detrended cross-correlation analysis\n(MF-DCCA) approaches can be used to quantify such cross-correlations, such as\nthe MF-DCCA based on detrended fluctuation analysis (MF-X-DFA) method. We\ndevelop in this work a class of MF-DCCA algorithms based on the detrending\nmoving average analysis, called MF-X-DMA. The performances of the MF-X-DMA\nalgorithms are compared with the MF-X-DFA method by extensive numerical\nexperiments on pairs of time series generated from bivariate fractional\nBrownian motions, two-component autoregressive fractionally integrated moving\naverage processes and binomial measures, which have theoretical expressions of\nthe multifractal nature. In all cases, the scaling exponents $h_{xy}$ extracted\nfrom the MF-X-DMA and MF-X-DFA algorithms are very close to the theoretical\nvalues. For bivariate fractional Brownian motions, the scaling exponent of the\ncross-correlation is independent of the cross-correlation coefficient between\ntwo time series and the MF-X-DFA and centered MF-X-DMA algorithms have\ncomparative performance, which outperform the forward and backward MF-X-DMA\nalgorithms. We apply these algorithms to the return time series of two stock\nmarket indexes and to their volatilities. For the returns, the centered\nMF-X-DMA algorithm gives the best estimates of $h_{xy}(q)$ since its\n$h_{xy}(2)$ is closest to 0.5 as expected, and the MF-X-DFA algorithm has the\nsecond best performance. For the volatilities, the forward and backward\nMF-X-DMA algorithms give similar results, while the centered MF-X-DMA and the\nMF-X-DFA algorithms fails to extract rational multifractal nature.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.2577v2"
    },
    {
        "title": "Evolution of worldwide stock markets, correlation structure and\n  correlation based graphs",
        "authors": [
            "Dong-Ming Song",
            "Michele Tumminello",
            "Wei-Xing Zhou",
            "Rosario N. Mantegna"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We investigate the daily correlation present among market indices of stock\nexchanges located all over the world in the time period Jan 1996 - Jul 2009. We\ndiscover that the correlation among market indices presents both a fast and a\nslow dynamics. The slow dynamics reflects the development and consolidation of\nglobalization. The fast dynamics is associated with critical events that\noriginate in a specific country or region of the world and rapidly affect the\nglobal system. We provide evidence that the short term timescale of correlation\namong market indices is less than 3 trading months (about 60 trading days). The\naverage values of the non diagonal elements of the correlation matrix,\ncorrelation based graphs and the spectral properties of the largest eigenvalues\nand eigenvectors of the correlation matrix are carrying information about the\nfast and slow dynamics of correlation of market indices. We introduce a measure\nof mutual information based on link co-occurrence in networks, in order to\ndetect the fast dynamics of successive changes of correlation based graphs in a\nquantitative way.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5555v1"
    },
    {
        "title": "A semi-Markov model for price returns",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We study the high frequency price dynamics of traded stocks by a model of\nreturns using a semi-Markov approach. More precisely we assume that the\nintraday return are described by a discrete time homogeneous semi-Markov\nprocess and the overnight returns are modeled by a Markov chain. Based on this\nassumptions we derived the equations for the first passage time distribution\nand the volatility autocorreletion function. Theoretical results have been\ncompared with empirical findings from real data. In particular we analyzed high\nfrequency data from the Italian stock market from first of January 2007 until\nend of December 2010. The semi-Markov hypothesis is also tested through a\nnonparametric test of hypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.6143v1"
    },
    {
        "title": "Strategies used as spectroscopy of financial markets reveal new stylized\n  facts",
        "authors": [
            "Wei-Xing Zhou",
            "Guo-Hua Mu",
            "Wei Chen",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We propose a new set of stylized facts quantifying the structure of financial\nmarkets. The key idea is to study the combined structure of both investment\nstrategies and prices in order to open a qualitatively new level of\nunderstanding of financial and economic markets. We study the detailed order\nflow on the Shenzhen Stock Exchange of China for the whole year of 2003. This\nenormous dataset allows us to compare (i) a closed national market (A-shares)\nwith an international market (B-shares), (ii) individuals and institutions and\n(iii) real investors to random strategies with respect to timing that share\notherwise all other characteristics. We find that more trading results in\nsmaller net return due to trading frictions. We unveiled quantitative power\nlaws with non-trivial exponents, that quantify the deterioration of performance\nwith frequency and with holding period of the strategies used by investors.\nRandom strategies are found to perform much better than real ones, both for\nwinners and losers. Surprising large arbitrage opportunities exist, especially\nwhen using zero-intelligence strategies. This is a diagnostic of possible\ninefficiencies of these financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.3616v1"
    },
    {
        "title": "Maximum entropy distribution of stock price fluctuations",
        "authors": [
            "Rosario Bartiromo"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  The principle of absence of arbitrage opportunities allows obtaining the\ndistribution of stock price fluctuations by maximizing its information entropy.\nThis leads to a physical description of the underlying dynamics as a random\nwalk characterized by a stochastic diffusion coefficient and constrained to a\ngiven value of the expected volatility, taking in this way into account the\ninformation provided by the existence of an option market. This model is\nvalidated by a comprehensive comparison with observed distributions of both\nprice return and diffusion coefficient. Expected volatility is the only\nparameter in the model and can be obtained by analysing option prices. We give\nan analytic formulation of the probability density function for price returns\nwhich can be used to extract expected volatility from stock option data. This\ndistribution is of high practical interest since it should be preferred to a\nGaussian when dealing with the problem of pricing derivative financial\ncontracts.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.4957v4"
    },
    {
        "title": "Renyi's information transfer between financial time series",
        "authors": [
            "Petr Jizba",
            "Hagen Kleinert",
            "Mohammad Shefaat"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  In this paper, we quantify the statistical coherence between financial time\nseries by means of the Renyi entropy. With the help of Campbell's coding\ntheorem we show that the Renyi entropy selectively emphasizes only certain\nsectors of the underlying empirical distribution while strongly suppressing\nothers. This accentuation is controlled with Renyi's parameter q. To tackle the\nissue of the information flow between time series we formulate the concept of\nRenyi's transfer entropy as a measure of information that is transferred only\nbetween certain parts of underlying distributions. This is particularly\npertinent in financial time series where the knowledge of marginal events such\nas spikes or sudden jumps is of a crucial importance. We apply the Renyian\ninformation flow to stock market time series from 11 world stock indices as\nsampled at a daily rate in the time period 02.01.1990 - 31.12.2009.\nCorresponding heat maps and net information flows are represented graphically.\nA detailed discussion of the transfer entropy between the DAX and S&P500\nindices based on minute tick data gathered in the period from 02.04.2008 to\n11.09.2009 is also provided. Our analysis shows that the bivariate information\nflow between world markets is strongly asymmetric with a distinct information\nsurplus flowing from the Asia-Pacific region to both European and US markets.\nAn important yet less dramatic excess of information also flows from Europe to\nthe US. This is particularly clearly seen from a careful analysis of Renyi\ninformation flow between the DAX and S&P500 indices.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.5913v3"
    },
    {
        "title": "Time-Bridge Estimators of Integrated Variance",
        "authors": [
            "A. Saichev",
            "D. Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We present a set of log-price integrated variance estimators, equal to the\nsum of open-high-low-close bridge estimators of spot variances within $n$\nsubsequent time-step intervals. The main characteristics of some of the\nintroduced estimators is to take into account the information on the occurrence\ntimes of the high and low values. The use of the high's and low's of the bridge\nassociated with the original process makes the estimators significantly more\nefficient that the standard realized variance estimators and its\ngeneralizations. Adding the information on the occurrence times of the high and\nlow values improves further the efficiency of the estimators, much above those\nof the well-known realized variance estimator and those derived from the sum of\nGarman and Klass spot variance estimators. The exact analytical results are\nderived for the case where the underlying log-price process is an It\\^o\nstochastic process. Our results suggests more efficient ways to record\nfinancial prices at intermediate frequencies.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.2611v1"
    },
    {
        "title": "Market inefficiency identified by both single and multiple currency\n  trends",
        "authors": [
            "Tomáš Tokár",
            "Denis Horváth"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Many studies have shown that there are good reasons to claim very low\npredictability of currency nevertheless, the deviations from true randomness\nexist which have potential predictive and prognostic power [J.James,\nQuantitative finance 3 (2003) C75-C77]. We analyze the local trends which are\nof the main focus of the technical analysis. In this article we introduced\nvarious statistical quantities examining role of single temporal discretized\ntrend or multitude of trends corresponding to different time delays. Our\nspecific analysis based on Euro-dollar currency pair data at the one minute\nfrequency suggests the importance of cumulative nonrandom effect of trends on\nthe forecasting performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.2612v1"
    },
    {
        "title": "Memory effects in stock price dynamics: evidences of technical trading",
        "authors": [
            "Federico Garzarelli",
            "Matthieu Cristelli",
            "Andrea Zaccaria",
            "Luciano Pietronero"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Technical trading represents a class of investment strategies for Financial\nMarkets based on the analysis of trends and recurrent patterns of price time\nseries. According standard economical theories these strategies should not be\nused because they cannot be profitable. On the contrary it is well-known that\ntechnical traders exist and operate on different time scales. In this paper we\ninvestigate if technical trading produces detectable signals in price time\nseries and if some kind of memory effect is introduced in the price dynamics.\nIn particular we focus on a specific figure called supports and resistances. We\nfirst develop a criterion to detect the potential values of supports and\nresistances. As a second step, we show that memory effects in the price\ndynamics are associated to these selected values. In fact we show that prices\nmore likely re-bounce than cross these values. Such an effect is a quantitative\nevidence of the so-called self-fulfilling prophecy that is the\nself-reinforcement of agents' belief and sentiment about future stock prices'\nbehavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5197v1"
    },
    {
        "title": "An empirical test for Eurozone contagion using an asset-pricing model\n  with heavy-tailed stochastic volatility",
        "authors": [
            "Nicholas G. Polson",
            "James G. Scott"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  This paper proposes an empirical test of financial contagion in European\nequity markets during the tumultuous period of 2008-2011. Our analysis shows\nthat traditional GARCH and Gaussian stochastic-volatility models are unable to\nexplain two key stylized features of global markets during presumptive\ncontagion periods: shocks to aggregate market volatility can be sudden and\nexplosive, and they are associated with specific directional biases in the\ncross-section of country-level returns. Our model repairs this deficit by\nassuming that the random shocks to volatility are heavy-tailed and correlated\ncross-sectionally, both with each other and with returns. The fundamental\nconclusion of our analysis is that great care is needed in modeling volatility\nif one wishes to characterize the relationship between volatility and contagion\nthat is predicted by economic theory.\n  In analyzing daily data, we find evidence for significant contagion effects\nduring the major EU crisis periods of May 2010 and August 2011, where contagion\nis defined as excess correlation in the residuals from a factor model\nincorporating global and regional market risk factors. Some of this excess\ncorrelation can be explained by quantifying the impact of shocks to aggregate\nvolatility in the cross-section of expected returns - but only, it turns out,\nif one is extremely careful in accounting for the explosive nature of these\nshocks. We show that global markets have time-varying cross-sectional\nsensitivities to these shocks, and that high sensitivities strongly predict\nperiods of financial crisis. Moreover, the pattern of temporal changes in\ncorrelation structure between volatility and returns is readily interpretable\nin terms of the major events of the periods in question.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.5789v2"
    },
    {
        "title": "Coupled Oscillator Model of the Business Cycle with Fluctuating Goods\n  Markets",
        "authors": [
            "Y. Ikeda",
            "H. Aoyama",
            "Y. Fujiwara",
            "H. Iyetomi",
            "K. Ogimoto",
            "W. Souma",
            "H. Yoshikawa"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  The sectoral synchronization observed for the Japanese business cycle in the\nIndices of Industrial Production data is an example of synchronization. The\nstability of this synchronization under a shock, e.g., fluctuation of supply or\ndemand, is a matter of interest in physics and economics. We consider an\neconomic system made up of industry sectors and goods markets in order to\nanalyze the sectoral synchronization observed for the Japanese business cycle.\nA coupled oscillator model that exhibits synchronization is developed based on\nthe Kuramoto model with inertia by adding goods markets, and analytic solutions\nof the stationary state and the coupling strength are obtained. We simulate the\neffects on synchronization of a sectoral shock for systems with different price\nelasticities and the coupling strengths. Synchronization is reproduced as an\nequilibrium solution in a nearest neighbor graph. Analysis of the order\nparameters shows that the synchronization is stable for a finite elasticity,\nwhereas the synchronization is broken and the oscillators behave like a giant\noscillator with a certain frequency additional to the common frequency for zero\nelasticity.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.6679v1"
    },
    {
        "title": "On the scaling of the distribution of daily price fluctuations in\n  Mexican financial market index",
        "authors": [
            "Lester Alfonso",
            "Ricardo Mansilla",
            "Cesar A. Terrero-Escalante"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  In this paper, a statistical analysis of log-return fluctuations of the IPC,\nthe Mexican Stock Market Index is presented. A sample of daily data covering\nthe period from $04/09/2000-04/09/2010$ was analyzed, and fitted to different\ndistributions. Tests of the goodness of fit were performed in order to\nquantitatively asses the quality of the estimation. Special attention was paid\nto the impact of the size of the sample on the estimated decay of the\ndistributions tail. In this study a forceful rejection of normality was\nobtained. On the other hand, the null hypothesis that the log-fluctuations are\nfitted to a $\\alpha$-stable L\\'evy distribution cannot be rejected at 5%\nsignificance level.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.2038v1"
    },
    {
        "title": "Markov Chains application to the financial-economic time series\n  prediction",
        "authors": [
            "Vladimir Soloviev",
            "Vladimir Saptsin",
            "Dmitry Chabanenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  In this research the technology of complex Markov chains is applied to\npredict financial time series. The main distinction of complex or high-order\nMarkov Chains and simple first-order ones is the existing of aftereffect or\nmemory. The technology proposes prediction with the hierarchy of time\ndiscretization intervals and splicing procedure for the prediction results at\nthe different frequency levels to the single prediction output time series. The\nhierarchy of time discretizations gives a possibility to use fractal properties\nof the given time series to make prediction on the different frequencies of the\nseries. The prediction results for world's stock market indices is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.5254v1"
    },
    {
        "title": "On return-volatility correlation in financial dynamics",
        "authors": [
            "J. Shen",
            "B. Zheng"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  With the daily and minutely data of the German DAX and Chinese indices, we\ninvestigate how the return-volatility correlation originates in financial\ndynamics. Based on a retarded volatility model, we may eliminate or generate\nthe return-volatility correlation of the time series, while other\ncharacteristics, such as the probability distribution of returns and long-range\ntime-correlation of volatilities etc., remain essentially unchanged. This\nsuggests that the leverage effect or anti-leverage effect in financial markets\narises from a kind of feedback return-volatility interactions, rather than the\nlong-range time-correlation of volatilities and asymmetric probability\ndistribution of returns. Further, we show that large volatilities dominate the\nreturn-volatility correlation in financial dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.0342v1"
    },
    {
        "title": "Cross-correlation in financial dynamics",
        "authors": [
            "J. Shen",
            "B. Zheng"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  To investigate the universal structure of interactions in financial dynamics,\nwe analyze the cross-correlation matrix C of price returns of the Chinese stock\nmarket, in comparison with those of the American and Indian stock markets. As\nan important emerging market, the Chinese market exhibits much stronger\ncorrelations than the developed markets. In the Chinese market, the\ninteractions between the stocks in a same business sector are weak, while extra\ninteractions in unusual sectors are detected. Using a variation of the\ntwo-factor model, we simulate the interactions in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.0344v1"
    },
    {
        "title": "Identifying States of a Financial Market",
        "authors": [
            "Michael C. Münnix",
            "Takashi Shimada",
            "Rudi Schäfer",
            "Francois Leyvraz Thomas H. Seligman",
            "Thomas Guhr",
            "H. E. Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The understanding of complex systems has become a central issue because\ncomplex systems exist in a wide range of scientific disciplines. Time series\nare typical experimental results we have about complex systems. In the analysis\nof such time series, stationary situations have been extensively studied and\ncorrelations have been found to be a very powerful tool. Yet most natural\nprocesses are non-stationary. In particular, in times of crisis, accident or\ntrouble, stationarity is lost. As examples we may think of financial markets,\nbiological systems, reactors or the weather. In non-stationary situations\nanalysis becomes very difficult and noise is a severe problem. Following a\nnatural urge to search for order in the system, we endeavor to define states\nthrough which systems pass and in which they remain for short times. Success in\nthis respect would allow to get a better understanding of the system and might\neven lead to methods for controlling the system in more efficient ways.\n  We here concentrate on financial markets because of the easy access we have\nto good data and because of the strong non-stationary effects recently seen. We\nanalyze the S&P 500 stocks in the 19-year period 1992-2010. Here, we propose\nsuch an above mentioned definition of state for a financial market and use it\nto identify points of drastic change in the correlation structure. These points\nare mapped to occurrences of financial crises. We find that a wide variety of\ncharacteristic correlation structure patterns exist in the observation time\nwindow, and that these characteristic correlation structure patterns can be\nclassified into several typical \"market states\". Using this classification we\nrecognize transitions between different market states. A similarity measure we\ndevelop thus affords means of understanding changes in states and of\nrecognizing developments not previously seen.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.1623v1"
    },
    {
        "title": "Assessing market uncertainty by means of a time-varying intermittency\n  parameter for asset price fluctuations",
        "authors": [
            "Martin Rypdal",
            "Espen Sirnes",
            "Ola Løvsletten",
            "Kristoffer Rypdal"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Maximum likelihood estimation applied to high-frequency data allows us to\nquantify intermittency in the fluctu- ations of asset prices. From time records\nas short as one month these methods permit extraction of a meaningful\nintermittency parameter {\\lambda} characterising the degree of volatility\nclustering of asset prices. We can therefore study the time evolution of\nvolatility clustering and test the statistical significance of this\nvariability. By analysing data from the Oslo Stock Exchange, and comparing the\nresults with the investment grade spread, we find that the estimates of\n{\\lambda} are lower at times of high market uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4877v1"
    },
    {
        "title": "Aftershock prediction for high-frequency financial markets' dynamics",
        "authors": [
            "Fulvio Baldovin",
            "Francesco Camana",
            "Michele Caraglio",
            "Attilio L. Stella",
            "Marco Zamparo"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The occurrence of aftershocks following a major financial crash manifests the\ncritical dynamical response of financial markets. Aftershocks put additional\nstress on markets, with conceivable dramatic consequences. Such a phenomenon\nhas been shown to be common to most financial assets, both at high and low\nfrequency. Its present-day description relies on an empirical characterization\nproposed by Omori at the end of 1800 for seismic earthquakes. We point out the\nlimited predictive power in this phenomenological approach and present a\nstochastic model, based on the scaling symmetry of financial assets, which is\npotentially capable to predict aftershocks occurrence, given the main shock\nmagnitude. Comparisons with S&P high-frequency data confirm this predictive\npotential.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.5893v2"
    },
    {
        "title": "Applications of statistical mechanics to economics: Entropic origin of\n  the probability distributions of money, income, and energy consumption",
        "authors": [
            "Victor M. Yakovenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This Chapter is written for the Festschrift celebrating the 70th birthday of\nthe distinguished economist Duncan Foley from the New School for Social\nResearch in New York. This Chapter reviews applications of statistical physics\nmethods, such as the principle of entropy maximization, to the probability\ndistributions of money, income, and global energy consumption per capita. The\nexponential probability distribution of wages, predicted by the statistical\nequilibrium theory of a labor market developed by Foley in 1996, is supported\nby empirical data on income distribution in the USA for the majority (about\n97%) of population. In addition, the upper tail of income distribution (about\n3% of population) follows a power law and expands dramatically during financial\nbubbles, which results in a significant increase of the overall income\ninequality. A mathematical analysis of the empirical data clearly demonstrates\nthe two-class structure of a society, as pointed out Karl Marx and recently\nhighlighted by the Occupy Movement. Empirical data for the energy consumption\nper capita around the world are close to an exponential distribution, which can\nbe also explained by the entropy maximization principle.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.6483v1"
    },
    {
        "title": "A Comprehensive Analysis of Time Series Segmentation on the Japanese\n  Stock Prices",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This study conducts a comprehensive analysis of time series segmentation on\nthe Japanese stock prices listed on the first section of the Tokyo Stock\nExchange during the period from 4 January 2000 to 30 January 2012. A recursive\nsegmentation procedure is used under the assumption of a Gaussian mixture. The\ndaily number of each quintile of volatilities for all the segments is\ninvestigated empirically. It is found that from June 2004 to June 2007, a large\nmajority of stocks are stable and that from 2008 several stocks showed\ninstability. On March 2011, the daily number of instable securities steeply\nincreased due to societal turmoil influenced by the East Japan Great\nEarthquake. It is concluded that the number of stocks included in each quintile\nof volatilities provides useful information on macroeconomic situations.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.0332v2"
    },
    {
        "title": "Segmentation analysis on a multivariate time series of the foreign\n  exchange rates",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This study considers the multivariate segmentation procedure under the\nassumption of the multivariate Gaussian mixture. Jensen-Shannon divergence\nbetween two multivariate Gaussian distributions is employed as a discriminator\nand a recursive segmentation procedure is proposed. The daily log-return time\nseries for 30 currency pairs consisting of 12 currencies for the last decade\n(January 3, 2001 to December 30, 2011) are analyzed using the proposed method.\nThe proposed method can detect several important periods related to the\nsignificant affairs of the international economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.0336v1"
    },
    {
        "title": "Characterizing price index behavior through fluctuation dynamics",
        "authors": [
            "Prasanta K. Panigrahi",
            "Sayantan Ghosh",
            "Arjun Banerjee",
            "Jainendra Bahadur",
            "P. Manimaran"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We study the nature of fluctuations in variety of price indices involving\ncompanies listed on the New York Stock Exchange. The fluctuations at multiple\nscales are extracted through the use of wavelets belonging to Daubechies basis.\nThe fact that these basis sets satisfy vanishing moments conditions makes them\nideal to extract local polynomial trends, through the low pass or `average\ncoefficients'. Subtracting the trends from the original time series yields the\nfluctuations, at different scales, depending on the level of low-pass\ncoefficients used for finding the `average behavior'. The fluctuations are then\nstudied using wavelet based multifractal detrended fluctuation analysis to\nanalyze their self-similar and non-statistical properties. Due to the\nmultifractality of such time series, they deviate from Gaussian behavior in\ndifferent frequency regimes. Their departure from random matrix theory\npredictions in such regimes is also analyzed. These deviations and\nnon-statistical properties of the fluctuations can be instrumental in throwing\nsignificant light on the dynamics of financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1711v1"
    },
    {
        "title": "Weighted-indexed semi-Markov models for modeling financial returns",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  In this paper we propose a new stochastic model based on a generalization of\nsemi-Markov chains to study the high frequency price dynamics of traded stocks.\nWe assume that the financial returns are described by a weighted indexed\nsemi-Markov chain model. We show, through Monte Carlo simulations, that the\nmodel is able to reproduce important stylized facts of financial time series as\nthe first passage time distributions and the persistence of volatility. The\nmodel is applied to data from Italian and German stock market from first of\nJanuary 2007 until end of December 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2551v2"
    },
    {
        "title": "Universality class of balanced flows with bottlenecks: granular flows,\n  pedestrian fluxes and financial price dynamics",
        "authors": [
            "Daniel R. Parisi",
            "Didier Sornette",
            "Dirk Helbing"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We propose and document the evidence for an analogy between the dynamics of\ngranular counter-flows in the presence of bottlenecks or restrictions and\nfinancial price formation processes. Using extensive simulations, we find that\nthe counter-flows of simulated pedestrians through a door display many stylized\nfacts observed in financial markets when the density around the door is\ncompared with the logarithm of the price. The stylized properties are present\nalready when the agents in the pedestrian model are assumed to display a\nzero-intelligent behavior. If agents are given decision-making capacity and\nadapt to partially follow the majority, periods of herding behavior may\nadditionally occur. This generates the very slow decay of the autocorrelation\nof absolute return due to an intermittent dynamics. Our finding suggest that\nthe stylized facts in the fluctuations of the financial prices result from a\ncompetition of two groups with opposite interests in the presence of a\nconstraint funneling the flow of transactions to a narrow band of prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2915v1"
    },
    {
        "title": "Negative Kelvin temperatures in stock markets",
        "authors": [
            "J. L. Subias"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  A spin model relating physical to financial variables is presented. This work\nis the first to introduce the concept of negative absolute temperature into\nstock market dynamics by establishing a rigorous formal analogy between\nphysical and financial variables. Based on this model, an algorithm evaluating\nnegative temperatures was applied to an analysis of New York Stock Exchange\nquotations from November 2002 up to the present. We found that the magnitude of\nnegative temperature peaks correlates with subsequent index movement. Moreover,\na certain autocorrelation function decays as temperature increases. An effort\nwas directed to the search for patterns similar to known physical processes,\nsince the model hypotheses pointed to the possibility of such a similarity. A\nnumber of cases resembling known processes in phenomenological thermodynamics\nwere found, namely, population inversion and the magneto-caloric effect.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.1272v3"
    },
    {
        "title": "Statistical pairwise interaction model of stock market",
        "authors": [
            "Thomas Bury"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Financial markets are a classical example of complex systems as they comprise\nmany interacting stocks. As such, we can obtain a surprisingly good description\nof their structure by making the rough simplification of binary daily returns.\nSpin glass models have been applied and gave some valuable results but at the\nprice of restrictive assumptions on the market dynamics or others are\nagent-based models with rules designed in order to recover some empirical\nbehaviours. Here we show that the pairwise model is actually a statistically\nconsistent model with observed first and second moments of the stocks\norientation without making such restrictive assumptions. This is done with an\napproach based only on empirical data of price returns. Our data analysis of\nsix major indices suggests that the actual interaction structure may be thought\nas an Ising model on a complex network with interaction strengths scaling as\nthe inverse of the system size. This has potentially important implications\nsince many properties of such a model are already known and some techniques of\nthe spin glass theory can be straightforwardly applied. Typical behaviours, as\nmultiple equilibria or metastable states, different characteristic time scales,\nspatial patterns, order-disorder, could find an explanation in this picture.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.4420v6"
    },
    {
        "title": "Stock prices assessment: proposal of a new index based on volume\n  weighted historical prices through the use of computer modeling",
        "authors": [
            "Tiago Colliri",
            "Fernando F. Ferreira"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The importance of considering the volumes to analyze stock prices movements\ncan be considered as a well-accepted practice in the financial area. However,\nwhen we look at the scientific production in this field, we still cannot find a\nunified model that includes volume and price variations for stock assessment\npurposes. In this paper we present a computer model that could fulfill this\ngap, proposing a new index to evaluate stock prices based on their historical\nprices and volumes traded. Besides the model can be considered mathematically\nvery simple, it was able to improve significantly the performance of agents\noperating with real financial data. Based on the results obtained, and also on\nthe very intuitive logic of our model, we believe that the index proposed here\ncan be very useful to help investors on the activity of determining ideal price\nranges for buying and selling stocks in the financial market.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.5224v4"
    },
    {
        "title": "Distribution Of Wealth In A Network Model Of The Economy",
        "authors": [
            "Tao Ma",
            "John G. Holden",
            "R. A. Serota"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We show, analytically and numerically, that wealth distribution in the\nBouchaud-M\\'ezard network model of the economy is described by a\nthree-parameter generalized inverse gamma distribution. In the mean-field limit\nof a network with any two agents linked, it reduces to the inverse gamma\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.2696v1"
    },
    {
        "title": "Interest Rate Manipulation Detection using Time Series Clustering\n  Approach",
        "authors": [
            "Murphy Choy",
            "Enoch Chng",
            "Koo Ping Shung"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The Interbank Offered Rate is a vital benchmark interest rate in the\nfinancial markets of every country to which financial contracts are tied. In\nthe light of the recent LIBOR manipulation incident, this paper seeks to\naddress the fear that Interbank Offered Rate are entirely controlled by the\nbank. The paper will focus on the comparison between LIBOR and SIBOR especially\nwith regards to the behavior of the interest rate with time. Because of the\nnature of IBORs, banks will naturally be submitting similar rates which should\nnot differ excessively from the market as well as the other banks. We will\ncompare the LIBOR and SIBOR from 2005 to 2011 with respect to the 1 month rates\non an annual basis. We will present the result that the SIBOR is not\nmanipulated like LIBOR.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.2878v1"
    },
    {
        "title": "Network analysis of correlation strength between the most developed\n  countries",
        "authors": [
            "Janusz Miśkiewicz"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  A new algorithm of the analysis of correlation among economy time series is\nproposed. The algorithm is based on the power law classification scheme (PLCS)\nfollowed by the analysis of the network on the percolation threshold (NPT). The\nalgorithm was applied to the analysis of correlations among GDP per capita time\nseries of 19 most developed countries in the periods (1982, 2011), (1992, 2011)\nand (2002, 2011). The representative countries with respect to strength of\ncorrelation, convergence of time series and stability of correlation are\ndistinguished. The results are compared with ultrametric distance matrix\nanalysed by NPT.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.3599v1"
    },
    {
        "title": "Transition in the Waiting-Time Distribution of Price-Change Events in a\n  Global Socioeconomic System",
        "authors": [
            "Guannan Zhao",
            "Mark McDonald",
            "Dan Fenn",
            "Stacy Williams",
            "Neil F. Johnson"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The goal of developing a firmer theoretical understanding of inhomogenous\ntemporal processes -- in particular, the waiting times in some collective\ndynamical system -- is attracting significant interest among physicists.\nQuantifying the deviations in the waiting-time distribution away from one\ngenerated by a random process, may help unravel the feedback mechanisms that\ndrive the underlying dynamics. We analyze the waiting-time distributions of\nhigh frequency foreign exchange data for the best executable bid-ask prices\nacross all major currencies. We find that the lognormal distribution yields a\ngood overall fit for the waiting-time distribution between currency rate\nchanges if both short and long waiting times are included. If we restrict our\nstudy to long waiting-times, each currency pair's distribution is consistent\nwith a power law tail with exponent near to 3.5. However for short waiting\ntimes, the overall distribution resembles one generated by an archetypal\ncomplex systems model in which boundedly rational agents compete for limited\nresources. Our findings suggest a gradual transition arises in trading behavior\nbetween a fast regime in which traders act in a boundedly rational way, and a\nslower one in which traders' decisions are driven by generic feedback\nmechanisms across multiple timescales and hence produce similar power-law tails\nirrespective of currency type.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.2189v1"
    },
    {
        "title": "Structural and topological phase transitions on the German Stock\n  Exchange",
        "authors": [
            "M. Wiliński",
            "A. Sienkiewicz",
            "T. Gubiec",
            "R. Kutner",
            "Z. R. Struzik"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We find numerical and empirical evidence for dynamical, structural and\ntopological phase transitions on the (German) Frankfurt Stock Exchange (FSE) in\nthe temporal vicinity of the worldwide financial crash. Using the Minimal\nSpanning Tree (MST) technique, a particularly useful canonical tool of the\ngraph theory, two transitions of the topology of a complex network representing\nFSE were found. First transition is from a hierarchical scale-free MST\nrepresenting the stock market before the recent worldwide financial crash, to a\nsuperstar-like MST decorated by a scale-free hierarchy of trees representing\nthe market's state for the period containing the crash. Subsequently, a\ntransition is observed from this transient, (meta)stable state of the crash, to\na hierarchical scale-free MST decorated by several star-like trees after the\nworldwide financial crash. The phase transitions observed are analogous to the\nones we obtained earlier for the Warsaw Stock Exchange and more pronounced than\nthose found by Onnela-Chakraborti-Kaski-Kert\\'esz for S&P 500 index in the\nvicinity of Black Monday (October 19, 1987) and also in the vicinity of January\n1, 1998. Our results provide an empirical foundation for the future theory of\ndynamical, structural and topological phase transitions on financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2530v2"
    },
    {
        "title": "Reinterpretation of Sieczka-Hołyst financial market model",
        "authors": [
            "Mateusz Denys",
            "Tomasz Gubiec",
            "Ryszard Kutner"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this work we essentially reinterpreted the Sieczka-Ho{\\l}yst (SH) model to\nmake it more suited for description of real markets. For instance, this\nreinterpretation made it possible to consider agents as crafty. These agents\nencourage their neighbors to buy some stocks if agents have an opportunity to\nsell these stocks. Also, agents encourage them to sell some stocks if agents\nhave an opposite opportunity. Furthermore, in our interpretation price changes\nrespond only to the agents' opinions change. This kind of respond protects the\nstock market dynamics against the paradox (present in the SH model), where all\nagents e.g. buy stocks while the corresponding prices remain unchanged. In this\nwork we found circumstances, where distributions of returns (obtained for quite\ndifferent time scales) either obey power-law or have at least fat tails. We\nobtained these distributions from numerical simulations performed in the frame\nof our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.2535v1"
    },
    {
        "title": "Random cascade model in the limit of infinite integral scale as the\n  exponential of a non-stationary $1/f$ noise. Application to volatility\n  fluctuations in stock markets",
        "authors": [
            "J. F. Muzy",
            "R. Baile",
            "E. Bacry"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this paper we propose a new model for volatility fluctuations in financial\ntime series. This model relies on a non-stationary gaussian process that\nexhibits aging behavior. It turns out that its properties, over any finite time\ninterval, are very close to continuous cascade models. These latter models are\nindeed well known to reproduce faithfully the main stylized facts of financial\ntime series. However, it involve a large scale parameter (the so-called\n\"integral scale\" where the cascade is initiated) that is hard to interpret in\nfinance. Moreover the empirical value of the integral scale is in general\ndeeply correlated to the overall length of the sample. This feature is\nprecisely predicted by our model that turns out, as illustrated on various\nexamples from daily stock index data, to quantitatively reproduce the empirical\nobservations.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.4160v1"
    },
    {
        "title": "Dynamic structural and topological phase transitions on the Warsaw Stock\n  Exchange: A phenomenological approach",
        "authors": [
            "A. Sienkiewicz",
            "T. Gubiec",
            "R. Kutner",
            "Z. R. Struzik"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We study the crash dynamics of the Warsaw Stock Exchange (WSE) by using the\nMinimal Spanning Tree (MST) networks. We find the transition of the complex\nnetwork during its evolution from a (hierarchical) power law MST network,\nrepresenting the stable state of WSE before the recent worldwide financial\ncrash, to a superstar-like (or superhub) MST network of the market decorated by\na hierarchy of trees (being, perhaps, an unstable, intermediate market state).\nSubsequently, we observed a transition from this complex tree to the topology\nof the (hierarchical) power law MST network decorated by several star-like\ntrees or hubs. This structure and topology represent, perhaps, the WSE after\nthe worldwide financial crash, and could be considered to be an aftershock. Our\nresults can serve as an empirical foundation for a future theory of dynamic\nstructural and topological phase transitions on financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.6506v1"
    },
    {
        "title": "Critical reflexivity in financial markets: a Hawkes process analysis",
        "authors": [
            "Stephen J. Hardiman",
            "Nicolas Bercot",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We model the arrival of mid-price changes in the E-Mini S&P futures contract\nas a self-exciting Hawkes process. Using several estimation methods, we find\nthat the Hawkes kernel is power-law with a decay exponent close to -1.15 at\nshort times, less than approximately 10^3 seconds, and crosses over to a second\npower-law regime with a larger decay exponent of approximately -1.45 for longer\ntimes scales in the range [10^3, 10^6] seconds. More importantly, we find that\nthe Hawkes kernel integrates to unity independently of the analysed period,\nfrom 1998 to 2011. This suggests that markets are and have always been close to\ncriticality, challenging a recent study which indicates that reflexivity\n(endogeneity) has increased in recent years as a result of increased automation\nof trading. However, we note that the scale over which market events are\ncorrelated has decreased steadily over time with the emergence of higher\nfrequency trading.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.1405v2"
    },
    {
        "title": "Bridging stylized facts in finance and data non-stationarities",
        "authors": [
            "Sabrina Camargo",
            "Silvio M. Duarte Queiros",
            "Celia Anteneodo"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Employing a recent technique which allows the representation of nonstationary\ndata by means of a juxtaposition of locally stationary patches of different\nlength, we introduce a comprehensive analysis of the key observables in a\nfinancial market: the trading volume and the price fluctuations. From the\nsegmentation procedure we are able to introduce a quantitative description of a\ngroup of statistical features (stylizes facts) of the trading volume and price\nfluctuations, namely the tails of each distribution, the U-shaped profile of\nthe volume in a trading session and the evolution of the trading volume\nautocorrelation function. The segmentation of the trading volume series\nprovides evidence of slow evolution of the fluctuating parameters of each\npatch, pointing to the mixing scenario. Assuming that long-term features are\nthe outcome of a statistical mixture of simple local forms, we test and compare\ndifferent probability density functions to provide the long-term distribution\nof the trading volume, concluding that the log-normal gives the best agreement\nwith the empirical distribution. Moreover, the segmentation of the magnitude\nprice fluctuations are quite different from the results for the trading volume,\nindicating that changes in the statistics of price fluctuations occur at a\nfaster scale than in the case of trading volume.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.3197v2"
    },
    {
        "title": "A second-order stock market model",
        "authors": [
            "Robert Fernholz",
            "Tomoyuki Ichiba",
            "Ioannis Karatzas"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  A first-order model for a stock market assigns to each stock a return\nparameter and a variance parameter that depend only on the rank of the stock. A\nsecond-order model assigns these parameters based on both the rank and the name\nof the stock. First- and second-order models exhibit stability properties that\nmake them appropriate as a backdrop for the analysis of the idiosyncratic\nbehavior of individual stocks. Methods for the estimation of the parameters of\nsecond-order models are developed in this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.3870v1"
    },
    {
        "title": "Analysis of Realized Volatility in Two Trading Sessions of the Japanese\n  Stock Market",
        "authors": [
            "Tetsuya Takaishi",
            "Ting Ting Chen",
            "Zeyu Zheng"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We analyze realized volatilities constructed using high-frequency stock data\non the Tokyo Stock Exchange. In order to avoid non-trading hours issue in\nvolatility calculations we define two realized volatilities calculated\nseparately in the two trading sessions of the Tokyo Stock Exchange, i.e.\nmorning and afternoon sessions. After calculating the realized volatilities at\nvarious sampling frequencies we evaluate the bias from the microstructure noise\nas a function of sampling frequency. Taking into account of the bias to\nrealized volatility we examine returns standardized by realized volatilities\nand confirm that price returns on the Tokyo Stock Exchange are described\napproximately by Gaussian time series with time-varying volatility, i.e.\nconsistent with a mixture of distributions hypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.6006v1"
    },
    {
        "title": "Evolution of correlation structure of industrial indices of US equity\n  markets",
        "authors": [
            "Giuseppe Buccheri",
            "Stefano Marmi",
            "Rosario N. Mantegna"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We investigate the dynamics of correlations present between pairs of industry\nindices of US stocks traded in US markets by studying correlation based\nnetworks and spectral properties of the correlation matrix. The study is\nperformed by using 49 industry index time series computed by K. French and E.\nFama during the time period from July 1969 to December 2011 that is spanning\nmore than 40 years. We show that the correlation between industry indices\npresents both a fast and a slow dynamics. The slow dynamics has a time scale\nlonger than five years showing that a different degree of diversification of\nthe investment is possible in different periods of time. On top to this slow\ndynamics, we also detect a fast dynamics associated with exogenous or\nendogenous events. The fast time scale we use is a monthly time scale and the\nevaluation time period is a 3 month time period. By investigating the\ncorrelation dynamics monthly, we are able to detect two examples of fast\nvariations in the first and second eigenvalue of the correlation matrix. The\nfirst occurs during the dot-com bubble (from March 1999 to April 2001) and the\nsecond occurs during the period of highest impact of the subprime crisis (from\nAugust 2008 to August 2009).\n",
        "pdf_link": "http://arxiv.org/pdf/1306.4769v1"
    },
    {
        "title": "Fractality of profit landscapes and validation of time series models for\n  stock prices",
        "authors": [
            "Il Gu Yi",
            "Gabjin Oh",
            "Beom Jun Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We apply a simple trading strategy for various time series of real and\nartificial stock prices to understand the origin of fractality observed in the\nresulting profit landscapes. The strategy contains only two parameters $p$ and\n$q$, and the sell (buy) decision is made when the log return is larger\n(smaller) than $p$ ($-q$). We discretize the unit square $(p, q) \\in [0, 1]\n\\times [0, 1]$ into the $N \\times N$ square grid and the profit $\\Pi (p, q)$ is\ncalculated at the center of each cell. We confirm the previous finding that\nlocal maxima in profit landscapes are scattered in a fractal-like fashion: The\nnumber M of local maxima follows the power-law form $M \\sim N^{a}$, but the\nscaling exponent $a$ is found to differ for different time series. From\ncomparisons of real and artificial stock prices, we find that the fat-tailed\nreturn distribution is closely related to the exponent $a \\approx 1.6$ observed\nfor real stock markets. We suggest that the fractality of profit landscape\ncharacterized by $a \\approx 1.6$ can be a useful measure to validate time\nseries model for stock prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1749v1"
    },
    {
        "title": "Following a Trend with an Exponential Moving Average: Analytical Results\n  for a Gaussian Model",
        "authors": [
            "D. S. Grebenkov",
            "J. Serror"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We investigate how price variations of a stock are transformed into profits\nand losses (P&Ls) of a trend following strategy. In the frame of a Gaussian\nmodel, we derive the probability distribution of P&Ls and analyze its moments\n(mean, variance, skewness and kurtosis) and asymptotic behavior (quantiles). We\nshow that the asymmetry of the distribution (with often small losses and less\nfrequent but significant profits) is reminiscent to trend following strategies\nand less dependent on peculiarities of price variations. At short times, trend\nfollowing strategies admit larger losses than one may anticipate from standard\nGaussian estimates, while smaller losses are ensured at longer times. Simple\nexplicit formulas characterizing the distribution of P&Ls illustrate the basic\nmechanisms of momentum trading, while general matrix representations can be\napplied to arbitrary Gaussian models. We also compute explicitly annualized\nrisk adjusted P&L and strategy turnover to account for transaction costs. We\ndeduce the trend following optimal timescale and its dependence on both\nauto-correlation level and transaction costs. Theoretical results are\nillustrated on the Dow Jones index.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5658v1"
    },
    {
        "title": "Modeling of Stock Returns and Trading Volume",
        "authors": [
            "Taisei Kaizoji"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this study, we investigate the statistical properties of the returns and\nthe trading volume. We show a typical example of power-law distributions of the\nreturn and of the trading volume. Next, we propose an interacting agent model\nof stock markets inspired from statistical mechanics [24] to explore the\nempirical findings. We show that as the interaction among the interacting\ntraders strengthens both the returns and the trading volume present power-law\nbehavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.2416v1"
    },
    {
        "title": "New measure of multifractality and its application in finances",
        "authors": [
            "Dariusz Grech",
            "Grzegorz Pamuła"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We provide an alternative method for analysis of multifractal properties of\ntime series. The new approach takes into account the behaviour of the whole\nmultifractal profile of the generalized Hurst exponent $h(q)$ for all moment\norders $q$, not limited only to the edge values of $h(q)$ describing in MFDFA\nscaling properties of smallest and largest fluctuations in signal. The meaning\nof this new measure is clarified and its properties are investigated for\nsynthetic multifractal data and real signals taken from stock market. We show\nthat the proposed new measure is free of problems one can meet in real\nnonstationary signals, while searching their multifractal signatures.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.5466v1"
    },
    {
        "title": "The fine structure of volatility feedback II: overnight and intra-day\n  effects",
        "authors": [
            "Pierre Blanc",
            "Rémy Chicheportiche",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We decompose, within an ARCH framework, the daily volatility of stocks into\novernight and intra-day contributions. We find, as perhaps expected, that the\novernight and intra-day returns behave completely differently. For example,\nwhile past intra-day returns affect equally the future intra-day and overnight\nvolatilities, past overnight returns have a weak effect on future intra-day\nvolatilities (except for the very next one) but impact substantially future\novernight volatilities. The exogenous component of overnight volatilities is\nfound to be close to zero, which means that the lion's share of overnight\nvolatility comes from feedback effects. The residual kurtosis of returns is\nsmall for intra-day returns but infinite for overnight returns. We provide a\nplausible interpretation for these findings, and show that our\nIntra-Day/Overnight model significantly outperforms the standard ARCH framework\nbased on daily returns for Out-of-Sample predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.5806v2"
    },
    {
        "title": "Limit theorems for nearly unstable Hawkes processes",
        "authors": [
            "Thibault Jaisson",
            "Mathieu Rosenbaum"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Because of their tractability and their natural interpretations in term of\nmarket quantities, Hawkes processes are nowadays widely used in high-frequency\nfinance. However, in practice, the statistical estimation results seem to show\nthat very often, only nearly unstable Hawkes processes are able to fit the data\nproperly. By nearly unstable, we mean that the $L^1$ norm of their kernel is\nclose to unity. We study in this work such processes for which the stability\ncondition is almost violated. Our main result states that after suitable\nrescaling, they asymptotically behave like integrated Cox-Ingersoll-Ross\nmodels. Thus, modeling financial order flows as nearly unstable Hawkes\nprocesses may be a good way to reproduce both their high and low frequency\nstylized facts. We then extend this result to the Hawkes-based price model\nintroduced by Bacry et al. [Quant. Finance 13 (2013) 65-77]. We show that under\na similar criticality condition, this process converges to a Heston model.\nAgain, we recover well-known stylized facts of prices, both at the\nmicrostructure level and at the macroscopic scale.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.2033v2"
    },
    {
        "title": "A statistical physics perspective on criticality in financial markets",
        "authors": [
            "Thomas Bury"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Stock markets are complex systems exhibiting collective phenomena and\nparticular features such as synchronization, fluctuations distributed as\npower-laws, non-random structures and similarity to neural networks. Such\nspecific properties suggest that markets operate at a very special point.\nFinancial markets are believed to be critical by analogy to physical systems\nbut few statistically founded evidence have been given. Through a data-based\nmethodology and comparison to simulations inspired by statistical physics of\ncomplex systems, we show that the Dow Jones and indices sets are not rigorously\ncritical. However, financial systems are closer to the criticality in the crash\nneighborhood.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.2446v3"
    },
    {
        "title": "Predicting trend reversals using market instantaneous state",
        "authors": [
            "Thomas Bury"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Collective behaviours taking place in financial markets reveal strongly\ncorrelated states especially during a crisis period. A natural hypothesis is\nthat trend reversals are also driven by mutual influences between the different\nstock exchanges. Using a maximum entropy approach, we find coordinated\nbehaviour during trend reversals dominated by the pairwise component. In\nparticular, these events are predicted with high significant accuracy by the\nensemble's instantaneous state.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.8169v5"
    },
    {
        "title": "Measures of uncertainty in market network analysis",
        "authors": [
            "V. A. Kalyagin",
            "A. P. Koldanov",
            "P. A. Koldanov",
            "P. M. Pardalos",
            "V. A. Zamaraev"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Statistical uncertainty of different filtration techniques for market network\nanalysis is studied. Two measures of statistical uncertainty are discussed. One\nis based on conditional risk for multiple decision statistical procedures and\nanother one is based on average fraction of errors. It is shown that for some\nimportant cases the second measure is a particular case of the first one.\nStatistical uncertainty for some popular market network structures is analyzed.\nResults of numerical evaluation of statistical uncertainty for minimum spanning\ntree, market graph, maximum cliques and maximum independent sets are given. The\nmost stable structures are derived.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.2273v1"
    },
    {
        "title": "Structural Changes on Warsaw's Stock Exchange: the end of Financial\n  Crisis",
        "authors": [
            "Paweł Fiedor"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this paper we analyse the structure of Warsaw's stock market using complex\nsystems methodology together with network science and information theory. We\nfind minimal spanning trees for log returns on Warsaw's stock exchange for\nyearly times series between 2000 and 2013. For each stock in those trees we\ncalculate its Markov centrality measure to estimate its importance in the\nnetwork. We also estimate entropy rate for each of those time series using\nLempel-Ziv algorithm based estimator to study the predictability of those price\nchanges. The division of the studied stocks into 26 sectors allows us to study\nthe changing structure of the Warsaw's stock market and conclude that the\nfinancial crisis sensu stricto has ended on Warsaw's stock market in 2012-13.\nWe also comment on the history and the outlook of the Warsaw's market based on\nthe log returns, their average, variability, entropy and the centrality of a\nstock in the dependency network.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4230v1"
    },
    {
        "title": "Stock Market Trend Analysis Using Hidden Markov Models",
        "authors": [
            "G. Kavitha",
            "A. Udhayakumar",
            "D. Nagarajan"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Price movements of stock market are not totally random. In fact, what drives\nthe financial market and what pattern financial time series follows have long\nbeen the interest that attracts economists, mathematicians and most recently\ncomputer scientists [17]. This paper gives an idea about the trend analysis of\nstock market behaviour using Hidden Markov Model (HMM). The trend once followed\nover a particular period will sure repeat in future. The one day difference in\nclose value of stocks for a certain period is found and its corresponding\nsteady state probability distribution values are determined. The pattern of the\nstock market behaviour is then decided based on these probability values for a\nparticular time. The goal is to figure out the hidden state sequence given the\nobservation sequence so that the trend can be analyzed using the steady state\nprobability distribution( ) values. Six optimal hidden state sequences are\ngenerated and compared. The one day difference in close value when considered\nis found to give the best optimum state sequence.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.4771v1"
    },
    {
        "title": "Nucleation, condensation and lambda-transition on a real-life stock\n  market",
        "authors": [
            "M. Wilinski",
            "B. Szewczak",
            "T. Gubiec",
            "R. Kutner",
            "Z. R. Struzik"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We fill a void in merging empirical and phenomenological characterisation of\nthe dynamical phase transitions in complex systems by identifying three of them\non real-life financial markets. We extract and interpret the empirical,\nnumerical, and semi-analytical evidences for the existence of these phase\ntransitions, by considering the Frankfurt Stock Exchange (FSE), as a typical\nexample of a financial market of a medium size. Using the canonical object for\nthe graph theory, i.e. the Minimal Spanning Tree (MST) network, we observe: (i)\nThe initial phase transition from the equilibrium to non-equilibrium MST\nnetwork in its nucleation phase, occurring at some critical time. Coalescence\nof edges on the FSE's transient leader is observed within the nucleation and is\napproximately characterized by the Lifsthiz-Slyozov growth exponent; (ii) The\nnucleation accelerates and transforms to the condensation process, in the\nsecond phase transition, forming a logarithmically diverging lambda-peak of\nshort-range order parameters at the subsequent critical time - an analogon of\nsuch a transition in superfluidity; (iii) In the third phase transition, the\npeak logarithmically decreases over three quarters of the year, resulting in a\nfew loosely connected sub-graphs. This peak is reminiscent of a non-equilibrium\nsuperstar-like superhub or a `dragon king' effect, abruptly accelerating the\nevolution of the leader company. All these phase transitions are caused by the\nfew richest vertices, which drift towards the leader and provide the most of\nthe edges increasing the leader's degree. Thus, we capture an amazing\nphenomenon, likely of a more universal character, where a peripheral vertex\nbecomes the one which is over dominating the complex network during an\nexceptionally long period of time.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.5753v3"
    },
    {
        "title": "Spatial and temporal structures of four financial markets in Greater\n  China",
        "authors": [
            "F. Y. Ouyang",
            "B. Zheng",
            "X. F. Jiang"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We investigate the spatial and temporal structures of four financial markets\nin Greater China. In particular, we uncover different characteristics of the\nfour markets by analyzing the sector and subsector structures which are\ndetected through the random matrix theory. Meanwhile, we observe that the\nTaiwan and Hongkong stock markets show a negative return-volatility\ncorrelation, i.e., the so-called leverage effect. The Shanghai and Shenzhen\nstock markets are more complicated. Before the year 2000, the two markets\nexhibit a strong positive return-volatility correlation, which is called the\nanti-leverage effect. After 2000, however, it gradually changes to the leverage\neffect. We also find that the recurrence interval distributions of both the\ntrading volume volatilities and price volatilities follow a power law behavior,\nwhile the exponents vary among different markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1046v1"
    },
    {
        "title": "Partial correlation analysis: Applications for financial markets",
        "authors": [
            "Dror Y. Kenett",
            "Xuqing Huang",
            "Irena Vodenska",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  The presence of significant cross-correlations between the synchronous time\nevolution of a pair of equity returns is a well-known empirical fact. The\nPearson correlation is commonly used to indicate the level of similarity in the\nprice changes for a given pair of stocks, but it does not measure whether other\nstocks influence the relationship between them. To explore the influence of a\nthird stock on the relationship between two stocks, we use a partial\ncorrelation measurement to determine the underlying relationships between\nfinancial assets. Building on previous work, we present a statistically robust\napproach to extract the underlying relationships between stocks from four\ndifferent financial markets: the United States, the United Kingdom, Japan, and\nIndia. This methodology provides new insights into financial market dynamics\nand uncovers implicit influences in play between stocks. To demonstrate the\ncapabilities of this methodology, we (i) quantify the influence of different\ncompanies and, by studying market similarity across time, present new insights\ninto market structure and market stability, and (ii) we present a practical\napplication, which provides information on the how a company is influenced by\ndifferent economic sectors, and how the sectors interact with each other. These\nexamples demonstrate the effectiveness of this methodology in uncovering\ninformation valuable for a range of individuals, including not only investors\nand traders but also regulators and policy makers.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1405v1"
    },
    {
        "title": "Correlation and Network Topologies in Global and Local Stock Indices",
        "authors": [
            "Ashadun Nobi",
            "Sungmin Lee",
            "Doo Hwan Kim",
            "Jae Woo Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  This study examined how the correlation and network structure of 30 global\nindices and 145 local Korean indices belonging to the KOSPI 200 have changed\nduring the 13-year period, 2000-2012. The correlations among the indices were\ncalculated. The results showed that although the average correlations of the\nglobal indices increased with time, the local indices showed a decreasing trend\nexcept for drastic changes during crises. The average correlation of the local\nindices exceeded the global indices during the crises from 2000-2002, implying\na strong correlation structure among the local indices during this period due\nto the detrimental effect of the dot-com bubble. The threshold networks (TN)\nwere constructed in the observation time window by assigning a threshold value\nand determining the network topologies. A significant change in the network\ntopologies was observed due to the financial crises in both markets. The\nJaccard similarities were also determined using the common links of TNs. The\nTNs of the financial network were not consistent with the evolution of the\ntime, and the successive TNs of the global indices were more similar than those\nof the successive local indices. Finally, the Jaccard similarities identified\nthe change in the market state due to a crisis in both markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1552v1"
    },
    {
        "title": "Using Twitter to Model the EUR/USD Exchange Rate",
        "authors": [
            "Dietmar Janetzko"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Fast, global, and sensitively reacting to political, economic and social\nevents of any kind, these are attributes that social media like Twitter share\nwith foreign exchange markets. The leading assumption of this paper is that\ninformation which can be distilled from public debates on Twitter has\npredictive content for exchange rate movements. This assumption prompted a\nTwitter-based exchange rate model that harnesses regARIMA analyses for\nshort-term out-of-sample ex post forecasts of the daily closing prices of\nEUR/USD spot exchange rates. The analyses used Tweet counts collected from\nJanuary 1, 2012 - September 27, 2013. To identify concepts mentioned on Twitter\nwith a predictive potential the analysis followed a 2-step selection. Firstly,\na heuristic qualitative analysis assembled a long list of 594 concepts, e.g.,\nMerkel, Greece, Cyprus, crisis, chaos, growth, unemployment expected to covary\nwith the ups and downs of the EUR/USD exchange rate. Secondly, cross-validation\nusing window averaging with a fixed-sized rolling origin was deployed to select\nconcepts and corresponding univariate time series that had error scores below\nchance level as defined by the random walk model. With regard to a short list\nof 17 concepts (covariates), in particular SP (Standard & Poor's) and risk, the\nout-of-sample predictive accuracy of the Twitter-based regARIMA model was found\nto be repeatedly better than that obtained from both the random walk model and\na random noise covariate in 1-step ahead forecasts of the EUR/USD exchange\nrate. This advantage was evident on the level of forecast error metrics (MSFE,\nMAE) when a majority vote over different estimation windows was conducted. The\nresults challenge the semi-strong form of the efficient market hypothesis\n(Fama, 1970, 1991) which when applied to the FX market maintains that all\npublicly available information is already integrated into exchange rates.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1624v1"
    },
    {
        "title": "The adaptive nature of liquidity taking in limit order books",
        "authors": [
            "Damian Eduardo Taranto",
            "Giacomo Bormetti",
            "Fabrizio Lillo"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In financial markets, the order flow, defined as the process assuming value\none for buy market orders and minus one for sell market orders, displays a very\nslowly decaying autocorrelation function. Since orders impact prices,\nreconciling the persistence of the order flow with market efficiency is a\nsubtle issue. A possible solution is provided by asymmetric liquidity, which\nstates that the impact of a buy or sell order is inversely related to the\nprobability of its occurrence. We empirically find that when the order flow\npredictability increases in one direction, the liquidity in the opposite side\ndecreases, but the probability that a trade moves the price decreases\nsignificantly. While the last mechanism is able to counterbalance the\npersistence of order flow and restore efficiency and diffusivity, the first\nacts in opposite direction. We introduce a statistical order book model where\nthe persistence of the order flow is mitigated by adjusting the market order\nvolume to the predictability of the order flow. The model reproduces the\ndiffusive behaviour of prices at all time scales without fine-tuning the values\nof parameters, as well as the behaviour of most order book quantities as a\nfunction of the local predictability of order flow.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.0842v2"
    },
    {
        "title": "Consentaneous agent-based and stochastic model of the financial markets",
        "authors": [
            "V. Gontis",
            "A. Kononovicius"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We are looking for the agent-based treatment of the financial markets\nconsidering necessity to build bridges between microscopic, agent based, and\nmacroscopic, phenomenological modeling. The acknowledgment that agent-based\nmodeling framework, which may provide qualitative and quantitative\nunderstanding of the financial markets, is very ambiguous emphasizes the\nexceptional value of well defined analytically tractable agent systems. Herding\nas one of the behavior peculiarities considered in the behavioral finance is\nthe main property of the agent interactions we deal with in this contribution.\nLooking for the consentaneous agent-based and macroscopic approach we combine\ntwo origins of the noise: exogenous one, related to the information flow, and\nendogenous one, arising form the complex stochastic dynamics of agents. As a\nresult we propose a three state agent-based herding model of the financial\nmarkets. From this agent-based model we derive a set of stochastic differential\nequations, which describes underlying macroscopic dynamics of agent population\nand log price in the financial markets. The obtained solution is then subjected\nto the exogenous noise, which shapes instantaneous return fluctuations. We test\nboth Gaussian and q-Gaussian noise as a source of the short term fluctuations.\nThe resulting model of the return in the financial markets with the same set of\nparameters reproduces empirical probability and spectral densities of absolute\nreturn observed in New York, Warsaw and NASDAQ OMX Vilnius Stock Exchanges. Our\nresult confirms the prevalent idea in behavioral finance that herding\ninteractions may be dominant over agent rationality and contribute towards\nbubble formation.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.1574v2"
    },
    {
        "title": "Least quartic Regression Criterion with Application to Finance",
        "authors": [
            "Giuseppe arbia"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  This article proposes a new method for the estimation of the parameters of a\nsimple linear regression model which accounts for the role of co-moments in\nnon-Gaussian distributions being based on the minimization of a quartic loss\nfunction. Although the proposed method is very general, we examine its\napplication to finance. In fact, in this field the contribution of the\nco-moments in explaining the return-generating process is of paramount\nimportance when evaluating the systematic risk of an asset within the framework\nof the Capital Asset Pricing Model (CAPM). The suggested new method contributes\nto this literature by showing that, in the presence of non-normality, the\nregression slope can be expressed as a function of the co-kurtosis between the\nreturns of a risky asset and the market proxy. The paper provides an\nillustration of the method based on some empirical financial data referring to\n40 industrial sector assets rates of the Italian stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.4171v1"
    },
    {
        "title": "Collective behaviours in the stock market -- A maximum entropy approach",
        "authors": [
            "Thomas Bury"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Scale invariance, collective behaviours and structural reorganization are\ncrucial for portfolio management (portfolio composition, hedging, alternative\ndefinition of risk, etc.). This lack of any characteristic scale and such\nelaborated behaviours find their origin in the theory of complex systems. There\nare several mechanisms which generate scale invariance but maximum entropy\nmodels are able to explain both scale invariance and collective behaviours. The\nstudy of the structure and collective modes of financial markets attracts more\nand more attention. It has been shown that some agent based models are able to\nreproduce some stylized facts. Despite their partial success, there is still\nthe problem of rules design. In this work, we used a statistical inverse\napproach to model the structure and co-movements in financial markets. Inverse\nmodels restrict the number of assumptions. We found that a pairwise maximum\nentropy model is consistent with the data and is able to describe the complex\nstructure of financial systems. We considered the existence of a critical state\nwhich is linked to how the market processes information, how it responds to\nexogenous inputs and how its structure changes. The considered data sets did\nnot reveal a persistent critical state but rather oscillations between order\nand disorder. In this framework, we also showed that the collective modes are\nmostly dominated by pairwise co-movements and that univariate models are not\ngood candidates to model crashes. The analysis also suggests a genuine adaptive\nprocess since both the maximum variance of the log-likelihood and the accuracy\nof the predictive scheme vary through time. This approach may provide some clue\nto crash precursors and may provide highlights on how a shock spreads in a\nfinancial network and if it will lead to a crash. The natural continuation of\nthe present work could be the study of such a mechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5179v2"
    },
    {
        "title": "Branching ratio approximation for the self-exciting Hawkes process",
        "authors": [
            "Stephen J. Hardiman",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We introduce a model-independent approximation for the branching ratio of\nHawkes self-exciting point processes. Our estimator requires knowing only the\nmean and variance of the event count in a sufficiently large time window,\nstatistics that are readily obtained from empirical data. The method we propose\ngreatly simplifies the estimation of the Hawkes branching ratio, recently\nproposed as a proxy for market endogeneity and formerly estimated using\nnumerical likelihood maximisation. We employ our new method to support recent\ntheoretical and experimental results indicating that the best fitting Hawkes\nmodel to describe S&P futures price changes is in fact critical (now and in the\nrecent past) in light of the long memory of financial market activity.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5227v3"
    },
    {
        "title": "Relation between Financial Market Structure and the Real Economy:\n  Comparison between Clustering Methods",
        "authors": [
            "Nicolo Musmeci",
            "Tomaso Aste",
            "Tiziana Di Matteo"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We quantify the amount of information filtered by different hierarchical\nclustering methods on correlations between stock returns comparing it with the\nunderlying industrial activity structure. Specifically, we apply, for the first\ntime to financial data, a novel hierarchical clustering approach, the Directed\nBubble Hierarchical Tree and we compare it with other methods including the\nLinkage and k-medoids. In particular, by taking the industrial sector\nclassification of stocks as a benchmark partition, we evaluate how the\ndifferent methods retrieve this classification. The results show that the\nDirected Bubble Hierarchical Tree can outperform other methods, being able to\nretrieve more information with fewer clusters. Moreover, we show that the\neconomic information is hidden at different levels of the hierarchical\nstructures depending on the clustering method. The dynamical analysis on a\nrolling window also reveals that the different methods show different degrees\nof sensitivity to events affecting financial markets, like crises. These\nresults can be of interest for all the applications of clustering methods to\nportfolio optimization and risk hedging.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0496v2"
    },
    {
        "title": "A variation of the Dragulescu-Yakovenko income model",
        "authors": [
            "José María Sarabia",
            "Faustino Prieto",
            "Vanesa Jordá"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In the context of the Dragulescu-Yakovenko (2000) model, we show that\nempirical income distribution with truncated datasets, cannot be properly\nmodeled by the one-parameter exponential distribution. However, a truncated\nversion characterized by an exponential distribution with two parameters gives\nan accurate fit.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5083v1"
    },
    {
        "title": "Sector-Based Factor Models for Asset Returns",
        "authors": [
            "Angela Gu",
            "Patrick Zeng"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Factor analysis is a statistical technique employed to evaluate how observed\nvariables correlate through common factors and unique variables. While it is\noften used to analyze price movement in the unstable stock market, it does not\nalways yield easily interpretable results. In this study, we develop improved\nfactor models by explicitly incorporating sector information on our studied\nstocks. We add eleven sectors of stocks as defined by the IBES, represented by\nrespective sector-specific factors, to non-specific market factors to revise\nthe factor model. We then develop an expectation maximization (EM) algorithm to\ncompute our revised model with 15 years' worth of S&P 500 stocks' daily close\nprices. Our results in most sectors show that nearly all of these factor\ncomponents have the same sign, consistent with the intuitive idea that stocks\nin the same sector tend to rise and fall in coordination over time. Results\nobtained by the classic factor model, in contrast, had a homogeneous blend of\npositive and negative components. We conclude that results produced by our\nsector-based factor model are more interpretable than those produced by the\nclassic non-sector-based model for at least some stock sectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.2794v1"
    },
    {
        "title": "Spectrum-based estimators of the bivariate Hurst exponent",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We introduce two new estimators of the bivariate Hurst exponent in the\npower-law cross-correlations setting -- the cross-periodogram and local\n$X$-Whittle estimators -- as generalizations of their univariate counterparts.\nAs the spectrum-based estimators are dependent on a part of the spectrum taken\ninto consideration during estimation, a simulation study showing performance of\nthe estimators under varying bandwidth parameter as well as correlation between\nprocesses and their specification is provided as well. The newly introduced\nestimators are less biased than the already existent averaged periodogram\nestimator which, however, has slightly lower variance. The spectrum-based\nestimators can serve as a good complement to the popular time domain\nestimators.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.6637v2"
    },
    {
        "title": "Detrended fluctuation analysis as a regression framework: Estimating\n  dependence at different scales",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We propose a framework combining detrended fluctuation analysis with standard\nregression methodology. The method is built on detrended variances and\ncovariances and it is designed to estimate regression parameters at different\nscales and under potential non-stationarity and power-law correlations. The\nformer feature allows for distinguishing between effects for a pair of\nvariables from different temporal perspectives. The latter ones make the method\na significant improvement over the standard least squares estimation.\nTheoretical claims are supported by Monte Carlo simulations. The method is then\napplied on selected examples from physics, finance, environmental science and\nepidemiology. For most of the studied cases, the relationship between variables\nof interest varies strongly across scales.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.0496v2"
    },
    {
        "title": "Universality of Tsallis q-exponential of interoccurrence times within\n  the microscopic model of cunning agents",
        "authors": [
            "Mateusz Denys",
            "Tomasz Gubiec",
            "Ryszard Kutner"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We proposed the agent-based model of financial markets where agents (or\ntraders) are represented by three-state spins located on the plane lattice or\nsocial network. The spin variable represents only the individual opinion\n(advice) that each trader gives to his nearest neighbors. In the model the\nagents can be considered as cunning. For instance, although agent having\ncurrently a maximal value of the spin advises his nearest neighbors to buy some\nstocks he, perfidiously, will sell some stocks in the next Monte Carlo step or\nwill occupy a neutral position. In general, the trader has three possibilities:\nhe can buy some stocks if his opinion change within a single time step is\npositive, sell some stocks if this change is negative, or remain inactive if\nhis opinion is unchanged. The predictions of our model, found by simulations,\nwell agree with the empirical universal distribution of interoccurrence times\nbetween daily losses below negative thresholds following the Tsallis\nq-exponential.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1689v1"
    },
    {
        "title": "Nonlinear GARCH model and 1/f noise",
        "authors": [
            "Aleksejus Kononovicius",
            "Julius Ruseckas"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Auto-regressive conditionally heteroskedastic (ARCH) family models are still\nused, by practitioners in business and economic policy making, as a conditional\nvolatility forecasting models. Furthermore ARCH models still are attracting an\ninterest of the researchers. In this contribution we consider the well known\nGARCH(1,1) process and its nonlinear modifications, reminiscent of NGARCH\nmodel. We investigate the possibility to reproduce power law statistics,\nprobability density function and power spectral density, using ARCH family\nmodels. For this purpose we derive stochastic differential equations from the\nGARCH processes in consideration. We find the obtained equations to be similar\nto a general class of stochastic differential equations known to reproduce\npower law statistics. We show that linear GARCH(1,1) process has power law\ndistribution, but its power spectral density is Brownian noise-like. However,\nthe nonlinear modifications exhibit both power law distribution and power\nspectral density of the power law form, including 1/f noise.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.6244v2"
    },
    {
        "title": "Assessment of 48 Stock markets using adaptive multifractal approach",
        "authors": [
            "Paulo Ferreira",
            "Andreia Dionísio",
            "S. M. S. Movahed"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Stock market comovements are examined using cointegration, Granger causality\ntests and nonlinear approaches in context of mutual information and\ncorrelations. Underlying data sets are affected by non-stationarities and\ntrends, we also apply AMF-DFA and AMF-DXA. We find only 170 pair of Stock\nmarkets cointegrated, and according to the Granger causality and mutual\ninformation, we realize that the strongest relations lies between emerging\nmarkets, and between emerging and frontier markets. According to scaling\nexponent given by AMF-DFA, $h(q=2)>1$, we find that all underlying data sets\nbelong to non-stationary process. According to EMH, only 8 markets are\nclassified in uncorrelated processes at $2\\sigma$ confidence interval. 6 Stock\nmarkets belong to anti-correlated class and dominant part of markets has memory\nin corresponding daily index prices during January 1995 to February 2014.\nNew-Zealand with $H=0.457\\pm0.004$ and Jordan with $H=0.602\\pm 0.006$ are far\nfrom EMH. The nature of cross-correlation exponents based on AMF-DXA is almost\nmultifractal for all pair of Stock markets. The empirical relation, $H_{xy}\\le\n[H_{xx}+H_{yy}]/2$, is confirmed. Mentioned relation for $q>0$ is also\nsatisfied while for $q<0$ there is a deviation from this relation confirming\nbehavior of markets for small fluctuations is affected by contribution of major\npair. For larger fluctuations, the cross-correlation contains information from\nboth local and global conditions. Width of singularity spectrum for\nauto-correlation and cross-correlation are $\\Delta \\alpha_{xx}\\in\n[0.304,0.905]$ and $\\Delta \\alpha_{xy}\\in [0.246,1.178]$, respectively. The\nwide range of singularity spectrum for cross-correlation confirms that the\nbilateral relation between Stock markets is more complex. The value of\n$\\sigma_{DCCA}$ indicates that all pairs of stock market studied in this time\ninterval belong to cross-correlated processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.05603v2"
    },
    {
        "title": "Cross correlations in European government bonds and EuroStoxx",
        "authors": [
            "Jan Jurczyk",
            "Alexander Eckrot"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We use principle component analysis (PCA) of cross correlations in European\ngovernment bonds and European stocks to investigate the systemic risk contained\nin the European economy. We tackle the task to visualize the evolution of risk,\nintroducing the conditional average rolling sum (CARS). Using this tool we see\nthat the risk of government bonds and stocks had an independent movement. But\nin the course of the European sovereign debt crisis the coupling between bonds\nand stocks has strongly ncreased. This results in an in-phase oscillation of\nrisk for both markets since mid 2010. In our data, we observe a steep amplitude\nincrease, suggesting a high vulnerability of the two coupled systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.07367v3"
    },
    {
        "title": "Club Convergence of House Prices: Evidence from China's Ten Key Cities",
        "authors": [
            "Hao Meng",
            "Wen-Jie Xie",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The latest global financial tsunami and its follow-up global economic\nrecession has uncovered the crucial impact of housing markets on financial and\neconomic systems. The Chinese stock market experienced a markedly fall during\nthe global financial tsunami and China's economy has also slowed down by about\n2\\%-3\\% when measured in GDP. Nevertheless, the housing markets in diverse\nChinese cities seemed to continue the almost nonstop mania for more than ten\nyears. However, the structure and dynamics of the Chinese housing market are\nless studied. Here we perform an extensive study of the Chinese housing market\nby analyzing ten representative key cities based on both linear and nonlinear\neconophysical and econometric methods. We identify a common collective driving\nforce which accounts for 96.5\\% of the house price growth, indicating very high\nsystemic risk in the Chinese housing market. The ten key cities can be\ncategorized into clubs and the house prices of the cities in the same club\nexhibit an evident convergence. These findings from different methods are\nbasically consistent with each other. The identified city clubs are also\nconsistent with the conventional classification of city tiers. The house prices\nof the first-tier cities grow the fastest, and those of the third- and\nfourth-tier cities rise the slowest, which illustrates the possible presence of\na ripple effect in the diffusion of house prices in different cities.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.05550v1"
    },
    {
        "title": "Prices of Options as Opinion Dynamics of the Market Players with Limited\n  Social Influence",
        "authors": [
            "Elad Oster",
            "Alexander Feigel"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The dynamics of market prices is described as the evolution of opinions in\nthe trading community regarding future market behavior. The price then is a\nfunction of the voting process of the market players in favor to raise or\nreduce the value of a stock. The model presented in this paper is suited for\npricing of options and was verified against real market data. The model allows\nderiving the parameters of market players from available real market data,\nespecially maximum possible correlation (herding) and anti-correlation between\nthe players' opinions. The deviations of market prices from those predicted by\nthe Black-Scholes model, such as smile and skew implied volatilities, are\ninterpreted as the current values and limits of social influence of the market\nplayers, respectively. To the best of our knowledge, this is the first work\nthat discriminates skew and smile phenomena. Our approach unifies and develops\na further connection between trading, voters' model, and statistical physics\nanalogies of opinion dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08785v1"
    },
    {
        "title": "Mesoscopic Community Structure of Financial Markets Revealed by Price\n  and Sign Fluctuations",
        "authors": [
            "Assaf Almog",
            "Ferry Besamusca",
            "Mel MacMahon",
            "Diego Garlaschelli"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The mesoscopic organization of complex systems, from financial markets to the\nbrain, is an intermediate between the microscopic dynamics of individual units\n(stocks or neurons, in the mentioned cases), and the macroscopic dynamics of\nthe system as a whole. The organization is determined by \"communities\" of units\nwhose dynamics, represented by time series of activity, is more strongly\ncorrelated internally than with the rest of the system. Recent studies have\nshown that the binary projections of various financial and neural time series\nexhibit nontrivial dynamical features that resemble those of the original data.\nThis implies that a significant piece of information is encoded into the binary\nprojection (i.e. the sign) of such increments. Here, we explore whether the\nbinary signatures of multiple time series can replicate the same complex\ncommunity organization of the financial market, as the original weighted time\nseries. We adopt a method that has been specifically designed to detect\ncommunities from cross-correlation matrices of time series data. Our analysis\nshows that the simpler binary representation leads to a community structure\nthat is almost identical with that obtained using the full weighted\nrepresentation. These results confirm that binary projections of financial time\nseries contain significant structural information.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00590v1"
    },
    {
        "title": "U.S. stock market interaction network as learned by the Boltzmann\n  Machine",
        "authors": [
            "Stanislav S. Borysov",
            "Yasser Roudi",
            "Alexander V. Balatsky"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We study historical dynamics of joint equilibrium distribution of stock\nreturns in the U.S. stock market using the Boltzmann distribution model being\nparametrized by external fields and pairwise couplings. Within Boltzmann\nlearning framework for statistical inference, we analyze historical behavior of\nthe parameters inferred using exact and approximate learning algorithms. Since\nthe model and inference methods require use of binary variables, effect of this\nmapping of continuous returns to the discrete domain is studied. The presented\nanalysis shows that binarization preserves market correlation structure.\nProperties of distributions of external fields and couplings as well as\nindustry sector clustering structure are studied for different historical dates\nand moving window sizes. We found that a heavy positive tail in the\ndistribution of couplings is responsible for the sparse market clustering\nstructure. We also show that discrepancies between the model parameters might\nbe used as a precursor of financial instabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02280v2"
    },
    {
        "title": "Inference on the Sharpe ratio via the upsilon distribution",
        "authors": [
            "Steven E. Pav"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The upsilon distribution, the sum of independent chi random variates and a\nnormal, is introduced. As a special case, the upsilon distribution includes\nLecoutre's lambda-prime distribution. The upsilon distribution finds\napplication in Frequentist inference on the Sharpe ratio, including hypothesis\ntests on independent samples, confidence intervals, and prediction intervals,\nas well as their Bayesian counterparts. These tests are extended to the case of\nfactor models of returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.00829v3"
    },
    {
        "title": "Modelling Financial Markets by Self-Organized Criticality",
        "authors": [
            "A. E. Biondo",
            "A. Pluchino",
            "A. Rapisarda"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We present a financial market model, characterized by self-organized\ncriticality, that is able to generate endogenously a realistic price dynamics\nand to reproduce well-known stylized facts. We consider a community of\nheterogeneous traders, composed by chartists and fundamentalists, and focus on\nthe role of informative pressure on market participants, showing how the\nspreading of information, based on a realistic imitative behavior, drives\ncontagion and causes market fragility. In this model imitation is not intended\nas a change in the agent's group of origin, but is referred only to the price\nformation process. We introduce in the community also a variable number of\nrandom traders in order to study their possible beneficial role in stabilizing\nthe market, as found in other studies. Finally we also suggest some\ncounterintuitive policy strategies able to dampen fluctuations by means of a\npartial reduction of information.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.04298v2"
    },
    {
        "title": "Forecasting Exchange Rates Using Time Series Analysis: The sample of the\n  currency of Kazakhstan",
        "authors": [
            "Daniya Tlegenova"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  This paper models yearly exchange rates between USD/KZT, EUR/KZT and SGD/KZT,\nand compares the actual data with developed forecasts using time series\nanalysis over the period from 2006 to 2014. The official yearly data of\nNational Bank of the Republic of Kazakhstan is used for present study. The main\ngoal of this paper is to apply the ARIMA model for forecasting of yearly\nexchange rates of USD/KZT, EUR/KZT and SGD/KZT. The accuracy of the forecast is\ncompared with Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE)\nand Root Mean Squared Error (RMSE).\n",
        "pdf_link": "http://arxiv.org/pdf/1508.07534v1"
    },
    {
        "title": "A proposal of a methodological framework with experimental guidelines to\n  investigate clustering stability on financial time series",
        "authors": [
            "Gautier Marti",
            "Philippe Very",
            "Philippe Donnat",
            "Frank Nielsen"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We present in this paper an empirical framework motivated by the practitioner\npoint of view on stability. The goal is to both assess clustering validity and\nyield market insights by providing through the data perturbations we propose a\nmulti-view of the assets' clustering behaviour. The perturbation framework is\nillustrated on an extensive credit default swap time series database available\nonline at www.datagrapple.com.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.05475v1"
    },
    {
        "title": "Joint multifractal analysis based on the partition function approach:\n  Analytical analysis, numerical simulation and empirical application",
        "authors": [
            "Wen-Jie Xie",
            "Zhi-Qiang Jiang",
            "Gao-Feng Gu",
            "Xiong Xiong",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Many complex systems generate multifractal time series which are long-range\ncross-correlated. Numerous methods have been proposed to characterize the\nmultifractal nature of these long-range cross correlations. However, several\nimportant issues about these methods are not well understood and most methods\nconsider only one moment order. We study the joint multifractal analysis based\non partition function with two moment orders, which was initially invented to\ninvestigate fluid fields, and derive analytically several important properties.\nWe apply the method numerically to binomial measures with multifractal cross\ncorrelations and bivariate fractional Brownian motions without multifractal\ncross correlations. For binomial multifractal measures, the explicit\nexpressions of mass function, singularity strength and multifractal spectrum of\nthe cross correlations are derived, which agree excellently with the numerical\nresults. We also apply the method to stock market indexes and unveil intriguing\nmultifractality in the cross correlations of index volatilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.05952v1"
    },
    {
        "title": "Mean-Reverting Portfolios: Tradeoffs Between Sparsity and Volatility",
        "authors": [
            "Marco Cuturi",
            "Alexandre d'Aspremont"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Mean-reverting assets are one of the holy grails of financial markets: if\nsuch assets existed, they would provide trivially profitable investment\nstrategies for any investor able to trade them, thanks to the knowledge that\nsuch assets oscillate predictably around their long term mean. The modus\noperandi of cointegration-based trading strategies [Tsay, 2005, {\\S}8] is to\ncreate first a portfolio of assets whose aggregate value mean-reverts, to\nexploit that knowledge by selling short or buying that portfolio when its value\ndeviates from its long-term mean. Such portfolios are typically selected using\ntools from cointegration theory [Engle and Granger, 1987, Johansen, 1991],\nwhose aim is to detect combinations of assets that are stationary, and\ntherefore mean-reverting. We argue in this work that focusing on stationarity\nonly may not suffice to ensure profitability of cointegration-based strategies.\nWhile it might be possible to create syn- thetically, using a large array of\nfinancial assets, a portfolio whose aggre- gate value is stationary and\ntherefore mean-reverting, trading such a large portfolio incurs in practice\nimportant trade or borrow costs. Looking for stationary portfolios formed by\nmany assets may also result in portfolios that have a very small volatility and\nwhich require significant leverage to be profitable. We study in this work\nalgorithmic approaches that can take mitigate these effects by searching for\nmaximally mean-reverting portfo- lios which are sufficiently sparse and/or\nvolatile.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.05954v1"
    },
    {
        "title": "Price response in correlated financial markets: empirical results",
        "authors": [
            "Shanshan Wang",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Previous studies of the stock price response to individual trades focused on\nsingle stocks. We empirically investigate the price response of one stock to\nthe trades of other stocks. How large is the impact of one stock on others and\nvice versa? -- This impact of trades on the price change across stocks appears\nto be transient instead of permanent. Performing different averages, we\ndistinguish active and passive responses. The two average responses show\ndifferent characteristic dependences on the time lag. The passive response\nexhibits a shorter response period with sizeable volatilities, and the active\nresponse a longer period. We also study the response for a given stock with\nrespect to different sectors and to the whole market. Furthermore, we compare\nthe self-response with the various cross-responses. The correlation of the\ntrade signs is a short-memory process for a pair of stocks, but it turns into a\nlong-memory process when averaged over different pairs of stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.03205v2"
    },
    {
        "title": "Optimal ETF Selection for Passive Investing",
        "authors": [
            "David Puelz",
            "Carlos M. Carvalho",
            "P. Richard Hahn"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  This paper considers the problem of isolating a small number of exchange\ntraded funds (ETFs) that suffice to capture the fundamental dimensions of\nvariation in U.S. financial markets. First, the data is fit to a vector-valued\nBayesian regression model, which is a matrix-variate generalization of the well\nknown stochastic search variable selection (SSVS) of George and McCulloch\n(1993). ETF selection is then performed using the decoupled shrinkage and\nselection (DSS) procedure described in Hahn and Carvalho (2015), adapted in two\nways: to the vector-response setting and to incorporate stochastic covariates.\nThe selected set of ETFs is obtained under a number of different penalty and\nmodeling choices. Optimal portfolios are constructed from selected ETFs by\nmaximizing the Sharpe ratio posterior mean, and they are compared to the\n(unknown) optimal portfolio based on the full Bayesian model. We compare our\nselection results to popular ETF advisor Wealthfront.com. Additionally, we\nconsider selecting ETFs by modeling a large set of mutual funds.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.03385v2"
    },
    {
        "title": "Multifractal Flexibly Detrended Fluctuation Analysis",
        "authors": [
            "Rafal Rak",
            "Pawel Zięba"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Multifractal time series analysis is a approach that shows the possible\ncomplexity of the system. Nowadays, one of the most popular and the best\nmethods for determining multifractal characteristics is Multifractal Detrended\nFluctuation Analysis (MFDFA). However, it has some drawback. One of its core\nelements is detrending of the series. In the classical MFDFA a trend is\nestimated by fitting a polynomial of degree $m$ where $m=const$. We propose\nthat the degree $m$ of a polynomial was not constant ($m\\neq const$) and its\nselection was ruled by an established criterion. Taking into account the above\namendment, we examine the multifractal spectra both for artificial and\nreal-world mono- and the multifractal time series. Unlike classical MFDFA\nmethod, obtained singularity spectra almost perfectly reflects the theoretical\nresults and for real time series we observe a significant right side shift of\nthe spectrum.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.05115v1"
    },
    {
        "title": "Networks, Dynamic Factors, and the Volatility Analysis of\n  High-Dimensional Financial Series",
        "authors": [
            "Matteo Barigozzi",
            "Marc Hallin"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We consider weighted directed networks for analysing, over the period\n2000-2013, the interdependencies between volatilities of a large panel of\nstocks belonging to the S\\&P100 index. In particular, we focus on the so-called\n{\\it Long-Run Variance Decomposition Network} (LVDN), where the nodes are\nstocks, and the weight associated with edge $(i,j)$ represents the proportion\nof $h$-step-ahead forecast error variance of variable $i$ accounted for by\nvariable $j$'s innovations. To overcome the curse of dimensionality, we\ndecompose the panel into a component driven by few global, market-wide,\nfactors, and an idiosyncratic one modelled by means of a sparse vector\nautoregression (VAR) model. Inversion of the VAR together with suitable\nidentification restrictions, produces the estimated network, by means of which\nwe can assess how {\\it systemic} each firm is.~Our analysis demonstrates the\nprominent role of financial firms as sources of contagion, especially during\nthe~2007-2008 crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.05118v2"
    },
    {
        "title": "Uncovering the evolution of non-stationary stochastic variables: the\n  example of asset volume-price fluctuations",
        "authors": [
            "Paulo Rocha",
            "Frank Raischel",
            "João P. Boto",
            "Pedro G. Lind"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We present a framework for describing the evolution of stochastic observables\nhaving a non-stationary distribution of values. The framework is applied to\nempirical volume-prices from assets traded at the New York stock exchange.\nUsing Kullback-Leibler divergence we evaluate the best model out from four\nbiparametric models standardly used in the context of financial data analysis.\nIn our present data sets we conclude that the inverse $\\Gamma$-distribution is\na good model, particularly for the distribution tail of the largest\nvolume-price fluctuations. Extracting the time-series of the corresponding\nparameter values we show that they evolve in time as stochastic variables\nthemselves. For the particular case of the parameter controlling the\nvolume-price distribution tail we are able to extract an Ornstein-Uhlenbeck\nequation which describes the fluctuations of the largest volume-prices observed\nin the data. Finally, we discuss how to bridge from the stochastic evolution of\nthe distribution parameters to the stochastic evolution of the (non-stationary)\nobservable and put our conclusions into perspective for other applications in\ngeophysics and biology.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.07280v1"
    },
    {
        "title": "Sparse Kalman Filtering Approaches to Covariance Estimation from High\n  Frequency Data in the Presence of Jumps",
        "authors": [
            "Michael Ho",
            "Jack Xin"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Estimation of the covariance matrix of asset returns from high frequency data\nis complicated by asynchronous returns, market mi- crostructure noise and\njumps. One technique for addressing both asynchronous returns and market\nmicrostructure is the Kalman-EM (KEM) algorithm. However the KEM approach\nassumes log-normal prices and does not address jumps in the return process\nwhich can corrupt estimation of the covariance matrix.\n  In this paper we extend the KEM algorithm to price models that include jumps.\nWe propose two sparse Kalman filtering approaches to this problem. In the first\napproach we develop a Kalman Expectation Conditional Maximization (KECM)\nalgorithm to determine the un- known covariance as well as detecting the jumps.\nFor this algorithm we consider Laplace and the spike and slab jump models, both\nof which promote sparse estimates of the jumps. In the second method we take a\nBayesian approach and use Gibbs sampling to sample from the posterior\ndistribution of the covariance matrix under the spike and slab jump model.\nNumerical results using simulated data show that each of these approaches\nprovide for improved covariance estima- tion relative to the KEM method in a\nvariety of settings where jumps occur.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.02185v2"
    },
    {
        "title": "Modelling intensities of order flows in a limit order book",
        "authors": [
            "Ioane Muni Toke",
            "Nakahiro Yoshida"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We propose a parametric model for the simulation of limit order books. We\nassume that limit orders, market orders and cancellations are submitted\naccording to point processes with state-dependent intensities. We propose new\nfunctional forms for these intensities, as well as new models for the placement\nof limit orders and cancellations. For cancellations, we introduce the concept\nof \"priority index\" to describe the selection of orders to be cancelled in the\norder book. Parameters of the model are estimated using likelihood\nmaximization. We illustrate the performance of the model by providing extensive\nsimulation results, with a comparison to empirical data and a standard Poisson\nreference.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.03944v1"
    },
    {
        "title": "Power-law cross-correlations estimation under heavy tails",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We examine the performance of six estimators of the power-law\ncross-correlations -- the detrended cross-correlation analysis, the detrending\nmoving-average cross-correlation analysis, the height cross-correlation\nanalysis, the averaged periodogram estimator, the cross-periodogram estimator\nand the local cross-Whittle estimator -- under heavy-tailed distributions. The\nselection of estimators allows to separate these into the time and frequency\ndomain estimators. By varying the characteristic exponent of the\n$\\alpha$-stable distributions which controls the tails behavior, we report\nseveral interesting findings. First, the frequency domain estimators are\npractically unaffected by heavy tails bias-wise. Second, the time domain\nestimators are upward biased for heavy tails but they have lower estimator\nvariance than the other group for short series. Third, specific estimators are\nmore appropriate depending on distributional properties and length of the\nanalyzed series. In addition, we provide a discussion of implications of these\nresults for empirical applications as well as theoretical explanations.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.05385v2"
    },
    {
        "title": "Deviations from universality in the fluctuation behavior of a\n  heterogeneous complex system reveal intrinsic properties of components: The\n  case of the international currency market",
        "authors": [
            "Abhijit Chakraborty",
            "Soumya Easwaran",
            "Sitabhra Sinha"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Identifying behavior that is relatively invariant under different conditions\nis a challenging task in far-from-equilibrium complex systems. As an example of\nhow the existence of a semi-invariant signature can be masked by the\nheterogeneity in the properties of the components comprising such systems, we\nconsider the exchange rate dynamics in the international currency market. We\nshow that the exponents characterizing the heavy tails of fluctuation\ndistributions for different currencies systematically diverge from a putative\nuniversal form associated with the median value (~2) of the exponents. We\nrelate the degree of deviation of a particular currency from such an \"inverse\nsquare law\" to fundamental macroscopic properties of the corresponding economy,\nviz., measures of per capita production output and diversity of export\nproducts. We also show that in contrast to uncorrelated random walks exhibited\nby the exchange rate dynamics for currencies belonging to developed economies,\nthose of the less developed economies show characteristics of sub-diffusive\nprocesses which we relate to the anti-correlated nature of the corresponding\nfluctuations. Approaches similar to that presented here may help in identifying\ninvariant features obscured by the heterogeneous nature of components in other\ncomplex systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.06111v2"
    },
    {
        "title": "Artificial Neural Network and Time Series Modeling Based Approach to\n  Forecasting the Exchange Rate in a Multivariate Framework",
        "authors": [
            "Tamal Datta Chaudhuri",
            "Indranil Ghosh"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Any discussion on exchange rate movements and forecasting should include\nexplanatory variables from both the current account and the capital account of\nthe balance of payments. In this paper, we include such factors to forecast the\nvalue of the Indian rupee vis a vis the US Dollar. Further, factors reflecting\npolitical instability and lack of mechanism for enforcement of contracts that\ncan affect both direct foreign investment and also portfolio investment, have\nbeen incorporated. The explanatory variables chosen are the 3 month Rupee\nDollar futures exchange rate (FX4), NIFTY returns (NIFTYR), Dow Jones\nIndustrial Average returns (DJIAR), Hang Seng returns (HSR), DAX returns (DR),\ncrude oil price (COP), CBOE VIX (CV) and India VIX (IV). To forecast the\nexchange rate, we have used two different classes of frameworks namely,\nArtificial Neural Network (ANN) based models and Time Series Econometric\nmodels. Multilayer Feed Forward Neural Network (MLFFNN) and Nonlinear\nAutoregressive models with Exogenous Input (NARX) Neural Network are the\napproaches that we have used as ANN models. Generalized Autoregressive\nConditional Heteroskedastic (GARCH) and Exponential Generalized Autoregressive\nConditional Heteroskedastic (EGARCH) techniques are the ones that we have used\nas Time Series Econometric methods. Within our framework, our results indicate\nthat, although the two different approaches are quite efficient in forecasting\nthe exchange rate, MLFNN and NARX are the most efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.02093v1"
    },
    {
        "title": "Fluctuation of USA Gold Price - Revisited with Chaos-based Complex\n  Network Method",
        "authors": [
            "Susmita Bhaduri",
            "Dipak Ghosh",
            "Subhadeep Ghosh"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We give emphasis on the use of chaos-based rigorous nonlinear technique\ncalled Visibility Graph Analysis, to study one economic time series - gold\nprice of USA. This method can offer reliable results with fiinite data. This\npaper reports the result of such an analysis on the times series depicting the\nfluctuation of gold price of USA for the span of 25 years(1990 - 2013). This\nanalysis reveals that a quantitative parameter from the theory can explain\nsatisfactorily the real life nature of fluctuation of gold price of USA and\nhence building a strong database in terms of a quantitative parameter which can\neventually be used for forecasting purpose.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.01103v1"
    },
    {
        "title": "Time-scale effects on the gain-loss asymmetry in stock indices",
        "authors": [
            "Bulcsú Sándor",
            "Ingve Simonsen",
            "Bálint Zsolt Nagy",
            "Zoltán Néda"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The gain-loss asymmetry, observed in the inverse statistics of stock indices\nis present for logarithmic return levels that are over $2\\%$, and it is the\nresult of the non-Pearson type auto-correlations in the index. These\nnon-Pearson type correlations can be viewed also as functionally dependent\ndaily volatilities, extending for a finite time interval. A generalized\ntime-window shuffling method is used to show the existence of such\nauto-correlations. Their characteristic time-scale proves to be smaller (less\nthan $25$ trading days) than what was previously believed. It is also found\nthat this characteristic time-scale has decreased with the appearance of\nprogram trading in the stock market transactions. Connections with the leverage\neffect are also established.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04506v2"
    },
    {
        "title": "Quantile Dependence between Stock Markets and its Application in\n  Volatility Forecasting",
        "authors": [
            "Heejoon Han"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  This paper examines quantile dependence between international stock markets\nand evaluates its use for improving volatility forecasting. First, we analyze\nquantile dependence and directional predictability between the US stock market\nand stock markets in the UK, Germany, France and Japan. We use the\ncross-quantilogram, which is a correlation statistic of quantile hit processes.\nThe detailed dependence between stock markets depends on specific quantile\nranges and this dependence is generally asymmetric; the negative spillover\neffect is stronger than the positive spillover effect and there exists strong\ndirectional predictability from the US market to the UK, Germany, France and\nJapan markets. Second, we consider a simple quantile-augmented volatility model\nthat accommodates the quantile dependence and directional predictability\nbetween the US market and these other markets. The quantile-augmented\nvolatility model provides superior in-sample and out-of-sample volatility\nforecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.07193v1"
    },
    {
        "title": "Financial Market Dynamics: Superdiffusive or not?",
        "authors": [
            "Sandhya Devi"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The behavior of stock market returns over a period of 1-60 days has been\ninvestigated for S&P 500 and Nasdaq within the framework of nonextensive\nTsallis statistics. Even for such long terms, the distributions of the returns\nare non-Gaussian. They have fat tails indicating that the stock returns do not\nfollow a random walk model. In this work, a good fit to a Tsallis q-Gaussian\ndistribution is obtained for the distributions of all the returns using the\nmethod of Maximum Likelihood Estimate. For all the regions of data considered,\nthe values of the scaling parameter q, estimated from one day returns, lie in\nthe range 1.4 to 1.65. The estimated inverse mean square deviations (beta) show\na power law behavior in time with exponent values between -0.91 and -1.1\nindicating normal to mildly subdiffusive behavior. Quite often, the dynamics of\nmarket return distributions is modelled by a Fokker-Plank (FP) equation either\nwith a linear drift and a nonlinear diffusion term or with just a nonlinear\ndiffusion term. Both of these cases support a q-Gaussian distribution as a\nsolution. The distributions obtained from current estimated parameters are\ncompared with the solutions of the FP equations. For negligible drift term, the\ninverse mean square deviations (betaFP) from the FP model follow a power law\nwith exponent values between -1.25 and -1.48 indicating superdiffusion. When\nthe drift term is non-negligible, the corresponding betaFP do not follow a\npower law and become stationary after certain characteristic times that depend\non the values of the drift parameter and q. Neither of these behaviors is\nsupported by the results of the empirical fit.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.07752v3"
    },
    {
        "title": "Stochastic Tail Exponent For Asymmetric Power Laws",
        "authors": [
            "Nassim Nicholas Taleb"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We examine random variables in the power law/regularly varying class with\nstochastic tail exponent, the exponent $\\alpha$ having its own distribution. We\nshow the effect of stochasticity of $\\alpha$ on the expectation and higher\nmoments of the random variable. For instance, the moments of a right-tailed or\nright-asymmetric variable, when finite, increase with the variance of $\\alpha$;\nthose of a left-asymmetric one decreases. The same applies to conditional\nshortfall (CVar), or mean-excess functions. We prove the general case and\nexamine the specific situation of lognormally distributed $\\alpha \\in\n[b,\\infty), b>1$. The stochasticity of the exponent induces a significant bias\nin the estimation of the mean and higher moments in the presence of data\nuncertainty. This has consequences on sampling error as uncertainty about\n$\\alpha$ translates into a higher expected mean. The bias is conserved under\nsummation, even upon large enough a number of summands to warrant convergence\nto the stable distribution. We establish inequalities related to the asymmetry.\nWe also consider the situation of capped power laws (i.e. with compact\nsupport), and apply it to the study of violence by Cirillo and Taleb (2016). We\nshow that uncertainty concerning the historical data increases the true mean.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.02369v3"
    },
    {
        "title": "Taylor's Law of temporal fluctuation scaling in stock illiquidity",
        "authors": [
            "Qing Cai",
            "Hai-Chuan Xu",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Taylor's law of temporal fluctuation scaling, variance $\\sim$ $a($mean$)^b$,\nis ubiquitous in natural and social sciences. We report for the first time\nconvincing evidence of a solid temporal fluctuation scaling law in stock\nilliquidity by investigating the mean-variance relationship of the\nhigh-frequency illiquidity of almost all stocks traded on the Shanghai Stock\nExchange (SHSE) and the Shenzhen Stock Exchange (SZSE) during the period from\n1999 to 2011. Taylor's law holds for A-share markets (SZSE Main Board, SZSE\nSmall & Mediate Enterprise Board, SZSE Second Board, and SHSE Main Board) and\nB-share markets (SZSE B-share and SHSE B-share). We find that the scaling\nexponent $b$ is greater than 2 for the A-share markets and less than 2 for the\nB-share markets. We further unveil that Taylor's law holds for stocks in 17\nindustry categories, in 28 industrial sectors and in 31 provinces and\ndirect-controlled municipalities with the majority of scaling exponents\n$b\\in(2,3)$. We also investigate the $\\Delta{t}$-min illiquidity and find that\nthe scaling exponent $b(\\Delta{t})$ increases logarithmically for small\n$\\Delta{t}$ values and decreases fast to a stable level.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.01149v1"
    },
    {
        "title": "Minimum spanning tree filtering of correlations for varying time scales\n  and size of fluctuations",
        "authors": [
            "Jaroslaw Kwapien",
            "Pawel Oswiecimka",
            "Marcin Forczek",
            "Stanislaw Drozdz"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Based on a recently proposed $q$-dependent detrended cross-correlation\ncoefficient $\\rho_q$, we generalize the concept of minimum spanning tree (MST)\nby introducing a family of $q$-dependent minimum spanning trees ($q$MST) that\nare selective to cross-correlations between different fluctuation amplitudes\nand different time scales. They inherit this ability directly from the\ncoefficients $\\rho_q$ that are processed here to construct a distance matrix.\nConventional MST with detrending corresponds in this context to $q=2$. We apply\nthe $q$MSTs to sample empirical data from the stock market and discuss the\nresults. We show that the $q$MST graphs can complement $\\rho_q$ in\ndisentangling correlations that cannot be observed by the MST graphs based on\n$\\rho_{\\rm DCCA}$ and, therefore, they can be useful in many areas where the\nmultivariate cross-correlations are of interest. We apply our method to data\nfrom the stock market and obtain more information about correlation structure\nof the data than by using $q=2$ only. We show that two sets of signals that\ndiffer from each other statistically can give comparable trees for $q=2$, while\nonly by using the trees for $q \\ne 2$ we become able to distinguish between\nthese sets. We also show that a family of $q$MSTs for a range of $q$ express\nthe diversity of correlations in a manner resembling the multifractal analysis,\nwhere one computes a spectrum of the generalized fractal dimensions, the\ngeneralized Hurst exponents, or the multifractal singularity spectra: the more\ndiverse the correlations are, the more variable the tree topology is for\ndifferent $q$s. Our analysis exhibits that the stocks belonging to the same or\nsimilar industrial sectors are correlated via the fluctuations of moderate\namplitudes, while the largest fluctuations often happen to synchronize in those\nstocks that do not necessarily belong to the same industry.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08416v2"
    },
    {
        "title": "Theory of earthquakes interevent times applied to financial markets",
        "authors": [
            "Maciej Jagielski",
            "Ryszard Kutner",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We analyze the probability density function (PDF) of waiting times between\nfinancial loss exceedances. The empirical PDFs are fitted with the self-excited\nHawkes conditional Poisson process with a long power law memory kernel. The\nHawkes process is the simplest extension of the Poisson process that takes into\naccount how past events influence the occurrence of future events. By analyzing\nthe empirical data for 15 different financial assets, we show that the\nformalism of the Hawkes process used for earthquakes can successfully model the\nPDF of interevent times between successive market losses.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08921v2"
    },
    {
        "title": "Emergence of world-stock-market network",
        "authors": [
            "M. Saeedian",
            "T. Jamali",
            "M. Z. Kamali",
            "H. Bayani",
            "T. Yasseri",
            "G. R. Jafari"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  In the age of globalization, it is natural that the stock market of each\ncountry is not independent form the other markets. In this case, collective\nbehavior could be emerged form their dependency together. This article studies\nthe collective behavior of a set of forty influential markets in the world\neconomy with the aim of exploring a global financial structure that could be\ncalled world-stock-market network. Towards this end, we analyze the\ncross-correlation matrix of the indices of these forty markets using Random\nMatrix Theory (RMT). We find the degree of collective behavior among the\nmarkets and the share of each market in their structural formation. This\nfinding together with the results obtained from the same calculation on four\nstock markets reinforce the idea of a world financial market. Finally, we draw\nthe dendrogram of the cross-correlation matrix to make communities in this\nabstract global market visible. The dendrogram, drawn by at least thirty\npercent of correlation, shows that the world financial market comprises three\ncommunities each of which includes stock markets with geographical proximity.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.08781v1"
    },
    {
        "title": "Analysis of Realized Volatility for Nikkei Stock Average on the Tokyo\n  Stock Exchange",
        "authors": [
            "Tetsuya Takaishi",
            "Toshiaki Watanabe"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We calculate realized volatility of the Nikkei Stock Average (Nikkei225)\nIndex on the Tokyo Stock Exchange and investigate the return dynamics. To avoid\nthe bias on the realized volatility from the non-trading hours issue we\ncalculate realized volatility separately in the two trading sessions, i.e.\nmorning and afternoon, of the Tokyo Stock Exchange and find that the\nmicrostructure noise decreases the realized volatility at small sampling\nfrequency. Using realized volatility as a proxy of the integrated volatility we\nstandardize returns in the morning and afternoon sessions and investigate the\nnormality of the standardized returns by calculating variance, kurtosis and 6th\nmoment. We find that variance, kurtosis and 6th moment are consistent with\nthose of the standard normal distribution, which indicates that the return\ndynamics of the Nikkei Stock Average are well described by a Gaussian random\nprocess with time-varying volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.09386v1"
    },
    {
        "title": "Social dynamics of financial networks",
        "authors": [
            "Teruyoshi Kobayashi",
            "Taro Takaguchi"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The global financial crisis in 2007-2009 demonstrated that systemic risk can\nspread all over the world through a complex web of financial linkages, yet we\nstill lack fundamental knowledge about the evolution of the financial web. In\nparticular, interbank credit networks shape the core of the financial system,\nin which a time-varying interconnected risk emerges from a massive number of\ntemporal transactions between banks. The current lack of understanding of the\nmechanics of interbank networks makes it difficult to evaluate and control\nsystemic risk. Here, we uncover fundamental dynamics of interbank networks by\nseeking the patterns of daily transactions between individual banks. We find\nstable interaction patterns between banks from which distinctive network-scale\ndynamics emerge. In fact, the dynamical patterns discovered at the local and\nnetwork scales share common characteristics with social communication patterns\nof humans. To explain the origin of \"social\" dynamics in interbank networks, we\nprovide a simple model that allows us to generate a sequence of synthetic daily\nnetworks characterized by the observed dynamical properties. The discovery of\ndynamical principles at the daily resolution will enhance our ability to assess\nsystemic risk and could contribute to the real-time management of financial\nstability.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.10832v2"
    },
    {
        "title": "Machine Learning for Better Models for Predicting Bond Prices",
        "authors": [
            "Swetava Ganguli",
            "Jared Dunnmon"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Bond prices are a reflection of extremely complex market interactions and\npolicies, making prediction of future prices difficult. This task becomes even\nmore challenging due to the dearth of relevant information, and accuracy is not\nthe only consideration--in trading situations, time is of the essence. Thus,\nmachine learning in the context of bond price predictions should be both fast\nand accurate. In this course project, we use a dataset describing the previous\n10 trades of a large number of bonds among other relevant descriptive metrics\nto predict future bond prices. Each of 762,678 bonds in the dataset is\ndescribed by a total of 61 attributes, including a ground truth trade price. We\nevaluate the performance of various supervised learning algorithms for\nregression followed by ensemble methods, with feature and model selection\nconsiderations being treated in detail. We further evaluate all methods on both\naccuracy and speed. Finally, we propose a novel hybrid time-series aided\nmachine learning method that could be applied to such datasets in future work.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01142v1"
    },
    {
        "title": "Stochastic modelling of non-stationary financial assets",
        "authors": [
            "Joana Estevens",
            "Paulo Rocha",
            "Joao Boto",
            "Pedro Lind"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We model non-stationary volume-price distributions with a log-normal\ndistribution and collect the time series of its two parameters. The time series\nof the two parameters are shown to be stationary and Markov-like and\nconsequently can be modelled with Langevin equations, which are derived\ndirectly from their series of values. Having the evolution equations of the\nlog-normal parameters, we reconstruct the statistics of the first moments of\nvolume-price distributions which fit well the empirical data. Finally, the\nproposed framework is general enough to study other non-stationary stochastic\nvariables in other research fields, namely biology, medicine and geology.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01145v1"
    },
    {
        "title": "Optimum thresholding using mean and conditional mean square error",
        "authors": [
            "José E. Figueroa-López",
            "Cecilia Mancini"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We consider a univariate semimartingale model for (the logarithm of) an asset\nprice, containing jumps having possibly infinite activity (IA). The\nnonparametric threshold estimator of the integrated variance IV proposed in\nMancini 2009 is constructed using observations on a discrete time grid, and\nprecisely it sums up the squared increments of the process when they are below\na threshold, a deterministic function of the observation step and possibly of\nthe coefficients of X. All the threshold functions satisfying given conditions\nallow asymptotically consistent estimates of IV, however the finite sample\nproperties of the truncated realized variation can depend on the specific\nchoice of the threshold. We aim here at optimally selecting the threshold by\nminimizing either the estimation mean square error (MSE) or the conditional\nmean square error (cMSE). The last criterion allows to reach a threshold which\nis optimal not in mean but for the specific volatility (and jumps paths) at\nhand. A parsimonious characterization of the optimum is established, which\nturns out to be asymptotically proportional to the L\\'evy's modulus of\ncontinuity of the underlying Brownian motion. Moreover, minimizing the cMSE\nenables us to propose a novel implementation scheme for approximating the\noptimal threshold. Monte Carlo simulations illustrate the superior performance\nof the proposed method.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04339v1"
    },
    {
        "title": "High-Frequency Jump Tests: Which Test Should We Use?",
        "authors": [
            "Worapree Maneesoonthorn",
            "Gael M. Martin",
            "Catherine S. Forbes"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We conduct an extensive evaluation of price jump tests based on\nhigh-frequency financial data. After providing a concise review of multiple\nalternative tests, we document the size and power of all tests in a range of\nempirically relevant scenarios. Particular focus is given to the robustness of\ntest performance to the presence of jumps in volatility and microstructure\nnoise, and to the impact of sampling frequency. The paper concludes by\nproviding guidelines for empirical researchers about which test to choose in\nany given setting.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.09520v3"
    },
    {
        "title": "Data science for assessing possible tax income manipulation: The case of\n  Italy",
        "authors": [
            "Marcel Ausloos",
            "Roy Cerqueti",
            "Tariq A. Mir"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  This paper explores a real-world fundamental theme under a data science\nperspective. It specifically discusses whether fraud or manipulation can be\nobserved in and from municipality income tax size distributions, through their\naggregation from citizen fiscal reports. The study case pertains to official\ndata obtained from the Italian Ministry of Economics and Finance over the\nperiod 2007-2011. All Italian (20) regions are considered. The considered data\nscience approach concretizes in the adoption of the Benford first digit law as\nquantitative tool. Marked disparities are found, - for several regions, leading\nto unexpected \"conclusions\". The most eye browsing regions are not the expected\nones according to classical imagination about Italy financial shadow matters.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.02129v1"
    },
    {
        "title": "A Modified Levy Jump-Diffusion Model Based on Market Sentiment Memory\n  for Online Jump Prediction",
        "authors": [
            "Zheqing Zhu",
            "Jian-guo Liu",
            "Lei Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  In this paper, we propose a modified Levy jump diffusion model with market\nsentiment memory for stock prices, where the market sentiment comes from data\nmining implementation using Tweets on Twitter. We take the market sentiment\nprocess, which has memory, as the signal of Levy jumps in the stock price. An\nonline learning and optimization algorithm with the Unscented Kalman filter\n(UKF) is then proposed to learn the memory and to predict possible price jumps.\nExperiments show that the algorithm provides a relatively good performance in\nidentifying asset return trends.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.03611v1"
    },
    {
        "title": "A Structural Model for Fluctuations in Financial Markets",
        "authors": [
            "Kartik Anand",
            "Jonathan Khedair",
            "Reimer Kuehn"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  In this paper we provide a comprehensive analysis of a structural model for\nthe dynamics of prices of assets traded in a market originally proposed in [1].\nThe model takes the form of an interacting generalization of the geometric\nBrownian motion model. It is formally equivalent to a model describing the\nstochastic dynamics of a system of analogue neurons, which is expected to\nexhibit glassy properties and thus many meta-stable states in a large portion\nof its parameter space. We perform a generating functional analysis,\nintroducing a slow driving of the dynamics to mimic the effect of slowly\nvarying macro-economic conditions. Distributions of asset returns over various\ntime separations are evaluated analytically and are found to be fat-tailed in a\nmanner broadly in line with empirical observations. Our model also allows to\nidentify collective, interaction mediated properties of pricing distributions\nand it predicts pricing distributions which are significantly broader than\ntheir non-interacting counterparts, if interactions between prices in the model\ncontain a ferro-magnetic bias. Using simulations, we are able to substantiate\none of the main hypotheses underlying the original modelling, viz. that the\nphenomenon of volatility clustering can be rationalised in terms of an\ninterplay between the dynamics within meta-stable states and the dynamics of\noccasional transitions between them.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.10277v1"
    },
    {
        "title": "Forecasting dynamic return distributions based on ordered binary choice",
        "authors": [
            "Stanislav Anatolyev",
            "Jozef Barunik"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We present a simple approach to forecasting conditional probability\ndistributions of asset returns. We work with a parsimonious specification of\nordered binary choice regression that imposes a connection on sign\npredictability across different quantiles. The model forecasts the future\nconditional probability distributions of returns quite precisely when using a\npast indicator and past volatility proxy as predictors. Direct benefits of the\nmodel are revealed in an empirical application to the 29 most liquid U.S.\nstocks. The forecast probability distribution is translated to significant\neconomic gains in a simple trading strategy. Our approach can also be useful in\nmany other applications where conditional distribution forecasts are desired.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.05681v3"
    },
    {
        "title": "Statistical properties of market collective responses",
        "authors": [
            "Shanshan Wang",
            "Sebastian Neusüß",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We empirically analyze the price and liquidity responses to trade signs,\ntraded volumes and signed traded volumes. Utilizing the singular value\ndecomposition, we explore the interconnections of price responses and of\nliquidity responses across the whole market. The statistical characteristics of\ntheir singular vectors are well described by the $t$ location-scale\ndistribution. Furthermore, we discuss the relation between prices and liquidity\nwith respect to their overlapping factors. The factors of price and liquidity\nchanges are non-random when these factors are related to the traded volumes.\nThis means that the traded volumes play a critical role in the price change\ninduced by the liquidity change. In contrast, the two kinds of factors are\nweakly overlapping when they are related to the trade signs and signed traded\nvolumes. Hence, an imbalance of liquidity is related to the price change.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.07630v1"
    },
    {
        "title": "Using nonlinear stochastic and deterministic (chaotic tools) to test the\n  EMH of two Electricity Markets the case of Italy and Greece",
        "authors": [
            "George P Papaioannou",
            "Christos Dikaiakos",
            "Anargyros Dramountanis",
            "Dionysios S Georgiadis",
            "Panagiotis G Papaioannou"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Utilization of non-linear tools to characterize the state of development of\nthe electricity markets in Italy and Greece. This is equivalent to testing the\nEfficient Market Hypothesis on these markets. The tools include a variety of\ncomplexity measures like Maximal Lyapunov and Hurst exponents and HHI index for\nmarket concentration and Entropy, a measure of uncertainty and complexity in a\ndynamical system, applied on the electricity wholesale marginal prices PUN and\nSMP of Italy and Greece.Our aim is to measure the complexity and dimensionality\nof the manifold on which the underlying stochastic dynamical system, govenring\nthe prices, evolve. We also use the conditional volatility of prices, which is\na measure of the market risk, and its connection with stability, and Hurst\nexponent to investigate the properties of the fluctuations of the prices which\nare the footprints of the idiosyncrracies of each market.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.10552v1"
    },
    {
        "title": "Universal fluctuations in growth dynamics of economic systems",
        "authors": [
            "Nathan C. Frey",
            "Sakib Matin",
            "H. Eugene Stanley",
            "Michael Salinger"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The growth of business firms is an example of a system of complex interacting\nunits that resembles complex interacting systems in nature such as earthquakes.\nRemarkably, work in econophysics has provided evidence that the statistical\nproperties of the growth of business firms follow the same sorts of power laws\nthat characterize physical systems near their critical points. Given how\neconomies change over time, whether these statistical properties are\npersistent, robust, and universal like those of physical systems remains an\nopen question. Here, we show that the scaling properties of firm growth\npreviously demonstrated for publicly-traded U.S. manufacturing firms from 1974\nto 1993 apply to the same sorts of firms from 1993 to 2015, to firms in other\nbroad sectors (such as materials), and to firms in new sectors (such as\nInternet services). We measure virtually the same scaling exponent for\nmanufacturing for the 1993 to 2015 period as for the 1974 to 1993 period and\nvirtually the same scaling exponent for other sectors as for manufacturing.\nFurthermore, we show that fluctuations of the growth rate for new industries\nself-organize into a power law over relatively short time scales.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.02003v2"
    },
    {
        "title": "The consentaneous model of the financial markets exhibiting spurious\n  nature of long-range memory",
        "authors": [
            "Vygintas Gontis",
            "Aleksejus Kononovicius"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  It is widely accepted that there is strong persistence in the volatility of\nfinancial time series. The origin of the observed persistence, or long-range\nmemory, is still an open problem as the observed phenomenon could be a spurious\neffect. Earlier we have proposed the consentaneous model of the financial\nmarkets based on the non-linear stochastic differential equations. The\nconsentaneous model successfully reproduces empirical probability and power\nspectral densities of volatility. This approach is qualitatively different from\nmodels built using fractional Brownian motion. In this contribution we\ninvestigate burst and inter-burst duration statistics of volatility in the\nfinancial markets employing the consentaneous model. Our analysis provides an\nevidence that empirical statistical properties of burst and inter-burst\nduration can be explained by non-linear stochastic differential equations\ndriving the volatility in the financial markets. This serves as an strong\nargument that long-range memory in finance can have spurious nature.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.05121v2"
    },
    {
        "title": "Deep Learning for Forecasting Stock Returns in the Cross-Section",
        "authors": [
            "Masaya Abe",
            "Hideki Nakayama"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Many studies have been undertaken by using machine learning techniques,\nincluding neural networks, to predict stock returns. Recently, a method known\nas deep learning, which achieves high performance mainly in image recognition\nand speech recognition, has attracted attention in the machine learning field.\nThis paper implements deep learning to predict one-month-ahead stock returns in\nthe cross-section in the Japanese stock market and investigates the performance\nof the method. Our results show that deep neural networks generally outperform\nshallow neural networks, and the best networks also outperform representative\nmachine learning models. These results indicate that deep learning shows\npromise as a skillful machine learning method to predict stock returns in the\ncross-section.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.01777v4"
    },
    {
        "title": "The Network of U.S. Mutual Fund Investments: Diversification, Similarity\n  and Fragility throughout the Global Financial Crisis",
        "authors": [
            "Danilo Delpini",
            "Stefano Battiston",
            "Guido Caldarelli",
            "Massimo Riccaboni"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Network theory proved recently to be useful in the quantification of many\nproperties of financial systems. The analysis of the structure of investment\nportfolios is a major application since their eventual correlation and overlap\nimpact the actual risk diversification by individual investors. We investigate\nthe bipartite network of US mutual fund portfolios and their assets. We follow\nits evolution during the Global Financial Crisis and analyse the interplay\nbetween diversification, as understood in classical portfolio theory, and\nsimilarity of the investments of different funds. We show that, on average,\nportfolios have become more diversified and less similar during the crisis.\nHowever, we also find that large overlap is far more likely than expected from\nmodels of random allocation of investments. This indicates the existence of\nstrong correlations between fund portfolio strategies. We introduce a\nsimplified model of propagation of financial shocks, that we exploit to show\nthat a systemic risk component origins from the similarity of portfolios. The\nnetwork is still vulnerable after crisis because of this effect, despite the\nincrease in the diversification of portfolios. Our results indicate that\ndiversification may even increase systemic risk when funds diversify in the\nsame way. Diversification and similarity can play antagonistic roles and the\ntrade-off between the two should be taken into account to properly assess\nsystemic risk.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02205v1"
    },
    {
        "title": "Ranking Causal Influence of Financial Markets via Directed Information\n  Graphs",
        "authors": [
            "Theo Diamandis",
            "Yonathan Murin",
            "Andrea Goldsmith"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  A non-parametric method for ranking stock indices according to their mutual\ncausal influences is presented. Under the assumption that indices reflect the\nunderlying economy of a country, such a ranking indicates which countries exert\nthe most economic influence in an examined subset of the global economy. The\nproposed method represents the indices as nodes in a directed graph, where the\nedges' weights are estimates of the pair-wise causal influences, quantified\nusing the directed information functional. This method facilitates using a\nrelatively small number of samples from each index. The indices are then ranked\naccording to their net-flow in the estimated graph (sum of the incoming weights\nsubtracted from the sum of outgoing weights). Daily and minute-by-minute data\nfrom nine indices (three from Asia, three from Europe and three from the US)\nwere analyzed. The analysis of daily data indicates that the US indices are the\nmost influential, which is consistent with intuition that the indices\nrepresenting larger economies usually exert more influence. Yet, it is also\nshown that an index representing a small economy can strongly influence an\nindex representing a large economy if the smaller economy is indicative of a\nlarger phenomenon. Finally, it is shown that while inter-region interactions\ncan be captured using daily data, intra-region interactions require more\nfrequent samples.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06896v1"
    },
    {
        "title": "Immediate Causality Network of Stock Markets",
        "authors": [
            "Li Zhou",
            "Lu Qiu",
            "Changgui Gu",
            "Huijie Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  A financial system contains many elements networked by their relationships.\nExtensive works show that topological structure of the network stores rich\ninformation on evolutionary behaviors of the system such as early warning\nsignals of collapses and/or crises. Existing works focus mainly on the network\nstructure within a single stock market, while a collapse/crisis occurs in a\nmacro-scale covering several or even all markets in the world. This mismatch of\nscale leads to unacceptable noise to the topological structure, and lack of\ninformation stored in relationships between different markets. In this work by\nusing the transfer entropy we reconstruct the influential network between ten\ntypical stock markets distributed in the world. Interesting findings include,\nbefore a financial crisis the connection strength reaches a maxima, which can\nact as an early warning signal of financial crises; The markets in America are\nmono-directionally and strongly influenced by that in Europe and act as the\ncenter; Some strongly linked pairs have also close correlations. The findings\nare helpful in understanding the evolution and modelling the dynamical process\nof the global financial system.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.02699v1"
    },
    {
        "title": "High-Dimensional Estimation, Basis Assets, and the Adaptive Multi-Factor\n  Model",
        "authors": [
            "Liao Zhu",
            "Sumanta Basu",
            "Robert A. Jarrow",
            "Martin T. Wells"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  The paper proposes a new algorithm for the high-dimensional financial data --\nthe Groupwise Interpretable Basis Selection (GIBS) algorithm, to estimate a new\nAdaptive Multi-Factor (AMF) asset pricing model, implied by the recently\ndeveloped Generalized Arbitrage Pricing Theory, which relaxes the convention\nthat the number of risk-factors is small. We first obtain an adaptive\ncollection of basis assets and then simultaneously test which basis assets\ncorrespond to which securities, using high-dimensional methods. The AMF model,\nalong with the GIBS algorithm, is shown to have a significantly better fitting\nand prediction power than the Fama-French 5-factor model.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.08472v7"
    },
    {
        "title": "Analyzing order flows in limit order books with ratios of Cox-type\n  intensities",
        "authors": [
            "Ioane Muni Toke",
            "Nakahiro Yoshida"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We introduce a Cox-type model for relative intensities of orders flows in a\nlimit order book. The model assumes that all intensities share a common\nbaseline intensity, which may for example represent the global market activity.\nParameters can be estimated by quasi likelihood maximization, without any\ninterference from the baseline intensity. Consistency and asymptotic behavior\nof the estimators are given in several frameworks, and model selection is\ndiscussed with information criteria and penalization. The model is well-suited\nfor high-frequency financial data: fitted models using easily interpretable\ncovariates show an excellent agreement with empirical data. Extensive\ninvestigation on tick data consequently helps identifying trading signals and\nimportant factors determining the limit order book dynamics. We also illustrate\nthe potential use of the framework for out-of-sample predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06682v3"
    },
    {
        "title": "Description of Incomplete Financial Markets for the Discrete Time\n  Evolution of Risk Assets",
        "authors": [
            "N. S. Gonchar"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In the paper, the martingales and super-martingales relative to a regular set\nof measures are systematically studied. The notion of local regular\nsuper-martingale relative to a set of equivalent measures is introduced and the\nnecessary and sufficient conditions of the local regularity of it in the\ndiscrete case are founded. The regular set of measures play fundamental role\nfor the description of incomplete markets. In the partial case, the description\nof the regular set of measures is presented. The notion of completeness of the\nregular set of measures have the important significance for the simplification\nof the proof of the optional decomposition for super-martingales. Using this\nnotion, the important inequalities for some random values are obtained. These\ninequalities give the simple proof of the optional decomposition of the\nmajorized super-martingales. The description of all local regular\nsuper-martingales relative to the regular set of measures is presented. It is\nproved that every majorized super-martingale relative to the complete set of\nmeasures is a local regular one. In the case, as evolution of a risk asset is\ngiven by the discrete geometric Brownian motion, the financial market is\nincomplete and a new formula for the fair price of super-hedge is founded.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.09366v1"
    },
    {
        "title": "Using published bid/ask curves to error dress spot electricity price\n  forecasts",
        "authors": [
            "Gunnhildur H. Steinbakk",
            "Alex Lenkoski",
            "Ragnar Bang Huseby",
            "Anders Løland",
            "Tor Arne Øigård"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Accurate forecasts of electricity spot prices are essential to the daily\noperational and planning decisions made by power producers and distributors.\nTypically, point forecasts of these quantities suffice, particularly in the\nNord Pool market where the large quantity of hydro power leads to price\nstability. However, when situations become irregular, deviations on the price\nscale can often be extreme and difficult to pinpoint precisely, which is a\nresult of the highly varying marginal costs of generating facilities at the\nedges of the load curve. In these situations it is useful to supplant a point\nforecast of price with a distributional forecast, in particular one whose tails\nare adaptive to the current production regime. This work outlines a methodology\nfor leveraging published bid/ask information from the Nord Pool market to\nconstruct such adaptive predictive distributions. Our methodology is a\nnon-standard application of the concept of error-dressing, which couples a\nfeature driven error distribution in volume space with a non-linear\ntransformation via the published bid/ask curves to obtain highly non-symmetric,\nadaptive price distributions. Using data from the Nord Pool market, we show\nthat our method outperforms more standard forms of distributional modeling. We\nfurther show how such distributions can be used to render `warning systems'\nthat issue reliable probabilities of prices exceeding various important\nthresholds.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.02433v1"
    },
    {
        "title": "Zero-Inflated Autoregressive Conditional Duration Model for Discrete\n  Trade Durations with Excessive Zeros",
        "authors": [
            "Francisco Blasques",
            "Vladimír Holý",
            "Petra Tomanová"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In finance, durations between successive transactions are usually modeled by\nthe autoregressive conditional duration model based on a continuous\ndistribution omitting zero values. Zero or close-to-zero durations can be\ncaused by either split transactions or independent transactions. We propose a\ndiscrete model allowing for excessive zero values based on the zero-inflated\nnegative binomial distribution with score dynamics. This model allows to\ndistinguish between the processes generating split and standard transactions.\nWe use the existing theory on score models to establish the invertibility of\nthe score filter and verify that sufficient conditions hold for the consistency\nand asymptotic normality of the maximum likelihood of the model parameters. In\nan empirical study, we find that split transactions cause between 92 and 98\npercent of zero and close-to-zero values. Furthermore, the loss of decimal\nplaces in the proposed approach is less severe than the incorrect treatment of\nzero values in continuous models.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07318v4"
    },
    {
        "title": "Fast Training Algorithms for Deep Convolutional Fuzzy Systems with\n  Application to Stock Index Prediction",
        "authors": [
            "Li-Xin Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  A deep convolutional fuzzy system (DCFS) on a high-dimensional input space is\na multi-layer connection of many low-dimensional fuzzy systems, where the input\nvariables to the low-dimensional fuzzy systems are selected through a moving\nwindow across the input spaces of the layers. To design the DCFS based on\ninput-output data pairs, we propose a bottom-up layer-by-layer scheme.\nSpecifically, by viewing each of the first-layer fuzzy systems as a weak\nestimator of the output based only on a very small portion of the input\nvariables, we design these fuzzy systems using the WM Method. After the\nfirst-layer fuzzy systems are designed, we pass the data through the first\nlayer to form a new data set and design the second-layer fuzzy systems based on\nthis new data set in the same way as designing the first-layer fuzzy systems.\nRepeating this process layer-by-layer we design the whole DCFS. We also propose\na DCFS with parameter sharing to save memory and computation. We apply the DCFS\nmodels to predict a synthetic chaotic plus random time-series and the real Hang\nSeng Index of the Hong Kong stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.11226v2"
    },
    {
        "title": "Neural Network-based Automatic Factor Construction",
        "authors": [
            "Jie Fang",
            "Jianwu Lin",
            "Shutao Xia",
            "Yong Jiang",
            "Zhikang Xia",
            "Xiang Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Instead of conducting manual factor construction based on traditional and\nbehavioural finance analysis, academic researchers and quantitative investment\nmanagers have leveraged Genetic Programming (GP) as an automatic feature\nconstruction tool in recent years, which builds reverse polish mathematical\nexpressions from trading data into new factors. However, with the development\nof deep learning, more powerful feature extraction tools are available. This\npaper proposes Neural Network-based Automatic Factor Construction (NNAFC), a\ntailored neural network framework that can automatically construct diversified\nfinancial factors based on financial domain knowledge and a variety of neural\nnetwork structures. The experiment results show that NNAFC can construct more\ninformative and diversified factors than GP, to effectively enrich the current\nfactor pool. For the current market, both fully connected and recurrent neural\nnetwork structures are better at extracting information from financial time\nseries than convolution neural network structures. Moreover, new factors\nconstructed by NNAFC can always improve the return, Sharpe ratio, and the max\ndraw-down of a multi-factor quantitative investment strategy due to their\nintroducing more information and diversification to the existing factor pool.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06225v3"
    },
    {
        "title": "Learning low-frequency temporal patterns for quantitative trading",
        "authors": [
            "Joel da Costa",
            "Tim Gebbie"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We consider the viability of a modularised mechanistic online machine\nlearning framework to learn signals in low-frequency financial time series\ndata. The framework is proved on daily sampled closing time-series data from\nJSE equity markets. The input patterns are vectors of pre-processed sequences\nof daily, weekly and monthly or quarterly sampled feature changes. The data\nprocessing is split into a batch processed step where features are learnt using\na stacked autoencoder via unsupervised learning, and then both batch and online\nsupervised learning are carried out using these learnt features, with the\noutput being a point prediction of measured time-series feature fluctuations.\nWeight initializations are implemented with restricted Boltzmann machine\npre-training, and variance based initializations. Historical simulations are\nthen run using an online feedforward neural network initialised with the\nweights from the batch training and validation step. The validity of results\nare considered under a rigorous assessment of backtest overfitting using both\ncombinatorially symmetrical cross validation and probabilistic and deflated\nSharpe ratios. Results are used to develop a view on the phenomenology of\nfinancial markets and the value of complex historical data-analysis for trading\nunder the unstable adaptive dynamics that characterise financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09481v1"
    },
    {
        "title": "Using detrended deconvolution foreign exchange network to identify\n  currency status",
        "authors": [
            "Pengfei Xi",
            "Shiyang Lai",
            "Xueying Wang",
            "Weiqiang Huang"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This article proposed a hybrid detrended deconvolution foreign exchange\nnetwork construction method (DDFEN), which combined the detrended\ncross-correlation analysis coefficient (DCCC) and the network deconvolution\nmethod together. DDFEN is designed to reveal the `true' correlation of\ncurrencies by filtering indirect effects in the foreign exchange networks\n(FXNs). The empirical results show that DDFEN can reflect the change of\ncurrency status in the long term and also perform more stable than traditional\nnetwork construction methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09482v1"
    },
    {
        "title": "Quantifying the impact of COVID-19 on the US stock market: An analysis\n  from multi-source information",
        "authors": [
            "Asim Kumer Dey",
            "Toufiqul Haq",
            "Kumer Das",
            "Irina Panovska"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We develop a novel temporal complex network approach to quantify the US\ncounty level spread dynamics of COVID-19. The objective is to study the effects\nof the local spread dynamics, COVID-19 cases and death, and Google search\nactivities on the US stock market. We use both conventional econometric and\nMachine Learning (ML) models. The results suggest that COVID-19 cases and\ndeaths, its local spread, and Google searches have impacts on abnormal stock\nprices between January 2020 to May 2020. In addition, incorporating information\nabout local spread significantly improves the performance of forecasting models\nof the abnormal stock prices at longer forecasting horizons. On the other hand,\nalthough a few COVID-19 related variables, e.g., US total deaths and US new\ncases exhibit causal relationships on price volatility, COVID-19 cases and\ndeaths, local spread of COVID-19, and Google search activities do not have\nimpacts on price volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.10885v3"
    },
    {
        "title": "Investigation of Flash Crash via Topological Data Analysis",
        "authors": [
            "Wonse Kim",
            "Younng-Jin Kim",
            "Gihyun Lee",
            "Woong Kook"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Topological data analysis has been acknowledged as one of the most successful\nmathematical data analytic methodologies in various fields including medicine,\ngenetics, and image analysis. In this paper, we explore the potential of this\nmethodology in finance by applying persistence landscape and dynamic time\nseries analysis to analyze an extreme event in the stock market, known as Flash\nCrash. We will provide results of our empirical investigation to confirm the\neffectiveness of our new method not only for the characterization of this\nextreme event but also for its prediction purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.11558v1"
    },
    {
        "title": "Share Price Prediction of Aerospace Relevant Companies with Recurrent\n  Neural Networks based on PCA",
        "authors": [
            "Linyu Zheng",
            "Hongmei He"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The capital market plays a vital role in marketing operations for aerospace\nindustry. However, due to the uncertainty and complexity of the stock market\nand many cyclical factors, the stock prices of listed aerospace companies\nfluctuate significantly. This makes the share price prediction challengeable.\nTo improve the prediction of share price for aerospace industry sector and well\nunderstand the impact of various indicators on stock prices, we provided a\nhybrid prediction model by the combination of Principal Component Analysis\n(PCA) and Recurrent Neural Networks. We investigated two types of aerospace\nindustries (manufacturer and operator). The experimental results show that PCA\ncould improve both accuracy and efficiency of prediction. Various factors could\ninfluence the performance of prediction models, such as finance data, extracted\nfeatures, optimisation algorithms, and parameters of the prediction model. The\nselection of features may depend on the stability of historical data: technical\nfeatures could be the first option when the share price is stable, whereas\nfundamental features could be better when the share price has high fluctuation.\nThe delays of RNN also depend on the stability of historical data for different\ntypes of companies. It would be more accurate through using short-term\nhistorical data for aerospace manufacturers, whereas using long-term historical\ndata for aerospace operating airlines. The developed model could be an\nintelligent agent in an automatic stock prediction system, with which, the\nfinancial industry could make a prompt decision for their economic strategies\nand business activities in terms of predicted future share price, thus\nimproving the return on investment. Currently, COVID-19 severely influences\naerospace industries. The developed approach can be used to predict the share\nprice of aerospace industries at post COVID-19 time.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.11788v1"
    },
    {
        "title": "Optimal models of extreme volume-prices are time-dependent",
        "authors": [
            "Paulo Rocha",
            "Frank Raischel",
            "João Pedro Boto",
            "Pedro G. Lind"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We present evidence that the best model for empirical volume-price\ndistributions is not always the same and it strongly depends in (i) the region\nof the volume-price spectrum that one wants to model and (ii) the period in\ntime that is being modelled. To show these two features we analyze stocks of\nthe New York stock market with four different models: Gamma, inverse-gamma,\nlog-normal, and Weibull distributions. To evaluate the accuracy of each model\nwe use standard relative deviations as well as the Kullback-Leibler distance\nand introduce an additional distance particularly suited to evaluate how\naccurate are the models for the distribution tails (large volume-price).\nFinally we put our findings in perspective and discuss how they can be extended\nto other situations in finance engineering.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.6257v1"
    },
    {
        "title": "Herding interactions as an opportunity to prevent extreme events in\n  financial markets",
        "authors": [
            "Aleksejus Kononovicius",
            "Vygintas Gontis"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  A characteristic feature of complex systems in general is a tight coupling\nbetween their constituent parts. In complex socio-economic systems this kind of\nbehavior leads to self-organization, which may be both desirable (e.g. social\ncooperation) and undesirable (e.g. mass panic, financial \"bubbles\" or\n\"crashes\"). Abundance of the empirical data as well as general insights into\nthe trading behavior enables the creation of simple agent-based models\nreproducing sophisticated statistical features of the financial markets. In\nthis contribution we consider a possibility to prevent self-organized extreme\nevents in artificial financial market setup built upon a simple agent-based\nherding model. We show that introduction of agents with predefined\nfundamentalist trading behavior helps to significantly reduce the probability\nof the extreme price fluctuations events. We also test random trading control\nstrategy, which was previously found to be promising, and find that its impact\non the market is rather ambiguous. Though some of the results indicate that it\nmight actually stabilize financial fluctuations.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.8024v5"
    },
    {
        "title": "Scaling analysis of time series of daily prices from stock markets of\n  transitional economies in the Western Balkans",
        "authors": [
            "Darko Sarvan",
            "Djordje Stratimirovic",
            "Suzana Blesic",
            "Vladimir Miljkovic"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In this paper we have analyzed scaling properties of time series of stock\nmarket indices (SMIs) of developing economies of Western Balkans, and have\ncompared the results we have obtained with the results from more developed\neconomies. We have used three different techniques of data analysis to obtain\nand verify our findings: Detrended Fluctuation Analysis (DFA) method, Detrended\nMoving Average (DMA) method, and Wavelet Transformation (WT) analysis. We have\nfound scaling behavior in all SMI data sets that we have analyzed. The scaling\nof our SMI series changes from long-range correlated to slightly\nanti-correlated behavior with the change in growth or maturity of the economy\nthe stock market is embedded in. We also report the presence of effects of\npotential periodic-like influences on the SMI data that we have analyzed. One\nsuch influence is visible in all our SMI series, and appears at a period\n$T_{p}\\approx 90$ days. We propose that the existence of various periodic-like\ninfluences on SMI data may partially explain the observed difference in types\nof correlated behavior of corresponding scaling functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.8119v1"
    },
    {
        "title": "Transitions between superstatistical regimes: validity, breakdown and\n  applications",
        "authors": [
            "Petr Jizba",
            "Jan Korbel",
            "Hynek Lavička",
            "Martin Prokš",
            "Václav Svoboda",
            "Christian Beck"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Superstatistics is a widely employed tool of non-equilibrium statistical\nphysics which plays an important role in analysis of hierarchical complex\ndynamical systems. Yet, its \"canonical\" formulation in terms of a single\nnuisance parameter is often too restrictive when applied to complex empirical\ndata. Here we show that a multi-scale generalization of the superstatistics\nparadigm is more versatile, allowing to address such pertinent issues as\ntransmutation of statistics or inter-scale stochastic behavior. To put some\nflesh on the bare bones, we provide a numerical evidence for a transition\nbetween two superstatistics regimes, by analyzing high-frequency (minute-tick)\ndata for share-price returns of seven selected companies. Salient issues, such\nas breakdown of superstatistics in fractional diffusion processes or connection\nwith Brownian subordination are also briefly discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.04838v2"
    },
    {
        "title": "Wax and wane of the cross-sectional momentum and contrarian effects:\n  Evidence from the Chinese stock markets",
        "authors": [
            "H. -L. Shi",
            "W. -X. Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  This paper investigates the time-varying risk-premium relation of the Chinese\nstock markets within the framework of cross-sectional momentum and contrarian\neffects by adopting the Capital Asset Pricing Model and the French-Fama three\nfactor model. The evolving arbitrage opportunities are also studied by\nquantifying the performance of time-varying cross-sectional momentum and\ncontrarian effects in the Chinese stock markets. The relation between the\ncontrarian profitability and market condition factors that could characterize\nthe investment context is also investigated. The results reveal that the\nrisk-premium relation varies over time, and the arbitrage opportunities based\non the contrarian portfolios wax and wane over time. The performance of\ncontrarian portfolios are highly dependent on several market conditions. The\nperiods with upward trend of market state, higher market volatility and\nliquidity, lower macroeconomics uncertainty are related to higher contrarian\nprofitability. These findings are consistent with the Adaptive Markets\nHypothesis and have practical implications for market participants.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05552v1"
    },
    {
        "title": "Linear and nonlinear correlations in order aggressiveness of Chinese\n  stocks",
        "authors": [
            "Peng Yue",
            "Hai-Chuan Xu",
            "Wei Chen",
            "Xiong Xiong",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The diagonal effect of orders is well documented in different markets, which\nstates that orders are more likely to be followed by orders of the same\naggressiveness and implies the presence of short-term correlations in order\nflows. Based on the order flow data of 43 Chinese stocks, we investigate if\nthere are long-range correlations in the time series of order aggressiveness.\nThe detrending moving average analysis shows that there are crossovers in the\nscaling behaviors of overall fluctuations and order aggressiveness exhibits\nlinear long-term correlations. We design an objective procedure to determine\nthe two Hurst indexes delimited by the crossover scale. We find no correlations\nin the short term and strong correlations in the long term for all stocks\nexcept for an outlier stock. The long-term correlation is found to depend on\nseveral firm specific characteristics. We also find that there are nonlinear\nlong-term correlations in the order aggressiveness when we perform the\nmultifractal detrending moving average analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05604v1"
    },
    {
        "title": "Correlations and Flow of Information between The New York Times and\n  Stock Markets",
        "authors": [
            "Andrés García-Medina",
            "Leonidas Sandoval Junior",
            "Efraín Urrutia Bañuelos",
            "A. M. Martínez-Argüello"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We use Random Matrix Theory (RMT) and information theory to analyze the\ncorrelations and flow of information between 64,939 news from The New York\nTimes and 40 world financial indices during 10 months along the period\n2015-2016. The set of news was quantified and transformed into daily polarity\ntime series using tools from sentiment analysis. Results from RMT shows that a\ncommon factor lead the world indices and news, and even share the same\ndynamics. Furthermore, the global correlation structure has found preserved\nwhen adding white noise, which indicate that correlations are not due to sample\nsize effects. Likewise, we found a lot of information flowing from news to\nworld indices for specific delay, being of practical interest for trading\npurpose. Our results suggest a deep relationship between news and world\nindices, and show a situation where news drive world market movements, giving a\nnew evidence to support behavioral finance as the current economic paradigm.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05778v1"
    },
    {
        "title": "Spurious memory in non-equilibrium stochastic models of imitative\n  behavior",
        "authors": [
            "Vygintas Gontis",
            "Aleksejus Kononovicius"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The origin of the long-range memory in the non-equilibrium systems is still\nan open problem as the phenomenon can be reproduced using models based on\nMarkov processes. In these cases a notion of spurious memory is introduced. A\ngood example of Markov processes with spurious memory is stochastic process\ndriven by a non-linear stochastic differential equation (SDE). This example is\nat odds with models built using fractional Brownian motion (fBm). We analyze\ndifferences between these two cases seeking to establish possible empirical\ntests of the origin of the observed long-range memory. We investigate\nprobability density functions (PDFs) of burst and inter-burst duration in\nnumerically obtained time series and compare with the results of fBm. Our\nanalysis confirms that the characteristic feature of the processes described by\na one-dimensional SDE is the power-law exponent $3/2$ of the burst or\ninter-burst duration PDF. This property of stochastic processes might be used\nto detect spurious memory in various non-equilibrium systems, where observed\nmacroscopic behavior can be derived from the imitative interactions of agents.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09801v1"
    },
    {
        "title": "NAPLES;Mining the lead-lag Relationship from Non-synchronous and\n  High-frequency Data",
        "authors": [
            "Katsuya Ito",
            "Kei Nakagawa"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In time-series analysis, the term \"lead-lag effect\" is used to describe a\ndelayed effect on a given time series caused by another time series. lead-lag\neffects are ubiquitous in practice and are specifically critical in formulating\ninvestment strategies in high-frequency trading. At present, there are three\nmajor challenges in analyzing the lead-lag effects. First, in practical\napplications, not all time series are observed synchronously. Second, the size\nof the relevant dataset and rate of change of the environment is increasingly\nfaster, and it is becoming more difficult to complete the computation within a\nparticular time limit. Third, some lead-lag effects are time-varying and only\nlast for a short period, and their delay lengths are often affected by external\nfactors. In this paper, we propose NAPLES (Negative And Positive lead-lag\nEStimator), a new statistical measure that resolves all these problems. Through\nexperiments on artificial and real datasets, we demonstrate that NAPLES has a\nstrong correlation with the actual lead-lag effects, including those triggered\nby significant macroeconomic announcements.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.00724v1"
    },
    {
        "title": "Predicting Bank Loan Default with Extreme Gradient Boosting",
        "authors": [
            "Rising Odegua"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Loan default prediction is one of the most important and critical problems\nfaced by banks and other financial institutions as it has a huge effect on\nprofit. Although many traditional methods exist for mining information about a\nloan application, most of these methods seem to be under-performing as there\nhave been reported increases in the number of bad loans. In this paper, we use\nan Extreme Gradient Boosting algorithm called XGBoost for loan default\nprediction. The prediction is based on a loan data from a leading bank taking\ninto consideration data sets from both the loan application and the demographic\nof the applicant. We also present important evaluation metrics such as\nAccuracy, Recall, precision, F1-Score and ROC area of the analysis. This paper\nprovides an effective basis for loan credit approval in order to identify risky\ncustomers from a large number of loan applications using predictive modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.02011v1"
    },
    {
        "title": "Forecasting Realized Volatility Matrix With Copula-Based Models",
        "authors": [
            "Wenjing Wang",
            "Minjing Tao"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Multivariate volatility modeling and forecasting are crucial in financial\neconomics. This paper develops a copula-based approach to model and forecast\nrealized volatility matrices. The proposed copula-based time series models can\ncapture the hidden dependence structure of realized volatility matrices. Also,\nthis approach can automatically guarantee the positive definiteness of the\nforecasts through either Cholesky decomposition or matrix logarithm\ntransformation. In this paper we consider both multivariate and bivariate\ncopulas; the types of copulas include Student's t, Clayton and Gumbel copulas.\nIn an empirical application, we find that for one-day ahead volatility matrix\nforecasting, these copula-based models can achieve significant performance both\nin terms of statistical precision as well as creating economically\nmean-variance efficient portfolio. Among the copulas we considered, the\nmultivariate-t copula performs better in statistical precision, while\nbivariate-t copula has better economical performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.08849v1"
    },
    {
        "title": "Parametric and nonparametric models and methods in financial\n  econometrics",
        "authors": [
            "Zhibiao Zhao"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Financial econometrics has become an increasingly popular research field. In\nthis paper we review a few parametric and nonparametric models and methods used\nin this area. After introducing several widely used continuous-time and\ndiscrete-time models, we study in detail dependence structures of discrete\nsamples, including Markovian property, hidden Markovian structure, contaminated\nobservations, and random samples. We then discuss several popular parametric\nand nonparametric estimation methods. To avoid model mis-specification, model\nvalidation plays a key role in financial modeling. We discuss several model\nvalidation techniques, including pseudo-likelihood ratio test, nonparametric\ncurve regression based test, residuals based test, generalized likelihood ratio\ntest, simultaneous confidence band construction, and density based test.\nFinally, we briefly touch on tools for studying large sample properties.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.1599v2"
    },
    {
        "title": "Multifractal analysis of Chinese stock volatilities based on partition\n  function approach",
        "authors": [
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We have performed detailed multifractal analysis on the minutely volatility\nof two indexes and 1139 stocks in the Chinese stock markets based on the\npartition function approach. The partition function $\\chi_q(s)$ scales as a\npower law with respect to box size $s$. The scaling exponents $\\tau(q)$ form a\nnonlinear function of $q$. Statistical tests based on bootstrapping show that\nthe extracted multifractal nature is significant at the 1% significance level.\nThe individual securities can be well modeled by the $p$-model in turbulence\nwith $p = 0.40 \\pm 0.02$. Based on the idea of ensemble averaging (including\nquenched and annealed average), we treat each stock exchange as a whole and\nconfirm the existence of multifractal nature in the Chinese stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.1710v2"
    },
    {
        "title": "Econometrics as Sorcery",
        "authors": [
            "G. Innocenti",
            "D. Materassi"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The paper deals with the problem of identifying the internal dependencies and\nsimilarities among a large number of random processes. Linear models are\nconsidered to describe the relations among the time series and the energy\nassociated to the corresponding modeling error is the criterion adopted to\nquantify their similarities. Such an approach is interpreted in terms of graph\ntheory suggesting a natural way to group processes together when one provides\nthe best model to explain the other. Moreover, the clustering technique\nintroduced in this paper will turn out to be the dynamical generalization of\nother multivariate procedures described in literature.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.3047v1"
    },
    {
        "title": "Direct evidence for inversion formula in multifractal financial\n  volatility measure",
        "authors": [
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The inversion formula for conservative multifractal measures was unveiled\nmathematically a decade ago, which is however not well tested in real complex\nsystems. In this Letter, we propose to verify the inversion formula using\nhigh-frequency turbulent financial data. We construct conservative volatility\nmeasure based on minutely S&P 500 index from 1982 to 1999 and its inverse\nmeasure of exit time. Both the direct and inverse measures exhibit nice\nmultifractal nature, whose scaling ranges are not irrelevant. Empirical\ninvestigation shows that the inversion formula holds in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.3494v1"
    },
    {
        "title": "Forecasting volatility with the multifractal random walk model",
        "authors": [
            "Jean Duchon",
            "Raoul Robert",
            "Vincent Vargas"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We study the problem of forecasting volatility for the multifractal random\nwalk model. In order to avoid the ill posed problem of estimating the\ncorrelation length T of the model, we introduce a limiting object defined in a\nquotient space; formally, this object is an infinite range logvolatility. For\nthis object and the non limiting object, we obtain precise prediction formulas\nand we apply them to the problem of forecasting volatility and pricing options\nwith the MRW model in the absence of a reliable estimate of the average\nvolatility and T.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.4220v1"
    },
    {
        "title": "Is it possible to predict long-term success with k-NN? Case Study of\n  four market indices (FTSE100, DAX, HANGSENG, NASDAQ)",
        "authors": [
            "Y. Shi",
            "A. N. Gorban",
            "T. Y. Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  This case study tests the possibility of prediction for \"success\" (or\n\"winner\") components of four stock & shares market indices in a time period of\nthree years from 02-Jul-2009 to 29-Jun-2012.We compare their performance ain\ntwo time frames: initial frame three months at the beginning\n(02/06/2009-30/09/2009) and the final three month frame\n(02/04/2012-29/06/2012). To label the components, average price ratio between\ntwo time frames in descending order is computed. The average price ratio is\ndefined as the ratio between the mean prices of the beginning and final time\nperiod. The \"winner\" components are referred to the top one third of total\ncomponents in the same order as average price ratio it means the mean price of\nfinal time period is relatively higher than the beginning time period. The\n\"loser\" components are referred to the last one third of total components in\nthe same order as they have higher mean prices of beginning time period. We\nanalyse, is there any information about the winner-looser separation in the\ninitial fragments of the daily closing prices log-returns time series. The\nLeave-One-Out Cross-Validation with k-NN algorithm is applied on the daily\nlog-return of components using a distance and proximity in the experiment. By\nlooking at the error analysis, it shows that for HANGSENG and DAX index, there\nare clear signs of possibility to evaluate the probability of long-term\nsuccess. The correlation distance matrix histograms and 2-D/3-D elastic maps\ngenerated from ViDaExpert show that the winner components are closer to each\nother and winner/loser components are separable on elastic maps for HANGSENG\nand DAX index while for the negative possibility indices, there is no sign of\nseparation.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.8308v1"
    },
    {
        "title": "Stochastic Evolution of Stock Market Volume-Price Distributions",
        "authors": [
            "Paulo Rocha",
            "Frank Raischel",
            "João P. da Cruz",
            "Pedro G. Lind"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Using available data from the New York stock market (NYSM) we test four\ndifferent bi-parametric models to fit the correspondent volume-price\ndistributions at each $10$-minute lag: the Gamma distribution, the inverse\nGamma distribution, the Weibull distribution and the log-normal distribution.\nThe volume-price data, which measures market capitalization, appears to follow\na specific statistical pattern, other than the evolution of prices measured in\nsimilar studies. We find that the inverse Gamma model gives a superior fit to\nthe volume-price evolution than the other models. We then focus on the inverse\nGamma distribution as a model for the NYSM data and analyze the evolution of\nthe pair of distribution parameters as a stochastic process. Assuming that the\nevolution of these parameters is governed by coupled Langevin equations, we\nderive the corresponding drift and diffusion coefficients, which then provide\ninsight for understanding the mechanisms underlying the evolution of the stock\nmarket.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.1730v2"
    },
    {
        "title": "Bayesian DEJD model and detection of asymmetric jumps",
        "authors": [
            "Maciej Kostrzewski"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  News might trigger jump arrivals in financial time series. The \"bad\" and\n\"good\" news seems to have distinct impact. In the research, a double\nexponential jump distribution is applied to model downward and upward jumps.\nBayesian double exponential jump-diffusion model is proposed. Theorems stated\nin the paper enable estimation of the model's parameters, detection of jumps\nand analysis of jump frequency. The methodology, founded upon the idea of\nlatent variables, is illustrated with two empirical studies, employing both\nsimulated and real-world data (the KGHM index). News might trigger jump\narrivals in financial time series. The \"bad\" and \"good\" news seems to have\ndistinct impact. In the research, a double exponential jump distribution is\napplied to model downward and upward jumps. Bayesian double exponential\njump-diffusion model is proposed. Theorems stated in the paper enable\nestimation of the model's parameters, detection of jumps and analysis of jump\nfrequency. The methodology, founded upon the idea of latent variables, is\nillustrated with two empirical studies, employing both simulated and real-world\ndata (the KGHM index).\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2050v1"
    },
    {
        "title": "A multivariate model for financial indices and an algorithm for\n  detection of jumps in the volatility",
        "authors": [
            "Mario Bonino",
            "Matteo Camelia",
            "Paolo Pigato"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We consider a mean-reverting stochastic volatility model which satisfies some\nrelevant stylized facts of financial markets. We introduce an algorithm for the\ndetection of peaks in the volatility profile, that we apply to the time series\nof Dow Jones Industrial Average and Financial Times Stock Exchange 100 in the\nperiod 1984-2013. Based on empirical results, we propose a bivariate version of\nthe model, for which we find an explicit expression for the decay over time of\ncross-asset correlations between absolute returns. We compare our theoretical\npredictions with empirical estimates on the same financial time series, finding\nan excellent agreement.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7632v2"
    },
    {
        "title": "Transition from lognormal to chi-square superstatistics for financial\n  time series",
        "authors": [
            "Dan Xu",
            "Christian Beck"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Share price returns on different time scales can be well modelled by a\nsuperstatistical dynamics. Here we provide an investigation which type of\nsuperstatistics is most suitable to properly describe share price dynamics on\nvarious time scales. It is shown that while chi-square superstatistics works\nwell on a time scale of days, on a much smaller time scale of minutes the price\nchanges are better described by lognormal superstatistics. The system dynamics\nthus exhibits a transition from lognormal to chi-square superstatistics as a\nfunction of time scale. We discuss a more general model interpolating between\nboth statistics which fits the observed data very well. We also present results\non correlation functions of the extracted superstatistical volatility\nparameter, which exhibits exponential decay for returns on large time scales,\nwhereas for returns on small time scales there are long-range correlations and\npower-law decay.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01660v2"
    },
    {
        "title": "Autoregressive approaches to import--export time series II: a concrete\n  case study",
        "authors": [
            "Luca Di Persio",
            "Chiara Segala"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The present work constitutes the second part of a two-paper project that, in\nparticular, deals with an in-depth study of effective techniques used in\neconometrics in order to make accurate forecasts in the concrete framework of\none of the major economies of the most productive Italian area, namely the\nprovince of Verona. It is worth mentioning that this region is indubitably\nrecognized as the core of the commercial engine of the whole Italian country.\nThis is why our analysis has a concrete impact; it is based on real data, and\nthis is also the reason why particular attention has been taken in treating the\nrelevant economical data and in choosing the right methods to manage them to\nobtain good forecasts. In particular, we develop an approach mainly based on\nvector autoregression where lagged values of two or more variables are\nconsidered, Granger causality, and the stochastic trend approach useful to work\nwith the cointegration phenomenon.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01984v1"
    },
    {
        "title": "Multifractal characterization of gold market: a multifractal detrended\n  fluctuation analysis",
        "authors": [
            "Provash Mali",
            "Amitabha Mukhopadhyay"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The multifractal detrended fluctuation analysis technique is employed to\nanalyze the time series of gold consumer price index (CPI) and the market trend\nof three world's highest gold consuming countries, namely China, India and\nTurkey for the period: 1993-July 2013. Various multifractal variables, such as\nthe generalized Hurst exponent, the multifractal exponent and the singularity\nspectrum, are calculated and the results are fitted to the generalized binomial\nmultifractal (GBM) series that consists of only two parameters. Special\nemphasis is given to identify the possible source(s) of multifractality in\nthese series. Our analysis shows that the CPI series and all three market\nseries are of multifractal nature. The origin of multifractality for the CPI\ntime series and Indian market series is found due to a long-range time\ncorrelation, whereas it is mostly due to the fat-tailed probability\ndistributions of the values for the Chinese and Turkey markets. The GBM model\nseries more or less describes all the time series analyzed here.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.08847v1"
    },
    {
        "title": "Efficient asymptotic variance reduction when estimating volatility in\n  high frequency data",
        "authors": [
            "Simon Clinet",
            "Yoann Potiron"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  This paper shows how to carry out efficient asymptotic variance reduction\nwhen estimating volatility in the presence of stochastic volatility and\nmicrostructure noise with the realized kernels (RK) from [Barndorff-Nielsen et\nal., 2008] and the quasi-maximum likelihood estimator (QMLE) studied in [Xiu,\n2010]. To obtain such a reduction, we chop the data into B blocks, compute the\nRK (or QMLE) on each block, and aggregate the block estimates. The ratio of\nasymptotic variance over the bound of asymptotic efficiency converges as B\nincreases to the ratio in the parametric version of the problem, i.e. 1.0025 in\nthe case of the fastest RK Tukey-Hanning 16 and 1 for the QMLE. The impact of\nstochastic sampling times and jump in the price process is examined carefully.\nThe finite sample performance of both estimators is investigated in\nsimulations, while empirical work illustrates the gain in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.01185v3"
    },
    {
        "title": "Burst and inter-burst duration statistics as empirical test of\n  long-range memory in the financial markets",
        "authors": [
            "V. Gontis",
            "A. Kononovicius"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We address the problem of long-range memory in the financial markets. There\nare two conceptually different ways to reproduce power-law decay of\nauto-correlation function: using fractional Brownian motion as well as\nnon-linear stochastic differential equations. In this contribution we address\nthis problem by analyzing empirical return and trading activity time series\nfrom the Forex. From the empirical time series we obtain probability density\nfunctions of burst and inter-burst duration. Our analysis reveals that the\npower-law exponents of the obtained probability density functions are close to\n$3/2$, which is a characteristic feature of the one-dimensional stochastic\nprocesses. This is in a good agreement with earlier proposed model of absolute\nreturn based on the non-linear stochastic differential equations derived from\nthe agent-based herding model.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.01255v1"
    },
    {
        "title": "Time series momentum and contrarian effects in the Chinese stock market",
        "authors": [
            "Huai-Long Shi",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  This paper concentrates on the time series momentum or contrarian effects in\nthe Chinese stock market. We evaluate the performance of the time series\nmomentum strategy applied to major stock indices in mainland China and explore\nthe relation between the performance of time series momentum strategies and\nsome firm-specific characteristics. Our findings indicate that there is a time\nseries momentum effect in the short run and a contrarian effect in the long run\nin the Chinese stock market. The performances of the time series momentum and\ncontrarian strategies are highly dependent on the look-back and holding periods\nand firm-specific characteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.07374v1"
    },
    {
        "title": "Correlations and Clustering in Wholesale Electricity Markets",
        "authors": [
            "Tianyu Cui",
            "Francesco Caravelli",
            "Cozmin Ududec"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We study the structure of locational marginal prices in day-ahead and\nreal-time wholesale electricity markets. In particular, we consider the case of\ntwo North American markets and show that the price correlations contain\ninformation on the locational structure of the grid. We study various\nclustering methods and introduce a type of correlation function based on event\nsynchronization for spiky time series, and another based on string correlations\nof location names provided by the markets. This allows us to reconstruct\naspects of the locational structure of the grid.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.11184v3"
    },
    {
        "title": "Selecting stock pairs for pairs trading while incorporating lead-lag\n  relationship",
        "authors": [
            "Kartikay Gupta",
            "Niladri Chatterjee"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Pairs Trading is carried out in the financial market to earn huge profits\nfrom known equilibrium relation between pairs of stock. In financial markets,\nseldom it is seen that stock pairs are correlated at particular lead or lag.\nThis lead-lag relationship has been empirically studied in various financial\nmarkets. Earlier research works have suggested various measures for identifying\nthe best pairs for pairs trading, but they do not consider this lead-lag\neffect. The present study proposes a new distance measure which incorporates\nthe lead-lag relationship between the stocks while selecting the best pairs for\npairs trading. Further, the lead-lag value between the stocks is allowed to\nvary continuously over time. The proposed measures importance has been\nshow-cased through experimentation on two different datasets, one corresponding\nto Indian companies and another corresponding to American companies. When the\nproposed measure is clubbed with SSD measure, i.e., when pairs are identified\nthrough optimising both these measures, then the selected pairs consistently\ngenerate the best profit, as compared to all other measures. Finally, possible\ngeneralisation and extension of the proposed distance measure have been\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05057v2"
    },
    {
        "title": "Neural Network Models for Stock Selection Based on Fundamental Analysis",
        "authors": [
            "Yuxuan Huang",
            "Luiz Fernando Capretz",
            "Danny Ho"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Application of neural network architectures for financial prediction has been\nactively studied in recent years. This paper presents a comparative study that\ninvestigates and compares feed-forward neural network (FNN) and adaptive neural\nfuzzy inference system (ANFIS) on stock prediction using fundamental financial\nratios. The study is designed to evaluate the performance of each architecture\nbased on the relative return of the selected portfolios with respect to the\nbenchmark stock index. The results show that both architectures possess the\nability to separate winners and losers from a sample universe of stocks, and\nthe selected portfolios outperform the benchmark. Our study argues that FNN\nshows superior performance over ANFIS.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05327v1"
    },
    {
        "title": "From asymptotic properties of general point processes to the ranking of\n  financial agents",
        "authors": [
            "Othmane Mounjid",
            "Mathieu Rosenbaum",
            "Pamela Saliba"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We propose a general non-linear order book model that is built from the\nindividual behaviours of the agents. Our framework encompasses Markovian and\nHawkes based models. Under mild assumptions, we prove original results on the\nergodicity and diffusivity of such system. Then we provide closed form formulas\nfor various quantities of interest: stationary distribution of the best bid and\nask quantities, spread, liquidity fluctuations and price volatility. These\nformulas are expressed in terms of individual order flows of market\nparticipants. Our approach enables us to establish a ranking methodology for\nthe market makers with respect to the quality of their trading.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05420v1"
    },
    {
        "title": "Time scales in stock markets",
        "authors": [
            "Ajit Mahata",
            "Md Nurujjaman"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Different investment strategies are adopted in short-term and long-term\ndepending on the time scales, even though time scales are adhoc in nature.\nEmpirical mode decomposition based Hurst exponent analysis and variance\ntechnique have been applied to identify the time scales for short-term and\nlong-term investment from the decomposed intrinsic mode functions(IMF). Hurst\nexponent ($H$) is around 0.5 for the IMFs with time scales from few days to 3\nmonths, and $H\\geq0.75$ for the IMFs with the time scales $\\geq5$ months. Short\nterm time series [$X_{ST}(t)$] with time scales from few days to 3 months and\n$H~0.5$ and long term time series [$X_{LT}(t)$] with time scales $\\geq5$ and\n$H\\geq0.75$, which represent the dynamics of the market, are constructed from\nthe IMFs. The $X_{ST}(t)$ and $X_{LT}(t)$ show that the market is random in\nshort-term and correlated in long term. The study also show that the\n$X_{LT}(t)$ is correlated with fundamentals of the company. The analysis will\nbe useful for investors to design the investment and trading strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05494v1"
    },
    {
        "title": "Signatures of crypto-currency market decoupling from the Forex",
        "authors": [
            "Stanisław Drożdż",
            "Ludovico Minati",
            "Paweł Oświęcimka",
            "Marek Stanuszek",
            "Marcin Wątorek"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Based on the high-frequency recordings from Kraken, a cryptocurrency exchange\nand professional trading platform that aims to bring Bitcoin and other\ncryptocurrencies into the mainstream, the multiscale cross-correlations\ninvolving the Bitcoin (BTC), Ethereum (ETH), Euro (EUR) and US dollar (USD) are\nstudied over the period between July 1, 2016 and December 31, 2018. It is shown\nthat the multiscaling characteristics of the exchange rate fluctuations related\nto the cryptocurrency market approach those of the Forex. This, in particular,\napplies to the BTC/ETH exchange rate, whose Hurst exponent by the end of 2018\nstarted approaching the value of 0.5, which is characteristic of the mature\nworld markets. Furthermore, the BTC/ETH direct exchange rate has already\ndeveloped multifractality, which manifests itself via broad singularity\nspectra. A particularly significant result is that the measures applied for\ndetecting cross-correlations between the dynamics of the BTC/ETH and EUR/USD\nexchange rates do not show any noticeable relationships. This may be taken as\nan indication that the cryptocurrency market has begun decoupling itself from\nthe Forex.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07834v2"
    },
    {
        "title": "Investment Ranking Challenge: Identifying the best performing stocks\n  based on their semi-annual returns",
        "authors": [
            "Shanka Subhra Mondal",
            "Sharada Prasanna Mohanty",
            "Benjamin Harlander",
            "Mehmet Koseoglu",
            "Lance Rane",
            "Kirill Romanov",
            "Wei-Kai Liu",
            "Pranoot Hatwar",
            "Marcel Salathe",
            "Joe Byrum"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In the IEEE Investment ranking challenge 2018, participants were asked to\nbuild a model which would identify the best performing stocks based on their\nreturns over a forward six months window. Anonymized financial predictors and\nsemi-annual returns were provided for a group of anonymized stocks from 1996 to\n2017, which were divided into 42 non-overlapping six months period. The second\nhalf of 2017 was used as an out-of-sample test of the model's performance.\nMetrics used were Spearman's Rank Correlation Coefficient and Normalized\nDiscounted Cumulative Gain (NDCG) of the top 20% of a model's predicted\nrankings. The top six participants were invited to describe their approach. The\nsolutions used were varied and were based on selecting a subset of data to\ntrain, combination of deep and shallow neural networks, different boosting\nalgorithms, different models with different sets of features, linear support\nvector machine, combination of convoltional neural network (CNN) and Long short\nterm memory (LSTM).\n",
        "pdf_link": "http://arxiv.org/pdf/1906.08636v1"
    },
    {
        "title": "Capturing dynamics of post-earnings-announcement drift using genetic\n  algorithm-optimised supervised learning",
        "authors": [
            "Zhengxin Joseph Ye",
            "Bjorn W. Schuller"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  While Post-Earnings-Announcement Drift (PEAD) is one of the most studied\nstock market anomalies, the current literature is often limited in explaining\nthis phenomenon by a small number of factors using simpler regression methods.\nIn this paper, we use a machine learning based approach instead, and aim to\ncapture the PEAD dynamics using data from a large group of stocks and a wide\nrange of both fundamental and technical factors. Our model is built around the\nExtreme Gradient Boosting (XGBoost) and uses a long list of engineered input\nfeatures based on quarterly financial announcement data from 1,106 companies in\nthe Russell 1000 index between 1997 and 2018. We perform numerous experiments\non PEAD predictions and analysis and have the following contributions to the\nliterature. First, we show how Post-Earnings-Announcement Drift can be analysed\nusing machine learning methods and demonstrate such methods' prowess in\nproducing credible forecasting on the drift direction. It is the first time\nPEAD dynamics are studied using XGBoost. We show that the drift direction is in\nfact driven by different factors for stocks from different industrial sectors\nand in different quarters and XGBoost is effective in understanding the\nchanging drivers. Second, we show that an XGBoost well optimised by a Genetic\nAlgorithm can help allocate out-of-sample stocks to form portfolios with higher\npositive returns to long and portfolios with lower negative returns to short, a\nfinding that could be adopted in the process of developing market neutral\nstrategies. Third, we show how theoretical event-driven stock strategies have\nto grapple with ever changing market prices in reality, reducing their\neffectiveness. We present a tactic to remedy the difficulty of buying into a\nmoving market when dealing with PEAD signals.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.03094v1"
    },
    {
        "title": "Machine Learning for Temporal Data in Finance: Challenges and\n  Opportunities",
        "authors": [
            "Jason Wittenbach",
            "Brian d'Alessandro",
            "C. Bayan Bruss"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Temporal data are ubiquitous in the financial services (FS) industry --\ntraditional data like economic indicators, operational data such as bank\naccount transactions, and modern data sources like website clickstreams -- all\nof these occur as a time-indexed sequence. But machine learning efforts in FS\noften fail to account for the temporal richness of these data, even in cases\nwhere domain knowledge suggests that the precise temporal patterns between\nevents should contain valuable information. At best, such data are often\ntreated as uniform time series, where there is a sequence but no sense of exact\ntiming. At worst, rough aggregate features are computed over a pre-selected\nwindow so that static sample-based approaches can be applied (e.g. number of\nopen lines of credit in the previous year or maximum credit utilization over\nthe previous month). Such approaches are at odds with the deep learning\nparadigm which advocates for building models that act directly on raw or\nlightly processed data and for leveraging modern optimization techniques to\ndiscover optimal feature transformations en route to solving the modeling task\nat hand. Furthermore, a full picture of the entity being modeled (customer,\ncompany, etc.) might only be attainable by examining multiple data streams that\nunfold across potentially vastly different time scales. In this paper, we\nexamine the different types of temporal data found in common FS use cases,\nreview the current machine learning approaches in this area, and finally assess\nchallenges and opportunities for researchers working at the intersection of\nmachine learning for temporal data and applications in FS.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.05636v1"
    },
    {
        "title": "Stock Price Prediction Using Machine Learning and LSTM-Based Deep\n  Learning Models",
        "authors": [
            "Sidra Mehtab",
            "Jaydip Sen",
            "Abhishek Dutta"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Prediction of stock prices has been an important area of research for a long\ntime. While supporters of the efficient market hypothesis believe that it is\nimpossible to predict stock prices accurately, there are formal propositions\ndemonstrating that accurate modeling and designing of appropriate variables may\nlead to models using which stock prices and stock price movement patterns can\nbe very accurately predicted. In this work, we propose an approach of hybrid\nmodeling for stock price prediction building different machine learning and\ndeep learning-based models. For the purpose of our study, we have used NIFTY 50\nindex values of the National Stock Exchange (NSE) of India, during the period\nDecember 29, 2014 till July 31, 2020. We have built eight regression models\nusing the training data that consisted of NIFTY 50 index records during\nDecember 29, 2014 till December 28, 2018. Using these regression models, we\npredicted the open values of NIFTY 50 for the period December 31, 2018 till\nJuly 31, 2020. We, then, augment the predictive power of our forecasting\nframework by building four deep learning-based regression models using long-and\nshort-term memory (LSTM) networks with a novel approach of walk-forward\nvalidation. We exploit the power of LSTM regression models in forecasting the\nfuture NIFTY 50 open values using four different models that differ in their\narchitecture and in the structure of their input data. Extensive results are\npresented on various metrics for the all the regression models. The results\nclearly indicate that the LSTM-based univariate model that uses one-week prior\ndata as input for predicting the next week open value of the NIFTY 50 time\nseries is the most accurate model.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.10819v1"
    },
    {
        "title": "Network geometry and market instability",
        "authors": [
            "Areejit Samal",
            "Hirdesh K. Pharasi",
            "Sarath Jyotsna Ramaia",
            "Harish Kannan",
            "Emil Saucan",
            "Jürgen Jost",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The complexity of financial markets arise from the strategic interactions\namong agents trading stocks, which manifest in the form of vibrant correlation\npatterns among stock prices. Over the past few decades, complex financial\nmarkets have often been represented as networks whose interacting pairs of\nnodes are stocks, connected by edges that signify the correlation strengths.\nHowever, we often have interactions that occur in groups of three or more\nnodes, and these cannot be described simply by pairwise interactions but we\nalso need to take the relations between these interactions into account. Only\nrecently, researchers have started devoting attention to the higher-order\narchitecture of complex financial systems, that can significantly enhance our\nability to estimate systemic risk as well as measure the robustness of\nfinancial systems in terms of market efficiency. Geometry-inspired network\nmeasures, such as the Ollivier-Ricci curvature and Forman-Ricci curvature, can\nbe used to capture the network fragility and continuously monitor financial\ndynamics. Here, we explore the utility of such discrete Ricci curvatures in\ncharacterizing the structure of financial systems, and further, evaluate them\nas generic indicators of the market instability. For this purpose, we examine\nthe daily returns from a set of stocks comprising the USA S&P-500 and the\nJapanese Nikkei-225 over a 32-year period, and monitor the changes in the\nedge-centric network curvatures. We find that the different geometric measures\ncapture well the system-level features of the market and hence we can\ndistinguish between the normal or `business-as-usual' periods and all the major\nmarket crashes. This can be very useful in strategic designing of financial\nsystems and regulating the markets in order to tackle financial instabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.12335v3"
    },
    {
        "title": "Modeling and analysis of the effect of COVID-19 on the stock price: V\n  and L-shape recovery",
        "authors": [
            "Ajit Mahata",
            "Anish rai",
            "Om Prakash",
            "Md Nurujjaman"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The emergence of the COVID-19 pandemic, a new and novel risk factor, leads to\nthe stock price crash due to the investors' rapid and synchronous sell-off.\nHowever, within a short period, the quality sectors start recovering from the\nbottom. A stock price model has been developed during such crises based on the\nnet-fund-flow ($\\Psi_t$) due to institutional investors, and financial\nantifragility ($\\phi$) of a company. We assume that during the crash, the stock\nprice fall is independent of the $\\phi$. We study the effects of shock lengths\nand $\\phi$ on the stock price during the crises period using the $\\Psi_t$\nobtained from synthetic and real fund flow data. We observed that the\npossibility of recovery of stock with $\\phi>0$, termed as quality stock,\ndecreases with an increase in shock-length beyond a specific period. A quality\nstock with higher $\\phi$ shows V-shape recovery and outperform others. The\nshock length and recovery period of quality stock are almost equal that is seen\nin the Indian market. Financially stressed stocks, i.e., the stocks with\n$\\phi<0$, show L-shape recovery during the pandemic. The stock data and model\nanalysis shows that the investors, in uncertainty like COVID-19, invest in\nquality stocks to restructure their portfolio to reduce the risk. The study may\nhelp the investors to make the right investment decision during a crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13076v3"
    },
    {
        "title": "Uncovering the network structure of the world currency market:\n  Cross-correlations in the fluctuations of daily exchange rates",
        "authors": [
            "Sitabhra Sinha",
            "Uday Kovur"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  The cross-correlations between the exchange rate fluctuations of 74\ncurrencies over the period 1995-2012 are analyzed in this paper. The eigenvalue\ndistribution of the cross-correlation matrix exhibits a bulk which\napproximately matches the bounds predicted from random matrices constructed\nusing mutually uncorrelated time-series. However, a few large eigenvalues\ndeviating from the bulk contain important information about the global market\nmode as well as important clusters of strongly interacting currencies.We\nreconstruct the network structure of the world currency market by using two\ndifferent graph representation techniques, after filtering out the effects of\nglobal or market-wide signals on the one hand and random effects on the other.\nThe two networks reveal complementary insights about the major motive forces of\nthe global economy, including the identification of a group of potentially fast\ngrowing economies whose development trajectory may affect the global economy in\nthe future as profoundly as the rise of India and China has affected it in the\npast decades.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0239v1"
    },
    {
        "title": "Multivariate high-frequency financial data via semi-Markov processes",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this paper we propose a bivariate generalization of a weighted indexed\nsemi-Markov chains to study the high frequency price dynamics of traded stocks.\nWe assume that financial returns are described by a weighted indexed\nsemi-Markov chain model. We show, through Monte Carlo simulations, that the\nmodel is able to reproduce important stylized facts of financial time series\nlike the persistence of volatility and at the same time it can reproduce the\ncorrelation between stocks. The model is applied to data from Italian stock\nmarket from 1 January 2007 until the end of December 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.0436v1"
    },
    {
        "title": "Immediate price impact of a stock and its warrant: Power-law or\n  logarithmic model?",
        "authors": [
            "Hai-Chuan Xu",
            "Zhi-Qiang Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Based on the order flow data of a stock and its warrant, the immediate price\nimpacts of market orders are estimated by two competitive models, the power-law\nmodel (PL model) and the logarithmic model (LG model). We find that the PL\nmodel is overwhelmingly superior to the LG model, regarding the robustness of\nthe estimated parameters and the accuracy of out-of-sample forecasting. We also\nfind that the price impacts of ask and bid orders are consistent with each\nother for filled trades, since significant positive correlations are observed\nbetween the model parameters of both types of orders. Our findings may provide\nvaluable insights for optimal trade execution.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.04091v1"
    },
    {
        "title": "Random matrix approach to estimation of high-dimensional factor models",
        "authors": [
            "Joongyeub Yeo",
            "George Papanicolaou"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  In dealing with high-dimensional data sets, factor models are often useful\nfor dimension reduction. The estimation of factor models has been actively\nstudied in various fields. In the first part of this paper, we present a new\napproach to estimate high-dimensional factor models, using the empirical\nspectral density of residuals. The spectrum of covariance matrices from\nfinancial data typically exhibits two characteristic aspects: a few spikes and\nbulk. The former represent factors that mainly drive the features and the\nlatter arises from idiosyncratic noise. Motivated by these two aspects, we\nconsider a minimum distance between two spectrums; one from a covariance\nstructure model and the other from real residuals of financial data that are\nobtained by subtracting principal components. Our method simultaneously\nprovides estimators of the number of factors and information about correlation\nstructures in residuals. Using free random variable techniques, the proposed\nalgorithm can be implemented and controlled effectively. Monte Carlo\nsimulations confirm that our method is robust to noise or the presence of weak\nfactors. Furthermore, the application to financial time-series shows that our\nestimators capture essential aspects of market dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.05571v2"
    },
    {
        "title": "A diagnostic criterion for approximate factor structure",
        "authors": [
            "Patrick Gagliardini",
            "Elisa Ossola",
            "Olivier Scaillet"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We build a simple diagnostic criterion for approximate factor structure in\nlarge cross-sectional equity datasets. Given a model for asset returns with\nobservable factors, the criterion checks whether the error terms are weakly\ncross-sectionally correlated or share at least one unobservable common factor.\nIt only requires computing the largest eigenvalue of the empirical\ncross-sectional covariance matrix of the residuals of a large unbalanced panel.\nA general version of this criterion allows us to determine the number of\nomitted common factors. The panel data model accommodates both time-invariant\nand time-varying factor structures. The theory applies to random coefficient\npanel models with interactive fixed effects under large cross-section and\ntime-series dimensions. The empirical analysis runs on monthly and quarterly\nreturns for about ten thousand US stocks from January 1968 to December 2011 for\nseveral time-invariant and time-varying specifications. For monthly returns, we\ncan choose either among time-invariant specifications with at least four\nfinancial factors, or a scaled three-factor specification. For quarterly\nreturns, we cannot select macroeconomic models without the market factor.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.04990v2"
    },
    {
        "title": "Stylized Facts and Simulating Long Range Financial Data",
        "authors": [
            "Laurie Davies",
            "Walter Krämer"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We propose a new method (implemented in an R-program) to simulate long-range\ndaily stock-price data. The program reproduces various stylized facts much\nbetter than various parametric models from the extended GARCH-family. In\nparticular, the empirically observed changes in unconditional variance are\ntruthfully mirrored in the simulated data.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.05229v1"
    },
    {
        "title": "The Stock Market Has Grown Unstable Since February 2018",
        "authors": [
            "Blake C. Stacey",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  On the fifth of February, 2018, the Dow Jones Industrial Average dropped\n1,175.21 points, the largest single-day fall in history in raw point terms.\nThis followed a 666-point loss on the second, and another drop of over a\nthousand points occurred three days later. It is natural to ask whether these\nevents indicate a transition to a new regime of market behavior, particularly\ngiven the dramatic fluctuations --- both gains and losses --- in the weeks\nsince. To illuminate this matter, we can apply a model grounded in the science\nof complex systems, a model that demonstrated considerable success at\nunraveling the stock-market dynamics from the 1980s through the 2000s. By using\nlarge-scale comovement of stock prices as an early indicator of unhealthy\nmarket dynamics, this work found that abrupt drops in a certain parameter $U$\nprovide an early warning of single-day panics and economic crises. Decreases in\n$U$ indicate regimes of \"high co-movement\", a market behavior that is not the\nsame as volatility, though market volatility can be a component of co-movement.\nApplying the same analysis to stock-price data from the beginning of 2016 until\nnow, we find that the $U$ value for the period since 5 February is\nsignificantly lower than for the period before. This decrease entered the\n\"danger zone\" in the last week of May, 2018.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.00529v1"
    },
    {
        "title": "Comparing Alternatives to Measure the Impact of DDoS Attack\n  Announcements on Target Stock Prices",
        "authors": [
            " Abhishta",
            "Reinoud Joosten",
            "Lambert J. M. Nieuwenhuis"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  The attack intensity of distributed denial of service (DDoS) attacks is\nincreasing every year. Botnets based on internet of things (IOT) devices are\nnow being used to conduct DDoS attacks. The estimation of direct and indirect\neconomic damages caused by these attacks is a complex problem. One of the\nindirect damage of a DDoS attack can be on the market value of the victim firm.\nIn this article we analyze the impact of 45 different DDoS attack announcements\non victim's stock prices. We find that previous studies have a mixed conclusion\non the impact of DDoS attack announcements on the victim's stock price. Hence,\nin this article we evaluate this impact using three different approaches and\ncompare the results. In the first approach, we use the assume the cumulative\nabnormal returns to be normally distributed and test the hypothesis that a DDoS\nattack announcement has no impact on the victim's stock price. In the latter\ntwo methods, we do not assume a distribution and use the empirical distribution\nof cumulative abnormal returns to test the hypothesis. We find that the\nassumption of cumulative abnormal returns being normally distributed leads to\noverestimation/underestimation of the impact. Finally, we analyze the impact of\nDDoS attack announcement on victim's stock price in each of the 45 cases and\npresent our results.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01781v1"
    },
    {
        "title": "Asymmetric response to PMI announcements in China's stock returns",
        "authors": [
            "Yingli Wang",
            "Xiaoguang Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Considered an important macroeconomic indicator, the Purchasing Managers'\nIndex (PMI) on Manufacturing generally assumes that PMI announcements will\nproduce an impact on stock markets. International experience suggests that\nstock markets react to negative PMI news. In this research, we empirically\ninvestigate the stock market reaction towards PMI in China. The asymmetric\neffects of PMI announcements on the stock market are observed: no market\nreaction is generated towards negative PMI announcements, while a positive\nreaction is generally generated for positive PMI news. We further find that the\npositive reaction towards the positive PMI news occurs 1 day before the\nannouncement and lasts for nearly 3 days, and the positive reaction is observed\nin the context of expanding economic conditions. By contrast, the negative\nreaction towards negative PMI news is prevalent during downward economic\nconditions for stocks with low market value, low institutional shareholding\nratios or high price earnings. Our study implies that China's stock market\nfavors risk to a certain extent given the vast number of individual investors\nin the country, and there may exist information leakage in the market.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.04347v1"
    },
    {
        "title": "Martingales and Super-martingales Relative to a Convex Set of Equivalent\n  Measures",
        "authors": [
            "Nicholas S. Gonchar"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In the paper, the martingales and super-martingales relative to a convex set\nof equivalent measures are systematically studied. The notion of local regular\nsuper-martingale relative to a convex set of equivalent measures is introduced\nand the necessary and sufficient conditions of the local regularity of it in\nthe discrete case are founded. The description of all local regular\nsuper-martingales relative to a convex set of equivalent measures is presented.\nThe notion of the complete set of equivalent measures is introduced. We prove\nthat every bounded in some sense super-martingale relative to the complete set\nof equivalent measures is local regular. A new definition of the fair price of\ncontingent claim in an incomplete market is given and the formula for the fair\nprice of Standard Option of European type is found. The proved Theorems are the\ngeneralization of the famous Doob decomposition for super-martingale onto the\ncase of super-martingales relative to a convex set of equivalent measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.05557v1"
    },
    {
        "title": "Data on the annual aggregated income taxes of the Italian municipalities\n  over the quinquennium 2007-2011",
        "authors": [
            "Marcel Ausloos",
            "Roy Cerqueti",
            "Tariq A. Mir"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  This dataset contains the annual aggregated income taxes of all the Italian\nmunicipalities over the years 2007-2011. Data are clustered over the Italian\nregions and provinces. The source of the data is the Italian Ministry of\nEconomics and Finance. The administrative variations in Italy over the\nquinquennium have been taken into account. Data are useful to understand the\neconomic structure of Italy at the microscopic level of municipalities. They\ncan serve also for making comparisons between economical aspects and other\nfeatures of the Italian cities.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.10935v1"
    },
    {
        "title": "Approximation of the first passage time distribution for the birth-death\n  processes",
        "authors": [
            "Aleksejus Kononovicius",
            "Vygintas Gontis"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We propose a general method to obtain approximation of the first passage time\ndistribution for the birth-death processes. We rely on the general properties\nof birth-death processes, Keilson's theorem and the concept of Riemann sum to\nobtain closed-form expressions. We apply the method to the three selected\nbirth-death processes and the sophisticated order-book model exhibiting\nlong-range memory. We discuss how our approach contributes to the competition\nbetween spurious and true long-range memory models.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.00924v1"
    },
    {
        "title": "A changepoint approach for the identification of financial extreme\n  regimes",
        "authors": [
            "Chiara Lattanzi",
            "Manuele Leonelli"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Inference over tails is usually performed by fitting an appropriate limiting\ndistribution over observations that exceed a fixed threshold. However, the\nchoice of such threshold is critical and can affect the inferential results.\nExtreme value mixture models have been defined to estimate the threshold using\nthe full dataset and to give accurate tail estimates. Such models assume that\nthe tail behavior is constant for all observations. However, the extreme\nbehavior of financial returns often changes considerably in time and such\nchanges occur by sudden shocks of the market. Here we extend the extreme value\nmixture model class to formally take into account distributional extreme\nchangepoints, by allowing for the presence of regime-dependent parameters\nmodelling the tail of the distribution. This extension formally uses the full\ndataset to both estimate the thresholds and the extreme changepoint locations,\ngiving uncertainty measures for both quantities. Estimation of functions of\ninterest in extreme value analyses is performed via MCMC algorithms. Our\napproach is evaluated through a series of simulations, applied to real data\nsets and assessed against competing approaches. Evidence demonstrates that the\ninclusion of different extreme regimes outperforms both static and dynamic\ncompeting approaches in financial applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.09205v1"
    },
    {
        "title": "Market efficiency, liquidity, and multifractality of Bitcoin: A dynamic\n  study",
        "authors": [
            "Tetsuya Takaishi",
            "Takanori Adachi"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This letter investigates the dynamic relationship between market efficiency,\nliquidity, and multifractality of Bitcoin. We find that before 2013 liquidity\nis low and the Hurst exponent is less than 0.5, indicating that the Bitcoin\ntime series is anti-persistent. After 2013, as liquidity increased, the Hurst\nexponent rose to approximately 0.5, improving market efficiency. For several\nperiods, however, the Hurst exponent was found to be significantly less than\n0.5, making the time series anti-persistent during those periods. We also\ninvestigate the multifractal degree of the Bitcoin time series using the\ngeneralized Hurst exponent and find that the multifractal degree is related to\nmarket efficiency in a non-linear manner.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.09253v1"
    },
    {
        "title": "Fractal Time Series Analysis of Social Network Activities",
        "authors": [
            "Lyudmyla Kirichenko",
            "Vitalii Bulakh",
            "Tamara Radivilova"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In the work, a comparative correlation and fractal analysis of time series of\nBitcoin crypto currency rate and community activities in social networks\nassociated with Bitcoin was conducted. A significant correlation between the\nBitcoin rate and the community activities was detected. Time series fractal\nanalysis indicated the presence of self-similar and multifractal properties.\nThe results of researches showed that the series having a strong correlation\ndependence have a similar multifractal structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.01018v1"
    },
    {
        "title": "Sustainable Investing and the Cross-Section of Returns and Maximum\n  Drawdown",
        "authors": [
            "Lisa R. Goldberg",
            "Saad Mouti"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We use supervised learning to identify factors that predict the cross-section\nof returns and maximum drawdown for stocks in the US equity market. Our data\nrun from January 1970 to December 2019 and our analysis includes ordinary least\nsquares, penalized linear regressions, tree-based models, and neural networks.\nWe find that the most important predictors tended to be consistent across\nmodels, and that non-linear models had better predictive power than linear\nmodels. Predictive power was higher in calm periods than in stressed periods.\nEnvironmental, social, and governance indicators marginally impacted the\npredictive power of non-linear models in our data, despite their negative\ncorrelation with maximum drawdown and positive correlation with returns. Upon\nexploring whether ESG variables are captured by some models, we find that ESG\ndata contribute to the prediction nonetheless.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.05237v2"
    },
    {
        "title": "Cointegration in high frequency data",
        "authors": [
            "Simon Clinet",
            "Yoann Potiron"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this paper, we consider a framework adapting the notion of cointegration\nwhen two asset prices are generated by a driftless It\\^{o}-semimartingale\nfeaturing jumps with infinite activity, observed regularly and synchronously at\nhigh frequency. We develop a regression based estimation of the cointegrated\nrelations method and show the related consistency and central limit theory when\nthere is cointegration within that framework. We also provide a Dickey-Fuller\ntype residual based test for the null of no cointegration against the\nalternative of cointegration, along with its limit theory. Under no\ncointegration, the asymptotic limit is the same as that of the original\nDickey-Fuller residual based test, so that critical values can be easily\ntabulated in the same way. Finite sample indicates adequate size and good power\nproperties in a variety of realistic configurations, outperforming original\nDickey-Fuller and Phillips-Perron type residual based tests, whose sizes are\ndistorted by non ergodic time-varying variance and power is altered by price\njumps. Two empirical examples consolidate the Monte-Carlo evidence that the\nadapted tests can be rejected while the original tests are not, and vice versa.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.07081v2"
    },
    {
        "title": "Diagnosis and Prediction of the 2015 Chinese Stock Market Bubble",
        "authors": [
            "Min Shu",
            "Wei Zhu"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this study, we perform a novel analysis of the 2015 financial bubble in\nthe Chinese stock market by calibrating the Log Periodic Power Law Singularity\n(LPPLS) model to two important Chinese stock indices, SSEC and SZSC, from early\n2014 to June 2015. The back tests of the 2015 Chinese stock market bubbles\nindicates that the LPPLS model can readily detect the bubble behavior of the\nfaster-than-exponential increase corrected by the accelerating\nlogarithm-periodic oscillations in the 2015 Chinese Stock market. The existence\nof log-periodicity is detected by applying the Lomb spectral analysis on the\ndetrended residuals. The Ornstein-Uhlenbeck property and the stationarity of\nthe LPPLS fitting residuals are confirmed by the two Unit-root tests\n(Philips-Perron test and Dickery-Fuller test). According to our analysis, the\nactual critical day t_c can be well predicted by the LPPLS model as far back as\ntwo months before the actual bubble crash. Compared to the traditional\noptimization method used in the LPPLS model, we find the covariance matrix\nadaptation evolution strategy (CMA-ES) to have a significantly lower\ncomputation cost, and thus recommend this as a better alternative algorithm for\nLPPLS model fit. Furthermore, in the LPPLS fitting with expanding windows, the\ngap (tc -t2) shows a significant decrease when the end day t2 approaches the\nactual bubble crash time. The change rate of the gap (tc-t2) may be used as an\nadditional indicator besides the key indicator tc to improve the prediction of\nbubble burst.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.09633v2"
    },
    {
        "title": "Detection of Chinese Stock Market Bubbles with LPPLS Confidence\n  Indicator",
        "authors": [
            "Min Shu",
            "Wei Zhu"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We present an advance bubble detection methodology based on the Log Periodic\nPower Law Singularity (LPPLS) confidence indicator for the early causal\nidentification of positive and negative bubbles in the Chinese stock market\nusing the daily data on the Shanghai Shenzhen CSI 300 stock market index from\nJanuary 2002 through April 2018. We account for the damping condition of LPPLS\nmodel in the search space and implement the stricter filter conditions for the\nqualification of the valid LPPLS fits by taking account of the maximum relative\nerror, performing the Lomb log-periodic test of the detrended residual, and\nunit-root tests of the logarithmic residual based on both the Phillips-Perron\ntest and Dickey-Fuller test to improve the performance of LPPLS confidence\nindicator. Our analysis shows that the LPPLS detection strategy diagnoses the\npositive bubbles and negative bubbles corresponding to well-known historical\nevents, implying the detection strategy based on the LPPLS confidence indicator\nhas an outstanding performance to identify the bubbles in advance. We find that\nthe probability density distribution of the estimated beginning time of bubbles\nappears to be skewed and the mass of the distribution is concentrated on the\narea where the price starts to have an obvious super-exponentially growth. This\nstudy is the first work in the literature that identifies the existence of\nbubbles in the Chinese stock market using the daily data of CSI 300 index with\nthe advance bubble detection methodology of LPPLS confidence indicator. We have\nshown that it is possible to detect the potential positive and negative bubbles\nand crashes ahead of time, which in turn limits the bubble sizes and eventually\nminimizes the damages from the bubble crash.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.09640v2"
    },
    {
        "title": "A Robust Predictive Model for Stock Price Prediction Using Deep Learning\n  and Natural Language Processing",
        "authors": [
            "Sidra Mehtab",
            "Jaydip Sen"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Prediction of future movement of stock prices has been a subject matter of\nmany research work. There is a gamut of literature of technical analysis of\nstock prices where the objective is to identify patterns in stock price\nmovements and derive profit from it. Improving the prediction accuracy remains\nthe single most challenge in this area of research. We propose a hybrid\napproach for stock price movement prediction using machine learning, deep\nlearning, and natural language processing. We select the NIFTY 50 index values\nof the National Stock Exchange of India, and collect its daily price movement\nover a period of three years (2015 to 2017). Based on the data of 2015 to 2017,\nwe build various predictive models using machine learning, and then use those\nmodels to predict the closing value of NIFTY 50 for the period January 2018\ntill June 2019 with a prediction horizon of one week. For predicting the price\nmovement patterns, we use a number of classification techniques, while for\npredicting the actual closing price of the stock, various regression models\nhave been used. We also build a Long and Short-Term Memory - based deep\nlearning network for predicting the closing price of the stocks and compare the\nprediction accuracies of the machine learning models with the LSTM model. We\nfurther augment the predictive model by integrating a sentiment analysis module\non twitter data to correlate the public sentiment of stock prices with the\nmarket sentiment. This has been done using twitter sentiment and previous week\nclosing values to predict stock price movement for the next week. We tested our\nproposed scheme using a cross validation method based on Self Organizing Fuzzy\nNeural Networks and found extremely interesting results.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.07700v1"
    },
    {
        "title": "DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using\n  Financial News",
        "authors": [
            "Xinyi Li",
            "Yinchuan Li",
            "Hongyang Yang",
            "Liuqing Yang",
            "Xiao-Yang Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Stock price prediction is important for value investments in the stock\nmarket. In particular, short-term prediction that exploits financial news\narticles is promising in recent years. In this paper, we propose a novel deep\nneural network DP-LSTM for stock price prediction, which incorporates the news\narticles as hidden information and integrates difference news sources through\nthe differential privacy mechanism. First, based on the autoregressive moving\naverage model (ARMA), a sentiment-ARMA is formulated by taking into\nconsideration the information of financial news articles in the model. Then, an\nLSTM-based deep neural network is designed, which consists of three components:\nLSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM\nscheme can reduce prediction errors and increase the robustness. Extensive\nexperiments on S&P 500 stocks show that (i) the proposed DP-LSTM achieves 0.32%\nimprovement in mean MPA of prediction result, and (ii) for the prediction of\nthe market index S&P 500, we achieve up to 65.79% improvement in MSE.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.10806v1"
    },
    {
        "title": "Alpha Discovery Neural Network based on Prior Knowledge",
        "authors": [
            "Jie Fang",
            "Shutao Xia",
            "Jianwu Lin",
            "Zhikang Xia",
            "Xiang Liu",
            "Yong Jiang"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Genetic programming (GP) is the state-of-the-art in financial automated\nfeature construction task. It employs reverse polish expression to represent\nfeatures and then conducts the evolution process. However, with the development\nof deep learning, more powerful feature extraction tools are available. This\npaper proposes Alpha Discovery Neural Network (ADNN), a tailored neural network\nstructure which can automatically construct diversified financial technical\nindicators based on prior knowledge. We mainly made three contributions. First,\nwe use domain knowledge in quantitative trading to design the sampling rules\nand object function. Second, pre-training and model pruning has been used to\nreplace genetic programming, because it can conduct more efficient evolution\nprocess. Third, the feature extractors in ADNN can be replaced by different\nfeature extractors and produce different functions. The experiment results show\nthat ADNN can construct more informative and diversified features than GP,\nwhich can effectively enriches the current factor pool. The fully-connected\nnetwork and recurrent network are better at extracting information from the\nfinancial time series than the convolution neural network. In real practice,\nfeatures constructed by ADNN can always improve multi-factor strategies'\nrevenue, sharpe ratio, and max draw-down, compared with the investment\nstrategies without these factors.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11761v8"
    },
    {
        "title": "A new approach for trading based on Long Short Term Memory technique",
        "authors": [
            "Zineb Lanbouri",
            "Saaid Achchab"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The stock market prediction has always been crucial for stakeholders, traders\nand investors. We developed an ensemble Long Short Term Memory (LSTM) model\nthat includes two-time frequencies (annual and daily parameters) in order to\npredict the next-day Closing price (one step ahead). Based on a four-step\napproach, this methodology is a serial combination of two LSTM algorithms. The\nempirical experiment is applied to 417 NY stock exchange companies. Based on\nOpen High Low Close metrics and other financial ratios, this approach proves\nthat the stock market prediction can be improved.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.03333v1"
    },
    {
        "title": "A tail dependence-based MST and their topological indicators in\n  modelling systemic risk in the European insurance sector",
        "authors": [
            "Anna Denkowska",
            "Stanisław Wanat"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In the present work we analyse the dynamics of indirect connections between\ninsurance companies that result from market price channels. In our analysis we\nassume that the stock quotations of insurance companies reflect market\nsentiments which constitute a very important systemic risk factor.\nInterlinkages between insurers and their dynamics have a direct impact on\nsystemic risk contagion in the insurance sector. We propose herein a new hybrid\napproach to the analysis of interlinkages dynamics based on combining the\ncopula-DCC-GARCH model and Minimum Spanning Trees (MST). Using the\ncopula-DCC-GARCH model we determine the tail dependence coefficients. Then, for\neach analysed period we construct MST based on these coefficients. The dynamics\nis analysed by means of time series of selected topological indicators of the\nMSTs in the years 2005-2019. Our empirical results show the usefulness of the\nproposed approach to the analysis of systemic risk in the insurance sector. The\ntimes series obtained from the proposed hybrid approach reflect the phenomena\noccurring on the market. The analysed MST topological indicators can be\nconsidered as systemic risk predictors.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.06567v2"
    },
    {
        "title": "Finance from the viewpoint of physics",
        "authors": [
            "A. Jakovac"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this note we review the basic mathematical ideas used in finance in the\nlanguage of modern physics. We focus on discrete time formalism, derive path\nintegral and Green's function formulas for pricing. We also discuss various\nrisk mitigation methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.09446v2"
    },
    {
        "title": "The Impact of Oil and Gold Prices Shock on Tehran Stock Exchange: A\n  Copula Approach",
        "authors": [
            "Amir T. Payandeh Najafabadi",
            "Marjan Qazvini",
            "Reza Ofoghi"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  There are several researches that deal with the behavior of SEs and their\nrelationships with different economical factors. These range from papers\ndealing with this subject through econometrical procedures to statistical\nmethods known as copula. This article considers the impact of oil and gold\nprice on Tehran Stock Exchange market (TSE). Oil and gold are two factors that\nare essential for the economy of Iran and their price are determined in the\nglobal market. The model used in this study is ARIMA-Copula. We used data from\nJanuary 1998 to January 2011 as training data to find the appropriate model.\nThe cross validation of model is measured by data from January 2011 to June\n2011. We conclude that: (i) there is no significant direct relationship between\ngold price and the TSE index, but the TSE is indirectly influenced by gold\nprice through other factors such as oil; and (ii) the TSE is not independent of\nthe volatility in oil price and Clayton copula can describe such dependence\nstructure between TSE and the oil price. Based on the property of Clayton\ncopula, which has lower tail dependency, as the oil price drops, stock index\nfalls. This means that decrease in oil price has an adverse effect on Iranian\neconomy.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.11275v1"
    },
    {
        "title": "Methods for forecasting the effect of exogenous risk on stock markets",
        "authors": [
            "Karina Arias-Calluari",
            "Fernando Alonso-Marroquin",
            "Morteza Nattagh-Najafi",
            "Michael Harré"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Markets are subjected to both endogenous and exogenous risks that have caused\ndisruptions to financial and economic markets around the globe, leading\neventually to fast stock market declines. In the past, markets have recovered\nafter any economic disruption. On this basis, we focus on the outbreak of\nCOVID-19 as a case study of an exogenous risk and analyze its impact on the\nStandard and Poor's 500 (S\\&P500) index. We assumed that the S\\&P500 index\nreaches a minimum before rising again in the not-too-distant future. Here we\npresent two cases to forecast the S\\&P500 index. The first case uses an\nestimation of expected deaths released on 02/04/2020 by the University of\nWashington. For the second case, it is assumed that the peak number of deaths\nwill occur 2-months since the first confirmed case occurred in the USA. The\ndecline and recovery in the index were estimated for the following three months\nafter the initial point of the predicted trend. The forecast is a projection of\na prediction with stochastic fluctuations described by $q$-gaussian diffusion\nprocess with three spatio-temporal regimes. Our forecast was made on the\npremise that any market response can be decomposed into an overall\ndeterministic trend and a stochastic term. The prediction was based on the\ndeterministic part and for this case study is approximated by the extrapolation\nof the S\\&P500 data trend in the initial stages of the outbreak. The stochastic\nfluctuations have the same structure as the one derived from the past 24 years.\nA reasonable forecast was achieved with 85\\% of accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03969v5"
    },
    {
        "title": "Multi-Graph Convolutional Network for Relationship-Driven Stock Movement\n  Prediction",
        "authors": [
            "Jiexia Ye",
            "Juanjuan Zhao",
            "Kejiang Ye",
            "Chengzhong Xu"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Stock price movement prediction is commonly accepted as a very challenging\ntask due to the volatile nature of financial markets. Previous works typically\npredict the stock price mainly based on its own information, neglecting the\ncross effect among involved stocks. However, it is well known that an\nindividual stock price is correlated with prices of other stocks in complex\nways. To take the cross effect into consideration, we propose a deep learning\nframework, called Multi-GCGRU, which comprises graph convolutional network\n(GCN) and gated recurrent unit (GRU) to predict stock movement. Specifically,\nwe first encode multiple relationships among stocks into graphs based on\nfinancial domain knowledge and utilize GCN to extract the cross effect based on\nthese pre-defined graphs. To further get rid of prior knowledge, we explore an\nadaptive relationship learned by data automatically. The cross-correlation\nfeatures produced by GCN are concatenated with historical records and then fed\ninto GRU to model the temporal dependency of stock prices. Experiments on two\nstock indexes in China market show that our model outperforms other baselines.\nNote that our model is rather feasible to incorporate more effective stock\nrelationships containing expert knowledge, as well as learn data-driven\nrelationship.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.04955v3"
    },
    {
        "title": "Detecting discrete processes with the Epps effect",
        "authors": [
            "Patrick Chang",
            "Etienne Pienaar",
            "Tim Gebbie"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The Epps effect is key phenomenology relating to high frequency correlation\ndynamics in financial markets. We argue that it can be used to provide insight\ninto whether tick data is best represented as samples from Brownian diffusions,\nor as samples from truly discrete events represented as connected point\nprocesses. We derive the Epps effect arising from asynchrony and provide a\nrefined method to correct for the effect. We then propose three experiments\nwhich show how to discriminate between possible underlying representations.\nThese in turn demonstrate how a simple Hawkes representation recovers\nphenomenology reported in the literature that cannot be recovered using a\nBrownian representation without additional ad hoc model complexity. However,\ncomplex ad hoc noise models built on Brownian motions cannot in general be\ndiscriminated relative to a Hawkes representation. Nevertheless, we argue that\nhigh frequency correlation dynamics are most faithfully recovered when tick\ndata is represented as a web of interconnected discrete events rather than\nbeing samples from continuous Brownian diffusions even when combined with\nnoise.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.10568v4"
    },
    {
        "title": "Daily Middle-Term Probabilistic Forecasting of Power Consumption in\n  North-East England",
        "authors": [
            "Roberto Baviera",
            "Giuseppe Messuti"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Probabilistic forecasting of power consumption in a middle-term horizon\n(months to a year) is a main challenge in the energy sector. It plays a key\nrole in planning future generation plants and transmission grid. We propose a\nnew model that incorporates trend and seasonality features as in traditional\ntime-series analysis and weather conditions as explicative variables in a\nparsimonious machine learning approach, known as Gaussian Process. Applying to\na daily power consumption dataset in North East England provided by one of the\nlargest energy suppliers, we obtain promising results in Out-of-Sample density\nforecasts up to one year, even using a small dataset, with only a two-year\nIn-Sample data. In order to verify the quality of the achieved power\nconsumption probabilistic forecast we consider measures that are common in the\nenergy sector as pinball loss and Winkler score and backtesting conditional and\nunconditional tests, standard in the banking sector after the introduction of\nBasel II Accords.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.13005v2"
    },
    {
        "title": "Stocks and Cryptocurrencies: Anti-fragile or Robust?",
        "authors": [
            "Darío Alatorre",
            "Carlos Gershenson",
            "José L. Mateos"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In contrast with robust systems that resist noise or fragile systems that\nbreak with noise, antifragility is defined as a property of complex systems\nthat benefit from noise or disorder. Here we define and test a simple measure\nof antifragility for complex dynamical systems. In this work we use our\nantifragility measure to analyze real data from return prices in the stock and\ncryptocurrency markets. Our definition of antifragility is the product of the\nreturn price and a perturbation. We explore different types of perturbations\nthat typically arise from within the system. Our results suggest that for both\nthe stock market and the cryptocurrency market, the tendency among the 'top\nperformers' is to be robust rather than antifragile. It would be important to\nexplore other possible definitions of antifragility to understand its role in\nfinancial markets and in complex dynamical systems in general.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.13033v3"
    },
    {
        "title": "Using Machine Learning to Forecast Future Earnings",
        "authors": [
            "Xinyue Cui",
            "Zhaoyu Xu",
            "Yue Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this essay, we have comprehensively evaluated the feasibility and\nsuitability of adopting the Machine Learning Models on the forecast of\ncorporation fundamentals (i.e. the earnings), where the prediction results of\nour method have been thoroughly compared with both analysts' consensus\nestimation and traditional statistical models. As a result, our model has\nalready been proved to be capable of serving as a favorable auxiliary tool for\nanalysts to conduct better predictions on company fundamentals. Compared with\nprevious traditional statistical models being widely adopted in the industry\nlike Logistic Regression, our method has already achieved satisfactory\nadvancement on both the prediction accuracy and speed. Meanwhile, we are also\nconfident enough that there are still vast potentialities for this model to\nevolve, where we do hope that in the near future, the machine learning model\ncould generate even better performances compared with professional analysts.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.13995v1"
    },
    {
        "title": "Specialization of strategies and herding behavior of trading firms in a\n  financial market",
        "authors": [
            "Fabrizio Lillo",
            "Esteban Moro",
            "Gabriella Vaglica",
            "Rosario N. Mantegna"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The understanding of complex social or economic systems is an important\nscientific challenge. Here we present a comprehensive study of the Spanish\nStock Exchange showing that most financial firms trading in that market are\ncharacterized by a resulting strategy and can be classified in groups of firms\nwith different specialization. Few large firms overally act as trending firms\nwhereas many heterogeneous firm act as reversing firms. The herding properties\nof these two groups are markedly different and consistently observed over a\nfour-year period of trading.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.0385v1"
    },
    {
        "title": "Indication of multiscaling in the volatility return intervals of stock\n  markets",
        "authors": [
            "Fengzhong Wang",
            "Kazuko Yamasaki",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The distribution of the return intervals $\\tau$ between volatilities above a\nthreshold $q$ for financial records has been approximated by a scaling\nbehavior. To explore how accurate is the scaling and therefore understand the\nunderlined non-linear mechanism, we investigate intraday datasets of 500 stocks\nwhich consist of the Standard & Poor's 500 index. We show that the cumulative\ndistribution of return intervals has systematic deviations from scaling. We\nsupport this finding by studying the m-th moment $\\mu_m \\equiv\n<(\\tau/<\\tau>)^m>^{1/m}$, which show a certain trend with the mean interval\n$<\\tau>$. We generate surrogate records using the Schreiber method, and find\nthat their cumulative distributions almost collapse to a single curve and\nmoments are almost constant for most range of $<\\tau>$. Those substantial\ndifferences suggest that non-linear correlations in the original volatility\nsequence account for the deviations from a single scaling law. We also find\nthat the original and surrogate records exhibit slight tendencies for short and\nlong $<\\tau>$, due to the discreteness and finite size effects of the records\nrespectively. To avoid as possible those effects for testing the multiscaling\nbehavior, we investigate the moments in the range $10<<\\tau>\\leq100$, and find\nthe exponent $\\alpha$ from the power law fitting $\\mu_m\\sim<\\tau>^\\alpha$ has a\nnarrow distribution around $\\alpha\\neq0$ which depend on m for the 500 stocks.\nThe distribution of $\\alpha$ for the surrogate records are very narrow and\ncentered around $\\alpha=0$. This suggests that the return interval distribution\nexhibit multiscaling behavior due to the non-linear correlations in the\noriginal volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.4638v1"
    },
    {
        "title": "A semi-Markov model with memory for price changes",
        "authors": [
            "Guglielmo D'Amico",
            "Filippo Petroni"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We study the high frequency price dynamics of traded stocks by a model of\nreturns using a semi-Markov approach. More precisely we assume that the\nintraday returns are described by a discrete time homogeneous semi-Markov which\ndepends also on a memory index. The index is introduced to take into account\nperiods of high and low volatility in the market. First of all we derive the\nequations governing the process and then theoretical results have been compared\nwith empirical findings from real data. In particular we analyzed high\nfrequency data from the Italian stock market from first of January 2007 until\nend of December 2010.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.4259v2"
    },
    {
        "title": "Performance Analysis of Hybrid Forecasting Model In Stock Market\n  Forecasting",
        "authors": [
            "Mahesh S. Khadka",
            "K. M. George",
            "N. Park",
            "J. B. Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This paper presents performance analysis of hybrid model comprise of\nconcordance and Genetic Programming (GP) to forecast financial market with some\nexisting models. This scheme can be used for in depth analysis of stock market.\nDifferent measures of concordances such as Kendalls Tau, Ginis Mean Difference,\nSpearmans Rho, and weak interpretation of concordance are used to search for\nthe pattern in past that look similar to present. Genetic Programming is then\nused to match the past trend to present trend as close as possible. Then\nGenetic Program estimates what will happen next based on what had happened\nnext. The concept is validated using financial time series data (S&P 500 and\nNASDAQ indices) as sample data sets. The forecasted result is then compared\nwith standard ARIMA model and other model to analyse its performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.4608v2"
    },
    {
        "title": "The behavior of dealers and clients on the European corporate bond\n  market: the case of Multi-Dealer-to-Client platforms",
        "authors": [
            "Jean-David Fermanian",
            "Olivier Guéant",
            "Jiang Pu"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  For the last two decades, most financial markets have undergone an evolution\ntoward electronification. The market for corporate bonds is one of the last\nmajor financial markets to follow this unavoidable path. Traditionally\nquote-driven i.e., dealer-driven) rather than order-driven, the market for\ncorporate bonds is still mainly dominated by voice trading, but a lot of\nelectronic platforms have emerged. These electronic platforms make it possible\nfor buy-side agents to simultaneously request several dealers for quotes, or\neven directly trade with other buy-siders. The research presented in this\narticle is based on a large proprietary database of requests for quotes (RFQ)\nsent, through the multi-dealer-to-client (MD2C) platform operated by Bloomberg\nFixed Income Trading, to one of the major liquidity providers in European\ncorporate bonds. Our goal is (i) to model the RFQ process on these platforms\nand the resulting competition between dealers, and (ii) to use our model in\norder to implicit from the RFQ database the behavior of both dealers and\nclients on MD2C platforms.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.07773v4"
    },
    {
        "title": "A bootstrap test to detect prominent Granger-causalities across\n  frequencies",
        "authors": [
            "Matteo Farné",
            "Angela Montanari"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Granger-causality in the frequency domain is an emerging tool to analyze the\ncausal relationship between two time series. We propose a bootstrap test on\nunconditional and conditional Granger-causality spectra, as well as on their\ndifference, to catch particularly prominent causality cycles in relative terms.\nIn particular, we consider a stochastic process derived applying independently\nthe stationary bootstrap to the original series. Our null hypothesis is that\neach causality or causality difference is equal to the median across\nfrequencies computed on that process. In this way, we are able to disambiguate\ncausalities which depart significantly from the median one obtained ignoring\nthe causality structure. Our test shows power one as the process tends to\nnon-stationarity, thus being more conservative than parametric alternatives. As\nan example, we infer about the relationship between money stock and GDP in the\nEuro Area via our approach, considering inflation, unemployment and interest\nrates as conditioning variables. We point out that during the period 1999-2017\nthe money stock aggregate M1 had a significant impact on economic output at all\nfrequencies, while the opposite relationship is significant only at high\nfrequencies.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00374v2"
    },
    {
        "title": "Efficient construction of threshold networks of stock markets",
        "authors": [
            "Xin-Jian Xu",
            "Kuo Wang",
            "Liucun Zhu",
            "Li-Jie Zhang"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Although the threshold network is one of the most used tools to characterize\nthe underlying structure of a stock market, the identification of the optimal\nthreshold to construct a reliable stock network remains challenging. In this\npaper, the concept of dynamic consistence between the threshold network and the\nstock market is proposed. The optimal threshold is estimated by maximizing the\nconsistence function. The application of this procedure to stocks belonging to\nStandard \\& Pool's 500 Index from January 2006 to December 2011 yields the\nthreshold value 0.28. In analyzing topological characteristics of the generated\nnetwork, three globally financial crises can be distinguished well from the\nevolutionary perspective.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.06223v2"
    },
    {
        "title": "The cooling-off effect of price limits in the Chinese stock markets",
        "authors": [
            "Yu-Lei Wan",
            "Gang-Jin Wang",
            "Zhi-Qiang Jiang",
            "Wen-Jie Xie",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In this paper, we investigate the cooling-off effect (opposite to the magnet\neffect) from two aspects. Firstly, from the viewpoint of dynamics, we study the\nexistence of the cooling-off effect by following the dynamical evolution of\nsome financial variables over a period of time before the stock price hits its\nlimit. Secondly, from the probability perspective, we investigate, with the\nlogit model, the existence of the cooling-off effect through analyzing the\nhigh-frequency data of all A-share common stocks traded on the Shanghai Stock\nExchange and the Shenzhen Stock Exchange from 2000 to 2011 and inspecting the\ntrading period from the opening phase prior to the moment that the stock price\nhits its limits. A comparison is made of the properties between up-limit hits\nand down-limit hits, and the possible difference will also be compared between\nbullish and bearish market state by dividing the whole period into three\nalternating bullish periods and three bearish periods. We find that the\ncooling-off effect emerges for both up-limit hits and down-limit hits, and the\ncooling-off effect of the down-limit hits is stronger than that of the up-limit\nhits. The difference of the cooling-off effect between bullish period and\nbearish period is quite modest. Moreover, we examine the sub-optimal orders\neffect, and infer that the professional individual investors and institutional\ninvestors play a positive role in the cooling-off effects. All these findings\nindicate that the price limit trading rule exerts a positive effect on\nmaintaining the stability of the Chinese stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09422v1"
    },
    {
        "title": "Cluster analysis of stocks using price movements of high frequency data\n  from National Stock Exchange",
        "authors": [
            "Charu Sharma",
            "Amber Habib",
            "Sunil Bowry"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  This paper aims to develop new techniques to describe joint behavior of\nstocks, beyond regression and correlation. For example, we want to identify the\nclusters of the stocks that move together. Our work is based on applying Kernel\nPrincipal Component Analysis(KPCA) and Functional Principal Component\nAnalysis(FPCA) to high frequency data from NSE. Since we dealt with high\nfrequency data with a tick size of 30 seconds, FPCA seems to be an ideal\nchoice. FPCA is a functional variant of PCA where each sample point is\nconsidered to be a function in Hilbert space L^2. On the other hand, KPCA is an\nextension of PCA using kernel methods. Results obtained from FPCA and Gaussian\nKernel PCA seems to be in synergy but with a lag. There were two prominent\nclusters that showed up in our analysis, one corresponding to the banking\nsector and another corresponding to the IT sector. The other smaller clusters\nwere seen from the automobile industry and the energy sector. IT sector was\nseen interacting with these small clusters. The learning gained from these\ninteractions is substantial as one can use it significantly to develop trading\nstrategies for intraday traders.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09514v1"
    },
    {
        "title": "Directed Continuous-Time Random Walk with memory",
        "authors": [
            "Jarosław Klamut",
            "Tomasz Gubiec"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We propose a new Directed Continuous-Time Random Walk (CTRW) model with\nmemory. As CTRW trajectory consists of spatial jumps preceded by waiting times,\nin Directed CTRW, we consider the case with only positive spatial jumps.\nMoreover, we consider the memory in the model as each spatial jump depends on\nthe previous one. Our model is motivated by the financial application of the\nCTRW presented in [Phys. Rev. E 82:046119][Eur. Phys. J. B 90:50]. As CTRW can\nsuccessfully describe the short term negative autocorrelation of returns in\nhigh-frequency financial data (caused by the bid-ask bounce phenomena), we\nasked ourselves to what extent the observed long-term autocorrelation of\nabsolute values of returns can be explained by the same phenomena. It turned\nout that the bid-ask bounce can be responsible only for the small fraction of\nthe memory observed in the high-frequency financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.01934v1"
    },
    {
        "title": "Quantifying Volatility Reduction in German Day-ahead Spot Market in the\n  Period 2006 through 2016",
        "authors": [
            "Abdolrahman Khoshrou",
            "Eric J. Pauwels"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In Europe, Germany is taking the lead in the switch from the conventional to\nrenewable energy. This poses new challenges as wind and solar energy are\nfundamentally intermittent, weather-dependent and less predictable. It is\ntherefore of considerable interest to investigate the evolution of price\nvolatility in this post-transition era. There are a number of reasons, however,\nthat makes the practical studies difficult. For instance, EPEX prices can be\nzero or negative. Consequently, the standard approach in financial time series\nanalysis to switch to logarithmic measures is inapplicable. Furthermore, in\ncontrast to the stock market prices which are only available for trading days,\nEPEX prices cover the whole year, including weekends and holidays. Accordingly,\nthere is a lot of underlying variability in the data which has nothing to do\nwith volatility, but simply reflects diurnal activity patterns. An important\ndistinction of the present work is the application of matrix decomposition\ntechniques, namely the singular value decomposition (SVD), for defining an\nalternative notion of volatility. This approach is systematically more robust\ntoward outliers and also the diurnal patterns. Our observations show that the\nday-ahead market is becoming less volatile in recent years.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.07328v1"
    },
    {
        "title": "Modeling joint probability distribution of yield curve parameters",
        "authors": [
            "Jarek Duda",
            "Małgorzata Snarska"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  US Yield curve has recently collapsed to its most flattened level since\nsubprime crisis and is close to the inversion. This fact has gathered attention\nof investors around the world and revived the discussion of proper modeling and\nforecasting yield curve, since changes in interest rate structure are believed\nto represent investors expectations about the future state of economy and have\nforeshadowed recessions in the United States. While changes in term structure\nof interest rates are relatively easy to interpret they are however very\ndifficult to model and forecast due to no proper economic theory underlying\nsuch events. Yield curves are usually represented by multivariate sparse time\nseries, at any point in time infinite dimensional curve is portrayed via\nrelatively few points in a multivariate space of data and as a consequence\nmultimodal statistical dependencies behind these curves are relatively hard to\nextract and forecast via typical multivariate statistical methods.We propose to\nmodel yield curves via reconstruction of joint probability distribution of\nparameters in functional space as a high degree polynomial. Thanks to adoption\nof an orthonormal basis, the MSE estimation of coefficients of a given function\nis an average over a data sample in the space of functions. Since such\npolynomial coefficients are independent and have cumulant-like interpretation\nie.describe corresponding perturbation from an uniform joint distribution, our\napproach can also be extended to any d-dimensional space of yield curve\nparameters (also in neighboring times) due to controllable accuracy. We believe\nthat this approach to modeling of local behavior of a sparse multivariate\ncurved time series can complement prediction from standard models like ARIMA,\nthat are using long range dependencies, but provide only inaccurate prediction\nof probability distribution, often as just Gaussian with constant width.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.11743v1"
    },
    {
        "title": "Deep Learning in Asset Pricing",
        "authors": [
            "Luyang Chen",
            "Markus Pelger",
            "Jason Zhu"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We use deep neural networks to estimate an asset pricing model for individual\nstock returns that takes advantage of the vast amount of conditioning\ninformation, while keeping a fully flexible form and accounting for\ntime-variation. The key innovations are to use the fundamental no-arbitrage\ncondition as criterion function, to construct the most informative test assets\nwith an adversarial approach and to extract the states of the economy from many\nmacroeconomic time series. Our asset pricing model outperforms out-of-sample\nall benchmark approaches in terms of Sharpe ratio, explained variation and\npricing errors and identifies the key factors that drive asset prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00745v6"
    },
    {
        "title": "Forecasting the Volatilities of Philippine Stock Exchange Composite\n  Index Using the Generalized Autoregressive Conditional Heteroskedasticity\n  Modeling",
        "authors": [
            "Novy Ann M. Etac",
            "Roel F. Ceballos"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This study was conducted to find an appropriate statistical model to forecast\nthe volatilities of PSEi using the model Generalized Autoregressive Conditional\nHeteroskedasticity (GARCH). Using the R software, the log returns of PSEi is\nmodeled using various ARIMA models and with the presence of heteroskedasticity,\nthe log returns was modeled using GARCH. Based on the analysis, GARCH models\nare the most appropriate to use for the log returns of PSEi. Among the selected\nGARCH models, GARCH (1,2) has the lowest AIC value and also has the highest LL\nvalue implying that GARCH (1,2) is the best model for the log returns of PSEi.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00749v1"
    },
    {
        "title": "Feature Engineering for Mid-Price Prediction with Deep Learning",
        "authors": [
            "Adamantios Ntakaris",
            "Giorgio Mirone",
            "Juho Kanniainen",
            "Moncef Gabbouj",
            "Alexandros Iosifidis"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Mid-price movement prediction based on limit order book (LOB) data is a\nchallenging task due to the complexity and dynamics of the LOB. So far, there\nhave been very limited attempts for extracting relevant features based on LOB\ndata. In this paper, we address this problem by designing a new set of\nhandcrafted features and performing an extensive experimental evaluation on\nboth liquid and illiquid stocks. More specifically, we implement a new set of\neconometrical features that capture statistical properties of the underlying\nsecurities for the task of mid-price prediction. Moreover, we develop a new\nexperimental protocol for online learning that treats the task as a\nmulti-objective optimization problem and predicts i) the direction of the next\nprice movement and ii) the number of order book events that occur until the\nchange takes place. In order to predict the mid-price movement, the features\nare fed into nine different deep learning models based on multi-layer\nperceptrons (MLP), convolutional neural networks (CNN) and long short-term\nmemory (LSTM) neural networks. The performance of the proposed method is then\nevaluated on liquid and illiquid stocks, which are based on TotalView-ITCH US\nand Nordic stocks, respectively. For some stocks, results suggest that the\ncorrect choice of a feature set and a model can lead to the successful\nprediction of how long it takes to have a stock price movement.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05384v3"
    },
    {
        "title": "Copula estimation for nonsynchronous financial data",
        "authors": [
            "Arnab Chakrabarti",
            "Rituparna Sen"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Copula is a powerful tool to model multivariate data. We propose the\nmodelling of intraday financial returns of multiple assets through copula. The\nproblem originates due to the asynchronous nature of intraday financial data.\nWe propose a consistent estimator of the correlation coefficient in case of\nElliptical copula and show that the plug-in copula estimator is uniformly\nconvergent. For non-elliptical copulas, we capture the dependence through\nKendall's Tau. We demonstrate underestimation of the copula parameter and use a\nquadratic model to propose an improved estimator. In simulations, the proposed\nestimator reduces the bias significantly for a general class of copulas. We\napply the proposed methods to real data of several stock prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.10182v2"
    },
    {
        "title": "Quantile-Frequency Analysis and Spectral Divergence Metrics for\n  Diagnostic Checks of Time Series With Nonlinear Dynamics",
        "authors": [
            "Ta-Hsin Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Nonlinear dynamic volatility has been observed in many financial time series.\nThe recently proposed quantile periodogram offers an alternative way to examine\nthis phenomena in the frequency domain. The quantile periodogram is constructed\nfrom trigonometric quantile regression of time series data at different\nfrequencies and quantile levels. It is a useful tool for quantile-frequency\nanalysis (QFA) of nonlinear serial dependence. This paper introduces a number\nof spectral divergence metrics based on the quantile periodogram for diagnostic\nchecks of financial time series models and model-based discriminant analysis.\nThe parametric bootstrapping technique is employed to compute the $p$-values of\nthe metrics. The usefulness of the proposed method is demonstrated empirically\nby a case study using the daily log returns of the S\\&P 500 index over three\nperiods of time together with their GARCH-type models. The results show that\nthe QFA method is able to provide additional insights into the goodness of fit\nof these financial time series models that may have been missed by conventional\ntests. The results also show that the QFA method offers a more informative way\nof discriminant analysis for detecting regime changes in time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02545v1"
    },
    {
        "title": "Modeling microstructure price dynamics with symmetric Hawkes and\n  diffusion model using ultra-high-frequency stock data",
        "authors": [
            "Kyungsub Lee",
            "Byoung Ki Seo"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This study examine the theoretical and empirical perspectives of the\nsymmetric Hawkes model of the price tick structure. Combined with the maximum\nlikelihood estimation, the model provides a proper method of volatility\nestimation specialized in ultra-high-frequency analysis. Empirical studies\nbased on the model using the ultra-high-frequency data of stocks in the S\\&P\n500 are performed. The performance of the volatility measure, intraday\nestimation, and the dynamics of the parameters are discussed. A new approach of\ndiffusion analogy to the symmetric Hawkes model is proposed with the\ndistributional properties very close to the Hawkes model. As a diffusion\nprocess, the model provides more analytical simplicity when computing the\nvariance formula, incorporating skewness and examining the probabilistic\nproperty. An estimation of the diffusion model is performed using the simulated\nmaximum likelihood method and shows similar patterns to the Hawkes model.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.05089v1"
    },
    {
        "title": "A multi-scale symmetry analysis of uninterrupted trends returns of daily\n  financial indices",
        "authors": [
            "C. M. Rodríguez-Martínez",
            "H. F. Coronel-Brizio",
            "A. R. Hernández-Montoya"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We present a symmetry analysis of the distribution of variations of different\nfinancial indices, by means of a statistical procedure developed by the authors\nbased on a symmetry statistic by Einmahl and Mckeague. We applied this\nstatistical methodology to financial uninterrupted daily trends returns and to\nother derived observable. In our opinion, to study distributional symmetry,\ntrends returns offer more advantages than the commonly used daily financial\nreturns; the two most important being: 1) Trends returns involve sampling over\ndifferent time scales and 2) By construction, this variable time series\ncontains practically the same number of non-negative and negative entry values.\nWe also show that these time multi-scale returns display distributional\nbi-modality. Daily financial indices analyzed in this work, are the Mexican\nIPC, the American DJIA, DAX from Germany and the Japanese Market index Nikkei,\ncovering a time period from 11-08-1991 to 06-30-2017. We show that, at the time\nscale resolution and significance considered in this paper, it is almost always\nfeasible to find an interval of possible symmetry points containing one most\nplausible symmetry point denoted by C. Finally, we study the temporal evolution\nof C showing that this point is seldom zero and responds with sensitivity to\nextreme market events.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.11204v1"
    },
    {
        "title": "Systemic Risk Clustering of China Internet Financial Based on t-SNE\n  Machine Learning Algorithm",
        "authors": [
            "Mi Chuanmin",
            "Xu Runjie",
            "Lin Qingtong"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  With the rapid development of Internet finance, a large number of studies\nhave shown that Internet financial platforms have different financial systemic\nrisk characteristics when they are subject to macroeconomic shocks or fragile\ninternal crisis. From the perspective of regional development of Internet\nfinance, this paper uses t-SNE machine learning algorithm to obtain data mining\nof China's Internet finance development index involving 31 provinces and 335\ncities and regions. The conclusion of the peak and thick tail characteristics,\nthen proposed three classification risks of Internet financial systemic risk,\nproviding more regionally targeted recommendations for the systematic risk of\nInternet finance.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03808v1"
    },
    {
        "title": "Estimating the volatility of Bitcoin using GARCH models",
        "authors": [
            "Samuel Asante Gyamerah"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this paper, an application of three GARCH-type models (sGARCH, iGARCH, and\ntGARCH) with Student t-distribution, Generalized Error distribution (GED), and\nNormal Inverse Gaussian (NIG) distribution are examined. The new development\nallows for the modeling of volatility clustering effects, the leptokurtic and\nthe skewed distributions in the return series of Bitcoin. Comparative to the\ntwo distributions, the normal inverse Gaussian distribution captured adequately\nthe fat tails and skewness in all the GARCH type models. The tGARCH model was\nthe best model as it described the asymmetric occurrence of shocks in the\nBitcoin market. That is, the response of investors to the same amount of good\nand bad news are distinct. From the empirical results, it can be concluded that\ntGARCH-NIG was the best model to estimate the volatility in the return series\nof Bitcoin. Generally, it would be optimal to use the NIG distribution in GARCH\ntype models since time series of most cryptocurrency are leptokurtic.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.04903v2"
    },
    {
        "title": "Continuous Time Random Walk with correlated waiting times. The crucial\n  role of inter-trade times in volatility clustering",
        "authors": [
            "Jarosław Klamut",
            "Tomasz Gubiec"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In many physical, social or economical phenomena we observe changes of a\nstudied quantity only in discrete, irregularly distributed points in time. The\nstochastic process used by physicists to describe this kind of variables is the\nContinuous Time Random Walk (CTRW). Despite the popularity of this type of\nstochastic processes and strong empirical motivation, models with a long-term\nmemory within the sequence of time intervals between observations are missing.\nHere, we fill this gap by introducing a new family of CTRWs. The memory is\nintroduced to the model by the assumption that many consecutive time intervals\ncan be the same. Surprisingly, in this process we can observe a slowly decaying\nnonlinear autocorrelation function without a fat-tailed distribution of time\nintervals. Our model applied to high-frequency stock market data can\nsuccessfully describe the slope of decay of nonlinear autocorrelation function\nof stock market returns. The model achieves this result with no dependence\nbetween consecutive price changes. It proves the crucial role of inter-event\ntimes in the volatility clustering phenomenon observed in all stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.04986v2"
    },
    {
        "title": "Change-point Analysis in Financial Networks",
        "authors": [
            "Sayantan Banerjee",
            "Kousik Guhathakurta"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  A major impact of globalization has been the information flow across the\nfinancial markets rendering them vulnerable to financial contagion. Research\nhas focused on network analysis techniques to understand the extent and nature\nof such information flow. It is now an established fact that a stock market\ncrash in one country can have a serious impact on other markets across the\nglobe. It follows that such crashes or critical regimes will affect the network\ndynamics of the global financial markets. In this paper, we use sequential\nchange point detection in dynamic networks to detect changes in the network\ncharacteristics of thirteen stock markets across the globe. Our method helps us\nto detect changes in network behavior across all known stock market crashes\nduring the period of study. In most of the cases, we can detect a change in the\nnetwork characteristics prior to crash. Our work thus opens the possibility of\nusing this technique to create a warning bell for critical regimes in financial\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.05952v1"
    },
    {
        "title": "Predicting Indian stock market using the psycho-linguistic features of\n  financial news",
        "authors": [
            "B. Shravan Kumar",
            "Vadlamani Ravi",
            "Rishabh Miglani"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Financial forecasting using news articles is an emerging field. In this\npaper, we proposed hybrid intelligent models for stock market prediction using\nthe psycholinguistic variables (LIWC and TAALES) extracted from news articles\nas predictor variables. For prediction purpose, we employed various intelligent\ntechniques such as Multilayer Perceptron (MLP), Group Method of Data Handling\n(GMDH), General Regression Neural Network (GRNN), Random Forest (RF), Quantile\nRegression Random Forest (QRRF), Classification and regression tree (CART) and\nSupport Vector Regression (SVR). We experimented on the data of 12 companies\nstocks, which are listed in the Bombay Stock Exchange (BSE). We employed\nchi-squared and maximum relevance and minimum redundancy (MRMR) feature\nselection techniques on the psycho-linguistic features obtained from the new\narticles etc. After extensive experimentation, using the Diebold-Mariano test,\nwe conclude that GMDH and GRNN are statistically the best techniques in that\norder with respect to the MAPE and NRMSE values.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.06193v1"
    },
    {
        "title": "The impacts of asymmetry on modeling and forecasting realized volatility\n  in Japanese stock markets",
        "authors": [
            "Daiki Maki",
            "Yasushi Ota"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This study investigates the impacts of asymmetry on the modeling and\nforecasting of realized volatility in the Japanese futures and spot stock\nmarkets. We employ heterogeneous autoregressive (HAR) models allowing for three\ntypes of asymmetry: positive and negative realized semivariance (RSV),\nasymmetric jumps, and leverage effects. The estimation results show that\nleverage effects clearly influence the modeling of realized volatility models.\nLeverage effects exist for both the spot and futures markets in the Nikkei 225.\nAlthough realized semivariance aids better modeling, the estimations of RSV\nmodels depend on whether these models have leverage effects. Asymmetric jump\ncomponents do not have a clear influence on realized volatility models. While\nleverage effects and realized semivariance also improve the out-of-sample\nforecast performance of volatility models, asymmetric jumps are not useful for\npredictive ability. The empirical results of this study indicate that\nasymmetric information, in particular, leverage effects and realized\nsemivariance, yield better modeling and more accurate forecast performance.\nAccordingly, asymmetric information should be included when we model and\nforecast the realized volatility of Japanese stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.00158v1"
    },
    {
        "title": "A New Look to Three-Factor Fama-French Regression Model using Sample\n  Innovations",
        "authors": [
            "Javad Shaabani",
            "Ali Akbar Jafari"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The Fama-French model is widely used in assessing the portfolio's performance\ncompared to market returns. In Fama-French models, all factors are time-series\ndata. The cross-sectional data are slightly different from the time series\ndata. A distinct problem with time-series regressions is that R-squared in time\nseries regressions is usually very high, especially compared with typical\nR-squared for cross-sectional data. The high value of R-squared may cause\nmisinterpretation that the regression model fits the observed data well, and\nthe variance in the dependent variable is explained well by the independent\nvariables. Thus, to do regression analysis, and overcome with the serial\ndependence and volatility clustering, we use standard econometrics time series\nmodels to derive sample innovations. In this study, we revisit and validate the\nFama-French models in two different ways: using the factors and asset returns\nin the Fama-French model and considering the sample innovations in the\nFama-French model instead of studying the factors. Comparing the two methods\nconsidered in this study, we suggest the Fama-French model should be considered\nwith heavy tail distributions as the tail behavior is relevant in Fama-French\nmodels, including financial data, and the QQ plot does not validate that the\nchoice of the normal distribution as the theoretical distribution for the noise\nin the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.02467v1"
    },
    {
        "title": "Real-Time Prediction of BITCOIN Price using Machine Learning Techniques\n  and Public Sentiment Analysis",
        "authors": [
            "S M Raju",
            "Ali Mohammad Tarif"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Bitcoin is the first digital decentralized cryptocurrency that has shown a\nsignificant increase in market capitalization in recent years. The objective of\nthis paper is to determine the predictable price direction of Bitcoin in USD by\nmachine learning techniques and sentiment analysis. Twitter and Reddit have\nattracted a great deal of attention from researchers to study public sentiment.\nWe have applied sentiment analysis and supervised machine learning principles\nto the extracted tweets from Twitter and Reddit posts, and we analyze the\ncorrelation between bitcoin price movements and sentiments in tweets. We\nexplored several algorithms of machine learning using supervised learning to\ndevelop a prediction model and provide informative analysis of future market\nprices. Due to the difficulty of evaluating the exact nature of a Time\nSeries(ARIMA) model, it is often very difficult to produce appropriate\nforecasts. Then we continue to implement Recurrent Neural Networks (RNN) with\nlong short-term memory cells (LSTM). Thus, we analyzed the time series model\nprediction of bitcoin prices with greater efficiency using long short-term\nmemory (LSTM) techniques and compared the predictability of bitcoin price and\nsentiment analysis of bitcoin tweets to the standard method (ARIMA). The RMSE\n(Root-mean-square error) of LSTM are 198.448 (single feature) and 197.515\n(multi-feature) whereas the ARIMA model RMSE is 209.263 which shows that LSTM\nwith multi feature shows the more accurate result.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.14473v1"
    },
    {
        "title": "Improving MF-DFA model with applications in precious metals market",
        "authors": [
            "Zhongjun Wang",
            "Mengye Sun",
            "A. M. Elsawah"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  With the aggravation of the global economic crisis and inflation, the\nprecious metals with safe-haven function have become more popular. An improved\nMF-DFA method is proposed to analyze price fluctuations of the precious metals\nmarket. Based on the widely used multifractal detrended fluctuation analysis\nmethod (MF-DFA), we compare these two methods and find that the Bi-OSW-MF-DFA\nmethod possesses better efficiency. This article analyzes the degree of\nmultifractality between spot gold market and spot silver market as well as\ntheir risks. From the numerical results and figures, it is found that two\nelements constitute the contributions in the formation of multifractality in\ntime series and the risk of the spot silver market is higher than that of the\nspot gold market. This attempt could lead to a better understanding of\ncomplicated precious metals market.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.15214v1"
    },
    {
        "title": "Model of cunning agents",
        "authors": [
            "Mateusz Denys"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  A numerical agent-based spin model of financial markets, based on the Potts\nmodel from statistical mechanics, with a novel interpretation of the spin\nvariable (as regards financial-market models) is presented. In this model, a\nvalue of the spin variable is only the agent's opinion concerning current\nmarket situation, which he communicates to his nearest neighbors. Instead, the\nagent's action (i.e., buying, selling, or staying inactive) is connected with a\nchange of the spin variable. Hence, the agents can be considered as cunning in\nthis model. That is, these agents encourage their neighbors to buy stocks if\nthe agents have an opportunity to sell them, and the agents encourage their\nneighbors to sell stocks if the agents have a reversed opportunity. Predictions\nof the model are in good agreement with empirical data from various real-life\nfinancial markets. The model reproduces the shape of the usual and\nabsolute-value autocorrelation function of returns as well as the distribution\nof times between superthreshold losses.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08517v1"
    },
    {
        "title": "A geometric analysis of nonlinear dynamics and its application to\n  financial time series",
        "authors": [
            "Isao Shoji",
            "Masahiro Nozawa"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  A geometric method to analyze nonlinear oscillations is discussed. We\nconsider a nonlinear oscillation modeled by a second order ordinary\ndifferential equation without specifying the function form. By transforming the\ndifferential equation into the system of first order ordinary differential\nequations, the trajectory is embedded in $R^3$ as a curve, and thereby the time\nevolution of the original state can be translated into the behavior of the\ncurve in $R^3$, or the vector field along the curve. We analyze the vector\nfield to investigate the dynamic properties of a nonlinear oscillation. While\nthe function form of the model is unspecified, the vector fields and those\nassociated quantities can be estimated by a nonparametric filtering method. We\napply the proposed analysis to the time series of the Japanese stock price\nindex. The application shows that the vector field and its derivative will be\nused as the tools of picking up various signals that help understanding of the\ndynamic properties of the stock price index.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.11825v1"
    },
    {
        "title": "Network-centric indicators for fragility in global financial indices",
        "authors": [
            "Areejit Samal",
            "Sunil Kumar",
            "Yasharth Yadav",
            "Anirban Chakraborti"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Over the last two decades, financial systems have been studied and analysed\nfrom the perspective of complex networks, where the nodes and edges in the\nnetwork represent the various financial components and the strengths of\ncorrelations between them. Here, we adopt a similar network-based approach to\nanalyse the daily closing prices of 69 global financial market indices across\n65 countries over a period of 2000-2014. We study the correlations among the\nindices by constructing threshold networks superimposed over minimum spanning\ntrees at different time frames. We investigate the effect of critical events in\nfinancial markets (crashes and bubbles) on the interactions among the indices\nby performing both static and dynamic analyses of the correlations. We compare\nand contrast the structures of these networks during periods of crashes and\nbubbles, with respect to the normal periods in the market. In addition, we\nstudy the temporal evolution of traditional market indicators, various global\nnetwork measures and the recently developed edge-based curvature measures. We\nshow that network-centric measures can be extremely useful in monitoring the\nfragility in the global financial market indices.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.00070v1"
    },
    {
        "title": "REST: Relational Event-driven Stock Trend Forecasting",
        "authors": [
            "Wentao Xu",
            "Weiqing Liu",
            "Chang Xu",
            "Jiang Bian",
            "Jian Yin",
            "Tie-Yan Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Stock trend forecasting, aiming at predicting the stock future trends, is\ncrucial for investors to seek maximized profits from the stock market. Many\nevent-driven methods utilized the events extracted from news, social media, and\ndiscussion board to forecast the stock trend in recent years. However, existing\nevent-driven methods have two main shortcomings: 1) overlooking the influence\nof event information differentiated by the stock-dependent properties; 2)\nneglecting the effect of event information from other related stocks. In this\npaper, we propose a relational event-driven stock trend forecasting (REST)\nframework, which can address the shortcoming of existing methods. To remedy the\nfirst shortcoming, we propose to model the stock context and learn the effect\nof event information on the stocks under different contexts. To address the\nsecond shortcoming, we construct a stock graph and design a new propagation\nlayer to propagate the effect of event information from related stocks. The\nexperimental studies on the real-world data demonstrate the efficiency of our\nREST framework. The results of investment simulation show that our framework\ncan achieve a higher return of investment than baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.07372v2"
    },
    {
        "title": "Financial factors selection with knockoffs: fund replication,\n  explanatory and prediction networks",
        "authors": [
            "Damien Challet",
            "Christian Bongiorno",
            "Guillaume Pelletier"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We apply the knockoff procedure to factor selection in finance. By building\nfake but realistic factors, this procedure makes it possible to control the\nfraction of false discovery in a given set of factors. To show its versatility,\nwe apply it to fund replication and to the inference of explanatory and\nprediction networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.05921v1"
    },
    {
        "title": "Feature Learning for Stock Price Prediction Shows a Significant Role of\n  Analyst Rating",
        "authors": [
            "Jaideep Singh",
            "Matloob Khushi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  To reject the Efficient Market Hypothesis a set of 5 technical indicators and\n23 fundamental indicators was identified to establish the possibility of\ngenerating excess returns on the stock market. Leveraging these data points and\nvarious classification machine learning models, trading data of the 505\nequities on the US S&P500 over the past 20 years was analysed to develop a\nclassifier effective for our cause. From any given day, we were able to predict\nthe direction of change in price by 1% up to 10 days in the future. The\npredictions had an overall accuracy of 83.62% with a precision of 85% for buy\nsignals and a recall of 100% for sell signals. Moreover, we grouped equities by\ntheir sector and repeated the experiment to see if grouping similar assets\ntogether positively effected the results but concluded that it showed no\nsignificant improvements in the performance rejecting the idea of sector-based\nanalysis. Also, using feature ranking we could identify an even smaller set of\n6 indicators while maintaining similar accuracies as that from the original 28\nfeatures and also uncovered the importance of buy, hold and sell analyst\nratings as they came out to be the top contributors in the model. Finally, to\nevaluate the effectiveness of the classifier in real-life situations, it was\nbacktested on FAANG equities using a modest trading strategy where it generated\nhigh returns of above 60% over the term of the testing dataset. In conclusion,\nour proposed methodology with the combination of purposefully picked features\nshows an improvement over the previous studies, and our model predicts the\ndirection of 1% price changes on the 10th day with high confidence and with\nenough buffer to even build a robotic trading system.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09106v1"
    },
    {
        "title": "Randentropy: a software to measure inequality in random systems",
        "authors": [
            "Guglielmo D'Amico",
            "Stefania Scocchera",
            "Loriano Storchi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The software Randentropy is designed to estimate inequality in a random\nsystem where several individuals interact moving among many communities and\nproducing dependent random quantities of an attribute. The overall inequality\nis assessed by computing the Random Theil's Entropy. Firstly, the software\nestimates a piecewise homogeneous Markov chain by identifying the\nchanging-points and the relative transition probability matrices. Secondly, it\nestimates the multivariate distribution function of the attribute using a\ncopula function approach and finally, through a Monte Carlo algorithm,\nevaluates the expected value of the Random Theil's Entropy. Possible\napplications are discussed as related to the fields of finance and human\nmobility\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09107v1"
    },
    {
        "title": "Domain Specific Concept Drift Detectors for Predicting Financial Time\n  Series",
        "authors": [
            "Filippo Neri"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Concept drift detectors allow learning systems to maintain good accuracy on\nnon-stationary data streams. Financial time series are an instance of\nnon-stationary data streams whose concept drifts (market phases) are so\nimportant to affect investment decisions worldwide. This paper studies how\nconcept drift detectors behave when applied to financial time series. General\nresults are: a) concept drift detectors usually improve the runtime over\ncontinuous learning, b) their computational cost is usually a fraction of the\nlearning and prediction steps of even basic learners, c) it is important to\nstudy concept drift detectors in combination with the learning systems they\nwill operate with, and d) concept drift detectors can be directly applied to\nthe time series of raw financial data and not only to the model's accuracy one.\nMoreover, the study introduces three simple concept drift detectors, tailored\nto financial time series, and shows that two of them can be at least as\neffective as the most sophisticated ones from the state of the art when applied\nto financial time series. Currently submitted to Pattern Recognition\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14079v3"
    },
    {
        "title": "Forecasting with Deep Learning: S&P 500 index",
        "authors": [
            "Firuz Kamalov",
            "Linda Smail",
            "Ikhlaas Gurrib"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Stock price prediction has been the focus of a large amount of research but\nan acceptable solution has so far escaped academics. Recent advances in deep\nlearning have motivated researchers to apply neural networks to stock\nprediction. In this paper, we propose a convolution-based neural network model\nfor predicting the future value of the S&P 500 index. The proposed model is\ncapable of predicting the next-day direction of the index based on the previous\nvalues of the index. Experiments show that our model outperforms a number of\nbenchmarks achieving an accuracy rate of over 55%.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14080v1"
    },
    {
        "title": "Stock price forecast with deep learning",
        "authors": [
            "Firuz Kamalov",
            "Linda Smail",
            "Ikhlaas Gurrib"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In this paper, we compare various approaches to stock price prediction using\nneural networks. We analyze the performance fully connected, convolutional, and\nrecurrent architectures in predicting the next day value of S&P 500 index based\non its previous values. We further expand our analysis by including three\ndifferent optimization techniques: Stochastic Gradient Descent, Root Mean\nSquare Propagation, and Adaptive Moment Estimation. The numerical experiments\nreveal that a single layer recurrent neural network with RMSprop optimizer\nproduces optimal results with validation and test Mean Absolute Error of 0.0150\nand 0.0148 respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14081v1"
    },
    {
        "title": "Isolating the impact of trading on grid frequency fluctuations",
        "authors": [
            "Benjamin Schäfer",
            "Marc Timme",
            "Dirk Witthaut"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  To ensure reliable operation of power grids, their frequency shall stay\nwithin strict bounds. Multiple sources of disturbances cause fluctuations of\nthe grid frequency, ranging from changing demand over volatile feed-in to\nenergy trading. Here, we analyze frequency time series from the continental\nEuropean grid in 2011 and 2017 as a case study to isolate the impact of\ntrading. We find that trading at typical trading intervals such as full hours\nmodifies the frequency fluctuation statistics. While particularly large\nfrequency deviations in 2017 are not as frequent as in 2011, large deviations\nare more likely to occur shortly after the trading instances. A comparison\nbetween the two years indicates that trading at shorter intervals might be\nbeneficial for frequency quality and grid stability, because particularly large\nfluctuations are substantially diminished. Furthermore, we observe that the\nstatistics of the frequency fluctuations do not follow Gaussian distributions\nbut are better described using heavy-tailed and asymmetric distributions, for\nexample L\\'evy-stable distributions. Comparing intervals without trading to\nthose with trading instances indicates that frequency deviations near the\ntrading times are distributed more widely and thus extreme deviations are\norders of magnitude more likely. Finally, we briefly review a stochastic\nanalysis that allows a quantitative description of power grid frequency\nfluctuations.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14592v1"
    },
    {
        "title": "Reliability of MST identification in correlation-based market networks",
        "authors": [
            "V. A. Kalyagin",
            "A. P. Koldanov",
            "P. A. Koldanov"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Maximum spanning tree (MST) is a popular tool in market network analysis.\nLarge number of publications are devoted to the MST calculation and it's\ninterpretation for particular stock markets. However, much less attention is\npayed in the literature to the analysis of uncertainty of obtained results. In\nthe present paper we suggest a general framework to measure uncertainty of MST\nidentification. We study uncertainty in the framework of the concept of random\nvariable network (RVN). We consider different correlation based networks in the\nlarge class of elliptical distributions. We show that true MST is the same in\nthree networks: Pearson correlation network, Fechner correlation network, and\nKendall correlation network. We argue that among different measures of\nuncertainty the FDR (False Discovery Rate) is the most appropriated for MST\nidentification. We investigate FDR of Kruskal algorithm for MST identification\nand show that reliability of MST identification is different in these three\nnetworks. In particular, for Pearson correlation network the FDR essentially\ndepends on distribution of stock returns. We prove that for market network with\nFechner correlation the FDR is non sensitive to the assumption on stock's\nreturn distribution. Some interesting phenomena are discovered for Kendall\ncorrelation network. Our experiments show that FDR of Kruskal algorithm for MST\nidentification in Kendall correlation network weakly depend on distribution and\nat the same time the value of FDR is almost the best in comparison with MST\nidentification in other networks. These facts are important in practical\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14593v1"
    },
    {
        "title": "Text Mining of Stocktwits Data for Predicting Stock Prices",
        "authors": [
            "Mukul Jaggi",
            "Priyanka Mandal",
            "Shreya Narang",
            "Usman Naseem",
            "Matloob Khushi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Stock price prediction can be made more efficient by considering the price\nfluctuations and understanding the sentiments of people. A limited number of\nmodels understand financial jargon or have labelled datasets concerning stock\nprice change. To overcome this challenge, we introduced FinALBERT, an ALBERT\nbased model trained to handle financial domain text classification tasks by\nlabelling Stocktwits text data based on stock price change. We collected\nStocktwits data for over ten years for 25 different companies, including the\nmajor five FAANG (Facebook, Amazon, Apple, Netflix, Google). These datasets\nwere labelled with three labelling techniques based on stock price changes. Our\nproposed model FinALBERT is fine-tuned with these labels to achieve optimal\nresults. We experimented with the labelled dataset by training it on\ntraditional machine learning, BERT, and FinBERT models, which helped us\nunderstand how these labels behaved with different model architectures. Our\nlabelling method competitive advantage is that it can help analyse the\nhistorical data effectively, and the mathematical function can be easily\ncustomised to predict stock movement.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.16388v1"
    },
    {
        "title": "Cryptocurrency co-investment network: token returns reflect investment\n  patterns",
        "authors": [
            "Luca Mungo",
            "Silvia Bartolucci",
            "Laura Alessandretti"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Since the introduction of Bitcoin in 2009, the dramatic and unsteady\nevolution of the cryptocurrency market has also been driven by large\ninvestments by traditional and cryptocurrency-focused hedge funds.\nNotwithstanding their critical role, our understanding of the relationship\nbetween institutional investments and the evolution of the cryptocurrency\nmarket has remained limited, also due to the lack of comprehensive data\ndescribing investments over time. In this study, we present a quantitative\nstudy of cryptocurrency institutional investments based on a dataset collected\nfor 1324 currencies in the period between 2014 and 2022 from Crunchbase, one of\nthe largest platforms gathering business information. We show that the\nevolution of the cryptocurrency market capitalization is highly correlated with\nthe size of institutional investments, thus confirming their important role.\nFurther, we find that the market is dominated by the presence of a group of\nprominent investors who tend to specialise by focusing on particular\ntechnologies. Finally, studying the co-investment network of currencies that\nshare common investors, we show that assets with shared investors tend to be\ncharacterized by similar market behavior. Our work sheds light on the role\nplayed by institutional investors and provides a basis for further research on\ntheir influence in the cryptocurrency ecosystem.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.02027v3"
    },
    {
        "title": "Non-linear correlation analysis in financial markets using hierarchical\n  clustering",
        "authors": [
            "J. E. Salgado-Hernández",
            "Manan Vyas"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Distance correlation coefficient (DCC) can be used to identify new\nassociations and correlations between multiple variables. The distance\ncorrelation coefficient applies to variables of any dimension, can be used to\ndetermine smaller sets of variables that provide equivalent information, is\nzero only when variables are independent, and is capable of detecting nonlinear\nassociations that are undetectable by the classical Pearson correlation\ncoefficient (PCC). Hence, DCC provides more information than the PCC. We\nanalyze numerous pairs of stocks in S\\&P500 database with the distance\ncorrelation coefficient and provide an overview of stochastic evolution of\nfinancial market states based on these correlation measures obtained using\nagglomerative clustering.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.05080v1"
    },
    {
        "title": "Price impact in equity auctions: zero, then linear",
        "authors": [
            "Mohammed Salek",
            "Damien Challet",
            "Ioane Muni Toke"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Using high-quality data, we report several statistical regularities of equity\nauctions in the Paris stock exchange. First, the average order book density is\nlinear around the auction price at the time of auction clearing and has a large\npeak at the auction price. While the peak is due to slow traders, the order\ndensity shape is the result of subtle dynamics. The impact of a new market\norder or cancellation at the auction time can be decomposed into three parts as\na function of the size of the additional order: (1) zero impact, caused by the\ndiscrete nature of prices, sometimes up to a surprisingly large additional\nvolume relative to the auction volume (2) linear impact for additional orders\nup to a large fraction of the auction volume (3) for even larger orders price\nimpact is non-linear, frequently super-linear.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.05677v2"
    },
    {
        "title": "Sequential Graph Attention Learning for Predicting Dynamic Stock Trends\n  (Student Abstract)",
        "authors": [
            "Tzu-Ya Lai",
            "Wen Jung Cheng",
            "Jun-En Ding"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The stock market is characterized by a complex relationship between companies\nand the market. This study combines a sequential graph structure with attention\nmechanisms to learn global and local information within temporal time.\nSpecifically, our proposed \"GAT-AGNN\" module compares model performance across\nmultiple industries as well as within single industries. The results show that\nthe proposed framework outperforms the state-of-the-art methods in predicting\nstock trends across multiple industries on Taiwan Stock datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.10153v1"
    },
    {
        "title": "Effects of long memory in the order submission process on the properties\n  of recurrence intervals of large price fluctuations",
        "authors": [
            "Hao Meng",
            "Fei Ren",
            "Gao-Feng Gu",
            "Xiong Xiong",
            "Yong-Jie Zhang",
            "Wei-Xing Zhou",
            "Wei Zhang"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Understanding the statistical properties of recurrence intervals of extreme\nevents is crucial to risk assessment and management of complex systems. The\nprobability distributions and correlations of recurrence intervals for many\nsystems have been extensively investigated. However, the impacts of microscopic\nrules of a complex system on the macroscopic properties of its recurrence\nintervals are less studied. In this Letter, we adopt an order-driven stock\nmarket model to address this issue for stock returns. We find that the\ndistributions of the scaled recurrence intervals of simulated returns have a\npower law scaling with stretched exponential cutoff and the intervals possess\nmultifractal nature, which are consistent with empirical results. We further\ninvestigate the effects of long memory in the directions (or signs) and\nrelative prices of the order flow on the characteristic quantities of these\nproperties. It is found that the long memory in the order directions (Hurst\nindex $H_s$) has a negligible effect on the interval distributions and the\nmultifractal nature. In contrast, the power-law exponent of the interval\ndistribution increases linearly with respect to the Hurst index $H_x$ of the\nrelative prices, and the singularity width of the multifractal nature\nfluctuates around a constant value when $H_x<0.7$ and then increases with\n$H_x$. No evident effects of $H_s$ and $H_x$ are found on the long memory of\nthe recurrence intervals. Our results indicate that the nontrivial properties\nof the recurrence intervals of returns are mainly caused by traders' behaviors\nof persistently placing new orders around the best bid and ask prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.2825v1"
    },
    {
        "title": "Multifractal Height Cross-Correlation Analysis: A New Method for\n  Analyzing Long-Range Cross-Correlations",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We introduce a new method for detection of long-range cross-correlations and\nmultifractality - multifractal height cross-correlation analysis (MF-HXA) -\nbased on scaling of qth order covariances. MF-HXA is a bivariate generalization\nof the height-height correlation analysis of Barabasi & Vicsek [Barabasi, A.L.,\nVicsek, T.: Multifractality of self-affine fractals, Physical Review A 44(4),\n1991]. The method can be used to analyze long-range cross-correlations and\nmultifractality between two simultaneously recorded series. We illustrate a\npower of the method on both simulated and real-world time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3473v2"
    },
    {
        "title": "How are rescaled range analyses affected by different memory and\n  distributional properties? A Monte Carlo study",
        "authors": [
            "Ladislav Kristoufek"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  In this paper, we present the results of Monte Carlo simulations for two\npopular techniques of long-range correlations detection - classical and\nmodified rescaled range analyses. A focus is put on an effect of different\ndistributional properties on an ability of the methods to efficiently\ndistinguish between short and long-term memory. To do so, we analyze the\nbehavior of the estimators for independent, short-range dependent, and\nlong-range dependent processes with innovations from 8 different distributions.\nWe find that apart from a combination of very high levels of kurtosis and\nskewness, both estimators are quite robust to distributional properties.\nImportantly, we show that R/S is biased upwards (yet not strongly) for\nshort-range dependent processes, while M-R/S is strongly biased downwards for\nlong-range dependent processes regardless of the distribution of innovations.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3511v1"
    },
    {
        "title": "Determinants of immediate price impacts at the trade level in an\n  emerging order-driven market",
        "authors": [
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The common wisdom argues that, in general, large trades cause large price\nchanges, while small trades cause small price changes. However, for extremely\nlarge price changes, the trade size and news play a minor role, while the\nliquidity (especially price gaps on the limit order book) is a more influencing\nfactor. Hence, there might be other influencing factors of immediate price\nimpacts of trades. In this paper, through mechanical analysis of price\nvariations before and after a trade of arbitrary size, we identify that the\ntrade size, the bid-ask spread, the price gaps and the outstanding volumes at\nthe bid and ask sides of the limit order book have impacts on the changes of\nprices. We propose two regression models to investigate the influences of these\nmicroscopic factors on the price impact of buyer-initiated partially filled\ntrades, seller-initiated partially filled trades, buyer-initiated filled\ntrades, and seller-initiated filled trades. We find that they have\nquantitatively similar explanation powers and these factors can account for up\nto 44% of the price impacts. Large trade sizes, wide bid-ask spreads, high\nliquidity at the same side and low liquidity at the opposite side will cause a\nlarge price impact. We also find that the liquidity at the opposite side has a\nmore influencing impact than the liquidity at the same side. Our results shed\nnew lights on the determinants of immediate price impacts.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.5448v1"
    },
    {
        "title": "Modeling electricity spot prices using mean-reverting multifractal\n  processes",
        "authors": [
            "Martin Rypdal",
            "Ola Løvsletten"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We discuss stochastic modeling of volatility persistence and\nanti-correlations in electricity spot prices, and for this purpose we present\ntwo mean-reverting versions of the multifractal random walk (MRW). In the first\nmodel the anti-correlations are modeled in the same way as in an\nOrnstein-Uhlenbeck process, i.e. via a drift (damping) term, and in the second\nmodel the anti-correlations are included by letting the innovations in the MRW\nmodel be fractional Gaussian noise with H < 1/2. For both models we present\napproximate maximum likelihood methods, and we apply these methods to estimate\nthe parameters for the spot prices in the Nordic electricity market. The\nmaximum likelihood estimates show that electricity spot prices are\ncharacterized by scaling exponents that are significantly different from the\ncorresponding exponents in stock markets, confirming the exceptional nature of\nthe electricity market. In order to compare the damped MRW model with the\nfractional MRW model we use ensemble simulations and wavelet-based variograms,\nand we observe that certain features of the spot prices are better described by\nthe damped MRW model. The characteristic correlation time is estimated to\napproximately half a year.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.6137v1"
    },
    {
        "title": "An Empirical Method to Measure Stochasticity and Multifractality in\n  Nonlinear Time Series",
        "authors": [
            "Chih-Hao Lin",
            "Chia-Seng Chang",
            "Sai-Ping Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  An empirical algorithm is used here to study the stochastic and multifractal\nnature of nonlinear time series. A parameter can be defined to quantitatively\nmeasure the deviation of the time series from a Wiener process so that the\nstochasticity of different time series can be compared. The local volatility of\nthe time series under study can be constructed using this algorithm and the\nmultifractal structure of the time series can be analyzed by using this local\nvolatility. As an example, we employ this method to analyze financial time\nseries from different stock markets. The result shows that while developed\nmarkets evolve very much like an Ito process, the emergent markets are far from\nefficient. Differences about the multifractal structures and leverage effects\nbetween developed and emergent markets are discussed. The algorithm used here\ncan be applied in a similar fashion to study time series of other complex\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.1292v1"
    },
    {
        "title": "Complex temporal structure of activity in on-line electronic auctions",
        "authors": [
            "Frantisek Slanina"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We analyze empirical data from the internet auction site Aukro.cz. The time\nseries of activity shows truncated fractal structure on scales from about 1\nminute to about 1 day. The distribution of waiting times as well as the\ndistribution of number of auctions within fixed interval is a power law, with\nexponents $1.5$ and $3$, respectively. Possible implications for the modeling\nof stock-market fluctuations are briefly discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.2860v1"
    },
    {
        "title": "Correlation structure and principal components in global crude oil\n  market",
        "authors": [
            "Yue-Hua Dai",
            "Wen-Jie Xie",
            "Zhi-Qiang Jiang",
            "George J. Jiang",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  This article investigates the correlation structure of the global crude oil\nmarket using the daily returns of 71 oil price time series across the world\nfrom 1992 to 2012. We identify from the correlation matrix six clusters of time\nseries exhibiting evident geographical traits, which supports Weiner's (1991)\nregionalization hypothesis of the global oil market. We find that intra-cluster\npairs of time series are highly correlated while inter-cluster pairs have\nrelatively low correlations. Principal component analysis shows that most\neigenvalues of the correlation matrix locate outside the prediction of the\nrandom matrix theory and these deviating eigenvalues and their corresponding\neigenvectors contain rich economic information. Specifically, the largest\neigenvalue reflects a collective effect of the global market, other four\nlargest eigenvalues possess a partitioning function to distinguish the six\nclusters, and the smallest eigenvalues highlight the pairs of time series with\nthe largest correlation coefficients. We construct an index of the global oil\nmarket based on the eigenfortfolio of the largest eigenvalue, which evolves\nsimilarly as the average price time series and has better performance than the\nbenchmark $1/N$ portfolio under the buy-and-hold strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.5000v1"
    },
    {
        "title": "Big Data, Socio-Psychological Theory, Algorithmic Text Analysis and\n  Predicting the Michigan Consumer Sentiment Index",
        "authors": [
            "Rickard Nyman",
            "Paul Ormerod"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We describe an exercise of using Big Data to predict the Michigan Consumer\nSentiment Index, a widely used indicator of the state of confidence in the US\neconomy. We carry out the exercise from a pure ex ante perspective. We use the\nmethodology of algorithmic text analysis of an archive of brokers' reports over\nthe period June 2010 through June 2013. The search is directed by the\nsocial-psychological theory of agent behaviour, namely conviction narrative\ntheory. We compare one month ahead forecasts generated this way over a 15 month\nperiod with the forecasts reported for the consensus predictions of Wall Street\neconomists. The former give much more accurate predictions, getting the\ndirection of change correct on 12 of the 15 occasions compared to only 7 for\nthe consensus predictions. We show that the approach retains significant\npredictive power even over a four month ahead horizon.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.5695v1"
    },
    {
        "title": "Localization in covariance matrices of coupled heterogenous\n  Ornstein-Uhlenbeck processes",
        "authors": [
            "Paolo Barucca"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We define a random-matrix ensemble given by the infinite-time covariance\nmatrices of Ornstein-Uhlenbeck processes at different temperatures coupled by a\nGaussian symmetric matrix. The spectral properties of this ensemble are shown\nto be in qualitative agreement with some stylized facts of financial markets.\nThrough the presented model formulas are given for the analysis of\nheterogeneous time-series. Furthermore evidence for a localization transition\nin eigenvectors related to small and large eigenvalues in cross-correlations\nanalysis of this model is found and a simple explanation of localization\nphenomena in financial time-series is provided. Finally we identify both in our\nmodel and in real financial data an inverted-bell effect in correlation between\nlocalized components and their local temperature: high and low\ntemperature/volatility components are the most localized ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.2031v2"
    },
    {
        "title": "Power law scaling and \"Dragon-Kings\" in distributions of intraday\n  financial drawdowns",
        "authors": [
            "Vladimir Filimonov",
            "Didier Sornette"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We investigate the distributions of epsilon-drawdowns and epsilon-drawups of\nthe most liquid futures financial contracts of the world at time scales of 30\nseconds. The epsilon-drawdowns (resp. epsilon- drawups) generalise the notion\nof runs of negative (resp. positive) returns so as to capture the risks to\nwhich investors are arguably the most concerned with. Similarly to the\ndistribution of returns, we find that the distributions of epsilon-drawdowns\nand epsilon-drawups exhibit power law tails, albeit with exponents\nsignificantly larger than those for the return distributions. This paradoxical\nresult can be attributed to (i) the existence of significant transient\ndependence between returns and (ii) the presence of large outliers\n(dragon-kings) characterizing the extreme tail of the drawdown/drawup\ndistributions deviating from the power law. The study of the tail dependence\nbetween the sizes, speeds and durations of drawdown/drawup indicates a clear\nrelationship between size and speed but none between size and duration. This\nimplies that the most extreme drawdown/drawup tend to occur fast and are\ndominated by a few very large returns. We discuss both the endogenous and\nexogenous origins of these extreme events.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.5037v2"
    },
    {
        "title": "Permutation approach, high frequency trading and variety of micro\n  patterns in financial time series",
        "authors": [
            "Cina Aghamohammadi",
            "Mehran Ebrahimian",
            "Hamed Tahmooresi"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Permutation approach is suggested as a method to investigate financial time\nseries in micro scales. The method is used to see how high frequency trading in\nrecent years has affected the micro patterns which may be seen in financial\ntime series. Tick to tick exchange rates are considered as examples. It is seen\nthat variety of patterns evolve through time; and that the scale over which the\ntarget markets have no dominant patterns, have decreased steadily over time\nwith the emergence of higher frequency trading.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.5254v1"
    },
    {
        "title": "Analyses of Statistical Structures in Economic Indices",
        "authors": [
            "Frank W. K. Firk"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  The complex, time-dependent statistical structures observed in the Dow Jones\nIndustrial Average on a typical trading day are modeled with Lorentzian\nfunctions. The resonant-like structures are characterized by the values of the\nbasic ratio: the average lifetime of the individual states associated with a\ngiven structural form divided by the average interval between adjacent states.\nValues of the ratio are determined for three structural forms characterized by\nthe average intervals: 50 to 100 seconds (the fine structure), approximately10\nminutes, and approximately1 hour (the intermediate structures I and II). During\nthe trading day the values of the basic ratio associated with the fine\nstructure of the index are found to lie in the narrow range from 0.49 to 0.52.\nThis finding is characteristic of the highly statistical nature of many-body\nsystems typified by daily trading. It is therefore proposed that the value of\nthis ratio, determined in the first hour-or-so on a given day, be used to\nprovide information concerning the likely performance of the fine, statistical\ncomponent of the index for the remainder of the trading day. For the\nintermediate structures the basic ratios are approximately 0.6 and therefore\nthey too can be analyzed as individual states.\n  Keywords: Analytical economics; Lorentzian analyses of statistical structures\nin the Dow Jones Industrial Average; basic parameters of economic indices.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.02216v1"
    },
    {
        "title": "Irreversibility of financial time series: a graph-theoretical approach",
        "authors": [
            "Lucas Lacasa",
            "Ryan Flanagan"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The relation between time series irreversibility and entropy production has\nbeen recently investigated in thermodynamic systems operating away from\nequilibrium. In this work we explore this concept in the context of financial\ntime series. We make use of visibility algorithms to quantify in\ngraph-theoretical terms time irreversibility of 35 financial indices evolving\nover the period 1998-2012. We show that this metric is complementary to\nstandard measures based on volatility and exploit it to both classify periods\nof financial stress and to rank companies accordingly. We then validate this\napproach by finding that a projection in principal components space of\nfinancial years based on time irreversibility features clusters together\nperiods of financial stress from stable periods. Relations between\nirreversibility, efficiency and predictability are briefly discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.01980v1"
    },
    {
        "title": "Decomposition of Time Series Data of Stock Markets and its Implications\n  for Prediction: An Application for the Indian Auto Sector",
        "authors": [
            "Jaydip Sen",
            "Tamal Datta Chaudhuri"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  With the rapid development and evolution of sophisticated algorithms for\nstatistical analysis of time series data, the research community has started\nspending considerable effort in technical analysis of such data. Forecasting is\nalso an area which has witnessed a paradigm shift in its approach. In this\nwork, we have used the time series of the index values of the Auto sector in\nIndia during January 2010 to December 2015 for a deeper understanding of the\nbehavior of its three constituent components, e.g., the Trend, the Seasonal\ncomponent, and the Random component. Based on this structural analysis, we have\nalso designed three approaches for forecasting and also computed their accuracy\nin prediction using suitably chosen training and test data sets. The results\nclearly demonstrate the accuracy of our decomposition results and efficiency of\nour forecasting techniques, even in presence of a dominant Random component in\nthe time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.02407v1"
    },
    {
        "title": "A comparison among some Hurst exponent approaches to predict nascent\n  bubbles in $500$ company stocks",
        "authors": [
            "M. Fernández-Martínez",
            "M. A Sánchez-Granero",
            "María José Muñoz Torrecillas",
            "Bill McKelvey"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  In this paper, three approaches to calculate the self-similarity exponent of\na time series are compared in order to determine which one performs best to\nidentify the transition from random efficient market behavior (EM) to herding\nbehavior (HB) and hence, to find out the beginning of a market bubble. In\nparticular, classical Detrended Fluctuation Analysis (DFA), Generalized Hurst\nExponent (GHE) and GM2 (one of Geometric Method-based algorithms) were applied\nfor self-similarity exponent calculation purposes. Traditionally, researchers\nhave been focused on identifying the beginning of a crash. Instead of this, we\nare pretty interested in identifying the beginning of the transition process\nfrom EM to a market bubble onset, what we consider could be more interesting.\nThe relevance of self-similarity index in such a context lies on the fact that\nit becomes a suitable indicator which allows to identify the raising of HB in\nfinancial markets. Overall, we could state that the greater the self-similarity\nexponent in financial series, the more likely the transition process to HB\ncould start. This fact is illustrated through actual S\\&P500 stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.04188v1"
    },
    {
        "title": "Cross-response in correlated financial markets: individual stocks",
        "authors": [
            "Shanshan Wang",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Previous studies of the stock price response to trades focused on the\ndynamics of single stocks, i.e. they addressed the self-response. We\nempirically investigate the price response of one stock to the trades of other\nstocks in a correlated market, i.e. the cross-responses. How large is the\nimpact of one stock on others and vice versa? -- This impact of trades on the\nprice change across stocks appears to be transient instead of permanent as we\ndiscuss from the viewpoint of market efficiency. Furthermore, we compare the\nself-responses on different scales and the self- and cross-responses on the\nsame scale. We also find that the cross-correlation of the trade signs turns\nout to be a short-memory process.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01580v2"
    },
    {
        "title": "Average cross-responses in correlated financial market",
        "authors": [
            "Shanshan Wang",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  There are non-vanishing price responses across different stocks in correlated\nfinancial markets. We further study this issue by performing different\naverages, which identify active and passive cross-responses. The two average\ncross-responses show different characteristic dependences on the time lag. The\npassive cross-response exhibits a shorter response period with sizeable\nvolatilities, while the corresponding period for the active cross-response is\nlonger. The average cross-responses for a given stock are evaluated either with\nrespect to the whole market or to different sectors. Using the response\nstrength, the influences of individual stocks are identified and discussed.\nMoreover, the various cross-responses as well as the average cross-responses\nare compared with the self-responses. In contrast, the short memory of trade\nsign cross-correlation for stock pairs, the sign cross-correlation has long\nmemory when averaged over different pairs of stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01586v2"
    },
    {
        "title": "Extracting Predictive Information from Heterogeneous Data Streams using\n  Gaussian Processes",
        "authors": [
            "Sid Ghoshal",
            "Stephen Roberts"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Financial markets are notoriously complex environments, presenting vast\namounts of noisy, yet potentially informative data. We consider the problem of\nforecasting financial time series from a wide range of information sources\nusing online Gaussian Processes with Automatic Relevance Determination (ARD)\nkernels. We measure the performance gain, quantified in terms of Normalised\nRoot Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearson\ncorrelation, from fusing each of four separate data domains: time series\ntechnicals, sentiment analysis, options market data and broker recommendations.\nWe show evidence that ARD kernels produce meaningful feature rankings that help\nretain salient inputs and reduce input dimensionality, providing a framework\nfor sifting through financial complexity. We measure the performance gain from\nfusing each domain's heterogeneous data streams into a single probabilistic\nmodel. In particular our findings highlight the critical value of options data\nin mapping out the curvature of price space and inspire an intuitive, novel\ndirection for research in financial prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.06202v2"
    },
    {
        "title": "On clustering financial time series: a need for distances between\n  dependent random variables",
        "authors": [
            "Gautier Marti",
            "Frank Nielsen",
            "Philippe Donnat",
            "Sébastien Andler"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The following working document summarizes our work on the clustering of\nfinancial time series. It was written for a workshop on information geometry\nand its application for image and signal processing. This workshop brought\nseveral experts in pure and applied mathematics together with applied\nresearchers from medical imaging, radar signal processing and finance. The\nauthors belong to the latter group. This document was written as a long\nintroduction to further development of geometric tools in financial\napplications such as risk or portfolio analysis. Indeed, risk and portfolio\nanalysis essentially rely on covariance matrices. Besides that the Gaussian\nassumption is known to be inaccurate, covariance matrices are difficult to\nestimate from empirical data. To filter noise from the empirical estimate,\nMantegna proposed using hierarchical clustering. In this work, we first show\nthat this procedure is statistically consistent. Then, we propose to use\nclustering with a much broader application than the filtering of empirical\ncovariance matrices from the estimate correlation coefficients. To be able to\ndo that, we need to obtain distances between the financial time series that\nincorporate all the available information in these cross-dependent random\nprocesses.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.07822v1"
    },
    {
        "title": "Coherence and incoherence collective behavior in financial market",
        "authors": [
            "Shangmei Zhao",
            "Qiuchao Xie",
            "Qing Lu",
            "Xin Jiang",
            "Wei Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Financial markets have been extensively studied as highly complex evolving\nsystems. In this paper, we quantify financial price fluctuations through a\ncoupled dynamical system composed of phase oscillators. We find a Financial\nCoherence and Incoherence (FCI) coexistence collective behavior emerges as the\nsystem evolves into the stable state, in which the stocks split into two\ngroups: one is represented by coherent, phase-locked oscillators, the other is\ncomposed of incoherent, drifting oscillators. It is demonstrated that the size\nof the coherent stock groups fluctuates during the economic periods according\nto real-world financial instabilities or shocks. Further, we introduce the\ncoherent characteristic matrix to characterize the involvement dynamics of\nstocks in the coherent groups. Clustering results on the matrix provides a\nnovel manifestation of the correlations among stocks in the economic periods.\nOur analysis for components of the groups is consistent with the Global\nIndustry Classification Standard (GICS) classification and can also figure out\nfeatures for newly developed industries. These results can provide potentially\nimplications on characterizing inner dynamical structure of financial markets\nand making optimal investment tragedies.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.02283v1"
    },
    {
        "title": "Volatility Forecasts Using Nonlinear Leverage Effects",
        "authors": [
            "Kenichiro McAlinn",
            "Asahi Ushio",
            "Teruo Nakatsuma"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The leverage effect-- the correlation between an asset's return and its\nvolatility-- has played a key role in forecasting and understanding volatility\nand risk. While it is a long standing consensus that leverage effects exist and\nimprove forecasts, empirical evidence paradoxically do not show that most\nindividual stocks exhibit this phenomena, mischaracterizing risk and therefore\nleading to poor predictive performance. We examine this paradox, with the goal\nto improve density forecasts, by relaxing the assumption of linearity in the\nleverage effect. Nonlinear generalizations of the leverage effect are proposed\nwithin the Bayesian stochastic volatility framework in order to capture\nflexible leverage structures, where small fluctuations in prices have a\ndifferent effect from large shocks. Efficient Bayesian sequential computation\nis developed and implemented to estimate this effect in a practical, on-line\nmanner. Examining 615 stocks that comprise the S\\&P500 and Nikkei 225, we find\nthat relaxing the linear assumption to our proposed nonlinear leverage effect\nfunction improves predictive performances for 89\\% of all stocks compared to\nthe conventional model assumption.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.06482v4"
    },
    {
        "title": "A unified approach to mortality modelling using state-space framework:\n  characterisation, identification, estimation and forecasting",
        "authors": [
            "Man Chung Fung",
            "Gareth W. Peters",
            "Pavel V. Shevchenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  This paper explores and develops alternative statistical representations and\nestimation approaches for dynamic mortality models. The framework we adopt is\nto reinterpret popular mortality models such as the Lee-Carter class of models\nin a general state-space modelling methodology, which allows modelling,\nestimation and forecasting of mortality under a unified framework. Furthermore,\nwe propose an alternative class of model identification constraints which is\nmore suited to statistical inference in filtering and parameter estimation\nsettings based on maximization of the marginalized likelihood or in Bayesian\ninference. We then develop a novel class of Bayesian state-space models which\nincorporate apriori beliefs about the mortality model characteristics as well\nas for more flexible and appropriate assumptions relating to heteroscedasticity\nthat present in observed mortality data. We show that multiple period and\ncohort effect can be cast under a state-space structure. To study long term\nmortality dynamics, we introduce stochastic volatility to the period effect.\nThe estimation of the resulting stochastic volatility model of mortality is\nperformed using a recent class of Monte Carlo procedure specifically designed\nfor state and parameter estimation in Bayesian state-space models, known as the\nclass of particle Markov chain Monte Carlo methods. We illustrate the framework\nwe have developed using Danish male mortality data, and show that incorporating\nheteroscedasticity and stochastic volatility markedly improves model fit\ndespite an increase of model complexity. Forecasting properties of the enhanced\nmodels are examined with long term and short term calibration periods on the\nreconstruction of life tables.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.09484v1"
    },
    {
        "title": "Aftershocks following crash of currency exchange rate: The case of\n  RUB/USD in 2014",
        "authors": [
            "Vasilya Usmanova",
            "Yury V. Lysogorskiy",
            "Sumiyoshi Abe"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The dynamical behavior of the currency exchange rate after its large-scale\ncatastrophe is discussed through a case study of the rate of Russian rubles to\nUS dollars after its crash in 2014. It is shown that, similarly to the case of\nthe stock market crash, the relaxation is characterized by a power law, which\nis in analogy with the Omori-Utsu law for earthquake aftershocks. The\nwaiting-time distribution is found to also obey a power law. Furthermore, the\nevent-event correlation is discussed, and the aging phenomenon and scaling\nproperty are observed. Comments are made on (non-)Markovianity of the\naftershock process and on a possible relevance of glassy dynamics to the market\nsystem after the crash.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.03246v2"
    },
    {
        "title": "Transfer entropy between communities in complex networks",
        "authors": [
            "Jan Korbel",
            "Xiongfei Jiang",
            "Bo Zheng"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  With the help of transfer entropy, we analyze information flows between\ncommunities of complex networks. We show that the transfer entropy provides a\ncoherent description of interactions between communities, including non-linear\ninteractions. To put some flesh on the bare bones, we analyze transfer\nentropies between communities of five largest financial markets, represented as\nnetworks of interacting stocks. Additionally, we discuss information transfer\nof rare events, which is analyzed by R\\'enyi transfer entropy.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.05543v1"
    },
    {
        "title": "Identification of Credit Risk Based on Cluster Analysis of Account\n  Behaviours",
        "authors": [
            "Maha Bakoben",
            "Tony Bellotti",
            "Niall Adams"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Assessment of risk levels for existing credit accounts is important to the\nimplementation of bank policies and offering financial products. This paper\nuses cluster analysis of behaviour of credit card accounts to help assess\ncredit risk level. Account behaviour is modelled parametrically and we then\nimplement the behavioural cluster analysis using a recently proposed\ndissimilarity measure of statistical model parameters. The advantage of this\nnew measure is the explicit exploitation of uncertainty associated with\nparameters estimated from statistical models. Interesting clusters of real\ncredit card behaviours data are obtained, in addition to superior prediction\nand forecasting of account default based on the clustering outcomes.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.07466v1"
    },
    {
        "title": "Connecting Sharpe ratio and Student t-statistic, and beyond",
        "authors": [
            "Eric Benhamou"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Sharpe ratio is widely used in asset management to compare and benchmark\nfunds and asset managers. It computes the ratio of the excess return over the\nstrategy standard deviation. However, the elements to compute the Sharpe ratio,\nnamely, the expected returns and the volatilities are unknown numbers and need\nto be estimated statistically. This means that the Sharpe ratio used by funds\nis subject to be error prone because of statistical estimation error. Lo\n(2002), Mertens (2002) derive explicit expressions for the statistical\ndistribution of the Sharpe ratio using standard asymptotic theory under several\nsets of assumptions (independent normally distributed - and identically\ndistributed returns). In this paper, we provide the exact distribution of the\nSharpe ratio for independent normally distributed return. In this case, the\nSharpe ratio statistic is up to a rescaling factor a non centered Student\ndistribution whose characteristics have been widely studied by statisticians.\nThe asymptotic behavior of our distribution provide the result of Lo (2002). We\nalso illustrate the fact that the empirical Sharpe ratio is asymptotically\noptimal in the sense that it achieves the Cramer Rao bound. We then study the\nempirical SR under AR(1) assumptions and investigate the effect of compounding\nperiod on the Sharpe (computing the annual Sharpe with monthly data for\ninstance). We finally provide general formula in this case of\nheteroscedasticity and autocorrelation.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.04233v4"
    },
    {
        "title": "Identifying long-term precursors of financial market crashes using\n  correlation patterns",
        "authors": [
            "Hirdesh K. Pharasi",
            "Kiran Sharma",
            "Rakesh Chatterjee",
            "Anirban Chakraborti",
            "Francois Leyvraz",
            "Thomas H. Seligman"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  The study of the critical dynamics in complex systems is always interesting\nyet challenging. Here, we choose financial market as an example of a complex\nsystem, and do a comparative analyses of two stock markets - the S&P 500 (USA)\nand Nikkei 225 (JPN). Our analyses are based on the evolution of\ncrosscorrelation structure patterns of short time-epochs for a 32-year period\n(1985-2016). We identify \"market states\" as clusters of similar correlation\nstructures, which occur more frequently than by pure chance (randomness). The\ndynamical transitions between the correlation structures reflect the evolution\nof the market states. Power mapping method from the random matrix theory is\nused to suppress the noise on correlation patterns, and an adaptation of the\nintra-cluster distance method is used to obtain the \"optimum\" number of market\nstates. We find that the USA is characterized by four market states and JPN by\nfive. We further analyze the co-occurrence of paired market states; the\nprobability of remaining in the same state is much higher than the transition\nto a different state. The transitions to other states mainly occur among the\nimmediately adjacent states, with a few rare intermittent transitions to the\nremote states. The state adjacent to the critical state (market crash) may\nserve as an indicator or a \"precursor\" for the critical state and this novel\nmethod of identifying the long-term precursors may be very helpful for\nconstructing the early warning system in financial markets, as well as in other\ncomplex systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.00885v2"
    },
    {
        "title": "A Bayesian GED-Gamma stochastic volatility model for return data: a\n  marginal likelihood approach",
        "authors": [
            "T. R. Santos"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Several studies explore inferences based on stochastic volatility (SV)\nmodels, taking into account the stylized facts of return data. The common\nproblem is that the latent parameters of many volatility models are\nhigh-dimensional and analytically intractable, which means inferences require\napproximations using, for example, the Markov Chain Monte Carlo or Laplace\nmethods. Some SV models are expressed as a linear Gaussian state-space model\nthat leads to a marginal likelihood, reducing the dimensionality of the\nproblem. Others are not linearized, and the latent parameters are integrated\nout. However, these present a quite restrictive evolution equation. Thus, we\npropose a Bayesian GED-Gamma SV model with a direct marginal likelihood that is\na product of the generalized Student's t-distributions in which the latent\nstates are related across time through a stationary Gaussian evolution\nequation. Then, an approximation is made for the prior distribution of\nlog-precision/volatility, without the need for model linearization. This also\nallows for the computation of the marginal likelihood function, where the\nhigh-dimensional latent states are integrated out and easily sampled in blocks\nusing a smoothing procedure. In addition, extensions of our GED-Gamma model are\neasily made to incorporate skew heavy-tailed distributions. We use the Bayesian\nestimator for the inference of static parameters, and perform a simulation\nstudy on several properties of the estimator. Our results show that the\nproposed model can be reasonably estimated. Furthermore, we provide case\nstudies of a Brazilian asset and the pound/dollar exchange rate to show the\nperformance of our approach in terms of fit and prediction.\n  Keywords: SV model, New sequential and smoothing procedures, Generalized\nStudent's t-distribution, Non-Gaussian errors, Heavy tails, Skewness\n",
        "pdf_link": "http://arxiv.org/pdf/1809.01489v1"
    },
    {
        "title": "Order book model with herd behavior exhibiting long-range memory",
        "authors": [
            "Aleksejus Kononovicius",
            "Julius Ruseckas"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In this work, we propose an order book model with herd behavior. The proposed\nmodel is built upon two distinct approaches: a recent empirical study of the\ndetailed order book records by Kanazawa et al. [Phys. Rev. Lett. 120, 138301]\nand financial herd behavior model. Combining these approaches allows us to\npropose a model that replicates the long-range memory of absolute returns and\ntrading activity. We compare the statistical properties of the model against\nthe empirical statistical properties of the Bitcoin exchange rates and New York\nstock exchange tickers. We also show that the fracture in the spectral density\nof the high-frequency absolute return time series might be related to the\nmechanism of convergence towards the equilibrium price.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.02772v3"
    },
    {
        "title": "Complex market dynamics in the light of random matrix theory",
        "authors": [
            "Hirdesh K. Pharasi",
            "Kiran Sharma",
            "Anirban Chakraborti",
            "Thomas H. Seligman"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We present a brief overview of random matrix theory (RMT) with the objectives\nof highlighting the computational results and applications in financial markets\nas complex systems. An oft-encountered problem in computational finance is the\nchoice of an appropriate epoch over which the empirical cross-correlation\nreturn matrix is computed. A long epoch would smoothen the fluctuations in the\nreturn time series and suffers from non-stationarity, whereas a short epoch\nresults in noisy fluctuations in the return time series and the correlation\nmatrices turn out to be highly singular. An effective method to tackle this\nissue is the use of the power mapping, where a non-linear distortion is applied\nto a short epoch correlation matrix. The value of distortion parameter controls\nthe noise-suppression. The distortion also removes the degeneracy of zero\neigenvalues. Depending on the correlation structures, interesting properties of\nthe eigenvalue spectra are found. We simulate different correlated Wishart\nmatrices to compare the results with empirical return matrices computed using\nthe S&P 500 (USA) market data for the period 1985-2016. We also briefly review\ntwo recent applications of RMT in financial stock markets: (i) Identification\nof \"market states\" and long-term precursor to a critical state; (ii)\nCharacterization of catastrophic instabilities (market crashes).\n",
        "pdf_link": "http://arxiv.org/pdf/1809.07100v2"
    },
    {
        "title": "The market nanostructure origin of asset price time reversal asymmetry",
        "authors": [
            "Marcus Cordi",
            "Damien Challet",
            "Serge Kassibrakis"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We introduce a framework to infer lead-lag networks between the states of\nelements of complex systems, determined at different timescales. As such\nnetworks encode the causal structure of a system, infering lead-lag networks\nfor many pairs of timescales provides a global picture of the mutual influence\nbetween timescales. We apply our method to two trader-resolved FX data sets and\ndocument strong and complex asymmetric influence of timescales on the structure\nof lead-lag networks. Expectedly, this asymmetry extends to trader activity:\nfor institutional clients in our dataset, past activity on timescales longer\nthan 3 hours is more correlated with future activity at shorter timescales than\nthe opposite (Zumbach effect), while a reverse Zumbach effect is found for past\ntimescales shorter than 3 hours; retail clients have a totally different, and\nmuch more intricate, structure of asymmetric timescale influence. The causality\nstructures are clearly caused by markedly different behaviors of the two types\nof traders. Hence, market nanostructure, i.e., market dynamics at the\nindividual trader level, provides an unprecedented insight into the causality\nstructure of financial markets, which is much more complex than previously\nthought.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.00834v3"
    },
    {
        "title": "Clustering patterns in efficiency and the coming-of-age of the\n  cryptocurrency market",
        "authors": [
            "Higor Y. D. Sigaki",
            "Matjaz Perc",
            "Haroldo V. Ribeiro"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The efficient market hypothesis has far-reaching implications for financial\ntrading and market stability. Whether or not cryptocurrencies are\ninformationally efficient has therefore been the subject of intense recent\ninvestigation. Here, we use permutation entropy and statistical complexity over\nsliding time-windows of price log returns to quantify the dynamic efficiency of\nmore than four hundred cryptocurrencies. We consider that a cryptocurrency is\nefficient within a time-window when these two complexity measures are\nstatistically indistinguishable from their values obtained on randomly shuffled\ndata. We find that 37% of the cryptocurrencies in our study stay efficient over\n80% of the time, whereas 20% are informationally efficient in less than 20% of\nthe time. Our results also show that the efficiency is not correlated with the\nmarket capitalization of the cryptocurrencies. A dynamic analysis of\ninformational efficiency over time reveals clustering patterns in which\ndifferent cryptocurrencies with similar temporal patterns form four clusters,\nand moreover, younger currencies in each group appear poised to follow the\ntrend of their 'elders'. The cryptocurrency market thus already shows notable\nadherence to the efficient market hypothesis, although data also reveals that\nthe coming-of-age of digital currencies is in this regard still very much\nunderway.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04967v1"
    },
    {
        "title": "Forecasting security's volatility using low-frequency historical data,\n  high-frequency historical data and option-implied volatility",
        "authors": [
            "Huiling Yuan",
            "Yong Zhou",
            "Zhiyuan Zhang",
            "Xiangyu Cui"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Low-frequency historical data, high-frequency historical data and option data\nare three major sources, which can be used to forecast the underlying\nsecurity's volatility. In this paper, we propose two econometric models, which\nintegrate three information sources. In GARCH-It\\^{o}-OI model, we assume that\nthe option-implied volatility can influence the security's future volatility,\nand the option-implied volatility is treated as an observable exogenous\nvariable. In GARCH-It\\^{o}-IV model, we assume that the option-implied\nvolatility can not influence the security's volatility directly, and the\nrelationship between the option-implied volatility and the security's\nvolatility is constructed to extract useful information of the underlying\nsecurity. After providing the quasi-maximum likelihood estimators for the\nparameters and establishing their asymptotic properties, we also conduct a\nseries of simulation analysis and empirical analysis to compare the proposed\nmodels with other popular models in the literature. We find that when the\nsampling interval of the high-frequency data is 5 minutes, the GARCH-It\\^{o}-OI\nmodel and GARCH-It\\^{o}-IV model has better forecasting performance than other\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.02666v1"
    },
    {
        "title": "From quadratic Hawkes processes to super-Heston rough volatility models\n  with Zumbach effect",
        "authors": [
            "Aditi Dandapani",
            "Paul Jusselin",
            "Mathieu Rosenbaum"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Using microscopic price models based on Hawkes processes, it has been shown\nthat under some no-arbitrage condition, the high degree of endogeneity of\nmarkets together with the phenomenon of metaorders splitting generate rough\nHeston-type volatility at the macroscopic scale. One additional important\nfeature of financial dynamics, at the heart of several influential works in\neconophysics, is the so-called feedback or Zumbach effect. This essentially\nmeans that past trends in returns convey significant information on future\nvolatility. A natural way to reproduce this property in microstructure modeling\nis to use quadratic versions of Hawkes processes. We show that after suitable\nrescaling, the long term limits of these processes are refined versions of\nrough Heston models where the volatility coefficient is enhanced compared to\nthe square root characterizing Heston-type dynamics. Furthermore the Zumbach\neffect remains explicit in these limiting rough volatility models.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.06151v2"
    },
    {
        "title": "Evaluating the Effectiveness of Common Technical Trading Models",
        "authors": [
            "Joseph Attia"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  How effective are the most common trading models? The answer may help\ninvestors realize upsides to using each model, act as a segue for investors\ninto more complex financial analysis and machine learning, and to increase\nfinancial literacy amongst students. Creating original versions of popular\nmodels, like linear regression, K-Nearest Neighbor, and moving average\ncrossovers, we can test how each model performs on the most popular stocks and\nlargest indexes. With the results for each, we can compare the models, and\nunderstand which model reliably increases performance. The trials showed that\nwhile all three models reduced losses on stocks with strong overall downward\ntrends, the two machine learning models did not work as well to increase\nprofits. Moving averages crossovers outperformed a continuous investment every\ntime, although did result in a more volatile investment as well. Furthermore,\nonce finished creating the program that implements moving average crossover,\nwhat are the optimal periods to use? A massive test consisting of 169,880\ntrials, showed the best periods to use to increase investment performance\n(5,10) and to decrease volatility (33,44). In addition, the data showed\nnumerous trends such as a smaller short SMA period is accompanied by higher\nperformance. Plotting volatility against performance shows that the high risk,\nhigh reward saying holds true and shows that for investments, as the volatility\nincreases so does its performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.10407v1"
    },
    {
        "title": "Investigating the effect of competitiveness power in estimating the\n  average weighted price in electricity market",
        "authors": [
            "Naser Rostamni",
            "Tarik A. Rashid"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This paper evaluates the impact of the power extent on price in the\nelectricity market. The competitiveness extent of the electricity market during\nspecific times in a day is considered to achieve this. Then, the effect of\ncompetitiveness extent on the forecasting precision of the daily power price is\nassessed. A price forecasting model based on multi-layer perception via back\npropagation with the Levenberg-Marquardt mechanism is used. The Residual Supply\nIndex (RSI) and other variables that affect prices are used as inputs to the\nmodel to evaluate the market competitiveness. The results show that using\nmarket power indices as inputs helps to increase forecasting accuracy. Thus,\nthe competitiveness extent of the market power in different daily time periods\nis a notable variable in price formation. Moreover, market players cannot\nignore the explanatory power of market power in price forecasting. In this\nresearch, the real data of the electricity market from 2013 is used and the\nmain source of data is the Grid Management Company in Iran.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11984v1"
    },
    {
        "title": "Marked Hawkes process modeling of price dynamics and volatility\n  estimation",
        "authors": [
            "Kyungsub Lee",
            "Byoung Ki Seo"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  A simple Hawkes model have been developed for the price tick structure\ndynamics incorporating market microstructure noise and trade clustering. In\nthis paper, the model is extended with random mark to deal with more realistic\nprice tick structures of equities. We examine the impact of jump in price\ndynamics to the future movements and dependency between the jump sizes and\nground intensities. We also derive the volatility formula based on stochastic\nand statistical methods and compare with realized volatility in simulation and\nempirical studies. The marked Hawkes model is useful to estimate the intraday\nvolatility similarly in the case of simple Hawkes model.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12025v1"
    },
    {
        "title": "Phase separation and scaling in correlation structures of financial\n  markets",
        "authors": [
            "Anirban Chakraborti",
            " Hrishidev",
            "Kiran Sharma",
            "Hirdesh K. Pharasi"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Financial markets, being spectacular examples of complex systems, display\nrich correlation structures among price returns of different assets. The\ncorrelation structures change drastically, akin to phase transitions in\nphysical phenomena, as do the influential stocks (leaders) and sectors\n(communities), during market events like crashes. It is crucial to detect their\nsignatures for timely intervention or prevention. Here we use eigenvalue\ndecomposition and eigen-entropy, computed from eigen-centralities of different\nstocks in the cross-correlation matrix, to extract information about the\ndisorder in the market. We construct a `phase space', where different market\nevents (bubbles, crashes, etc.) undergo phase separation and display\norder-disorder transitions. An entropy functional exhibits scaling behavior. We\npropose a generic indicator that facilitates the continuous monitoring of the\ninternal structure of the market -- important for managing risk and\nstress-testing the financial system. Our methodology would help in\nunderstanding and foreseeing tipping points or fluctuation patterns in complex\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06242v3"
    },
    {
        "title": "Stochastic leverage effect in high-frequency data: a Fourier based\n  analysis",
        "authors": [
            "Imma Valentina Curato",
            "Simona Sanfelici"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The stochastic leverage effect, defined as the standardized covariation\nbetween the returns and their related volatility, is analyzed in a stochastic\nvolatility model set-up. A novel estimator of the effect is defined using a\npre-estimation of the Fourier coefficients of the return and the volatility\nprocesses. The consistency of the estimator is proven. Moreover, its finite\nsample properties are studied in the presence of microstructure noise effects.\nThe Fourier methodology is applied to S\\&P500 futures prices to investigate the\nmagnitude of the stochastic leverage effect detectable at high-frequency.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06660v3"
    },
    {
        "title": "CorrGAN: Sampling Realistic Financial Correlation Matrices Using\n  Generative Adversarial Networks",
        "authors": [
            "Gautier Marti"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We propose a novel approach for sampling realistic financial correlation\nmatrices. This approach is based on generative adversarial networks.\nExperiments demonstrate that generative adversarial networks are able to\nrecover most of the known stylized facts about empirical correlation matrices\nestimated on asset returns. This is the first time such results are documented\nin the literature. Practical financial applications range from trading\nstrategies enhancement to risk and portfolio stress testing. Such generative\nmodels can also help ground empirical finance deeper into science by allowing\nfor falsifiability of statements and more objective comparison of empirical\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09504v2"
    },
    {
        "title": "Applications of deep learning in stock market prediction: recent\n  progress",
        "authors": [
            "Weiwei Jiang"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Stock market prediction has been a classical yet challenging problem, with\nthe attention from both economists and computer scientists. With the purpose of\nbuilding an effective prediction model, both linear and machine learning tools\nhave been explored for the past couple of decades. Lately, deep learning models\nhave been introduced as new frontiers for this topic and the rapid development\nis too fast to catch up. Hence, our motivation for this survey is to give a\nlatest review of recent works on deep learning models for stock market\nprediction. We not only category the different data sources, various neural\nnetwork structures, and common used evaluation metrics, but also the\nimplementation and reproducibility. Our goal is to help the interested\nresearchers to synchronize with the latest progress and also help them to\neasily reproduce the previous studies as baselines. Base on the summary, we\nalso highlight some future research directions in this topic.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.01859v1"
    },
    {
        "title": "Investigating the influence Brexit had on Financial Markets, in\n  particular the GBP/EUR exchange rate",
        "authors": [
            "Michael Filletti"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  On 23rd June 2016, 51.9% of British voters voted to leave the European Union,\ntriggering a process and events that have led to the United Kingdom leaving the\nEU, an event that has become known as 'Brexit'. In this piece of research, we\ninvestigate the effects of this entire process on the currency markets,\nspecifically the GBP/EUR exchange rate. Financial markets are known to be\nsensitive to news articles and media, and the aim of this research is to\nevaluate the magnitude of impact of relevant events, as well as whether the\nimpact was positive or negative for the GBP.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.05895v1"
    },
    {
        "title": "The Low-volatility Anomaly and the Adaptive Multi-Factor Model",
        "authors": [
            "Robert A. Jarrow",
            "Rinald Murataj",
            "Martin T. Wells",
            "Liao Zhu"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The paper provides a new explanation of the low-volatility anomaly. We use\nthe Adaptive Multi-Factor (AMF) model estimated by the Groupwise Interpretable\nBasis Selection (GIBS) algorithm to find those basis assets significantly\nrelated to low and high volatility portfolios. These two portfolios load on\nvery different factors, indicating that volatility is not an independent risk,\nbut that it's related to existing risk factors. The out-performance of the\nlow-volatility portfolio is due to the (equilibrium) performance of these\nloaded risk factors. The AMF model outperforms the Fama-French 5-factor model\nboth in-sample and out-of-sample.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.08302v2"
    },
    {
        "title": "Market structure dynamics during COVID-19 outbreak",
        "authors": [
            "Pier Francesco Procacci",
            "Carolyn E. Phelan",
            "Tomaso Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this note, we discuss the impact of the COVID-19 outbreak from the\nperspective of the market-structure. We observe that the US market-structure\nhas dramatically changed during the past four weeks and that the level of\nchange has followed the number of infected cases reported in the USA.\nPresently, market-structure resembles most closely the structure during the\nmiddle of the 2008 crisis but there are signs that it may be starting to evolve\ninto a new structure altogether. This is the first article of a series where we\nwill be analyzing and discussing market-structure as it evolves to a state of\nfurther instability or, more optimistically, stabilization and recovery.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10922v1"
    },
    {
        "title": "Company classification using machine learning",
        "authors": [
            "Sven Husmann",
            "Antoniya Shivarova",
            "Rick Steinert"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The recent advancements in computational power and machine learning\nalgorithms have led to vast improvements in manifold areas of research.\nEspecially in finance, the application of machine learning enables both\nresearchers and practitioners to gain new insights into financial data and\nwell-studied areas such as company classification. In our paper, we demonstrate\nthat unsupervised machine learning algorithms can be used to visualize and\nclassify company data in an economically meaningful and effective way. In\nparticular, we implement the data-driven dimension reduction and visualization\ntool t-distributed stochastic neighbor embedding (t-SNE) in combination with\nspectral clustering. The resulting company groups can then be utilized by\nexperts in the field for empirical analysis and optimal decision making. By\nproviding an exemplary out-of-sample study within a portfolio optimization\nframework, we show that the application of t-SNE and spectral clustering\nimproves the overall portfolio performance. Therefore, we introduce our\napproach to the financial community as a valuable technique in the context of\ndata analysis and company classification.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01496v2"
    },
    {
        "title": "Deep learning for Stock Market Prediction",
        "authors": [
            "Mojtaba Nabipour",
            "Pooyan Nayyeri",
            "Hamed Jabani",
            "Amir Mosavi"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Prediction of stock groups' values has always been attractive and challenging\nfor shareholders. This paper concentrates on the future prediction of stock\nmarket groups. Four groups named diversified financials, petroleum,\nnon-metallic minerals and basic metals from Tehran stock exchange are chosen\nfor experimental evaluations. Data are collected for the groups based on ten\nyears of historical records. The values predictions are created for 1, 2, 5,\n10, 15, 20 and 30 days in advance. The machine learning algorithms utilized for\nprediction of future values of stock market groups. We employed Decision Tree,\nBagging, Random Forest, Adaptive Boosting (Adaboost), Gradient Boosting and\neXtreme Gradient Boosting (XGBoost), and Artificial neural network (ANN),\nRecurrent Neural Network (RNN) and Long short-term memory (LSTM). Ten technical\nindicators are selected as the inputs into each of the prediction models.\nFinally, the result of predictions is presented for each technique based on\nthree metrics. Among all the algorithms used in this paper, LSTM shows more\naccurate results with the highest model fitting ability. Also, for tree-based\nmodels, there is often an intense competition between Adaboost, Gradient\nBoosting, and XGBoost.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01497v1"
    },
    {
        "title": "Financial Market Trend Forecasting and Performance Analysis Using LSTM",
        "authors": [
            "Jonghyeon Min"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The financial market trend forecasting method is emerging as a hot topic in\nfinancial markets today. Many challenges still currently remain, and various\nresearches related thereto have been actively conducted. Especially, recent\nresearch of neural network-based financial market trend prediction has\nattracted much attention. However, previous researches do not deal with the\nfinancial market forecasting method based on LSTM which has good performance in\ntime series data. There is also a lack of comparative analysis in the\nperformance of neural network-based prediction techniques and traditional\nprediction techniques. In this paper, we propose a financial market trend\nforecasting method using LSTM and analyze the performance with existing\nfinancial market trend forecasting methods through experiments. This method\nprepares the input data set through the data preprocessing process so as to\nreflect all the fundamental data, technical data and qualitative data used in\nthe financial data analysis, and makes comprehensive financial market analysis\nthrough LSTM. In this paper, we experiment and compare performances of existing\nfinancial market trend forecasting models, and performance according to the\nfinancial market environment. In addition, we implement the proposed method\nusing open sources and platform and forecast financial market trends using\nvarious financial data indicators.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01502v1"
    },
    {
        "title": "The leverage effect and other stylized facts displayed by Bitcoin\n  returns",
        "authors": [
            "F. N. M. de Sousa Filho",
            "J. N. Silva",
            "M. A. Bertella",
            "E. Brigatti"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper, we explore some stylized facts of the Bitcoin market using the\nBTC-USD exchange rate time series of historical intraday data from 2013 to\n2020. Bitcoin presents some very peculiar idiosyncrasies, like the absence of\nmacroeconomic fundamentals or connections with underlying assets or benchmarks,\nan asymmetry between demand and supply and the presence of inefficiency in the\nform of strong arbitrage opportunity. Nevertheless, all these elements seem to\nbe marginal in the definition of the structural statistical properties of this\nvirtual financial asset, which result to be analogous to general individual\nstocks or indices. In contrast, we find some clear differences, compared to\nfiat money exchange rates time series, in the values of the linear\nautocorrelation and, more surprisingly, in the presence of the leverage effect.\nWe also explore the dynamics of correlations, monitoring the shifts in the\nevolution of the Bitcoin market. This analysis is able to distinguish between\ntwo different regimes: a stochastic process with weaker memory signatures and\ncloser to Gaussianity between the Mt. Gox incident and the late 2015, and a\ndynamics with relevant correlations and strong deviations from Gaussianity\nbefore and after this interval.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.05870v2"
    },
    {
        "title": "Examining Lead-Lag Relationships In-Depth, With Focus On FX Market As\n  Covid-19 Crises Unfolds",
        "authors": [
            "Kartikay Gupta",
            "Niladri Chatterjee"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The lead-lag relationship plays a vital role in financial markets. It is the\nphenomenon where a certain price-series lags behind and partially replicates\nthe movement of leading time-series. The present research proposes a new\ntechnique which helps better identify the lead-lag relationship empirically.\nApart from better identifying the lead-lag path, the technique also gives a\nmeasure for adjudging closeness between financial time-series. Also, the\nproposed measure is closely related to correlation, and it uses Dynamic\nProgramming technique for finding the optimal lead-lag path. Further, it\nretains most of the properties of a metric, so much so, it is termed as loose\nmetric. Tests are performed on Synthetic Time Series (STS) with known lead-lag\nrelationship and comparisons are done with other state-of-the-art models on the\nbasis of significance and forecastability. The proposed technique gives the\nbest results in both the tests. It finds paths which are all statistically\nsignificant, and its forecasts are closest to the target values. Then, we use\nthe measure to study the topology evolution of the Foreign Exchange market, as\nthe COVID-19 pandemic unfolds. Here, we study the FX currency prices of 29\nprominent countries of the world. It is observed that as the crises unfold, all\nthe currencies become strongly interlinked to each other. Also, USA Dollar\nstarts playing even more central role in the FX market. Finally, we mention\nseveral other application areas of the proposed technique for designing\nintelligent systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.10560v2"
    },
    {
        "title": "Impact of COVID-19 on Forecasting Stock Prices: An Integration of\n  Stationary Wavelet Transform and Bidirectional Long Short-Term Memory",
        "authors": [
            "Daniel Štifanić",
            "Jelena Musulin",
            "Adrijana Miočević",
            "Sandi Baressi Šegota",
            "Roman Šubić",
            "Zlatan Car"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  COVID-19 is an infectious disease that mostly affects the respiratory system.\nAt the time of this research being performed, there were more than 1.4 million\ncases of COVID-19, and one of the biggest anxieties is not just our health, but\nour livelihoods, too. In this research, authors investigate the impact of\nCOVID-19 on the global economy, more specifically, the impact of COVID-19 on\nfinancial movement of Crude Oil price and three U.S. stock indexes: DJI, S&P\n500 and NASDAQ Composite. The proposed system for predicting commodity and\nstock prices integrates the Stationary Wavelet Transform (SWT) and\nBidirectional Long Short-Term Memory (BDLSTM) networks. Firstly, SWT is used to\ndecompose the data into approximation and detail coefficients. After\ndecomposition, data of Crude Oil price and stock market indexes along with\nCOVID-19 confirmed cases were used as input variables for future price movement\nforecasting. As a result, the proposed system BDLSTM+WT-ADA achieved\nsatisfactory results in terms of five-day Crude Oil price forecast.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.02673v1"
    },
    {
        "title": "Fourier instantaneous estimators and the Epps effect",
        "authors": [
            "Patrick Chang"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We compare the Malliavin-Mancino and Cuchiero-Teichmann Fourier instantaneous\nestimators to investigate the impact of the Epps effect arising from asynchrony\nin the instantaneous estimates. We demonstrate the instantaneous Epps effect\nunder a simulation setting and provide a simple method to ameliorate the\neffect. We find that using the previous tick interpolation in the\nCuchiero-Teichmann estimator results in unstable estimates when dealing with\nasynchrony, while the ability to bypass the time domain with the\nMalliavin-Mancino estimator allows it to produce stable estimates and is\ntherefore better suited for ultra-high frequency finance. An empirical analysis\nusing Trade and Quote data from the Johannesburg Stock Exchange illustrates the\ninstantaneous Epps effect and how the intraday correlation dynamics can vary\nbetween days for the same equity pair.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.03453v2"
    },
    {
        "title": "Estimation of time-varying kernel densities and chronology of the impact\n  of COVID-19 on financial markets",
        "authors": [
            "Matthieu Garcin",
            "Jules Klein",
            "Sana Laaribi"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The time-varying kernel density estimation relies on two free parameters: the\nbandwidth and the discount factor. We propose to select these parameters so as\nto minimize a criterion consistent with the traditional requirements of the\nvalidation of a probability density forecast. These requirements are both the\nuniformity and the independence of the so-called probability integral\ntransforms, which are the forecast time-varying cumulated distributions applied\nto the observations. We thus build a new numerical criterion incorporating both\nthe uniformity and independence properties by the mean of an adapted\nKolmogorov-Smirnov statistic. We apply this method to financial markets during\nthe COVID-19 crisis. We determine the time-varying density of daily price\nreturns of several stock indices and, using various divergence statistics, we\nare able to describe the chronology of the crisis as well as regional\ndisparities. For instance, we observe a more limited impact of COVID-19 on\nfinancial markets in China, a strong impact in the US, and a slow recovery in\nEurope.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.09043v2"
    },
    {
        "title": "Generating Trading Signals by ML algorithms or time series ones?",
        "authors": [
            "Omid Safarzadeh"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This research investigates efficiency on-line learning Algorithms to generate\ntrading signals.I employed technical indicators based on high frequency stock\nprices and generated trading signals through ensemble of Random Forests.\nSimilarly, Kalman Filter was used for signaling trading positions. Comparing\nTime Series methods with Machine Learning methods, results spurious of Kalman\nFilter to Random Forests in case of on-line learning predictions of stock\nprices\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11098v1"
    },
    {
        "title": "A Novel Ensemble Deep Learning Model for Stock Prediction Based on Stock\n  Prices and News",
        "authors": [
            "Yang Li",
            "Yi Pan"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In recent years, machine learning and deep learning have become popular\nmethods for financial data analysis, including financial textual data,\nnumerical data, and graphical data. This paper proposes to use sentiment\nanalysis to extract useful information from multiple textual data sources and a\nblending ensemble deep learning model to predict future stock movement. The\nblending ensemble model contains two levels. The first level contains two\nRecurrent Neural Networks (RNNs), one Long-Short Term Memory network (LSTM) and\none Gated Recurrent Units network (GRU), followed by a fully connected neural\nnetwork as the second level model. The RNNs, LSTM, and GRU models can\neffectively capture the time-series events in the input data, and the fully\nconnected neural network is used to ensemble several individual prediction\nresults to further improve the prediction accuracy. The purpose of this work is\nto explain our design philosophy and show that ensemble deep learning\ntechnologies can truly predict future stock price trends more effectively and\ncan better assist investors in making the right investment decision than other\ntraditional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12620v1"
    },
    {
        "title": "The role of global economic policy uncertainty in predicting crude oil\n  futures volatility: Evidence from a two-factor GARCH-MIDAS model",
        "authors": [
            "Peng-Fei Dai",
            "Xiong Xiong",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This paper aims to examine whether the global economic policy uncertainty\n(GEPU) and uncertainty changes have different impacts on crude oil futures\nvolatility. We establish single-factor and two-factor models under the\nGARCH-MIDAS framework to investigate the predictive power of GEPU and GEPU\nchanges excluding and including realized volatility. The findings show that the\nmodels with rolling-window specification perform better than those with\nfixed-span specification. For single-factor models, the GEPU index and its\nchanges, as well as realized volatility, are consistent effective factors in\npredicting the volatility of crude oil futures. Specially, GEPU changes have\nstronger predictive power than the GEPU index. For two-factor models, GEPU is\nnot an effective forecast factor for the volatility of WTI crude oil futures or\nBrent crude oil futures. The two-factor model with GEPU changes contains more\ninformation and exhibits stronger forecasting ability for crude oil futures\nmarket volatility than the single-factor models. The GEPU changes are indeed\nthe main source of long-term volatility of the crude oil futures.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12838v1"
    },
    {
        "title": "Are low frequency macroeconomic variables important for high frequency\n  electricity prices?",
        "authors": [
            "Claudia Foroni",
            "Francesco Ravazzolo",
            "Luca Rossini"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Recent research finds that forecasting electricity prices is very relevant.\nIn many applications, it might be interesting to predict daily electricity\nprices by using their own lags or renewable energy sources. However, the recent\nturmoil of energy prices and the Russian-Ukrainian war increased attention in\nevaluating the relevance of industrial production and the Purchasing Managers'\nIndex output survey in forecasting the daily electricity prices. We develop a\nBayesian reverse unrestricted MIDAS model which accounts for the mismatch in\nfrequency between the daily prices and the monthly macro variables in Germany\nand Italy. We find that the inclusion of macroeconomic low frequency variables\nis more important for short than medium term horizons by means of point and\ndensity measures. In particular, accuracy increases by combining hard and soft\ninformation, while using only surveys gives less accurate forecasts than using\nonly industrial production data.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.13566v2"
    },
    {
        "title": "Investment sizing with deep learning prediction uncertainties for\n  high-frequency Eurodollar futures trading",
        "authors": [
            "Trent Spears",
            "Stefan Zohren",
            "Stephen Roberts"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this work we show that prediction uncertainty estimates gleaned from deep\nlearning models can be useful inputs for influencing the relative allocation of\nrisk capital across trades. In this way, consideration of uncertainty is\nimportant because it permits the scaling of investment size across trade\nopportunities in a principled and data-driven way. We showcase this insight\nwith a prediction model and find clear outperformance based on a Sharpe ratio\nmetric, relative to trading strategies that either do not take uncertainty into\naccount, or that utilize an alternative market-based statistic as a proxy for\nuncertainty. Of added novelty is our modelling of high-frequency data at the\ntop level of the Eurodollar Futures limit order book for each trading day of\n2018, whereby we predict interest rate curve changes on small time horizons. We\nare motivated to study the market for these popularly-traded interest rate\nderivatives since it is deep and liquid, and contributes to the efficient\nfunctioning of global finance -- though there is relatively little by way of\nits modelling contained in the academic literature. Hence, we verify the\nutility of prediction models and uncertainty estimates for trading applications\nin this complex and multi-dimensional asset price space.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.15982v1"
    },
    {
        "title": "Evaluation of company investment value based on machine learning",
        "authors": [
            "Junfeng Hu",
            "Xiaosa Li",
            "Yuru Xu",
            "Shaowu Wu",
            "Bin Zheng"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper, company investment value evaluation models are established\nbased on comprehensive company information. After data mining and extracting a\nset of 436 feature parameters, an optimal subset of features is obtained by\ndimension reduction through tree-based feature selection, followed by the\n5-fold cross-validation using XGBoost and LightGBM models. The results show\nthat the Root-Mean-Square Error (RMSE) reached 3.098 and 3.059, respectively.\nIn order to further improve the stability and generalization capability,\nBayesian Ridge Regression has been used to train a stacking model based on the\nXGBoost and LightGBM models. The corresponding RMSE is up to 3.047. Finally,\nthe importance of different features to the LightGBM model is analysed.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.01996v1"
    },
    {
        "title": "Unconventional Policies Effects on Stock Market Volatility: A MAP\n  Approach",
        "authors": [
            "Demetrio Lacava",
            "Giampiero M. Gallo",
            "Edoardo Otranto"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Taking the European Central Bank unconventional policies as a reference, we\nsuggest a class of Multiplicative Error Models (MEM) taylored to analyze the\nimpact such policies have on stock market volatility. The new set of models,\ncalled MEM with Asymmetry and Policy effects (MAP), keeps the base volatility\ndynamics separate from a component reproducing policy effects, with an increase\nin volatility on announcement days and a decrease unfolding implementation\neffects. When applied to four Eurozone markets, a Model Confidence Set approach\nfinds a significant improvement of the forecasting power of the proxy after the\nExpanded Asset Purchase Programme implementation; a multi--step ahead\nforecasting exercise estimates the duration of the effect, and, by shocking the\npolicy variable, we are able to quantify the reduction in volatility which is\nmore marked for debt--troubled countries.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08259v2"
    },
    {
        "title": "Is Image Encoding Beneficial for Deep Learning in Finance? An Analysis\n  of Image Encoding Methods for the Application of Convolutional Neural\n  Networks in Finance",
        "authors": [
            "Dan Wang",
            "Tianrui Wang",
            "Ionuţ Florescu"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In 2012, SEC mandated all corporate filings for any company doing business in\nUS be entered into the Electronic Data Gathering, Analysis, and Retrieval\n(EDGAR) system. In this work we are investigating ways to analyze the data\navailable through EDGAR database. This may serve portfolio managers (pension\nfunds, mutual funds, insurance, hedge funds) to get automated insights into\ncompanies they invest in, to better manage their portfolios. The analysis is\nbased on Artificial Neural Networks applied to the data.} In particular, one of\nthe most popular machine learning methods, the Convolutional Neural Network\n(CNN) architecture, originally developed to interpret and classify images, is\nnow being used to interpret financial data. This work investigates the best way\nto input data collected from the SEC filings into a CNN architecture. We\nincorporate accounting principles and mathematical methods into the design of\nthree image encoding methods. Specifically, two methods are derived from\naccounting principles (Sequential Arrangement, Category Chunk Arrangement) and\none is using a purely mathematical technique (Hilbert Vector Arrangement). In\nthis work we analyze fundamental financial data as well as financial ratio data\nand study companies from the financial, healthcare and IT sectors in the United\nStates. We find that using imaging techniques to input data for CNN works\nbetter for financial ratio data but is not significantly better than simply\nusing the 1D input directly for fundamental data. We do not find the Hilbert\nVector Arrangement technique to be significantly better than other imaging\ntechniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08698v1"
    },
    {
        "title": "Financial Data Analysis Using Expert Bayesian Framework For Bankruptcy\n  Prediction",
        "authors": [
            "Amir Mukeri",
            "Habibullah Shaikh",
            "D. P. Gaikwad"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In recent years, bankruptcy forecasting has gained lot of attention from\nresearchers as well as practitioners in the field of financial risk management.\nFor bankruptcy prediction, various approaches proposed in the past and\ncurrently in practice relies on accounting ratios and using statistical\nmodeling or machine learning methods. These models have had varying degrees of\nsuccesses. Models such as Linear Discriminant Analysis or Artificial Neural\nNetwork employ discriminative classification techniques. They lack explicit\nprovision to include prior expert knowledge. In this paper, we propose another\nroute of generative modeling using Expert Bayesian framework. The biggest\nadvantage of the proposed framework is an explicit inclusion of expert judgment\nin the modeling process. Also the proposed methodology provides a way to\nquantify uncertainty in prediction. As a result the model built using Bayesian\nframework is highly flexible, interpretable and intuitive in nature. The\nproposed approach is well suited for highly regulated or safety critical\napplications such as in finance or in medical diagnosis. In such cases accuracy\nin the prediction is not the only concern for decision makers. Decision makers\nand other stakeholders are also interested in uncertainty in the prediction as\nwell as interpretability of the model. We empirically demonstrate these\nbenefits of proposed framework on real world dataset using Stan, a\nprobabilistic programming language. We found that the proposed model is either\ncomparable or superior to the other existing methods. Also resulting model has\nmuch less False Positive Rate compared to many existing state of the art\nmethods. The corresponding R code for the experiments is available at Github\nrepository.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.13892v2"
    },
    {
        "title": "Price response functions and spread impact in correlated financial\n  markets",
        "authors": [
            "Juan C. Henao-Londono",
            "Sebastian M. Krause",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Recent research on the response of stock prices to trading activity revealed\nlong lasting effects, even across stocks of different companies. These results\nimply non-Markovian effects in price formation and when trading many stocks at\nthe same time, in particular trading costs and price correlations. How the\nprice response is measured depends on data set and research focus. However, it\nis important to clarify, how the details of the price response definition\nmodify the results. Here, we evaluate different price response implementations\nfor the Trades and Quotes (TAQ) data set from the NASDAQ stock market and find\nthat the results are qualitatively the same for two different definitions of\ntime scale, but the response can vary by up to a factor of two. Further, we\nshow the key importance of the order between trade signs and returns,\ndisplaying the changes in the signal strength. Moreover, we confirm the\ndominating contribution of immediate price response directly after a trade, as\nwe find that delayed responses are suppressed. Finally, we test the impact of\nthe spread in the price response, detecting that large spreads have stronger\nimpact.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15105v1"
    },
    {
        "title": "Evaluating data augmentation for financial time series classification",
        "authors": [
            "Elizabeth Fons",
            "Paula Dawson",
            "Xiao-jun Zeng",
            "John Keane",
            "Alexandros Iosifidis"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Data augmentation methods in combination with deep neural networks have been\nused extensively in computer vision on classification tasks, achieving great\nsuccess; however, their use in time series classification is still at an early\nstage. This is even more so in the field of financial prediction, where data\ntends to be small, noisy and non-stationary. In this paper we evaluate several\naugmentation methods applied to stocks datasets using two state-of-the-art deep\nlearning models. The results show that several augmentation methods\nsignificantly improve financial performance when used in combination with a\ntrading strategy. For a relatively small dataset ($\\approx30K$ samples),\naugmentation methods achieve up to $400\\%$ improvement in risk adjusted return\nperformance; for a larger stock dataset ($\\approx300K$ samples), results show\nup to $40\\%$ improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15111v1"
    },
    {
        "title": "Event-Driven Learning of Systematic Behaviours in Stock Markets",
        "authors": [
            "Xianchao Wu"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  It is reported that financial news, especially financial events expressed in\nnews, provide information to investors' long/short decisions and influence the\nmovements of stock markets. Motivated by this, we leverage financial event\nstreams to train a classification neural network that detects latent\nevent-stock linkages and stock markets' systematic behaviours in the U.S. stock\nmarket. Our proposed pipeline includes (1) a combined event extraction method\nthat utilizes Open Information Extraction and neural co-reference resolution,\n(2) a BERT/ALBERT enhanced representation of events, and (3) an extended\nhierarchical attention network that includes attentions on event, news and\ntemporal levels. Our pipeline achieves significantly better accuracies and\nhigher simulated annualized returns than state-of-the-art models when being\napplied to predicting Standard\\&Poor 500, Dow Jones, Nasdaq indices and 10\nindividual stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15586v1"
    },
    {
        "title": "Time-Invariance Coefficients Tests with the Adaptive Multi-Factor Model",
        "authors": [
            "Liao Zhu",
            "Robert A. Jarrow",
            "Martin T. Wells"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The purpose of this paper is to test the time-invariance of the beta\ncoefficients estimated by the Adaptive Multi-Factor (AMF) model. The AMF model\nis implied by the generalized arbitrage pricing theory (GAPT), which implies\nconstant beta coefficients. The AMF model utilizes a Groupwise Interpretable\nBasis Selection (GIBS) algorithm to identify the relevant factors from among\nall traded ETFs. We compare the AMF model with the Fama-French 5-factor (FF5)\nmodel. We show that for nearly all time periods with length less than 6 years,\nthe beta coefficients are time-invariant for the AMF model, but not for the FF5\nmodel. This implies that the AMF model with a rolling window (such as 5 years)\nis more consistent with realized asset returns than is the FF5 model.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.04171v2"
    },
    {
        "title": "Comparing the market microstructure between two South African exchanges",
        "authors": [
            "Ivan Jericevich",
            "Patrick Chang",
            "Tim Gebbie"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We consider shared listings on two South African equity exchanges: the\nJohannesburg Stock Exchange (JSE) and the A2X Exchange. A2X is an alternative\nexchange that provides for both shared listings and new listings within the\nfinancial market ecosystem of South Africa. From a science perspective it\nprovides the opportunity to compare markets trading similar shares, in a\nsimilar regulatory and economic environment, but with vastly different\nliquidity, costs and business models. A2X currently has competitive settlement\nand transaction pricing when compared to the JSE, but the JSE has deeper\nliquidity. In pursuit of an empirical understanding of how these differences\nrelate to their respective price response dynamics, we compare the\ndistributions and auto-correlations of returns on different time scales; we\ncompare price impact and master curves; and we compare the cost of trading on\neach exchange. This allows us to empirically compare the two markets. We find\nthat various stylised facts become similar as the measurement or sampling time\nscale increase. However, the same securities can have vastly different price\nresponses irrespective of time scales. This is not surprising given the\ndifferent liquidity and order-book resilience. Here we demonstrate that direct\ncosts dominate the cost of trading, and the importance of competitively\npositioning cost ceilings. Universality is crucial for being able to\nmeaningfully compare cross-exchange price responses, but in the case of A2X, it\nhas yet to emerge in a meaningful way due to the infancy of the exchange --\nmaking meaningful comparisons difficult.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.04367v1"
    },
    {
        "title": "Pattern recognition in micro-trading behaviors before stock price jumps:\n  A framework based on multivariate time series analysis",
        "authors": [
            "Ao Kong",
            "Robert Azencott",
            "Hongliang Zhu",
            "Xindan Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Studying the micro-trading behaviors before stock price jumps is an important\nproblem for financial regulations and investment decisions. In this study, we\nprovide a new framework to study pre-jump trading behaviors based on\nmultivariate time series analysis. Different from the existing literature, our\nmethodology takes into account the temporal information embedded in the\ntrading-related attributes and can better evaluate and compare the abnormality\nlevels of different attributes. Moreover, it can explore the joint\ninformativeness of the attributes as well as select a subset of highly\ninformative but minimally redundant attributes to analyze the homogeneous and\nidiosyncratic patterns in the pre-jump trades of individual stocks. In\naddition, our analysis involves a set of technical indicators to describe\nmicro-trading behaviors. To illustrate the viability of the proposed\nmethodology, an application case is conducted based on the level-2 data of 189\nconstituent stocks of the China Security Index 300. The individual and joint\ninformativeness levels of the attributes in predicting price jumps are\nevaluated and compared. To this end, our experiment provides a set of jump\nindicators that can represent the pre-jump trading behaviors in the Chinese\nstock market and have detected some stocks with extremely abnormal pre-jump\ntrades.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.04939v2"
    },
    {
        "title": "Robust Analysis of Stock Price Time Series Using CNN and LSTM-Based Deep\n  Learning Models",
        "authors": [
            "Sidra Mehtab",
            "Jaydip Sen",
            "Subhasis Dasgupta"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Prediction of stock price and stock price movement patterns has always been a\ncritical area of research. While the well-known efficient market hypothesis\nrules out any possibility of accurate prediction of stock prices, there are\nformal propositions in the literature demonstrating accurate modeling of the\npredictive systems that can enable us to predict stock prices with a very high\nlevel of accuracy. In this paper, we present a suite of deep learning-based\nregression models that yields a very high level of accuracy in stock price\nprediction. To build our predictive models, we use the historical stock price\ndata of a well-known company listed in the National Stock Exchange (NSE) of\nIndia during the period December 31, 2012 to January 9, 2015. The stock prices\nare recorded at five minutes intervals of time during each working day in a\nweek. Using these extremely granular stock price data, we build four\nconvolutional neural network (CNN) and five long- and short-term memory\n(LSTM)-based deep learning models for accurate forecasting of the future stock\nprices. We provide detailed results on the forecasting accuracies of all our\nproposed models based on their execution time and their root mean square error\n(RMSE) values.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.08011v2"
    },
    {
        "title": "The Epps effect under alternative sampling schemes",
        "authors": [
            "Patrick Chang",
            "Etienne Pienaar",
            "Tim Gebbie"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Time and the choice of measurement time scales is fundamental to how we\nchoose to represent information and data in finance. This choice implies both\nthe units and the aggregation scales for the resulting statistical measurables\nused to describe a financial system. It also defines how we measure the\nrelationship between different traded instruments. As we move from\nhigh-frequency time scales, when individual trade and quote events occur, to\nthe mesoscales when correlations emerge in ways that can conform to various\nlatent models; it remains unclear what choice of time and sampling rates are\nappropriate to faithfully capture system dynamics and asset correlations for\ndecision making. The Epps effect is the key phenomenology that couples the\nemergence of correlations to the choice of sampling time scales. Here we\nconsider and compare the Epps effect under different sampling schemes in order\nto contrast three choices of time: calendar time, volume time and trade time.\nUsing a toy model based on a Hawkes process, we are able to achieve simulation\nresults that conform well with empirical dynamics. Concretely, we find that the\nEpps effect is present under all three definitions of time and that\ncorrelations emerge faster under trade time compared to calendar time, whereas\ncorrelations emerge linearly under volume time.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.11281v2"
    },
    {
        "title": "Asymmetric excitation of left- and right-tail extreme events probed\n  using a Hawkes model: application to financial returns",
        "authors": [
            "Matthew F. Tomlinson",
            "David Greenwood",
            "Marcin Mucha-Kruczynski"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We construct a two-tailed peak-over-threshold Hawkes model that captures\nasymmetric self- and cross-excitation in and between left- and right-tail\nextreme values within a time series. We demonstrate its applicability by\ninvestigating extreme gains and losses within the daily log-returns of the S&P\n500 equity index. We find that the arrivals of extreme losses and gains are\ndescribed by a common conditional intensity to which losses contribute twice as\nmuch as gains. However, the contribution of the former decays almost five times\nmore quickly than that of the latter. We attribute these asymmetries to the\ndifferent reactions of market traders to extreme upward and downward movements\nof asset prices: an example of negativity bias, wherein trauma is more salient\nthan euphoria.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.12291v2"
    },
    {
        "title": "Dynamics, behaviours, and anomaly persistence in cryptocurrencies and\n  equities surrounding COVID-19",
        "authors": [
            "Nick James"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper uses new and recently introduced methodologies to study the\nsimilarity in the dynamics and behaviours of cryptocurrencies and equities\nsurrounding the COVID-19 pandemic. We study two collections; 45\ncryptocurrencies and 72 equities, both independently and in conjunction. First,\nwe examine the evolution of cryptocurrency and equity market dynamics, with a\nparticular focus on their change during the COVID-19 pandemic. We demonstrate\nmarkedly more similar dynamics during times of crisis. Next, we apply recently\nintroduced methods to contrast trajectories, erratic behaviours, and extreme\nvalues among the two multivariate time series. Finally, we introduce a new\nframework for determining the persistence of market anomalies over time.\nSurprisingly, we find that although cryptocurrencies exhibit stronger\ncollective dynamics and correlation in all market conditions, equities behave\nmore similarly in their trajectories, extremes, and show greater persistence in\nanomalies over time.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.00576v3"
    },
    {
        "title": "Theory and Applications of Financial Chaos Index",
        "authors": [
            "Masoud Ataei",
            "Shengyuan Chen",
            "Zijiang Yang",
            "M. Reza Peyghami"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We develop a new stock market index that captures the chaos existing in the\nmarket by measuring the mutual changes of asset prices. This new index relies\non a tensor-based embedding of the stock market information, which in turn\nfrees it from the restrictive value- or capitalization-weighting assumptions\nthat commonly underlie other various popular indexes. We show that our index is\na robust estimator of the market volatility which enables us to characterize\nthe market by performing the task of segmentation with a high degree of\nreliability. In addition, we analyze the dynamics and kinematics of the\nrealized market volatility as compared to the implied volatility by introducing\na time-dependent dynamical system model. Our computational results which\npertain to the time period from January 1990 to December 2019 imply that there\nexist a bidirectional causal relation between the processes underlying the\nrealized and implied volatility of the stock market within the given time\nperiod, where it is shown that the later has a stronger causal effect on the\nformer as compared to the opposite. This result connotes that the implied\nvolatility of the market plays a key role in characterization of the market's\nrealized volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.02288v1"
    },
    {
        "title": "Forecasting Commodity Prices Using Long Short-Term Memory Neural\n  Networks",
        "authors": [
            "Racine Ly",
            "Fousseini Traore",
            "Khadim Dia"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper applies a recurrent neural network (RNN) method to forecast cotton\nand oil prices. We show how these new tools from machine learning, particularly\nLong-Short Term Memory (LSTM) models, complement traditional methods. Our\nresults show that machine learning methods fit reasonably well the data but do\nnot outperform systematically classical methods such as Autoregressive\nIntegrated Moving Average (ARIMA) models in terms of out of sample forecasts.\nHowever, averaging the forecasts from the two type of models provide better\nresults compared to either method. Compared to the ARIMA and the LSTM, the Root\nMean Squared Error (RMSE) of the average forecast was 0.21 and 21.49 percent\nlower respectively for cotton. For oil, the forecast averaging does not provide\nimprovements in terms of RMSE. We suggest using a forecast averaging method and\nextending our analysis to a wide range of commodity prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03087v2"
    },
    {
        "title": "Unraveling S&P500 stock volatility and networks -- An\n  encoding-and-decoding approach",
        "authors": [
            "Xiaodong Wang",
            "Fushing Hsieh"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Volatility of financial stock is referring to the degree of uncertainty or\nrisk embedded within a stock's dynamics. Such risk has been received huge\namounts of attention from diverse financial researchers. By following the\nconcept of regime-switching model, we proposed a non-parametric approach, named\nencoding-and-decoding, to discover multiple volatility states embedded within a\ndiscrete time series of stock returns. The encoding is performed across the\nentire span of temporal time points for relatively extreme events with respect\nto a chosen quantile-based threshold. As such the return time series is\ntransformed into Bernoulli-variable processes. In the decoding phase, we\ncomputationally seek for locations of change points via estimations based on a\nnew searching algorithm in conjunction with the information criterion applied\non the observed collection of recurrence times upon the binary process. Besides\nthe independence required for building the Geometric distributional likelihood\nfunction, the proposed approach can functionally partition the entire return\ntime series into a collection of homogeneous segments without any assumptions\nof dynamic structure and underlying distributions. In the numerical\nexperiments, our approach is found favorably compared with parametric models\nlike Hidden Markov Model. In the real data applications, we introduce the\napplication of our approach in forecasting stock returns. Finally, volatility\ndynamic of every single stock of S&P500 is revealed, and a stock network is\nconsequently established to represent dependency relations derived through\nconcurrent volatility states among S&P500.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.09395v3"
    },
    {
        "title": "Absolute Value Constraint: The Reason for Invalid Performance Evaluation\n  Results of Neural Network Models for Stock Price Prediction",
        "authors": [
            "Yi Wei"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Neural networks for stock price prediction(NNSPP) have been popular for\ndecades. However, most of its study results remain in the research paper and\ncannot truly play a role in the securities market. One of the main reasons\nleading to this situation is that the prediction error(PE) based evaluation\nresults have statistical flaws. Its prediction results cannot represent the\nmost critical financial direction attributes. So it cannot provide investors\nwith convincing, interpretable, and consistent model performance evaluation\nresults for practical applications in the securities market. To illustrate, we\nhave used data selected from 20 stock datasets over six years from the Shanghai\nand Shenzhen stock market in China, and 20 stock datasets from NASDAQ and NYSE\nin the USA. We implement six shallow and deep neural networks to predict stock\nprices and use four prediction error measures for evaluation. The results show\nthat the prediction error value only partially reflects the model accuracy of\nthe stock price prediction, and cannot reflect the change in the direction of\nthe model predicted stock price. This characteristic determines that PE is not\nsuitable as an evaluation indicator of NNSPP. Otherwise, it will bring huge\npotential risks to investors. Therefore, this paper establishes an experiment\nplatform to confirm that the PE method is not suitable for the NNSPP\nevaluation, and provides a theoretical basis for the necessity of creating a\nnew NNSPP evaluation method in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.10942v2"
    },
    {
        "title": "Modelling Sovereign Credit Ratings: Evaluating the Accuracy and Driving\n  Factors using Machine Learning Techniques",
        "authors": [
            "Bart H. L. Overes",
            "Michel van der Wel"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Sovereign credit ratings summarize the creditworthiness of countries. These\nratings have a large influence on the economy and the yields at which\ngovernments can issue new debt. This paper investigates the use of a Multilayer\nPerceptron (MLP), Classification and Regression Trees (CART), Support Vector\nMachines (SVM), Na\\\"ive Bayes (NB), and an Ordered Logit (OL) model for the\nprediction of sovereign credit ratings. We show that MLP is best suited for\npredicting sovereign credit ratings, with a random cross-validated accuracy of\n68%, followed by CART (59%), SVM (41%), NB (38%), and OL (33%). Investigation\nof the determining factors shows that there is some heterogeneity in the\nimportant variables across the models. However, the two models with the highest\nout-of-sample predictive accuracy, MLP and CART, show a lot of similarities in\nthe influential variables, with regulatory quality, and GDP per capita as\ncommon important variables. Consistent with economic theory, a higher\nregulatory quality and/or GDP per capita are associated with a higher credit\nrating.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.12684v2"
    },
    {
        "title": "Profitability Analysis in Stock Investment Using an LSTM-Based Deep\n  Learning Model",
        "authors": [
            "Jaydip Sen",
            "Abhishek Dutta",
            "Sidra Mehtab"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Designing robust systems for precise prediction of future prices of stocks\nhas always been considered a very challenging research problem. Even more\nchallenging is to build a system for constructing an optimum portfolio of\nstocks based on the forecasted future stock prices. We present a deep\nlearning-based regression model built on a long-and-short-term memory network\n(LSTM) network that automatically scraps the web and extracts historical stock\nprices based on a stock's ticker name for a specified pair of start and end\ndates, and forecasts the future stock prices. We deploy the model on 75\nsignificant stocks chosen from 15 critical sectors of the Indian stock market.\nFor each of the stocks, the model is evaluated for its forecast accuracy.\nMoreover, the predicted values of the stock prices are used as the basis for\ninvestment decisions, and the returns on the investments are computed.\nExtensive results are presented on the performance of the model. The analysis\nof the results demonstrates the efficacy and effectiveness of the system and\nenables us to compare the profitability of the sectors from the point of view\nof the investors in the stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.06259v1"
    },
    {
        "title": "A comparative study of Different Machine Learning Regressors For Stock\n  Market Prediction",
        "authors": [
            "Nazish Ashfaq",
            "Zubair Nawaz",
            "Muhammad Ilyas"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  For the development of successful share trading strategies, forecasting the\ncourse of action of the stock market index is important. Effective prediction\nof closing stock prices could guarantee investors attractive benefits. Machine\nlearning algorithms have the ability to process and forecast almost reliable\nclosing prices for historical stock patterns. In this article, we intensively\nstudied NASDAQ stock market and targeted to choose the portfolio of ten\ndifferent companies belongs to different sectors. The objective is to compute\nopening price of next day stock using historical data. To fulfill this task\nnine different Machine Learning regressor applied on this data and evaluated\nusing MSE and R2 as performance metric.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.07469v1"
    },
    {
        "title": "Nonparametric Test for Volatility in Clustered Multiple Time Series",
        "authors": [
            "Erniel B. Barrios",
            "Paolo Victor T. Redondo"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Contagion arising from clustering of multiple time series like those in the\nstock market indicators can further complicate the nature of volatility,\nrendering a parametric test (relying on asymptotic distribution) to suffer from\nissues on size and power. We propose a test on volatility based on the\nbootstrap method for multiple time series, intended to account for possible\npresence of contagion effect. While the test is fairly robust to distributional\nassumptions, it depends on the nature of volatility. The test is correctly\nsized even in cases where the time series are almost nonstationary. The test is\nalso powerful specially when the time series are stationary in mean and that\nvolatility are contained only in fewer clusters. We illustrate the method in\nglobal stock prices data.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14412v3"
    },
    {
        "title": "Order flow in the financial markets from the perspective of the\n  Fractional Lévy stable motion",
        "authors": [
            "Vygintas Gontis"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  It is a challenging task to identify the best possible models based on given\nempirical data of observed time series. Though the financial markets provide us\nwith a vast amount of empirical data, the best model selection is still a big\nchallenge for researchers. The widely used long-range memory and\nself-similarity estimators give varying values of the parameters as these\nestimators themselves are developed for the specific models of time series.\nHere we investigate from the general fractional L\\'evy stable motion\nperspective the order disbalance time series constructed from the limit order\nbook data of the financial markets. Our results suggest that previous findings\nof persistence in order flow could be related to the power-law distribution of\norder sizes and other deviations from the normal distribution. Still, orders\nhave stable estimates of anti-correlation for the 18 randomly selected stocks\nwhen Absolute value and Higuchi's estimators are implemented. Though the burst\nduration analysis based on the first passage problem of time series and\nimplemented in this research gives slightly higher estimates of the Hurst and\nmemory parameters, it qualitatively supports the importance of the power-law\ndistribution of order sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02057v2"
    },
    {
        "title": "Stock Price Forecasting in Presence of Covid-19 Pandemic and Evaluating\n  Performances of Machine Learning Models for Time-Series Forecasting",
        "authors": [
            "Navid Mottaghi",
            "Sara Farhangdoost"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  With the heightened volatility in stock prices during the Covid-19 pandemic,\nthe need for price forecasting has become more critical. We investigated the\nforecast performance of four models including Long-Short Term Memory, XGBoost,\nAutoregression, and Last Value on stock prices of Facebook, Amazon, Tesla,\nGoogle, and Apple in COVID-19 pandemic time to understand the accuracy and\npredictability of the models in this highly volatile time region. To train the\nmodels, the data of all stocks are split into train and test datasets. The test\ndataset starts from January 2020 to April 2021 which covers the COVID-19\npandemic period. The results show that the Autoregression and Last value models\nhave higher accuracy in predicting the stock prices because of the strong\ncorrelation between the previous day and the next day's price value.\nAdditionally, the results suggest that the machine learning models (Long-Short\nTerm Memory and XGBoost) are not performing as well as Autoregression models\nwhen the market experiences high volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02785v1"
    },
    {
        "title": "Symbol Dynamics, Information theory and Complexity of Economic time\n  series",
        "authors": [
            "Geoffrey Ducournau"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We propose to examine the predictability and the complexity characteristics\nof the Standard&Poor500 dynamics behaviors in a coarse-grained way using the\nsymbolic dynamics method and under the prism of the Information theory through\nthe concept of entropy and uncertainty. We believe that experimental\nmeasurement of entropy as a way of examining the complexity of a system is more\nrelevant than more common tests of universality in the transition to chaos\nbecause it does not make any prior prejudices on the underlying causes\nassociated with the system dynamics, whether deterministic or stochastic. We\nregard the studied economic time series as being complex and propose to express\nit in terms of the amount of information this last is producing on different\ntime scales and according to various scaling parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.04131v1"
    },
    {
        "title": "Dynamic Portfolio Allocation in High Dimensions using Sparse Risk\n  Factors",
        "authors": [
            "Bruno P. C. Levy",
            "Hedibert F. Lopes"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We propose a fast and flexible method to scale multivariate return volatility\npredictions up to high-dimensions using a dynamic risk factor model. Our\napproach increases parsimony via time-varying sparsity on factor loadings and\nis able to sequentially learn the use of constant or time-varying parameters\nand volatilities. We show in a dynamic portfolio allocation problem with 452\nstocks from the S&P 500 index that our dynamic risk factor model is able to\nproduce more stable and sparse predictions, achieving not just considerable\nportfolio performance improvements but also higher utility gains for the\nmean-variance investor compared to the traditional Wishart benchmark and the\npassive investment on the market index.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.06584v2"
    },
    {
        "title": "Dependence Modeling and Risk Assessment of a Financial Portfolio with\n  ARMA-APARCH-EVT models based on HACs",
        "authors": [
            "Dodo Natatou Moutari",
            "Hassane Abba Mallam",
            "Diakarya Barro",
            "Bisso Saley"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This study aims to widen the sphere of pratical applicability of the HAC\nmodel combined with the ARMA-APARCH volatility forecast model and the extreme\nvalues theory. A sequential process of modeling of the VaR of a portfolio based\non the ARMA-APARCH-EVT-HAC model was discussed. The empirical analysis\nconducted with data from international stock market indices clearly illustrates\nthe performance and accuracy of modeling based on HACs.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.09473v1"
    },
    {
        "title": "Design and Analysis of Robust Deep Learning Models for Stock Price\n  Prediction",
        "authors": [
            "Jaydip Sen",
            "Sidra Mehtab"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Building predictive models for robust and accurate prediction of stock prices\nand stock price movement is a challenging research problem to solve. The\nwell-known efficient market hypothesis believes in the impossibility of\naccurate prediction of future stock prices in an efficient stock market as the\nstock prices are assumed to be purely stochastic. However, numerous works\nproposed by researchers have demonstrated that it is possible to predict future\nstock prices with a high level of precision using sophisticated algorithms,\nmodel architectures, and the selection of appropriate variables in the models.\nThis chapter proposes a collection of predictive regression models built on\ndeep learning architecture for robust and precise prediction of the future\nprices of a stock listed in the diversified sectors in the National Stock\nExchange (NSE) of India. The Metastock tool is used to download the historical\nstock prices over a period of two years (2013- 2014) at 5 minutes intervals.\nWhile the records for the first year are used to train the models, the testing\nis carried out using the remaining records. The design approaches of all the\nmodels and their performance results are presented in detail. The models are\nalso compared based on their execution time and accuracy of prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.09664v1"
    },
    {
        "title": "Two Price Regimes in Limit Order Books: Liquidity Cushion and Fragmented\n  Distant Field",
        "authors": [
            "Sebastian M. Krause",
            "Edgar Jungblut",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The distribution of liquidity within the limit order book is essential for\nthe impact of market orders on the stock price and the emergence of price\nshocks. Limit orders are characterized by stylized facts: The number of\ninserted limit orders declines with the price distance from the quotes\nfollowing a power law and limit order lifetimes and volumes are power law\ndistributed. Strong dependencies among these quantities add to the complexity\nof limit order books. Here we analyze the limit order book in the dimensions of\nprice, time, limit order lifetime and volume altogether. This allows us to\nidentify regularities which are not visible in marginal distributions.\nParticularly we find that the limit order book is divided into two regimes.\nAround the quotes we find a densely filled regime with mostly short living\nlimit orders closely adapting to the price. Far away from the quotes we find a\nsparse filling with long living limit orders, mostly inserted at particular\ntimes of the day being prone to flash crashes. We determine the characteristics\nof those two regimes and point out the main differences. Based on our research\nwe propose a model for simulating the regime around the quotes.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.11691v2"
    },
    {
        "title": "Next-Day Bitcoin Price Forecast Based on Artificial intelligence Methods",
        "authors": [
            "Liping Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In recent years, Bitcoin price prediction has attracted the interest of\nresearchers and investors. However, the accuracy of previous studies is not\nwell enough. Machine learning and deep learning methods have been proved to\nhave strong prediction ability in this area. This paper proposed a method\ncombined with Ensemble Empirical Mode Decomposition (EEMD) and a deep learning\nmethod called long short-term memory (LSTM) to research the problem of next-day\nBitcoin price forecast.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.12961v1"
    },
    {
        "title": "Stock Market Analysis with Text Data: A Review",
        "authors": [
            "Kamaladdin Fataliyev",
            "Aneesh Chivukula",
            "Mukesh Prasad",
            "Wei Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Stock market movements are influenced by public and private information\nshared through news articles, company reports, and social media discussions.\nAnalyzing these vast sources of data can give market participants an edge to\nmake profit. However, the majority of the studies in the literature are based\non traditional approaches that come short in analyzing unstructured, vast\ntextual data. In this study, we provide a review on the immense amount of\nexisting literature of text-based stock market analysis. We present input data\ntypes and cover main textual data sources and variations. Feature\nrepresentation techniques are then presented. Then, we cover the analysis\ntechniques and create a taxonomy of the main stock market forecast models.\nImportantly, we discuss representative work in each category of the taxonomy,\nanalyzing their respective contributions. Finally, this paper shows the\nfindings on unaddressed open problems and gives suggestions for future work.\nThe aim of this study is to survey the main stock market analysis models, text\nrepresentation techniques for financial market prediction, shortcomings of\nexisting techniques, and propose promising directions for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.12985v2"
    },
    {
        "title": "Measuring Financial Time Series Similarity With a View to Identifying\n  Profitable Stock Market Opportunities",
        "authors": [
            "Rian Dolphin",
            "Barry Smyth",
            "Yang Xu",
            "Ruihai Dong"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Forecasting stock returns is a challenging problem due to the highly\nstochastic nature of the market and the vast array of factors and events that\ncan influence trading volume and prices. Nevertheless it has proven to be an\nattractive target for machine learning research because of the potential for\neven modest levels of prediction accuracy to deliver significant benefits. In\nthis paper, we describe a case-based reasoning approach to predicting stock\nmarket returns using only historical pricing data. We argue that one of the\nimpediments for case-based stock prediction has been the lack of a suitable\nsimilarity metric when it comes to identifying similar pricing histories as the\nbasis for a future prediction -- traditional Euclidean and correlation based\napproaches are not effective for a variety of reasons -- and in this regard, a\nkey contribution of this work is the development of a novel similarity metric\nfor comparing historical pricing data. We demonstrate the benefits of this\nmetric and the case-based approach in a real-world application in comparison to\na variety of conventional benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.03926v1"
    },
    {
        "title": "Dynamics of the market states in the space of correlation matrices with\n  applications to financial markets",
        "authors": [
            "Hirdesh K. Pharasi",
            "Suchetana Sadhukhan",
            "Parisa Majari",
            "Anirban Chakraborti",
            "Thomas H. Seligman"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The concept of states of financial markets based on correlations has gained\nincreasing attention during the last 10 years. We propose to retrace some\nimportant steps up to 2018, and then give a more detailed view of recent\ndevelopments that attempt to make the use of this more practical. Finally, we\ntry to give a glimpse to the future proposing the analysis of trajectories in\ncorrelation matrix space directly or in terms of symbolic dynamics as well as\nattempts to analyze the clusters that make up the states in a random matrix\ncontext.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.05663v1"
    },
    {
        "title": "Clustering and attention model based for intelligent trading",
        "authors": [
            "Mimansa Rana",
            "Nanxiang Mao",
            "Ming Ao",
            "Xiaohui Wu",
            "Poning Liang",
            "Matloob Khushi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The foreign exchange market has taken an important role in the global\nfinancial market. While foreign exchange trading brings high-yield\nopportunities to investors, it also brings certain risks. Since the\nestablishment of the foreign exchange market in the 20th century, foreign\nexchange rate forecasting has become a hot issue studied by scholars from all\nover the world. Due to the complexity and number of factors affecting the\nforeign exchange market, technical analysis cannot respond to administrative\nintervention or unexpected events. Our team chose several pairs of foreign\ncurrency historical data and derived technical indicators from 2005 to 2021 as\nthe dataset and established different machine learning models for event-driven\nprice prediction for oversold scenario.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06782v2"
    },
    {
        "title": "Credit scoring using neural networks and SURE posterior probability\n  calibration",
        "authors": [
            "Matthieu Garcin",
            "Samuel Stéphan"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In this article we compare the performances of a logistic regression and a\nfeed forward neural network for credit scoring purposes. Our results show that\nthe logistic regression gives quite good results on the dataset and the neural\nnetwork can improve a little the performance. We also consider different sets\nof features in order to assess their importance in terms of prediction\naccuracy. We found that temporal features (i.e. repeated measures over time)\ncan be an important source of information resulting in an increase in the\noverall model accuracy. Finally, we introduce a new technique for the\ncalibration of predicted probabilities based on Stein's unbiased risk estimate\n(SURE). This calibration technique can be applied to very general calibration\nfunctions. In particular, we detail this method for the sigmoid function as\nwell as for the Kumaraswamy function, which includes the identity as a\nparticular case. We show that stacking the SURE calibration technique with the\nclassical Platt method can improve the calibration of predicted probabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.07206v1"
    },
    {
        "title": "Predicting Daily Trading Volume via Various Hidden States",
        "authors": [
            "Shaojun Ma",
            "Pengcheng Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Predicting intraday trading volume plays an important role in trading alpha\nresearch. Existing methods such as rolling means(RM) and a two-states based\nKalman Filtering method have been presented in this topic. We extend two states\ninto various states in Kalman Filter framework to improve the accuracy of\nprediction. Specifically, for different stocks we utilize cross validation and\ndetermine best states number by minimizing mean squared error of the trading\nvolume. We demonstrate the effectivity of our method through a series of\ncomparison experiments and numerical analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.07678v1"
    },
    {
        "title": "cCorrGAN: Conditional Correlation GAN for Learning Empirical Conditional\n  Distributions in the Elliptope",
        "authors": [
            "Gautier Marti",
            "Victor Goubet",
            "Frank Nielsen"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We propose a methodology to approximate conditional distributions in the\nelliptope of correlation matrices based on conditional generative adversarial\nnetworks. We illustrate the methodology with an application from quantitative\nfinance: Monte Carlo simulations of correlated returns to compare risk-based\nportfolio construction methods. Finally, we discuss about current limitations\nand advocate for further exploration of the elliptope geometry to improve\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.10606v1"
    },
    {
        "title": "Reference Class Selection in Similarity-Based Forecasting of Sales\n  Growth",
        "authors": [
            "Etienne Theising",
            "Dominik Wied",
            "Daniel Ziggel"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper proposes a general method to handle forecasts exposed to\nbehavioural bias by finding appropriate outside views, in our case corporate\nsales forecasts of analysts. The idea is to find reference classes, i.e. peer\ngroups, for each analyzed company separately that share similarities to the\nfirm of interest with respect to a specific predictor. The classes are regarded\nto be optimal if the forecasted sales distributions match the actual\ndistributions as closely as possible. The forecast quality is measured by\napplying goodness-of-fit tests on the estimated probability integral\ntransformations and by comparing the predicted quantiles. The method is\nout-of-sample backtested on a data set consisting of 21,808 US firms over the\ntime period 1950 - 2019, which is also descriptively analyzed. It appears that\nin particular the past operating margins are good predictors for the\ndistribution of future sales. A case study compares the outside view of our\ndistributional forecasts with actual analysts' forecasts and emphasizes the\nrelevance of our approach in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.11133v2"
    },
    {
        "title": "Temporal-Relational Hypergraph Tri-Attention Networks for Stock Trend\n  Prediction",
        "authors": [
            "Chaoran Cui",
            "Xiaojie Li",
            "Juan Du",
            "Chunyun Zhang",
            "Xiushan Nie",
            "Meng Wang",
            "Yilong Yin"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Predicting the future price trends of stocks is a challenging yet intriguing\nproblem given its critical role to help investors make profitable decisions. In\nthis paper, we present a collaborative temporal-relational modeling framework\nfor end-to-end stock trend prediction. The temporal dynamics of stocks is\nfirstly captured with an attention-based recurrent neural network. Then,\ndifferent from existing studies relying on the pairwise correlations between\nstocks, we argue that stocks are naturally connected as a collective group, and\nintroduce the hypergraph structures to jointly characterize the stock\ngroup-wise relationships of industry-belonging and fund-holding. A novel\nhypergraph tri-attention network (HGTAN) is proposed to augment the hypergraph\nconvolutional networks with a hierarchical organization of intra-hyperedge,\ninter-hyperedge, and inter-hypergraph attention modules. In this manner, HGTAN\nadaptively determines the importance of nodes, hyperedges, and hypergraphs\nduring the information propagation among stocks, so that the potential\nsynergies between stock movements can be fully exploited. Extensive experiments\non real-world data demonstrate the effectiveness of our approach. Also, the\nresults of investment simulation show that our approach can achieve a more\ndesirable risk-adjusted return. The data and codes of our work have been\nreleased at https://github.com/lixiaojieff/HGTAN.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14033v2"
    },
    {
        "title": "A data-science-driven short-term analysis of Amazon, Apple, Google, and\n  Microsoft stocks",
        "authors": [
            "Shubham Ekapure",
            "Nuruddin Jiruwala",
            "Sohan Patnaik",
            "Indranil SenGupta"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In this paper, we implement a combination of technical analysis and\nmachine/deep learning-based analysis to build a trend classification model. The\ngoal of the paper is to apprehend short-term market movement, and incorporate\nit to improve the underlying stochastic model. Also, the analysis presented in\nthis paper can be implemented in a \\emph{model-independent} fashion. We execute\na data-science-driven technique that makes short-term forecasts dependent on\nthe price trends of current stock market data. Based on the analysis, three\ndifferent labels are generated for a data set: $+1$ (buy signal), $0$ (hold\nsignal), or $-1$ (sell signal). We propose a detailed analysis of four major\nstocks- Amazon, Apple, Google, and Microsoft. We implement various technical\nindicators to label the data set according to the trend and train various\nmodels for trend estimation. Statistical analysis of the outputs and\nclassification results are obtained.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14695v1"
    },
    {
        "title": "Factor Representation and Decision Making in Stock Markets Using Deep\n  Reinforcement Learning",
        "authors": [
            "Zhaolu Dong",
            "Shan Huang",
            "Simiao Ma",
            "Yining Qian"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Deep Reinforcement learning is a branch of unsupervised learning in which an\nagent learns to act based on environment state in order to maximize its total\nreward. Deep reinforcement learning provides good opportunity to model the\ncomplexity of portfolio choice in high-dimensional and data-driven environment\nby leveraging the powerful representation of deep neural networks. In this\npaper, we build a portfolio management system using direct deep reinforcement\nlearning to make optimal portfolio choice periodically among S\\&P500 underlying\nstocks by learning a good factor representation (as input). The result shows\nthat an effective learning of market conditions and optimal portfolio\nallocations can significantly outperform the average market.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.01758v1"
    },
    {
        "title": "Bilinear Input Normalization for Neural Networks in Financial\n  Forecasting",
        "authors": [
            "Dat Thanh Tran",
            "Juho Kanniainen",
            "Moncef Gabbouj",
            "Alexandros Iosifidis"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Data normalization is one of the most important preprocessing steps when\nbuilding a machine learning model, especially when the model of interest is a\ndeep neural network. This is because deep neural network optimized with\nstochastic gradient descent is sensitive to the input variable range and prone\nto numerical issues. Different than other types of signals, financial\ntime-series often exhibit unique characteristics such as high volatility,\nnon-stationarity and multi-modality that make them challenging to work with,\noften requiring expert domain knowledge for devising a suitable processing\npipeline. In this paper, we propose a novel data-driven normalization method\nfor deep neural networks that handle high-frequency financial time-series. The\nproposed normalization scheme, which takes into account the bimodal\ncharacteristic of financial multivariate time-series, requires no expert\nknowledge to preprocess a financial time-series since this step is formulated\nas part of the end-to-end optimization process. Our experiments, conducted with\nstate-of-the-arts neural networks and high-frequency data from two large-scale\nlimit order books coming from the Nordic and US markets, show significant\nimprovements over other normalization techniques in forecasting future stock\nprice dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.00983v1"
    },
    {
        "title": "Forecasting High-Dimensional Covariance Matrices of Asset Returns with\n  Hybrid GARCH-LSTMs",
        "authors": [
            "Lucien Boulet"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Several academics have studied the ability of hybrid models mixing univariate\nGeneralized Autoregressive Conditional Heteroskedasticity (GARCH) models and\nneural networks to deliver better volatility predictions than purely\neconometric models. Despite presenting very promising results, the\ngeneralization of such models to the multivariate case has yet to be studied.\nMoreover, very few papers have examined the ability of neural networks to\npredict the covariance matrix of asset returns, and all use a rather small\nnumber of assets, thus not addressing what is known as the curse of\ndimensionality. The goal of this paper is to investigate the ability of hybrid\nmodels, mixing GARCH processes and neural networks, to forecast covariance\nmatrices of asset returns. To do so, we propose a new model, based on\nmultivariate GARCHs that decompose volatility and correlation predictions. The\nvolatilities are here forecast using hybrid neural networks while correlations\nfollow a traditional econometric process. After implementing the models in a\nminimum variance portfolio framework, our results are as follows. First, the\naddition of GARCH parameters as inputs is beneficial to the model proposed.\nSecond, the use of one-hot-encoding to help the neural network differentiate\nbetween each stock improves the performance. Third, the new model proposed is\nvery promising as it not only outperforms the equally weighted portfolio, but\nalso by a significant margin its econometric counterpart that uses univariate\nGARCHs to predict the volatilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.01044v1"
    },
    {
        "title": "Stock Price Prediction Under Anomalous Circumstances",
        "authors": [
            "Jinlong Ruan",
            "Wei Wu",
            "Jiebo Luo"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The stock market is volatile and complicated, especially in 2020. Because of\na series of global and regional \"black swans,\" such as the COVID-19 pandemic,\nthe U.S. stock market triggered the circuit breaker three times within one week\nof March 9 to 16, which is unprecedented throughout history. Affected by the\nwhole circumstance, the stock prices of individual corporations also plummeted\nby rates that were never predicted by any pre-developed forecasting models. It\nreveals that there was a lack of satisfactory models that could predict the\nchanges in stocks prices when catastrophic, highly unlikely events occur. To\nfill the void of such models and to help prevent investors from heavy losses\nduring uncertain times, this paper aims to capture the movement pattern of\nstock prices under anomalous circumstances. First, we detect outliers in\nsequential stock prices by fitting a standard ARIMA model and identifying the\npoints where predictions deviate significantly from actual values. With the\nselected data points, we train ARIMA and LSTM models at the single-stock level,\nindustry level, and general market level, respectively. Since the public moods\naffect the stock market tremendously, a sentiment analysis is also incorporated\ninto the models in the form of sentiment scores, which are converted from\ncomments about specific stocks on Reddit. Based on 100 companies' stock prices\nin the period of 2016 to 2020, the models achieve an average prediction\naccuracy of 98% which can be used to optimize existing prediction\nmethodologies.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.15059v1"
    },
    {
        "title": "Dropping diversity of products of large US firms: Models and measures",
        "authors": [
            "Ananthan Nambiar",
            "Tobias Rubel",
            "James McCaull",
            "Jon deVries",
            "Mark Bedau"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  It is widely assumed that in our lifetimes the products available in the\nglobal economy have become more diverse. This assumption is difficult to\ninvestigate directly, however, because it is difficult to collect the necessary\ndata about every product in an economy each year. We solve this problem by\nmining publicly available textual descriptions of the products of every large\nUS firms each year from 1997 to 2017. Although many aspects of economic\nproductivity have been steadily rising during this period, our text-based\nmeasurements show that the diversity of the products of at least large US firms\nhas steadily declined. This downward trend is visible using a variety of\nproduct diversity metrics, including some that depend on a measurement of the\nsimilarity of the products of every single pair of firms. The current state of\nthe art in comprehensive and detailed firm-similarity measurements is a Boolean\nword vector model due to Hoberg and Phillips. We measure diversity using\nfirm-similarities from this Boolean model and two more sophisticated variants,\nand we consistently observe a significant dropping trend in product diversity.\nThese results make it possible to frame and start to test specific hypotheses\nfor explaining the dropping product diversity trend.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.08367v1"
    },
    {
        "title": "Machine Learning in Finance-Emerging Trends and Challenges",
        "authors": [
            "Jaydip Sen",
            "Rajdeep Sen",
            "Abhishek Dutta"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The paradigm of machine learning and artificial intelligence has pervaded our\neveryday life in such a way that it is no longer an area for esoteric academics\nand scientists putting their effort to solve a challenging research problem.\nThe evolution is quite natural rather than accidental. With the exponential\ngrowth in processing speed and with the emergence of smarter algorithms for\nsolving complex and challenging problems, organizations have found it possible\nto harness a humongous volume of data in realizing solutions that have\nfar-reaching business values. This introductory chapter highlights some of the\nchallenges and barriers that organizations in the financial services sector at\nthe present encounter in adopting machine learning and artificial\nintelligence-based models and applications in their day-to-day operations.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.11999v1"
    },
    {
        "title": "Bank transactions embeddings help to uncover current macroeconomics",
        "authors": [
            "Maria Begicheva",
            "Alexey Zaytsev"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Macroeconomic indexes are of high importance for banks: many risk-control\ndecisions utilize these indexes. A typical workflow of these indexes evaluation\nis costly and protracted, with a lag between the actual date and available\nindex being a couple of months. Banks predict such indexes now using\nautoregressive models to make decisions in a rapidly changing environment.\nHowever, autoregressive models fail in complex scenarios related to appearances\nof crises.\n  We propose to use clients' financial transactions data from a large Russian\nbank to get such indexes. Financial transactions are long, and a number of\nclients is huge, so we develop an efficient approach that allows fast and\naccurate estimation of macroeconomic indexes based on a stream of transactions\nconsisting of millions of transactions. The approach uses a neural networks\nparadigm and a smart sampling scheme.\n  The results show that our neural network approach outperforms the baseline\nmethod on hand-crafted features based on transactions. Calculated embeddings\nshow the correlation between the client's transaction activity and bank\nmacroeconomic indexes over time.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.12000v3"
    },
    {
        "title": "Brownian Motion & The Stochastic Behaviour of Stocks",
        "authors": [
            "Yorgos Protonotarios",
            "Pantelis Tassopoulos"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We begin by exploring the intuition of Brownian motion by explaining its\nbirth through the observations of Robert Brown and later through Bachelier's\nwork on its applications to the financial market and finally its rigorous and\nconcretized form proposed by Norbert Wiener. The aforementioned motivates a\nstochastic differential equation to model the future price fluctuations of a\nstock traded wherein It\\^o integration is prominent and consequently expanded\nupon. The final part of this paper focuses on the accuracy of the model by\nbacktesting it with Apple stock and deriving a correlation coefficient.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.12001v1"
    },
    {
        "title": "HIST: A Graph-based Framework for Stock Trend Forecasting via Mining\n  Concept-Oriented Shared Information",
        "authors": [
            "Wentao Xu",
            "Weiqing Liu",
            "Lewen Wang",
            "Yingce Xia",
            "Jiang Bian",
            "Jian Yin",
            "Tie-Yan Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Stock trend forecasting, which forecasts stock prices' future trends, plays\nan essential role in investment. The stocks in a market can share information\nso that their stock prices are highly correlated. Several methods were recently\nproposed to mine the shared information through stock concepts (e.g.,\ntechnology, Internet Retail) extracted from the Web to improve the forecasting\nresults. However, previous work assumes the connections between stocks and\nconcepts are stationary, and neglects the dynamic relevance between stocks and\nconcepts, limiting the forecasting results. Moreover, existing methods overlook\nthe invaluable shared information carried by hidden concepts, which measure\nstocks' commonness beyond the manually defined stock concepts. To overcome the\nshortcomings of previous work, we proposed a novel stock trend forecasting\nframework that can adequately mine the concept-oriented shared information from\npredefined concepts and hidden concepts. The proposed framework simultaneously\nutilize the stock's shared information and individual information to improve\nthe stock trend forecasting performance. Experimental results on the real-world\ntasks demonstrate the efficiency of our framework on stock trend forecasting.\nThe investment simulation shows that our framework can achieve a higher\ninvestment return than the baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.13716v2"
    },
    {
        "title": "Deep Calibration of Interest Rates Model",
        "authors": [
            "Mohamed Ben Alaya",
            "Ahmed Kebaier",
            "Djibril Sarr"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  For any financial institution, it is essential to understand the behavior of\ninterest rates. Despite the growing use of Deep Learning, for many reasons\n(expertise, ease of use, etc.), classic rate models such as CIR and the\nGaussian family are still widely used. In this paper, we propose to calibrate\nthe five parameters of the G2++ model using Neural Networks. Our first model is\na Fully Connected Neural Network and is trained on covariances and correlations\nof Zero-Coupon and Forward rates. We show that covariances are more suited to\nthe problem than correlations due to the effects of the unfeasible\nbackpropagation phenomenon, which we analyze in this paper. The second model is\na Convolutional Neural Network trained on Zero-Coupon rates with no further\ntransformation. Our numerical tests show that our calibration based on deep\nlearning outperforms the classic calibration method used as a benchmark.\nAdditionally, our Deep Calibration approach is designed to be systematic. To\nillustrate this feature, we applied it to calibrate the popular CIR intensity\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.15133v2"
    },
    {
        "title": "Data-driven Hedging of Stock Index Options via Deep Learning",
        "authors": [
            "Jie Chen",
            "Lingfei Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We develop deep learning models to learn the hedge ratio for S&P500 index\noptions directly from options data. We compare different combinations of\nfeatures and show that a feedforward neural network model with time to\nmaturity, Black-Scholes delta and a sentiment variable (VIX for calls and index\nreturn for puts) as input features performs the best in the out-of-sample test.\nThis model significantly outperforms the standard hedging practice that uses\nthe Black-Scholes delta and a recent data-driven model. Our results demonstrate\nthe importance of market sentiment for hedging efficiency, a factor previously\nignored in developing hedging strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.03477v1"
    },
    {
        "title": "The Evolving Causal Structure of Equity Risk Factors",
        "authors": [
            "Gabriele D'Acunto",
            "Paolo Bajardi",
            "Francesco Bonchi",
            "Gianmarco De Francisci Morales"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In recent years, multi-factor strategies have gained increasing popularity in\nthe financial industry, as they allow investors to have a better understanding\nof the risk drivers underlying their portfolios. Moreover, such strategies\npromise to promote diversification and thus limit losses in times of financial\nturmoil. However, recent studies have reported a significant level of\nredundancy between these factors, which might enhance risk contagion among\nmulti-factor portfolios during financial crises. Therefore, it is of\nfundamental importance to better understand the relationships among factors.\n  Empowered by recent advances in causal structure learning methods, this paper\npresents a study of the causal structure of financial risk factors and its\nevolution over time. In particular, the data we analyze covers 11 risk factors\nconcerning the US equity market, spanning a period of 29 years at daily\nfrequency.\n  Our results show a statistically significant sparsifying trend of the\nunderlying causal structure. However, this trend breaks down during periods of\nfinancial stress, in which we can observe a densification of the causal network\ndriven by a growth of the out-degree of the market factor node. Finally, we\npresent a comparison with the analysis of factors cross-correlations, which\nfurther confirms the importance of causal analysis for gaining deeper insights\nin the dynamics of the factor system, particularly during economic downturns.\n  Our findings are especially significant from a risk-management perspective.\nThey link the evolution of the causal structure of equity risk factors with\nmarket volatility and a worsening macroeconomic environment, and show that, in\ntimes of financial crisis, exposure to different factors boils down to exposure\nto the market risk factor.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.05072v1"
    },
    {
        "title": "Effect of the U.S.--China Trade War on Stock Markets: A Financial\n  Contagion Perspective",
        "authors": [
            "Minseog Oh",
            "Donggyu Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In this paper, we investigate the effect of the U.S.--China trade war on\nstock markets from a financial contagion perspective, based on high-frequency\nfinancial data. Specifically, to account for risk contagion between the U.S.\nand China stock markets, we develop a novel jump-diffusion process. For\nexample, we consider three channels for volatility contagion--such as\nintegrated volatility, positive jump variation, and negative jump\nvariation--and each stock market is able to affect the other stock market as an\novernight risk factor. We develop a quasi-maximum likelihood estimator for\nmodel parameters and establish its asymptotic properties. Furthermore, to\nidentify contagion channels and test the existence of a structural break, we\npropose hypothesis test procedures. From the empirical study, we find evidence\nof financial contagion from the U.S. to China and evidence that the risk\ncontagion channel has changed from integrated volatility to negative jump\nvariation.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.09655v1"
    },
    {
        "title": "Forex Trading Volatility Prediction using Neural Network Models",
        "authors": [
            "Shujian Liao",
            "Jian Chen",
            "Hao Ni"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In this paper, we investigate the problem of predicting the future volatility\nof Forex currency pairs using the deep learning techniques. We show\nstep-by-step how to construct the deep-learning network by the guidance of the\nempirical patterns of the intra-day volatility. The numerical results show that\nthe multiscale Long Short-Term Memory (LSTM) model with the input of\nmulti-currency pairs consistently achieves the state-of-the-art accuracy\ncompared with both the conventional baselines, i.e. autoregressive and GARCH\nmodel, and the other deep learning models.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01166v2"
    },
    {
        "title": "Change of persistence in European electricity spot prices",
        "authors": [
            "Leonardo Rydin Gorjão",
            "Dirk Witthaut",
            "Pedro G. Lind",
            "Wided Medjroubi"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The European Power Exchange has introduced day-ahead auctions and continuous\ntrading spot markets to facilitate the insertion of renewable electricity.\nThese markets are designed to balance excess or lack of power in short time\nperiods, which leads to a large stochastic variability of the electricity\nprices. Furthermore, the different markets show different stochastic memory in\ntheir electricity price time series, which seem to be the cause for the large\nvolatility. In particular, we show the antithetical temporal correlation in the\nintraday 15 minutes spot markets in comparison to the day-ahead hourly market.\nWe contrast the results from Detrended Fluctuation Analysis (DFA) to a new\nmethod based on the Kramers--Moyal equation in scale. For very short term\n($<12$ hours), all price time series show positive temporal correlations (Hurst\nexponent $H>0.5$) except for the intraday 15 minute market, which shows strong\nnegative correlations ($H<0.5$). For longer term periods covering up to two\ndays, all price time series are anti-correlated ($H<0.5$).\n",
        "pdf_link": "http://arxiv.org/pdf/2112.03513v1"
    },
    {
        "title": "Dimensionality reduction for prediction: Application to Bitcoin and\n  Ethereum",
        "authors": [
            "Hugo Inzirillo",
            "Benjamin Mat"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The objective of this paper is to assess the performances of dimensionality\nreduction techniques to establish a link between cryptocurrencies. We have\nfocused our analysis on the two most traded cryptocurrencies: Bitcoin and\nEthereum. To perform our analysis, we took log returns and added some\ncovariates to build our data set. We first introduced the pearson correlation\ncoefficient in order to have a preliminary assessment of the link between\nBitcoin and Ethereum. We then reduced the dimension of our data set using\ncanonical correlation analysis and principal component analysis. After\nperforming an analysis of the links between Bitcoin and Ethereum with both\nstatistical techniques, we measured their performance on forecasting Ethereum\nreturns with Bitcoin s features.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.15036v2"
    },
    {
        "title": "Lunatic Stocks: Moon Phases as Irregular Sampling Features for Pattern\n  Recognition in the Stock Markets",
        "authors": [
            "Luis A. Mateos"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper presents a novel idea on incorporating the Moon phases to the\nclassic Gregorian (Solar) calendar time sampling methods for finding meaningful\npatterns in the stock markets. The four main Moon phases (New Moon, First\nquarter, Full Moon and Third quarter) are irregular in time but with well\ndefined sampling structure as the Moon orbits the Earth completing its period.\nA Full Moon may appear in one month of the year on the 2nd, on the next month\nthe Full moon may appear on the 4th and in the next ten years on the 13th of\nthe same month. This structure which is irregular in time makes it interesting\nto study together with the stock market data. Moreover, the moon affects\nmultiple physical things on the earth, such as the ocean tides, the behavior of\nliving organisms as well as humans mood and decision when risking and\ninvesting.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.15426v1"
    },
    {
        "title": "Exact Post-selection Inference For Tracking S&P500",
        "authors": [
            "Farshad Noravesh",
            "Hamid Boustanifar"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The problem that is solved in this paper is known as index tracking. The\nmethod of Lasso is used to reduce the dimensions of S&P500 index which has many\napplications in both investment and portfolio management algorithms. The\nnovelty of this paper is that post-selection inference is used to have better\nmodeling and inference for Lasso approach to index tracking. Both confidence\nintervals and curves indicate that the performance of Lasso type method for\ndimension reduction of S&P500 is remarkably high. Keywords: index tracking,\nlasso, post-selection inference, S&P500\n",
        "pdf_link": "http://arxiv.org/pdf/2112.15448v1"
    },
    {
        "title": "The Interpretability of LSTM Models for Predicting Oil Company Stocks:\n  Impact of Correlated Features",
        "authors": [
            "Javad T. Firouzjaee",
            "Pouriya Khaliliyan"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Oil companies are among the largest companies in the world whose economic\nindicators in the global stock market have a great impact on the world\neconomy\\cite{ec00} and market due to their relation to gold\\cite{ec01}, crude\noil\\cite{ec02}, and the dollar\\cite{ec03}. This study investigates the impact\nof correlated features on the interpretability of Long Short-Term\nMemory(LSTM)\\cite{ec04} models for predicting oil company stocks. To achieve\nthis, we designed a Standard Long Short-Term Memory (LSTM) network and trained\nit using various correlated datasets. Our approach aims to improve the accuracy\nof stock price prediction by considering the multiple factors affecting the\nmarket, such as crude oil prices, gold prices, and the US dollar. The results\ndemonstrate that adding a feature correlated with oil stocks does not improve\nthe interpretability of LSTM models. These findings suggest that while LSTM\nmodels may be effective in predicting stock prices, their interpretability may\nbe limited. Caution should be exercised when relying solely on LSTM models for\nstock price prediction as their lack of interpretability may make it difficult\nto fully understand the underlying factors driving stock price movements. We\nhave employed complexity analysis to support our argument, considering that\nfinancial markets encompass a form of physical complex system\\cite{ec05}. One\nof the fundamental challenges faced in utilizing LSTM models for financial\nmarkets lies in interpreting the unexpected feedback dynamics within them.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.00350v5"
    },
    {
        "title": "Bitcoin Price Predictive Modeling Using Expert Correction",
        "authors": [
            "Bohdan M. Pavlyshenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The paper studies the linear model for Bitcoin price which includes\nregression features based on Bitcoin currency statistics, mining processes,\nGoogle search trends, Wikipedia pages visits. The pattern of deviation of\nregression model prediction from real prices is simpler comparing to price time\nseries. It is assumed that this pattern can be predicted by an experienced\nexpert. In such a way, using the combination of the regression model and expert\ncorrection, one can receive better results than with either regression model or\nexpert opinion only. It is shown that Bayesian approach makes it possible to\nutilize the probabilistic approach using distributions with fat tails and take\ninto account the outliers in Bitcoin price time series.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.02729v1"
    },
    {
        "title": "Opinion Dynamics in Financial Markets via Random Networks",
        "authors": [
            "Mateus F. B. Granha",
            "André L. M. Vilela",
            "Chao Wang",
            "Kenric P. Nelson",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We investigate the financial market dynamics by introducing a heterogeneous\nagent-based opinion formation model. In this work, we organize the individuals\nin a financial market by their trading strategy, namely noise traders and\nfundamentalists. The opinion of a local majority compels the market exchanging\nbehavior of noise traders, whereas the global behavior of the market influences\nthe fundamentalist agents' decisions. We introduce a noise parameter $q$ to\nrepresent a level of anxiety and perceived uncertainty regarding the market\nbehavior, enabling the possibility for an adrift financial action. We place the\nindividuals as nodes in an Erd\\\"os-R\\'enyi random graph, where the links\nrepresent their social interaction. At a given time, they assume one of two\npossible opinion states $\\pm 1$ regarding buying or selling an asset. The model\nexhibits such fundamental qualitative and quantitative real-world market\nfeatures as the distribution of logarithmic returns with fat-tails, clustered\nvolatility, and long-term correlation of returns. We use Student's t\ndistributions to fit the histograms of logarithmic returns, showing the gradual\nshift from a leptokurtic to a mesokurtic regime, depending on the fraction of\nfundamentalist agents. We also compare our results with the distribution of\nlogarithmic returns of several real-world financial indices.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07214v1"
    },
    {
        "title": "Long Short-Term Memory Neural Network for Financial Time Series",
        "authors": [
            "Carmina Fjellström"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Performance forecasting is an age-old problem in economics and finance.\nRecently, developments in machine learning and neural networks have given rise\nto non-linear time series models that provide modern and promising alternatives\nto traditional methods of analysis. In this paper, we present an ensemble of\nindependent and parallel long short-term memory (LSTM) neural networks for the\nprediction of stock price movement. LSTMs have been shown to be especially\nsuited for time series data due to their ability to incorporate past\ninformation, while neural network ensembles have been found to reduce\nvariability in results and improve generalization. A binary classification\nproblem based on the median of returns is used, and the ensemble's forecast\ndepends on a threshold value, which is the minimum number of LSTMs required to\nagree upon the result. The model is applied to the constituents of the smaller,\nless efficient Stockholm OMX30 instead of other major market indices such as\nthe DJIA and S&P500 commonly found in literature. With a straightforward\ntrading strategy, comparisons with a randomly chosen portfolio and a portfolio\ncontaining all the stocks in the index show that the portfolio resulting from\nthe LSTM ensemble provides better average daily returns and higher cumulative\nreturns over time. Moreover, the LSTM portfolio also exhibits less volatility,\nleading to higher risk-return ratios.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.08218v1"
    },
    {
        "title": "Linear Laws of Markov Chains with an Application for Anomaly Detection\n  in Bitcoin Prices",
        "authors": [
            "Marcell T. Kurbucz",
            "Péter Pósfay",
            "Antal Jakovác"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The goals of this paper are twofold: (1) to present a new method that is able\nto find linear laws governing the time evolution of Markov chains and (2) to\napply this method for anomaly detection in Bitcoin prices. To accomplish these\ngoals, first, the linear laws of Markov chains are derived by using the time\nembedding of their (categorical) autocorrelation function. Then, a binary\nseries is generated from the first difference of Bitcoin exchange rate (against\nthe United States Dollar). Finally, the minimum number of parameters describing\nthe linear laws of this series is identified through stepped time windows.\nBased on the results, linear laws typically became more complex (containing an\nadditional third parameter that indicates hidden Markov property) in two\nperiods: before the crash of cryptocurrency markets inducted by the COVID-19\npandemic (12 March 2020), and before the record-breaking surge in the price of\nBitcoin (Q4 2020 - Q1 2021). In addition, the locally high values of this third\nparameter are often related to short-term price peaks, which suggests price\nmanipulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.09790v1"
    },
    {
        "title": "Predicting The Stock Trend Using News Sentiment Analysis and Technical\n  Indicators in Spark",
        "authors": [
            "Taylan Kabbani",
            "Fatih Enes Usta"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Predicting the stock market trend has always been challenging since its\nmovement is affected by many factors. Here, we approach the future trend\nprediction problem as a machine learning classification problem by creating\ntomorrow_trend feature as our label to be predicted. Different features are\ngiven to help the machine learning model predict the label of a given day;\nwhether it is an uptrend or downtrend, those features are technical indicators\ngenerated from the stock's price history. In addition, as financial news plays\na vital role in changing the investor's behavior, the overall sentiment score\non a given day is created from all news released on that day and added to the\nmodel as another feature. Three different machine learning models are tested in\nSpark (big-data computing platform), Logistic Regression, Random Forest, and\nGradient Boosting Machine. Random Forest was the best performing model with a\n63.58% test accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.12283v1"
    },
    {
        "title": "A Stock Trading System for a Medium Volatile Asset using Multi Layer\n  Perceptron",
        "authors": [
            "Ivan Letteri",
            "Giuseppe Della Penna",
            "Giovanni De Gasperis",
            "Abeer Dyoub"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Stock market forecasting is a lucrative field of interest with promising\nprofits but not without its difficulties and for some people could be even\ncauses of failure. Financial markets by their nature are complex, non-linear\nand chaotic, which implies that accurately predicting the prices of assets that\nare part of it becomes very complicated. In this paper we propose a stock\ntrading system having as main core the feed-forward deep neural networks (DNN)\nto predict the price for the next 30 days of open market, of the shares issued\nby Abercrombie & Fitch Co. (ANF) in the stock market of the New York Stock\nExchange (NYSE).\n  The system we have elaborated calculates the most effective technical\nindicator, applying it to the predictions computed by the DNNs, for generating\ntrades. The results showed an increase in values such as Expectancy Ratio of\n2.112% of profitable trades with Sharpe, Sortino, and Calmar Ratios of 2.194,\n3.340, and 12.403 respectively. As a verification, we adopted a backtracking\nsimulation module in our system, which maps trades to actual test data\nconsisting of the last 30 days of open market on the ANF asset. Overall, the\nresults were promising bringing a total profit factor of 3.2% in just one month\nfrom a very modest budget of $100. This was possible because the system reduced\nthe number of trades by choosing the most effective and efficient trades,\nsaving on commissions and slippage costs.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.12286v1"
    },
    {
        "title": "Simulating Using Deep Learning The World Trade Forecasting of\n  Export-Import Exchange Rate Convergence Factor During COVID-19",
        "authors": [
            "Effat Ara Easmin Lucky",
            "Md. Mahadi Hasan Sany",
            "Mumenunnesa Keya",
            "Md. Moshiur Rahaman",
            "Umme Habiba Happy",
            "Sharun Akter Khushbu",
            "Md. Arid Hasan"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  By trade we usually mean the exchange of goods between states and countries.\nInternational trade acts as a barometer of the economic prosperity index and\nevery country is overly dependent on resources, so international trade is\nessential. Trade is significant to the global health crisis, saving lives and\nlivelihoods. By collecting the dataset called \"Effects of COVID19 on trade\"\nfrom the state website NZ Tatauranga Aotearoa, we have developed a sustainable\nprediction process on the effects of COVID-19 in world trade using a deep\nlearning model. In the research, we have given a 180-day trade forecast where\nthe ups and downs of daily imports and exports have been accurately predicted\nin the Covid-19 period. In order to fulfill this prediction, we have taken data\nfrom 1st January 2015 to 30th May 2021 for all countries, all commodities, and\nall transport systems and have recovered what the world trade situation will be\nin the next 180 days during the Covid-19 period. The deep learning method has\nreceived equal attention from both investors and researchers in the field of\nin-depth observation. This study predicts global trade using the Long-Short\nTerm Memory. Time series analysis can be useful to see how a given asset,\nsecurity, or economy changes over time. Time series analysis plays an important\nrole in past analysis to get different predictions of the future and it can be\nobserved that some factors affect a particular variable from period to period.\nThrough the time series it is possible to observe how various economic changes\nor trade effects change over time. By reviewing these changes, one can be aware\nof the steps to be taken in the future and a country can be more careful in\nterms of imports and exports accordingly. From our time series analysis, it can\nbe said that the LSTM model has given a very gracious thought of the future\nworld import and export situation in terms of trade.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.12291v1"
    },
    {
        "title": "Comparative Study of Machine Learning Models for Stock Price Prediction",
        "authors": [
            "Ogulcan E. Orsel",
            "Sasha S. Yamada"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In this work, we apply machine learning techniques to historical stock prices\nto forecast future prices. To achieve this, we use recursive approaches that\nare appropriate for handling time series data. In particular, we apply a linear\nKalman filter and different varieties of long short-term memory (LSTM)\narchitectures to historical stock prices over a 10-year range (1/1/2011 -\n1/1/2021). We quantify the results of these models by computing the error of\nthe predicted values versus the historical values of each stock. We find that\nof the algorithms we investigated, a simple linear Kalman filter can predict\nthe next-day value of stocks with low-volatility (e.g., Microsoft) surprisingly\nwell. However, in the case of high-volatility stocks (e.g., Tesla) the more\ncomplex LSTM algorithms significantly outperform the Kalman filter. Our results\nshow that we can classify different types of stocks and then train an LSTM for\neach stock type. This method could be used to automate portfolio generation for\na target return rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03156v1"
    },
    {
        "title": "Beyond Trading Data: The Hidden Influence of Public Awareness and\n  Interest on Cryptocurrency Volatility",
        "authors": [
            "Zeyd Boukhers",
            "Azeddine Bouabdallah",
            "Cong Yang",
            "Jan Jürjens"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Since Bitcoin first appeared on the scene in 2009, cryptocurrencies have\nbecome a worldwide phenomenon as important decentralized financial assets.\nTheir decentralized nature, however, leads to notable volatility against\ntraditional fiat currencies, making the task of accurately forecasting the\ncrypto-fiat exchange rate complex. This study examines the various independent\nfactors that affect the volatility of the Bitcoin-Dollar exchange rate. To this\nend, we propose CoMForE, a multimodal AdaBoost-LSTM ensemble model, which not\nonly utilizes historical trading data but also incorporates public sentiments\nfrom related tweets, public interest demonstrated by search volumes, and\nblockchain hash-rate data. Our developed model goes a step further by\npredicting fluctuations in the overall cryptocurrency value distribution, thus\nincreasing its value for investment decision-making. We have subjected this\nmethod to extensive testing via comprehensive experiments, thereby validating\nthe importance of multimodal combination over exclusive reliance on trading\ndata. Further experiments show that our method significantly surpasses existing\nforecasting tools and methodologies, demonstrating a 19.29% improvement. This\nresult underscores the influence of external independent factors on\ncryptocurrency volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.08967v2"
    },
    {
        "title": "Machine Learning Models in Stock Market Prediction",
        "authors": [
            "Gurjeet Singh"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The paper focuses on predicting the Nifty 50 Index by using 8 Supervised\nMachine Learning Models. The techniques used for empirical study are Adaptive\nBoost (AdaBoost), k-Nearest Neighbors (kNN), Linear Regression (LR), Artificial\nNeural Network (ANN), Random Forest (RF), Stochastic Gradient Descent (SGD),\nSupport Vector Machine (SVM) and Decision Trees (DT). Experiments are based on\nhistorical data of Nifty 50 Index of Indian Stock Market from 22nd April, 1996\nto 16th April, 2021, which is time series data of around 25 years. During the\nperiod there were 6220 trading days excluding all the non trading days. The\nentire trading dataset was divided into 4 subsets of different size-25% of\nentire data, 50% of entire data, 75% of entire data and entire data. Each\nsubset was further divided into 2 parts-training data and testing data. After\napplying 3 tests- Test on Training Data, Test on Testing Data and Cross\nValidation Test on each subset, the prediction performance of the used models\nwere compared and after comparison, very interesting results were found. The\nevaluation results indicate that Adaptive Boost, k- Nearest Neighbors, Random\nForest and Decision Trees under performed with increase in the size of data\nset. Linear Regression and Artificial Neural Network shown almost similar\nprediction results among all the models but Artificial Neural Network took more\ntime in training and validating the model. Thereafter Support Vector Machine\nperformed better among rest of the models but with increase in the size of data\nset, Stochastic Gradient Descent performed better than Support Vector Machine.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.09359v1"
    },
    {
        "title": "Machine learning model to project the impact of Ukraine crisis",
        "authors": [
            "Javad T. Firouzjaee",
            "Pouriya Khaliliyan"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Russia's attack on Ukraine on Thursday 24 February 2022 hitched financial\nmarkets and the increased geopolitical crisis. In this paper, we select some\nmain economic indexes, such as Gold, Oil (WTI), NDAQ, and known currency which\nare involved in this crisis and try to find the quantitative effect of this war\non them. To quantify the war effect, we use the correlation feature and the\nrelationships between these economic indices, create datasets, and compare the\nresults of forecasts with real data. To study war effects, we use Machine\nLearning Linear Regression. We carry on empirical experiments and perform on\nthese economic indices datasets to evaluate and predict this war tolls and its\neffects on main economics indexes.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.01738v1"
    },
    {
        "title": "Predicting Value at Risk for Cryptocurrencies With Generalized Random\n  Forests",
        "authors": [
            "Rebekka Buse",
            "Konstantin Görgen",
            "Melanie Schienle"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We study the prediction of Value at Risk (VaR) for cryptocurrencies. In\ncontrast to classic assets, returns of cryptocurrencies are often highly\nvolatile and characterized by large fluctuations around single events.\nAnalyzing a comprehensive set of 105 major cryptocurrencies, we show that\nGeneralized Random Forests (GRF) (Athey, Tibshirani & Wager, 2019) adapted to\nquantile prediction have superior performance over other established methods\nsuch as quantile regression, GARCH-type and CAViaR models. This advantage is\nespecially pronounced in unstable times and for classes of highly-volatile\ncryptocurrencies. Furthermore, we identify important predictors during such\ntimes and show their influence on forecasting over time. Moreover, a\ncomprehensive simulation study also indicates that the GRF methodology is at\nleast on par with existing methods in VaR predictions for standard types of\nfinancial returns and clearly superior in the cryptocurrency setup.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08224v4"
    },
    {
        "title": "Favorit: farmers volatility risk treatment",
        "authors": [
            "Dadasaheb G. Godase",
            "P. R. Sheshagiri Rao",
            "Anil Gore"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  This paper seeks to develop a strategy based on analytics, for an individual\nIndian farmer to tackle market price fluctuations. The idea is to select a\nmonth (or a week or a day) on which to take the produce to market for a good\nreturn on the sale. The choice is based on the history of price data and\nassociated variability. Market-wise price data for the last decade or so are\nused. These ideas are applied to three vegetable crops, tomato, onion, and\ncoriander for some markets in the state of Maharashtra. It is proposed that\nsimilar work should be done crop-wise and market-wise for different parts of\nIndia by local academic groups from any of the subjects such as statistics,\nanalytics, data science, agriculture, business management, commerce, and\neconomics. The objective is to mitigate the adverse impact of price fluctuation\non farmers.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12395v2"
    },
    {
        "title": "Reducing overestimating and underestimating volatility via the augmented\n  blending-ARCH model",
        "authors": [
            "Jun Lu",
            "Shao Yi"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  SVR-GARCH model tends to \"backward eavesdrop\" when forecasting the financial\ntime series volatility in which case it tends to simply produce the prediction\nby deviating the previous volatility. Though the SVR-GARCH model has achieved\ngood performance in terms of various performance measurements, trading\nopportunities, peak or trough behaviors in the time series are all hampered by\nunderestimating or overestimating the volatility. We propose a blending ARCH\n(BARCH) and an augmented BARCH (aBARCH) model to overcome this kind of problem\nand make the prediction towards better peak or trough behaviors. The method is\nillustrated using real data sets including SH300 and S&P500. The empirical\nresults obtained suggest that the augmented and blending models improve the\nvolatility forecasting ability.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12456v1"
    },
    {
        "title": "Neural Network and Order Flow, Technical Analysis: Predicting short-term\n  direction of futures contract",
        "authors": [
            "Yiyang Zheng"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Predictions of short-term directional movement of the futures contract can be\nchallenging as its pricing is often based on multiple complex dynamic\nconditions. This work presents a method for predicting the short-term\ndirectional movement of an underlying futures contract. We engineered a set of\nfeatures from technical analysis, order flow, and order-book data. Then,\nTabnet, a deep learning neural network, is trained using these features. We\ntrain our model on the Silver Futures Contract listed on Shanghai Futures\nExchange and achieve an accuracy of 0.601 on predicting the directional change\nduring the selected period.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12457v1"
    },
    {
        "title": "An Exploratory Study of Stock Price Movements from Earnings Calls",
        "authors": [
            "Sourav Medya",
            "Mohammad Rasoolinejad",
            "Yang Yang",
            "Brian Uzzi"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Financial market analysis has focused primarily on extracting signals from\naccounting, stock price, and other numerical hard data reported in P&L\nstatements or earnings per share reports. Yet, it is well-known that the\ndecision-makers routinely use soft text-based documents that interpret the hard\ndata they narrate. Recent advances in computational methods for analyzing\nunstructured and soft text-based data at scale offer possibilities for\nunderstanding financial market behavior that could improve investments and\nmarket equity. A critical and ubiquitous form of soft data are earnings calls.\nEarnings calls are periodic (often quarterly) statements usually by CEOs who\nattempt to influence investors' expectations of a company's past and future\nperformance. Here, we study the statistical relationship between earnings\ncalls, company sales, stock performance, and analysts' recommendations. Our\nstudy covers a decade of observations with approximately 100,000 transcripts of\nearnings calls from 6,300 public companies from January 2010 to December 2019.\nIn this study, we report three novel findings. First, the buy, sell and hold\nrecommendations from professional analysts made prior to the earnings have low\ncorrelation with stock price movements after the earnings call. Second, using\nour graph neural network based method that processes the semantic features of\nearnings calls, we reliably and accurately predict stock price movements in\nfive major areas of the economy. Third, the semantic features of transcripts\nare more predictive of stock price movements than sales and earnings per share,\ni.e., traditional hard data in most of the cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12460v1"
    },
    {
        "title": "Bubble Prediction of Non-Fungible Tokens (NFTs): An Empirical\n  Investigation",
        "authors": [
            "Kensuke Ito",
            "Kyohei Shibano",
            "Gento Mogi"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Our study empirically predicts the bubble of non-fungible tokens (NFTs):\ntransferable and unique digital assets on public blockchains. This topic is\nimportant because, despite their strong market growth in 2021, NFTs on a\nproject basis have not been investigated in terms of bubble prediction.\nSpecifically, we applied the logarithmic periodic power law (LPPL) model to\ntime-series price data associated with four major NFT projects. The results\nindicate that, as of December 20, 2021, (i) NFTs, in general, are in a small\nbubble (a price decline is predicted), (ii) the Decentraland project is in a\nmedium bubble (a price decline is predicted), and (iii) the Ethereum Name\nService and ArtBlocks projects are in a small negative bubble (a price increase\nis predicted). A future work will involve a prediction refinement considering\nthe heterogeneity of NFTs, comparison with other methods, and the use of more\nenriched data.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.12587v2"
    },
    {
        "title": "Economic state classification and portfolio optimisation with\n  application to stagflationary environments",
        "authors": [
            "Nick James",
            "Max Menzies",
            "Kevin Chin"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Motivated by the current fears of a potentially stagflationary global\neconomic environment, this paper uses new and recently introduced mathematical\ntechniques to study multivariate time series pertaining to country inflation\n(CPI), economic growth (GDP) and equity index behaviours. We begin by assessing\nthe temporal evolution among various economic phenomena, and complement this\nanalysis with `economic driver analysis,' where we decouple country economic\ntrajectories and determine what is most important in their association. Next,\nwe study the temporal self-similarity of global inflation, growth and equity\nindex returns to identify the most anomalous historic periods, and windows in\nthe past that are most similar to current market dynamics. We then introduce a\nnew algorithm to construct economic state classifications and compute an\neconomic state integral, where countries are determined to belong in one of\nfour candidate states based on their inflation and growth behaviours. Finally,\nwe implement a decade-by-decade portfolio optimisation to determine which\nequity indices and portfolio assets have been most beneficial in maximising\nportfolio risk-adjusted returns in various market conditions. This could be of\ngreat interest to those looking for asset allocation guidance in the current\nperiod of high economic uncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.15911v4"
    },
    {
        "title": "Attention-based CNN-LSTM and XGBoost hybrid model for stock prediction",
        "authors": [
            "Zhuangwei Shi",
            "Yang Hu",
            "Guangliang Mo",
            "Jian Wu"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Stock market plays an important role in the economic development. Due to the\ncomplex volatility of the stock market, the research and prediction on the\nchange of the stock price, can avoid the risk for the investors. The\ntraditional time series model ARIMA can not describe the nonlinearity, and can\nnot achieve satisfactory results in the stock prediction. As neural networks\nare with strong nonlinear generalization ability, this paper proposes an\nattention-based CNN-LSTM and XGBoost hybrid model to predict the stock price.\nThe model constructed in this paper integrates the time series model, the\nConvolutional Neural Networks with Attention mechanism, the Long Short-Term\nMemory network, and XGBoost regressor in a non-linear relationship, and\nimproves the prediction accuracy. The model can fully mine the historical\ninformation of the stock market in multiple periods. The stock data is first\npreprocessed through ARIMA. Then, the deep learning architecture formed in\npretraining-finetuning framework is adopted. The pre-training model is the\nAttention-based CNN-LSTM model based on sequence-to-sequence framework. The\nmodel first uses convolution to extract the deep features of the original stock\ndata, and then uses the Long Short-Term Memory networks to mine the long-term\ntime series features. Finally, the XGBoost model is adopted for fine-tuning.\nThe results show that the hybrid model is more effective and the prediction\naccuracy is relatively high, which can help investors or institutions to make\ndecisions and achieve the purpose of expanding return and avoiding risk. Source\ncode is available at\nhttps://github.com/zshicode/Attention-CLX-stock-prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.02623v2"
    },
    {
        "title": "Forecasting Cryptocurrency Returns from Sentiment Signals: An Analysis\n  of BERT Classifiers and Weak Supervision",
        "authors": [
            "Duygu Ider",
            "Stefan Lessmann"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Anticipating price developments in financial markets is a topic of continued\ninterest in forecasting. Funneled by advancements in deep learning and natural\nlanguage processing (NLP) together with the availability of vast amounts of\ntextual data in form of news articles, social media postings, etc., an\nincreasing number of studies incorporate text-based predictors in forecasting\nmodels. We contribute to this literature by introducing weak learning, a\nrecently proposed NLP approach to address the problem that text data is\nunlabeled. Without a dependent variable, it is not possible to finetune\npretrained NLP models on a custom corpus. We confirm that finetuning using weak\nlabels enhances the predictive value of text-based features and raises forecast\naccuracy in the context of predicting cryptocurrency returns. More\nfundamentally, the modeling paradigm we present, weak labeling domain-specific\ntext and finetuning pretrained NLP models, is universally applicable in\n(financial) forecasting and unlocks new ways to leverage text data.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05781v3"
    },
    {
        "title": "Stock Price Prediction using Sentiment Analysis and Deep Learning for\n  Indian Markets",
        "authors": [
            "Narayana Darapaneni",
            "Anwesh Reddy Paduri",
            "Himank Sharma",
            "Milind Manjrekar",
            "Nutan Hindlekar",
            "Pranali Bhagat",
            "Usha Aiyer",
            "Yogesh Agarwal"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Stock market prediction has been an active area of research for a\nconsiderable period. Arrival of computing, followed by Machine Learning has\nupgraded the speed of research as well as opened new avenues. As part of this\nresearch study, we aimed to predict the future stock movement of shares using\nthe historical prices aided with availability of sentiment data. Two models\nwere used as part of the exercise, LSTM was the first model with historical\nprices as the independent variable. Sentiment Analysis captured using Intensity\nAnalyzer was used as the major parameter for Random Forest Model used for the\nsecond part, some macro parameters like Gold, Oil prices, USD exchange rate and\nIndian Govt. Securities yields were also added to the model for improved\naccuracy of the model. As the end product, prices of 4 stocks viz. Reliance,\nHDFC Bank, TCS and SBI were predicted using the aforementioned two models. The\nresults were evaluated using RMSE metric.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05783v1"
    },
    {
        "title": "Prediction of motor insurance claims occurrence as an imbalanced machine\n  learning problem",
        "authors": [
            "Sebastian Baran",
            "Przemysław Rola"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The insurance industry, with its large datasets, is a natural place to use\nbig data solutions. However it must be stressed, that significant number of\napplications for machine learning in insurance industry, like fraud detection\nor claim prediction, deals with the problem of machine learning on an\nimbalanced data set. This is due to the fact that frauds or claims are rare\nevents when compared with the entire population of drivers. The problem of\nimbalanced learning is often hard to overcome. Therefore, the main goal of this\nwork is to present and apply various methods of dealing with an imbalanced\ndataset in the context of claim occurrence prediction in car insurance. In\naddition, the above techniques are used to compare the results of machine\nlearning algorithms in the context of claim occurrence prediction in car\ninsurance. Our study covers the following techniques: logistic-regression,\ndecision tree, random forest, xgBoost, feed-forward network. The problem is the\nclassification one.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.06109v1"
    },
    {
        "title": "NFT Appraisal Prediction: Utilizing Search Trends, Public Market Data,\n  Linear Regression and Recurrent Neural Networks",
        "authors": [
            "Shrey Jain",
            "Camille Bruckmann",
            "Chase McDougall"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In this paper we investigate the correlation between NFT valuations and\nvarious features from three primary categories: public market data, NFT\nmetadata, and social trends data.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12932v1"
    },
    {
        "title": "High-Frequency-Based Volatility Model with Network Structure",
        "authors": [
            "Huiling Yuan",
            "Guodong Li",
            "Junhui Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  This paper introduces one new multivariate volatility model that can\naccommodate an appropriately defined network structure based on low-frequency\nand high-frequency data. The model reduces the number of unknown parameters and\nthe computational complexity substantially. The model parameterization and\niterative multistep-ahead forecasts are discussed and the targeting\nreparameterization is also presented. Quasi-likelihood functions for parameter\nestimation are proposed and their asymptotic properties are established. A\nseries of simulation experiments are carried out to assess the performance of\nthe estimation in finite samples. An empirical example is demonstrated that the\nproposed model outperforms the network GARCH model, with the gains being\nparticularly significant at short forecast horizons.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12933v1"
    },
    {
        "title": "Fitting Generalized Tempered Stable distribution: Fractional Fourier\n  Transform (FRFT) Approach",
        "authors": [
            "A. H. Nzokem",
            "V. T. Montshiwa"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The paper investigates the rich class of Generalized Tempered Stable\ndistribution, an alternative to Normal distribution and the $\\alpha$-Stable\ndistribution for modelling asset return and many physical and economic systems.\nFirstly, we explore some important properties of the Generalized Tempered\nStable (GTS) distribution. The theoretical tools developed are used to perform\nempirical analysis. The GTS distribution is fitted using S&P 500, SPY ETF and\nBitcoin BTC. The Fractional Fourier Transform (FRFT) technique evaluates the\nprobability density function and its derivatives in the maximum likelihood\nprocedure. Based on the results from the statistical inference and the\nKolmogorov-Smirnov (K-S) goodness-of-fit, the GTS distribution fits the\nunderlying distribution of the SPY ETF return. The right side of the Bitcoin\nBTC return, and the left side of the S&P 500 return underlying distributions\nfit the Tempered Stable distribution; while the left side of the Bitcoin BTC\nreturn and the right side of the S&P 500 return underlying distributions are\nmodelled by the compound Poisson process\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00586v2"
    },
    {
        "title": "Large Scale Probabilistic Simulation of Renewables Production",
        "authors": [
            "Mike Ludkovski",
            "Glen Swindle",
            "Eric Grannan"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We develop a probabilistic framework for joint simulation of short-term\nelectricity generation from renewable assets. In this paper we describe a\nmethod for producing hourly day-ahead scenarios of generated power at\ngrid-scale across hundreds of assets. These scenarios are conditional on\nspecified forecasts and yield a full uncertainty quantification both at the\nmarginal asset-level and across asset collections. Our simulation pipeline\nfirst applies asset calibration to normalize hourly, daily and seasonal\ngeneration profiles, and to Gaussianize the forecast--actuals distribution. We\nthen develop a novel clustering approach to stably estimate the covariance\nmatrix across assets; clustering is done hierarchically to achieve scalability.\nAn extended case study using an ERCOT-like system with nearly 500 solar and\nwind farms is used for illustration.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04736v1"
    },
    {
        "title": "Research on the correlation between text emotion mining and stock market\n  based on deep learning",
        "authors": [
            "Chenrui Zhang"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  This paper discusses how to crawl the data of financial forums such as stock\nbar, and conduct emotional analysis combined with the in-depth learning model.\nThis paper will use the Bert model to train the financial corpus and predict\nthe Shenzhen stock index. Through the comparative study of the maximal\ninformation coefficient (MIC), it is found that the emotional characteristics\nobtained by applying the BERT model to the financial corpus can be reflected in\nthe fluctuation of the stock market, which is conducive to effectively improve\nthe prediction accuracy. At the same time, this paper combines in-depth\nlearning with financial texts to further explore the impact mechanism of\ninvestor sentiment on the stock market through in-depth learning, which will\nhelp the national regulatory authorities and policy departments to formulate\nmore reasonable policies and guidelines for maintaining the stability of the\nstock market.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.06675v1"
    },
    {
        "title": "Development of a hybrid method for stock trading based on TOPSIS, EMD\n  and ELM",
        "authors": [
            "Elivelto Ebermam",
            "Helder Knidel",
            "Renato A. Krohling"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Deciding when to buy or sell a stock is not an easy task because the market\nis hard to predict, being influenced by political and economic factors. Thus,\nmethodologies based on computational intelligence have been applied to this\nchallenging problem. In this work, every day the stocks are ranked by technique\nfor order preference by similarity to ideal solution (TOPSIS) using technical\nanalysis criteria, and the most suitable stock is selected for purchase. Even\nso, it may occur that the market is not favorable to purchase on certain days,\nor even, the TOPSIS make an incorrect selection. To improve the selection,\nanother method should be used. So, a hybrid model composed of empirical mode\ndecomposition (EMD) and extreme learning machine (ELM) is proposed. The EMD\ndecomposes the series into several sub-series, and thus the main omponent\n(trend) is extracted. This component is processed by the ELM, which performs\nthe prediction of the next element of component. If the value predicted by the\nELM is greater than the last value, then the purchase of the stock is\nconfirmed. The method was applied in a universe of 50 stocks in the Brazilian\nmarket. The selection made by TOPSIS showed promising results when compared to\nthe random selection and the return generated by the Bovespa index.\nConfirmation with the EMD-ELM hybrid model was able to increase the percentage\nof profit tradings.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.06723v1"
    },
    {
        "title": "Predicting Stock Price Movement after Disclosure of Corporate Annual\n  Reports: A Case Study of 2021 China CSI 300 Stocks",
        "authors": [
            "Fengyu Han",
            "Yue Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In the current stock market, computer science and technology are more and\nmore widely used to analyse stocks. Not same as most related machine learning\nstock price prediction work, this work study the predicting the tendency of the\nstock price on the second day right after the disclosure of the companies'\nannual reports. We use a variety of different models, including decision tree,\nlogistic regression, random forest, neural network, prototypical networks. We\nuse two sets of financial indicators (key and expanded) to conduct experiments,\nthese financial indicators are obtained from the EastMoney website disclosed by\ncompanies, and finally we find that these models are not well behaved to\npredict the tendency. In addition, we also filter stocks with ROE greater than\n0.15 and net cash ratio greater than 0.9. We conclude that according to the\nfinancial indicators based on the just-released annual report of the company,\nthe predictability of the stock price movement on the second day after\ndisclosure is weak, with maximum accuracy about 59.6% and maximum precision\nabout 0.56 on our test set by the random forest classifier, and the stock\nfiltering does not improve the performance. And random forests perform best in\ngeneral among all these models which conforms to some work's findings.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.12528v2"
    },
    {
        "title": "Estimating the Currency Composition of Foreign Exchange Reserves",
        "authors": [
            "Matthew Ferranti"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Central banks manage about \\$12 trillion in foreign exchange reserves,\ninfluencing global exchange rates and asset prices. However, some of the\nlargest holders of reserves report minimal information about their currency\ncomposition, hindering empirical analysis. I describe a Hidden Markov Model to\nestimate the composition of a central bank's reserves by relating the\nfluctuation in the portfolio's valuation to the exchange rates of major reserve\ncurrencies. I apply the model to China and Singapore, two countries that\ncollectively hold about \\$3.4 trillion in reserves and conceal their\ncomposition. I find that both China's reserve composition likely resembles the\nglobal average, while Singapore probably holds fewer US dollars.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13751v4"
    },
    {
        "title": "Clustering of Excursion Sets in Financial Market",
        "authors": [
            "M. Shadmangohar",
            "S. M. S. Movahed"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Relying on the excursion set theory, we compute the number density of local\nextrema and crossing statistics versus the threshold for the stock market\nindices. Comparing the number density of excursion sets calculated numerically\nwith the theoretical prediction for the Gaussian process confirmed that all\ndata sets used in this paper have a surplus (almost lack) value of local\nextrema (up-crossing) density at low (high) thresholds almost around the mean\nvalue implying universal properties for stock indices. We estimate the\nclustering of geometrical measures based on the excess probability of finding\nthe pairs of excursion sets, which clarify well statistical coherency between\nmarkets located in the same geographical region. The cross-correlation of\nexcursion sets between various markets is also considered to construct the\nmatrix of agglomerative hierarchical clustering. Our results demonstrate that\nthe peak statistics is more capable of capturing blocks. Incorporating the\npartitioning approach, we implement the Singular Value Decomposition on the\nmatrix containing the maximum value of unweighted Two-Point Correlation\nFunction of peaks and up-crossing to compute the similarity measure. Our\nresults support that excursion sets are more sensitive than standard measures\nto elucidate the existence of {\\it a priori} crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.03221v1"
    },
    {
        "title": "A note on VIX for postprocessing quantitative strategies",
        "authors": [
            "Jun Lu",
            "Minhui Wu"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In this note, we introduce how to use Volatility Index (VIX) for\npostprocessing quantitative strategies so as to increase the Sharpe ratio and\nreduce trading risks. The signal from this procedure is an indicator of trading\nor not on a daily basis. Finally, we analyze this procedure on SH510300 and\nSH510050 assets. The strategies are evaluated by measurements of Sharpe ratio,\nmax drawdown, and Calmar ratio. However, there is always a risk of loss in\ntrading. The results from the tests are just examples of how the method works;\nno claim is made on the suggestion of real market positions.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.04887v1"
    },
    {
        "title": "Application of Hawkes volatility in the observation of filtered\n  high-frequency price process in tick structures",
        "authors": [
            "Kyungsub Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The Hawkes model is suitable for describing self and mutually exciting random\nevents.\n  In addition, the exponential decay in the Hawkes process allows us to\ncalculate the moment properties in the model.\n  However, due to the complexity of the model and formula, few studies have\nbeen conducted on the performance of Hawkes volatility.\n  In this study, we derived a variance formula that is directly applicable\nunder the general settings of both unmarked and marked Hawkes models for\ntick-level price dynamics.\n  In the marked model, the linear impact function and possible dependency\nbetween the marks and underlying processes are considered.\n  The Hawkes volatility is applied to the mid-price process filtered at\n0.1-second intervals to show reliable results;\n  furthermore, intraday estimation is expected to have high utilization in\nreal-time risk management.\n  We also note the increasing predictive power of intraday Hawkes volatility\nover time and examine the relationship between futures and stock volatilities.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.05939v2"
    },
    {
        "title": "Recurrence measures and transitions in stock market dynamics",
        "authors": [
            "Krishnadas M.",
            "K. P. Harikrishnan",
            "G. Ambika"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The financial markets are understood as complex dynamical systems whose\ndynamics is analysed mostly using nonstationary and brief data sets that\nusually come from stock markets. For such data sets, a reliable method of\nanalysis is based on recurrence plots and recurrence networks, constructed from\nthe data sets over the period of study. In this study, we do a comprehensive\nanalysis of the complexity of the underlying dynamics of 26 markets around the\nglobe using recurrence based measures. We also examine trends in the nature of\ntransitions as revealed from these measures by the sliding window analysis\nalong the time series during the global financial crisis of 2008 and compare\nthat with changes during the most recent pandemic related lock down. We show\nthat the measures derived from recurrence patterns can be used to capture the\nnature of transitions in stock market dynamics. Our study reveals that the\nchanges around 2008 indicate stochasticity driven transition, which is\ndifferent from the transition during the pandemic.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03456v1"
    },
    {
        "title": "Learning Financial Networks with High-frequency Trade Data",
        "authors": [
            "Kara Karpman",
            "Sumanta Basu",
            "David Easley"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Financial networks are typically estimated by applying standard time series\nanalyses to price-based economic variables collected at low-frequency (e.g.,\ndaily or monthly stock returns or realized volatility). These networks are used\nfor risk monitoring and for studying information flows in financial markets.\nHigh-frequency intraday trade data sets may provide additional insights into\nnetwork linkages by leveraging high-resolution information. However, such data\nsets pose significant modeling challenges due to their asynchronous nature,\nnonlinear dynamics, and nonstationarity. To tackle these challenges, we\nestimate financial networks using random forests. The edges in our network are\ndetermined by using microstructure measures of one firm to forecast the sign of\nthe change in a market measure (either realized volatility or returns kurtosis)\nof another firm. We first investigate the evolution of network connectivity in\nthe period leading up to the U.S. financial crisis of 2007-09. We find that the\nnetworks have the highest density in 2007, with high degree connectivity\nassociated with Lehman Brothers in 2006. A second analysis into the nature of\nlinkages among firms suggests that larger firms tend to offer better predictive\npower than smaller firms, a finding qualitatively consistent with prior works\nin the market microstructure literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03568v1"
    },
    {
        "title": "Transformer-Based Deep Learning Model for Stock Price Prediction: A Case\n  Study on Bangladesh Stock Market",
        "authors": [
            "Tashreef Muhammad",
            "Anika Bintee Aftab",
            "Md. Mainul Ahsan",
            "Maishameem Meherin Muhu",
            "Muhammad Ibrahim",
            "Shahidul Islam Khan",
            "Mohammad Shafiul Alam"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In modern capital market the price of a stock is often considered to be\nhighly volatile and unpredictable because of various social, financial,\npolitical and other dynamic factors. With calculated and thoughtful investment,\nstock market can ensure a handsome profit with minimal capital investment,\nwhile incorrect prediction can easily bring catastrophic financial loss to the\ninvestors. This paper introduces the application of a recently introduced\nmachine learning model - the Transformer model, to predict the future price of\nstocks of Dhaka Stock Exchange (DSE), the leading stock exchange in Bangladesh.\nThe transformer model has been widely leveraged for natural language processing\nand computer vision tasks, but, to the best of our knowledge, has never been\nused for stock price prediction task at DSE. Recently the introduction of\ntime2vec encoding to represent the time series features has made it possible to\nemploy the transformer model for the stock price prediction. This paper\nconcentrates on the application of transformer-based model to predict the price\nmovement of eight specific stocks listed in DSE based on their historical daily\nand weekly data. Our experiments demonstrate promising results and acceptable\nroot mean squared error on most of the stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.08300v1"
    },
    {
        "title": "A statistical test of market efficiency based on information theory",
        "authors": [
            "Xavier Brouty",
            "Matthieu Garcin"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We determine the amount of information contained in a time series of price\nreturns at a given time scale, by using a widespread tool of the information\ntheory, namely the Shannon entropy, applied to a symbolic representation of\nthis time series. By deriving the exact and the asymptotic distribution of this\nmarket information indicator in the case where the efficient market hypothesis\nholds, we develop a statistical test of market efficiency. We apply it to a\nreal dataset of stock indices, single stock, and cryptocurrency, for which we\nare able to determine at each date whether the efficient market hypothesis is\nto be rejected, with respect to a given confidence level.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.11976v1"
    },
    {
        "title": "Stock Market Prediction using Natural Language Processing -- A Survey",
        "authors": [
            "Om Mane",
            "Saravanakumar kandasamy"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The stock market is a network which provides a platform for almost all major\neconomic transactions. While investing in the stock market is a good idea,\ninvesting in individual stocks may not be, especially for the casual investor.\nSmart stock-picking requires in-depth research and plenty of dedication.\nPredicting this stock value offers enormous arbitrage profit opportunities.\nThis attractiveness of finding a solution has prompted researchers to find a\nway past problems like volatility, seasonality, and dependence on time. This\npaper surveys recent literature in the domain of natural language processing\nand machine learning techniques used to predict stock market movements. The\nmain contributions of this paper include the sophisticated categorizations of\nmany recent articles and the illustration of the recent trends of research in\nstock market prediction and its related areas.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.13564v1"
    },
    {
        "title": "Predict stock prices with ARIMA and LSTM",
        "authors": [
            "Ruochen Xiao",
            "Yingying Feng",
            "Lei Yan",
            "Yihan Ma"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  MAE, MSE and RMSE performance indicators are used to analyze the performance\nof different stocks predicted by LSTM and ARIMA models in this paper. 50 listed\ncompany stocks from finance.yahoo.com are selected as the research object in\nthe experiments. The dataset used in this work consists of the highest price on\ntransaction days, corresponding to the period from 01 January 2010 to 31\nDecember 2018. For LSTM model, the data from 01 January 2010 to 31 December\n2015 are selected as the training set, the data from 01 January 2016 to 31\nDecember 2017 as the validation set and the data from 01 January 2018 to 31\nDecember 2018 as the test set. In term of ARIMA model, the data from 01 January\n2016 to 31 December 2017 are selected as the training set, and the data from 01\nJanuary 2018 to 31 December 2018 as the test set. For both models, 60 days of\ndata are used to predict the next day. After analysis, it is suggested that\nboth ARIMA and LSTM models can predict stock prices, and the prediction results\nare generally consistent with the actual results;and LSTM has better\nperformance in predicting stock prices(especially in expressing stock price\nchanges), while the application of ARIMA is more convenient.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.02407v1"
    },
    {
        "title": "Predicting Mutual Funds' Performance using Deep Learning and Ensemble\n  Techniques",
        "authors": [
            "Nghia Chu",
            "Binh Dao",
            "Nga Pham",
            "Huy Nguyen",
            "Hien Tran"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Predicting fund performance is beneficial to both investors and fund\nmanagers, and yet is a challenging task. In this paper, we have tested whether\ndeep learning models can predict fund performance more accurately than\ntraditional statistical techniques. Fund performance is typically evaluated by\nthe Sharpe ratio, which represents the risk-adjusted performance to ensure\nmeaningful comparability across funds. We calculated the annualised Sharpe\nratios based on the monthly returns time series data for more than 600 open-end\nmutual funds investing in listed large-cap equities in the United States. We\nfind that long short-term memory (LSTM) and gated recurrent units (GRUs) deep\nlearning methods, both trained with modern Bayesian optimization, provide\nhigher accuracy in forecasting funds' Sharpe ratios than traditional\nstatistical ones. An ensemble method, which combines forecasts from LSTM and\nGRUs, achieves the best performance of all models. There is evidence to say\nthat deep learning and ensembling offer promising solutions in addressing the\nchallenge of fund performance forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.09649v3"
    },
    {
        "title": "Feature-Rich Long-term Bitcoin Trading Assistant",
        "authors": [
            "Jatin Nainani",
            "Nirman Taterh",
            "Md Ausaf Rashid",
            "Ankit Khivasara"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  For a long time predicting, studying and analyzing financial indices has been\nof major interest for the financial community. Recently, there has been a\ngrowing interest in the Deep-Learning community to make use of reinforcement\nlearning which has surpassed many of the previous benchmarks in a lot of\nfields. Our method provides a feature rich environment for the reinforcement\nlearning agent to work on. The aim is to provide long term profits to the user\nso, we took into consideration the most reliable technical indicators. We have\nalso developed a custom indicator which would provide better insights of the\nBitcoin market to the user. The Bitcoin market follows the emotions and\nsentiments of the traders, so another element of our trading environment is the\noverall daily Sentiment Score of the market on Twitter. The agent is tested for\na period of 685 days which also included the volatile period of Covid-19. It\nhas been capable of providing reliable recommendations which give an average\nprofit of about 69%. Finally, the agent is also capable of suggesting the\noptimal actions to the user through a website. Users on the website can also\naccess the visualizations of the indicators to help fortify their decisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.12664v1"
    },
    {
        "title": "Multiclass Sentiment Prediction for Stock Trading",
        "authors": [
            "Marshall R. McCraw"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Python was used to download and format NewsAPI article data relating to 400\npublicly traded, low cap. Biotech companies. Crowd-sourcing was used to label a\nsubset of this data to then train and evaluate a variety of models to classify\nthe public sentiment of each company. The best performing models were then used\nto show that trading entirely off public sentiment could provide market beating\nreturns.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.00870v1"
    },
    {
        "title": "Embedding-based neural network for investment return prediction",
        "authors": [
            "Jianlong Zhu",
            "Dan Xian",
            " Fengxiao",
            "Yichen Nie"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In addition to being familiar with policies, high investment returns also\nrequire extensive knowledge of relevant industry knowledge and news. In\naddition, it is necessary to leverage relevant theories for investment to make\ndecisions, thereby amplifying investment returns. A effective investment return\nestimate can feedback the future rate of return of investment behavior. In\nrecent years, deep learning are developing rapidly, and investment return\nprediction based on deep learning has become an emerging research topic. This\npaper proposes an embedding-based dual branch approach to predict an\ninvestment's return. This approach leverages embedding to encode the investment\nid into a low-dimensional dense vector, thereby mapping high-dimensional data\nto a low-dimensional manifold, so that highdimensional features can be\nrepresented competitively. In addition, the dual branch model realizes the\ndecoupling of features by separately encoding different information in the two\nbranches. In addition, the swish activation function further improves the model\nperformance. Our approach are validated on the Ubiquant Market Prediction\ndataset. The results demonstrate the superiority of our approach compared to\nXgboost, Lightgbm and Catboost.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.00876v1"
    },
    {
        "title": "Non-fungible token transactions: data and challenges",
        "authors": [
            "Jason B. Cho",
            "Sven Serneels",
            "David S. Matteson"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Non-fungible tokens (NFT) have recently emerged as a novel blockchain hosted\nfinancial asset class that has attracted major transaction volumes. Investment\ndecisions rely on data and adequate preprocessing and application of analytics\nto them. Both owing to the non-fungible nature of the tokens and to a\nblockchain being the primary data source, NFT transaction data pose several\nchallenges not commonly encountered in traditional financial data. Using data\nthat consist of the transaction history of eight highly valued NFT collections,\na selection of such challenges is illustrated. These are: price differentiation\nby token traits, the possible existence of lateral swaps and wash trades in the\ntransaction history and finally, severe volatility. While this paper merely\nscratches the surface of how data analytics can be applied in this context, the\ndata and challenges laid out here may present opportunities for future research\non the topic.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.07393v1"
    },
    {
        "title": "Predicting the State of Synchronization of Financial Time Series using\n  Cross Recurrence Plots",
        "authors": [
            "Mostafa Shabani",
            "Martin Magris",
            "George Tzagkarakis",
            "Juho Kanniainen",
            "Alexandros Iosifidis"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Cross-correlation analysis is a powerful tool for understanding the mutual\ndynamics of time series. This study introduces a new method for predicting the\nfuture state of synchronization of the dynamics of two financial time series.\nTo this end, we use the cross-recurrence plot analysis as a nonlinear method\nfor quantifying the multidimensional coupling in the time domain of two time\nseries and for determining their state of synchronization. We adopt a deep\nlearning framework for methodologically addressing the prediction of the\nsynchronization state based on features extracted from dynamically sub-sampled\ncross-recurrence plots. We provide extensive experiments on several stocks,\nmajor constituents of the S\\&P100 index, to empirically validate our approach.\nWe find that the task of predicting the state of synchronization of two time\nseries is in general rather difficult, but for certain pairs of stocks\nattainable with very satisfactory performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.14605v2"
    },
    {
        "title": "Incorporating Interactive Facts for Stock Selection via Neural Recursive\n  ODEs",
        "authors": [
            "Qiang Gao",
            "Xinzhu Zhou",
            "Kunpeng Zhang",
            "Li Huang",
            "Siyuan Liu",
            "Fan Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Stock selection attempts to rank a list of stocks for optimizing investment\ndecision making, aiming at minimizing investment risks while maximizing profit\nreturns. Recently, researchers have developed various (recurrent) neural\nnetwork-based methods to tackle this problem. Without exceptions, they\nprimarily leverage historical market volatility to enhance the selection\nperformance. However, these approaches greatly rely on discrete sampled market\nobservations, which either fail to consider the uncertainty of stock\nfluctuations or predict continuous stock dynamics in the future. Besides, some\nstudies have considered the explicit stock interdependence derived from\nmultiple domains (e.g., industry and shareholder). Nevertheless, the implicit\ncross-dependencies among different domains are under-explored. To address such\nlimitations, we present a novel stock selection solution -- StockODE, a latent\nvariable model with Gaussian prior. Specifically, we devise a Movement Trend\nCorrelation module to expose the time-varying relationships regarding stock\nmovements. We design Neural Recursive Ordinary Differential Equation Networks\n(NRODEs) to capture the temporal evolution of stock volatility in a continuous\ndynamic manner. Moreover, we build a hierarchical hypergraph to incorporate the\ndomain-aware dependencies among the stocks. Experiments conducted on two\nreal-world stock market datasets demonstrate that StockODE significantly\noutperforms several baselines, such as up to 18.57% average improvement\nregarding Sharpe Ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.15925v1"
    },
    {
        "title": "FinBERT-LSTM: Deep Learning based stock price prediction using News\n  Sentiment Analysis",
        "authors": [
            "Shayan Halder"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Economy is severely dependent on the stock market. An uptrend usually\ncorresponds to prosperity while a downtrend correlates to recession. Predicting\nthe stock market has thus been a centre of research and experiment for a long\ntime. Being able to predict short term movements in the market enables\ninvestors to reap greater returns on their investments. Stock prices are\nextremely volatile and sensitive to financial market. In this paper we use Deep\nLearning networks to predict stock prices, assimilating financial, business and\ntechnology news articles which present information about the market. First, we\ncreate a simple Multilayer Perceptron (MLP) network and then expand into more\ncomplex Recurrent Neural Network (RNN) like Long Short Term Memory (LSTM), and\nfinally propose FinBERT-LSTM model, which integrates news article sentiments to\npredict stock price with greater accuracy by analysing short-term market\ninformation. We then train the model on NASDAQ-100 index stock data and New\nYork Times news articles to evaluate the performance of MLP, LSTM, FinBERT-LSTM\nmodels using mean absolute error (MAE), mean absolute percentage error (MAPE)\nand accuracy metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07392v1"
    },
    {
        "title": "Time Series Analysis in American Stock Market Recovering in Post\n  COVID-19 Pandemic Period",
        "authors": [
            "Weilin Fu",
            "Zhuoran Li",
            "Yupeng Zhang",
            "Xingyou Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Every financial crisis has caused a dual shock to the global economy. The\nshortage of market liquidity, such as default in debt and bonds, has led to the\nspread of bankruptcies, such as Lehman Brothers in 2008. Using the data for the\nETFs of the S&P 500, Nasdaq 100, and Dow Jones Industrial Average collected\nfrom Yahoo Finance, this study implemented Deep Learning, Neuro Network, and\nTime-series to analyze the trend of the American Stock Market in the\npost-COVID-19 period. LSTM model in Neuro Network to predict the future trend,\nwhich suggests the US stock market keeps falling for the post-COVID-19 period.\nThis study reveals a reasonable allocation method of Long Short-Term Memory for\nwhich there is strong evidence.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.05369v1"
    },
    {
        "title": "NETpred: Network-based modeling and prediction of multiple connected\n  market indices",
        "authors": [
            "Alireza Jafari",
            "Saman Haratizadeh"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Market prediction plays a major role in supporting financial decisions. An\nemerging approach in this domain is to use graphical modeling and analysis to\nfor prediction of next market index fluctuations. One important question in\nthis domain is how to construct an appropriate graphical model of the data that\ncan be effectively used by a semi-supervised GNN to predict index fluctuations.\nIn this paper, we introduce a framework called NETpred that generates a novel\nheterogeneous graph representing multiple related indices and their stocks by\nusing several stock-stock and stock-index relation measures. It then thoroughly\nselects a diverse set of representative nodes that cover different parts of the\nstate space and whose price movements are accurately predictable. By assigning\ninitial predicted labels to such a set of nodes, NETpred makes sure that the\nsubsequent GCN model can be successfully trained using a semi-supervised\nlearning process. The resulting model is then used to predict the stock labels\nwhich are finally aggregated to infer the labels for all the index nodes in the\ngraph. Our comprehensive set of experiments shows that NETpred improves the\nperformance of the state-of-the-art baselines by 3%-5% in terms of F-score\nmeasure on different well-known data sets.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.05916v1"
    },
    {
        "title": "Nostradamus: Weathering Worth",
        "authors": [
            "Alapan Chaudhuri",
            "Zeeshan Ahmed",
            "Ashwin Rao",
            "Shivansh Subramanian",
            "Shreyas Pradhan",
            "Abhishek Mittal"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Nostradamus, inspired by the French astrologer and reputed seer, is a\ndetailed study exploring relations between environmental factors and changes in\nthe stock market. In this paper, we analyze associative correlation and\ncausation between environmental elements (including natural disasters, climate\nand weather conditions) and stock prices, using historical stock market data,\nhistorical climate data, and various climate indicators such as carbon dioxide\nemissions. We have conducted our study based on the US financial market, global\nclimate trends, and daily weather records to demonstrate a significant\nrelationship between climate and stock price fluctuation. Our analysis covers\nboth short-term and long-term rises and dips in company stock performances.\nLastly, we take four natural disasters as a case study to observe the effect\nthey have on people's emotional state and their influence on the stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.05933v2"
    },
    {
        "title": "Design interpretable experience of dynamical feed forward machine\n  learning model for forecasting NASDAQ",
        "authors": [
            "Pouriya Khalilian",
            "Sara Azizi",
            "Mohammad Hossein Amiri",
            "Javad T. Firouzjaee"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  National Association of Securities Dealers Automated Quotations(NASDAQ) is an\nAmerican stock exchange based. It is one of the most valuable stock economic\nindices in the world and is located in New York City \\cite{pagano2008quality}.\nThe volatility of the stock market and the influence of economic indicators\nsuch as crude oil, gold, and the dollar in the stock market, and NASDAQ shares\nare also affected and have a volatile and chaotic nature\n\\cite{firouzjaee2022lstm}.In this article, we have examined the effect of oil,\ndollar, gold, and the volatility of the stock market in the economic market,\nand then we have also examined the effect of these indicators on NASDAQ stocks.\nThen we started to analyze the impact of the feedback on the past prices of\nNASDAQ stocks and its impact on the current price. Using PCA and Linear\nRegression algorithm, we have designed an optimal dynamic learning experience\nfor modeling these stocks. The results obtained from the quantitative analysis\nare consistent with the results of the qualitative analysis of economic\nstudies, and the modeling done with the optimal dynamic experience of machine\nlearning justifies the current price of NASDAQ shares.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.12044v1"
    },
    {
        "title": "Modeling and Simulation of Financial Returns under Non-Gaussian\n  Distributions",
        "authors": [
            "Federica De Domenico",
            "Giacomo Livan",
            "Guido Montagna",
            "Oreste Nicrosini"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  It is well known that the probability distribution of high-frequency\nfinancial returns is characterized by a leptokurtic, heavy-tailed shape. This\nbehavior undermines the typical assumption of Gaussian log-returns behind the\nstandard approach to risk management and option pricing. Yet, there is no\nconsensus on what class of probability distributions should be adopted to\ndescribe financial returns and different models used in the literature have\ndemonstrated, to varying extent, an ability to reproduce empirically observed\nstylized facts. In order to provide some clarity, in this paper we perform a\nthorough study of the most popular models of return distributions as obtained\nin the empirical analyses of high-frequency financial data. We compare the\nstatistical properties and simulate the dynamics of non-Gaussian financial\nfluctuations by means of Monte Carlo sampling from the different models in\nterms of realistic tail exponents. Our findings show a noticeable consistency\nbetween the considered return distributions in the modeling of the scaling\nproperties of large price changes. We also discuss the convergence rate to the\nasymptotic distributions of the non-Gaussian stochastic processes and we study,\nas a first example of possible applications, the impact of our results on\noption pricing in comparison with the standard Black and Scholes approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.02769v1"
    },
    {
        "title": "DSE Stock Price Prediction using Hidden Markov Model",
        "authors": [
            "Raihan Tanvir",
            "Md Tanvir Rouf Shawon",
            "Md. Golam Rabiul Alam"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Stock market forecasting is a classic problem that has been thoroughly\ninvestigated using machine learning and artificial neural network based tools\nand techniques. Interesting aspects of this problem include its time reliance\nas well as its volatility and other complex relationships. To combine them,\nhidden markov models (HMMs) have been utilized to anticipate the price of\nstocks. We demonstrated the Maximum A Posteriori (MAP) HMM method for\npredicting stock prices for the next day based on previous data. An HMM is\ntrained by analyzing the fractional change in the stock price as well as the\nintraday high and low values. It is then utilized to produce a MAP estimate\nacross all possible stock prices for the next day. The approach demonstrated in\nour work is quite generalized and can be used to predict the stock price for\nany company, given that the HMM is trained on the dataset of that company's\nstocks dataset. We evaluated the accuracy of our models using some extensively\nused accuracy metrics for regression problems and came up with a satisfactory\noutcome.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.08911v1"
    },
    {
        "title": "Multi-kernel property in high-frequency price dynamics under Hawkes\n  model",
        "authors": [
            "Kyungsub Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This study investigates and uses multi-kernel Hawkes models to describe a\nhigh-frequency mid-price process. Each kernel represents a different responsive\nspeed of market participants. Using the conditional Hessian, we examine whether\nthe numerical optimizer effectively finds the global maximum of the\nlog-likelihood function under complicated modeling. Empirical studies that use\nstock prices in the US equity market show the existence of multi-kernels\nclassified as ultra-high-frequency (UHF), very-high-frequency (VHF), and\nhigh-frequency (HF). We estimate the conditional expectations of arrival times\nand the degree of contribution to the high-frequency activities for each\nkernel.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11822v1"
    },
    {
        "title": "Exploring the Advantages of Transformers for High-Frequency Trading",
        "authors": [
            "Fazl Barez",
            "Paul Bilokon",
            "Arthur Gervais",
            "Nikita Lisitsyn"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper explores the novel deep learning Transformers architectures for\nhigh-frequency Bitcoin-USDT log-return forecasting and compares them to the\ntraditional Long Short-Term Memory models. A hybrid Transformer model, called\n\\textbf{HFformer}, is then introduced for time series forecasting which\nincorporates a Transformer encoder, linear decoder, spiking activations, and\nquantile loss function, and does not use position encoding. Furthermore,\npossible high-frequency trading strategies for use with the HFformer model are\ndiscussed, including trade sizing, trading signal aggregation, and minimal\ntrading threshold. Ultimately, the performance of the HFformer and Long\nShort-Term Memory models are assessed and results indicate that the HFformer\nachieves a higher cumulative PnL than the LSTM when trading with multiple\nsignals during backtesting.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.13850v1"
    },
    {
        "title": "Stock Broad-Index Trend Patterns Learning via Domain Knowledge Informed\n  Generative Network",
        "authors": [
            "Jingyi Gu",
            "Fadi P. Deek",
            "Guiling Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Predicting the Stock movement attracts much attention from both industry and\nacademia. Despite such significant efforts, the results remain unsatisfactory\ndue to the inherently complicated nature of the stock market driven by factors\nincluding supply and demand, the state of the economy, the political climate,\nand even irrational human behavior. Recently, Generative Adversarial Networks\n(GAN) have been extended for time series data; however, robust methods are\nprimarily for synthetic series generation, which fall short for appropriate\nstock prediction. This is because existing GANs for stock applications suffer\nfrom mode collapse and only consider one-step prediction, thus underutilizing\nthe potential of GAN. Furthermore, merging news and market volatility are\nneglected in current GANs. To address these issues, we exploit expert domain\nknowledge in finance and, for the first time, attempt to formulate stock\nmovement prediction into a Wasserstein GAN framework for multi-step prediction.\nWe propose IndexGAN, which includes deliberate designs for the inherent\ncharacteristics of the stock market, leverages news context learning to\nthoroughly investigate textual information and develop an attentive seq2seq\nlearning network that captures the temporal dependency among stock prices,\nnews, and market sentiment. We also utilize the critic to approximate the\nWasserstein distance between actual and predicted sequences and develop a\nrolling strategy for deployment that mitigates noise from the financial market.\nExtensive experiments are conducted on real-world broad-based indices,\ndemonstrating the superior performance of our architecture over other\nstate-of-the-art baselines, also validating all its contributing components.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.14164v1"
    },
    {
        "title": "Cryptocurrencies Are Becoming Part of the World Global Financial Market",
        "authors": [
            "Marcin Wątorek",
            "Jarosław Kwapień",
            "Stanisław Drożdż"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In this study the cross-correlations between the cryptocurrency market\nrepresented by the two most liquid and highest-capitalized cryptocurrencies:\nbitcoin and ethereum, on the one side, and the instruments representing the\ntraditional financial markets: stock indices, Forex, commodities, on the other\nside, are measured in the period: January 2020--October 2022. Our purpose is to\naddress the question whether the cryptocurrency market still preserves its\nautonomy with respect to the traditional financial markets or it has already\naligned with them in expense of its independence. We are motivated by the fact\nthat some previous related studies gave mixed results. By calculating the\n$q$-dependent detrended cross-correlation coefficient based on the high\nfrequency 10 s data in the rolling window, the dependence on various time\nscales, different fluctuation magnitudes, and different market periods are\nexamined. There is a strong indication that the dynamics of the bitcoin and\nethereum price changes since the March 2020 Covid-19 panic is no longer\nindependent. Instead, it is related to the dynamics of the traditional\nfinancial markets, which is especially evident now in 2022, when the bitcoin\nand ethereum coupling to the US tech stocks is observed during the market bear\nphase. It is also worth emphasizing that the cryptocurrencies have begun to\nreact to the economic data such as the Consumer Price Index readings in a\nsimilar way as traditional instruments. Such a spontaneous coupling of the so\nfar independent degrees of freedom can be interpreted as a kind of phase\ntransition that resembles the collective phenomena typical for the complex\nsystems. Our results indicate that the cryptocurrencies cannot be considered as\na safe haven for the financial investments.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.00495v1"
    },
    {
        "title": "Optimal probabilistic forecasts for risk management",
        "authors": [
            "Yuru Sun",
            "Worapree Maneesoonthorn",
            "Ruben Loaiza-Maya",
            "Gael M. Martin"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper explores the implications of producing forecast distributions that\nare optimized according to scoring rules that are relevant to financial risk\nmanagement. We assess the predictive performance of optimal forecasts from\npotentially misspecified models for i) value-at-risk and expected shortfall\npredictions; and ii) prediction of the VIX volatility index for use in hedging\nstrategies involving VIX futures. Our empirical results show that calibrating\nthe predictive distribution using a score that rewards the accurate prediction\nof extreme returns improves the VaR and ES predictions. Tail-focused predictive\ndistributions are also shown to yield better outcomes in hedging strategies\nusing VIX futures.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.01651v1"
    },
    {
        "title": "Improving CNN-base Stock Trading By Considering Data Heterogeneity and\n  Burst",
        "authors": [
            "Keer Yang",
            "Guanqun Zhang",
            "Chuan Bi",
            "Qiang Guan",
            "Hailu Xu",
            "Shuai Xu"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In recent years, there have been quite a few attempts to apply intelligent\ntechniques to financial trading, i.e., constructing automatic and intelligent\ntrading framework based on historical stock price. Due to the unpredictable,\nuncertainty and volatile nature of financial market, researchers have also\nresorted to deep learning to construct the intelligent trading framework. In\nthis paper, we propose to use CNN as the core functionality of such framework,\nbecause it is able to learn the spatial dependency (i.e., between rows and\ncolumns) of the input data. However, different with existing deep\nlearning-based trading frameworks, we develop novel normalization process to\nprepare the stock data. In particular, we first empirically observe that the\nstock data is intrinsically heterogeneous and bursty, and then validate the\nheterogeneity and burst nature of stock data from a statistical perspective.\nNext, we design the data normalization method in a way such that the data\nheterogeneity is preserved and bursty events are suppressed. We verify out\ndeveloped CNN-based trading framework plus our new normalization method on 29\nstocks. Experiment results show that our approach can outperform other\ncomparing approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09407v1"
    },
    {
        "title": "Feature Engineering Methods on Multivariate Time-Series Data for\n  Financial Data Science Competitions",
        "authors": [
            "Thomas Wong",
            "Mauricio Barahona"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper is a work in progress. We are looking for collaborators to provide\nus financial datasets in Equity/Futures market to conduct more bench-marking\nstudies. The authors have papers employing similar methods applied on the\nNumerai dataset, which is freely available but obfuscated.\n  We apply different feature engineering methods for time-series to US market\nprice data. The predictive power of models are tested against Numerai-Signals\ntargets.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.16117v2"
    },
    {
        "title": "Modelling Determinants of Cryptocurrency Prices: A Bayesian Network\n  Approach",
        "authors": [
            "Rasoul Amirzadeh",
            "Asef Nazari",
            "Dhananjay Thiruvady",
            "Mong Shan Ee"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The growth of market capitalisation and the number of altcoins\n(cryptocurrencies other than Bitcoin) provide investment opportunities and\ncomplicate the prediction of their price movements. A significant challenge in\nthis volatile and relatively immature market is the problem of predicting\ncryptocurrency prices which needs to identify the factors influencing these\nprices. The focus of this study is to investigate the factors influencing\naltcoin prices, and these factors have been investigated from a causal analysis\nperspective using Bayesian networks. In particular, studying the nature of\ninteractions between five leading altcoins, traditional financial assets\nincluding gold, oil, and S\\&P 500, and social media is the research question.\nTo provide an answer to the question, we create causal networks which are built\nfrom the historic price data of five traditional financial assets, social media\ndata, and price data of altcoins. The ensuing networks are used for causal\nreasoning and diagnosis, and the results indicate that social media (in\nparticular Twitter data in this study) is the most significant influencing\nfactor of the prices of altcoins. Furthermore, it is not possible to generalise\nthe coins' reactions against the changes in the factors. Consequently, the\ncoins need to be studied separately for a particular price movement\ninvestigation.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.16148v1"
    },
    {
        "title": "Explaining Exchange Rate Forecasts with Macroeconomic Fundamentals Using\n  Interpretive Machine Learning",
        "authors": [
            "Davood Pirayesh Neghab",
            "Mucahit Cevik",
            "M. I. M. Wahab"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The complexity and ambiguity of financial and economic systems, along with\nfrequent changes in the economic environment, have made it difficult to make\nprecise predictions that are supported by theory-consistent explanations.\nInterpreting the prediction models used for forecasting important macroeconomic\nindicators is highly valuable for understanding relations among different\nfactors, increasing trust towards the prediction models, and making predictions\nmore actionable. In this study, we develop a fundamental-based model for the\nCanadian-U.S. dollar exchange rate within an interpretative framework. We\npropose a comprehensive approach using machine learning to predict the exchange\nrate and employ interpretability methods to accurately analyze the\nrelationships among macroeconomic variables. Moreover, we implement an ablation\nstudy based on the output of the interpretations to improve the predictive\naccuracy of the models. Our empirical results show that crude oil, as Canada's\nmain commodity export, is the leading factor that determines the exchange rate\ndynamics with time-varying effects. The changes in the sign and magnitude of\nthe contributions of crude oil to the exchange rate are consistent with\nsignificant events in the commodity and energy markets and the evolution of the\ncrude oil trend in Canada. Gold and the TSX stock index are found to be the\nsecond and third most important variables that influence the exchange rate.\nAccordingly, this analysis provides trustworthy and practical insights for\npolicymakers and economists and accurate knowledge about the predictive model's\ndecisions, which are supported by theoretical considerations.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.16149v1"
    },
    {
        "title": "Optimal Cross-Correlation Estimates from Asynchronous Tick-by-Tick\n  Trading Data",
        "authors": [
            "William H. Press"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Given two time series, A and B, sampled asynchronously at different times\n{t_A_i} and {t_B_j}, termed \"ticks\", how can one best estimate the correlation\ncoefficient \\rho between changes in A and B? We derive a natural,\nminimum-variance estimator that does not use any interpolation or binning, then\nderive from it a fast (linear time) estimator that is demonstrably nearly as\ngood. This \"fast tickwise estimator\" is compared in simulation to the usual\nmethod of interpolating changes to a regular grid. Even when the grid spacing\nis optimized for the particular parameters (not often possible in practice),\nthe fast tickwise estimator has generally smaller estimation errors, often by a\nlarge factor. These results are directly applicable to tick-by-tick price data\nof financial assets.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.16153v1"
    },
    {
        "title": "Entropy of financial time series due to the shock of war",
        "authors": [
            "Ewa A. Drzazga-Szczȩśniak",
            "Piotr Szczepanik",
            "Adam Z. Kaczmarek",
            "Dominik Szczȩśniak"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The concept of entropy is not uniquely relevant to the statistical mechanics\nbut among others it can play pivotal role in the analysis of a time series,\nparticularly the stock market data. In this area sudden events are especially\ninteresting as they describe abrupt data changes which may have long-lasting\neffects. Here, we investigate the impact of such events on the entropy of\nfinancial time series. As a case study we assume data of polish stock market in\nthe context of its main cumulative index. This index is discussed for the\nfinite time periods before and after outbreak of the 2022 Russian invasion of\nUkraine, acting as the sudden event. The analysis allows us to validate the\nentropy-based methodology in assessing market changes as driven by the extreme\nexternal factors. We show that qualitative features of market changes can be\ncaptured quantitatively in terms of the entropy. In addition to that, the\nmagnitude of the impact is analysed over various time periods in terms of the\nintroduced entropic index. To this end, the present work also attempts to\nanswer whether or not the recent war can be considered as a reason or at least\ncatalyst to the current economic crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.16155v1"
    },
    {
        "title": "Can ChatGPT Forecast Stock Price Movements? Return Predictability and\n  Large Language Models",
        "authors": [
            "Alejandro Lopez-Lira",
            "Yuehua Tang"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We document the capability of large language models (LLMs) like ChatGPT to\npredict stock price movements using news headlines, even without direct\nfinancial training. ChatGPT scores significantly predict out-of-sample daily\nstock returns, subsuming traditional methods, and predictability is stronger\namong smaller stocks and following negative news. To explain these findings, we\ndevelop a theoretical model incorporating information capacity constraints,\nunderreaction, limits-to-arbitrage, and LLMs. The model generates several key\npredictions, which we empirically test: (i) it establishes a critical threshold\nin AI capabilities necessary for profitable predictions, (ii) it demonstrates\nthat only advanced LLMs can effectively interpret complex information, and\n(iii) it predicts that widespread LLM adoption can enhance market efficiency.\nOur results suggest that sophisticated return forecasting is an emerging\ncapability of AI systems and that these technologies can alter information\ndiffusion and decision-making processes in financial markets. Finally, we\nintroduce an interpretability framework to evaluate LLMs' reasoning,\ncontributing to AI transparency and economic decision-making.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.07619v5"
    },
    {
        "title": "Parameterized Neural Networks for Finance",
        "authors": [
            "Daniel Oeltz",
            "Jan Hamaekers",
            "Kay F. Pilz"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We discuss and analyze a neural network architecture, that enables learning a\nmodel class for a set of different data samples rather than just learning a\nsingle model for a specific data sample. In this sense, it may help to reduce\nthe overfitting problem, since, after learning the model class over a larger\ndata sample consisting of such different data sets, just a few parameters need\nto be adjusted for modeling a new, specific problem. After analyzing the method\ntheoretically and by regression examples for different one-dimensional\nproblems, we finally apply the approach to one of the standard problems asset\nmanagers and banks are facing: the calibration of spread curves. The presented\nresults clearly show the potential that lies within this method. Furthermore,\nthis application is of particular interest to financial practitioners, since\nnearly all asset managers and banks which are having solutions in place may\nneed to adapt or even change their current methodologies when ESG ratings\nadditionally affect the bond spreads.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.08883v1"
    },
    {
        "title": "Identifying Trades Using Technical Analysis and ML/DL Models",
        "authors": [
            "Aayush Shah",
            "Mann Doshi",
            "Meet Parekh",
            "Nirmit Deliwala",
            "Pramila M. Chawan"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The importance of predicting stock market prices cannot be overstated. It is\na pivotal task for investors and financial institutions as it enables them to\nmake informed investment decisions, manage risks, and ensure the stability of\nthe financial system. Accurate stock market predictions can help investors\nmaximize their returns and minimize their losses, while financial institutions\ncan use this information to develop effective risk management policies.\nHowever, stock market prediction is a challenging task due to the complex\nnature of the stock market and the multitude of factors that can affect stock\nprices. As a result, advanced technologies such as deep learning are being\nincreasingly utilized to analyze vast amounts of data and provide valuable\ninsights into the behavior of the stock market. While deep learning has shown\npromise in accurately predicting stock prices, there is still much research to\nbe done in this area.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09936v1"
    },
    {
        "title": "Stock Price Predictability and the Business Cycle via Machine Learning",
        "authors": [
            "Li Rong Wang",
            "Hsuan Fu",
            "Xiuyi Fan"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We study the impacts of business cycles on machine learning (ML) predictions.\nUsing the S&P 500 index, we find that ML models perform worse during most\nrecessions, and the inclusion of recession history or the risk-free rate does\nnot necessarily improve their performance. Investigating recessions where\nmodels perform well, we find that they exhibit lower market volatility than\nother recessions. This implies that the improved performance is not due to the\nmerit of ML methods but rather factors such as effective monetary policies that\nstabilized the market. We recommend that ML practitioners evaluate their models\nduring both recessions and expansions.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09937v1"
    },
    {
        "title": "Using a Deep Learning Model to Simulate Human Stock Trader's Methods of\n  Chart Analysis",
        "authors": [
            "Sungwoo Kang",
            "Jong-Kook Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Despite the efficient market hypothesis, many studies suggest the existence\nof inefficiencies in the stock market leading to the development of techniques\nto gain above-market returns. Systematic trading has undergone significant\nadvances in recent decades with deep learning schemes emerging as a powerful\ntool for analyzing and predicting market behavior. In this paper, a method is\nproposed that is inspired by how professional technical analysts trade. This\nscheme looks at stock prices of the previous 600 days and predicts whether the\nstock price will rise or fall 10% or 20% within the next D days. The proposed\nmethod uses the Resnet's (a deep learning model) skip connections and logits to\nincrease the probability of the prediction. The model was trained and tested\nusing historical data from both the Korea and US stock markets. The backtest is\ndone using the data from 2020 to 2022. Using the proposed method for the Korea\nmarket it gave return of 75.36% having Sharpe ratio of 1.57, which far exceeds\nthe market return by 36% and 0.61, respectively. On the US market it gives\ntotal return of 27.17% with Sharpe ratio of 0.61, which outperforms other\nbenchmarks such as NASDAQ, S&P500, DOW JONES index by 17.69% and 0.27,\nrespectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.14870v3"
    },
    {
        "title": "Complexity measure, kernel density estimation, bandwidth selection, and\n  the efficient market hypothesis",
        "authors": [
            "Matthieu Garcin"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We are interested in the nonparametric estimation of the probability density\nof price returns, using the kernel approach. The output of the method heavily\nrelies on the selection of a bandwidth parameter. Many selection methods have\nbeen proposed in the statistical literature. We put forward an alternative\nselection method based on a criterion coming from information theory and from\nthe physics of complex systems: the bandwidth to be selected maximizes a new\nmeasure of complexity, with the aim of avoiding both overfitting and\nunderfitting. We review existing methods of bandwidth selection and show that\nthey lead to contradictory conclusions regarding the complexity of the\nprobability distribution of price returns. This has also some striking\nconsequences in the evaluation of the relevance of the efficient market\nhypothesis. We apply these methods to real financial data, focusing on the\nBitcoin.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.13123v1"
    },
    {
        "title": "Support for Stock Trend Prediction Using Transformers and Sentiment\n  Analysis",
        "authors": [
            "Harsimrat Kaeley",
            "Ye Qiao",
            "Nader Bagherzadeh"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Stock trend analysis has been an influential time-series prediction topic due\nto its lucrative and inherently chaotic nature. Many models looking to\naccurately predict the trend of stocks have been based on Recurrent Neural\nNetworks (RNNs). However, due to the limitations of RNNs, such as gradient\nvanish and long-term dependencies being lost as sequence length increases, in\nthis paper we develop a Transformer based model that uses technical stock data\nand sentiment analysis to conduct accurate stock trend prediction over long\ntime windows. This paper also introduces a novel dataset containing daily\ntechnical stock data and top news headline data spanning almost three years.\nStock prediction based solely on technical data can suffer from lag caused by\nthe inability of stock indicators to effectively factor in breaking market\nnews. The use of sentiment analysis on top headlines can help account for\nunforeseen shifts in market conditions caused by news coverage. We measure the\nperformance of our model against RNNs over sequence lengths spanning 5 business\ndays to 30 business days to mimic different length trading strategies. This\nreveals an improvement in directional accuracy over RNNs as sequence length is\nincreased, with the largest improvement being close to 18.63% at 30 business\ndays.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.14368v1"
    },
    {
        "title": "Predicting Stock Market Time-Series Data using CNN-LSTM Neural Network\n  Model",
        "authors": [
            "Aadhitya A",
            "Rajapriya R",
            "Vineetha R S",
            "Anurag M Bagde"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Stock market is often important as it represents the ownership claims on\nbusinesses. Without sufficient stocks, a company cannot perform well in\nfinance. Predicting a stock market performance of a company is nearly hard\nbecause every time the prices of a company stock keeps changing and not\nconstant. So, its complex to determine the stock data. But if the previous\nperformance of a company in stock market is known, then we can track the data\nand provide predictions to stockholders in order to wisely take decisions on\nhandling the stocks to a company. To handle this, many machine learning models\nhave been invented but they didn't succeed due to many reasons like absence of\nadvanced libraries, inaccuracy of model when made to train with real time data\nand much more. So, to track the patterns and the features of data, a CNN-LSTM\nNeural Network can be made. Recently, CNN is now used in Natural Language\nProcessing (NLP) based applications, so by identifying the features from stock\ndata and converting them into tensors, we can obtain the features and then send\nit to LSTM neural network to find the patterns and thereby predicting the stock\nmarket for given period of time. The accuracy of the CNN-LSTM NN model is found\nto be high even when allowed to train on real-time stock market data. This\npaper describes about the features of the custom CNN-LSTM model, experiments we\nmade with the model (like training with stock market datasets, performance\ncomparison with other models) and the end product we obtained at final stage.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.14378v1"
    },
    {
        "title": "Stock and market index prediction using Informer network",
        "authors": [
            "Yuze Lu",
            "Hailong Zhang",
            "Qiwen Guo"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Applications of deep learning in financial market prediction has attracted\nhuge attention from investors and researchers. In particular, intra-day\nprediction at the minute scale, the dramatically fluctuating volume and stock\nprices within short time periods have posed a great challenge for the\nconvergence of networks result. Informer is a more novel network, improved on\nTransformer with smaller computational complexity, longer prediction length and\nglobal time stamp features. We have designed three experiments to compare\nInformer with the commonly used networks LSTM, Transformer and BERT on 1-minute\nand 5-minute frequencies for four different stocks/ market indices. The\nprediction results are measured by three evaluation criteria: MAE, RMSE and\nMAPE. Informer has obtained best performance among all the networks on every\ndataset. Network without the global time stamp mechanism has significantly\nlower prediction effect compared to the complete Informer; it is evident that\nthis mechanism grants the time series to the characteristics and substantially\nimproves the prediction accuracy of the networks. Finally, transfer learning\ncapability experiment is conducted, Informer also achieves a good performance.\nInformer has good robustness and improved performance in market prediction,\nwhich can be exactly adapted to real trading.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.14382v1"
    },
    {
        "title": "Using Internal Bar Strength as a Key Indicator for Trading Country ETFs",
        "authors": [
            "Aditya Pandey",
            "Kunal Joshi"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This report aims to investigate the effectiveness of using internal bar\nstrength (IBS) as a key indicator for trading country exchange-traded funds\n(ETFs). The study uses a quantitative approach to analyze historical price data\nfor a bucket of country ETFs over a period of 10 years and uses the idea of\nMean Reversion to create a profitable trading strategy. Our findings suggest\nthat IBS can be a useful technical indicator for predicting short-term price\nmovements in this basket of ETFs.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12434v1"
    },
    {
        "title": "Fractal properties, information theory, and market efficiency",
        "authors": [
            "Xavier Brouty",
            "Matthieu Garcin"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Considering that both the entropy-based market information and the Hurst\nexponent are useful tools for determining whether the efficient market\nhypothesis holds for a given asset, we study the link between the two\napproaches. We thus provide a theoretical expression for the market information\nwhen log-prices follow either a fractional Brownian motion or its stationary\nextension using the Lamperti transform. In the latter model, we show that a\nHurst exponent close to 1/2 can lead to a very high informativeness of the time\nseries, because of the stationarity mechanism. In addition, we introduce a\nmultiscale method to get a deeper interpretation of the entropy and of the\nmarket information, depending on the size of the information set. Applications\nto Bitcoin, CAC 40 index, Nikkei 225 index, and EUR/USD FX rate, using daily or\nintraday data, illustrate the methodological content.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.13371v1"
    },
    {
        "title": "Higher-order Graph Attention Network for Stock Selection with Joint\n  Analysis",
        "authors": [
            "Yang Qiao",
            "Yiping Xia",
            "Xiang Li",
            "Zheng Li",
            "Yan Ge"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Stock selection is important for investors to construct profitable\nportfolios. Graph neural networks (GNNs) are increasingly attracting\nresearchers for stock prediction due to their strong ability of relation\nmodelling and generalisation. However, the existing GNN methods only focus on\nsimple pairwise stock relation and do not capture complex higher-order\nstructures modelling relations more than two nodes. In addition, they only\nconsider factors of technical analysis and overlook factors of fundamental\nanalysis that can affect the stock trend significantly. Motivated by them, we\npropose higher-order graph attention network with joint analysis (H-GAT). H-GAT\nis able to capture higher-order structures and jointly incorporate factors of\nfundamental analysis with factors of technical analysis. Specifically, the\nsequential layer of H-GAT take both types of factors as the input of a\nlong-short term memory model. The relation embedding layer of H-GAT constructs\na higher-order graph and learn node embedding with GAT. We then predict the\nranks of stock return. Extensive experiments demonstrate the superiority of our\nH-GAT method on the profitability test and Sharp ratio over both NSDAQ and NYSE\ndatasets\n",
        "pdf_link": "http://arxiv.org/pdf/2306.15526v1"
    },
    {
        "title": "Are there Dragon Kings in the Stock Market?",
        "authors": [
            "Jiong Liu",
            "M. Dashti Moghaddam",
            "R. A. Serota"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We undertake a systematic study of historic market volatility spanning\nroughly five preceding decades. We focus specifically on the time series of\nrealized volatility (RV) of the S&P500 index and its distribution function. As\nexpected, the largest values of RV coincide with the largest economic upheavals\nof the period: Savings and Loan Crisis, Tech Bubble, Financial Crisis and Covid\nPandemic. We address the question of whether these values belong to one of the\nthree categories: Black Swans (BS), that is they lie on scale-free, power-law\ntails of the distribution; Dragon Kings (DK), defined as statistically\nsignificant upward deviations from BS; or Negative Dragons Kings (nDK), defined\nas statistically significant downward deviations from BS. In analyzing the\ntails of the distribution with RV > 40, we observe the appearance of\n\"potential\" DK which eventually terminate in an abrupt plunge to nDK. This\nphenomenon becomes more pronounced with the increase of the number of days over\nwhich the average RV is calculated -- here from daily, n=1, to \"monthly,\" n=21.\nWe fit the entire distribution with a modified Generalized Beta (mGB)\ndistribution function, which terminates at a finite value of the variable but\nexhibits a long power-law stretch prior to that, as well as Generalized Beta\nPrime (GB2) distribution function, which has a power-law tail. We also fit the\ntails directly with a straight line on a log-log scale. In order to ascertain\nBS, DK or nDK behavior, all fits include their confidence intervals and\np-values are evaluated for the data points to check if they can come from the\nrespective distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.03693v1"
    },
    {
        "title": "Contrasting the efficiency of stock price prediction models using\n  various types of LSTM models aided with sentiment analysis",
        "authors": [
            "Varun Sangwan",
            "Vishesh Kumar Singh",
            "Bibin Christopher V"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Our research aims to find the best model that uses companies projections and\nsector performances and how the given company fares accordingly to correctly\npredict equity share prices for both short and long term goals.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.07868v1"
    },
    {
        "title": "Joint Latent Topic Discovery and Expectation Modeling for Financial\n  Markets",
        "authors": [
            "Lili Wang",
            "Chenghan Huang",
            "Chongyang Gao",
            "Weicheng Ma",
            "Soroush Vosoughi"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In the pursuit of accurate and scalable quantitative methods for financial\nmarket analysis, the focus has shifted from individual stock models to those\ncapturing interrelations between companies and their stocks. However, current\nrelational stock methods are limited by their reliance on predefined stock\nrelationships and the exclusive consideration of immediate effects. To address\nthese limitations, we present a groundbreaking framework for financial market\nanalysis. This approach, to our knowledge, is the first to jointly model\ninvestor expectations and automatically mine latent stock relationships.\nComprehensive experiments conducted on China's CSI 300, one of the world's\nlargest markets, demonstrate that our model consistently achieves an annual\nreturn exceeding 10%. This performance surpasses existing benchmarks, setting a\nnew state-of-the-art standard in stock return prediction and multiyear trading\nsimulations (i.e., backtesting).\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08649v1"
    },
    {
        "title": "Shannon entropy to quantify complexity in the financial market",
        "authors": [
            "Alexis Rodriguez Carranza",
            "José Luis Ponte Bejarano",
            "Juan Carlos Ponte Bejarano",
            "Segundo Eloy Soto Abanto"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In this paper we study the complexity in the information traffic that occurs\nin the peruvian financial market, using the Shannon entropy. Different series\nof prices of shares traded on the Lima stock exchange are used to reconstruct\nthe unknown dynamics. We present numerical simulations on the reconstructed\ndynamics and we calculate the Shannon entropy to measure its complexity\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08666v1"
    },
    {
        "title": "Memory Effects, Multiple Time Scales and Local Stability in Langevin\n  Models of the S&P500 Market Correlation",
        "authors": [
            "Tobias Wand",
            "Martin Heßler",
            "Oliver Kamps"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The analysis of market correlations is crucial for optimal portfolio\nselection of correlated assets, but their memory effects have often been\nneglected. In this work, we analyse the mean market correlation of the S&P500\nwhich corresponds to the main market mode in principle component analysis. We\nfit a generalised Langevin equation (GLE) to the data whose memory kernel\nimplies that there is a significant memory effect in the market correlation\nranging back at least three trading weeks. The memory kernel improves the\nforecasting accuracy of the GLE compared to models without memory and hence,\nsuch a memory effect has to be taken into account for optimal portfolio\nselection to minimise risk or for predicting future correlations. Moreover, a\nBayesian resilience estimation provides further evidence for non-Markovianity\nin the data and suggests the existence of a hidden slow time scale that\noperates on much slower times than the observed daily market data. Assuming\nthat such a slow time scale exists, our work supports previous research on the\nexistence of locally stable market states.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.12744v1"
    },
    {
        "title": "Microstructure-Empowered Stock Factor Extraction and Utilization",
        "authors": [
            "Xianfeng Jiao",
            "Zizhong Li",
            "Chang Xu",
            "Yang Liu",
            "Weiqing Liu",
            "Jiang Bian"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  High-frequency quantitative investment is a crucial aspect of stock\ninvestment. Notably, order flow data plays a critical role as it provides the\nmost detailed level of information among high-frequency trading data, including\ncomprehensive data from the order book and transaction records at the tick\nlevel. The order flow data is extremely valuable for market analysis as it\nequips traders with essential insights for making informed decisions. However,\nextracting and effectively utilizing order flow data present challenges due to\nthe large volume of data involved and the limitations of traditional factor\nmining techniques, which are primarily designed for coarser-level stock data.\nTo address these challenges, we propose a novel framework that aims to\neffectively extract essential factors from order flow data for diverse\ndownstream tasks across different granularities and scenarios. Our method\nconsists of a Context Encoder and an Factor Extractor. The Context Encoder\nlearns an embedding for the current order flow data segment's context by\nconsidering both the expected and actual market state. In addition, the Factor\nExtractor uses unsupervised learning methods to select such important signals\nthat are most distinct from the majority within the given context. The\nextracted factors are then utilized for downstream tasks. In empirical studies,\nour proposed framework efficiently handles an entire year of stock order flow\ndata across diverse scenarios, offering a broader range of applications\ncompared to existing tick-level approaches that are limited to only a few days\nof stock data. We demonstrate that our method extracts superior factors from\norder flow data, enabling significant improvement for stock trend prediction\nand order execution tasks at the second and minute level.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08135v1"
    },
    {
        "title": "Effects of Daily News Sentiment on Stock Price Forecasting",
        "authors": [
            "S. Srinivas",
            "R. Gadela",
            "R. Sabu",
            "A. Das",
            "G. Nath",
            "V. Datla"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Predicting future prices of a stock is an arduous task to perform. However,\nincorporating additional elements can significantly improve our predictions,\nrather than relying solely on a stock's historical price data to forecast its\nfuture price. Studies have demonstrated that investor sentiment, which is\nimpacted by daily news about the company, can have a significant impact on\nstock price swings. There are numerous sources from which we can get this\ninformation, but they are cluttered with a lot of noise, making it difficult to\naccurately extract the sentiments from them. Hence the focus of our research is\nto design an efficient system to capture the sentiments from the news about the\nNITY50 stocks and investigate how much the financial news sentiment of these\nstocks are affecting their prices over a period of time. This paper presents a\nrobust data collection and preprocessing framework to create a news database\nfor a timeline of around 3.7 years, consisting of almost half a million news\narticles. We also capture the stock price information for this timeline and\ncreate multiple time series data, that include the sentiment scores from\nvarious sections of the article, calculated using different sentiment\nlibraries. Based on this, we fit several LSTM models to forecast the stock\nprices, with and without using the sentiment scores as features and compare\ntheir performances.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08549v1"
    },
    {
        "title": "Recurrent Neural Networks with more flexible memory: better predictions\n  than rough volatility",
        "authors": [
            "Damien Challet",
            "Vincent Ragel"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We extend recurrent neural networks to include several flexible timescales\nfor each dimension of their output, which mechanically improves their abilities\nto account for processes with long memory or with highly disparate time scales.\nWe compare the ability of vanilla and extended long short term memory networks\n(LSTMs) to predict asset price volatility, known to have a long memory.\nGenerally, the number of epochs needed to train extended LSTMs is divided by\ntwo, while the variation of validation and test losses among models with the\nsame hyperparameters is much smaller. We also show that the model with the\nsmallest validation loss systemically outperforms rough volatility predictions\nby about 20% when trained and tested on a dataset with multiple time series.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08550v1"
    },
    {
        "title": "BIRP: Bitcoin Information Retrieval Prediction Model Based on Multimodal\n  Pattern Matching",
        "authors": [
            "Minsuk Kim",
            "Byungchul Kim",
            "Junyeong Yong",
            "Jeongwoo Park",
            "Gyeongmin Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Financial time series have historically been assumed to be a martingale\nprocess under the Random Walk hypothesis. Instead of making investment\ndecisions using the raw prices alone, various multimodal pattern matching\nalgorithms have been developed to help detect subtly hidden repeatable patterns\nwithin the financial market. Many of the chart-based pattern matching tools\nonly retrieve similar past chart (PC) patterns given the current chart (CC)\npattern, and leaves the entire interpretive and predictive analysis, thus\nultimately the final investment decision, to the investors. In this paper, we\npropose an approach of ranking similar PC movements given the CC information\nand show that exploiting this as additional features improves the directional\nprediction capacity of our model. We apply our ranking and directional\nprediction modeling methodologies on Bitcoin due to its highly volatile prices\nthat make it challenging to predict its future movements.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08558v1"
    },
    {
        "title": "iCOS: Option-Implied COS Method",
        "authors": [
            "Evgenii Vladimirov"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper proposes the option-implied Fourier-cosine method, iCOS, for\nnon-parametric estimation of risk-neutral densities, option prices, and option\nsensitivities. The iCOS method leverages the Fourier-based COS technique,\nproposed by Fang and Oosterlee (2008), by utilizing the option-implied cosine\nseries coefficients. Notably, this procedure does not rely on any model\nassumptions about the underlying asset price dynamics, it is fully\nnon-parametric, and it does not involve any numerical optimization. These\nfeatures make it rather general and computationally appealing. Furthermore, we\nderive the asymptotic properties of the proposed non-parametric estimators and\nstudy their finite-sample behavior in Monte Carlo simulations. Our empirical\nanalysis using S&P 500 index options and Amazon equity options illustrates the\neffectiveness of the iCOS method in extracting valuable information from option\nprices under different market conditions. Additionally, we apply our\nmethodology to dissect and quantify observation and discretization errors in\nthe VIX index.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.00943v2"
    },
    {
        "title": "Desenvolvimento de modelo para predição de cotações de\n  ação baseada em análise de sentimentos de tweets",
        "authors": [
            "Mario Mitsuo Akita",
            "Everton Josue da Silva"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Training machine learning models for predicting stock market share prices is\nan active area of research since the automatization of trading such papers was\navailable in real time. While most of the work in this field of research is\ndone by training Neural networks based on past prices of stock shares, in this\nwork, we use iFeel 2.0 platform to extract 19 sentiment features from posts\nobtained from microblog platform Twitter that mention the company Petrobras.\nThen, we used those features to train XBoot models to predict future stock\nprices for the referred company. Later, we simulated the trading of Petrobras'\nshares based on the model's outputs and determined the gain of R$88,82 (net) in\na 250-day period when compared to a 100 random models' average performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.06538v1"
    },
    {
        "title": "Feature selection and regression methods for stock price prediction\n  using technical indicators",
        "authors": [
            "Fatemeh Moodi",
            "Amir Jahangard-Rafsanjani",
            "Sajad Zarifzadeh"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Due to the influence of many factors, including technical indicators on stock\nprice prediction, feature selection is important to choose the best indicators.\nThis study uses technical indicators and features selection and regression\nmethods to solve the problem of closing the stock market price. The aim of this\nresearch is to predict the stock market price with the least error. By the\nproposed method, the data created by the 3-day time window were converted to\nthe appropriate input for regression methods. In this paper, 10 regressor and\n123 technical indicators have been examined on data of the last 13 years of\nApple Company. The results have been investigated by 5 error-based evaluation\ncriteria. Based on results of the proposed method, MLPSF has 56/47% better\nperformance than MLP. Also, SVRSF has 67/42% improved compared to SVR. LRSF was\n76.7 % improved compared to LR. The RISF method also improved 72.82 % of Ridge\nregression. The DTRSB method had 24.23 % improvement over DTR. KNNSB had 15.52\n% improvement over KNN regression. RFSB had a 6 % improvement over RF. GBRSF\nalso improved at 7% over GBR. Finally, ADASF and ADASB also had a 4%\nimprovement over the ADA regression. Also, Ridge and LinearRegression had the\nbest results for stock price prediction. Based on results, the best indicators\nto predict stock price are: the Squeeze_pro, Percentage Price Oscillator,\nThermo, Decay, Archer On-Balance Volume, Bollinger Bands, Squeeze and Ichimoku\nindicator. According to the results, the use of suitable combination of\nsuggested indicators along with regression methods has resulted in high\naccuracy in predicting the closing price.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.09903v4"
    },
    {
        "title": "Dual-Class Stocks: Can They Serve as Effective Predictors?",
        "authors": [
            "Veli Safak"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Kardemir Karabuk Iron Steel Industry Trade & Co. Inc., ranked as the 24th\nlargest industrial company in Turkey, offers three distinct stocks listed on\nthe Borsa Istanbul: KRDMA, KRDMB, and KRDMD. These stocks, sharing the sole\ndifference in voting power, have exhibited significant price divergence over an\nextended period. This paper conducts an in-depth analysis of the divergence\npatterns observed in these three stock prices from January 2001 to July 2023.\nAdditionally, it introduces an innovative training set selection rule tailored\nfor LSTM models, incorporating a rolling training set, and demonstrates its\nsignificant predictive superiority over the conventional use of LSTM models\nwith large training sets. Despite their strong correlation, the study found no\ncompelling evidence supporting the efficiency of dual-class stocks as\npredictors of each other's performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.16845v1"
    },
    {
        "title": "Correlation structure analysis of the global agricultural futures market",
        "authors": [
            "Yun-Shi Dai",
            "Ngoc Quang Anh Huynh",
            "Qing-Huan Zheng",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper adopts the random matrix theory (RMT) to analyze the correlation\nstructure of the global agricultural futures market from 2000 to 2020. It is\nfound that the distribution of correlation coefficients is asymmetric and right\nskewed, and many eigenvalues of the correlation matrix deviate from the RMT\nprediction. The largest eigenvalue reflects a collective market effect common\nto all agricultural futures, the other largest deviating eigenvalues can be\nimplemented to identify futures groups, and there are modular structures based\non regional properties or agricultural commodities among the significant\nparticipants of their corresponding eigenvectors. Except for the smallest\neigenvalue, other smallest deviating eigenvalues represent the agricultural\nfutures pairs with highest correlations. This paper can be of reference and\nsignificance for using agricultural futures to manage risk and optimize asset\nallocation.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.16849v1"
    },
    {
        "title": "The impact of the Russia-Ukraine conflict on the extreme risk spillovers\n  between agricultural futures and spots",
        "authors": [
            "Wei-Xing Zhou",
            "Yun-Shi Dai",
            "Kiet Tuan Duong",
            "Peng-Fei Dai"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The ongoing Russia-Ukraine conflict between two major agricultural powers has\nposed significant threats and challenges to the global food system and world\nfood security. Focusing on the impact of the conflict on the global\nagricultural market, we propose a new analytical framework for tail dependence,\nand combine the Copula-CoVaR method with the ARMA-GARCH-skewed Student-t model\nto examine the tail dependence structure and extreme risk spillover between\nagricultural futures and spots over the pre- and post-outbreak periods. Our\nresults indicate that the tail dependence structures in the futures-spot\nmarkets of soybean, maize, wheat, and rice have all reacted to the\nRussia-Ukraine conflict. Furthermore, the outbreak of the conflict has\nintensified risks of the four agricultural markets in varying degrees, with the\nwheat market being affected the most. Additionally, all the agricultural\nfutures markets exhibit significant downside and upside risk spillovers to\ntheir corresponding spot markets before and after the outbreak of the conflict,\nwhereas the strengths of these extreme risk spillover effects demonstrate\nsignificant asymmetries at the directional (downside versus upside) and\ntemporal (pre-outbreak versus post-outbreak) levels.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.16850v1"
    },
    {
        "title": "Towards a data-driven debt collection strategy based on an advanced\n  machine learning framework",
        "authors": [
            "Abel Sancarlos",
            "Edgar Bahilo",
            "Pablo Mozo",
            "Lukas Norman",
            "Obaid Ur Rehma",
            "Mihails Anufrijevs"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The European debt purchase market as measured by the total book value of\npurchased debt approached 25bn euros in 2020 and it was growing at double-digit\nrates. This is an example of how big the debt collection and debt purchase\nindustry has grown and the important impact it has in the financial sector.\nHowever, in order to ensure an adequate return during the debt collection\nprocess, a good estimation of the propensity to pay and/or the expected\ncashflow is crucial. These estimations can be employed, for instance, to create\ndifferent strategies during the amicable collection to maximize quality\nstandards and revenues. And not only that, but also to prioritize the cases in\nwhich a legal process is necessary when debtors are unreachable for an amicable\nnegotiation. This work offers a solution for these estimations. Specifically, a\nnew machine learning modelling pipeline is presented showing how outperforms\ncurrent strategies employed in the sector. The solution contains a\npre-processing pipeline and a model selector based on the best model\ncalibration. Performance is validated with real historical data of the debt\nindustry.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06292v1"
    },
    {
        "title": "Earnings Prediction Using Recurrent Neural Networks",
        "authors": [
            "Moritz Scherrmann",
            "Ralf Elsas"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Firm disclosures about future prospects are crucial for corporate valuation\nand compliance with global regulations, such as the EU's MAR and the US's SEC\nRule 10b-5 and RegFD. To comply with disclosure obligations, issuers must\nidentify nonpublic information with potential material impact on security\nprices as only new, relevant and unexpected information materially affects\nprices in efficient markets. Financial analysts, assumed to represent public\nknowledge on firms' earnings prospects, face limitations in offering\ncomprehensive coverage and unbiased estimates. This study develops a neural\nnetwork to forecast future firm earnings, using four decades of financial data,\naddressing analysts' coverage gaps and potentially revealing hidden insights.\nThe model avoids selectivity and survivorship biases as it allows for missing\ndata. Furthermore, the model is able to produce both fiscal-year-end and\nquarterly earnings predictions. Its performance surpasses benchmark models from\nthe academic literature by a wide margin and outperforms analysts' forecasts\nfor fiscal-year-end earnings predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.10756v1"
    },
    {
        "title": "Generative Machine Learning for Multivariate Equity Returns",
        "authors": [
            "Ruslan Tepelyan",
            "Achintya Gopal"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The use of machine learning to generate synthetic data has grown in\npopularity with the proliferation of text-to-image models and especially large\nlanguage models. The core methodology these models use is to learn the\ndistribution of the underlying data, similar to the classical methods common in\nfinance of fitting statistical models to data. In this work, we explore the\nefficacy of using modern machine learning methods, specifically conditional\nimportance weighted autoencoders (a variant of variational autoencoders) and\nconditional normalizing flows, for the task of modeling the returns of\nequities. The main problem we work to address is modeling the joint\ndistribution of all the members of the S&P 500, or, in other words, learning a\n500-dimensional joint distribution. We show that this generative model has a\nbroad range of applications in finance, including generating realistic\nsynthetic data, volatility and correlation estimation, risk analysis (e.g.,\nvalue at risk, or VaR, of portfolios), and portfolio optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14735v1"
    },
    {
        "title": "Improved Data Generation for Enhanced Asset Allocation: A Synthetic\n  Dataset Approach for the Fixed Income Universe",
        "authors": [
            "Szymon Kubiak",
            "Tillman Weyde",
            "Oleksandr Galkin",
            "Dan Philps",
            "Ram Gopal"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We present a novel process for generating synthetic datasets tailored to\nassess asset allocation methods and construct portfolios within the fixed\nincome universe. Our approach begins by enhancing the CorrGAN model to generate\nsynthetic correlation matrices. Subsequently, we propose an Encoder-Decoder\nmodel that samples additional data conditioned on a given correlation matrix.\nThe resulting synthetic dataset facilitates in-depth analyses of asset\nallocation methods across diverse asset universes. Additionally, we provide a\ncase study that exemplifies the use of the synthetic dataset to improve\nportfolios constructed within a simulation-based asset allocation process.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.16004v1"
    },
    {
        "title": "Linear and nonlinear causality in financial markets",
        "authors": [
            "Haochun Ma",
            "Davide Prosperino",
            "Alexander Haluszczynski",
            "Christoph Räth"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Identifying and quantifying co-dependence between financial instruments is a\nkey challenge for researchers and practitioners in the financial industry.\nLinear measures such as the Pearson correlation are still widely used today,\nalthough their limited explanatory power is well known. In this paper we\npresent a much more general framework for assessing co-dependencies by\nidentifying and interpreting linear and nonlinear causalities in the complex\nsystem of financial markets. To do so, we use two different causal inference\nmethods, transfer entropy and convergent cross-mapping, and employ Fourier\ntransform surrogates to separate their linear and nonlinear contributions. We\nfind that stock indices in Germany and the U.S. exhibit a significant degree of\nnonlinear causality and that correlation, while a very good proxy for linear\ncausality, disregards nonlinear effects and hence underestimates causality\nitself. The presented framework enables the measurement of nonlinear causality,\nthe correlation-causality fallacy, and motivates how causality can be used for\ninferring market signals, pair trading, and risk management of portfolios. Our\nresults suggest that linear and nonlinear causality can be used as early\nwarning indicators of abnormal market behavior, allowing for better trading\nstrategies and risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.16185v1"
    },
    {
        "title": "Structured factor copulas for modeling the systemic risk of European and\n  United States banks",
        "authors": [
            "Hoang Nguyen",
            "Audronė Virbickaitė",
            "M. Concepción Ausín",
            "Pedro Galeano"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In this paper, we employ Credit Default Swaps (CDS) to model the joint and\nconditional distress probabilities of banks in Europe and the U.S. using factor\ncopulas. We propose multi-factor, structured factor, and factor-vine models\nwhere the banks in the sample are clustered according to their geographic\nlocation. We find that within each region, the co-dependence between banks is\nbest described using both, systematic and idiosyncratic, financial contagion\nchannels. However, if we consider the banking system as a whole, then the\nsystematic contagion channel prevails, meaning that the distress probabilities\nare driven by a latent global factor and region-specific factors. In all cases,\nthe co-dependence structure of bank CDS spreads is highly correlated in the\ntail. The out-of-sample forecasts of several measures of systematic risk allow\nus to identify the periods of distress in the banking sector over the recent\nyears including the COVID-19 pandemic, the interest rate hikes in 2022, and the\nbanking crisis in 2023.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.03443v1"
    },
    {
        "title": "Optimal Linear Signal: An Unsupervised Machine Learning Framework to\n  Optimize PnL with Linear Signals",
        "authors": [
            "Pierre Renucci"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This study presents an unsupervised machine learning approach for optimizing\nProfit and Loss (PnL) in quantitative finance. Our algorithm, akin to an\nunsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL\ngenerated from signals constructed linearly from exogenous variables. The\nmethodology employs a linear relationship between exogenous variables and the\ntrading signal, with the objective of maximizing the Sharpe Ratio through\nparameter optimization. Empirical application on an ETF representing U.S.\nTreasury bonds demonstrates the model's effectiveness, supported by\nregularization techniques to mitigate overfitting. The study concludes with\npotential avenues for further development, including generalized time steps and\nenhanced corrective terms.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05337v1"
    },
    {
        "title": "Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market\n  Wraps?",
        "authors": [
            "Baptiste Lefort",
            "Eric Benhamou",
            "Jean-Jacques Ohana",
            "David Saltiel",
            "Beatrice Guez",
            "Damien Challet"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to\n2023, reposted on large financial media, to determine how global news headlines\nmay affect stock market movements using ChatGPT and a two-stage prompt\napproach. We document a statistically significant positive correlation between\nthe sentiment score and future equity market returns over short to medium term,\nwhich reverts to a negative correlation over longer horizons. Validation of\nthis correlation pattern across multiple equity markets indicates its\nrobustness across equity regions and resilience to non-linearity, evidenced by\ncomparison of Pearson and Spearman correlations. Finally, we provide an\nestimate of the optimal horizon that strikes a balance between reactivity to\nnew information and correlation.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05447v1"
    },
    {
        "title": "CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine\n  Learning Methods",
        "authors": [
            "Yue Chen",
            "Xingyi Andrew",
            "Salintip Supasanya"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Historically, the economic recession often came abruptly and disastrously.\nFor instance, during the 2008 financial crisis, the SP 500 fell 46 percent from\nOctober 2007 to March 2009. If we could detect the signals of the crisis\nearlier, we could have taken preventive measures. Therefore, driven by such\nmotivation, we use advanced machine learning techniques, including Random\nForest and Extreme Gradient Boosting, to predict any potential market crashes\nmainly in the US market. Also, we would like to compare the performance of\nthese methods and examine which model is better for forecasting US stock market\ncrashes. We apply our models on the daily financial market data, which tend to\nbe more responsive with higher reporting frequencies. We consider 75\nexplanatory variables, including general US stock market indexes, SP 500 sector\nindexes, as well as market indicators that can be used for the purpose of\ncrisis prediction. Finally, we conclude, with selected classification metrics,\nthat the Extreme Gradient Boosting method performs the best in predicting US\nstock market crisis events.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.06172v1"
    },
    {
        "title": "CNN-DRL for Scalable Actions in Finance",
        "authors": [
            "Sina Montazeri",
            "Akram Mirzaeinia",
            "Haseebullah Jumakhan",
            "Amir Mirzaeinia"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The published MLP-based DRL in finance has difficulties in learning the\ndynamics of the environment when the action scale increases. If the buying and\nselling increase to one thousand shares, the MLP agent will not be able to\neffectively adapt to the environment. To address this, we designed a CNN agent\nthat concatenates the data from the last ninety days of the daily feature\nvector to create the CNN input matrix. Our extensive experiments demonstrate\nthat the MLP-based agent experiences a loss corresponding to the initial\nenvironment setup, while our designed CNN remains stable, effectively learns\nthe environment, and leads to an increase in rewards.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.06179v1"
    },
    {
        "title": "Stylized Facts and Market Microstructure: An In-Depth Exploration of\n  German Bond Futures Market",
        "authors": [
            "Hamza Bodor",
            "Laurent Carlier"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper presents an in-depth analysis of stylized facts in the context of\nfutures on German bonds. The study examines four futures contracts on German\nbonds: Schatz, Bobl, Bund and Buxl, using tick-by-tick limit order book\ndatasets. It uncovers a range of stylized facts and empirical observations,\nincluding the distribution of order sizes, patterns of order flow, and\ninter-arrival times of orders. The findings reveal both commonalities and\nunique characteristics across the different futures, thereby enriching our\nunderstanding of these markets. Furthermore, the paper introduces insightful\nrealism metrics that can be used to benchmark market simulators. The study\ncontributes to the literature on financial stylized facts by extending\nempirical observations to this class of assets, which has been relatively\nunderexplored in existing research. This work provides valuable guidance for\nthe development of more accurate and realistic market simulators.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10722v1"
    },
    {
        "title": "Discrete Hawkes process with flexible residual distribution and filtered\n  historical simulation",
        "authors": [
            "Kyungsub Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We introduce a new model which can be considered as a extended version of the\nHawkes process in a discrete sense. This model enables the integration of\nvarious residual distributions while preserving the fundamental properties of\nthe original Hawkes process. The rich nature of this model enables a filtered\nhistorical simulation which incorporate the properties of original time series\nmore accurately. The process naturally extends to multi-variate models with\neasy implementations of estimation and simulation. We investigate the effect of\nflexible residual distribution on estimation of high frequency financial data\ncompared with the Hawkes process.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.13890v1"
    },
    {
        "title": "From GARCH to Neural Network for Volatility Forecast",
        "authors": [
            "Pengfei Zhao",
            "Haoren Zhu",
            "Wilfred Siu Hung NG",
            "Dik Lun Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Volatility, as a measure of uncertainty, plays a crucial role in numerous\nfinancial activities such as risk management. The Econometrics and Machine\nLearning communities have developed two distinct approaches for financial\nvolatility forecasting: the stochastic approach and the neural network (NN)\napproach. Despite their individual strengths, these methodologies have\nconventionally evolved in separate research trajectories with little\ninteraction between them. This study endeavors to bridge this gap by\nestablishing an equivalence relationship between models of the GARCH family and\ntheir corresponding NN counterparts. With the equivalence relationship\nestablished, we introduce an innovative approach, named GARCH-NN, for\nconstructing NN-based volatility models. It obtains the NN counterparts of\nGARCH models and integrates them as components into an established NN\narchitecture, thereby seamlessly infusing volatility stylized facts (SFs)\ninherent in the GARCH models into the neural network. We develop the GARCH-LSTM\nmodel to showcase the power of the GARCH-NN approach. Experiment results\nvalidate that amalgamating the NN counterparts of the GARCH family models into\nestablished NN models leads to enhanced outcomes compared to employing the\nstochastic and NN models in isolation.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06642v1"
    },
    {
        "title": "A Study on Stock Forecasting Using Deep Learning and Statistical Models",
        "authors": [
            "Himanshu Gupta",
            "Aditya Jaiswal"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Predicting a fast and accurate model for stock price forecasting is been a\nchallenging task and this is an active area of research where it is yet to be\nfound which is the best way to forecast the stock price. Machine learning, deep\nlearning and statistical analysis techniques are used here to get the accurate\nresult so the investors can see the future trend and maximize the return of\ninvestment in stock trading. This paper will review many deep learning\nalgorithms for stock price forecasting. We use a record of s&p 500 index data\nfor training and testing. The survey motive is to check various deep learning\nand statistical model techniques for stock price forecasting that are Moving\nAverages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL\nCNN which are deep learning models. It will discuss various models, including\nthe Auto regression integration moving average model, the Recurrent neural\nnetwork model, the long short-term model which is the type of RNN used for long\ndependency for data, the convolutional neural network model, and the full\nconvolutional neural network model, in terms of error calculation or percentage\nof accuracy that how much it is accurate which measures by the function like\nRoot mean square error, mean absolute error, mean squared error. The model can\nbe used to predict the stock price by checking the low MAE value as lower the\nMAE value the difference between the predicting and the actual value will be\nless and this model will predict the price more accurately than other models.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06689v1"
    },
    {
        "title": "RAGIC: Risk-Aware Generative Adversarial Model for Stock Interval\n  Construction",
        "authors": [
            "Jingyi Gu",
            "Wenlu Du",
            "Guiling Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Efforts to predict stock market outcomes have yielded limited success due to\nthe inherently stochastic nature of the market, influenced by numerous\nunpredictable factors. Many existing prediction approaches focus on\nsingle-point predictions, lacking the depth needed for effective\ndecision-making and often overlooking market risk. To bridge this gap, we\npropose a novel model, RAGIC, which introduces sequence generation for stock\ninterval prediction to quantify uncertainty more effectively. Our approach\nleverages a Generative Adversarial Network (GAN) to produce future price\nsequences infused with randomness inherent in financial markets. RAGIC's\ngenerator includes a risk module, capturing the risk perception of informed\ninvestors, and a temporal module, accounting for historical price trends and\nseasonality. This multi-faceted generator informs the creation of\nrisk-sensitive intervals through statistical inference, incorporating\nhorizon-wise insights. The interval's width is carefully adjusted to reflect\nmarket volatility. Importantly, our approach relies solely on publicly\navailable data and incurs only low computational overhead. RAGIC's evaluation\nacross globally recognized broad-based indices demonstrates its balanced\nperformance, offering both accuracy and informativeness. Achieving a consistent\n95% coverage, RAGIC maintains a narrow interval width. This promising outcome\nsuggests that our approach effectively addresses the challenges of stock market\nprediction while incorporating vital risk considerations.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.10760v1"
    },
    {
        "title": "Stylized Facts of High-Frequency Bitcoin Time Series",
        "authors": [
            "Yaoyue Tang",
            "Karina Arias-Calluari",
            "Michael S. Harré",
            "Fernando Alonso-Marroquin"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper analyses the high-frequency intraday Bitcoin dataset from 2019 to\n2022. During this time frame, the Bitcoin market index exhibited two distinct\nperiods characterized by abrupt changes in volatility. The Bitcoin price\nreturns for both periods can be described by an anomalous diffusion process,\ntransitioning from subdiffusion for short intervals to weak superdiffusion over\nlonger time intervals. The characteristic features related to this anomalous\nbehavior studied in the present paper include heavy tails, which can be\ndescribed using a $q$-Gaussian distribution and correlations. When we sample\nthe autocorrelation of absolute returns, we observe a power-law relationship,\nindicating time dependency in both periods initially. The ensemble\nautocorrelation of returns decays rapidly and exhibits periodicity. We fitted\nthe autocorrelation with a power law and a cosine function to capture both the\ndecay and the fluctuation and found that the two periods have distinctive\nperiodicity. Further study involves the analysis of endogenous effects within\nthe Bitcoin time series, which are examined through detrending analysis. We\nfound that both periods are multifractal and present self-similarity in the\ndetrended probability density function (PDF). The Hurst exponent over short\ntime intervals shifts from less than 0.5 ($\\sim$ 0.42) in Period 1 to be closer\nto 0.5 in Period 2 ($\\sim$ 0.49), indicating the market is more efficient at\nshort time scales.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.11930v1"
    },
    {
        "title": "Blockchain Metrics and Indicators in Cryptocurrency Trading",
        "authors": [
            "Juan C. King",
            "Roberto Dale",
            "José M. Amigó"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The objective of this paper is the construction of new indicators that can be\nuseful to operate in the cryptocurrency market. These indicators are based on\npublic data obtained from the blockchain network, specifically from the nodes\nthat make up Bitcoin mining. Therefore, our analysis is unique to that network.\nThe results obtained with numerical simulations of algorithmic trading and\nprediction via statistical models and Machine Learning demonstrate the\nimportance of variables such as the hash rate, the difficulty of mining or the\ncost per transaction when it comes to trade Bitcoin assets or predict the\ndirection of price. Variables obtained from the blockchain network will be\ncalled here blockchain metrics. The corresponding indicators (inspired by the\n\"Hash Ribbon\") perform well in locating buy signals. From our results, we\nconclude that such blockchain indicators allow obtaining information with a\nstatistical advantage in the highly volatile cryptocurrency market.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00770v1"
    },
    {
        "title": "Combating Financial Crimes with Unsupervised Learning Techniques:\n  Clustering and Dimensionality Reduction for Anti-Money Laundering",
        "authors": [
            "Ahmed N. Bakry",
            "Almohammady S. Alsharkawy",
            "Mohamed S. Farag",
            "Kamal R. Raslan"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Anti-Money Laundering (AML) is a crucial task in ensuring the integrity of\nfinancial systems. One keychallenge in AML is identifying high-risk groups\nbased on their behavior. Unsupervised learning, particularly clustering, is a\npromising solution for this task. However, the use of hundreds of features\ntodescribe behavior results in a highdimensional dataset that negatively\nimpacts clustering performance.In this paper, we investigate the effectiveness\nof combining clustering method agglomerative hierarchicalclustering with four\ndimensionality reduction techniques -Independent Component Analysis (ICA),\nandKernel Principal Component Analysis (KPCA), Singular Value Decomposition\n(SVD), Locality Preserving Projections (LPP)- to overcome the issue of\nhigh-dimensionality in AML data and improve clusteringresults. This study aims\nto provide insights into the most effective way of reducing the dimensionality\nofAML data and enhance the accuracy of clustering-based AML systems. The\nexperimental results demonstrate that KPCA outperforms other dimension\nreduction techniques when combined with agglomerativehierarchical clustering.\nThis superiority is observed in the majority of situations, as confirmed by\nthreedistinct validation indices.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00777v1"
    },
    {
        "title": "Applying News and Media Sentiment Analysis for Generating Forex Trading\n  Signals",
        "authors": [
            "Oluwafemi F Olaiyapo"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The objective of this research is to examine how sentiment analysis can be\nemployed to generate trading signals for the Foreign Exchange (Forex) market.\nThe author assessed sentiment in social media posts and news articles\npertaining to the United States Dollar (USD) using a combination of methods:\nlexicon-based analysis and the Naive Bayes machine learning algorithm. The\nfindings indicate that sentiment analysis proves valuable in forecasting market\nmovements and devising trading signals. Notably, its effectiveness is\nconsistent across different market conditions. The author concludes that by\nanalyzing sentiment expressed in news and social media, traders can glean\ninsights into prevailing market sentiments towards the USD and other pertinent\ncountries, thereby aiding trading decision-making. This study underscores the\nimportance of weaving sentiment analysis into trading strategies as a pivotal\ntool for predicting market dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00785v1"
    },
    {
        "title": "Chain-structured neural architecture search for financial time series\n  forecasting",
        "authors": [
            "Denis Levchenko",
            "Efstratios Rappos",
            "Shabnam Ataee",
            "Biagio Nigro",
            "Stephan Robert-Nicoud"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Neural architecture search (NAS) emerged as a way to automatically optimize\nneural networks for a specific task and dataset. Despite an abundance of\nresearch on NAS for images and natural language applications, similar studies\nfor time series data are lacking. Among NAS search spaces, chain-structured are\nthe simplest and most applicable to small datasets like time series. We compare\nthree popular NAS strategies on chain-structured search spaces: Bayesian\noptimization (specifically Tree-structured Parzen Estimator), the hyperband\nmethod, and reinforcement learning in the context of financial time series\nforecasting. These strategies were employed to optimize simple well-understood\nneural architectures like the MLP, 1D CNN, and RNN, with more complex temporal\nfusion transformers (TFT) and their own optimizers included for comparison. We\nfind Bayesian optimization and the hyperband method performing best among the\nstrategies, and RNN and 1D CNN best among the architectures, but all methods\nwere very close to each other with a high variance due to the difficulty of\nworking with financial datasets. We discuss our approach to overcome the\nvariance and provide implementation recommendations for future users and\nresearchers.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.14695v2"
    },
    {
        "title": "Distributional Reference Class Forecasting of Corporate Sales Growth\n  With Multiple Reference Variables",
        "authors": [
            "Etienne Theising"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper introduces an approach to reference class selection in\ndistributional forecasting with an application to corporate sales growth rates\nusing several co-variates as reference variables, that are implicit predictors.\nThe method can be used to detect expert or model-based forecasts exposed to\n(behavioral) bias or to forecast distributions with reference classes. These\nare sets of similar entities, here firms, and rank based algorithms for their\nselection are proposed, including an optional preprocessing data dimension\nreduction via principal components analysis. Forecasts are optimal if they\nmatch the underlying distribution as closely as possible. Probability integral\ntransform values rank the forecast capability of different reference variable\nsets and algorithms in a backtest on a data set of 21,808 US firms over the\ntime period 1950 - 2019. In particular, algorithms on dimension reduced\nvariables perform well using contemporaneous balance sheet and financial market\nparameters along with past sales growth rates and past operating margins\nchanges. Comparisions of actual analysts' estimates to distributional forecasts\nand of historic distributional forecasts to realized sales growth illustrate\nthe practical use of the method.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.03402v1"
    },
    {
        "title": "Comparative Study of Bitcoin Price Prediction",
        "authors": [
            "Ali Mohammadjafari"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Prediction of stock prices has been a crucial and challenging task,\nespecially in the case of highly volatile digital currencies such as Bitcoin.\nThis research examineS the potential of using neural network models, namely\nLSTMs and GRUs, to forecast Bitcoin's price movements. We employ five-fold\ncross-validation to enhance generalization and utilize L2 regularization to\nreduce overfitting and noise. Our study demonstrates that the GRUs models offer\nbetter accuracy than LSTMs model for predicting Bitcoin's price. Specifically,\nthe GRU model has an MSE of 4.67, while the LSTM model has an MSE of 6.25 when\ncompared to the actual prices in the test set data. This finding indicates that\nGRU models are better equipped to process sequential data with long-term\ndependencies, a characteristic of financial time series data such as Bitcoin\nprices. In summary, our results provide valuable insights into the potential of\nneural network models for accurate Bitcoin price prediction and emphasize the\nimportance of employing appropriate regularization techniques to enhance model\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.08089v1"
    },
    {
        "title": "The $κ$-generalised Distribution for Stock Returns",
        "authors": [
            "Samuel Forbes"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Empirical evidence shows stock returns are often heavy-tailed rather than\nnormally distributed. The $\\kappa$-generalised distribution, originated in the\ncontext of statistical physics by Kaniadakis, is characterised by the\n$\\kappa$-exponential function that is asymptotically exponential for small\nvalues and asymptotically power law for large values. This proves to be a\nuseful property and makes it a good candidate distribution for many types of\nquantities. In this paper we focus on fitting historic daily stock returns for\nthe FTSE 100 and the top 100 Nasdaq stocks. Using a Monte-Carlo goodness of fit\ntest there is evidence that the $\\kappa$-generalised distribution is a good fit\nfor a significant proportion of the 200 stock returns analysed.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.09929v1"
    },
    {
        "title": "Prediction of Cryptocurrency Prices through a Path Dependent Monte Carlo\n  Simulation",
        "authors": [
            "Ayush Singh",
            "Anshu K. Jha",
            "Amit N. Kumar"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In this paper, our focus lies on the Merton's jump diffusion model, employing\njump processes characterized by the compound Poisson process. Our primary\nobjective is to forecast the drift and volatility of the model using a variety\nof methodologies. We adopt an approach that involves implementing different\ndrift, volatility, and jump terms within the model through various machine\nlearning techniques, traditional methods, and statistical methods on\nprice-volume data. Additionally, we introduce a path-dependent Monte Carlo\nsimulation to model cryptocurrency prices, taking into account the volatility\nand unexpected jumps in prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12988v1"
    },
    {
        "title": "A K-means Algorithm for Financial Market Risk Forecasting",
        "authors": [
            "Jinxin Xu",
            "Kaixian Xu",
            "Yue Wang",
            "Qinyan Shen",
            "Ruisi Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Financial market risk forecasting involves applying mathematical models,\nhistorical data analysis and statistical methods to estimate the impact of\nfuture market movements on investments. This process is crucial for investors\nto develop strategies, financial institutions to manage assets and regulators\nto formulate policy. In today's society, there are problems of high error rate\nand low precision in financial market risk prediction, which greatly affect the\naccuracy of financial market risk prediction. K-means algorithm in machine\nlearning is an effective risk prediction technique for financial market. This\nstudy uses K-means algorithm to develop a financial market risk prediction\nsystem, which significantly improves the accuracy and efficiency of financial\nmarket risk prediction. Ultimately, the outcomes of the experiments confirm\nthat the K-means algorithm operates with user-friendly simplicity and achieves\na 94.61% accuracy rate\n",
        "pdf_link": "http://arxiv.org/pdf/2405.13076v1"
    },
    {
        "title": "Electricity Spot Prices Forecasting Using Stochastic Volatility Models",
        "authors": [
            "Andrei Renatovich Batyrov"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  There are several approaches to modeling and forecasting time series as\napplied to prices of commodities and financial assets. One of the approaches is\nto model the price as a non-stationary time series process with heteroscedastic\nvolatility (variance of price). The goal of the research is to generate\nprobabilistic forecasts of day-ahead electricity prices in a spot marker\nemploying stochastic volatility models. A typical stochastic volatility model -\nthat treats the volatility as a latent stochastic process in discrete time - is\nexplored first. Then the research focuses on enriching the baseline model by\nintroducing several exogenous regressors. A better fitting model - as compared\nto the baseline model - is derived as a result of the research. Out-of-sample\nforecasts confirm the applicability and robustness of the enriched model. This\nmodel may be used in financial derivative instruments for hedging the risk\nassociated with electricity trading. Keywords: Electricity spot prices\nforecasting, Stochastic volatility, Exogenous regressors, Autoregression,\nBayesian inference, Stan\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19405v1"
    },
    {
        "title": "NeuralBeta: Estimating Beta Using Deep Learning",
        "authors": [
            "Yuxin Liu",
            "Jimin Lin",
            "Achintya Gopal"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Traditional approaches to estimating beta in finance often involve rigid\nassumptions and fail to adequately capture beta dynamics, limiting their\neffectiveness in use cases like hedging. To address these limitations, we have\ndeveloped a novel method using neural networks called NeuralBeta, which is\ncapable of handling both univariate and multivariate scenarios and tracking the\ndynamic behavior of beta. To address the issue of interpretability, we\nintroduce a new output layer inspired by regularized weighted linear\nregression, which provides transparency into the model's decision-making\nprocess. We conducted extensive experiments on both synthetic and market data,\ndemonstrating NeuralBeta's superior performance compared to benchmark methods\nacross various scenarios, especially instances where beta is highly\ntime-varying, e.g., during regime shifts in the market. This model not only\nrepresents an advancement in the field of beta estimation, but also shows\npotential for applications in other financial contexts that assume linear\nrelationships.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.01387v2"
    },
    {
        "title": "NeuralFactors: A Novel Factor Learning Approach to Generative Modeling\n  of Equities",
        "authors": [
            "Achintya Gopal"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The use of machine learning for statistical modeling (and thus, generative\nmodeling) has grown in popularity with the proliferation of time series models,\ntext-to-image models, and especially large language models. Fundamentally, the\ngoal of classical factor modeling is statistical modeling of stock returns, and\nin this work, we explore using deep generative modeling to enhance classical\nfactor models. Prior work has explored the use of deep generative models in\norder to model hundreds of stocks, leading to accurate risk forecasting and\nalpha portfolio construction; however, that specific model does not allow for\neasy factor modeling interpretation in that the factor exposures cannot be\ndeduced. In this work, we introduce NeuralFactors, a novel machine-learning\nbased approach to factor analysis where a neural network outputs factor\nexposures and factor returns, trained using the same methodology as variational\nautoencoders. We show that this model outperforms prior approaches both in\nterms of log-likelihood performance and computational efficiency. Further, we\nshow that this method is competitive to prior work in generating realistic\nsynthetic data, covariance estimation, risk analysis (e.g., value at risk, or\nVaR, of portfolios), and portfolio optimization. Finally, due to the connection\nto classical factor analysis, we analyze how the factors our model learns\ncluster together and show that the factor exposures could be used for embedding\nstocks.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.01499v1"
    },
    {
        "title": "Comparative analysis of stationarity for Bitcoin and the S&P500",
        "authors": [
            "Yaoyue Tang",
            "Karina Arias-Calluari",
            "Michael S. Harré"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper compares and contrasts stationarity between the conventional stock\nmarket and cryptocurrency. The dataset used for the analysis is the intraday\nprice indices of the S&P500 from 1996 to 2023 and the intraday Bitcoin indices\nfrom 2019 to 2023, both in USD. We adopt the definition of `wide sense\nstationary', which constrains the time independence of the first and second\nmoments of a time series. The testing method used in this paper follows the\nWiener-Khinchin Theorem, i.e., that for a wide sense stationary process, the\npower spectral density and the autocorrelation are a Fourier transform pair. We\ndemonstrate that localized stationarity can be achieved by truncating the time\nseries into segments, and for each segment, detrending and normalizing the\nprice return are required. These results show that the S&P500 price return can\nachieve stationarity for the full 28-year period with a detrending window of 12\nmonths and a constrained normalization window of 10 minutes. With truncated\nsegments, a larger normalization window can be used to establish stationarity,\nindicating that within the segment the data is more homogeneous. For Bitcoin\nprice return, the segment with higher volatility presents stationarity with a\nnormalization window of 60 minutes, whereas stationarity cannot be established\nin other segments.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.02973v1"
    },
    {
        "title": "Strong denoising of financial time-series",
        "authors": [
            "Matthias J. Feiler"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In this paper we introduce a method for significantly improving the signal to\nnoise ratio in financial data. The approach relies on combining a target\nvariable with different context variables and use auto-encoders (AEs) to learn\nreconstructions of the combined inputs. The objective is to obtain agreement\namong pairs of AEs which are trained on related but different inputs and for\nwhich they are forced to find common ground. The training process is set up as\na \"conversation\" where the models take turns at producing a prediction\n(speaking) and reconciling own predictions with the output of the other AE\n(listening), until an agreement is reached. This leads to a new way of\nconstraining the complexity of the data representation generated by the AE.\nUnlike standard regularization whose strength needs to be decided by the\ndesigner, the proposed mutual regularization uses the partner network to detect\nand amend the lack of generality of the learned representation of the data. The\nintegration of alternative perspectives enhances the de-noising capacity of a\nsingle AE and allows us to discover new regularities in financial time-series\nwhich can be converted into profitable trading strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.05690v1"
    },
    {
        "title": "Method of Moments Estimation for Affine Stochastic Volatility Models",
        "authors": [
            "Yan-Feng Wu",
            "Xiangyu Yang",
            "Jian-Qiang Hu"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We develop moment estimators for the parameters of affine stochastic\nvolatility models. We first address the challenge of calculating moments for\nthe models by introducing a recursive equation for deriving closed-form\nexpressions for moments of any order. Consequently, we propose our moment\nestimators. We then establish a central limit theorem for our estimators and\nderive the explicit formulas for the asymptotic covariance matrix. Finally, we\nprovide numerical results to validate our method.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.09185v1"
    },
    {
        "title": "Model-based and empirical analyses of stochastic fluctuations in economy\n  and finance",
        "authors": [
            "Rubina Zadourian"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The objective of this work is the investigation of complexity, asymmetry,\nstochasticity and non-linearity of the financial and economic systems by using\nthe tools of statistical mechanics and information theory. More precisely, this\nthesis concerns statistical-based modeling and empirical analyses with\napplications in finance, forecasting, production processes and game theory. In\nthese areas the time dependence of probability distributions is of prime\ninterest and can be measured or exactly calculated for model systems. The\ncorrelation coefficients and moments are among the useful quantities to\ndescribe the dynamics and the correlations between random variables. However,\nthe full investigation can only be achieved if the probability distribution\nfunction of the variable is known; its derivation is one of the main focuses of\nthe present work.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.16010v1"
    },
    {
        "title": "Fundamental properties of linear factor models",
        "authors": [
            "Damir Filipovic",
            "Paul Schneider"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We study conditional linear factor models in the context of asset pricing\npanels. Our analysis focuses on conditional means and covariances to\ncharacterize the cross-sectional and inter-temporal properties of returns and\nfactors as well as their interrelationships. We also review the conditions\noutlined in Kozak and Nagel (2024) and show how the conditional mean-variance\nefficient portfolio of an unbalanced panel can be spanned by low-dimensional\nfactor portfolios, even without assuming invertibility of the conditional\ncovariance matrices. Our analysis provides a comprehensive foundation for the\nspecification and estimation of conditional linear factor models.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.02521v2"
    },
    {
        "title": "Combining supervised and unsupervised learning methods to predict\n  financial market movements",
        "authors": [
            "Gabriel Rodrigues Palma",
            "Mariusz Skoczeń",
            "Phil Maguire"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The decisions traders make to buy or sell an asset depend on various\nanalyses, with expertise required to identify patterns that can be exploited\nfor profit. In this paper we identify novel features extracted from emergent\nand well-established financial markets using linear models and Gaussian Mixture\nModels (GMM) with the aim of finding profitable opportunities. We used\napproximately six months of data consisting of minute candles from the Bitcoin,\nPepecoin, and Nasdaq markets to derive and compare the proposed novel features\nwith commonly used ones. These features were extracted based on the previous 59\nminutes for each market and used to identify predictions for the hour ahead. We\nexplored the performance of various machine learning strategies, such as Random\nForests (RF) and K-Nearest Neighbours (KNN) to classify market movements. A\nnaive random approach to selecting trading decisions was used as a benchmark,\nwith outcomes assumed to be equally likely. We used a temporal cross-validation\napproach using test sets of 40%, 30% and 20% of total hours to evaluate the\nlearning algorithms' performances. Our results showed that filtering the time\nseries facilitates algorithms' generalisation. The GMM filtering approach\nrevealed that the KNN and RF algorithms produced higher average returns than\nthe random algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.03762v1"
    },
    {
        "title": "Leveraging RNNs and LSTMs for Synchronization Analysis in the Indian\n  Stock Market: A Threshold-Based Classification Approach",
        "authors": [
            "Sanjay Sathish",
            "Charu C Sharma"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Our research presents a new approach for forecasting the synchronization of\nstock prices using machine learning and non-linear time-series analysis. To\ncapture the complex non-linear relationships between stock prices, we utilize\nrecurrence plots (RP) and cross-recurrence quantification analysis (CRQA). By\ntransforming Cross Recurrence Plot (CRP) data into a time-series format, we\nenable the use of Recurrent Neural Networks (RNN) and Long Short-Term Memory\n(LSTM) networks for predicting stock price synchronization through both\nregression and classification. We apply this methodology to a dataset of 20\nhighly capitalized stocks from the Indian market over a 21-year period. The\nfindings reveal that our approach can predict stock price synchronization, with\nan accuracy of 0.98 and F1 score of 0.83 offering valuable insights for\ndeveloping effective trading strategies and risk management tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.06728v1"
    },
    {
        "title": "Cross-Lingual News Event Correlation for Stock Market Trend Prediction",
        "authors": [
            "Sahar Arshad",
            "Nikhar Azhar",
            "Sana Sajid",
            "Seemab Latif",
            "Rabia Latif"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In the modern economic landscape, integrating financial services with\nFinancial Technology (FinTech) has become essential, particularly in stock\ntrend analysis. This study addresses the gap in comprehending financial\ndynamics across diverse global economies by creating a structured financial\ndataset and proposing a cross-lingual Natural Language-based Financial\nForecasting (NLFF) pipeline for comprehensive financial analysis. Utilizing\nsentiment analysis, Named Entity Recognition (NER), and semantic textual\nsimilarity, we conducted an analytical examination of news articles to extract,\nmap, and visualize financial event timelines, uncovering the correlation\nbetween news events and stock market trends. Our method demonstrated a\nmeaningful correlation between stock price movements and cross-linguistic news\nsentiments, validated by processing two-year cross-lingual news data on two\nprominent sectors of the Pakistan Stock Exchange. This study offers significant\ninsights into key events, ensuring a substantial decision margin for investors\nthrough effective visualization and providing optimal investment opportunities.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00024v1"
    },
    {
        "title": "SARF: Enhancing Stock Market Prediction with Sentiment-Augmented Random\n  Forest",
        "authors": [
            "Saber Talazadeh",
            "Dragan Perakovic"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Stock trend forecasting, a challenging problem in the financial domain,\ninvolves ex-tensive data and related indicators. Relying solely on empirical\nanalysis often yields unsustainable and ineffective results. Machine learning\nresearchers have demonstrated that the application of random forest algorithm\ncan enhance predictions in this context, playing a crucial auxiliary role in\nforecasting stock trends. This study introduces a new approach to stock market\nprediction by integrating sentiment analysis using FinGPT generative AI model\nwith the traditional Random Forest model. The proposed technique aims to\noptimize the accuracy of stock price forecasts by leveraging the nuanced\nunderstanding of financial sentiments provided by FinGPT. We present a new\nmethodology called \"Sentiment-Augmented Random Forest\" (SARF), which\nin-corporates sentiment features into the Random Forest framework. Our\nexperiments demonstrate that SARF outperforms conventional Random Forest and\nLSTM models with an average accuracy improvement of 9.23% and lower prediction\nerrors in pre-dicting stock market movements.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.07143v1"
    },
    {
        "title": "Kendall Correlation Coefficients for Portfolio Optimization",
        "authors": [
            "Tomas Espana",
            "Victor Le Coz",
            "Matteo Smerlak"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Markowitz's optimal portfolio relies on the accurate estimation of\ncorrelations between asset returns, a difficult problem when the number of\nobservations is not much larger than the number of assets. Using powerful\nresults from random matrix theory, several schemes have been developed to\n\"clean\" the eigenvalues of empirical correlation matrices. By contrast, the (in\npractice equally important) problem of correctly estimating the eigenvectors of\nthe correlation matrix has received comparatively little attention. Here we\ndiscuss a class of correlation estimators generalizing Kendall's rank\ncorrelation coefficient which improve the estimation of both eigenvalues and\neigenvectors in data-poor regimes. Using both synthetic and real financial\ndata, we show that these generalized correlation coefficients yield Markowitz\nportfolios with lower out-of-sample risk than those obtained with rotationally\ninvariant estimators. Central to these results is a property shared by all\nKendall-like estimators but not with classical correlation coefficients: zero\neigenvalues only appear when the number of assets becomes proportional to the\nsquare of the number of data points.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.17366v1"
    },
    {
        "title": "Fitting the seven-parameter Generalized Tempered Stable distribution to\n  the financial data",
        "authors": [
            "Aubain Nzokem",
            "Daniel Maposa"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The paper proposes and implements a methodology to fit a seven-parameter\nGeneralized Tempered Stable (GTS) distribution to financial data. The\nnonexistence of the mathematical expression of the GTS probability density\nfunction makes the maximum likelihood estimation (MLE) inadequate for providing\nparameter estimations. Based on the function characteristic and the fractional\nFourier transform (FRFT), we provide a comprehensive approach to circumvent the\nproblem and yield a good parameter estimation of the GTS probability. The\nmethodology was applied to fit two heavily tailed data (Bitcoin and Ethereum\nreturns) and two peaked data (S\\&P 500 and SPY ETF returns). For each index,\nthe estimation results show that the six-parameter estimations are\nstatistically significant except for the local parameter, $\\mu$. The\ngoodness-of-fit was assessed through Kolmogorov-Smirnov, Anderson-Darling, and\nPearson's chi-squared statistics. While the two-parameter geometric Brownian\nmotion (GBM) hypothesis is always rejected, the GTS distribution fits\nsignificantly with a very high p-value; and outperforms the Kobol,\nCarr-Geman-Madan-Yor, and Bilateral Gamma distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19751v2"
    },
    {
        "title": "Achilles, Neural Network to Predict the Gold Vs US Dollar Integration\n  with Trading Bot for Automatic Trading",
        "authors": [
            "Angel Varela"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Predicting the stock market is a big challenge for the machine learning\nworld. It is known how difficult it is to have accurate and consistent\npredictions with ML models. Some architectures are able to capture the movement\nof stocks but almost never are able to be launched to the production world. We\npresent Achilles, with a classical architecture of LSTM(Long Short Term Memory)\nneural network this model is able to predict the Gold vs USD commodity. With\nthe predictions minute-per-minute of this model we implemented a trading bot to\nrun during 23 days of testing excluding weekends. At the end of the testing\nperiod we generated $1623.52 in profit with the methodology used. The results\nof our method demonstrate Machine Learning can successfully be implemented to\npredict the Gold vs USD commodity.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.21291v2"
    },
    {
        "title": "Representation Learning for Regime detection in Block Hierarchical\n  Financial Markets",
        "authors": [
            "Alexa Orton",
            "Tim Gebbie"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We consider financial market regime detection from the perspective of deep\nrepresentation learning of the causal information geometry underpinning traded\nasset systems using a hierarchical correlation structure to characterise market\nevolution. We assess the robustness of three toy models: SPDNet, SPD-NetBN and\nU-SPDNet whose architectures respect the underlying Riemannian manifold of\ninput block hierarchical SPD correlation matrices. Market phase detection for\neach model is carried out using three data configurations: randomised JSE Top\n60 data, synthetically-generated block hierarchical SPD matrices and\nblock-resampled chronology-preserving JSE Top 60 data. We show that using a\nsingular performance metric is misleading in our financial market investment\nuse cases where deep learning models overfit in learning spatio-temporal\ncorrelation dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.22346v1"
    },
    {
        "title": "Log Heston Model for Monthly Average VIX",
        "authors": [
            "Jihyun Park",
            "Andrey Sarantsev"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We model time series of VIX (monthly average) and monthly stock index\nreturns. We use log-Heston model: logarithm of VIX is modeled as an\nautoregression of order 1. Our main insight is that normalizing monthly stock\nindex returns (dividing them by VIX) makes them much closer to independent\nidentically distributed Gaussian. The resulting model is mean-reverting, and\nthe innovations are non-Gaussian. The combined stochastic volatility model fits\nwell, and captures Pareto-like tails of real-world stock market returns. This\nworks for small and large stock indices, for both price and total returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.22471v1"
    },
    {
        "title": "The VIX as Stochastic Volatility for Corporate Bonds",
        "authors": [
            "Jihyun Park",
            "Andrey Sarantsev"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Classic stochastic volatility models assume volatility is unobservable. We\nuse the Volatility Index: S&P 500 VIX to observe it, to easier fit the model.\nWe apply it to corporate bonds. We fit autoregression for corporate rates and\nfor risk spreads between these rates and Treasury rates. Next, we divide\nresiduals by VIX. Our main idea is such division makes residuals closer to the\nideal case of a Gaussian white noise. This is remarkable, since these residuals\nand VIX come from separate market segments. Similarly, we model corporate bond\nreturns as a linear function of rates and rate changes. Our article has two\nmain parts: Moody's AAA and BAA spreads; Bank of America investment-grade and\nhigh-yield rates, spreads, and returns. We analyze long-term stability of these\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.22498v5"
    },
    {
        "title": "Generalized Distribution Prediction for Asset Returns",
        "authors": [
            "Ísak Pétursson",
            "María Óskarsdóttir"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We present a novel approach for predicting the distribution of asset returns\nusing a quantile-based method with Long Short-Term Memory (LSTM) networks. Our\nmodel is designed in two stages: the first focuses on predicting the quantiles\nof normalized asset returns using asset-specific features, while the second\nstage incorporates market data to adjust these predictions for broader economic\nconditions. This results in a generalized model that can be applied across\nvarious asset classes, including commodities, cryptocurrencies, as well as\nsynthetic datasets. The predicted quantiles are then converted into full\nprobability distributions through kernel density estimation, allowing for more\nprecise return distribution predictions and inferencing. The LSTM model\nsignificantly outperforms a linear quantile regression baseline by 98% and a\ndense neural network model by over 50%, showcasing its ability to capture\ncomplex patterns in financial return distributions across both synthetic and\nreal-world data. By using exclusively asset-class-neutral features, our model\nachieves robust, generalizable results.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.23296v1"
    },
    {
        "title": "News-Driven Stock Price Forecasting in Indian Markets: A Comparative\n  Study of Advanced Deep Learning Models",
        "authors": [
            "Kaushal Attaluri",
            "Mukesh Tripathi",
            "Srinithi Reddy",
            " Shivendra"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Forecasting stock market prices remains a complex challenge for traders,\nanalysts, and engineers due to the multitude of factors that influence price\nmovements. Recent advancements in artificial intelligence (AI) and natural\nlanguage processing (NLP) have significantly enhanced stock price prediction\ncapabilities. AI's ability to process vast and intricate data sets has led to\nmore sophisticated forecasts. However, achieving consistently high accuracy in\nstock price forecasting remains elusive. In this paper, we leverage 30 years of\nhistorical data from national banks in India, sourced from the National Stock\nExchange, to forecast stock prices. Our approach utilizes state-of-the-art deep\nlearning models, including multivariate multi-step Long Short-Term Memory\n(LSTM), Facebook Prophet with LightGBM optimized through Optuna, and Seasonal\nAuto-Regressive Integrated Moving Average (SARIMA). We further integrate\nsentiment analysis from tweets and reliable financial sources such as Business\nStandard and Reuters, acknowledging their crucial influence on stock price\nfluctuations.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05788v1"
    },
    {
        "title": "Comparative Analysis of LSTM, GRU, and Transformer Models for Stock\n  Price Prediction",
        "authors": [
            "Jue Xiao",
            "Tingting Deng",
            "Shuochen Bi"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In recent fast-paced financial markets, investors constantly seek ways to\ngain an edge and make informed decisions. Although achieving perfect accuracy\nin stock price predictions remains elusive, artificial intelligence (AI)\nadvancements have significantly enhanced our ability to analyze historical data\nand identify potential trends. This paper takes AI driven stock price trend\nprediction as the core research, makes a model training data set of famous\nTesla cars from 2015 to 2024, and compares LSTM, GRU, and Transformer Models.\nThe analysis is more consistent with the model of stock trend prediction, and\nthe experimental results show that the accuracy of the LSTM model is 94%. These\nmethods ultimately allow investors to make more informed decisions and gain a\nclearer insight into market behaviors.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05790v1"
    },
    {
        "title": "Graph Neural Networks for Financial Fraud Detection: A Review",
        "authors": [
            "Dawei Cheng",
            "Yao Zou",
            "Sheng Xiang",
            "Changjun Jiang"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The landscape of financial transactions has grown increasingly complex due to\nthe expansion of global economic integration and advancements in information\ntechnology. This complexity poses greater challenges in detecting and managing\nfinancial fraud. This review explores the role of Graph Neural Networks (GNNs)\nin addressing these challenges by proposing a unified framework that\ncategorizes existing GNN methodologies applied to financial fraud detection.\nSpecifically, by examining a series of detailed research questions, this review\ndelves into the suitability of GNNs for financial fraud detection, their\ndeployment in real-world scenarios, and the design considerations that enhance\ntheir effectiveness. This review reveals that GNNs are exceptionally adept at\ncapturing complex relational patterns and dynamics within financial networks,\nsignificantly outperforming traditional fraud detection methods. Unlike\nprevious surveys that often overlook the specific potentials of GNNs or address\nthem only superficially, our review provides a comprehensive, structured\nanalysis, distinctly focusing on the multifaceted applications and deployments\nof GNNs in financial fraud detection. This review not only highlights the\npotential of GNNs to improve fraud detection mechanisms but also identifies\ncurrent gaps and outlines future research directions to enhance their\ndeployment in financial systems. Through a structured review of over 100\nstudies, this review paper contributes to the understanding of GNN applications\nin financial fraud detection, offering insights into their adaptability and\npotential integration strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05815v2"
    },
    {
        "title": "BreakGPT: Leveraging Large Language Models for Predicting Asset Price\n  Surges",
        "authors": [
            "Aleksandr Simonyan"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper introduces BreakGPT, a novel large language model (LLM)\narchitecture adapted specifically for time series forecasting and the\nprediction of sharp upward movements in asset prices. By leveraging both the\ncapabilities of LLMs and Transformer-based models, this study evaluates\nBreakGPT and other Transformer-based models for their ability to address the\nunique challenges posed by highly volatile financial markets. The primary\ncontribution of this work lies in demonstrating the effectiveness of combining\ntime series representation learning with LLM prediction frameworks. We showcase\nBreakGPT as a promising solution for financial forecasting with minimal\ntraining and as a strong competitor for capturing both local and global\ntemporal dependencies.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.06076v1"
    },
    {
        "title": "Modelling financial returns with mixtures of generalized normal\n  distributions",
        "authors": [
            "Pierdomenico Duttilo"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This PhD Thesis presents an investigation into the analysis of financial\nreturns using mixture models, focusing on mixtures of generalized normal\ndistributions (MGND) and their extensions. The study addresses several critical\nissues encountered in the estimation process and proposes innovative solutions\nto enhance accuracy and efficiency. In Chapter 2, the focus lies on the MGND\nmodel and its estimation via expectation conditional maximization (ECM) and\ngeneralized expectation maximization (GEM) algorithms. A thorough exploration\nreveals a degeneracy issue when estimating the shape parameter. Several\nalgorithms are proposed to overcome this critical issue. Chapter 3 extends the\ntheoretical perspective by applying the MGND model on several stock market\nindices. A two-step approach is proposed for identifying turmoil days and\nestimating returns and volatility. Chapter 4 introduces constrained mixture of\ngeneralized normal distributions (CMGND), enhancing interpretability and\nefficiency by imposing constraints on parameters. Simulation results highlight\nthe benefits of constrained parameter estimation. Finally, Chapter 5 introduces\ngeneralized normal distribution-hidden Markov models (GND-HMMs) able to capture\nthe dynamic nature of financial returns. This manuscript contributes to the\nstatistical modelling of financial returns by offering flexible, parsimonious,\nand interpretable frameworks. The proposed mixture models capture complex\npatterns in financial data, thereby facilitating more informed decision-making\nin financial analysis and risk management.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.11847v1"
    },
    {
        "title": "Robust Graph Neural Networks for Stability Analysis in Dynamic Networks",
        "authors": [
            "Xin Zhang",
            "Zhen Xu",
            "Yue Liu",
            "Mengfang Sun",
            "Tong Zhou",
            "Wenying Sun"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In the current context of accelerated globalization and digitalization, the\ncomplexity and uncertainty of financial markets are increasing, and the\nidentification and prevention of economic risks have become a key link in\nmaintaining the stability of the financial system. Traditional risk\nidentification methods often have limitations because they are difficult to\ncope with the multi-level and dynamically changing complex relationships in\nfinancial networks. With the rapid development of financial technology, graph\nneural network (GNN) technology, as an emerging deep learning method, has\ngradually shown great potential in the field of financial risk management. GNN\ncan map transaction behaviors, financial institutions, individuals, and their\ninteractive relationships in financial networks into graph structures, and\neffectively capture potential patterns and abnormal signals in financial data\nthrough embedded representation learning. Using this technology, financial\ninstitutions can extract valuable information from complex transaction\nnetworks, identify hidden dangers or abnormal behaviors that may cause systemic\nrisks in a timely manner, optimize decision-making processes, and improve the\naccuracy of risk warnings. This paper explores the economic risk identification\nalgorithm based on the GNN algorithm, aiming to provide financial institutions\nand regulators with more intelligent technical tools to help maintain the\nsecurity and stability of the financial market. Improving the efficiency of\neconomic risk identification through innovative technical means is expected to\nfurther enhance the risk resistance of the financial system and lay the\nfoundation for building a robust global financial system.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.11848v1"
    },
    {
        "title": "Advance Detection Of Bull And Bear Phases In Cryptocurrency Markets",
        "authors": [
            "Rahul Arulkumaran",
            "Suyash Kumar",
            "Shikha Tomar",
            "Manideep Gongalla",
            " Harshitha"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Cryptocurrencies are highly volatile financial instruments with more and more\nnew retail investors joining the scene with each passing day. Bitcoin has\nalways proved to determine in which way the rest of the cryptocurrency market\nis headed towards. As of today Bitcoin has a market dominance of close to 50\npercent. Bull and bear phases in cryptocurrencies are determined based on the\nperformance of Bitcoin over the 50 Day and 200 Day Moving Averages. The aim of\nthis paper is to foretell the performance of bitcoin in the near future by\nemploying predictive algorithms. This predicted data will then be used to\ncalculate the 50 Day and 200 Day Moving Averages and subsequently plotted to\nestablish the potential bull and bear phases.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13586v1"
    },
    {
        "title": "Can ChatGPT Overcome Behavioral Biases in the Financial Sector?\n  Classify-and-Rethink: Multi-Step Zero-Shot Reasoning in the Gold Investment",
        "authors": [
            "Shuoling Liu",
            "Gaoguo Jia",
            "Yuhang Jiang",
            "Liyuan Chen",
            "Qiang Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Large Language Models (LLMs) have achieved remarkable success recently,\ndisplaying exceptional capabilities in creating understandable and organized\ntext. These LLMs have been utilized in diverse fields, such as clinical\nresearch, where domain-specific models like Med-Palm have achieved human-level\nperformance. Recently, researchers have employed advanced prompt engineering to\nenhance the general reasoning ability of LLMs. Despite the remarkable success\nof zero-shot Chain-of-Thoughts (CoT) in solving general reasoning tasks, the\npotential of these methods still remains paid limited attention in the\nfinancial reasoning task.To address this issue, we explore multiple prompt\nstrategies and incorporated semantic news information to improve LLMs'\nperformance on financial reasoning tasks.To the best of our knowledge, we are\nthe first to explore this important issue by applying ChatGPT to the gold\ninvestment.In this work, our aim is to investigate the financial reasoning\ncapabilities of LLMs and their capacity to generate logical and persuasive\ninvestment opinions. We will use ChatGPT, one of the most powerful LLMs\nrecently, and prompt engineering to achieve this goal. Our research will focus\non understanding the ability of LLMs in sophisticated analysis and reasoning\nwithin the context of investment decision-making. Our study finds that ChatGPT\nwith CoT prompt can provide more explainable predictions and overcome\nbehavioral biases, which is crucial in finance-related tasks and can achieve\nhigher investment returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13599v2"
    },
    {
        "title": "A Full-History Network Dataset for BTC Asset Decentralization Profiling",
        "authors": [
            "Ling Cheng",
            "Qian Shao",
            "Fengzhu Zeng",
            "Feida Zhu"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Since its advent in 2009, Bitcoin (BTC) has garnered increasing attention\nfrom both academia and industry. However, due to the massive transaction\nvolume, no systematic study has quantitatively measured the asset\ndecentralization degree specifically from a network perspective.\n  In this paper, by conducting a thorough analysis of the BTC transaction\nnetwork, we first address the significant gap in the availability of\nfull-history BTC graph and network property dataset, which spans over 15 years\nfrom the genesis block (1st March, 2009) to the 845651-th block (29, May 2024).\nWe then present the first systematic investigation to profile BTC's asset\ndecentralization and design several decentralization degrees for\nquantification. Through extensive experiments, we emphasize the significant\nrole of network properties and our network-based decentralization degree in\nenhancing Bitcoin analysis. Our findings demonstrate the importance of our\ncomprehensive dataset and analysis in advancing research on Bitcoin's\ntransaction dynamics and decentralization, providing valuable insights into the\nnetwork's structure and its implications.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13603v1"
    },
    {
        "title": "A New Way: Kronecker-Factored Approximate Curvature Deep Hedging and its\n  Benefits",
        "authors": [
            "Tsogt-Ochir Enkhbayar"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper advances the computational efficiency of Deep Hedging frameworks\nthrough the novel integration of Kronecker-Factored Approximate Curvature\n(K-FAC) optimization. While recent literature has established Deep Hedging as a\ndata-driven alternative to traditional risk management strategies, the\ncomputational burden of training neural networks with first-order methods\nremains a significant impediment to practical implementation. The proposed\narchitecture couples Long Short-Term Memory (LSTM) networks with K-FAC\nsecond-order optimization, specifically addressing the challenges of sequential\nfinancial data and curvature estimation in recurrent networks. Empirical\nvalidation using simulated paths from a calibrated Heston stochastic volatility\nmodel demonstrates that the K-FAC implementation achieves marked improvements\nin convergence dynamics and hedging efficacy. The methodology yields a 78.3%\nreduction in transaction costs ($t = 56.88$, $p < 0.001$) and a 34.4% decrease\nin profit and loss (P&L) variance compared to Adam optimization. Moreover, the\nK-FAC-enhanced model exhibits superior risk-adjusted performance with a Sharpe\nratio of 0.0401, contrasting with $-0.0025$ for the baseline model. These\nresults provide compelling evidence that second-order optimization methods can\nmaterially enhance the tractability of Deep Hedging implementations. The\nfindings contribute to the growing literature on computational methods in\nquantitative finance while highlighting the potential for advanced optimization\ntechniques to bridge the gap between theoretical frameworks and practical\napplications in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15002v1"
    },
    {
        "title": "What events matter for exchange rate volatility ?",
        "authors": [
            "Igor Martins",
            "Hedibert Freitas Lopes"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper expands on stochastic volatility models by proposing a data-driven\nmethod to select the macroeconomic events most likely to impact volatility. The\npaper identifies and quantifies the effects of macroeconomic events across\nmultiple countries on exchange rate volatility using high-frequency currency\nreturns, while accounting for persistent stochastic volatility effects and\nseasonal components capturing time-of-day patterns. Given the hundreds of\nmacroeconomic announcements and their lags, we rely on sparsity-based methods\nto select relevant events for the model. We contribute to the exchange rate\nliterature in four ways: First, we identify the macroeconomic events that drive\ncurrency volatility, estimate their effects and connect them to macroeconomic\nfundamentals. Second, we find a link between intraday seasonality, trading\nvolume, and the opening hours of major markets across the globe. We provide a\nsimple labor-based explanation for this observed pattern. Third, we show that\nincluding macroeconomic events and seasonal components is crucial for\nforecasting exchange rate volatility. Fourth, our proposed model yields the\nlowest volatility and highest Sharpe ratio in portfolio allocations when\ncompared to standard SV and GARCH models.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.16244v1"
    },
    {
        "title": "Auto-Generating Earnings Report Analysis via a Financial-Augmented LLM",
        "authors": [
            "Van-Duc Le"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Financial analysis heavily relies on the evaluation of earnings reports to\ngain insights into company performance. Traditional generation of these reports\nrequires extensive financial expertise and is time-consuming. With the\nimpressive progress in Large Language Models (LLMs), a wide variety of\nfinancially focused LLMs has emerged, addressing tasks like sentiment analysis\nand entity recognition in the financial domain. This paper presents a novel\nchallenge: developing an LLM specifically for automating the generation of\nearnings reports analysis. Our methodology involves an in-depth analysis of\nexisting earnings reports followed by a unique approach to fine-tune an LLM for\nthis purpose. This approach combines retrieval augmentation and the generation\nof instruction-based data, specifically tailored for the financial sector, to\nenhance the LLM's performance. With extensive financial documents, we construct\nfinancial instruction data, enabling the refined adaptation of our LLM to\nfinancial contexts. Preliminary results indicate that our augmented LLM\noutperforms general open-source models and rivals commercial counterparts like\nGPT-3.5 in financial applications. Our research paves the way for streamlined\nand insightful automation in financial report generation, marking a significant\nstride in the field of financial analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08179v1"
    },
    {
        "title": "Online High-Frequency Trading Stock Forecasting with Automated Feature\n  Clustering and Radial Basis Function Neural Networks",
        "authors": [
            "Adamantios Ntakaris",
            "Gbenga Ibikunle"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This study presents an autonomous experimental machine learning protocol for\nhigh-frequency trading (HFT) stock price forecasting that involves a dual\ncompetitive feature importance mechanism and clustering via shallow neural\nnetwork topology for fast training. By incorporating the k-means algorithm into\nthe radial basis function neural network (RBFNN), the proposed method addresses\nthe challenges of manual clustering and the reliance on potentially\nuninformative features. More specifically, our approach involves a dual\ncompetitive mechanism for feature importance, combining the mean-decrease\nimpurity (MDI) method and a gradient descent (GD) based feature importance\nmechanism. This approach, tested on HFT Level 1 order book data for 20 S&P 500\nstocks, enhances the forecasting ability of the RBFNN regressor. Our findings\nsuggest that an autonomous approach to feature selection and clustering is\ncrucial, as each stock requires a different input feature space. Overall, by\nautomating the feature selection and clustering processes, we remove the need\nfor manual topological grid search and provide a more efficient way to predict\nLOB's mid-price.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.16160v2"
    },
    {
        "title": "Minimal Batch Adaptive Learning Policy Engine for Real-Time Mid-Price\n  Forecasting in High-Frequency Trading",
        "authors": [
            "Adamantios Ntakaris",
            "Gbenga Ibikunle"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  High-frequency trading (HFT) has transformed modern financial markets, making\nreliable short-term price forecasting models essential. In this study, we\npresent a novel approach to mid-price forecasting using Level 1 limit order\nbook (LOB) data from NASDAQ, focusing on 100 U.S. stocks from the S&P 500 index\nduring the period from September to November 2022. Expanding on our previous\nwork with Radial Basis Function Neural Networks (RBFNN), which leveraged\nautomated feature importance techniques based on mean decrease impurity (MDI)\nand gradient descent (GD), we introduce the Adaptive Learning Policy Engine\n(ALPE) - a reinforcement learning (RL)-based agent designed for batch-free,\nimmediate mid-price forecasting. ALPE incorporates adaptive epsilon decay to\ndynamically balance exploration and exploitation, outperforming a diverse range\nof highly effective machine learning (ML) and deep learning (DL) models in\nforecasting performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.19372v2"
    },
    {
        "title": "Assets Forecasting with Feature Engineering and Transformation Methods\n  for LightGBM",
        "authors": [
            "Konstantinos-Leonidas Bisdoulis"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Fluctuations in the stock market rapidly shape the economic world and\nconsumer markets, impacting millions of individuals. Hence, accurately\nforecasting it is essential for mitigating risks, including those associated\nwith inactivity. Although research shows that hybrid models of Deep Learning\n(DL) and Machine Learning (ML) yield promising results, their computational\nrequirements often exceed the capabilities of average personal computers,\nrendering them inaccessible to many. In order to address this challenge in this\npaper we optimize LightGBM (an efficient implementation of gradient-boosted\ndecision trees (GBDT)) for maximum performance, while maintaining low\ncomputational requirements. We introduce novel feature engineering techniques\nincluding indicator-price slope ratios and differences of close and open prices\ndivided by the corresponding 14-period Exponential Moving Average (EMA),\ndesigned to capture market dynamics and enhance predictive accuracy.\nAdditionally, we test seven different feature and target variable\ntransformation methods, including returns, logarithmic returns, EMA ratios and\ntheir standardized counterparts as well as EMA difference ratios, so as to\nidentify the most effective ones weighing in both efficiency and accuracy. The\nresults demonstrate Log Returns, Returns and EMA Difference Ratio constitute\nthe best target variable transformation methods, with EMA ratios having a lower\npercentage of correct directional forecasts, and standardized versions of\ntarget variable transformations requiring significantly more training time.\nMoreover, the introduced features demonstrate high feature importance in\npredictive performance across all target variable transformation methods. This\nstudy highlights an accessible, computationally efficient approach to stock\nmarket forecasting using LightGBM, making advanced forecasting techniques more\nwidely attainable.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.07580v1"
    },
    {
        "title": "Stock market return distributions: from past to present",
        "authors": [
            "S. Drozdz",
            "M. Forczek",
            "J. Kwapien",
            "P. Oswiecimka",
            "R. Rak"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We show that recent stock market fluctuations are characterized by the\ncumulative distributions whose tails on short, minute time scales exhibit power\nscaling with the scaling index alpha > 3 and this index tends to increase\nquickly with decreasing sampling frequency. Our study is based on\nhigh-frequency recordings of the S&P500, DAX and WIG20 indices over the\ninterval May 2004 - May 2006. Our findings suggest that dynamics of the\ncontemporary market may differ from the one observed in the past. This effect\nindicates a constantly increasing efficiency of world markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0664v1"
    },
    {
        "title": "Collective behavior of stock price movements in an emerging market",
        "authors": [
            "Raj Kumar Pan",
            "Sitabhra Sinha"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  To investigate the universality of the structure of interactions in different\nmarkets, we analyze the cross-correlation matrix C of stock price fluctuations\nin the National Stock Exchange (NSE) of India. We find that this emerging\nmarket exhibits strong correlations in the movement of stock prices compared to\ndeveloped markets, such as the New York Stock Exchange (NYSE). This is shown to\nbe due to the dominant influence of a common market mode on the stock prices.\nBy comparison, interactions between related stocks, e.g., those belonging to\nthe same business sector, are much weaker. This lack of distinct sector\nidentity in emerging markets is explicitly shown by reconstructing the network\nof mutually interacting stocks. Spectral analysis of C for NSE reveals that,\nthe few largest eigenvalues deviate from the bulk of the spectrum predicted by\nrandom matrix theory, but they are far fewer in number compared to, e.g., NYSE.\nWe show this to be due to the relative weakness of intra-sector interactions\nbetween stocks, compared to the market mode, by modeling stock price dynamics\nwith a two-factor model. Our results suggest that the emergence of an internal\nstructure comprising multiple groups of strongly coupled components is a\nsignature of market development.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0773v2"
    },
    {
        "title": "The Epps effect revisited",
        "authors": [
            "Bence Toth",
            "Janos Kertesz"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We analyse the dependence of stock return cross-correlations on the sampling\nfrequency of the data known as the Epps effect: For high resolution data the\ncross-correlations are significantly smaller than their asymptotic value as\nobserved on daily data. The former description implies that changing trading\nfrequency should alter the characteristic time of the phenomenon. This is not\ntrue for the empirical data: The Epps curves do not scale with market activity.\nThe latter result indicates that the time scale of the phenomenon is connected\nto the reaction time of market participants (this we denote as human time\nscale), independent of market activity. In this paper we give a new description\nof the Epps effect through the decomposition of cross-correlations. After\ntesting our method on a model of generated random walk price changes we justify\nour analytical results by fitting the Epps curves of real world data.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.1099v2"
    },
    {
        "title": "Uncovering the Internal Structure of the Indian Financial Market:\n  Cross-correlation behavior in the NSE",
        "authors": [
            "Sitabhra Sinha",
            "Raj Kumar Pan"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The cross-correlations between price fluctuations of 201 frequently traded\nstocks in the National Stock Exchange (NSE) of India are analyzed in this\npaper. We use daily closing prices for the period 1996-2006, which coincides\nwith the period of rapid transformation of the market following liberalization.\nThe eigenvalue distribution of the cross-correlation matrix, $\\mathbf{C}$, of\nNSE is found to be similar to that of developed markets, such as the New York\nStock Exchange (NYSE): the majority of eigenvalues fall within the bounds\nexpected for a random matrix constructed from mutually uncorrelated time\nseries. Of the few largest eigenvalues that deviate from the bulk, the largest\nis identified with market-wide movements. The intermediate eigenvalues that\noccur between the largest and the bulk have been associated in NYSE with\nspecific business sectors with strong intra-group interactions. However, in the\nIndian market, these deviating eigenvalues are comparatively very few and lie\nmuch closer to the bulk. We propose that this is because of the relative lack\nof distinct sector identity in the market, with the movement of stocks\ndominantly influenced by the overall market trend. This is shown by explicit\nconstruction of the interaction network in the market, first by generating the\nminimum spanning tree from the unfiltered correlation matrix, and later, using\nan improved method of generating the graph after filtering out the market mode\nand random effects from the data. Both methods show, compared to developed\nmarkets, the relative absence of clusters of co-moving stocks that belong to\nthe same business sector. This is consistent with the general belief that\nemerging markets tend to be more correlated than developed markets.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.2115v1"
    },
    {
        "title": "Classical and quantum randomness and the financial market",
        "authors": [
            "Andrei Khrennikov"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We analyze complexity of financial (and general economic) processes by\ncomparing classical and quantum-like models for randomness. Our analysis\nimplies that it might be that a quantum-like probabilistic description is more\nnatural for financial market than the classical one. A part of our analysis is\ndevoted to study the possibility of application of the quantum probabilistic\nmodel to agents of financial market. We show that, although the direct quantum\n(physical) reduction (based on using the scales of quantum mechanics) is\nmeaningless, one may apply so called quantum-like models. In our approach\nquantum-like probabilistic behaviour is a consequence of contextualy of\nstatistical data in finances (and economics in general). However, our\nhypothesis on \"quantumness\" of financial data should be tested experimentally\n(as opposed to the conventional description based on the noncontextual\nclassical probabilistic approach). We present a new statistical test based on a\ngeneralization of the well known in quantum physics Bell's inequality.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.2865v1"
    },
    {
        "title": "Modeling the Epps effect of cross correlations in asset prices",
        "authors": [
            "Bence Toth",
            "Balint Toth",
            "Janos Kertesz"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We review the decomposition method of stock return cross-correlations,\npresented previously for studying the dependence of the correlation coefficient\non the resolution of data (Epps effect). Through a toy model of random\nwalk/Brownian motion and memoryless renewal process (i.e. Poisson point\nprocess) of observation times we show that in case of analytical treatability,\nby decomposing the correlations we get the exact result for the frequency\ndependence. We also demonstrate that our approach produces reasonable fitting\nof the dependence of correlations on the data resolution in case of empirical\ndata. Our results indicate that the Epps phenomenon is a product of the finite\ntime decay of lagged correlations of high resolution data, which does not scale\nwith activity. The characteristic time is due to a human time scale, the time\nneeded to react to news.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.3798v1"
    },
    {
        "title": "Microscopic Origin of Non-Gaussian Distributions of Financial Returns",
        "authors": [
            "T. S. Biro",
            "R. Rosenfeld"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  In this paper we study the possible microscopic origin of heavy-tailed\nprobability density distributions for the price variation of financial\ninstruments. We extend the standard log-normal process to include another\nrandom component in the so-called stochastic volatility models. We study these\nmodels under an assumption, akin to the Born-Oppenheimer approximation, in\nwhich the volatility has already relaxed to its equilibrium distribution and\nacts as a background to the evolution of the price process. In this\napproximation, we show that all models of stochastic volatility should exhibit\na scaling relation in the time lag of zero-drift modified log-returns. We\nverify that the Dow-Jones Industrial Average index indeed follows this scaling.\nWe then focus on two popular stochastic volatility models, the Heston and\nHull-White models. In particular, we show that in the Hull-White model the\nresulting probability distribution of log-returns in this approximation\ncorresponds to the Tsallis (t-Student) distribution. The Tsallis parameters are\ngiven in terms of the microscopic stochastic volatility model. Finally, we show\nthat the log-returns for 30 years Dow Jones index data is well fitted by a\nTsallis distribution, obtaining the relevant parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.4112v2"
    },
    {
        "title": "Are all highly liquid securities within the same class?",
        "authors": [
            "Silvio M. Duarte Queiros"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  In this manuscript we analyse the leading statistical properties of\nfluctuations of (log) 3-month US Treasury bill quotation in the secondary\nmarket, namely: probability density function, autocorrelation, absolute values\nautocorrelation, and absolute values persistency. We verify that this financial\ninstrument, in spite of its high liquidity, shows very peculiar properties.\nParticularly, we verify that log-fluctuations belong to the Levy class of\nstochastic variables.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.1247v2"
    },
    {
        "title": "Uncertainty in the Fluctuations of the Price of Stocks",
        "authors": [
            "G. R. Jafari",
            "M. Sadegh Movahed",
            "P. Noroozzadeh",
            "A. Bahraminasab",
            "Muhammad Sahimi",
            "F. Ghasemi",
            "M. Reza Rahimi Tabar"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We report on a study of the Tehran Price Index (TEPIX) from 2001 to 2006 as\nan emerging market that has been affected by several political crises during\nthe recent years, and analyze the non-Gaussian probability density function\n(PDF) of the log returns of the stocks' prices. We show that while the average\nof the index did not fall very much over the time period of the study, its\nday-to-day fluctuations strongly increased due to the crises. Using an approach\nbased on multiplicative processes with a detrending procedure, we study the\nscale-dependence of the non-Gaussian PDFs, and show that the temporal\ndependence of their tails indicates a gradual and systematic increase in the\nprobability of the appearance of large increments in the returns on approaching\ndistinct critical time scales over which the TEPIX has exhibited maximum\nuncertainty.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.1460v1"
    },
    {
        "title": "The fractional volatility model: An agent-based interpretation",
        "authors": [
            "R. Vilela Mendes"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Based on criteria of mathematical simplicity and consistency with empirical\nmarket data, a model with volatility driven by fractional noise has been\nconstructed which provides a fairly accurate mathematical parametrization of\nthe data. Here, some features of the model are discussed and, using agent-based\nmodels, one tries to find which agent strategies and (or) properties of the\nfinancial institutions might be responsible for the features of the fractional\nvolatility model.\n",
        "pdf_link": "http://arxiv.org/pdf/0706.3827v2"
    },
    {
        "title": "The Local Fractal Properties of the Financial Time Series on the Polish\n  Stock Exchange Market",
        "authors": [
            "D. Grech",
            "G. Pamuła"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We investigate the local fractal properties of the financial time series\nbased on the evolution of the Warsaw Stock Exchange Index (WIG) connected with\nthe largest developing financial market in Europe. Calculating the local Hurst\nexponent for the WIG time series we find an interesting dependence between the\nbehavior of the local fractal properties of the WIG time series and the crashes\nappearance on the financial market.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.0353v1"
    },
    {
        "title": "Fast estimation of multivariate stochastic volatility",
        "authors": [
            "Kostas Triantafyllopoulos",
            "Giovanni Montana"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  In this paper we develop a Bayesian procedure for estimating multivariate\nstochastic volatility (MSV) using state space models. A multiplicative model\nbased on inverted Wishart and multivariate singular beta distributions is\nproposed for the evolution of the volatility, and a flexible sequential\nvolatility updating is employed. Being computationally fast, the resulting\nestimation procedure is particularly suitable for on-line forecasting. Three\nperformance measures are discussed in the context of model selection: the\nlog-likelihood criterion, the mean of standardized one-step forecast errors,\nand sequential Bayes factors. Finally, the proposed methods are applied to a\ndata set comprising eight exchange rates vis-a-vis the US dollar.\n",
        "pdf_link": "http://arxiv.org/pdf/0708.4376v2"
    },
    {
        "title": "Are volatility estimators robust with respect to modeling assumptions?",
        "authors": [
            "Yingying Li",
            "Per A. Mykland"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We consider microstructure as an arbitrary contamination of the underlying\nlatent securities price, through a Markov kernel $Q$. Special cases include\nadditive error, rounding and combinations thereof. Our main result is that,\nsubject to smoothness conditions, the two scales realized volatility is robust\nto the form of contamination $Q$. To push the limits of our result, we show\nwhat happens for some models that involve rounding (which is not, of course,\nsmooth) and see in this situation how the robustness deteriorates with\ndecreasing smoothness. Our conclusion is that under reasonable smoothness, one\ndoes not need to consider too closely how the microstructure is formed, while\nif severe non-smoothness is suspected, one needs to pay attention to the\nprecise structure and also the use to which the estimator of volatility will be\nput.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.0440v1"
    },
    {
        "title": "Application of spectral methods for high-frequency financial data to\n  quantifying states of market participants",
        "authors": [
            "Aki-Hiro Sato"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Empirical analysis of the foreign exchange market is conducted based on\nmethods to quantify similarities among multi-dimensional time series with\nspectral distances introduced in [A.-H. Sato, Physica A, 382 (2007) 258--270].\nAs a result it is found that the similarities among currency pairs fluctuate\nwith the rotation of the earth, and that the similarities among best quotation\nrates are associated with those among quotation frequencies. Furthermore it is\nshown that the Jensen-Shannon spectral divergence is proportional to a mean of\nthe Kullback-Leibler spectral distance both empirically and numerically. It is\nconfirmed that these spectral distances are connected with distributions for\nbehavioral parameters of the market participants from numerical simulation.\nThis concludes that spectral distances of representative quantities of\nfinancial markets are related into diversification of behavioral parameters of\nthe market participants.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.1530v1"
    },
    {
        "title": "Volatility return intervals analysis of the Japanese market",
        "authors": [
            "Woo-Sung Jung",
            "Fengzhong Wang",
            "Shlomo Havlin",
            "Taisei Kaizoji",
            "Hie-Tae Moon",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We investigate scaling and memory effects in return intervals between price\nvolatilities above a certain threshold $q$ for the Japanese stock market using\ndaily and intraday data sets. We find that the distribution of return intervals\ncan be approximated by a scaling function that depends only on the ratio\nbetween the return interval $\\tau$ and its mean $<\\tau>$. We also find memory\neffects such that a large (or small) return interval follows a large (or small)\ninterval by investigating the conditional distribution and mean return\ninterval. The results are similar to previous studies of other markets and\nindicate that similar statistical features appear in different financial\nmarkets. We also compare our results between the period before and after the\nbig crash at the end of 1989. We find that scaling and memory effects of the\nreturn intervals show similar features although the statistical properties of\nthe returns are different.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.1725v1"
    },
    {
        "title": "Effectiveness of Measures of Performance During Speculative Bubbles",
        "authors": [
            "Filippo Petroni",
            "Giulia Rotundo"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Statistical analysis of financial data most focused on testing the validity\nof Brownian motion (Bm). Analysis performed on several time series have shown\ndeviation from the Bm hypothesis, that is at the base of the evaluation of many\nfinancial derivatives. We inquiry in the behavior of measures of performance\nbased on maximum drawdown movements (MDD), testing their stability when the\nunderlying process deviates from the Bm hypothesis. In particular we consider\nthe fractional Brownian motion (fBm), and fluctuations estimated empirically on\nraw market data. The case study of the rising part of speculative bubbles is\nreported.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.2423v1"
    },
    {
        "title": "Flexible least squares for temporal data mining and statistical\n  arbitrage",
        "authors": [
            "Giovanni Montana",
            "Kostas Triantafyllopoulos",
            "Theodoros Tsagaris"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  A number of recent emerging applications call for studying data streams,\npotentially infinite flows of information updated in real-time. When multiple\nco-evolving data streams are observed, an important task is to determine how\nthese streams depend on each other, accounting for dynamic dependence patterns\nwithout imposing any restrictive probabilistic law governing this dependence.\nIn this paper we argue that flexible least squares (FLS), a penalized version\nof ordinary least squares that accommodates for time-varying regression\ncoefficients, can be deployed successfully in this context. Our motivating\napplication is statistical arbitrage, an investment strategy that exploits\npatterns detected in financial data streams. We demonstrate that FLS is\nalgebraically equivalent to the well-known Kalman filter equations, and take\nadvantage of this equivalence to gain a better understanding of FLS and suggest\na more efficient algorithm. Promising experimental results obtained from a\nFLS-based algorithmic trading system for the S&P 500 Futures Index are\nreported.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.3884v1"
    },
    {
        "title": "The Student ensemble of correlation matrices: eigenvalue spectrum and\n  Kullback-Leibler entropy",
        "authors": [
            "Giulio Biroli",
            "Jean-Philippe Bouchaud",
            "Marc Potters"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We study a new ensemble of random correlation matrices related to\nmultivariate Student (or more generally elliptic) random variables. We\nestablish the exact density of states of empirical correlation matrices that\ngeneralizes the Marcenko-Pastur result. The comparison between the theoretical\ndensity of states in the Student case and empirical financial data is\nsurprisingly good, even if we are still able to detect systematic deviations.\nFinally, we compute explicitely the Kullback-Leibler entropies of empirical\nStudent matrices, which are found to be independent of the true correlation\nmatrix, as in the Gaussian case. We provide numerically exact values for these\nKullback-Leibler entropies.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.0802v1"
    },
    {
        "title": "Martingales, the Efficient Market Hypothesis, and Spurious Stylized\n  Facts",
        "authors": [
            "Joseph L. McCauley",
            "Kevin E. Bassler",
            "Gemunu H. Gunaratne"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The condition for stationary increments, not scaling, detemines long time\npair autocorrelations. An incorrect assumption of stationary increments\ngenerates spurious stylized facts, fat tails and a Hurst exponent H_s=1/2, when\nthe increments are nonstationary, as they are in FX markets. The\nnonstationarity arises from systematic uneveness in noise traders' behavior.\nSpurious results arise mathematically from using a log increment with a\n'sliding window'. We explain why a hard to beat market demands martingale\ndynamics , and martingales with nonlinear variance generate nonstationary\nincrements. The nonstationarity is exhibited directly for Euro/Dollar FX data.\nWe observe that the Hurst exponent H_s generated by the using the sliding\nwindow technique on a time series plays the same role as does Mandelbrot's\nJoseph exponent. Finally, Mandelbrot originally assumed that the 'badly\nbehaved' second moment of cotton returns is due to fat tails, but that\nnonconvergent behavior is instead direct evidence for nonstationary increments.\nSummarizing, the evidence for scaling and fat tails as the basis for\neconophysics and financial economics is provided neither by FX markets nor by\ncotton price data.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.2583v1"
    },
    {
        "title": "Moment Methods for Exotic Volatility Derivatives",
        "authors": [
            "Claudio Albanese",
            "Adel Osseiran"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The latest generation of volatility derivatives goes beyond variance and\nvolatility swaps and probes our ability to price realized variance and sojourn\ntimes along bridges for the underlying stock price process. In this paper, we\ngive an operator algebraic treatment of this problem based on Dyson expansions\nand moment methods and discuss applications to exotic volatility derivatives.\nThe methods are quite flexible and allow for a specification of the underlying\nprocess which is semi-parametric or even non-parametric, including\nstate-dependent local volatility, jumps, stochastic volatility and regime\nswitching. We find that volatility derivatives are particularly well suited to\nbe treated with moment methods, whereby one extrapolates the distribution of\nthe relevant path functionals on the basis of a few moments. We consider a\nnumber of exotics such as variance knockouts, conditional corridor variance\nswaps, gamma swaps and variance swaptions and give valuation formulas in\ndetail.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.2991v1"
    },
    {
        "title": "A stochastic theory for temporal fluctuations in self-organized critical\n  systems",
        "authors": [
            "Martin Rypdal",
            "Kristoffer Rypdal"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  A stochastic theory for the toppling activity in sandpile models is\ndeveloped, based on a simple mean-field assumption about the toppling process.\nThe theory describes the process as an anti-persistent Gaussian walk, where the\ndiffusion coefficient is proportional to the activity. It is formulated as a\ngeneralization of the It\\^{o} stochastic differential equation with an\nanti-persistent fractional Gaussian noise source. An essential element of the\ntheory is re-scaling to obtain a proper thermodynamic limit, and it captures\nall temporal features of the toppling process obtained by numerical simulation\nof the Bak-Tang-Wiesenfeld sandpile in this limit.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4010v2"
    },
    {
        "title": "Forbidden patterns in financial time series",
        "authors": [
            "Massimiliano Zanin"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The existence of forbidden patterns, i.e., certain missing sequences in a\ngiven time series, is a recently proposed instrument of potential application\nin the study of time series. Forbidden patterns are related to the permutation\nentropy, which has the basic properties of classic chaos indicators, thus\nallowing to separate deterministic (usually chaotic) from random series;\nhowever, it requires less values of the series to be calculated, and it is\nsuitable for using with small datasets. In this Letter, the appearance of\nforbidden patterns is studied in different economical indicators like stock\nindices (Dow Jones Industrial Average and Nasdaq Composite), NYSE stocks (IBM\nand Boeing) and others (10-year Bond interest rate), to find evidences of\ndeterministic behavior in their evolutions.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.0729v2"
    },
    {
        "title": "Smearing Distributions and their use in Financial Markets",
        "authors": [
            "Petr Jizba",
            "Hagen Kleinert"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  It is shown that superpositions of path integrals with arbitrary Hamiltonians\nand different scaling parameters v (\"variances\") obey the Chapman-Kolmogorov\nrelation for Markovian processes if and only if the corresponding smearing\ndistributions for v have a specific functional form. Ensuing \"smearing\"\ndistributions substantially simplify the coupled system of Fokker-Planck\nequations for smeared and un-smeared conditional probabilities. Simple\napplication in financial models with stochastic volatility is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/0712.0083v1"
    },
    {
        "title": "Multivariate stochastic volatility with Bayesian dynamic linear models",
        "authors": [
            "K. Triantafyllopoulos"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  This paper develops a Bayesian procedure for estimation and forecasting of\nthe volatility of multivariate time series. The foundation of this work is the\nmatrix-variate dynamic linear model, for the volatility of which we adopt a\nmultiplicative stochastic evolution, using Wishart and singular multivariate\nbeta distributions. A diagonal matrix of discount factors is employed in order\nto discount the variances element by element and therefore allowing a flexible\nand pragmatic variance modelling approach. Diagnostic tests and sequential\nmodel monitoring are discussed in some detail. The proposed estimation theory\nis applied to a four-dimensional time series, comprising spot prices of\naluminium, copper, lead and zinc of the London metal exchange. The empirical\nfindings suggest that the proposed Bayesian procedure can be effectively\napplied to financial data, overcoming many of the disadvantages of existing\nvolatility models.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.0214v1"
    },
    {
        "title": "Forecasting with time-varying vector autoregressive models",
        "authors": [
            "K. Triantafyllopoulos"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The purpose of this paper is to propose a time-varying vector autoregressive\nmodel (TV-VAR) for forecasting multivariate time series. The model is casted\ninto a state-space form that allows flexible description and analysis. The\nvolatility covariance matrix of the time series is modelled via inverted\nWishart and singular multivariate beta distributions allowing a fully conjugate\nBayesian inference. Model performance and model comparison is done via the\nlikelihood function, sequential Bayes factors, the mean of squared standardized\nforecast errors, the mean of absolute forecast errors (known also as mean\nabsolute deviation), and the mean forecast error. Bayes factors are also used\nin order to choose the autoregressive order of the model. Multi-step\nforecasting is discussed in detail and a flexible formula is proposed to\napproximate the forecast function. Two examples, consisting of bivariate data\nof IBM shares and of foreign exchange (FX) rates for 8 currencies, illustrate\nthe methods. For the IBM data we discuss model performance and multi-step\nforecasting in some detail. For the FX data we discuss sequential portfolio\nallocation; for both data sets our empirical findings suggest that the TV-VAR\nmodels outperform the widely used VAR models.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.0220v2"
    },
    {
        "title": "Multivariate stochastic volatility using state space models",
        "authors": [
            "K. Triantafyllopoulos"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  A Bayesian procedure is developed for multivariate stochastic volatility,\nusing state space models. An autoregressive model for the log-returns is\nemployed. We generalize the inverted Wishart distribution to allow for\ndifferent correlation structure between the observation and state innovation\nvectors and we extend the convolution between the Wishart and the multivariate\nsingular beta distribution. A multiplicative model based on the generalized\ninverted Wishart and multivariate singular beta distributions is proposed for\nthe evolution of the volatility and a flexible sequential volatility updating\nis employed. The proposed algorithm for the volatility is fast and\ncomputationally cheap and it can be used for on-line forecasting. The methods\nare illustrated with an example consisting of foreign exchange rates data of 8\ncurrencies. The empirical results suggest that time-varying correlations can be\nestimated efficiently, even in situations of high dimensional data.\n",
        "pdf_link": "http://arxiv.org/pdf/0802.0223v1"
    },
    {
        "title": "Cross-correlations in Warsaw Stock Exchange",
        "authors": [
            "R. Rak",
            "J. Kwapien",
            "S. Drozdz",
            "P. Oswiecimka"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We study the inter-stock correlations for the largest companies listed on\nWarsaw Stock Exchange and included in the WIG20 index. Our results from the\ncorrelation matrix analysis indicate that the Polish stock market can be well\ndescribed by a one factor model. We also show that the stock-stock correlations\ntend to increase with the time scale of returns and they approach a saturation\nlevel for the time scales of at least 200 min, i.e. an order of magnitude\nlonger than in the case of some developed markets. We also show that the\nstrength of correlations among the stocks crucially depends on their\ncapitalization. These results combined with our earlier findings together\nsuggest that now the Polish stock market situates itself somewhere between an\nemerging market phase and a mature market phase.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.0057v1"
    },
    {
        "title": "Double Power Law Decay of the Persistence in Financial Markets",
        "authors": [
            "S. Jain",
            "T. Yamano"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The persistence phenomenon is studied in the Japanese financial market by\nusing a novel mapping of the time evolution of the values of shares quoted on\nthe Nikkei Index onto Ising spins. The method is applied to historical end of\nday data from the Japanese stock market during 2002. By studying the time\ndependence of the spins, we find clear evidence for a double-power law decay of\nthe proportion of shares that remain either above or below ` starting\\rq\\\nvalues chosen at random. The results are consistent with a recent analysis of\nthe data from the London FTSE100 market. The slopes of the power-laws are also\nin agreement. We estimate a long time persistence exponent for the underlying\nJapanese financial market to be 0.5.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.0436v1"
    },
    {
        "title": "Stock price jumps: news and volume play a minor role",
        "authors": [
            "Armand Joulin",
            "Augustin Lefevre",
            "Daniel Grunberg",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  In order to understand the origin of stock price jumps, we cross-correlate\nhigh-frequency time series of stock returns with different news feeds. We find\nthat neither idiosyncratic news nor market wide news can explain the frequency\nand amplitude of price jumps. We find that the volatility patterns around jumps\nand around news are quite different: jumps are followed by increased\nvolatility, whereas news tend on average to be followed by lower volatility\nlevels. The shape of the volatility relaxation is also markedly different in\nthe two cases. Finally, we provide direct evidence that large transaction\nvolumes are_not_ responsible for large price jumps. We conjecture that most\nprice jumps are induced by order flow fluctuations close to the point of\nvanishing liquidity.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.1769v1"
    },
    {
        "title": "Correlations in commodity markets",
        "authors": [
            "Paweł Sieczka",
            "Janusz A. Hołyst"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  In this paper we analyzed dependencies in commodity markets investigating\ncorrelations of future contracts for commodities over the period 1998.09.01 -\n2007.12.14. We constructed a minimal spanning tree based on the correlation\nmatrix. The tree provides evidence for sector clusterization of investigated\ncontracts. We also studied dynamical properties of commodity dependencies. It\nturned out that the market was constantly getting more correlated within the\ninvestigated period, although the increase of correlation was distributed\nnonuniformly among all contracts, and depended on contracts branches.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.3884v2"
    },
    {
        "title": "ARCH and GARCH Models vs. Martingale Volatility of Finance Market\n  Returns",
        "authors": [
            "Joseph L. McCauley"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  ARCH and GARCH models assume either i.i.d. or (what economists lable as)\nwhite noise as is usual in regression analysis while assuming memory in a\nconditional mean square fluctuation with stationary increments. We will show\nthat ARCH/GARCH is inconsistent with uncorrelated increments, violating the\ni.i.d. and white assumptions and finance data and the efficient market\nhypothesis as well.\n",
        "pdf_link": "http://arxiv.org/pdf/0803.4480v1"
    },
    {
        "title": "Log-Normal continuous cascades: aggregation properties and estimation.\n  Application to financial time-series",
        "authors": [
            "E. Bacry",
            "A. Kozhemyak",
            "J. -F. Muzy"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Log-normal continuous random cascades form a class of multifractal processes\nthat has already been successfully used in various fields. Several statistical\nissues related to this model are studied. We first make a quick but extensive\nreview of their main properties and show that most of these properties can be\nanalytically studied. We then develop an approximation theory of these\nprocesses in the limit of small intermittency $\\lambda^2\\ll 1$, i.e., when the\ndegree of multifractality is small. This allows us to prove that the\nprobability distributions associated with these processes possess some very\nsimple aggregation properties accross time scales. Such a control of the\nprocess properties at different time scales, allows us to address the problem\nof parameter estimation. We show that one has to distinguish two different\nasymptotic regimes: the first one, referred to as the ''low frequency regime'',\ncorresponds to taking a sample whose overall size increases whereas the second\none, referred to as the ''high frequency regime'', corresponds to sampling the\nprocess at an increasing sampling rate. We show that, the first regime leads to\nconvergent estimators whereas, in the high frequency regime, the situation is\nmuch more intricate : only the intermittency coefficient $\\lambda^2$ can be\nestimated using a consistent estimator. However, we show that, in practical\nsituations, one can detect the nature of the asymptotic regime (low frequency\nversus high frequency) and consequently decide whether the estimations of the\nother parameters are reliable or not. We finally illustrate how both our\nresults on parameter estimation and on aggregation properties, allow one to\nsuccessfully use these models for modelization and prediction of financial time\nseries.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0185v1"
    },
    {
        "title": "Role of scaling in the statistical modeling of finance",
        "authors": [
            "Attilio L. Stella",
            "Fulvio Baldovin"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  Modeling the evolution of a financial index as a stochastic process is a\nproblem awaiting a full, satisfactory solution since it was first formulated by\nBachelier in 1900. Here it is shown that the scaling with time of the return\nprobability density function sampled from the historical series suggests a\nsuccessful model. The resulting stochastic process is a heteroskedastic,\nnon-Markovian martingale, which can be used to simulate index evolution on the\nbasis of an auto-regressive strategy. Results are fully consistent with\nvolatility clustering and with the multi-scaling properties of the return\ndistribution. The idea of basing the process construction on scaling, and the\nconstruction itself, are closely inspired by the probabilistic renormalization\ngroup approach of statistical mechanics and by a recent formulation of the\ncentral limit theorem for sums of strongly correlated random variables.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0331v1"
    },
    {
        "title": "Time vs. Ensemble Averages for Nonstationary Time Series",
        "authors": [
            "Joseph L. McCauley"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We analyze the question whether sliding window time averages applied to\nstationary increment processes converge to a limit in probability. The question\ncenters on averages, correlations, and densities constructed via time averages\nof the increment x(t,T)=x(t+T)-x(t)and the assumption is that the increment is\ndistributed independently of t. We show that the condition for applying\nTchebyshev's Theorem to time averages of functions of stationary increments is\nstrongly violated. We argue that, for both stationary and nonstationary\nincrements, Tchebyshev's Theorem provides the basis for constructing emsemble\naverages and densities from a single, historic time series if, as in FX\nmarkets, the series shows a definite statistical periodicity on the average.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.0902v1"
    },
    {
        "title": "Cross-correlation of long-range correlated series",
        "authors": [
            "Sergio Arianos",
            "Anna Carbone"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  A method for estimating the cross-correlation $C_{xy}(\\tau)$ of long-range\ncorrelated series $x(t)$ and $y(t)$, at varying lags $\\tau$ and scales $n$, is\nproposed. For fractional Brownian motions with Hurst exponents $H_1$ and $H_2$,\nthe asymptotic expression of $C_{xy}(\\tau)$ depends only on the lag $\\tau$\n(wide-sense stationarity) and scales as a power of $n$ with exponent\n${H_1+H_2}$ for $\\tau\\to 0$. The method is illustrated on (i) financial series,\nto show the leverage effect; (ii) genomic sequences, to estimate the\ncorrelations between structural parameters along the chromosomes.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.2064v3"
    },
    {
        "title": "Comparison of detrending methods for fluctuation analysis",
        "authors": [
            "Amir Bashan",
            "Ronny Bartsch",
            "Jan W. Kantelhardt",
            "Shlomo Havlin"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We examine several recently suggested methods for the detection of long-range\ncorrelations in data series based on similar ideas as the well-established\nDetrended Fluctuation Analysis (DFA). In particular, we present a detailed\ncomparison between the regular DFA and two recently suggested methods: the\nCentered Moving Average (CMA) Method and a Modified Detrended Fluctuation\nAnalysis (MDFA). We find that CMA is performing equivalently as DFA in long\ndata with weak trends and slightly superior to DFA in short data with weak\ntrends. When comparing standard DFA to MDFA we observe that DFA performs\nslightly better in almost all examples we studied. We also discuss how several\ntypes of trends affect the different types of DFA. For weak trends in the data,\nthe new methods are comparable with DFA in these respects. However, if the\nfunctional form of the trend in data is not a-priori known, DFA remains the\nmethod of choice. Only a comparison of DFA results, using different detrending\npolynomials, yields full recognition of the trends. A comparison with\nindependent methods is recommended for proving long-range correlations.\n",
        "pdf_link": "http://arxiv.org/pdf/0804.4081v1"
    },
    {
        "title": "GARCH modelling in continuous time for irregularly spaced time series\n  data",
        "authors": [
            "Ross A. Maller",
            "Gernot Müller",
            "Alex Szimayer"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The discrete-time GARCH methodology which has had such a profound influence\non the modelling of heteroscedasticity in time series is intuitively well\nmotivated in capturing many `stylized facts' concerning financial series, and\nis now almost routinely used in a wide range of situations, often including\nsome where the data are not observed at equally spaced intervals of time.\nHowever, such data is more appropriately analyzed with a continuous-time model\nwhich preserves the essential features of the successful GARCH paradigm. One\npossible such extension is the diffusion limit of Nelson, but this is\nproblematic in that the discrete-time GARCH model and its continuous-time\ndiffusion limit are not statistically equivalent. As an alternative,\nKl\\\"{u}ppelberg et al. recently introduced a continuous-time version of the\nGARCH (the `COGARCH' process) which is constructed directly from a background\ndriving L\\'{e}vy process. The present paper shows how to fit this model to\nirregularly spaced time series data using discrete-time GARCH methodology, by\napproximating the COGARCH with an embedded sequence of discrete-time GARCH\nseries which converges to the continuous-time model in a strong sense (in\nprobability, in the Skorokhod metric), as the discrete approximating grid grows\nfiner. This property is also especially useful in certain other applications,\nsuch as options pricing. The way is then open to using, for the COGARCH,\nsimilar statistical techniques to those already worked out for GARCH models and\nto illustrate this, an empirical investigation using stock index data is\ncarried out.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.2096v1"
    },
    {
        "title": "Using self-similarity and renormalization group to analyze time series",
        "authors": [
            "Giovanni Arcioni"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  An algorithm based on Renormalization Group (RG) to analyze time series\nforecasting was proposed in cond-mat/0110285. In this paper we explicitly code\nand test it. We choose in particular some financial time series (stocks,\nindexes and commodities) with daily data and compute one step ahead forecasts.\nWe then construct some indicators to evaluate performances. The algorithm is\nsupposed to prescribe the future development of the time series by using the\nself-similarity property intrinsically present in RG approach. This property\ncould be potentially very attractive for the purpose of building winning\ntrading systems. We discuss some relevant points along this direction. Although\ncurrent performances have to be improved the algorithm seems quite reactive to\nvarious combinations of input parameters and different past values sequences.\nThis makes it a potentially good candidate to detect sharp market movements. We\nfinally mention current drawbacks and sketch how to improve them.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.3213v1"
    },
    {
        "title": "The escape problem under stochastic volatility: the Heston model",
        "authors": [
            "Jaume Masoliver",
            "Josep Perello"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We solve the escape problem for the Heston random diffusion model. We obtain\nexact expressions for the survival probability (which ammounts to solving the\ncomplete escape problem) as well as for the mean exit time. We also average the\nvolatility in order to work out the problem for the return alone regardless\nvolatility. We look over these results in terms of the dimensionless normal\nlevel of volatility --a ratio of the three parameters that appear in the Heston\nmodel-- and analyze their form in several assymptotic limits. Thus, for\ninstance, we show that the mean exit time grows quadratically with large spans\nwhile for small spans the growth is systematically slower depending on the\nvalue of the normal level. We compare our results with those of the Wiener\nprocess and show that the assumption of stochastic volatility, in an apparent\nparadoxical way, increases survival and prolongs the escape time.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1014v1"
    },
    {
        "title": "Statistical properties of volatility return intervals of Chinese stocks",
        "authors": [
            "Fei Ren",
            "Liang Guo",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The statistical properties of the return intervals $\\tau_q$ between\nsuccessive 1-min volatilities of 30 liquid Chinese stocks exceeding a certain\nthreshold $q$ are carefully studied. The Kolmogorov-Smirnov (KS) test shows\nthat 12 stocks exhibit scaling behaviors in the distributions of $\\tau_q$ for\ndifferent thresholds $q$. Furthermore, the KS test and weighted KS test shows\nthat the scaled return interval distributions of 6 stocks (out of the 12\nstocks) can be nicely fitted by a stretched exponential function\n$f(\\tau/\\bar{\\tau})\\sim e^{- \\alpha (\\tau/\\bar{\\tau})^{\\gamma}}$ with\n$\\gamma\\approx0.31$ under the significance level of 5%, where $\\bar{\\tau}$ is\nthe mean return interval. The investigation of the conditional probability\ndistribution $P_q(\\tau | \\tau_0)$ and the mean conditional return interval\n$<\\tau| \\tau_0>$ demonstrates the existence of short-term correlation between\nsuccessive return interval intervals. We further study the mean return interval\n$<\\tau| \\tau_0>$ after a cluster of $n$ intervals and the fluctuation $F(l)$\nusing detrended fluctuation analysis and find that long-term memory also exists\nin the volatility return intervals.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1818v1"
    },
    {
        "title": "Market dynamics after large financial crash",
        "authors": [
            "G. L. Buchbinder",
            "K. M. Chistilin"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The model describing market dynamics after a large financial crash is\nconsidered in terms of the stochastic differential equation of Ito. Physically,\nthe model presents an overdamped Brownian particle moving in the nonstationary\none-dimensional potential $U$ under the influence of the variable noise\nintensity, depending on the particle position $x$. Based on the empirical data\nthe approximate estimation of the Kramers-Moyal coefficients $D_{1,2}$ allow to\npredicate quite definitely the behavior of the potential introduced by $D_1 = -\n\\partial U /\\partial x$ and the volatility $\\sim \\sqrt{D_2}$. It has been shown\nthat the presented model describes well enough the best known empirical facts\nrelative to the large financial crash of October 1987. \\\n",
        "pdf_link": "http://arxiv.org/pdf/0807.2083v1"
    },
    {
        "title": "Joint analysis and estimation of stock prices and trading volume in\n  Barndorff-Nielsen and Shephard stochastic volatility models",
        "authors": [
            "Friedrich Hubalek",
            "Petra Posedel"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We introduce a variant of the Barndorff-Nielsen and Shephard stochastic\nvolatility model where the non Gaussian Ornstein-Uhlenbeck process describes\nsome measure of trading intensity like trading volume or number of trades\ninstead of unobservable instantaneous variance. We develop an explicit\nestimator based on martingale estimating functions in a bivariate model that is\nnot a diffusion, but admits jumps. It is assumed that both the quantities are\nobserved on a discrete grid of fixed width, and the observation horizon tends\nto infinity. We show that the estimator is consistent and asymptotically normal\nand give explicit expressions of the asymptotic covariance matrix. Our method\nis illustrated by a finite sample experiment and a statistical analysis on the\nInternational Business Machines Corporation (IBM) stock from the New York Stock\nExchange (NYSE) and the Microsoft Corporation (MSFT) stock from Nasdaq during a\nhistory of five years.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.3464v2"
    },
    {
        "title": "Asymptotic analysis for a simple explicit estimator in Barndorff-Nielsen\n  and Shephard stochastic volatility models",
        "authors": [
            "Friedrich Hubalek",
            "Petra Posedel"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We provide a simple explicit estimator for discretely observed\nBarndorff-Nielsen and Shephard models, prove rigorously consistency and\nasymptotic normality based on the single assumption that all moments of the\nstationary distribution of the variance process are finite, and give explicit\nexpressions for the asymptotic covariance matrix.\n  We develop in detail the martingale estimating function approach for a\nbivariate model, that is not a diffusion, but admits jumps. We do not use\nergodicity arguments.\n  We assume that both, logarithmic returns and instantaneous variance are\nobserved on a discrete grid of fixed width, and the observation horizon tends\nto infinity. As the instantaneous variance is not observable in practice, our\nresults cannot be applied immediately. Our purpose is to provide a theoretical\nanalysis as a starting point and benchmark for further developments concerning\noptimal martingale estimating functions, and for theoretical and empirical\ninvestigations, that replace the variance process with a substitute, such as\nnumber or volume of trades or implied variance from option data.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.3479v1"
    },
    {
        "title": "Financial Time Series Analysis of SV Model by Hybrid Monte Carlo",
        "authors": [
            "Tetsuya Takaishi"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We apply the hybrid Monte Carlo (HMC) algorithm to the financial time sires\nanalysis of the stochastic volatility (SV) model for the first time. The HMC\nalgorithm is used for the Markov chain Monte Carlo (MCMC) update of volatility\nvariables of the SV model in the Bayesian inference. We compute parameters of\nthe SV model from the artificial financial data and compare the results from\nthe HMC algorithm with those from the Metropolis algorithm. We find that the\nHMC decorrelates the volatility variables faster than the Metropolis algorithm.\nWe also make an empirical analysis based on the Yen/Dollar exchange rates.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.4394v1"
    },
    {
        "title": "Fractality feature in oil price fluctuations",
        "authors": [
            "M. Momeni",
            "I. Kourakis",
            "K. Talebi"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The scaling properties of oil price fluctuations are described as a\nnon-stationary stochastic process realized by a time series of finite length.\nAn original model is used to extract the scaling exponent of the fluctuation\nfunctions within a non-stationary process formulation. It is shown that, when\nreturns are measured over intervals less than 10 days, the Probability Density\nFunctions (PDFs) exhibit self-similarity and monoscaling, in contrast to the\nmultifractal behavior of the PDFs at macro-scales (typically larger than one\nmonth). We find that the time evolution of the distributions are well fitted by\na Levy distribution law at micro-scales. The relevance of a Levy distribution\nis made plausible by a simple model of nonlinear transfer\n",
        "pdf_link": "http://arxiv.org/pdf/0809.1139v1"
    },
    {
        "title": "Clustering of discretely observed diffusion processes",
        "authors": [
            "Alessandro De Gregorio",
            "Stefano Maria Iacus"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  In this paper a new dissimilarity measure to identify groups of assets\ndynamics is proposed. The underlying generating process is assumed to be a\ndiffusion process solution of stochastic differential equations and observed at\ndiscrete time. The mesh of observations is not required to shrink to zero. As\ndistance between two observed paths, the quadratic distance of the\ncorresponding estimated Markov operators is considered. Analysis of both\nsynthetic data and real financial data from NYSE/NASDAQ stocks, give evidence\nthat this distance seems capable to catch differences in both the drift and\ndiffusion coefficients contrary to other commonly used metrics.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.3902v1"
    },
    {
        "title": "Universality in the stock exchange",
        "authors": [
            "Rui Gonçalves",
            "Alberto Pinto"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We analyze the constituents stocks of the Dow Jones Industrial Average\n(DJIA30) and the Standard & Poor's 100 index (S&P100) of the NYSE stock\nexchange market. Surprisingly, we discover the data collapse of the histograms\nof the DJIA30 price fluctuations and of the S&P100 price fluctuations to the\nuniversal non-parametric Bramwell-Holdsworth-Pinton (BHP) distribution. Since\nthe BHP probability density function appears in several other dissimilar\nphenomena, our result reveals an universal feature of the stock exchange\nmarket.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.2508v2"
    },
    {
        "title": "A model of returns for the post-credit-crunch reality: Hybrid Brownian\n  motion with price feedback",
        "authors": [
            "William T. Shaw"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The market events of 2007-2009 have reinvigorated the search for realistic\nreturn models that capture greater likelihoods of extreme movements. In this\npaper we model the medium-term log-return dynamics in a market with both\nfundamental and technical traders. This is based on a Poisson trade arrival\nmodel with variable size orders. With simplifications we are led to a hybrid\nSDE mixing both arithmetic and geometric Brownian motions, whose solution is\ngiven by a class of integrals of exponentials of one Brownian motion against\nanother, in forms considered by Yor and collaborators. The reduction of the\nhybrid SDE to a single Brownian motion leads to an SDE of the form considered\nby Nagahara, which is a type of \"Pearson diffusion\", or equivalently a\nhyperbolic OU SDE. Various dynamics and equilibria are possible depending on\nthe balance of trades. Under mean-reverting circumstances we arrive naturally\nat an equilibrium fat-tailed return distribution with a Student or Pearson Type\nIV form. Under less restrictive assumptions richer dynamics are possible,\nincluding bimodal structures. The phenomenon of variance explosion is\nidentified that gives rise to much larger price movements that might have a\npriori been expected, so that \"$25\\sigma$\" events are significantly more\nprobable. We exhibit simple example solutions of the Fokker-Planck equation\nthat shows how such variance explosion can hide beneath a standard Gaussian\nfacade. These are elementary members of an extended class of distributions with\na rich and varied structure, capable of describing a wide range of market\nbehaviours. Several approaches to the density function are possible, and an\nexample of the computation of a hyperbolic VaR is given. The model also\nsuggests generalizations of the Bougerol identity.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0182v5"
    },
    {
        "title": "Statistical properties of information flow in financial time series",
        "authors": [
            "Cheoljun Eom",
            "Okyu Kwon",
            "Woo-Sung Jung"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  This paper has been withdrawn by the authors.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0448v2"
    },
    {
        "title": "On non-existence of a one factor interest rate model for volatility\n  averaged generalized Fong-Vasicek term structures",
        "authors": [
            "B. Stehlikova",
            "D. Sevcovic"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The purpose of this paper is to study the generalized Fong--Vasicek\ntwo-factor interest rate model with stochastic volatility. In this model the\ndispersion of the stochastic short rate (square of volatility) is assumed to be\nstochastic as well and it follows a non-negative process with volatility\nproportional to the square root of dispersion. The drift of the stochastic\nprocess for the dispersion is assumed to be in a rather general form including,\nin particular, linear function having one root (yielding the original\nFong--Vasicek model or a cubic like function having three roots (yielding a\ngeneralized Fong--Vasicek model for description of the volatility clustering).\nWe consider averaged bond prices with respect to the limiting distribution of\nstochastic dispersion. The averaged bond prices depend on time and current\nlevel of the short rate like it is the case in many popular one-factor interest\nrate model including in particular the Vasicek and Cox--Ingersoll-Ross model.\nHowever, as a main result of this paper we show that there is no such\none-factor model yielding the same bond prices as the averaged values described\nabove.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.0473v1"
    },
    {
        "title": "Estimation of the instantaneous volatility",
        "authors": [
            "A. Alvarez",
            "F. Panloup",
            "M. Pontier",
            "N. Savy"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  This paper is concerned with the estimation of the volatility process in a\nstochastic volatility model of the following form: $dX_t=a_tdt+\\sigma_tdW_t$,\nwhere $X$ denotes the log-price and $\\sigma$ is a c\\`adl\\`ag semi-martingale.\nIn the spirit of a series of recent works on the estimation of the cumulated\nvolatility, we here focus on the instantaneous volatility for which we study\nestimators built as finite differences of the \\textit{power variations} of the\nlog-price. We provide central limit theorems with an optimal rate depending on\nthe local behavior of $\\sigma$. In particular, these theorems yield some\nconfidence intervals for $\\sigma_t$.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.3538v4"
    },
    {
        "title": "Stochastic Volatility Models Including Open, Close, High and Low Prices",
        "authors": [
            "Abel Rodriguez",
            "Henryk Gzyl",
            "German Molina",
            "Enrique ter Horst"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Mounting empirical evidence suggests that the observed extreme prices within\na trading period can provide valuable information about the volatility of the\nprocess within that period. In this paper we define a class of stochastic\nvolatility models that uses opening and closing prices along with the minimum\nand maximum prices within a trading period to infer the dynamics underlying the\nvolatility process of asset prices and compares it with similar models that\nhave been previously presented in the literature. The paper also discusses\nsequential Monte Carlo algorithms to fit this class of models and illustrates\nits features using both a simulation study and data form the SP500 index.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.1315v1"
    },
    {
        "title": "An Analysis of the Japanese Credit Network",
        "authors": [
            "G. De Masi",
            "Y. Fujiwara",
            "M. Gallegati",
            "B. Greenwald",
            "J. E. Stiglitz"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  An analysis of the Japanese credit market in 2004 between banks and quoted\nfirms is done in this paper using the tools of the networks theory. It can be\npointed out that: (i) a backbone of the credit channel emerges, where some\nlinks play a crucial role; (ii) big banks privilege long-term contracts; the\n\"minimal spanning trees\" (iii) disclose a highly hierarchical backbone, where\nthe central positions are occupied by the largest banks, and emphasize (iv) a\nstrong geographical characterization, while (v) the clusters of firms do not\nhave specific common properties. Moreover, (vi) while larger firms have\nmultiple lending in large, (vii) the demand for credit (long vs. short term\ndebt and multi-credit lines) of firms with similar sizes is very heterogeneous.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.2384v2"
    },
    {
        "title": "Quantitative law describing market dynamics before and after\n  interest-rate change",
        "authors": [
            "Alexander M. Petersen",
            "Fengzhong Wang",
            "Shlomo Havlin",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We study the behavior of U.S. markets both before and after U.S. Federal Open\nMarket Committee (FOMC) meetings, and show that the announcement of a U.S.\nFederal Reserve rate change causes a financial shock, where the dynamics after\nthe announcement is described by an analogue of the Omori earthquake law. We\nquantify the rate n(t) of aftershocks following an interest rate change at time\nT, and find power-law decay which scales as n(t-T) (t-T)^-$\\Omega$, with\n$\\Omega$ positive. Surprisingly, we find that the same law describes the rate\nn'(|t-T|) of \"pre-shocks\" before the interest rate change at time T. This is\nthe first study to quantitatively relate the size of the market response to the\nnews which caused the shock and to uncover the presence of quantifiable\npreshocks. We demonstrate that the news associated with interest rate change is\nresponsible for causing both the anticipation before the announcement and the\nsurprise after the announcement. We estimate the magnitude of financial news\nusing the relative difference between the U. S. Treasury Bill and the Federal\nFunds Effective rate. Our results are consistent with the \"sign effect,\" in\nwhich \"bad news\" has a larger impact than \"good news.\" Furthermore, we observe\nsignificant volatility aftershocks, confirming a \"market underreaction\" that\nlasts at least 1 trading day.\n",
        "pdf_link": "http://arxiv.org/pdf/0903.0010v3"
    },
    {
        "title": "The Size Variance Relationship of Business Firm Growth Rates",
        "authors": [
            "Massimo Riccaboni",
            "Fabio Pammolli",
            "Sergey V. Buldyrev",
            "Linda Ponta",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  The relationship between the size and the variance of firm growth rates is\nknown to follow an approximate power-law behavior $\\sigma(S) \\sim\nS^{-\\beta(S)}$ where $S$ is the firm size and $\\beta(S)\\approx 0.2$ is an\nexponent weakly dependent on $S$. Here we show how a model of proportional\ngrowth which treats firms as classes composed of various number of units of\nvariable size, can explain this size-variance dependence. In general, the model\npredicts that $\\beta(S)$ must exhibit a crossover from $\\beta(0)=0$ to\n$\\beta(\\infty)=1/2$. For a realistic set of parameters, $\\beta(S)$ is\napproximately constant and can vary in the range from 0.14 to 0.2 depending on\nthe average number of units in the firm. We test the model with a unique\nindustry specific database in which firm sales are given in terms of the sum of\nthe sales of all their products. We find that the model is consistent with the\nempirically observed size-variance relationship.\n",
        "pdf_link": "http://arxiv.org/pdf/0904.1404v2"
    },
    {
        "title": "Statistical Properties of Fluctuations: A Method to Check Market\n  Behavior",
        "authors": [
            "Prasanta K. Panigrahi",
            "Sayantan Ghosh",
            "P. Manimaran",
            "Dilip P. Ahalpara"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We analyze the Bombay stock exchange (BSE) price index over the period of\nlast 12 years. Keeping in mind the large fluctuations in last few years, we\ncarefully find out the transient, non-statistical and locally structured\nvariations. For that purpose, we make use of Daubechies wavelet and\ncharacterize the fractal behavior of the returns using a recently developed\nwavelet based fluctuation analysis method. the returns show a fat-tail\ndistribution as also weak non-statistical behavior. We have also carried out\ncontinuous wavelet as well as Fourier power spectral analysis to characterize\nthe periodic nature and correlation properties of the time series.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.4237v1"
    },
    {
        "title": "Universal Correlations and Power-Law Tails in Financial Covariance\n  Matrices",
        "authors": [
            "Gernot Akemann",
            "Jonit Fischmann",
            "Pierpaolo Vivo"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Signatures of universality are detected by comparing individual eigenvalue\ndistributions and level spacings from financial covariance matrices to random\nmatrix predictions. A chopping procedure is devised in order to produce a\nstatistical ensemble of asset-price covariances from a single instance of\nfinancial data sets. Local results for the smallest eigenvalue and individual\nspacings are very stable upon reshuffling the time windows and assets. They are\nin good agreement with the universal Tracy-Widom distribution and Wigner\nsurmise, respectively. This suggests a strong degree of robustness especially\nin the low-lying sector of the spectra, most relevant for portfolio selections.\nConversely, the global spectral density of a single covariance matrix as well\nas the average over all unfolded nearest-neighbour spacing distributions\ndeviate from standard Gaussian random matrix predictions. The data are in fair\nagreement with a recently introduced generalised random matrix model, with\ncorrelations showing a power-law decay.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.5249v1"
    },
    {
        "title": "Robust Estimators in Generalized Pareto Models",
        "authors": [
            "Peter Ruckdeschel",
            "Nataliya Horbenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  This paper deals with optimally-robust parameter estimation in generalized\nPareto distributions (GPDs). These arise naturally in many situations where one\nis interested in the behavior of extreme events as motivated by the\nPickands-Balkema-de Haan extreme value theorem (PBHT). The application we have\nin mind is calculation of the regulatory capital required by Basel II for a\nbank to cover operational risk. In this context the tail behavior of the\nunderlying distribution is crucial. This is where extreme value theory enters,\nsuggesting to estimate these high quantiles parameterically using, e.g. GPDs.\nRobust statistics in this context offers procedures bounding the influence of\nsingle observations, so provides reliable inference in the presence of moderate\ndeviations from the distributional model assumptions, respectively from the\nmechanisms underlying the PBHT.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1476v6"
    },
    {
        "title": "Emergence of universal scaling in financial markets from mean-field\n  dynamics",
        "authors": [
            "S. V. Vikram",
            "Sitabhra Sinha"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Collective phenomena with universal properties have been observed in many\ncomplex systems with a large number of components. Here we present a\nmicroscopic model of the emergence of scaling behavior in such systems, where\nthe interaction dynamics between individual components is mediated by a global\nvariable making the mean-field description exact. Using the example of\nfinancial markets, we show that asset price can be such a global variable with\nthe critical role of coordinating the actions of agents who are otherwise\nindependent. The resulting model accurately reproduces empirical properties\nsuch as the universal scaling of the price fluctuation and volume\ndistributions, long-range correlations in volatility and multiscaling.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.0628v1"
    },
    {
        "title": "Mesoscopic modelling of financial markets",
        "authors": [
            "S. Cordier",
            "L. Pareschi",
            "C. Piatecki"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  We derive a mesoscopic description of the behavior of a simple financial\nmarket where the agents can create their own portfolio between two investment\nalternatives: a stock and a bond. The model is derived starting from the\nLevy-Levy-Solomon microscopic model (Econ. Lett., 45, (1994), 103--111) using\nthe methods of kinetic theory and consists of a linear Boltzmann equation for\nthe wealth distribution of the agents coupled with an equation for the price of\nthe stock. From this model, under a suitable scaling, we derive a Fokker-Planck\nequation and show that the equation admits a self-similar lognormal behavior.\nSeveral numerical examples are also reported to validate our analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.2743v1"
    },
    {
        "title": "A quantum model for the stock market",
        "authors": [
            "Chao Zhang",
            "Lu Huang"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  Beginning with several basic hypotheses of quantum mechanics, we give a new\nquantum model in econophysics. In this model, we define wave functions and\noperators of the stock market to establish the Schr\\\"odinger equation for the\nstock price. Based on this theoretical framework, an example of a driven\ninfinite quantum well is considered, in which we use a cosine distribution to\nsimulate the state of stock price in equilibrium. After adding an external\nfield into the Hamiltonian to analytically calculate the wave function, the\ndistribution and the average value of the rate of return are shown.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.4843v2"
    },
    {
        "title": "Predicting economic market crises using measures of collective panic",
        "authors": [
            "Dion Harmon",
            "Marcus A. M. de Aguiar",
            "David D. Chinellato",
            "Dan Braha",
            "Irving R. Epstein",
            "Yaneer Bar-Yam"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Predicting panic is of critical importance in many areas of human and animal\nbehavior, notably in the context of economics. The recent financial crisis is a\ncase in point. Panic may be due to a specific external threat, or\nself-generated nervousness. Here we show that the recent economic crisis and\nearlier large single-day panics were preceded by extended periods of high\nlevels of market mimicry --- direct evidence of uncertainty and nervousness,\nand of the comparatively weak influence of external news. High levels of\nmimicry can be a quite general indicator of the potential for self-organized\ncrises.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.2620v1"
    },
    {
        "title": "A dynamic hybrid model based on wavelets and fuzzy regression for time\n  series estimation",
        "authors": [
            "Olfa Zaafrane",
            "Anouar Ben Mabrouk"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  In the present paper, a fuzzy logic based method is combined with wavelet\ndecomposition to develop a step-by-step dynamic hybrid model for the estimation\nof financial time series. Empirical tests on fuzzy regression, wavelet\ndecomposition as well as the new hybrid model are conducted on the well known\n$SP500$ index financial time series. The empirical tests show an efficiency of\nthe hybrid model.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3702v1"
    },
    {
        "title": "Black swans or dragon kings? A simple test for deviations from the power\n  law",
        "authors": [
            "Joanna Janczura",
            "Rafal Weron"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We develop a simple test for deviations from power law tails, which is based\non the asymptotic properties of the empirical distribution function. We use\nthis test to answer the question whether great natural disasters, financial\ncrashes or electricity price spikes should be classified as dragon kings or\n'only' as black swans.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3712v1"
    },
    {
        "title": "Minding impacting events in a model of stochastic variance",
        "authors": [
            "Silvio M. Duarte Queiros",
            "Evaldo M. F. Curado",
            "Fernando D. Nobre"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We introduce a generalisation of the well-known ARCH process, widely used for\ngenerating uncorrelated stochastic time series with long-term non-Gaussian\ndistributions and long-lasting correlations in the (instantaneous) standard\ndeviation exhibiting a clustering profile. Specifically, inspired by the fact\nthat in a variety of systems impacting events are hardly forgot, we split the\nprocess into two different regimes: a first one for regular periods where the\naverage volatility of the fluctuations within a certain period of time is below\na certain threshold and another one when the local standard deviation\noutnumbers it. In the former situation we use standard rules for\nheteroscedastic processes whereas in the latter case the system starts\nrecalling past values that surpassed the threshold. Our results show that for\nappropriate parameter values the model is able to provide fat tailed\nprobability density functions and strong persistence of the instantaneous\nvariance characterised by large values of the Hurst exponent is greater than\n0.8, which are ubiquitous features in complex systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.4819v2"
    },
    {
        "title": "Record statistics for biased random walks, with an application to\n  financial data",
        "authors": [
            "Gregor Wergen",
            "Miro Bogner",
            "Joachim Krug"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We consider the occurrence of record-breaking events in random walks with\nasymmetric jump distributions. The statistics of records in symmetric random\nwalks was previously analyzed by Majumdar and Ziff and is well understood.\nUnlike the case of symmetric jump distributions, in the asymmetric case the\nstatistics of records depends on the choice of the jump distribution. We\ncompute the record rate $P_n(c)$, defined as the probability for the $n$th\nvalue to be larger than all previous values, for a Gaussian jump distribution\nwith standard deviation $\\sigma$ that is shifted by a constant drift $c$. For\nsmall drift, in the sense of $c/\\sigma \\ll n^{-1/2}$, the correction to\n$P_n(c)$ grows proportional to arctan$(\\sqrt{n})$ and saturates at the value\n$\\frac{c}{\\sqrt{2} \\sigma}$. For large $n$ the record rate approaches a\nconstant, which is approximately given by\n$1-(\\sigma/\\sqrt{2\\pi}c)\\textrm{exp}(-c^2/2\\sigma^2)$ for $c/\\sigma \\gg 1$.\nThese asymptotic results carry over to other continuous jump distributions with\nfinite variance. As an application, we compare our analytical results to the\nrecord statistics of 366 daily stock prices from the Standard & Poors 500\nindex. The biased random walk accounts quantitatively for the increase in the\nnumber of upper records due to the overall trend in the stock prices, and after\ndetrending the number of upper records is in good agreement with the symmetric\nrandom walk. However the number of lower records in the detrended data is\nsignificantly reduced by a mechanism that remains to be identified.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.0893v1"
    },
    {
        "title": "On interrelations of recurrences and connectivity trends between stock\n  indices",
        "authors": [
            "B. Goswami",
            "G. Ambika",
            "N. Marwan",
            "J. Kurths"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Financial data has been extensively studied for correlations using Pearson's\ncross-correlation coefficient {\\rho} as the point of departure. We employ an\nestimator based on recurrence plots --- the Correlation of Probability of\nRecurrence (CPR) --- to analyze connections between nine stock indices spread\nworldwide. We suggest a slight modification of the CPR approach in order to get\nmore robust results. We examine trends in CPR for an approximately 19-month\nwindow moved along the time series and compare them to {\\rho}. Binning CPR into\nthree levels of connectedness: strong, moderate and weak, we extract the trends\nin number of connections in each bin over time. We also look at the behavior of\nCPR during the Dot-Com bubble by shifting the time series to align their peaks.\nCPR mainly uncovers that the markets move in and out of periods of strong\nconnectivity erratically, instead of moving monotonously towards increasing\nglobal connectivity. This is in contrast to {\\rho}, which gives a picture of\never increasing correlation. CPR also exhibits that time shifted markets have\nhigh connectivity around the Dot-Com bubble of 2000. We stress on the\nimportance of significance testing in interpreting measures applied to field\ndata. CPR is more robust to significance testing. It has the additional\nadvantages of being robust to noise, and reliable for short time series lengths\nand low frequency of sampling. Further, it is more sensitive to changes than\n{\\rho} as it captures correlations between the essential dynamics of the\nunderlying systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.5189v1"
    },
    {
        "title": "Goodness-of-Fit tests with Dependent Observations",
        "authors": [
            "Remy Chicheportiche",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We revisit the Kolmogorov-Smirnov and Cram\\'er-von Mises goodness-of-fit\n(GoF) tests and propose a generalisation to identically distributed, but\ndependent univariate random variables. We show that the dependence leads to a\nreduction of the \"effective\" number of independent observations. The\ngeneralised GoF tests are not distribution-free but rather depend on all the\nlagged bivariate copulas. These objects, that we call \"self-copulas\", encode\nall the non-linear temporal dependences. We introduce a specific, log-normal\nmodel for these self-copulas, for which a number of analytical results are\nderived. An application to financial time series is provided. As is well known,\nthe dependence is to be long-ranged in this case, a finding that we confirm\nusing self-copulas. As a consequence, the acceptance rates for GoF tests are\nsubstantially higher than if the returns were iid random variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.3016v2"
    },
    {
        "title": "Scaling properties and universality of first-passage time probabilities\n  in financial markets",
        "authors": [
            "Josep Perelló",
            "Mario Gutiérrez-Roig",
            "Jaume Masoliver"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Financial markets provide an ideal frame for the study of crossing or\nfirst-passage time events of non-Gaussian correlated dynamics mainly because\nlarge data sets are available. Tick-by-tick data of six futures markets are\nherein considered resulting in fat tailed first-passage time probabilities. The\nscaling of the return with the standard deviation collapses the probabilities\nof all markets examined, and also for different time horizons, into single\ncurves, suggesting that first-passage statistics is market independent (at\nleast for high-frequency data). On the other hand, a very closely related\nquantity, the survival probability, shows, away from the center and tails of\nthe distribution, a hyperbolic $t^{-1/2}$ decay typical of a Markovian dynamics\nalbeit the existence of memory in markets. Modifications of the Weibull and\nStudent distributions are good candidates for the phenomenological description\nof first-passage time properties under certain regimes. The scaling strategies\nshown may be useful for risk control and algorithmic trading.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.1174v2"
    },
    {
        "title": "The effect of round-off error on long memory processes",
        "authors": [
            "Gabriele La Spada",
            "Fabrizio Lillo"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We study how the round-off (or discretization) error changes the statistical\nproperties of a Gaussian long memory process. We show that the autocovariance\nand the spectral density of the discretized process are asymptotically rescaled\nby a factor smaller than one, and we compute exactly this scaling factor.\nConsequently, we find that the discretized process is also long memory with the\nsame Hurst exponent as the original process. We consider the properties of two\nestimators of the Hurst exponent, namely the local Whittle (LW) estimator and\nthe Detrended Fluctuation Analysis (DFA). By using analytical considerations\nand numerical simulations we show that, in presence of round-off error, both\nestimators are severely negatively biased in finite samples. Under regularity\nconditions we prove that the LW estimator applied to discretized processes is\nconsistent and asymptotically normal. Moreover, we compute the asymptotic\nproperties of the DFA for a generic (i.e. non Gaussian) long memory process and\nwe apply the result to discretized processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4476v3"
    },
    {
        "title": "Multi-agent based analysis of financial data",
        "authors": [
            "Tomáš Tokár",
            "Denis Horváth",
            "Michal Hnatich"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  In this work the system of agents is applied to establish a model of the\nnonlinear distributed signal processing. The evolution of the system of the\nagents - by the prediction time scale diversified trend followers, has been\nstudied for the stochastic time-varying environments represented by the real\ncurrency-exchange time series. The time varying population and its statistical\ncharacteristics have been analyzed in the non-interacting and interacting\ncases. The outputs of our analysis are presented in the form of the mean\nlife-times, mean utilities and corresponding distributions. They show that\npopulations are susceptible to the strength and form of inter-agent\ninteraction. We believe that our results will be useful for the development of\nthe robust adaptive prediction systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.2603v1"
    },
    {
        "title": "Dynamics of Bid-ask Spread Return and Volatility of the Chinese Stock\n  Market",
        "authors": [
            "Tian Qiu",
            "Guang Chen",
            "Li-Xin Zhong",
            "Xiao-Run Wu"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Bid-ask spread is taken as an important measure of the financial market\nliquidity. In this article, we study the dynamics of the spread return and the\nspread volatility of four liquid stocks in the Chinese stock market, including\nthe memory effect and the multifractal nature. By investigating the\nautocorrelation function and the Detrended Fluctuation Analysis (DFA), we find\nthat the spread return is lack of long-range memory, while the spread\nvolatility is long-range time correlated. Moreover, by applying the\nMultifractal Detrended Fluctuation Analysis (MF-DFA), the spread return is\nobserved to possess a strong multifractality, which is similar to the dynamics\nof a variety of financial quantities. Differently from the spread return, the\nspread volatility exhibits a weak multifractal nature.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.4455v1"
    },
    {
        "title": "Web search queries can predict stock market volumes",
        "authors": [
            "Ilaria Bordino",
            "Stefano Battiston",
            "Guido Caldarelli",
            "Matthieu Cristelli",
            "Antti Ukkonen",
            "Ingmar Weber"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  We live in a computerized and networked society where many of our actions\nleave a digital trace and affect other people's actions. This has lead to the\nemergence of a new data-driven research field: mathematical methods of computer\nscience, statistical physics and sociometry provide insights on a wide range of\ndisciplines ranging from social science to human mobility. A recent important\ndiscovery is that query volumes (i.e., the number of requests submitted by\nusers to search engines on the www) can be used to track and, in some cases, to\nanticipate the dynamics of social phenomena. Successful exemples include\nunemployment levels, car and home sales, and epidemics spreading. Few recent\nworks applied this approach to stock prices and market sentiment. However, it\nremains unclear if trends in financial markets can be anticipated by the\ncollective wisdom of on-line users on the web. Here we show that trading\nvolumes of stocks traded in NASDAQ-100 are correlated with the volumes of\nqueries related to the same stocks. In particular, query volumes anticipate in\nmany cases peaks of trading by one day or more. Our analysis is carried out on\na unique dataset of queries, submitted to an important web search engine, which\nenable us to investigate also the user behavior. We show that the query volume\ndynamics emerges from the collective but seemingly uncoordinated activity of\nmany users. These findings contribute to the debate on the identification of\nearly warnings of financial systemic risk, based on the activity of users of\nthe www.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.4784v3"
    },
    {
        "title": "Predicting Financial Markets: Comparing Survey, News, Twitter and Search\n  Engine Data",
        "authors": [
            "Huina Mao",
            "Scott Counts",
            "Johan Bollen"
        ],
        "category": "q-fin.ST",
        "published_year": "2011",
        "summary": "  Financial market prediction on the basis of online sentiment tracking has\ndrawn a lot of attention recently. However, most results in this emerging\ndomain rely on a unique, particular combination of data sets and sentiment\ntracking tools. This makes it difficult to disambiguate measurement and\ninstrument effects from factors that are actually involved in the apparent\nrelation between online sentiment and market values. In this paper, we survey a\nrange of online data sets (Twitter feeds, news headlines, and volumes of Google\nsearch queries) and sentiment tracking methods (Twitter Investor Sentiment,\nNegative News Sentiment and Tweet & Google Search volumes of financial terms),\nand compare their value for financial prediction of market indices such as the\nDow Jones Industrial Average, trading volumes, and market volatility (VIX), as\nwell as gold prices. We also compare the predictive power of traditional\ninvestor sentiment survey data, i.e. Investor Intelligence and Daily Sentiment\nIndex, against those of the mentioned set of online sentiment indicators. Our\nresults show that traditional surveys of Investor Intelligence are lagging\nindicators of the financial markets. However, weekly Google Insight Search\nvolumes on financial search queries do have predictive value. An indicator of\nTwitter Investor Sentiment and the frequency of occurrence of financial terms\non Twitter in the previous 1-2 days are also found to be very statistically\nsignificant predictors of daily market log return. Survey sentiment indicators\nare however found not to be statistically significant predictors of financial\nmarket values, once we control for all other mood indicators as well as the\nVIX.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.1051v1"
    },
    {
        "title": "Confidence sets in nonparametric calibration of exponential Lévy\n  models",
        "authors": [
            "Jakob Söhl"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Confidence intervals and joint confidence sets are constructed for the\nnonparametric calibration of exponential L\\'evy models based on prices of\nEuropean options. To this end, we show joint asymptotic normality in the\nspectral calibration method for the estimators of the volatility, the drift,\nthe jump intensity and the L\\'evy density at finitely many points.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.6611v2"
    },
    {
        "title": "Comprehensive Analysis of Market Conditions in the Foreign Exchange\n  Market: Fluctuation Scaling and Variance-Covariance Matrix",
        "authors": [
            "Aki-Hiro Sato",
            "Takaki Hayashi",
            "Janusz A. Hołyst"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We investigate quotation and transaction activities in the foreign exchange\nmarket for every week during the period of June 2007 to December 2010. A\nscaling relationship between the mean values of number of quotations (or number\nof transactions) for various currency pairs and the corresponding standard\ndeviations holds for a majority of the weeks. However, the scaling breaks in\nsome time intervals, which is related to the emergence of market shocks. There\nis a monotonous relationship between values of scaling indices and global\naverages of currency pair cross-correlations when both quantities are observed\nfor various window lengths $\\Delta t$.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.0426v1"
    },
    {
        "title": "Identifying financial crises in real time",
        "authors": [
            "Eder Lucio Fonseca",
            "Fernando F. Ferreira",
            "Paulsamy Muruganandam",
            "Hilda A. Cerdeira"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Following the thermodynamic formulation of multifractal measure that was\nshown to be capable of detecting large fluctuations at an early stage, here we\npropose a new index which permits us to distinguish events like financial\ncrisis in real time . We calculate the partition function from where we obtain\nthermodynamic quantities analogous to free energy and specific heat. The index\nis defined as the normalized energy variation and it can be used to study the\nbehavior of stochastic time series, such as financial market daily data. Famous\nfinancial market crashes - Black Thursday (1929), Black Monday (1987) and\nSubprime crisis (2008) - are identified with clear and robust results. The\nmethod is also applied to the market fluctuations of 2011. From these results\nit appears as if the apparent crisis of 2011 is of a different nature from the\nother three. We also show that the analysis has forecasting capabilities.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.3136v3"
    },
    {
        "title": "Singularity strength based characterization of financial networks",
        "authors": [
            "Sayantan Ghosh",
            "Uwe Jaekel",
            "Francesco Petruccione"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Financial markets are well known examples of multi-fractal complex systems\nthat have garnered much interest in their characterization through complex\nnetwork theory. The recent studies have used correlation based distance metrics\nfor defining and analyzing financial networks. In this work the singularity\nstrength is employed to define a distance metric and the existence of\nhierarchical structure in the Johannesburg Stock Exchange is investigated. The\nmulti-fractal nature of the financial market, which is otherwise hidden in the\ncorrelation coefficient based prescriptions, is analyzed through the use of the\nsingularity strength based method. The presence of a super cluster is exhibited\nin the network which accounts for half of the network size and is homogeneous\nin the sectoral composition of the South African market.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.1710v1"
    },
    {
        "title": "Equilibrium Distribution of Labor Productivity: A Theoretical Model",
        "authors": [
            "Hideaki Aoyama",
            "Hiroshi Iyetomi",
            "Hiroshi Yoshikawa"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We construct a theoretical model for equilibrium distribution of workers\nacross sectors with different labor productivity, assuming that a sector can\naccommodate a limited number of workers which depends only on its productivity.\nA general formula for such distribution of productivity is obtained, using the\ndetail-balance condition necessary for equilibrium in the Ehrenfest-Brillouin\nmodel. We also carry out an empirical analysis on the average number of workers\nin given productivity sectors on the basis of an exhaustive dataset in Japan.\nThe theoretical formula succeeds in explaining the two distinctive\nobservational facts in a unified way, that is, a Boltzmann distribution with\nnegative temperature on low-to-medium productivity side and a decreasing part\nin a power-law form on high productivity side.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.2470v1"
    },
    {
        "title": "Benford's law and Theil transform of financial data",
        "authors": [
            "Paulette Clippe",
            "Marcel Ausloos"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Among econophysics investigations, studies of religious groups have been of\ninterest. On one hand, the present paper concerns the Antoinist community\nfinancial reports, - a community which appeared at the end of the 19-th century\nin Belgium. Several growth-decay regimes have been previously found over\ndifferent time spans. However, there is common suspicion about sect finances.\nIn that spirit, the Antoinist community yearly financial reports, income and\nexpenses, are hereby examined along the so-called Benford's law. The latter is\noften used as a test about possible accounting wrongdoings. On the other hand,\nBenford's law is known to be invariant under scale and base transformation.\nTherefore, as a further test, of both such data and Benford's law use, the\nyearly financial reports are nonlinearly remapped through a sort of Theil\ntransformation, i.e. based on a log-transformation. The resulting data is again\nanalyzed along the Benford's law scheme. Bizarre, puzzling, features are seen.\nHowever, it is emphasized that such a non-linear transformation can shift the\nargument toward a more objective conclusion. In an Appendix, some brief\ndiscussion is made on why the original Theil mapping should not be used. In a\nsecond Appendix, an imperfect Benford's law-like form, - better suited for\nanomalous distributions, is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.5896v1"
    },
    {
        "title": "Heavy-Tailed Features and Empirical Analysis of the Limit Order Book\n  Volume Profiles in Futures Markets",
        "authors": [
            "Kylie-Anne Richards",
            "Gareth W. Peters",
            "William Dunsmuir"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  This paper poses a few fundamental questions regarding the attributes of the\nvolume profile of a Limit Order Books stochastic structure by taking into\nconsideration aspects of intraday and interday statistical features, the impact\nof different exchange features and the impact of market participants in\ndifferent asset sectors. This paper aims to address the following questions:\n  1. Is there statistical evidence that heavy-tailed sub-exponential volume\nprofiles occur at different levels of the Limit Order Book on the bid and ask\nand if so does this happen on intra or interday time scales ?\n  2.In futures exchanges, are heavy tail features exchange (CBOT, CME, EUREX,\nSGX and COMEX) or asset class (government bonds, equities and precious metals)\ndependent and do they happen on ultra-high (<1sec) or mid-range (1sec -10min)\nhigh frequency data?\n  3.Does the presence of stochastic heavy-tailed volume profile features evolve\nin a manner that would inform or be indicative of market participant behaviors,\nsuch as high frequency algorithmic trading, quote stuffing and price discovery\nintra-daily?\n  4. Is there statistical evidence for a need to consider dynamic behavior of\nthe parameters of models for Limit Order Book volume profiles on an intra-daily\ntime scale ?\n  Progress on aspects of each question is obtained via statistically rigorous\nresults to verify the empirical findings for an unprecedentedly large set of\nfutures market LOB data. The data comprises several exchanges, several futures\nasset classes and all trading days of 2010, using market depth (Type II) order\nbook data to 5 levels on the bid and ask.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.7215v2"
    },
    {
        "title": "Market structure explained by pairwise interactions",
        "authors": [
            "Thomas Bury"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Financial markets are a typical example of complex systems where interactions\nbetween constituents lead to many remarkable features. Here, we show that a\npairwise maximum entropy model (or auto-logistic model) is able to describe\nswitches between ordered (strongly correlated) and disordered market states. In\nthis framework, the influence matrix may be thought as a dissimilarity measure\nand we explain how it can be used to study market structure. We make the link\nwith the graph-theoretic description of stock markets reproducing the\nnon-random and scale-free topology, shrinking length during crashes and\nmeaningful clustering features as expected. The pairwise model provides an\nalternative method to study financial networks which may be useful for\ncharacterization of abnormal market states (crises and bubbles), in capital\nallocation or for the design of regulation rules.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.8380v4"
    },
    {
        "title": "Hurst Exponents For Short Time Series",
        "authors": [
            "Jingzhao Qi",
            "Huijie Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  A new concept, called balanced estimator of diffusion entropy, is proposed to\ndetect scalings in short time series. The effectiveness of the method is\nverified by means of a large number of artificial fractional Brownian motions.\nIt is used also to detect scaling properties and structural breaks in stock\nprice series of Shanghai Stock market.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.2862v1"
    },
    {
        "title": "Modeling Financial Volatility in the Presence of Abrupt Changes",
        "authors": [
            "Gordon J. Ross"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  The volatility of financial instruments is rarely constant, and usually\nvaries over time. This creates a phenomenon called volatility clustering, where\nlarge price movements on one day are followed by similarly large movements on\nsuccessive days, creating temporal clusters. The GARCH model, which treats\nvolatility as a drift process, is commonly used to capture this behavior.\nHowever research suggests that volatility is often better described by a\nstructural break model, where the volatility undergoes abrupt jumps in addition\nto drift. Most efforts to integrate these jumps into the GARCH methodology have\nresulted in models which are either very computationally demanding, or which\nmake problematic assumptions about the distribution of the instruments, often\nassuming that they are Gaussian. We present a new approach which uses ideas\nfrom nonparametric statistics to identify structural break points without\nmaking such distributional assumptions, and then models drift separately within\neach identified regime. Using our method, we investigate the volatility of\nseveral major stock indexes, and find that our approach can potentially give an\nimproved fit compared to more commonly used techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6016v1"
    },
    {
        "title": "Dynamics of episodic transient correlations in currency exchange rate\n  returns and their predictability",
        "authors": [
            "Milan Žukovič"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We study the dynamics of the linear and non-linear serial dependencies in\nfinancial time series in a rolling window framework. In particular, we focus on\nthe detection of episodes of statistically significant two- and three-point\ncorrelations in the returns of several leading currency exchange rates that\ncould offer some potential for their predictability. We employ a rolling window\napproach in order to capture the correlation dynamics for different window\nlengths and analyze the distributions of periods with statistically significant\ncorrelations. We find that for sufficiently large window lengths these\ndistributions fit well to power-law behavior. We also measure the\npredictability itself by a hit rate, i.e. the rate of consistency between the\nsigns of the actual returns and their predictions, obtained from a simple\ncorrelation-based predictor. It is found that during these relatively brief\nperiods the returns are predictable to a certain degree and the predictability\ndepends on the selection of the window length.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.1893v1"
    },
    {
        "title": "A Method for Comparing Hedge Funds",
        "authors": [
            "Uri Kartoun"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  The paper presents new machine learning methods: signal composition, which\nclassifies time-series regardless of length, type, and quantity; and\nself-labeling, a supervised-learning enhancement. The paper describes further\nthe implementation of the methods on a financial search engine system to\nidentify behavioral similarities among time-series representing monthly returns\nof 11,312 hedge funds operated during approximately one decade (2000 - 2010).\nThe presented approach of cross-category and cross-location classification\nassists the investor to identify alternative investments.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.0073v2"
    },
    {
        "title": "Volatility Inference in the Presence of Both Endogenous Time and\n  Microstructure Noise",
        "authors": [
            "Yingying Li",
            "Zhiyuan Zhang",
            "Xinghua Zheng"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this article we consider the volatility inference in the presence of both\nmarket microstructure noise and endogenous time. Estimators of the integrated\nvolatility in such a setting are proposed, and their asymptotic properties are\nstudied. Our proposed estimator is compared with the existing popular\nvolatility estimators via numerical studies. The results show that our\nestimator can have substantially better performance when time endogeneity\nexists.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.5809v1"
    },
    {
        "title": "Do wealth distributions follow power laws? Evidence from \"rich lists\"",
        "authors": [
            "Michal Brzezinski"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We use data on wealth of the richest persons taken from the \"rich lists\"\nprovided by business magazines like Forbes to verify if upper tails of wealth\ndistributions follow, as often claimed, a power-law behaviour. The data sets\nused cover the world's richest persons over 1996-2012, the richest Americans\nover 1988-2012, the richest Chinese over 2006-2012 and the richest Russians\nover 2004-2011. Using a recently introduced comprehensive empirical methodology\nfor detecting power laws, which allows for testing goodness of fit as well as\nfor comparing the power-law model with rival distributions, we find that a\npower-law model is consistent with data only in 35% of the analysed data sets.\nMoreover, even if wealth data are consistent with the power-law model, usually\nthey are also consistent with some rivals like the log-normal or stretched\nexponential distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.0212v1"
    },
    {
        "title": "An Information-Theoretic Test for Dependence with an Application to the\n  Temporal Structure of Stock Returns",
        "authors": [
            "Galen Sher",
            "Pedro Vitoria"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Information theory provides ideas for conceptualising information and\nmeasuring relationships between objects. It has found wide application in the\nsciences, but economics and finance have made surprisingly little use of it. We\nshow that time series data can usefully be studied as information -- by noting\nthe relationship between statistical redundancy and dependence, we are able to\nuse the results of information theory to construct a test for joint dependence\nof random variables. The test is in the same spirit of those developed by\nRyabko and Astola (2005, 2006b,a), but differs from these in that we add extra\nrandomness to the original stochatic process. It uses data compression to\nestimate the entropy rate of a stochastic process, which allows it to measure\ndependence among sets of random variables, as opposed to the existing\neconometric literature that uses entropy and finds itself restricted to\npairwise tests of dependence. We show how serial dependence may be detected in\nS&P500 and PSI20 stock returns over different sample periods and frequencies.\nWe apply the test to synthetic data to judge its ability to recover known\ntemporal dependence structures.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.0353v2"
    },
    {
        "title": "Some applications of first-passage ideas to finance",
        "authors": [
            "Rémy Chicheportiche",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  Many problems in finance are related to first passage times. Among all of\nthem, we chose three on which we contributed personally. Our first example\nrelates Kolmogorov-Smirnov like goodness-of-fit tests, modified in such a way\nthat tail events and core events contribute equally to the test (in the\nstandard Kolmogorov-Smirnov, the tails contribute very little to the measure of\ngoodness-of-fit). We show that this problem can be mapped onto that of a random\nwalk inside moving walls. The second example is the optimal time to sell an\nasset (modelled as a random walk with drift) such that the sell time is as\nclose as possible to the time at which the asset reaches its maximum value. The\nlast example concerns optimal trading in the presence of transaction costs. In\nthis case, the optimal strategy is to wait until the predictor reaches (plus or\nminus) a threshold value before buying or selling. The value of this threshold\nis found by mapping the problem onto that of a random walk between two walls.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.3110v1"
    },
    {
        "title": "Non-linear dependences in finance",
        "authors": [
            "Rémy Chicheportiche"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  The thesis is composed of three parts. Part I introduces the mathematical and\nstatistical tools that are relevant for the study of dependences, as well as\nstatistical tests of Goodness-of-fit for empirical probability distributions. I\npropose two extensions of usual tests when dependence is present in the sample\ndata and when observations have a fat-tailed distribution. The financial\ncontent of the thesis starts in Part II. I present there my studies regarding\nthe \"cross-sectional\" dependences among the time series of daily stock returns,\ni.e. the instantaneous forces that link several stocks together and make them\nbehave somewhat collectively rather than purely independently. A calibration of\na new factor model is presented here, together with a comparison to\nmeasurements on real data. Finally, Part III investigates the temporal\ndependences of single time series, using the same tools and measures of\ncorrelation. I propose two contributions to the study of the origin and\ndescription of \"volatility clustering\": one is a generalization of the\nARCH-like feedback construction where the returns are self-exciting, and the\nother one is a more original description of self-dependences in terms of\ncopulas. The latter can be formulated model-free and is not specific to\nfinancial time series. In fact, I also show here how concepts like recurrences,\nrecords, aftershocks and waiting times, that characterize the dynamics in a\ntime series can be written in the unifying framework of the copula.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.5073v1"
    },
    {
        "title": "Frequency Effects on Predictability of Stock Returns",
        "authors": [
            "Paweł Fiedor"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We propose that predictability is a prerequisite for profitability on\nfinancial markets. We look at ways to measure predictability of price changes\nusing information theoretic approach and employ them on all historical data\navailable for NYSE 100 stocks. This allows us to determine whether frequency of\nsampling price changes affects the predictability of those. We also relations\nbetween price changes predictability and the deviation of the price formation\nprocesses from iid as well as the stock's sector. We also briefly comment on\nthe complicated relationship between predictability of price changes and the\nprofitability of algorithmic trading.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.5540v2"
    },
    {
        "title": "Modeling of Volatility with Non-linear Time Series Model",
        "authors": [
            "Kim Song Yon",
            "Kim Mun Chol"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In this paper, non-linear time series models are used to describe volatility\nin financial time series data. To describe volatility, two of the non-linear\ntime series are combined into form TAR (Threshold Auto-Regressive Model) with\nAARCH (Asymmetric Auto-Regressive Conditional Heteroskedasticity) error term\nand its parameter estimation is studied.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.1154v2"
    },
    {
        "title": "Financial interaction networks inferred from traded volumes",
        "authors": [
            "Hongli Zeng",
            "Rémi Lemoy",
            "Mikko Alava"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  In order to use the advanced inference techniques available for Ising models,\nwe transform complex data (real vectors) into binary strings, by local\naveraging and thresholding. This transformation introduces parameters, which\nmust be varied to characterize the behaviour of the system. The approach is\nillustrated on financial data, using three inference methods -- equilibrium,\nsynchronous and asynchronous inference -- to construct functional connections\nbetween stocks. We show that the traded volume information is enough to obtain\nwell known results about financial markets, which use however the presumably\nricher price information: collective behaviour (\"market mode\") and strong\ninteractions within industry sectors. Synchronous and asynchronous Ising\ninference methods give results which are coherent with equilibrium ones, and\nmore detailed since the obtained interaction networks are directed.\n",
        "pdf_link": "http://arxiv.org/pdf/1311.3871v1"
    },
    {
        "title": "Multiscaling edge effects in an agent-based money emergence model",
        "authors": [
            "Paweł Oświęcimka",
            "Stanisław Drożdż",
            "Robert Gębarowski",
            "Andrzej Z. Górski",
            "Jarosław Kwapień"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  An agent-based computational economical toy model for the emergence of money\nfrom the initial barter trading, inspired by Menger's postulate that money can\nspontaneously emerge in a commodity exchange economy, is extensively studied.\nThe model considered, while manageable, is significantly complex, however. It\nis already able to reveal phenomena that can be interpreted as emergence and\ncollapse of money as well as the related competition effects. In particular, it\nis shown that - as an extra emerging effect - the money lifetimes near the\ncritical threshold value develop multiscaling, which allow one to set parallels\nto critical phenomena and, thus, to the real financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.4803v2"
    },
    {
        "title": "Intra-day variability of the stock market activity versus stationarity\n  of the financial time series",
        "authors": [
            "T. Gubiec",
            "M. Wiliński"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We describe the impact of the intra-day activity pattern on the\nautocorrelation function estimator. We obtain an exact formula relating\nestimators of the autocorrelation functions of non-stationary process to its\nstationary counterpart. Hence, we proved that the day seasonality of\ninter-transaction times extends the memory of as well the process itself as its\nabsolute value. That is, both processes relaxation to zero is longer.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.6255v1"
    },
    {
        "title": "On the Complexity and Behaviour of Cryptocurrencies Compared to Other\n  Markets",
        "authors": [
            "Daniel Wilson-Nunn",
            "Hector Zenil"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We show that the behaviour of Bitcoin has interesting similarities to stock\nand precious metal markets, such as gold and silver. We report that whilst\nLitecoin, the second largest cryptocurrency, closely follows Bitcoin's\nbehaviour, it does not show all the reported properties of Bitcoin. Agreements\nbetween apparently disparate complexity measures have been found, and it is\nshown that statistical, information-theoretic, algorithmic and fractal measures\nhave different but interesting capabilities of clustering families of markets\nby type. The report is particularly interesting because of the range and novel\nuse of some measures of complexity to characterize price behaviour, because of\nthe IRS designation of Bitcoin as an investment property and not a currency,\nand the announcement of the Canadian government's own electronic currency\nMintChip.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.1924v1"
    },
    {
        "title": "Estimation of slowly decreasing Hawkes kernels: Application to high\n  frequency order book modelling",
        "authors": [
            "Emmanuel Bacry",
            "Thibault Jaisson",
            "Jean-Francois Muzy"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  We present a modified version of the non parametric Hawkes kernel estimation\nprocedure studied in arXiv:1401.0903 that is adapted to slowly decreasing\nkernels. We show on numerical simulations involving a reasonable number of\nevents that this method allows us to estimate faithfully a power-law decreasing\nkernel over at least 6 decades. We then propose a 8-dimensional Hawkes model\nfor all events associated with the first level of some asset order book.\nApplying our estimation procedure to this model, allows us to uncover the main\nproperties of the coupled dynamics of trade, limit and cancel orders in\nrelationship with the mid-price variations.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7096v1"
    },
    {
        "title": "ANN Model to Predict Stock Prices at Stock Exchange Markets",
        "authors": [
            "B. W. Wanjawa",
            "L. Muchemi"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  Stock exchanges are considered major players in financial sectors of many\ncountries. Most Stockbrokers, who execute stock trade, use technical,\nfundamental or time series analysis in trying to predict stock prices, so as to\nadvise clients. However, these strategies do not usually guarantee good returns\nbecause they guide on trends and not the most likely price. It is therefore\nnecessary to explore improved methods of prediction.\n  The research proposes the use of Artificial Neural Network that is\nfeedforward multi-layer perceptron with error backpropagation and develops a\nmodel of configuration 5:21:21:1 with 80% training data in 130,000 cycles. The\nresearch develops a prototype and tests it on 2008-2012 data from stock markets\ne.g. Nairobi Securities Exchange and New York Stock Exchange, where prediction\nresults show MAPE of between 0.71% and 2.77%. Validation done with Encog and\nNeuroph realized comparable results. The model is thus capable of prediction on\ntypical stock markets.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.06434v1"
    },
    {
        "title": "An Ordinal Pattern Approach to Detect and to Model Leverage Effects and\n  Dependence Structures Between Financial Time Series",
        "authors": [
            "Alexander Schnurr"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We introduce two types of ordinal pattern dependence between time series.\nPositive (resp. negative) ordinal pattern dependence can be seen as a\nnon-paramatric and in particular non-linear counterpart to positive (resp.\nnegative) correlation. We show in an explorative study that both types of this\ndependence show up in real world financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.07321v1"
    },
    {
        "title": "Dynamics of quasi-stationary systems: Finance as an example",
        "authors": [
            "Philip Rinn",
            "Yuriy Stepanov",
            "Joachim Peinke",
            "Thomas Guhr",
            "Rudi Schäfer"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We propose a combination of cluster analysis and stochastic process analysis\nto characterize high-dimensional complex dynamical systems by few dominating\nvariables. As an example, stock market data are analyzed for which the\ndynamical stability as well as transitions between different stable states are\nfound. This combined method also allows to set up new criteria for merging\nclusters to simplify the complexity of the system. The low-dimensional approach\nallows to recover the high-dimensional fixed points of the system by means of\nan optimization procedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.07522v1"
    },
    {
        "title": "Stability and Hierarchy of Quasi-Stationary States: Financial Markets as\n  an Example",
        "authors": [
            "Yuriy Stepanov",
            "Philip Rinn",
            "Thomas Guhr",
            "Joachim Peinke",
            "Rudi Schäfer"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We combine geometric data analysis and stochastic modeling to describe the\ncollective dynamics of complex systems. As an example we apply this approach to\nfinancial data and focus on the non-stationarity of the market correlation\nstructure. We identify the dominating variable and extract its explicit\nstochastic model. This allows us to establish a connection between its time\nevolution and known historical events on the market. We discuss the dynamics,\nthe stability and the hierarchy of the recently proposed quasi-stationary\nmarket states.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00556v1"
    },
    {
        "title": "Constructing Analytically Tractable Ensembles of Non-Stationary\n  Covariances with an Application to Financial Data",
        "authors": [
            "Frederik Meudt",
            "Martin Theissen",
            "Rudi Schäfer",
            "Thomas Guhr"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  In complex systems, crucial parameters are often subject to unpredictable\nchanges in time. Climate, biological evolution and networks provide numerous\nexamples for such non-stationarities. In many cases, improved statistical\nmodels are urgently called for. In a general setting, we study systems of\ncorrelated quantities to which we refer as amplitudes. We are interested in the\ncase of non-stationarity, i.e., seemingly random covariances. We present a\ngeneral method to derive the distribution of the covariances from the\ndistribution of the amplitudes. To ensure analytical tractability, we construct\na properly deformed Wishart ensemble of random matrices. We apply our method to\nfinancial returns where the wealth of data allows us to carry out statistically\nsignificant tests. The ensemble that we find is characterized by an algebraic\ndistribution which improves the understanding of large events.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.01584v2"
    },
    {
        "title": "Detrended partial cross-correlation analysis of two nonstationary time\n  series influenced by common external forces",
        "authors": [
            "Xi-Yuan Qian",
            "Ya-Min Liu",
            "Zhi-Qiang Jiang",
            "Boris Podobnik",
            "Wei-Xing Zhou",
            "H. Eugene Stanley"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  When common factors strongly influence two power-law cross-correlated time\nseries recorded in complex natural or social systems, using classic detrended\ncross-correlation analysis (DCCA) without considering these common factors will\nbias the results. We use detrended partial cross-correlation analysis (DPXA) to\nuncover the intrinsic power-law cross-correlations between two simultaneously\nrecorded time series in the presence of nonstationarity after removing the\neffects of other time series acting as common forces. The DPXA method is a\ngeneralization of the detrended cross-correlation analysis that takes into\naccount partial correlation analysis. We demonstrate the method by using\nbivariate fractional Brownian motions contaminated with a fractional Brownian\nmotion. We find that the DPXA is able to recover the analytical cross Hurst\nindices, and thus the multi-scale DPXA coefficients are a viable alternative to\nthe conventional cross-correlation coefficient. We demonstrate the advantage of\nthe DPXA coefficients over the DCCA coefficients by analyzing contaminated\nbivariate fractional Brownian motions. We calculate the DPXA coefficients and\nuse them to extract the intrinsic cross-correlation between crude oil and gold\nfutures by taking into consideration the impact of the US dollar index. We\ndevelop the multifractal DPXA (MF-DPXA) method in order to generalize the DPXA\nmethod and investigate multifractal time series. We analyze multifractal\nbinomial measures masked with strong white noises and find that the MF-DPXA\nmethod quantifies the hidden multifractal nature while the MF-DCCA method\nfails.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.02435v2"
    },
    {
        "title": "Google matrix of the world network of economic activities",
        "authors": [
            "V. Kandiah",
            "H. Escaith",
            "D. L. Shepelyansky"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Using the new data from the OECD-WTO world network of economic activities we\nconstruct the Google matrix $G$ of this directed network and perform its\ndetailed analysis. The network contains 58 countries and 37 activity sectors\nfor years 1995 and 2008. The construction of $G$, based on Markov chain\ntransitions, treats all countries on equal democratic grounds while the\ncontribution of activity sectors is proportional to their exchange monetary\nvolume. The Google matrix analysis allows to obtain reliable ranking of\ncountries and activity sectors and to determine the sensitivity of\nCheiRank-PageRank commercial balance of countries in respect to price\nvariations and labor cost in various countries. We demonstrate that the\ndeveloped approach takes into account multiplicity of network links with\neconomy interactions between countries and activity sectors thus being more\nefficient compared to the usual export-import analysis. The spectrum and\neigenstates of $G$ are also analyzed being related to specific activity\ncommunities of countries.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.06773v1"
    },
    {
        "title": "Sharper asset ranking from total drawdown durations",
        "authors": [
            "Damien Challet"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  The total duration of drawdowns is shown to provide a moment-free, unbiased,\nefficient and robust estimator of Sharpe ratios both for Gaussian and\nheavy-tailed price returns. We then use this quantity to infer an analytic\nexpression of the bias of moment-based Sharpe ratio estimators as a function of\nthe return distribution tail exponent. The heterogeneity of tail exponents at\nany given time among assets implies that our new method yields significantly\ndifferent asset rankings than those of moment-based methods, especially in\nperiods large volatility. This is fully confirmed by using 20 years of\nhistorical data on 3449 liquid US equities.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.01333v6"
    },
    {
        "title": "Predictability of price movements in deregulated electricity markets",
        "authors": [
            "Olga Y. Uritskaya",
            "Vadim M. Uritsky"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  In this paper we investigate predictability of electricity prices in the\nCanadian provinces of Alberta and Ontario, as well as in the US Mid-C market.\nUsing scale-dependent detrended fluctuation analysis, spectral analysis, and\nthe probability distribution analysis we show that the studied markets exhibit\nstrongly anti-persistent properties suggesting that their dynamics can be\npredicted based on historic price records across the range of time scales from\none hour to one month. For both Canadian markets, the price movements reveal\nthree types of correlated behavior which can be used for forecasting. The\ndiscovered scenarios remain the same on different time scales up to one month\nas well as for on- and off- peak electricity data. These scenarios represent\nsharp increases of prices and are not present in the Mid-C market due to its\nlower volatility. We argue that extreme price movements in this market should\nfollow the same tendency as the more volatile Canadian markets. The estimated\nvalues of the Pareto indices suggest that the prediction of these events can be\nstatistically stable. The results obtained provide new relevant information for\nmanaging financial risks associated with the dynamics of electricity\nderivatives over time frame exceeding one day.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.08117v1"
    },
    {
        "title": "Long-range memory and multifractality in gold markets",
        "authors": [
            "Provash Mali",
            "Amitabha Mukhopadhyay"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Long-range correlation and fluctuation in the gold market time series of\nworld's two leading gold consuming countries, namely China and India, are\nstudied. For both the market series during the period 1985-2013 we observe a\nlong-range persistence of memory in the sequences of maxima (minima) of returns\nin successive time windows of fixed length, but the series as a whole are found\nto be uncorrelated. Multifractal analysis for these series as well as for the\nsequences of maxima (minima) is carried out in terms of the multifractal\ndetrended fluctuation analysis (MF-DFA) method. We observe a weak multifractal\nstructure for the original series that is mainly originated from the fat-tailed\nprobability distribution function of the values, and the multifractal nature of\nthe original time series is enriched into their sequences of maximal (minimal)\nreturns. A quantitative measure of multifractality is provided by using a set\nof \"complexity parameters\".\n",
        "pdf_link": "http://arxiv.org/pdf/1505.08136v1"
    },
    {
        "title": "Contagion effects in the world network of economic activities",
        "authors": [
            "V. Kandiah",
            "H. Escaith",
            "D. L. Shepelyansky"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Using the new data from the OECD-WTO world network of economic activities we\nconstruct the Google matrix $G$ of this directed network and perform its\ndetailed analysis. The network contains 58 countries and 37 activity sectors\nfor years 1995, 2000, 2005, 2008, 2009. The construction of $G$, based on\nMarkov chain transitions, treats all countries on equal democratic grounds\nwhile the contribution of activity sectors is proportional to their exchange\nmonetary volume. The Google matrix analysis allows to obtain reliable ranking\nof countries and activity sectors and to determine the sensitivity of\nCheiRank-PageRank commercial balance of countries in respect to price\nvariations and labor cost in various countries. We demonstrate that the\ndeveloped approach takes into account multiplicity of network links with\neconomy interactions between countries and activity sectors thus being more\nefficient compared to the usual export-import analysis. Our results highlight\nthe striking increase of the influence of German economic activity on other\ncountries during the period 1995 to 2009 while the influence of Eurozone\ndecreases during the same period. We compare our results with the similar\nanalysis of the world trade network from the UN COMTRADE database. We argue\nthat the knowledge of network structure allows to analyze the effects of\neconomic influence and contagion propagation over the world economy.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.03278v1"
    },
    {
        "title": "Multi-scaling of wholesale electricity prices",
        "authors": [
            "Francesco Caravelli",
            "James Requeima",
            "Cozmin Ududec",
            "Ali Ashtari",
            "Tiziana Di Matteo",
            "Tomaso Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We empirically analyze the most volatile component of the electricity price\ntime series from two North-American wholesale electricity markets. We show that\nthese time series exhibit fluctuations which are not described by a Brownian\nMotion, as they show multi-scaling, high Hurst exponents and sharp price\nmovements. We use the generalized Hurst exponent (GHE, $H(q)$) to show that\nalthough these time-series have strong cyclical components, the fluctuations\nexhibit persistent behaviour, i.e., $H(q)>0.5$. We investigate the\neffectiveness of the GHE as a predictive tool in a simple linear forecasting\nmodel, and study the forecast error as a function of $H(q)$, with $q=1$ and\n$q=2$. Our results suggest that the GHE can be used as prediction tool for\nthese time series when the Hurst exponent is dynamically evaluated on rolling\ntime windows of size $\\approx 50 - 100$ hours. These results are also compared\nto the case in which the cyclical components have been subtracted from the time\nseries, showing the importance of cyclicality in the prediction power of the\nHurst exponent.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.06219v1"
    },
    {
        "title": "Novel and topical business news and their impact on stock market\n  activities",
        "authors": [
            "Takayuki Mizuno",
            "Takaaki Ohnishi",
            "Tsutomu Watanabe"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  We propose an indicator to measure the degree to which a particular news\narticle is novel, as well as an indicator to measure the degree to which a\nparticular news item attracts attention from investors. The novelty measure is\nobtained by comparing the extent to which a particular news article is similar\nto earlier news articles, and an article is regarded as novel if there was no\nsimilar article before it. On the other hand, we say a news item receives a lot\nof attention and thus is highly topical if it is simultaneously reported by\nmany news agencies and read by many investors who receive news from those\nagencies. The topicality measure for a news item is obtained by counting the\nnumber of news articles whose content is similar to an original news article\nbut which are delivered by other news agencies. To check the performance of the\nindicators, we empirically examine how these indicators are correlated with\nintraday financial market indicators such as the number of transactions and\nprice volatility. Specifically, we use a dataset consisting of over 90 million\nbusiness news articles reported in English and a dataset consisting of\nminute-by-minute stock prices on the New York Stock Exchange and the NASDAQ\nStock Market from 2003 to 2014, and show that stock prices and transaction\nvolumes exhibited a significant response to a news article when it is novel and\ntopical.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.06477v1"
    },
    {
        "title": "GMM Estimation of Affine Term Structure Models",
        "authors": [
            "Jaroslava Hlouskova",
            "Leopold Sögner"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  This article investigates parameter estimation of affine term structure\nmodels by means of the generalized method of moments. Exact moments of the\naffine latent process as well as of the yields are obtained by using results\nderived for p-polynomial processes. Then the generalized method of moments,\ncombined with Quasi-Bayesian methods, is used to get reliable parameter\nestimates and to perform inference. After a simulation study, the estimation\nprocedure is applied to empirical interest rate data.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.01661v1"
    },
    {
        "title": "Statistical inference for the doubly stochastic self-exciting process",
        "authors": [
            "Simon Clinet",
            "Yoann Potiron"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  We introduce and show the existence of a Hawkes self-exciting point process\nwith exponentially-decreasing kernel and where parameters are time-varying. The\nquantity of interest is defined as the integrated parameter\n$T^{-1}\\int_0^T\\theta_t^*dt$, where $\\theta_t^*$ is the time-varying parameter,\nand we consider the high-frequency asymptotics. To estimate it na\\\"ively, we\nchop the data into several blocks, compute the maximum likelihood estimator\n(MLE) on each block, and take the average of the local estimates. The\nasymptotic bias explodes asymptotically, thus we provide a non-na\\\"ive\nestimator which is constructed as the na\\\"ive one when applying a first-order\nbias reduction to the local MLE. We show the associated central limit theorem.\nMonte Carlo simulations show the importance of the bias correction and that the\nmethod performs well in finite sample, whereas the empirical study discusses\nthe implementation in practice and documents the stochastic behavior of the\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.05831v3"
    },
    {
        "title": "Multivariate Mixed Tempered Stable Distribution",
        "authors": [
            "Asmerilda Hitaj",
            "Friedrich Hubalek",
            "Lorenzo Mercuri",
            "Edit Rroji"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  The multivariate version of the Mixed Tempered Stable is proposed. It is a\ngeneralization of the Normal Variance Mean Mixtures. Characteristics of this\nnew distribution and its capacity in fitting tails and capturing dependence\nstructure between components are investigated. We discuss a random number\ngenerating procedure and introduce an estimation methodology based on the\nminimization of a distance between empirical and theoretical characteristic\nfunctions. Asymptotic tail behavior of the univariate Mixed Tempered Stable is\nexploited in the estimation procedure in order to obtain a better model\nfitting. Advantages of the multivariate Mixed Tempered Stable distribution are\ndiscussed and illustrated via simulation study.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.00926v3"
    },
    {
        "title": "Feasible Invertibility Conditions for Maximum Likelihood Estimation for\n  Observation-Driven Models",
        "authors": [
            "F Blasques",
            "P Gorgi",
            "S Koopman",
            "O Wintenberger"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Invertibility conditions for observation-driven time series models often fail\nto be guaranteed in empirical applications. As a result, the asymptotic theory\nof maximum likelihood and quasi-maximum likelihood estimators may be\ncompromised. We derive considerably weaker conditions that can be used in\npractice to ensure the consistency of the maximum likelihood estimator for a\nwide class of observation-driven time series models. Our consistency results\nhold for both correctly specified and misspecified models. The practical\nrelevance of the theory is highlighted in a set of empirical examples. We\nfurther obtain an asymptotic test and confidence bounds for the unfeasible \"\ntrue \" invertibility region of the parameter space.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.02863v1"
    },
    {
        "title": "Diffusive and arrested-like dynamics in currency exchange markets",
        "authors": [
            "Joaquim Clara-Rahola",
            "Antonio M. Puertas",
            "Miguel Angel Sanchez-Granero",
            "Juan E. Trinidad-Segovia",
            "F. Javier de las Nieves"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  This work studies the symmetry between colloidal dynamics and the dynamics of\nthe Euro--US Dollar currency exchange market (EURUSD). We consider the EURUSD\nprice in the time range between 2001 and 2015, where we find significant\nqualitative symmetry between fluctuation distributions from this market and the\nones belonging to colloidal particles in supercooled or arrested states. In\nparticular, we find that models used for arrested physical systems are suitable\nfor describing the EURUSD fluctuation distributions. Whereas the corresponding\nmean squared price displacement (MSPD) to the EURUSD is diffusive for all\nyears, when focusing in selected time frames within a day, we find a two-step\nMSPD when the New York Stock Exchange market closes, comparable to the dynamics\nin supercooled systems. This is corroborated by looking at the price\ncorrelation functions and non-Gaussian parameters, and can be described by the\ntheoretical model. We discuss the origin and implications of this analogy.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.03195v1"
    },
    {
        "title": "Quantifying instabilities in Financial Markets",
        "authors": [
            "Bruna Amin Gonçalves",
            "Laura Carpi",
            "Osvaldo A. Rosso",
            "Martin G. Ravetti",
            "A. P. F Atman"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Financial global crisis has devastating impacts to economies since early XX\ncentury and continues to impose increasing collateral damages for governments,\nenterprises, and society in general. Up to now, all efforts to obtain efficient\nmethods to predict these events have been disappointing. However, the quest for\na robust estimator of the degree of the market efficiency, or even, a crisis\npredictor, is still one of the most studied subjects in the field. We present\nhere an original contribution that combines Information Theory with graph\nconcepts, to study the return rate series of 32 global trade markets.\nSpecifically, we propose a very simple quantifier that shows to be highly\ncorrelated with global financial instability periods, being also a good\nestimator of the market crisis risk and market resilience. We show that this\nestimator displays striking results when applied to countries that played\ncentral roles during the last major global market crisis. The simplicity and\neffectiveness of our quantifier allow us to anticipate its use in a wide range\nof disciplines.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05499v1"
    },
    {
        "title": "Anomalous Scaling of Stochastic Processes and the Moses Effect",
        "authors": [
            "Lijian Chen",
            "Kevin E. Bassler",
            "Joseph L. McCauley",
            "Gemunu H. Gunaratne"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  The state of a stochastic process evolving over a time $t$ is typically\nassumed to lie on a normal distribution whose width scales like $t^{1/2}$.\nHowever, processes where the probability distribution is not normal and the\nscaling exponent differs from $\\frac{1}{2}$ are known. The search for possible\norigins of such \"anomalous\" scaling and approaches to quantify them are the\nmotivations for the work reported here. In processes with stationary\nincrements, where the stochastic process is time-independent, auto-correlations\nbetween increments and infinite variance of increments can cause anomalous\nscaling. These sources have been referred to as the $\\it{Joseph}$ $\\it{effect}$\nthe $\\it{Noah}$ $\\it{effect}$, respectively. If the increments are\nnon-stationary, then scaling of increments with $t$ can also lead to anomalous\nscaling, a mechanism we refer to as the $\\it{Moses}$ $\\it{effect}$. Scaling\nexponents quantifying the three effects are defined and related to the Hurst\nexponent that characterizes the overall scaling of the stochastic process.\nMethods of time series analysis that enable accurate independent measurement of\neach exponent are presented. Simple stochastic processes are used to illustrate\neach effect. Intraday Financial time series data is analyzed, revealing that\nits anomalous scaling is due only to the Moses effect. In the context of\nfinancial market data, we reiterate that the Joseph exponent, not the Hurst\nexponent, is the appropriate measure to test the efficient market hypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05818v1"
    },
    {
        "title": "Testing the causality of Hawkes processes with time reversal",
        "authors": [
            "Marcus Cordi",
            "Damien Challet",
            "Ioane Muni Toke"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We show that univariate and symmetric multivariate Hawkes processes are only\nweakly causal: the true log-likelihoods of real and reversed event time vectors\nare almost equal, thus parameter estimation via maximum likelihood only weakly\ndepends on the direction of the arrow of time. In ideal (synthetic) conditions,\ntests of goodness of parametric fit unambiguously reject backward event times,\nwhich implies that inferring kernels from time-symmetric quantities, such as\nthe autocovariance of the event rate, only rarely produce statistically\nsignificant fits. Finally, we find that fitting financial data with\nmany-parameter kernels may yield significant fits for both arrows of time for\nthe same event time vector, sometimes favouring the backward time direction.\nThis goes to show that a significant fit of Hawkes processes to real data with\nflexible kernels does not imply a definite arrow of time unless one tests it.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08516v1"
    },
    {
        "title": "Benford's law first significant digit and distribution distances for\n  testing the reliability of financial reports in developing countries",
        "authors": [
            "Jing Shi",
            "Marcel Ausloos",
            "Tingting Zhu"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We discuss a common suspicion about reported financial data, in 10 industrial\nsectors of the 6 so called \"main developing countries\" over the time interval\n[2000-2014]. These data are examined through Benford's law first significant\ndigit and through distribution distances tests. It is shown that several\nvisually anomalous data have to be a priori removed. Thereafter, the\ndistributions much better follow the first digit significant law, indicating\nthe usefulness of a Benford's law test from the research starting line. The\nsame holds true for distance tests. A few outliers are pointed out.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00131v1"
    },
    {
        "title": "Linear and nonlinear market correlations: characterizing financial\n  crises and portfolio optimization",
        "authors": [
            "Alexander Haluszczynski",
            "Ingo Laut",
            "Heike Modest",
            "Christoph Räth"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Pearson correlation and mutual information based complex networks of the\nday-to-day returns of US S&P500 stocks between 1985 and 2015 have been\nconstructed in order to investigate the mutual dependencies of the stocks and\ntheir nature. We show that both networks detect qualitative differences\nespecially during (recent) turbulent market periods thus indicating strongly\nfluctuating interconnections between the stocks of different companies in\nchanging economic environments. A measure for the strength of nonlinear\ndependencies is derived using surrogate data and leads to interesting\nobservations during periods of financial market crises. In contrast to the\nexpectation that dependencies reduce mainly to linear correlations during\ncrises we show that (at least in the 2008 crisis) nonlinear effects are\nsignificantly increasing. It turns out that the concept of centrality within a\nnetwork could potentially be used as some kind of an early warning indicator\nfor abnormal market behavior as we demonstrate with the example of the 2008\nsubprime mortgage crisis. Finally, we apply a Markowitz mean variance portfolio\noptimization and integrate the measure of nonlinear dependencies to scale the\ninvestment exposure. This leads to significant outperformance as compared to a\nfully invested portfolio.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.02661v1"
    },
    {
        "title": "A Second Order Cumulant Spectrum Test That a Stochastic Process is\n  Strictly Stationary and a Step Toward a Test for Graph Signal Strict\n  Stationarity",
        "authors": [
            "Denisa Roberts",
            "Douglas Patterson"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  This article develops a statistical test for the null hypothesis of strict\nstationarity of a discrete time stochastic process in the frequency domain.\nWhen the null hypothesis is true, the second order cumulant spectrum is zero at\nall the discrete Fourier frequency pairs in the principal domain. The test uses\na window averaged sample estimate of the second order cumulant spectrum to\nbuild a test statistic with an asymptotic complex standard normal distribution.\nWe derive the test statistic, study the properties of the test and demonstrate\nits application using 137Cs gamma ray decay data. Future areas of research\ninclude testing for strict stationarity of graph signals, with applications in\nlearning convolutional neural networks on graphs, denoising, and inpainting.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06727v2"
    },
    {
        "title": "Bitcoin market route to maturity? Evidence from return fluctuations,\n  temporal correlations and multiscaling effects",
        "authors": [
            "Stanisław Drożdż",
            "Robert Gębarowski",
            "Ludovico Minati",
            "Paweł Oświęcimka",
            "Marcin Wątorek"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Based on 1-minute price changes recorded since year 2012, the fluctuation\nproperties of the rapidly-emerging Bitcoin (BTC) market are assessed over\nchosen sub-periods, in terms of return distributions, volatility\nautocorrelation, Hurst exponents and multiscaling effects. The findings are\ncompared to the stylized facts of mature world markets. While early trading was\naffected by system-specific irregularities, it is found that over the months\npreceding Apr 2018 all these statistical indicators approach the features\nhallmarking maturity. This can be taken as an indication that the Bitcoin\nmarket, and possibly other cryptocurrencies, carry concrete potential of\nimminently becoming a regular market, alternative to the foreign exchange\n(Forex). Since high-frequency price data are available since the beginning of\ntrading, the Bitcoin offers a unique window into the statistical\ncharacteristics of a market maturation trajectory.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.05916v2"
    },
    {
        "title": "Nonlinearity in stock networks",
        "authors": [
            "David Hartman",
            "Jaroslav Hlinka"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Stock networks, constructed from stock price time series, are a\nwell-established tool for the characterization of complex behavior in stock\nmarkets. Following Mantegna's seminal paper, the linear Pearson's correlation\ncoefficient between pairs of stocks has been the usual way to determine network\nedges. Recently, possible effects of nonlinearity on the graph-theoretical\nproperties of such networks have been demonstrated when using nonlinear\nmeasures such as mutual information instead of linear correlation. In this\npaper, we quantitatively characterize the nonlinearity in stock time series and\nthe effect it has on stock network properties. This is achieved by a systematic\nmulti-step approach that allows us to quantify the nonlinearity of coupling;\ncorrect its effects wherever it is caused by simple univariate non-Gaussianity;\npotentially localize in space and time any remaining strong sources of this\nnonlinearity; and, finally, study the effect nonlinearity has on global network\nproperties. By applying this multi-step approach to stocks included in three\nprominent indices (NYSE100, FTSE100 and SP500), we establish that the apparent\nnonlinearity that has been observed is largely due to univariate\nnon-Gaussianity. Furthermore, strong nonstationarity in a few specific stocks\nmay play a role. In particular, the sharp decrease in some stocks during the\nglobal financial crisis of 2008 gives rise to apparent nonlinear dependencies\namong stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10264v2"
    },
    {
        "title": "Neural networks for stock price prediction",
        "authors": [
            "Yue-Gang Song",
            "Yu-Long Zhou",
            "Ren-Jie Han"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Due to the extremely volatile nature of financial markets, it is commonly\naccepted that stock price prediction is a task full of challenge. However in\norder to make profits or understand the essence of equity market, numerous\nmarket participants or researchers try to forecast stock price using various\nstatistical, econometric or even neural network models. In this work, we survey\nand compare the predictive power of five neural network models, namely, back\npropagation (BP) neural network, radial basis function (RBF) neural network,\ngeneral regression neural network (GRNN), support vector machine regression\n(SVMR), least squares support vector machine regresssion (LS-SVMR). We apply\nthe five models to make price prediction of three individual stocks, namely,\nBank of China, Vanke A and Kweichou Moutai. Adopting mean square error and\naverage absolute percentage error as criteria, we find BP neural network\nconsistently and robustly outperforms the other four models.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11317v1"
    },
    {
        "title": "Long Short-Term Memory Networks for CSI300 Volatility Prediction with\n  Baidu Search Volume",
        "authors": [
            "Yu-Long Zhou",
            "Ren-Jie Han",
            "Qian Xu",
            "Wei-Ke Zhang"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Intense volatility in financial markets affect humans worldwide. Therefore,\nrelatively accurate prediction of volatility is critical. We suggest that\nmassive data sources resulting from human interaction with the Internet may\noffer a new perspective on the behavior of market participants in periods of\nlarge market movements. First we select 28 key words, which are related to\nfinance as indicators of the public mood and macroeconomic factors. Then those\n28 words of the daily search volume based on Baidu index are collected\nmanually, from June 1, 2006 to October 29, 2017. We apply a Long Short-Term\nMemory neural network to forecast CSI300 volatility using those search volume\ndata. Compared to the benchmark GARCH model, our forecast is more accurate,\nwhich demonstrates the effectiveness of the LSTM neural network in volatility\nforecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11954v1"
    },
    {
        "title": "Dynamic Advisor-Based Ensemble (dynABE): Case study in stock trend\n  prediction of critical metal companies",
        "authors": [
            "Zhengyang Dong"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Stock trend prediction is a challenging task due to the market's noise, and\nmachine learning techniques have recently been successful in coping with this\nchallenge. In this research, we create a novel framework for stock prediction,\nDynamic Advisor-Based Ensemble (dynABE). dynABE explores domain-specific areas\nbased on the companies of interest, diversifies the feature set by creating\ndifferent \"advisors\" that each handles a different area, follows an effective\nmodel ensemble procedure for each advisor, and combines the advisors together\nin a second-level ensemble through an online update strategy we developed.\ndynABE is able to adapt to price pattern changes of the market during the\nactive trading period robustly, without needing to retrain the entire model. We\ntest dynABE on three cobalt-related companies, and it achieves the best-case\nmisclassification error of 31.12% and an annualized absolute return of 359.55%\nwith zero maximum drawdown. dynABE also consistently outperforms the baseline\nmodels of support vector machine, neural network, and random forest in all case\nstudies.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.12111v4"
    },
    {
        "title": "Machine Learning for Yield Curve Feature Extraction: Application to\n  Illiquid Corporate Bonds",
        "authors": [
            "Greg Kirczenow",
            "Masoud Hashemi",
            "Ali Fathi",
            "Matt Davison"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  This paper studies an application of machine learning in extracting features\nfrom the historical market implied corporate bond yields. We consider an\nexample of a hypothetical illiquid fixed income market. After choosing a\nsurrogate liquid market, we apply the Denoising Autoencoder (DAE) algorithm to\nlearn the features of the missing yield parameters from the historical data of\nthe instruments traded in the chosen liquid market. The DAE algorithm is then\nchallenged by two \"point-in-time\" inpainting algorithms taken from the image\nprocessing and computer vision domain. It is observed that, when tested on\nunobserved rate surfaces, the DAE algorithm exhibits superior performance\nthanks to the features it has learned from the historical shapes of yield\ncurves.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.01102v1"
    },
    {
        "title": "Multifractal cross-correlations between the World Oil and other\n  Financial Markets in 2012-2017",
        "authors": [
            "Marcin Wątorek",
            "Stanisław Drożdż",
            "Paweł Oświȩcimka",
            "Marek Stanuszek"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Statistical and multiscaling characteristics of WTI Crude Oil prices\nexpressed in US dollar in relation to the most traded currencies as well as to\ngold futures and to the E-mini S$\\&$P500 futures prices on 5 min intra-day\nrecordings in the period January 2012 - December 2017 are studied. It is shown\nthat in most of the cases the tails of return distributions of the considered\nfinancial instruments follow the inverse cubic power law. The only exception is\nthe Russian ruble for which the distribution tail is heavier and scales with\nthe exponent close to 2. From the perspective of multiscaling the analysed time\nseries reveal the multifractal organization with the left-sided asymmetry of\nthe corresponding singularity spectra. Even more, all the considered financial\ninstruments appear to be multifractally cross-correlated with oil, especially\non the level of medium-size fluctuations, as the multifractal cross-correlation\nanalysis carried out by means of the multifractal cross-correlation analysis\n(MFCCA) and detrended cross-correlation coefficient $\\rho_q$ show. The degree\nof such cross-correlations is however varying among the financial instruments.\nThe strongest ties to the oil characterize currencies of the oil extracting\ncountries. Strength of this multifractal coupling appears to depend also on the\noil market trend. In the analysed time period the level of cross-correlations\nsystematically increases during the bear phase on the oil market and it\nsaturates after the trend reversal in 1st half of 2016. The same methodology is\nalso applied to identify possible causal relations between considered\nobservables. Searching for some related asymmetry in the information flow\nmediating cross-correlations indicates that it was the oil price that led the\nRussian ruble over the time period here considered rather than vice versa.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.08548v2"
    },
    {
        "title": "Econometric modelling and forecasting of intraday electricity prices",
        "authors": [
            "Michał Narajewski",
            "Florian Ziel"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In the following paper, we analyse the ID$_3$-Price in the German Intraday\nContinuous electricity market using an econometric time series model. A\nmultivariate approach is conducted for hourly and quarter-hourly products\nseparately. We estimate the model using lasso and elastic net techniques and\nperform an out-of-sample, very short-term forecasting study. The model's\nperformance is compared with benchmark models and is discussed in detail.\nForecasting results provide new insights to the German Intraday Continuous\nelectricity market regarding its efficiency and to the ID$_3$-Price behaviour.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09081v2"
    },
    {
        "title": "Cryptocurrency market structure: connecting emotions and economics",
        "authors": [
            "Tomaso Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We study the dependency and causality structure of the cryptocurrency market\ninvestigating collective movements of both prices and social sentiment related\nto almost two thousand cryptocurrencies traded during the first six months of\n2018. This is the first study of the whole cryptocurrency market structure. It\nintroduces several rigorous innovative methodologies applicable to this and to\nseveral other complex systems where a large number of variables interact in a\nnon-linear way, which is a distinctive feature of the digital economy. The\nanalysis of the dependency structure reveals that prices are significantly\ncorrelated with sentiment. The major, most capitalised cryptocurrencies, such\nas bitcoin, have a central role in the price correlation network but only a\nmarginal role in the sentiment network and in the network describing the\ninteractions between the two. The study of the causality structure reveals a\ncausality network that is consistently related with the correlation structures\nand shows that both prices cause sentiment and sentiment cause prices across\ncurrencies with the latter being stronger in size but smaller in number of\nsignificative interactions. Overall our study uncovers a complex and rich\nstructure of interrelations where prices and sentiment influence each other\nboth instantaneously and with lead-lag causal relations. A major finding is\nthat minor currencies, with small capitalisation, play a crucial role in\nshaping the overall dependency and causality structure. Despite the high level\nof noise and the short time-series we verified that these networks are\nsignificant with all links statistically validated and with a structural\norganisation consistently reproduced across all networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.00472v1"
    },
    {
        "title": "Influence of petroleum and gas trade on EU economies from the reduced\n  Google matrix analysis of UN COMTRADE data",
        "authors": [
            "Célestin Coquidé",
            "Leonardo Ermann",
            "José Lages",
            "D. L. Shepelyansky"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Using the United Nations COMTRADE database we apply the reduced Google matrix\n(REGOMAX) algorithm to analyze the multiproduct world trade in years 2004-2016.\nOur approach allows to determine the trade balance sensitivity of a group of\ncountries to a specific product price increase from a specific exporting\ncountry taking into account all direct and indirect trade pathways via all\nworld countries exchanging 61 UN COMTRADE identified trade products. On the\nbasis of this approach we present the influence of trade in petroleum and gas\nproducts from Russia, USA, Saudi Arabia and Norway determining the sensitivity\nof each EU country. We show that the REGOMAX approach provides a new and more\ndetailed analysis of trade influence propagation comparing to the usual\napproach based on export and import flows.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01820v1"
    },
    {
        "title": "Multi-stream RNN for Merchant Transaction Prediction",
        "authors": [
            "Zhongfang Zhuang",
            "Chin-Chia Michael Yeh",
            "Liang Wang",
            "Wei Zhang",
            "Junpeng Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Recently, digital payment systems have significantly changed people's\nlifestyles. New challenges have surfaced in monitoring and guaranteeing the\nintegrity of payment processing systems. One important task is to predict the\nfuture transaction statistics of each merchant. These predictions can thus be\nused to steer other tasks, ranging from fraud detection to recommendation. This\nproblem is challenging as we need to predict not only multivariate time series\nbut also multi-steps into the future. In this work, we propose a multi-stream\nRNN model for multi-step merchant transaction predictions tailored to these\nrequirements. The proposed multi-stream RNN summarizes transaction data in\ndifferent granularity and makes predictions for multiple steps in the future.\nOur extensive experimental results have demonstrated that the proposed model is\ncapable of outperforming existing state-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.01670v1"
    },
    {
        "title": "GA-MSSR: Genetic Algorithm Maximizing Sharpe and Sterling Ratio Method\n  for RoboTrading",
        "authors": [
            "Zezheng Zhang",
            "Matloob Khushi"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Foreign exchange is the largest financial market in the world, and it is also\none of the most volatile markets. Technical analysis plays an important role in\nthe forex market and trading algorithms are designed utilizing machine learning\ntechniques. Most literature used historical price information and technical\nindicators for training. However, the noisy nature of the market affects the\nconsistency and profitability of the algorithms. To address this problem, we\ndesigned trading rule features that are derived from technical indicators and\ntrading rules. The parameters of technical indicators are optimized to maximize\ntrading performance. We also proposed a novel cost function that computes the\nrisk-adjusted return, Sharpe and Sterling Ratio (SSR), in an effort to reduce\nthe variance and the magnitude of drawdowns. An automatic robotic trading\n(RoboTrading) strategy is designed with the proposed Genetic Algorithm\nMaximizing Sharpe and Sterling Ratio model (GA-MSSR) model. The experiment was\nconducted on intraday data of 6 major currency pairs from 2018 to 2019. The\nresults consistently showed significant positive returns and the performance of\nthe trading system is superior using the optimized rule-based features. The\nhighest return obtained was 320% annually using 5-minute AUDUSD currency pair.\nBesides, the proposed model achieves the best performance on risk factors,\nincluding maximum drawdowns and variance in return, comparing to benchmark\nmodels. The code can be accessed at\nhttps://github.com/zzzac/rule-based-forextrading-system\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09471v1"
    },
    {
        "title": "Stock Prediction: a method based on extraction of news features and\n  recurrent neural networks",
        "authors": [
            "Zeya Zhang",
            "Weizheng Chen",
            "Hongfei Yan"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  This paper proposed a method for stock prediction. In terms of feature\nextraction, we extract the features of stock-related news besides stock prices.\nWe first select some seed words based on experience which are the symbols of\ngood news and bad news. Then we propose an optimization method and calculate\nthe positive polar of all words. After that, we construct the features of news\nbased on the positive polar of their words. In consideration of sequential\nstock prices and continuous news effects, we propose a recurrent neural network\nmodel to help predict stock prices. Compared to SVM classifier with price\nfeatures, we find our proposed method has an over 5% improvement on stock\nprediction accuracy in experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07585v1"
    },
    {
        "title": "A study on the leverage effect on financial series using a TAR model: a\n  Bayesian approach",
        "authors": [
            "Oscar Espinosa",
            "Fabio Nieto"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This research shows that under certain mathematical conditions, a threshold\nautoregressive model (TAR) can represent the leverage effect based on its\nconditional variance function. Furthermore, the analytical expressions for the\nthird and fourth moment of the TAR model are obtained when it is weakly\nstationary.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05319v2"
    },
    {
        "title": "Improving S&P stock prediction with time series stock similarity",
        "authors": [
            "Lior Sidi"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Stock market prediction with forecasting algorithms is a popular topic these\ndays where most of the forecasting algorithms train only on data collected on a\nparticular stock. In this paper, we enriched the stock data with related stocks\njust as a professional trader would have done to improve the stock prediction\nmodels. We tested five different similarities functions and found\nco-integration similarity to have the best improvement on the prediction model.\nWe evaluate the models on seven S&P stocks from various industries over five\nyears period. The prediction model we trained on similar stocks had\nsignificantly better results with 0.55 mean accuracy, and 19.782 profit compare\nto the state of the art model with an accuracy of 0.52 and profit of 6.6.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05784v1"
    },
    {
        "title": "Deep Learning for Financial Applications : A Survey",
        "authors": [
            "Ahmet Murat Ozbayoglu",
            "Mehmet Ugur Gudelek",
            "Omer Berat Sezer"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Computational intelligence in finance has been a very popular topic for both\nacademia and financial industry in the last few decades. Numerous studies have\nbeen published resulting in various models. Meanwhile, within the Machine\nLearning (ML) field, Deep Learning (DL) started getting a lot of attention\nrecently, mostly due to its outperformance over the classical models. Lots of\ndifferent implementations of DL exist today, and the broad interest is\ncontinuing. Finance is one particular area where DL models started getting\ntraction, however, the playfield is wide open, a lot of research opportunities\nstill exist. In this paper, we tried to provide a state-of-the-art snapshot of\nthe developed DL models for financial applications, as of today. We not only\ncategorized the works according to their intended subfield in finance but also\nanalyzed them based on their DL models. In addition, we also aimed at\nidentifying possible future implementations and highlighted the pathway for the\nongoing research within the field.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05786v1"
    },
    {
        "title": "Gaussian process imputation of multiple financial series",
        "authors": [
            "Taco de Wolff",
            "Alejandro Cuevas",
            "Felipe Tobar"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In Financial Signal Processing, multiple time series such as financial\nindicators, stock prices and exchange rates are strongly coupled due to their\ndependence on the latent state of the market and therefore they are required to\nbe jointly analysed. We focus on learning the relationships among financial\ntime series by modelling them through a multi-output Gaussian process (MOGP)\nwith expressive covariance functions. Learning these market dependencies among\nfinancial series is crucial for the imputation and prediction of financial\nobservations. The proposed model is validated experimentally on two real-world\nfinancial datasets for which their correlations across channels are analysed.\nWe compare our model against other MOGPs and the independent Gaussian process\non real financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05789v1"
    },
    {
        "title": "A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data",
        "authors": [
            "Yang Yifan",
            "Guo Ju'e",
            "Sun Shaolong",
            "Li Yixin"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Faced with the growing research towards crude oil price fluctuations\ninfluential factors following the accelerated development of Internet\ntechnology, accessible data such as Google search volume index are increasingly\nquantified and incorporated into forecasting approaches. In this paper, we\napply multi-scale data that including both GSVI data and traditional economic\ndata related to crude oil price as independent variables and propose a new\nhybrid approach for monthly crude oil price forecasting. This hybrid approach,\nbased on divide and conquer strategy, consists of K-means method, kernel\nprincipal component analysis and kernel extreme learning machine , where\nK-means method is adopted to divide input data into certain clusters, KPCA is\napplied to reduce dimension, and KELM is employed for final crude oil price\nforecasting. The empirical result can be analyzed from data and method levels.\nAt the data level, GSVI data perform better than economic data in level\nforecasting accuracy but with opposite performance in directional forecasting\naccuracy because of Herd Behavior, while hybrid data combined their advantages\nand obtain best forecasting performance in both level and directional accuracy.\nAt the method level, the approaches with K-means perform better than those\nwithout K-means, which demonstrates that divide and conquer strategy can\neffectively improve the forecasting performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09656v1"
    },
    {
        "title": "Forecasting Foreign Exchange Rate: A Multivariate Comparative Analysis\n  between Traditional Econometric, Contemporary Machine Learning & Deep\n  Learning Techniques",
        "authors": [
            "Manav Kaushik",
            "A K Giri"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In todays global economy, accuracy in predicting macro-economic parameters\nsuch as the foreign the exchange rate or at least estimating the trend\ncorrectly is of key importance for any future investment. In recent times, the\nuse of computational intelligence-based techniques for forecasting\nmacroeconomic variables has been proven highly successful. This paper tries to\ncome up with a multivariate time series approach to forecast the exchange rate\n(USD/INR) while parallelly comparing the performance of three multivariate\nprediction modelling techniques: Vector Auto Regression (a Traditional\nEconometric Technique), Support Vector Machine (a Contemporary Machine Learning\nTechnique), and Recurrent Neural Networks (a Contemporary Deep Learning\nTechnique). We have used monthly historical data for several macroeconomic\nvariables from April 1994 to December 2018 for USA and India to predict USD-INR\nForeign Exchange Rate. The results clearly depict that contemporary techniques\nof SVM and RNN (Long Short-Term Memory) outperform the widely used traditional\nmethod of Auto Regression. The RNN model with Long Short-Term Memory (LSTM)\nprovides the maximum accuracy (97.83%) followed by SVM Model (97.17%) and VAR\nModel (96.31%). At last, we present a brief analysis of the correlation and\ninterdependencies of the variables used for forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.10247v1"
    },
    {
        "title": "Note on two phase phenomena in financial markets",
        "authors": [
            "Shi-Mei Jiang",
            "Shi-Min Cai",
            "Tao Zhou",
            "Pei-Ling Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  The two phase behavior in financial markets actually means the bifurcation\nphenomenon, which represents the change of the conditional probability from an\nunimodal to a bimodal distribution. In this paper, the bifurcation phenomenon\nin Hang-Seng index is carefully investigated. It is observed that the\nbifurcation phenomenon in financial index is not universal, but specific under\ncertain conditions. The phenomenon just emerges when the power-law exponent of\nabsolute increment distribution is between 1 and 2 with appropriate period.\nSimulations on a randomly generated time series suggest the bifurcation\nphenomenon itself is subject to the statistics of absolute increment, thus it\nmay not be able to reflect the essential financial behaviors. However, even\nunder the same distribution of absolute increment, the range where bifurcation\nphenomenon occurs is far different from real market to artificial data, which\nmay reflect certain market information.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.0108v1"
    },
    {
        "title": "Hausdorff clustering",
        "authors": [
            "N. Basalto",
            "R. Bellotti",
            "F. De Carlo",
            "P. Facchi",
            "E. Pantaleo",
            "S. Pascazio"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  A clustering algorithm based on the Hausdorff distance is introduced and\ncompared to the single and complete linkage. The three clustering procedures\nare applied to a toy example and to the time series of financial data. The\ndendrograms are scrutinized and their features confronted. The Hausdorff\nlinkage relies of firm mathematical grounds and turns out to be very effective\nwhen one has to discriminate among complex structures.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.0748v1"
    },
    {
        "title": "A Multifractal Analysis of Asian Foreign Exchange Markets",
        "authors": [
            "Gabjin Oh",
            "Cheoljun Eom",
            "Shlomo Havlin",
            "Woo-Sung Jung",
            "Fengzhong Wang",
            "H. Eugene Stanley",
            "Seunghwan Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  We analyze the multifractal spectra of daily foreign exchange rates for\nJapan, Hong-Kong, Korea, and Thailand with respect to the United States Dollar\nfrom 1991 to 2005. We find that the return time series show multifractal\nspectrum features for all four cases. To observe the effect of the Asian\ncurrency crisis, we also estimate the multifractal spectra of limited series\nbefore and after the crisis. We find that the Korean and Thai foreign exchange\nmarkets experienced a significant increase in multifractality compared to\nHong-Kong and Japan. We also show that the multifractality is stronge related\nto the presence of high values of returns in the series.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.1475v2"
    },
    {
        "title": "From short to fat tails in financial markets: A unified description",
        "authors": [
            "A. A. G. Cortines",
            "R. Riera",
            "C. Anteneodo"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  In complex systems such as turbulent flows and financial markets, the\ndynamics in long and short time-lags, signaled by Gaussian and fat-tailed\nstatistics, respectively, calls for a unified description. To address this\nissue we analyze a real dataset, namely, price fluctuations, in a wide range of\ntemporal scales to embrace both regimes. By means of Kramers-Moyal (KM)\ncoefficients evaluated from empirical time series, we obtain the evolution\nequation for the probability density function (PDF) of price returns. We also\npresent consistent asymptotic solutions for the timescale dependent equation\nthat emerges from the empirical analysis. From these solutions, new\nrelationships connecting PDF characteristics, such as tail exponents, to\nparameters of KM coefficients arise. The results reveal a dynamical path that\nleads from Gaussian to fat-tailed statistics, furnishing insights on other\ncomplex systems where akin crossover is observed.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.3263v1"
    },
    {
        "title": "The log-periodic-AR(1)-GARCH(1,1) model for financial crashes",
        "authors": [
            "L. Gazola",
            "C. Fernandes",
            "A. Pizzinga",
            "R. Riera"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  This paper intends to meet recent claims for the attainment of more rigorous\nstatistical methodology within the econophysics literature. To this end, we\nconsider an econometric approach to investigate the outcomes of the\nlog-periodic model of price movements, which has been largely used to forecast\nfinancial crashes. In order to accomplish reliable statistical inference for\nunknown parameters, we incorporate an autoregressive dynamic and a conditional\nheteroskedasticity structure in the error term of the original model, yielding\nthe log-periodic-AR(1)-GARCH(1,1) model. Both the original and the extended\nmodels are fitted to financial indices of U. S. market, namely S&P500 and\nNASDAQ. Our analysis reveal two main points: (i) the\nlog-periodic-AR(1)-GARCH(1,1) model has residuals with better statistical\nproperties and (ii) the estimation of the parameter concerning the time of the\nfinancial crash has been improved.\n",
        "pdf_link": "http://arxiv.org/pdf/0801.4341v1"
    },
    {
        "title": "Modeling record-breaking stock prices",
        "authors": [
            "Gregor Wergen"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We study the statistics of record-breaking events in daily stock prices of\n366 stocks from the Standard and Poors 500 stock index. Both the record events\nin the daily stock prices themselves and the records in the daily returns are\ndiscussed. In both cases we try to describe the record statistics of the stock\ndata with simple theoretical models. The daily returns are compared to i.i.d.\nRV's and the stock prices are modeled using a biased random walk, for which the\nrecord statistics are known. These models agree partly with the behavior of the\nstock data, but we also identify several interesting deviations. Most\nimportantly, the number of records in the stocks appears to be systematically\ndecreased in comparison with the random walk model. Considering the\nautoregressive AR(1) process, we can predict the record statistics of the daily\nstock prices more accurately. We also compare the stock data with simulations\nof the record statistics of the more complicated GARCH(1,1) model, which, in\ncombination with the AR(1) model, gives the best agreement with the\nobservational data. To better understand our findings, we discuss the survival\nand first-passage times of stock prices on certain intervals and analyze the\ncorrelations between the individual record events. After recapitulating some\nrecent results for the record statistics of ensembles of N stocks, we also\npresent some new observations for the weekly distributions of record events.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.2048v1"
    },
    {
        "title": "Model-Free Approaches to Discern Non-Stationary Microstructure Noise and\n  Time-Varying Liquidity in High-Frequency Data",
        "authors": [
            "Richard Y. Chen",
            "Per A. Mykland"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  In this paper, we provide non-parametric statistical tools to test\nstationarity of microstructure noise in general hidden Ito semimartingales, and\ndiscuss how to measure liquidity risk using high frequency financial data. In\nparticular, we investigate the impact of non-stationary microstructure noise on\nsome volatility estimators, and design three complementary tests by exploiting\nedge effects, information aggregation of local estimates and high-frequency\nasymptotic approximation. The asymptotic distributions of these tests are\navailable under both stationary and non-stationary assumptions, thereby enable\nus to conservatively control type-I errors and meanwhile ensure the proposed\ntests enjoy the asymptotically optimal statistical power. Besides it also\nenables us to empirically measure aggregate liquidity risks by these test\nstatistics. As byproducts, functional dependence and endogenous microstructure\nnoise are briefly discussed. Simulation with a realistic configuration\ncorroborates our theoretical results, and our empirical study indicates the\nprevalence of non-stationary microstructure noise in New York Stock Exchange.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.06159v4"
    },
    {
        "title": "Performance of information criteria used for model selection of Hawkes\n  process models of financial data",
        "authors": [
            "J. M. Chen",
            "A. G. Hawkes",
            "E. Scalas",
            "M. Trinh"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  We test three common information criteria (IC) for selecting the order of a\nHawkes process with an intensity kernel that can be expressed as a mixture of\nexponential terms. These processes find application in high-frequency financial\ndata modelling. The information criteria are Akaike's information criterion\n(AIC), the Bayesian information criterion (BIC) and the Hannan-Quinn criterion\n(HQ). Since we work with simulated data, we are able to measure the performance\nof model selection by the success rate of the IC in selecting the model that\nwas used to generate the data. In particular, we are interested in the relation\nbetween correct model selection and underlying sample size. The analysis\nincludes realistic sample sizes and parameter sets from recent literature where\nparameters were estimated using empirical financial intra-day data. We compare\nour results to theoretical predictions and similar empirical findings on the\nasymptotic distribution of model selection for consistent and inconsistent IC.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.06055v2"
    },
    {
        "title": "Structural Change in (Economic) Time Series",
        "authors": [
            "Christian Kleiber"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Methods for detecting structural changes, or change points, in time series\ndata are widely used in many fields of science and engineering. This chapter\nsketches some basic methods for the analysis of structural changes in time\nseries data. The exposition is confined to retrospective methods for univariate\ntime series. Several recent methods for dating structural changes are compared\nusing a time series of oil prices spanning more than 60 years. The methods\nbroadly agree for the first part of the series up to the mid-1980s, for which\nchanges are associated with major historical events, but provide somewhat\ndifferent solutions thereafter, reflecting a gradual increase in oil prices\nthat is not well described by a step function. As a further illustration, 1990s\ndata on the volatility of the Hang Seng stock market index are reanalyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.06913v1"
    },
    {
        "title": "Tests for the weights of the global minimum variance portfolio in a\n  high-dimensional setting",
        "authors": [
            "Taras Bodnar",
            "Solomiia Dmytriv",
            "Nestor Parolya",
            "Wolfgang Schmid"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  In this study, we construct two tests for the weights of the global minimum\nvariance portfolio (GMVP) in a high-dimensional setting, namely, when the\nnumber of assets $p$ depends on the sample size $n$ such that $\\frac{p}{n}\\to c\n\\in (0,1)$ as $n$ tends to infinity. In the case of a singular covariance\nmatrix with rank equal to $q$ we assume that $q/n\\to \\tilde{c}\\in(0, 1)$ as\n$n\\to\\infty$. The considered tests are based on the sample estimator and on the\nshrinkage estimator of the GMVP weights. We derive the asymptotic distributions\nof the test statistics under the null and alternative hypotheses. Moreover, we\nprovide a simulation study where the power functions and the receiver operating\ncharacteristic curves of the proposed tests are compared with other existing\napproaches. We observe that the test based on the shrinkage estimator performs\nwell even for values of $c$ close to one.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09587v3"
    },
    {
        "title": "Style Transfer with Time Series: Generating Synthetic Financial Data",
        "authors": [
            "Brandon Da Silva",
            "Sylvie Shang Shi"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Training deep learning models that generalize well to live deployment is a\nchallenging problem in the financial markets. The challenge arises because of\nhigh dimensionality, limited observations, changing data distributions, and a\nlow signal-to-noise ratio. High dimensionality can be dealt with using robust\nfeature selection or dimensionality reduction, but limited observations often\nresult in a model that overfits due to the large parameter space of most deep\nneural networks. We propose a generative model for financial time series, which\nallows us to train deep learning models on millions of simulated paths. We show\nthat our generative model is able to create realistic paths that embed the\nunderlying structure of the markets in a way stochastic processes cannot.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.03232v2"
    },
    {
        "title": "Likelihood Evaluation of Jump-Diffusion Models Using Deterministic\n  Nonlinear Filters",
        "authors": [
            "Jean-François Bégin",
            "Mathieu Boudreault"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this study, we develop a deterministic nonlinear filtering algorithm based\non a high-dimensional version of Kitagawa (1987) to evaluate the likelihood\nfunction of models that allow for stochastic volatility and jumps whose arrival\nintensity is also stochastic. We show numerically that the deterministic\nfiltering method is precise and much faster than the particle filter, in\naddition to yielding a smooth function over the parameter space. We then find\nthe maximum likelihood estimates of various models that include stochastic\nvolatility, jumps in the returns and variance, and also stochastic jump arrival\nintensity with the S&P 500 daily returns. During the Great Recession, the jump\narrival intensity increases significantly and contributes to the clustering of\nvolatility and negative returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.04322v2"
    },
    {
        "title": "Metaheuristics optimized feedforward neural networks for efficient stock\n  price prediction",
        "authors": [
            "Bradley J. Pillay",
            "Absalom E. Ezugwu"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The prediction of stock prices is an important task in economics, investment\nand making financial decisions. This has, for decades, spurred the interest of\nmany researchers to make focused contributions to the design of accurate stock\nprice predictive models; of which some have been utilized to predict the next\nday opening and closing prices of the stock indices. This paper proposes the\ndesign and implementation of a hybrid symbiotic organisms search trained\nfeedforward neural network model for effective and accurate stock price\nprediction. The symbiotic organisms search algorithm is used as an efficient\noptimization technique to train the feedforward neural networks, while the\nresulting training process is used to build a better stock price prediction\nmodel. Furthermore, the study also presents a comparative performance\nevaluation of three different stock price forecasting models; namely, the\nparticle swarm optimization trained feedforward neural network model, the\ngenetic algorithm trained feedforward neural network model and the well-known\nARIMA model. The system developed in support of this study utilizes sixteen\nstock indices as time series datasets for training and testing purpose. Three\nstatistical evaluation measures are used to compare the results of the\nimplemented models, namely the root mean squared error, the mean absolute\npercentage error and the mean absolution deviation. The computational results\nobtained reveal that the symbiotic organisms search trained feedforward neural\nnetwork model exhibits outstanding predictive performance compared to the other\nmodels. However, the performance study shows that the three metaheuristics\ntrained feedforward neural network models have promising predictive competence\nfor solving problems of high dimensional nonlinear time series data, which are\ndifficult to capture by traditional models.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.10121v3"
    },
    {
        "title": "Towards Earnings Call and Stock Price Movement",
        "authors": [
            "Zhiqiang Ma",
            "Grace Bang",
            "Chong Wang",
            "Xiaomo Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Earnings calls are hosted by management of public companies to discuss the\ncompany's financial performance with analysts and investors. Information\ndisclosed during an earnings call is an essential source of data for analysts\nand investors to make investment decisions. Thus, we leverage earnings call\ntranscripts to predict future stock price dynamics. We propose to model the\nlanguage in transcripts using a deep learning framework, where an attention\nmechanism is applied to encode the text data into vectors for the\ndiscriminative network classifier to predict stock price movements. Our\nempirical experiments show that the proposed model is superior to the\ntraditional machine learning baselines and earnings call information can boost\nthe stock price prediction performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01317v1"
    },
    {
        "title": "Forecasting Short-term load using Econometrics time series model with\n  T-student Distribution",
        "authors": [
            "Kasun Chandrarathna",
            "Arman Edalati",
            "AhmadReza Fourozan tabar"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  By significant improvements in modern electrical systems, planning for unit\ncommitment and power dispatching of them are two big concerns between the\nresearchers. Short-term load forecasting plays a significant role in planning\nand dispatching them. In recent years, numerous works have been done on\nShort-term load forecasting. Having an accurate model for predicting the load\ncan be beneficial for optimizing the electrical sources and protecting energy.\nSeveral models such as Artificial Intelligence and Statistics model have been\nused to improve the accuracy of load forecasting. Among the statistics models,\ntime series models show a great performance. In this paper, an Autoregressive\nintegrated moving average (SARIMA) - generalized autoregressive conditional\nheteroskedasticity (GARCH) model as a powerful tool for modeling the\nconditional mean and volatility of time series with the T-student Distribution\nis used to forecast electric load in short period of time. The attained model\nis compared with the ARIMA model with Normal Distribution. Finally, the\neffectiveness of the proposed approach is validated by applying real electric\nload data from the Electric Reliability Council of Texas (ERCOT). KEYWORDS:\nElectricity load, Forecasting, Econometrics Time Series Forecasting, SARIMA\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13595v1"
    },
    {
        "title": "Scaling symmetry, renormalization, and time series modeling",
        "authors": [
            "Marco Zamparo",
            "Fulvio Baldovin",
            "Michele Caraglio",
            "Attilio L. Stella"
        ],
        "category": "q-fin.ST",
        "published_year": "2013",
        "summary": "  We present and discuss a stochastic model of financial assets dynamics based\non the idea of an inverse renormalization group strategy. With this strategy we\nconstruct the multivariate distributions of elementary returns based on the\nscaling with time of the probability density of their aggregates. In its\nsimplest version the model is the product of an endogenous auto-regressive\ncomponent and a random rescaling factor designed to embody also exogenous\ninfluences. Mathematical properties like increments' stationarity and\nergodicity can be proven. Thanks to the relatively low number of parameters,\nmodel calibration can be conveniently based on a method of moments, as\nexemplified in the case of historical data of the S&P500 index. The calibrated\nmodel accounts very well for many stylized facts, like volatility clustering,\npower law decay of the volatility autocorrelation function, and multiscaling\nwith time of the aggregated return distribution. In agreement with empirical\nevidence in finance, the dynamics is not invariant under time reversal and,\nwith suitable generalizations, skewness of the return distribution and leverage\neffects can be included. The analytical tractability of the model opens\ninteresting perspectives for applications, for instance in terms of obtaining\nclosed formulas for derivative pricing. Further important features are: The\npossibility of making contact, in certain limits, with auto-regressive models\nwidely used in finance; The possibility of partially resolving the long-memory\nand short-memory components of the volatility, with consistent results when\napplied to historical series.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3243v3"
    },
    {
        "title": "Evaluating the Performance of ANN Prediction System at Shanghai Stock\n  Market in the Period 21-Sep-2016 to 11-Oct-2016",
        "authors": [
            "Barack Wamkaya Wanjawa"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  This research evaluates the performance of an Artificial Neural Network based\nprediction system that was employed on the Shanghai Stock Exchange for the\nperiod 21-Sep-2016 to 11-Oct-2016. It is a follow-up to a previous paper in\nwhich the prices were predicted and published before September 21. Stock market\nprice prediction remains an important quest for investors and researchers. This\nresearch used an Artificial Intelligence system, being an Artificial Neural\nNetwork that is feedforward multi-layer perceptron with error backpropagation\nfor prediction, unlike other methods such as technical, fundamental or time\nseries analysis. While these alternative methods tend to guide on trends and\nnot the exact likely prices, neural networks on the other hand have the ability\nto predict the real value prices, as was done on this research. Nonetheless,\ndetermination of suitable network parameters remains a challenge in neural\nnetwork design, with this research settling on a configuration of 5:21:21:1\nwith 80% training data or 4-year of training data as a good enough model for\nstock prediction, as already determined in a previous research by the author.\nThe comparative results indicate that neural network can predict typical stock\nmarket prices with mean absolute percentage errors that are as low as 1.95%\nover the ten prediction instances that was studied in this research.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02666v1"
    },
    {
        "title": "Predictability Hidden by Anomalous Observations",
        "authors": [
            "Lorenzo Camponovo",
            "Olivier Scaillet",
            "Fabio Trojani"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  Testing procedures for predictive regressions with lagged autoregressive\nvariables imply a suboptimal inference in presence of small violations of ideal\nassumptions. We propose a novel testing framework resistant to such violations,\nwhich is consistent with nearly integrated regressors and applicable to\nmulti-predictor settings, when the data may only approximately follow a\npredictive regression model. The Monte Carlo evidence demonstrates large\nimprovements of our approach, while the empirical analysis produces a strong\nrobust evidence of market return predictability hidden by anomalous\nobservations, both in- and out-of-sample, using predictive variables such as\nthe dividend yield or the volatility risk premium.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.05072v1"
    },
    {
        "title": "High-performance stock index trading: making effective use of a deep\n  LSTM neural network",
        "authors": [
            "Chariton Chalvatzis",
            "Dimitrios Hristu-Varsakelis"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We present a deep long short-term memory (LSTM)-based neural network for\npredicting asset prices, together with a successful trading strategy for\ngenerating profits based on the model's predictions. Our work is motivated by\nthe fact that the effectiveness of any prediction model is inherently coupled\nto the trading strategy it is used with, and vise versa. This highlights the\ndifficulty in developing models and strategies which are jointly optimal, but\nalso points to avenues of investigation which are broader than prevailing\napproaches. Our LSTM model is structurally simple and generates predictions\nbased on price observations over a modest number of past trading days. The\nmodel's architecture is tuned to promote profitability, as opposed to accuracy,\nunder a strategy that does not trade simply based on whether the price is\npredicted to rise or fall, but rather takes advantage of the distribution of\npredicted returns, and the fact that a prediction's position within that\ndistribution carries useful information about the expected profitability of a\ntrade. The proposed model and trading strategy were tested on the S&P 500, Dow\nJones Industrial Average (DJIA), NASDAQ and Russel 2000 stock indices, and\nachieved cumulative returns of 340%, 185%, 371% and 360%, respectively, over\n2010-2018, far outperforming the benchmark buy-and-hold strategy as well as\nother recent efforts.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.03125v2"
    },
    {
        "title": "Bayesian Nonparametric Adaptive Spectral Density Estimation for\n  Financial Time Series",
        "authors": [
            "Nick James",
            "Roman Marchant",
            "Richard Gerlach",
            "Sally Cripps"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Discrimination between non-stationarity and long-range dependency is a\ndifficult and long-standing issue in modelling financial time series. This\npaper uses an adaptive spectral technique which jointly models the\nnon-stationarity and dependency of financial time series in a non-parametric\nfashion assuming that the time series consists of a finite, but unknown number,\nof locally stationary processes, the locations of which are also unknown. The\nmodel allows a non-parametric estimate of the dependency structure by modelling\nthe auto-covariance function in the spectral domain. All our estimates are made\nwithin a Bayesian framework where we use aReversible Jump Markov Chain Monte\nCarlo algorithm for inference. We study the frequentist properties of our\nestimates via a simulation study, and present a novel way of generating time\nseries data from a nonparametric spectrum. Results indicate that our techniques\nperform well across a range of data generating processes. We apply our method\nto a number of real examples and our results indicate that several financial\ntime series exhibit both long-range dependency and non-stationarity.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.03350v1"
    },
    {
        "title": "Conditional Correlations and Principal Regression Analysis for Futures",
        "authors": [
            "Armine Karami",
            "Raphael Benichou",
            "Michael Benzaquen",
            "Jean-Philippe Bouchaud"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We explore the effect of past market movements on the instantaneous\ncorrelations between assets within the futures market. Quantifying this effect\nis of interest to estimate and manage the risk associated to portfolios of\nfutures in a non-stationary context. We apply and extend a previously reported\nmethod called the Principal Regression Analysis (PRA) to a universe of $84$\nfutures contracts between $2009$ and $2019$. We show that the past up (resp.\ndown) 10 day trends of a novel predictor -- the eigen-factor -- tend to reduce\n(resp. increase) instantaneous correlations. We then carry out a multifactor\nPRA on sectorial predictors corresponding to the four futures sectors (indexes,\ncommodities, bonds and currencies), and show that the effect of past market\nmovements on the future variations of the instantaneous correlations can be\ndecomposed into two significant components. The first component is due to the\nmarket movements within the index sector, while the second component is due to\nthe market movements within the bonds sector.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12354v2"
    },
    {
        "title": "Forecasting Bitcoin closing price series using linear regression and\n  neural networks models",
        "authors": [
            "Nicola Uras",
            "Lodovica Marchesi",
            "Michele Marchesi",
            "Roberto Tonelli"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This paper studies how to forecast daily closing price series of Bitcoin,\nusing data on prices and volumes of prior days. Bitcoin price behaviour is\nstill largely unexplored, presenting new opportunities. We compared our results\nwith two modern works on Bitcoin prices forecasting and with a well-known\nrecent paper that uses Intel, National Bank shares and Microsoft daily NASDAQ\nclosing prices spanning a 3-year interval. We followed different approaches in\nparallel, implementing both statistical techniques and machine learning\nalgorithms. The SLR model for univariate series forecast uses only closing\nprices, whereas the MLR model for multivariate series uses both price and\nvolume data. We applied the ADF -Test to these series, which resulted to be\nindistinguishable from a random walk. We also used two artificial neural\nnetworks: MLP and LSTM. We then partitioned the dataset into shorter sequences,\nrepresenting different price regimes, obtaining best result using more than one\nprevious price, thus confirming our regime hypothesis. All the models were\nevaluated in terms of MAPE and relativeRMSE. They performed well, and were\noverall better than those obtained in the benchmarks. Based on the results, it\nwas possible to demonstrate the efficacy of the proposed methodology and its\ncontribution to the state-of-the-art.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.01127v1"
    },
    {
        "title": "Forecasting NIFTY 50 benchmark Index using Seasonal ARIMA time series\n  models",
        "authors": [
            "Amit Tewari"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This paper analyses how Time Series Analysis techniques can be applied to\ncapture movement of an exchange traded index in a stock market. Specifically,\nSeasonal Auto Regressive Integrated Moving Average (SARIMA) class of models is\napplied to capture the movement of Nifty 50 index which is one of the most\nactively exchange traded contracts globally [1]. A total of 729 model parameter\ncombinations were evaluated and the most appropriate selected for making the\nfinal forecast based on AIC criteria [8]. NIFTY 50 can be used for a variety of\npurposes such as benchmarking fund portfolios, launching of index funds,\nexchange traded funds (ETFs) and structured products. The index tracks the\nbehaviour of a portfolio of blue chip companies, the largest and most liquid\nIndian securities and can be regarded as a true reflection of the Indian stock\nmarket [2].\n",
        "pdf_link": "http://arxiv.org/pdf/2001.08979v1"
    },
    {
        "title": "On the Bound of Cumulative Return in Trading Series and the Verification\n  Using Technical Trading Rules",
        "authors": [
            "Can Yang",
            "Junjie Zhai",
            "Helong Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Although there is a wide use of technical trading rules in stock markets, the\nprofitability of them still remains controversial. This paper first presents\nand proves the upper bound of cumulative return, and then introduces many of\nconventional technical trading rules. Furthermore, with the help of bootstrap\nmethodology, we investigate the profitability of technical trading rules on\ndifferent international stock markets, including developed markets and emerging\nmarkets. At last, the results show that the technical trading rules are hard to\nbeat the market, and even less profitable than the random trading strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.13974v1"
    },
    {
        "title": "Nonlinear behavior of the Chinese SSEC index with a unit root: Evidence\n  from threshold unit root tests",
        "authors": [
            "Xi-Yuan Qian",
            "Fu-Tie Song",
            "Wei-Xing Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We investigate the behavior of the Shanghai Stock Exchange Composite (SSEC)\nindex for the period from 1990:12 to 2007:06 using an unconstrained two-regime\nthreshold autoregressive (TAR) model with an unit root developed by Caner and\nHansen. The method allows us to simultaneously consider non-stationarity and\nnonlinearity in financial time series. Our finding indicates that the Shanghai\nstock market exhibits nonlinear behavior with two regimes and has unit roots in\nboth regimes. The important implications of the threshold effect in stock\nmarkets are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.2284v1"
    },
    {
        "title": "Multi-scale correlations in different futures markets",
        "authors": [
            "M. Bartolozzi",
            "C. Mellen",
            "T. Di Matteo",
            "T. Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  In the present work we investigate the multiscale nature of the correlations\nfor high frequency data (1 minute) in different futures markets over a period\nof two years, starting on the 1st of January 2003 and ending on the 31st of\nDecember 2004. In particular, by using the concept of \"local\" Hurst exponent,\nwe point out how the behaviour of this parameter, usually considered as a\nbenchmark for persistency/antipersistency recognition in time series, is\nlargely time-scale dependent in the market context. These findings are a direct\nconsequence of the intrinsic complexity of a system where trading strategies\nare scale-adaptive. Moreover, our analysis points out different regimes in the\ndynamical behaviour of the market indices under consideration.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.3321v2"
    },
    {
        "title": "On Geometric Ergodicity of Skewed - SVCHARME models",
        "authors": [
            "Jerzy P. Rydlewski",
            "Małgorzata Snarska"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  Markov Chain Monte Carlo is repeatedly used to analyze the properties of\nintractable distributions in a convenient way. In this paper we derive\nconditions for geometric ergodicity of a general class of nonparametric\nstochastic volatility models with skewness driven by hidden Markov Chain with\nswitching.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.1544v1"
    },
    {
        "title": "Universal features of price formation in financial markets: perspectives\n  from Deep Learning",
        "authors": [
            "Justin Sirignano",
            "Rama Cont"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Using a large-scale Deep Learning approach applied to a high-frequency\ndatabase containing billions of electronic market quotes and transactions for\nUS equities, we uncover nonparametric evidence for the existence of a universal\nand stationary price formation mechanism relating the dynamics of supply and\ndemand for a stock, as revealed through the order book, to subsequent\nvariations in its market price. We assess the model by testing its\nout-of-sample predictions for the direction of price moves given the history of\nprice and order flow, across a wide range of stocks and time periods. The\nuniversal price formation model is shown to exhibit a remarkably stable\nout-of-sample prediction accuracy across time, for a wide range of stocks from\ndifferent sectors. Interestingly, these results also hold for stocks which are\nnot part of the training sample, showing that the relations captured by the\nmodel are universal and not asset-specific.\n  The universal model --- trained on data from all stocks --- outperforms, in\nterms of out-of-sample prediction accuracy, asset-specific linear and nonlinear\nmodels trained on time series of any given stock, showing that the universal\nnature of price formation weighs in favour of pooling together financial data\nfrom various stocks, rather than designing asset- or sector-specific models as\ncommonly done. Standard data normalizations based on volatility, price level or\naverage spread, or partitioning the training data into sectors or categories\nsuch as large/small tick stocks, do not improve training results. On the other\nhand, inclusion of price and order flow history over many past observations is\nshown to improve forecasting performance, showing evidence of path-dependence\nin price dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.06917v1"
    },
    {
        "title": "Mining Illegal Insider Trading of Stocks: A Proactive Approach",
        "authors": [
            "Sheikh Rabiul Islam",
            "Sheikh Khaled Ghafoor",
            "William Eberle"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Illegal insider trading of stocks is based on releasing non-public\ninformation (e.g., new product launch, quarterly financial report, acquisition\nor merger plan) before the information is made public. Detecting illegal\ninsider trading is difficult due to the complex, nonlinear, and non-stationary\nnature of the stock market. In this work, we present an approach that detects\nand predicts illegal insider trading proactively from large heterogeneous\nsources of structured and unstructured data using a deep-learning based\napproach combined with discrete signal processing on the time series data. In\naddition, we use a tree-based approach that visualizes events and actions to\naid analysts in their understanding of large amounts of unstructured data.\nUsing existing data, we have discovered that our approach has a good success\nrate in detecting illegal insider trading patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00939v3"
    },
    {
        "title": "Robust estimation of superhedging prices",
        "authors": [
            "Jan Obloj",
            "Johannes Wiesel"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We consider statistical estimation of superhedging prices using historical\nstock returns in a frictionless market with d traded assets. We introduce a\nplugin estimator based on empirical measures and show it is consistent but\nlacks suitable robustness. To address this we propose novel estimators which\nuse a larger set of martingale measures defined through a tradeoff between the\nradius of Wasserstein balls around the empirical measure and the allowed norm\nof martingale densities. We establish consistency and robustness of these\nestimators and argue that they offer a superior performance relative to the\nplugin estimator. We generalise the results by replacing the superhedging\ncriterion with acceptance relative to a risk measure. We further extend our\nstudy, in part, to the case of markets with traded options, to a multiperiod\nsetting and to settings with model uncertainty. We also study convergence rates\nof estimators and convergence of superhedging strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.04211v3"
    },
    {
        "title": "Forecasting market states",
        "authors": [
            "Pier Francesco Procacci",
            "Tomaso Aste"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We propose a novel methodology to define, analyze and forecast market states.\nIn our approach market states are identified by a reference sparse precision\nmatrix and a vector of expectation values. In our procedure, each multivariate\nobservation is associated with a given market state accordingly to a\nminimization of a penalized Mahalanobis distance. The procedure is made\ncomputationally very efficient and can be used with a large number of assets.\nWe demonstrate that this procedure is successful at clustering different states\nof the markets in an unsupervised manner. In particular, we describe an\nexperiment with one hundred log-returns and two states in which the methodology\nautomatically associates states prevalently to pre- and post- crisis periods\nwith one state gathering periods with average positive returns and the other\nstate periods with average negative returns, therefore discovering\nspontaneously the common classification of `bull' and `bear' markets. In\nanother experiment, with again one hundred log-returns and two states, we\ndemonstrate that this procedure can be efficiently used to forecast off-sample\nfuture market states with significant prediction accuracy. This methodology\nopens the way to a range of applications in risk management and trading\nstrategies in the context where the correlation structure plays a central role.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.05836v3"
    },
    {
        "title": "Methods of nonlinear dynamics and the construction of cryptocurrency\n  crisis phenomena precursors",
        "authors": [
            "Vladimir Soloviev",
            "Andrey Belinskiy"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  This article demonstrates the possibility of constructing indicators of\ncritical and crisis phenomena in the volatile market of cryptocurrency. For\nthis purpose, the methods of the theory of complex systems such as recurrent\nanalysis of dynamic systems and the calculation of permutation entropy are\nused. It is shown that it is possible to construct dynamic measures of\ncomplexity, both recurrent and entropy, which behave in a proper way during\nactual pre-crisis periods. This fact is used to build predictors of crisis\nphenomena on the example of the main five crises recorded in the time series of\nthe key cryptocurrency bitcoin, the effectiveness of the proposed\nindicators-precursors of crises has been identified.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.05837v2"
    },
    {
        "title": "Bayesian prediction of jumps in large panels of time series data",
        "authors": [
            "Angelos Alexopoulos",
            "Petros Dellaportas",
            "Omiros Papaspiliopoulos"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We take a new look at the problem of disentangling the volatility and jumps\nprocesses of daily stock returns. We first provide a computational framework\nfor the univariate stochastic volatility model with Poisson-driven jumps that\noffers a competitive inference alternative to the existing tools. This\nmethodology is then extended to a large set of stocks for which we assume that\ntheir unobserved jump intensities co-evolve in time through a dynamic factor\nmodel. To evaluate the proposed modelling approach we conduct out-of-sample\nforecasts and we compare the posterior predictive distributions obtained from\nthe different models. We provide evidence that joint modelling of jumps\nimproves the predictive ability of the stochastic volatility models.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05312v4"
    },
    {
        "title": "A Weight-based Information Filtration Algorithm for Stock-Correlation\n  Networks",
        "authors": [
            "Seyed Soheil Hosseini",
            "Nick Wormald",
            "Tianhai Tian"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Several algorithms have been proposed to filter information on a complete\ngraph of correlations across stocks to build a stock-correlation network. Among\nthem the planar maximally filtered graph (PMFG) algorithm uses $3n-6$ edges to\nbuild a graph whose features include a high frequency of small cliques and a\ngood clustering of stocks. We propose a new algorithm which we call\nproportional degree (PD) to filter information on the complete graph of\nnormalised mutual information (NMI) across stocks. Our results show that the PD\nalgorithm produces a network showing better homogeneity with respect to\ncliques, as compared to economic sectoral classification than its PMFG\ncounterpart. We also show that the partition of the PD network obtained through\nnormalised spectral clustering (NSC) agrees better with the NSC of the complete\ngraph than the corresponding one obtained from PMFG. Finally, we show that the\nclusters in the PD network are more robust with respect to the removal of\nrandom sets of edges than those in the PMFG network.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.06007v1"
    },
    {
        "title": "Risk Management via Anomaly Circumvent: Mnemonic Deep Learning for\n  Midterm Stock Prediction",
        "authors": [
            "Xinyi Li",
            "Yinchuan Li",
            "Xiao-Yang Liu",
            "Christina Dan Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Midterm stock price prediction is crucial for value investments in the stock\nmarket. However, most deep learning models are essentially short-term and\napplying them to midterm predictions encounters large cumulative errors because\nthey cannot avoid anomalies. In this paper, we propose a novel deep neural\nnetwork Mid-LSTM for midterm stock prediction, which incorporates the market\ntrend as hidden states. First, based on the autoregressive moving average model\n(ARMA), a midterm ARMA is formulated by taking into consideration both hidden\nstates and the capital asset pricing model. Then, a midterm LSTM-based deep\nneural network is designed, which consists of three components: LSTM, hidden\nMarkov model and linear regression networks. The proposed Mid-LSTM can avoid\nanomalies to reduce large prediction errors, and has good explanatory effects\non the factors affecting stock prices. Extensive experiments on S&P 500 stocks\nshow that (i) the proposed Mid-LSTM achieves 2-4% improvement in prediction\naccuracy, and (ii) in portfolio allocation investment, we achieve up to 120.16%\nannual return and 2.99 average Sharpe ratio.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01112v1"
    },
    {
        "title": "HATS: A Hierarchical Graph Attention Network for Stock Movement\n  Prediction",
        "authors": [
            "Raehyun Kim",
            "Chan Ho So",
            "Minbyul Jeong",
            "Sanghoon Lee",
            "Jinkyu Kim",
            "Jaewoo Kang"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Many researchers both in academia and industry have long been interested in\nthe stock market. Numerous approaches were developed to accurately predict\nfuture trends in stock prices. Recently, there has been a growing interest in\nutilizing graph-structured data in computer science research communities.\nMethods that use relational data for stock market prediction have been recently\nproposed, but they are still in their infancy. First, the quality of collected\ninformation from different types of relations can vary considerably. No\nexisting work has focused on the effect of using different types of relations\non stock market prediction or finding an effective way to selectively aggregate\ninformation on different relation types. Furthermore, existing works have\nfocused on only individual stock prediction which is similar to the node\nclassification task. To address this, we propose a hierarchical attention\nnetwork for stock prediction (HATS) which uses relational data for stock market\nprediction. Our HATS method selectively aggregates information on different\nrelation types and adds the information to the representations of each company.\nSpecifically, node representations are initialized with features extracted from\na feature extraction module. HATS is used as a relational modeling module with\ninitialized node representations. Then, node representations with the added\ninformation are fed into a task-specific layer. Our method is used for\npredicting not only individual stock prices but also market index movements,\nwhich is similar to the graph classification task. The experimental results\nshow that performance can change depending on the relational data used. HATS\nwhich can automatically select information outperformed all the existing\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.07999v3"
    },
    {
        "title": "Stock Price Forecasting and Hypothesis Testing Using Neural Networks",
        "authors": [
            "Kerda Varaku"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this work we use Recurrent Neural Networks and Multilayer Perceptrons to\npredict NYSE, NASDAQ and AMEX stock prices from historical data. We experiment\nwith different architectures and compare data normalization techniques. Then,\nwe leverage those findings to question the efficient-market hypothesis through\na formal statistical test.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.11212v1"
    },
    {
        "title": "Are Bitcoins price predictable? Evidence from machine learning\n  techniques using technical indicators",
        "authors": [
            "Samuel Asante Gyamerah"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The uncertainties in future Bitcoin price make it difficult to accurately\npredict the price of Bitcoin. Accurately predicting the price for Bitcoin is\ntherefore important for decision-making process of investors and market players\nin the cryptocurrency market. Using historical data from 01/01/2012 to\n16/08/2019, machine learning techniques (Generalized linear model via penalized\nmaximum likelihood, random forest, support vector regression with linear\nkernel, and stacking ensemble) were used to forecast the price of Bitcoin. The\nprediction models employed key and high dimensional technical indicators as the\npredictors. The performance of these techniques were evaluated using mean\nabsolute percentage error (MAPE), root mean square error (RMSE), mean absolute\nerror (MAE), and coefficient of determination (R-squared). The performance\nmetrics revealed that the stacking ensemble model with two base learner (random\nforest and generalized linear model via penalized maximum likelihood) and\nsupport vector regression with linear kernel as meta-learner was the optimal\nmodel for forecasting Bitcoin price. The MAPE, RMSE, MAE, and R-squared values\nfor the stacking ensemble model were 0.0191%, 15.5331 USD, 124.5508 USD, and\n0.9967 respectively. These values show a high degree of reliability in\npredicting the price of Bitcoin using the stacking ensemble model. Accurately\npredicting the future price of Bitcoin will yield significant returns for\ninvestors and market players in the cryptocurrency market.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01268v1"
    },
    {
        "title": "Tehran Stock Exchange Prediction Using Sentiment Analysis of Online\n  Textual Opinions",
        "authors": [
            "Arezoo Hatefi Ghahfarrokhi",
            "Mehrnoush Shamsfard"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this paper, we investigate the impact of the social media data in\npredicting the Tehran Stock Exchange (TSE) variables for the first time. We\nconsider the closing price and daily return of three different stocks for this\ninvestigation. We collected our social media data from Sahamyab.com/stocktwits\nfor about three months. To extract information from online comments, we propose\na hybrid sentiment analysis approach that combines lexicon-based and\nlearning-based methods. Since lexicons that are available for the Persian\nlanguage are not practical for sentiment analysis in the stock market domain,\nwe built a particular sentiment lexicon for this domain. After designing and\ncalculating daily sentiment indices using the sentiment of the comments, we\nexamine their impact on the baseline models that only use historical market\ndata and propose new predictor models using multi regression analysis. In\naddition to the sentiments, we also examine the comments volume and the users'\nreliabilities. We conclude that the predictability of various stocks in TSE is\ndifferent depending on their attributes. Moreover, we indicate that for\npredicting the closing price only comments volume and for predicting the daily\nreturn both the volume and the sentiment of the comments could be useful. We\ndemonstrate that Users' Trust coefficients have different behaviors toward the\nthree stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03792v2"
    },
    {
        "title": "Validating Weak-form Market Efficiency in United States Stock Markets\n  with Trend Deterministic Price Data and Machine Learning",
        "authors": [
            "Samuel Showalter",
            "Jeffrey Gropp"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  The Efficient Market Hypothesis has been a staple of economics research for\ndecades. In particular, weak-form market efficiency -- the notion that past\nprices cannot predict future performance -- is strongly supported by\neconometric evidence. In contrast, machine learning algorithms implemented to\npredict stock price have been touted, to varying degrees, as successful.\nMoreover, some data scientists boast the ability to garner above-market returns\nusing price data alone. This study endeavors to connect existing econometric\nresearch on weak-form efficient markets with data science innovations in\nalgorithmic trading. First, a traditional exploration of stationarity in stock\nindex prices over the past decade is conducted with Augmented Dickey-Fuller and\nVariance Ratio tests. Then, an algorithmic trading platform is implemented with\nthe use of five machine learning algorithms. Econometric findings identify\npotential stationarity, hinting technical evaluation may be possible, though\nalgorithmic trading results find little predictive power in any machine\nlearning model, even when using trend-specific metrics. Accounting for\ntransaction costs and risk, no system achieved above-market returns\nconsistently. Our findings reinforce the validity of weak-form market\nefficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05151v1"
    },
    {
        "title": "To Detect Irregular Trade Behaviors In Stock Market By Using Graph Based\n  Ranking Methods",
        "authors": [
            "Loc Tran",
            "Linh Tran"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  To detect the irregular trade behaviors in the stock market is the important\nproblem in machine learning field. These irregular trade behaviors are\nobviously illegal. To detect these irregular trade behaviors in the stock\nmarket, data scientists normally employ the supervised learning techniques. In\nthis paper, we employ the three graph Laplacian based semi-supervised ranking\nmethods to solve the irregular trade behavior detection problem. Experimental\nresults show that that the un-normalized and symmetric normalized graph\nLaplacian based semi-supervised ranking methods outperform the random walk\nLaplacian based semi-supervised ranking method.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.08964v1"
    },
    {
        "title": "Structural Change Analysis of Active Cryptocurrency Market",
        "authors": [
            "C. Y. Tan",
            "Y. B. Koh",
            "K. H. Ng",
            "K. H. Ng"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Structural Change Analysis of Active Cryptocurrency Market\n",
        "pdf_link": "http://arxiv.org/pdf/1909.10679v1"
    },
    {
        "title": "Implied volatility surface predictability: the case of commodity markets",
        "authors": [
            "Fearghal Kearney",
            "Han Lin Shang",
            "Lisa Sheenan"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Recent literature seek to forecast implied volatility derived from equity,\nindex, foreign exchange, and interest rate options using latent factor and\nparametric frameworks. Motivated by increased public attention borne out of the\nfinancialization of futures markets in the early 2000s, we investigate if these\nextant models can uncover predictable patterns in the implied volatility\nsurfaces of the most actively traded commodity options between 2006 and 2016.\nAdopting a rolling out-of-sample forecasting framework that addresses the\ncommon multiple comparisons problem, we establish that, for energy and precious\nmetals options, explicitly modeling the term structure of implied volatility\nusing the Nelson-Siegel factors produces the most accurate forecasts.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.11009v1"
    },
    {
        "title": "Competition of noise and collectivity in global cryptocurrency trading:\n  route to a self-contained market",
        "authors": [
            "Stanisław Drożdż",
            "Ludovico Minati",
            "Paweł Oświęcimka",
            "Marek Stanuszek",
            "Marcin Wątorek"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Cross-correlations in fluctuations of the daily exchange rates within the\nbasket of the 100 highest-capitalization cryptocurrencies over the period\nOctober 1, 2015, through March 31, 2019, are studied. The corresponding\ndynamics predominantly involve one leading eigenvalue of the correlation\nmatrix, while the others largely coincide with those of Wishart random\nmatrices. However, the magnitude of the principal eigenvalue, and thus the\ndegree of collectivity, strongly depends on which cryptocurrency is used as a\nbase. It is largest when the base is the most peripheral cryptocurrency; when\nmore significant ones are taken into consideration, its magnitude\nsystematically decreases, nevertheless preserving a sizable gap with respect to\nthe random bulk, which in turn indicates that the organization of correlations\nbecomes more heterogeneous. This finding provides a criterion for recognizing\nwhich currencies or cryptocurrencies play a dominant role in the global\ncrypto-market. The present study shows that over the period under\nconsideration, the Bitcoin (BTC) predominates, hallmarking exchange rate\ndynamics at least as influential as the US dollar. The BTC started dominating\naround the year 2017, while further cryptocurrencies, like the Ethereum (ETH)\nand even Ripple (XRP), assumed similar trends. At the same time, the USD, an\noriginal value determinant for the cryptocurrency market, became increasingly\ndisconnected, its related characteristics eventually approaching those of a\nfictitious currency. These results are strong indicators of incipient\nindependence of the global cryptocurrency market, delineating a self-contained\ntrade resembling the Forex.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.08944v2"
    },
    {
        "title": "AdaVol: An Adaptive Recursive Volatility Prediction Method",
        "authors": [
            "Nicklas Werge",
            "Olivier Wintenberger"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Quasi-Maximum Likelihood (QML) procedures are theoretically appealing and\nwidely used for statistical inference. While there are extensive references on\nQML estimation in batch settings, it has attracted little attention in\nstreaming settings until recently. An investigation of the convergence\nproperties of the QML procedure in a general conditionally heteroscedastic time\nseries model is conducted, and the classical batch optimization routines\nextended to the framework of streaming and large-scale problems. An adaptive\nrecursive estimation routine for GARCH models named AdaVol is presented. The\nAdaVol procedure relies on stochastic approximations combined with the\ntechnique of Variance Targeting Estimation (VTE). This recursive method has\ncomputationally efficient properties, while VTE alleviates some convergence\ndifficulties encountered by the usual QML estimation due to a lack of\nconvexity. Empirical results demonstrate a favorable trade-off between AdaVol's\nstability and the ability to adapt to time-varying estimates for real-life\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.02077v4"
    },
    {
        "title": "Adversarial Robustness of Deep Convolutional Candlestick Learner",
        "authors": [
            "Jun-Hao Chen",
            "Samuel Yen-Chi Chen",
            "Yun-Cheng Tsai",
            "Chih-Shiang Shur"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Deep learning (DL) has been applied extensively in a wide range of fields.\nHowever, it has been shown that DL models are susceptible to a certain kinds of\nperturbations called \\emph{adversarial attacks}. To fully unlock the power of\nDL in critical fields such as financial trading, it is necessary to address\nsuch issues. In this paper, we present a method of constructing perturbed\nexamples and use these examples to boost the robustness of the model. Our\nalgorithm increases the stability of DL models for candlestick classification\nwith respect to perturbations in the input data.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.03686v1"
    },
    {
        "title": "Generating Realistic Stock Market Order Streams",
        "authors": [
            "Junyi Li",
            "Xitong Wang",
            "Yaoyang Lin",
            "Arunesh Sinha",
            "Micheal P. Wellman"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We propose an approach to generate realistic and high-fidelity stock market\ndata based on generative adversarial networks (GANs). Our Stock-GAN model\nemploys a conditional Wasserstein GAN to capture history dependence of orders.\nThe generator design includes specially crafted aspects including components\nthat approximate the market's auction mechanism, augmenting the order history\nwith order-book constructions to improve the generation task. We perform an\nablation study to verify the usefulness of aspects of our network structure. We\nprovide a mathematical characterization of distribution learned by the\ngenerator. We also propose statistics to measure the quality of generated\norders. We test our approach with synthetic and actual market data, compare to\nmany baseline generative models, and find the generated data to be close to\nreal data.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.04212v1"
    },
    {
        "title": "A Sentiment Analysis Approach to the Prediction of Market Volatility",
        "authors": [
            "Justina Deveikyte",
            "Helyette Geman",
            "Carlo Piccari",
            "Alessandro Provetti"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Prediction and quantification of future volatility and returns play an\nimportant role in financial modelling, both in portfolio optimization and risk\nmanagement. Natural language processing today allows to process news and social\nmedia comments to detect signals of investors' confidence. We have explored the\nrelationship between sentiment extracted from financial news and tweets and\nFTSE100 movements. We investigated the strength of the correlation between\nsentiment measures on a given day and market volatility and returns observed\nthe next day. The findings suggest that there is evidence of correlation\nbetween sentiment and stock market movements: the sentiment captured from news\nheadlines could be used as a signal to predict market returns; the same does\nnot apply for volatility. Also, in a surprising finding, for the sentiment\nfound in Twitter comments we obtained a correlation coefficient of -0.7, and\np-value below 0.05, which indicates a strong negative correlation between\npositive sentiment captured from the tweets on a given day and the volatility\nobserved the next day. We developed an accurate classifier for the prediction\nof market volatility in response to the arrival of new information by deploying\ntopic modelling, based on Latent Dirichlet Allocation, to extract feature\nvectors from a collection of tweets and financial news. The obtained features\nwere used as additional input to the classifier. Thanks to the combination of\nsentiment and topic modelling our classifier achieved a directional prediction\naccuracy for volatility of 63%.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.05906v1"
    },
    {
        "title": "On Technical Trading and Social Media Indicators in Cryptocurrencies'\n  Price Classification Through Deep Learning",
        "authors": [
            "Marco Ortu",
            "Nicola Uras",
            "Claudio Conversano",
            "Giuseppe Destefanis",
            "Silvia Bartolucci"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This work aims to analyse the predictability of price movements of\ncryptocurrencies on both hourly and daily data observed from January 2017 to\nJanuary 2021, using deep learning algorithms. For our experiments, we used\nthree sets of features: technical, trading and social media indicators,\nconsidering a restricted model of only technical indicators and an unrestricted\nmodel with technical, trading and social media indicators. We verified whether\nthe consideration of trading and social media indicators, along with the\nclassic technical variables (such as price's returns), leads to a significative\nimprovement in the prediction of cryptocurrencies price's changes. We conducted\nthe study on the two highest cryptocurrencies in volume and value (at the time\nof the study): Bitcoin and Ethereum. We implemented four different machine\nlearning algorithms typically used in time-series classification problems:\nMulti Layers Perceptron (MLP), Convolutional Neural Network (CNN), Long Short\nTerm Memory (LSTM) neural network and Attention Long Short Term Memory (ALSTM).\nWe devised the experiments using the advanced bootstrap technique to consider\nthe variance problem on test samples, which allowed us to evaluate a more\nreliable estimate of the model's performance. Furthermore, the Grid Search\ntechnique was used to find the best hyperparameters values for each implemented\nalgorithm. The study shows that, based on the hourly frequency results, the\nunrestricted model outperforms the restricted one. The addition of the trading\nindicators to the classic technical indicators improves the accuracy of Bitcoin\nand Ethereum price's changes prediction, with an increase of accuracy from a\nrange of 51-55% for the restricted model, to 67-84% for the unrestricted model.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.08189v2"
    },
    {
        "title": "Overnight GARCH-Itô Volatility Models",
        "authors": [
            "Donggyu Kim",
            "Minseok Shin",
            "Yazhen Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Various parametric volatility models for financial data have been developed\nto incorporate high-frequency realized volatilities and better capture market\ndynamics. However, because high-frequency trading data are not available during\nthe close-to-open period, the volatility models often ignore volatility\ninformation over the close-to-open period and thus may suffer from loss of\nimportant information relevant to market dynamics. In this paper, to account\nfor whole-day market dynamics, we propose an overnight volatility model based\non It\\^o diffusions to accommodate two different instantaneous volatility\nprocesses for the open-to-close and close-to-open periods. We develop a\nweighted least squares method to estimate model parameters for two different\nperiods and investigate its asymptotic properties. We conduct a simulation\nstudy to check the finite sample performance of the proposed model and method.\nFinally, we apply the proposed approaches to real trading data.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.13467v2"
    },
    {
        "title": "Forecasting high-frequency financial time series: an adaptive learning\n  approach with the order book data",
        "authors": [
            "Parley Ruogu Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper proposes a forecast-centric adaptive learning model that engages\nwith the past studies on the order book and high-frequency data, with\napplications to hypothesis testing. In line with the past literature, we\nproduce brackets of summaries of statistics from the high-frequency bid and ask\ndata in the CSI 300 Index Futures market and aim to forecast the one-step-ahead\nprices. Traditional time series issues, e.g. ARIMA order selection,\nstationarity, together with potential financial applications are covered in the\nexploratory data analysis, which pave paths to the adaptive learning model. By\ndesigning and running the learning model, we found it to perform well compared\nto the top fixed models, and some could improve the forecasting accuracy by\nbeing more stable and resilient to non-stationarity. Applications to hypothesis\ntesting are shown with a rolling window, and further potential applications to\nfinance and statistics are outlined.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.00264v1"
    },
    {
        "title": "Confronting Machine Learning With Financial Research",
        "authors": [
            "Kristof Lommers",
            "Ouns El Harzli",
            "Jack Kim"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This study aims to examine the challenges and applications of machine\nlearning for financial research. Machine learning algorithms have been\ndeveloped for certain data environments which substantially differ from the one\nwe encounter in finance. Not only do difficulties arise due to some of the\nidiosyncrasies of financial markets, there is a fundamental tension between the\nunderlying paradigm of machine learning and the research philosophy in\nfinancial economics. Given the peculiar features of financial markets and the\nempirical framework within social science, various adjustments have to be made\nto the conventional machine learning methodology. We discuss some of the main\nchallenges of machine learning in finance and examine how these could be\naccounted for. Despite some of the challenges, we argue that machine learning\ncould be unified with financial research to become a robust complement to the\neconometrician's toolbox. Moreover, we discuss the various applications of\nmachine learning in the research process such as estimation, empirical\ndiscovery, testing, causal inference and prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.00366v2"
    },
    {
        "title": "Scale matters: The daily, weekly and monthly volatility and\n  predictability of Bitcoin, Gold, and the S&P 500",
        "authors": [
            "Nassim Dehouche"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  A reputation of high volatility accompanies the emergence of Bitcoin as a\nfinancial asset. This paper intends to nuance this reputation and clarify our\nunderstanding of Bitcoin's volatility. Using daily, weekly, and monthly closing\nprices and log-returns data going from September 2014 to January 2021, we find\nthat Bitcoin is a prime example of an asset for which the two conceptions of\nvolatility diverge. We show that, historically, Bitcoin allies both high\nvolatility (high Standard Deviation) and high predictability (low Approximate\nEntropy), relative to Gold and S&P 500.\n  Moreover, using tools from Extreme Value Theory, we analyze the convergence\nof moments, and the mean excess functions of both the closing prices and the\nlog-returns of the three assets. We find that the closing price of Bitcoin is\nconsistent with a generalized Pareto distribution, when the closing prices of\nthe two other assets (Gold and S&P 500) present thin-tailed distributions.\nHowever, returns for all three assets are heavy tailed and second moments\n(variance, standard deviation) non-convergent. In the case of Bitcoin, lower\nsampling frequencies (monthly vs weekly, weekly vs daily) drastically reduce\nthe Kurtosis of log-returns and increase the convergence of empirical moments\nto their true value. The opposite effect is observed for Gold and S&P 500.\nThese properties suggest that Bitcoin's volatility is essentially an intra-day\nand intra-week phenomenon that is strongly attenuated on a weekly time-scale,\nand make it an attractive store of value to investors and speculators, but its\nhigh standard deviation excludes its use a currency.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.00395v1"
    },
    {
        "title": "Predicting the Behavior of Dealers in Over-The-Counter Corporate Bond\n  Markets",
        "authors": [
            "Yusen Lin",
            "Jinming Xue",
            "Louiqa Raschid"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Trading in Over-The-Counter (OTC) markets is facilitated by broker-dealers,\nin comparison to public exchanges, e.g., the New York Stock Exchange (NYSE).\nDealers play an important role in stabilizing prices and providing liquidity in\nOTC markets. We apply machine learning methods to model and predict the trading\nbehavior of OTC dealers for US corporate bonds. We create sequences of daily\nhistorical transaction reports for each dealer over a vocabulary of US\ncorporate bonds. Using this history of dealer activity, we predict the future\ntrading decisions of the dealer. We consider a range of neural network-based\nprediction models. We propose an extension, the Pointwise-Product ReZero (PPRZ)\nTransformer model, and demonstrate the improved performance of our model. We\nshow that individual history provides the best predictive model for the most\nactive dealers. For less active dealers, a collective model provides improved\nperformance. Further, clustering dealers based on their similarity can improve\nperformance. Finally, prediction accuracy varies based on the activity level of\nboth the bond and the dealer.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.09098v1"
    },
    {
        "title": "Democratization of Retail Trading: Can Reddit's WallStreetBets\n  Outperform Investment Bank Analysts?",
        "authors": [
            "Tolga Buz",
            "Gerard de Melo"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The recent hype around Reddit's WallStreetBets (WSB) community has inspired\nresearch on its impact on our economy and society. Still, one important\nquestion remains: Can WSB's community of anonymous contributors actually\nprovide valuable investment advice and possibly even outperform top financial\ninstitutions? We present a data-driven empirical study of investment\nrecommendations of WSB in comparison to recommendations made by leading\ninvestment banks, based on more than 1.6 million WSB posts published since\n2018. %enriched with stock market data. To this end, we extract and evaluate\ninvestment recommendations from WSB's raw text for all S&P 500 stocks and\ncompare their performance to more than 16,000 analyst recommendations from the\nlargest investment banks. While not all WSB recommendations prove profitable,\nour results show that they achieve average returns that compete with the best\nbanks and outperform them in certain cases. Furthermore, the WSB community has\nbeen better than almost all investment banks at detecting top-performing\nstocks. We conclude that WSB may indeed constitute a freely accessible,\nvaluable source of investment advice.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.00170v1"
    },
    {
        "title": "Leveraging Vision-Language Models for Granular Market Change Prediction",
        "authors": [
            "Christopher Wimmer",
            "Navid Rekabsaz"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Predicting future direction of stock markets using the historical data has\nbeen a fundamental component in financial forecasting. This historical data\ncontains the information of a stock in each specific time span, such as the\nopening, closing, lowest, and highest price. Leveraging this data, the future\ndirection of the market is commonly predicted using various time-series models\nsuch as Long-Short Term Memory networks. This work proposes modeling and\npredicting market movements with a fundamentally new approach, namely by\nutilizing image and byte-based number representation of the stock data\nprocessed with the recently introduced Vision-Language models. We conduct a\nlarge set of experiments on the hourly stock data of the German share index and\nevaluate various architectures on stock price prediction using historical stock\ndata. We conduct a comprehensive evaluation of the results with various metrics\nto accurately depict the actual performance of various approaches. Our\nevaluation results show that our novel approach based on representation of\nstock data as text (bytes) and image significantly outperforms strong deep\nlearning-based baselines.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.10166v1"
    },
    {
        "title": "Econophysics of a religious cult: the Antoinists in Belgium [1920-2000]",
        "authors": [
            "Marcel R. Ausloos"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  In the framework of applying econophysics ideas in religious topics, the\nfinances of the Antoinist religious movement organized in Belgium between 1920\nand 2000 are studied. The interest of investigating financial aspects of such\na, sometimes called, sect stems in finding characteristics of conditions and\nmechanisms under which definitely growth AND decay features of communities can\nbe understood. The legally reported yearly income and expenses between 1920 and\n2000 are studied. A three wave asymmetric regime is observed over a trend among\nmarked fluctuations at time of crises. The data analysis leads to propose a\ngeneral mechanistic model taking into account an average GDP growth, an\noscillatory monetary inflation and a logistic population drift.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4841v2"
    },
    {
        "title": "Heavy-tail driven by memory",
        "authors": [
            "Jongwook Kim",
            "Gabjin Oh"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We propose a stochastic process driven by memory effect with novel\ndistributions including both exponential and leptokurtic heavy-tailed\ndistributions. A class of distribution is analytically derived from the\ncontinuum limit of the discrete binary process with the renormalized\nauto-correlation and the closed form moment generating function is obtained,\nthus the cumulants are calculated and shown to be convergent. The other class\nof distributions are numerically investigated. The concoction of the two\nstochastic processes of the different signs of memory under regime switching\nmechanism does incarnate power-law decay behavior, which strongly implies that\nmemory is the alternative origin of heavy-tail.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.5690v4"
    },
    {
        "title": "A Random Matrix Approach to Dynamic Factors in macroeconomic data",
        "authors": [
            "Małgorzata Snarska"
        ],
        "category": "q-fin.ST",
        "published_year": "2012",
        "summary": "  We show how random matrix theory can be applied to develop new algorithms to\nextract dynamic factors from macroeconomic time series. In particular, we\nconsider a limit where the number of random variables N and the number of\nconsecutive time measurements T are large but the ratio N / T is fixed. In this\nregime the underlying random matrices are asymptotically equivalent to Free\nRandom Variables (FRV).Application of these methods for macroeconomic\nindicators for Poland economy is also presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.6544v1"
    },
    {
        "title": "Mutual Information Rate-Based Networks in Financial Markets",
        "authors": [
            "Paweł Fiedor"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In the last years efforts in econophysics have been shifted to study how\nnetwork theory can facilitate understanding of complex financial markets. Main\npart of these efforts is the study of correlation-based hierarchical networks.\nThis is somewhat surprising as the underlying assumptions of research looking\nat financial markets is that they behave chaotically. In fact it's common for\neconophysicists to estimate maximal Lyapunov exponent for log returns of a\ngiven financial asset to confirm that prices behave chaotically. Chaotic\nbehaviour is only displayed by dynamical systems which are either non-linear or\ninfinite-dimensional. Therefore it seems that non-linearity is an important\npart of financial markets, which is proved by numerous studies confirming\nfinancial markets display significant non-linear behaviour, yet network theory\nis used to study them using almost exclusively correlations and partial\ncorrelations, which are inherently dealing with linear dependencies only. In\nthis paper we introduce a way to incorporate non-linear dynamics and\ndependencies into hierarchical networks to study financial markets using mutual\ninformation and its dynamical extension: the mutual information rate. We\nestimate it using multidimensional Lempel-Ziv complexity and then convert it\ninto an Euclidean metric in order to find appropriate topological structure of\nnetworks modelling financial markets. We show that this approach leads to\ndifferent results than correlation-based approach used in most studies, on the\nbasis of 15 biggest companies listed on Warsaw Stock Exchange in the period of\n2009-2012 and 91 companies listed on NYSE100 between 2003 and 2013, using\nminimal spanning trees and planar maximally filtered graphs.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.2548v1"
    },
    {
        "title": "Multifractal Diffusion Entropy Analysis: Optimal Bin Width of\n  Probability Histograms",
        "authors": [
            "Petr Jizba",
            "Jan Korbel"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In the framework of Multifractal Diffusion Entropy Analysis we propose a\nmethod for choosing an optimal bin-width in histograms generated from\nunderlying probability distributions of interest. The method presented uses\ntechniques of R\\'{e}nyi's entropy and the mean squared error analysis to\ndiscuss the conditions under which the error in the multifractal spectrum\nestimation is minimal. We illustrate the utility of our approach by focusing on\na scaling behavior of financial time series. In particular, we analyze the\nS&P500 stock index as sampled at a daily rate in the time period 1950-2013. In\norder to demonstrate a strength of the method proposed we compare the\nmultifractal $\\delta$-spectrum for various bin-widths and show the robustness\nof the method, especially for large values of $q$. For such values, other\nmethods in use, e.g., those based on moment estimation, tend to fail for\nheavy-tailed data or data with long correlations. Connection between the\n$\\delta$-spectrum and R\\'{e}nyi's $q$ parameter is also discussed and\nelucidated on a simple example of multiscale time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.3316v2"
    },
    {
        "title": "Record statistics of financial time series and geometric random walks",
        "authors": [
            "Behlool Sabir",
            "M. S. Santhanam"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  The study of record statistics of correlated series is gaining momentum. In\nthis work, we study the records statistics of the time series of select stock\nmarket data and the geometric random walk, primarily through simulations. We\nshow that the distribution of the age of records is a power law with the\nexponent $\\alpha$ lying in the range $1.5 \\le \\alpha \\le 1.8$. Further, the\nlongest record ages follow the Fr\\'{e}chet distribution of extreme value\ntheory. The records statistics of geometric random walk series is in good\nagreement with that from the empirical stock data.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.3742v1"
    },
    {
        "title": "The Fourier estimation method with positive semi-definite estimators",
        "authors": [
            "Jirô Akahori",
            "Nien-Lin Liu",
            "Maria Elvira Mancino",
            "Yukie Yasuda"
        ],
        "category": "q-fin.ST",
        "published_year": "2014",
        "summary": "  In this paper we present a slight modification of the Fourier estimation\nmethod of the spot volatility (matrix) process of a continuous It\\^o\nsemimartingale where the estimators are always non-negative definite. Since the\nestimators are factorized, computational cost will be saved a lot.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.0112v1"
    },
    {
        "title": "Google matrix analysis of the multiproduct world trade network",
        "authors": [
            "Leonardo Ermann",
            "Dima L. Shepelyansky"
        ],
        "category": "q-fin.ST",
        "published_year": "2015",
        "summary": "  Using the United Nations COMTRADE database \\cite{comtrade} we construct the\nGoogle matrix $G$ of multiproduct world trade between the UN countries and\nanalyze the properties of trade flows on this network for years 1962 - 2010.\nThis construction, based on Markov chains, treats all countries on equal\ndemocratic grounds independently of their richness and at the same time it\nconsiders the contributions of trade products proportionally to their trade\nvolume. We consider the trade with 61 products for up to 227 countries. The\nobtained results show that the trade contribution of products is asymmetric:\nsome of them are export oriented while others are import oriented even if the\nranking by their trade volume is symmetric in respect to export and import\nafter averaging over all world countries. The construction of the Google matrix\nallows to investigate the sensitivity of trade balance in respect to price\nvariations of products, e.g. petroleum and gas, taking into account the world\nconnectivity of trade links. The trade balance based on PageRank and CheiRank\nprobabilities highlights the leading role of China and other BRICS countries in\nthe world trade in recent years. We also show that the eigenstates of $G$ with\nlarge eigenvalues select specific trade communities.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.03371v1"
    },
    {
        "title": "Local Parametric Estimation in High Frequency Data",
        "authors": [
            "Yoann Potiron",
            "Per Mykland"
        ],
        "category": "q-fin.ST",
        "published_year": "2016",
        "summary": "  In this paper, we give a general time-varying parameter model, where the\nmultidimensional parameter possibly includes jumps. The quantity of interest is\ndefined as the integrated value over time of the parameter process $\\Theta =\nT^{-1} \\int_0^T \\theta_t^* dt$. We provide a local parametric estimator (LPE)\nof $\\Theta$ and conditions under which we can show the central limit theorem.\nRoughly speaking those conditions correspond to some uniform limit theory in\nthe parametric version of the problem. The framework is restricted to the\nspecific convergence rate $n^{1/2}$. Several examples of LPE are studied:\nestimation of volatility, powers of volatility, volatility when incorporating\ntrading information and time-varying MA(1).\n",
        "pdf_link": "http://arxiv.org/pdf/1603.05700v4"
    },
    {
        "title": "Enhancing Stock Market Prediction with Extended Coupled Hidden Markov\n  Model over Multi-Sourced Data",
        "authors": [
            "Xi Zhang",
            "Yixuan Li",
            "Senzhang Wang",
            "Binxing Fang",
            "Philip S. Yu"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Traditional stock market prediction methods commonly only utilize the\nhistorical trading data, ignoring the fact that stock market fluctuations can\nbe impacted by various other information sources such as stock related events.\nAlthough some recent works propose event-driven prediction approaches by\nconsidering the event data, how to leverage the joint impacts of multiple data\nsources still remains an open research problem. In this work, we study how to\nexplore multiple data sources to improve the performance of the stock\nprediction. We introduce an Extended Coupled Hidden Markov Model incorporating\nthe news events with the historical trading data. To address the data sparsity\nissue of news events for each single stock, we further study the fluctuation\ncorrelations between the stocks and incorporate the correlations into the model\nto facilitate the prediction task. Evaluations on China A-share market data in\n2016 show the superior performance of our model against previous methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.00306v1"
    },
    {
        "title": "Superstatistics with cut-off tails for financial time series",
        "authors": [
            "Yusuke Uchiyama",
            "Takanori Kadoya"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  Financial time series have been investigated to follow fat-tailed\ndistributions. Further, an empirical probability distribution sometimes shows\ncut-off shapes on its tails. To describe this stylized fact, we incorporate the\ncut-off effect in superstatistics. Then we confirm that the presented\nstochastic model is capable of describing the statistical properties of real\nfinancial time series. In addition, we present an option pricing formula with\nrespect to superstatistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04775v1"
    },
    {
        "title": "Dynamical variety of shapes in financial multifractality",
        "authors": [
            "Stanisław Drożdż",
            "Rafał Kowalski",
            "Paweł Oświȩcimka",
            "Rafał Rak",
            "Robert Gȩbarowski"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  The concept of multifractality offers a powerful formal tool to filter out\nmultitude of the most relevant characteristics of complex time series. The\nrelated studies thus far presented in the scientific literature typically limit\nthemselves to evaluation of whether or not a time series is multifractal and\nwidth of the resulting singularity spectrum is considered a measure of the\ndegree of complexity involved. However, the character of the complexity of time\nseries generated by the natural processes usually appears much more intricate\nthan such a bare statement can reflect. As an example, based on the long-term\nrecords of S&P500 and NASDAQ - the two world leading stock market indices - the\npresent study shows that they indeed develop the multifractal features, but\nthese features evolve through a variety of shapes, most often strongly\nasymmetric, whose changes typically are correlated with the historically most\nsignificant events experienced by the world economy. Relating at the same time\nthe index multifractal singularity spectra to those of the component stocks\nthat form this index reflects the varying degree of correlations involved among\nthe stocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.06728v1"
    },
    {
        "title": "State-dependent Hawkes processes and their application to limit order\n  book modelling",
        "authors": [
            "Maxime Morariu-Patrichi",
            "Mikko S. Pakkanen"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We study statistical aspects of state-dependent Hawkes processes, which are\nan extension of Hawkes processes where a self- and cross-exciting counting\nprocess and a state process are fully coupled, interacting with each other. The\nexcitation kernel of the counting process depends on the state process that,\nreciprocally, switches state when there is an event in the counting process. We\nfirst establish the existence and uniqueness of state-dependent Hawkes\nprocesses and explain how they can be simulated. Then we develop maximum\nlikelihood estimation methodology for parametric specifications of the process.\nWe apply state-dependent Hawkes processes to high-frequency limit order book\ndata, allowing us to build a novel model that captures the feedback loop\nbetween the order flow and the shape of the limit order book. We estimate two\nspecifications of the model, using the bid-ask spread and the queue imbalance\nas state variables, and find that excitation effects in the order flow are\nstrongly state-dependent. Additionally, we find that the endogeneity of the\norder flow, measured by the magnitude of excitation, is also state-dependent,\nbeing more pronounced in disequilibrium states of the limit order book.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08060v3"
    },
    {
        "title": "Multi-channel discourse as an indicator for Bitcoin price and volume\n  movements",
        "authors": [
            "Marvin Aron Kennis"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  This research aims to identify how Bitcoin-related news publications and\nonline discourse are expressed in Bitcoin exchange movements of price and\nvolume. Being inherently digital, all Bitcoin-related fundamental data (from\nexchanges, as well as transactional data directly from the blockchain) is\navailable online, something that is not true for traditional businesses or\ncurrencies traded on exchanges. This makes Bitcoin an interesting subject for\nsuch research, as it enables the mapping of sentiment to fundamental events\nthat might otherwise be inaccessible. Furthermore, Bitcoin discussion largely\ntakes place on online forums and chat channels. In stock trading, the value of\nsentiment data in trading decisions has been demonstrated numerous times [1]\n[2] [3], and this research aims to determine whether there is value in such\ndata for Bitcoin trading models. To achieve this, data over the year 2015 has\nbeen collected from Bitcointalk.org, (the biggest Bitcoin forum in post\nvolume), established news sources such as Bloomberg and the Wall Street\nJournal, the complete /r/btc and /r/Bitcoin subreddits, and the bitcoin-otc and\nbitcoin-dev IRC channels. By analyzing this data on sentiment and volume, we\nfind weak to moderate correlations between forum, news, and Reddit sentiment\nand movements in price and volume from 1 to 5 days after the sentiment was\nexpressed. A Granger causality test confirms the predictive causality of the\nsentiment on the daily percentage price and volume movements, and at the same\ntime underscores the predictive causality of market movements on sentiment\nexpressions in online communities\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03146v1"
    },
    {
        "title": "Technical Analysis and Discrete False Discovery Rate: Evidence from MSCI\n  Indices",
        "authors": [
            "Georgios Sermpinis",
            "Arman Hassanniakalager",
            "Charalampos Stasinakis",
            "Ioannis Psaradellis"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  We investigate the performance of dynamic portfolios constructed using more\nthan 21,000 technical trading rules on 12 categorical and country-specific\nmarkets over the 2004-2015 study period, on rolling forward structures of\ndifferent lengths. We also introduce a discrete false discovery rate (DFRD+/-)\nmethod for controlling data snooping bias. Compared to the existing methods,\nDFRD+/- is adaptive and more powerful, and accommodates for discrete p-values.\nThe profitability, persistence and robustness of the technical rules are\nexamined. Technical analysis still has short-term value in advanced, emerging\nand frontier markets. Financial stress, the economic environment and market\ndevelopment seem to affect the performance of trading rules. A cross-validation\nexercise highlights the importance of frequent rebalancing and the variability\nof profitability in trading with technical analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.06766v2"
    },
    {
        "title": "Kalman filter demystified: from intuition to probabilistic graphical\n  model to real case in financial markets",
        "authors": [
            "Eric Benhamou"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In this paper, we revisit the Kalman filter theory. After giving the\nintuition on a simplified financial markets example, we revisit the maths\nunderlying it. We then show that Kalman filter can be presented in a very\ndifferent fashion using graphical models. This enables us to establish the\nconnection between Kalman filter and Hidden Markov Models. We then look at\ntheir application in financial markets and provide various intuitions in terms\nof their applicability for complex systems such as financial markets. Although\nthis paper has been written more like a self contained work connecting Kalman\nfilter to Hidden Markov Models and hence revisiting well known and establish\nresults, it contains new results and brings additional contributions to the\nfield. First, leveraging on the link between Kalman filter and HMM, it gives\nnew algorithms for inference for extended Kalman filters. Second, it presents\nan alternative to the traditional estimation of parameters using EM algorithm\nthanks to the usage of CMA-ES optimization. Third, it examines the application\nof Kalman filter and its Hidden Markov models version to financial markets,\nproviding various dynamics assumptions and tests. We conclude by connecting\nKalman filter approach to trend following technical analysis system and showing\ntheir superior performances for trend following detection.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11618v2"
    },
    {
        "title": "Improved Forecasting of Cryptocurrency Price using Social Signals",
        "authors": [
            "Maria Glenski",
            "Tim Weninger",
            "Svitlana Volkova"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Social media signals have been successfully used to develop large-scale\npredictive and anticipatory analytics. For example, forecasting stock market\nprices and influenza outbreaks. Recently, social data has been explored to\nforecast price fluctuations of cryptocurrencies, which are a novel disruptive\ntechnology with significant political and economic implications. In this paper\nwe leverage and contrast the predictive power of social signals, specifically\nuser behavior and communication patterns, from multiple social platforms GitHub\nand Reddit to forecast prices for three cyptocurrencies with high developer and\ncommunity interest - Bitcoin, Ethereum, and Monero. We evaluate the performance\nof neural network models that rely on long short-term memory units (LSTMs)\ntrained on historical price data and social data against price only LSTMs and\nbaseline autoregressive integrated moving average (ARIMA) models, commonly used\nto predict stock prices. Our results not only demonstrate that social signals\nreduce error when forecasting daily coin price, but also show that the language\nused in comments within the official communities on Reddit (r/Bitcoin,\nr/Ethereum, and r/Monero) are the best predictors overall. We observe that\nmodels are more accurate in forecasting price one day ahead for Bitcoin (4%\nroot mean squared percent error) compared to Ethereum (7%) and Monero (8%).\n",
        "pdf_link": "http://arxiv.org/pdf/1907.00558v1"
    },
    {
        "title": "Financial Time Series Data Processing for Machine Learning",
        "authors": [
            "Fabrice Daniel"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  This article studies the financial time series data processing for machine\nlearning. It introduces the most frequent scaling methods, then compares the\nresulting stationarity and preservation of useful information for trend\nforecasting. It proposes an empirical test based on the capability to learn\nsimple data relationship with simple models. It also speaks about the data\nsplit method specific to time series, avoiding unwanted overfitting and\nproposes various labelling for classification and regression.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.03010v1"
    },
    {
        "title": "Maximum Entropy approach to multivariate time series randomization",
        "authors": [
            "Riccardo Marcaccioli",
            "Giacomo Livan"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Natural and social multivariate systems are commonly studied through sets of\nsimultaneous and time-spaced measurements of the observables that drive their\ndynamics, i.e., through sets of time series. Typically, this is done via\nhypothesis testing: the statistical properties of the empirical time series are\ntested against those expected under a suitable null hypothesis. This is a very\nchallenging task in complex interacting systems, where statistical stability is\noften poor due to lack of stationarity and ergodicity. Here, we describe an\nunsupervised, data-driven framework to perform hypothesis testing in such\nsituations. This consists of a statistical mechanical approach - analogous to\nthe configuration model for networked systems - for ensembles of time series\ndesigned to preserve, on average, some of the statistical properties observed\non an empirical set of time series. We showcase its possible applications with\na case study on financial portfolio selection.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04925v4"
    },
    {
        "title": "Dreaming machine learning: Lipschitz extensions for reinforcement\n  learning on financial markets",
        "authors": [
            "J. M. Calabuig",
            "H. Falciani",
            "E. A. Sánchez-Pérez"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  We consider a quasi-metric topological structure for the construction of a\nnew reinforcement learning model in the framework of financial markets. It is\nbased on a Lipschitz type extension of reward functions defined in metric\nspaces. Specifically, the McShane and Whitney extensions are considered for a\nreward function which is defined by the total evaluation of the benefits\nproduced by the investment decision at a given time. We define the metric as a\nlinear combination of a Euclidean distance and an angular metric component. All\ninformation about the evolution of the system from the beginning of the time\ninterval is used to support the extension of the reward function, but in\naddition this data set is enriched by adding some artificially produced states.\nThus, the main novelty of our method is the way we produce more states -- which\nwe call \"dreams\" -- to enrich learning. Using some known states of the\ndynamical system that represents the evolution of the financial market, we use\nour technique to simulate new states by interpolating real states and\nintroducing some random variables. These new states are used to feed a learning\nalgorithm designed to improve the investment strategy by following a typical\nreinforcement learning scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.05697v2"
    },
    {
        "title": "Mid-price Prediction Based on Machine Learning Methods with Technical\n  and Quantitative Indicators",
        "authors": [
            "Adamantios Ntakaris",
            "Juho Kanniainen",
            "Moncef Gabbouj",
            "Alexandros Iosifidis"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Stock price prediction is a challenging task, but machine learning methods\nhave recently been used successfully for this purpose. In this paper, we\nextract over 270 hand-crafted features (factors) inspired by technical and\nquantitative analysis and tested their validity on short-term mid-price\nmovement prediction. We focus on a wrapper feature selection method using\nentropy, least-mean squares, and linear discriminant analysis. We also build a\nnew quantitative feature based on adaptive logistic regression for online\nlearning, which is constantly selected first among the majority of the proposed\nfeature selection methods. This study examines the best combination of features\nusing high frequency limit order book data from Nasdaq Nordic. Our results\nsuggest that sorting methods and classifiers can be used in such a way that one\ncan reach the best performance with a combination of only very few advanced\nhand-crafted features.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.09452v1"
    },
    {
        "title": "A Robust Transferable Deep Learning Framework for Cross-sectional\n  Investment Strategy",
        "authors": [
            "Kei Nakagawa",
            "Masaya Abe",
            "Junpei Komiyama"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Stock return predictability is an important research theme as it reflects our\neconomic and social organization, and significant efforts are made to explain\nthe dynamism therein. Statistics of strong explanative power, called \"factor\"\nhave been proposed to summarize the essence of predictive stock returns.\nAlthough machine learning methods are increasingly popular in stock return\nprediction, an inference of the stock returns is highly elusive, and still most\ninvestors, if partly, rely on their intuition to build a better decision\nmaking. The challenge here is to make an investment strategy that is consistent\nover a reasonably long period, with the minimum human decision on the entire\nprocess. To this end, we propose a new stock return prediction framework that\nwe call Ranked Information Coefficient Neural Network (RIC-NN). RIC-NN is a\ndeep learning approach and includes the following three novel ideas: (1)\nnonlinear multi-factor approach, (2) stopping criteria with ranked information\ncoefficient (rank IC), and (3) deep transfer learning among multiple regions.\nExperimental comparison with the stocks in the Morgan Stanley Capital\nInternational (MSCI) indices shows that RIC-NN outperforms not only\noff-the-shelf machine learning methods but also the average return of major\nequity investment funds in the last fourteen years.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.01491v1"
    },
    {
        "title": "Entropic Dynamic Time Warping Kernels for Co-evolving Financial Time\n  Series Analysis",
        "authors": [
            "Lu Bai",
            "Lixin Cui",
            "Lixiang Xu",
            "Yue Wang",
            "Zhihong Zhang",
            "Edwin R. Hancock"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  In this work, we develop a novel framework to measure the similarity between\ndynamic financial networks, i.e., time-varying financial networks.\nParticularly, we explore whether the proposed similarity measure can be\nemployed to understand the structural evolution of the financial networks with\ntime. For a set of time-varying financial networks with each vertex\nrepresenting the individual time series of a different stock and each edge\nbetween a pair of time series representing the absolute value of their Pearson\ncorrelation, our start point is to compute the commute time matrix associated\nwith the weighted adjacency matrix of the network structures, where each\nelement of the matrix can be seen as the enhanced correlation value between\npairwise stocks. For each network, we show how the commute time matrix allows\nus to identify a reliable set of dominant correlated time series as well as an\nassociated dominant probability distribution of the stock belonging to this\nset. Furthermore, we represent each original network as a discrete dominant\nShannon entropy time series computed from the dominant probability\ndistribution. With the dominant entropy time series for each pair of financial\nnetworks to hand, we develop a similarity measure based on the classical\ndynamic time warping framework, for analyzing the financial time-varying\nnetworks. We show that the proposed similarity measure is positive definite and\nthus corresponds to a kernel measure on graphs. The proposed kernel bridges the\ngap between graph kernels and the classical dynamic time warping framework for\nmultiple financial time series analysis. Experiments on time-varying networks\nextracted through New York Stock Exchange (NYSE) database demonstrate the\neffectiveness of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09153v1"
    },
    {
        "title": "Inference of Binary Regime Models with Jump Discontinuities",
        "authors": [
            "Milan Kumar Das",
            "Anindya Goswami",
            "Sharan Rajani"
        ],
        "category": "q-fin.ST",
        "published_year": "2019",
        "summary": "  Identifying the instances of jumps in a discrete-time-series sample of a jump\ndiffusion model is a challenging task. We have developed a novel statistical\ntechnique for jump detection and volatility estimation in a return time series\ndata using a threshold method. The consistency of the volatility estimator has\nbeen obtained. Since we have derived the threshold and the volatility estimator\nsimultaneously by solving an implicit equation, we have obtained unprecedented\naccuracy across a wide range of parameter values. Using this method, the\nincrements attributed to jumps have been removed from a large collection of\nhistorical data of Indian sectorial indices. Subsequently, we have tested the\npresence of regime-switching dynamics in the volatility coefficient using a new\ndiscriminating statistic. The statistic has been shown to be sensitive to the\ntransition kernel of the regime-switching model. We perform the testing using\nthe Bootstrap method and find a clear indication of presence of multiple\nregimes of volatility in the data. A link to all Python codes is given in the\nconclusion. The methodology is suitable for analyzing high-frequency data and\nmay be applied for algorithmic trading.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.10606v4"
    },
    {
        "title": "Deep Probabilistic Modelling of Price Movements for High-Frequency\n  Trading",
        "authors": [
            "Ye-Sheen Lim",
            "Denise Gorse"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper we propose a deep recurrent architecture for the probabilistic\nmodelling of high-frequency market prices, important for the risk management of\nautomated trading systems. Our proposed architecture incorporates probabilistic\nmixture models into deep recurrent neural networks. The resulting deep mixture\nmodels simultaneously address several practical challenges important in the\ndevelopment of automated high-frequency trading strategies that were previously\nneglected in the literature: 1) probabilistic forecasting of the price\nmovements; 2) single objective prediction of both the direction and size of the\nprice movements. We train our models on high-frequency Bitcoin market data and\nevaluate them against benchmark models obtained from the literature. We show\nthat our model outperforms the benchmark models in both a metric-based test and\nin a simulated trading scenario\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01498v1"
    },
    {
        "title": "Deep Recurrent Modelling of Stationary Bitcoin Price Formation Using the\n  Order Flow",
        "authors": [
            "Ye-Sheen Lim",
            "Denise Gorse"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this paper we propose a deep recurrent model based on the order flow for\nthe stationary modelling of the high-frequency directional prices movements.\nThe order flow is the microsecond stream of orders arriving at the exchange,\ndriving the formation of prices seen on the price chart of a stock or currency.\nTo test the stationarity of our proposed model we train our model on data\nbefore the 2017 Bitcoin bubble period and test our model during and after the\nbubble. We show that without any retraining, the proposed model is temporally\nstable even as Bitcoin trading shifts into an extremely volatile \"bubble\ntrouble\" period. The significance of the result is shown by benchmarking\nagainst existing state-of-the-art models in the literature for modelling price\nformation using deep learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01499v1"
    },
    {
        "title": "Machine Learning Algorithms for Financial Asset Price Forecasting",
        "authors": [
            "Philip Ndikum"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This research paper explores the performance of Machine Learning (ML)\nalgorithms and techniques that can be used for financial asset price\nforecasting. The prediction and forecasting of asset prices and returns remains\none of the most challenging and exciting problems for quantitative finance and\npractitioners alike. The massive increase in data generated and captured in\nrecent years presents an opportunity to leverage Machine Learning algorithms.\nThis study directly compares and contrasts state-of-the-art implementations of\nmodern Machine Learning algorithms on high performance computing (HPC)\ninfrastructures versus the traditional and highly popular Capital Asset Pricing\nModel (CAPM) on U.S equities data. The implemented Machine Learning models -\ntrained on time series data for an entire stock universe (in addition to\nexogenous macroeconomic variables) significantly outperform the CAPM on\nout-of-sample (OOS) test data.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01504v1"
    },
    {
        "title": "Bayesian Consensus: Consensus Estimates from Miscalibrated Instruments\n  under Heteroscedastic Noise",
        "authors": [
            "Chirag Nagpal",
            "Robert E. Tillman",
            "Prashant Reddy",
            "Manuela Veloso"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We consider the problem of aggregating predictions or measurements from a set\nof human forecasters, models, sensors or other instruments which may be subject\nto bias or miscalibration and random heteroscedastic noise. We propose a\nBayesian consensus estimator that adjusts for miscalibration and noise and show\nthat this estimator is unbiased and asymptotically more efficient than naive\nalternatives. We further propose a Hierarchical Bayesian Model that leverages\nour proposed estimator and apply it to two real world forecasting challenges\nthat require consensus estimates from error prone individual estimates:\nforecasting influenza like illness (ILI) weekly percentages and forecasting\nannual earnings of public companies. We demonstrate that our approach is\neffective at mitigating bias and error and results in more accurate forecasts\nthan existing consensus models.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06565v2"
    },
    {
        "title": "From code to market: Network of developers and correlated returns of\n  cryptocurrencies",
        "authors": [
            "Lorenzo Lucchini",
            "Laura Alessandretti",
            "Bruno Lepri",
            "Angela Gallo",
            "Andrea Baronchelli"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  \"Code is law\" is the funding principle of cryptocurrencies. The security,\ntransferability, availability and other properties of a crypto-asset are\ndetermined by the code through which it is created. If code is open source, as\nit happens for most cryptocurrencies, this principle would prevent\nmanipulations and grant transparency to users and traders. However, this\napproach considers cryptocurrencies as isolated entities thus neglecting\npossible connections between them. Here, we show that 4% of developers\ncontribute to the code of more than one cryptocurrency and that the market\nreflects these cross-asset dependencies. In particular, we reveal that the\nfirst coding event linking two cryptocurrencies through a common developer\nleads to the synchronisation of their returns in the following months. Our\nresults identify a clear link between the collaborative development of\ncryptocurrencies and their market behaviour. More broadly, our work reveals a\nso-far overlooked systemic dimension for the transparency of code-based\necosystems and we anticipate it will be of interest to researchers, investors\nand regulators.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.07290v2"
    },
    {
        "title": "Long memory in select stock returns using an alternative wavelet\n  log-scale alignment approach",
        "authors": [
            "Avishek Bhandari",
            "Bandi Kamaiah"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This study investigates the efficiency of some select stock markets. Using an\nimproved wavelet estimator of long range dependence, we show evidence of long\nmemory in the stock returns of some emerging Asian economies. However,\ndeveloped markets of Europe and the United States did not exhibit long memory\nthereby confirming the efficiency of developed stock markets. On the other\nhand, emerging Asian markets are found to be less efficient as long memory is\nmore pronounced in these markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.08550v1"
    },
    {
        "title": "A Time Series Analysis-Based Stock Price Prediction Using Machine\n  Learning and Deep Learning Models",
        "authors": [
            "Sidra Mehtab",
            "Jaydip Sen"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Prediction of future movement of stock prices has always been a challenging\ntask for the researchers. While the advocates of the efficient market\nhypothesis (EMH) believe that it is impossible to design any predictive\nframework that can accurately predict the movement of stock prices, there are\nseminal work in the literature that have clearly demonstrated that the\nseemingly random movement patterns in the time series of a stock price can be\npredicted with a high level of accuracy. Design of such predictive models\nrequires choice of appropriate variables, right transformation methods of the\nvariables, and tuning of the parameters of the models. In this work, we present\na very robust and accurate framework of stock price prediction that consists of\nan agglomeration of statistical, machine learning and deep learning models. We\nuse the daily stock price data, collected at five minutes interval of time, of\na very well known company that is listed in the National Stock Exchange (NSE)\nof India. The granular data is aggregated into three slots in a day, and the\naggregated data is used for building and training the forecasting models. We\ncontend that the agglomerative approach of model building that uses a\ncombination of statistical, machine learning, and deep learning approaches, can\nvery effectively learn from the volatile and random movement patterns in a\nstock price data. We build eight classification and eight regression models\nbased on statistical and machine learning approaches. In addition to these\nmodels, a deep learning regression model using a long-and-short-term memory\n(LSTM) network is also built. Extensive results have been presented on the\nperformance of these models, and the results are critically analyzed.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11697v2"
    },
    {
        "title": "Uncertainty-Aware Lookahead Factor Models for Quantitative Investing",
        "authors": [
            "Lakshay Chauhan",
            "John Alberg",
            "Zachary C. Lipton"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  On a periodic basis, publicly traded companies report fundamentals, financial\ndata including revenue, earnings, debt, among others. Quantitative finance\nresearch has identified several factors, functions of the reported data that\nhistorically correlate with stock market performance. In this paper, we first\nshow through simulation that if we could select stocks via factors calculated\non future fundamentals (via oracle), that our portfolios would far outperform\nstandard factor models. Motivated by this insight, we train deep nets to\nforecast future fundamentals from a trailing 5-year history. We propose\nlookahead factor models which plug these predicted future fundamentals into\ntraditional factors. Finally, we incorporate uncertainty estimates from both\nneural heteroscedastic regression and a dropout-based heuristic, improving\nperformance by adjusting our portfolios to avert risk. In retrospective\nanalysis, we leverage an industry-grade portfolio simulator (backtester) to\nshow simultaneous improvement in annualized return and Sharpe ratio.\nSpecifically, the simulated annualized return for the uncertainty-aware model\nis 17.7% (vs 14.0% for a standard factor model) and the Sharpe ratio is 0.84\n(vs 0.52).\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04082v2"
    },
    {
        "title": "Modelling time-varying interactions in complex systems: the Score Driven\n  Kinetic Ising Model",
        "authors": [
            "Carlo Campajola",
            "Domenico Di Gangi",
            "Fabrizio Lillo",
            "Daniele Tantari"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  A common issue when analyzing real-world complex systems is that the\ninteractions between the elements often change over time: this makes it\ndifficult to find optimal models that describe this evolution and that can be\nestimated from data, particularly when the driving mechanisms are not known.\nHere we offer a new perspective on the development of models for time-varying\ninteractions introducing a generalization of the well-known Kinetic Ising Model\n(KIM), a minimalistic pairwise constant interactions model which has found\napplications in multiple scientific disciplines. Keeping arbitrary choices of\ndynamics to a minimum and seeking information theoretical optimality, the\nScore-Driven methodology lets us significantly increase the knowledge that can\nbe extracted from data using the simple KIM. In particular, we first identify a\nparameter whose value at a given time can be directly associated with the local\npredictability of the dynamics. Then we introduce a method to dynamically learn\nthe value of such parameter from the data, without the need of specifying\nparametrically its dynamics. Finally, we extend our framework to disentangle\ndifferent sources (e.g. endogenous vs exogenous) of predictability in real\ntime.\n  We apply our methodology to several complex systems including financial\nmarkets, temporal (social) networks, and neuronal populations. Our results show\nthat the Score-Driven KIM produces insightful descriptions of the systems,\nallowing to predict forecasting accuracy in real time as well as to separate\ndifferent components of the dynamics. This provides a significant\nmethodological improvement for data analysis in a wide range of disciplines.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.15545v2"
    },
    {
        "title": "Stock2Vec: A Hybrid Deep Learning Framework for Stock Market Prediction\n  with Representation Learning and Temporal Convolutional Network",
        "authors": [
            "Xing Wang",
            "Yijun Wang",
            "Bin Weng",
            "Aleksandr Vinel"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We have proposed to develop a global hybrid deep learning framework to\npredict the daily prices in the stock market. With representation learning, we\nderived an embedding called Stock2Vec, which gives us insight for the\nrelationship among different stocks, while the temporal convolutional layers\nare used for automatically capturing effective temporal patterns both within\nand across series. Evaluated on S&P 500, our hybrid framework integrates both\nadvantages and achieves better performance on the stock price prediction task\nthan several popular benchmarked models.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.01197v1"
    },
    {
        "title": "A GMM approach to estimate the roughness of stochastic volatility",
        "authors": [
            "Anine E. Bolko",
            "Kim Christensen",
            "Mikko S. Pakkanen",
            "Bezirgen Veliyev"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We develop a GMM approach for estimation of log-normal stochastic volatility\nmodels driven by a fractional Brownian motion with unrestricted Hurst exponent.\nWe show that a parameter estimator based on the integrated variance is\nconsistent and, under stronger conditions, asymptotically normally distributed.\nWe inspect the behavior of our procedure when integrated variance is replaced\nwith a noisy measure of volatility calculated from discrete high-frequency\ndata. The realized estimator contains sampling error, which skews the fractal\ncoefficient toward \"illusive roughness.\" We construct an analytical approach to\ncontrol the impact of measurement error without introducing nuisance\nparameters. In a simulation study, we demonstrate convincing small sample\nproperties of our approach based both on integrated and realized variance over\nthe entire memory spectrum. We show the bias correction attenuates any\nsystematic deviance in the parameter estimates. Our procedure is applied to\nempirical high-frequency data from numerous leading equity indexes. With our\nrobust approach the Hurst index is estimated around 0.05, confirming roughness\nin stochastic volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04610v4"
    },
    {
        "title": "Choosing News Topics to Explain Stock Market Returns",
        "authors": [
            "Paul Glasserman",
            "Kriste Krstovski",
            "Paul Laliberte",
            "Harry Mamaysky"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We analyze methods for selecting topics in news articles to explain stock\nreturns. We find, through empirical and theoretical results, that supervised\nLatent Dirichlet Allocation (sLDA) implemented through Gibbs sampling in a\nstochastic EM algorithm will often overfit returns to the detriment of the\ntopic model. We obtain better out-of-sample performance through a random search\nof plain LDA models. A branching procedure that reinforces effective topic\nassignments often performs best. We test methods on an archive of over 90,000\nnews articles about S&P 500 firms.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.07289v1"
    },
    {
        "title": "A Deep Learning Framework for Predicting Digital Asset Price Movement\n  from Trade-by-trade Data",
        "authors": [
            "Qi Zhao"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  This paper presents a deep learning framework based on Long Short-term Memory\nNetwork(LSTM) that predicts price movement of cryptocurrencies from\ntrade-by-trade data. The main focus of this study is on predicting short-term\nprice changes in a fixed time horizon from a looking back period. By carefully\ndesigning features and detailed searching for best hyper-parameters, the model\nis trained to achieve high performance on nearly a year of trade-by-trade data.\nThe optimal model delivers stable high performance(over 60% accuracy) on\nout-of-sample test periods. In a realistic trading simulation setting, the\nprediction made by the model could be easily monetized. Moreover, this study\nshows that the LSTM model could extract universal features from trade-by-trade\ndata, as the learned parameters well maintain their high performance on other\ncryptocurrency instruments that were not included in training data. This study\nexceeds existing researches in term of the scale and precision of data used, as\nwell as the high prediction accuracy achieved.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.07404v1"
    },
    {
        "title": "Hybrid Modelling Approaches for Forecasting Energy Spot Prices in EPEC\n  market",
        "authors": [
            "Tahir Miriyev",
            "Alessandro Contu",
            "Kevin Schafers",
            "Ion Gabriel Ion"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  In this work we considered several hybrid modelling approaches for\nforecasting energy spot prices in EPEC market. Hybridization is performed\nthrough combining a Naive model, Fourier analysis, ARMA and GARCH models, a\nmean-reversion and jump-diffusion model, and Recurrent Neural Networks (RNN).\nTraining data was given in terms of electricity prices for 2013-2014 years, and\ntest data as a year of 2015.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08400v1"
    },
    {
        "title": "On the impact of publicly available news and information transfer to\n  financial markets",
        "authors": [
            "Metod Jazbec",
            "Barna Pásztor",
            "Felix Faltings",
            "Nino Antulov-Fantulin",
            "Petter N. Kolm"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We quantify the propagation and absorption of large-scale publicly available\nnews articles from the World Wide Web to financial markets. To extract publicly\navailable information, we use the news archives from the Common Crawl, a\nnonprofit organization that crawls a large part of the web. We develop a\nprocessing pipeline to identify news articles associated with the constituent\ncompanies in the S\\&P 500 index, an equity market index that measures the stock\nperformance of U.S. companies. Using machine learning techniques, we extract\nsentiment scores from the Common Crawl News data and employ tools from\ninformation theory to quantify the information transfer from public news\narticles to the U.S. stock market. Furthermore, we analyze and quantify the\neconomic significance of the news-based information with a simple\nsentiment-based portfolio trading strategy. Our findings provides support for\nthat information in publicly available news on the World Wide Web has a\nstatistically and economically significant impact on events in financial\nmarkets.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12002v1"
    },
    {
        "title": "Multiscale characteristics of the emerging global cryptocurrency market",
        "authors": [
            "Marcin Wątorek",
            "Stanisław Drożdż",
            "Jarosław Kwapień",
            "Ludovico Minati",
            "Paweł Oświęcimka",
            "Marek Stanuszek"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  The review introduces the history of cryptocurrencies, offering a description\nof the blockchain technology behind them. Differences between cryptocurrencies\nand the exchanges on which they are traded have been shown. The central part\nsurveys the analysis of cryptocurrency price changes on various platforms. The\nstatistical properties of the fluctuations in the cryptocurrency market have\nbeen compared to the traditional markets. With the help of the latest\nstatistical physics methods the non-linear correlations and multiscale\ncharacteristics of the cryptocurrency market are analyzed. In the last part the\nco-evolution of the correlation structure among the 100 cryptocurrencies having\nthe largest capitalization is retraced. The detailed topology of cryptocurrency\nnetwork on the Binance platform from bitcoin perspective is also considered.\nFinally, an interesting observation on the Covid-19 pandemic impact on the\ncryptocurrency market is presented and discussed: recently we have witnessed a\n\"phase transition\" of the cryptocurrencies from being a hedge opportunity for\nthe investors fleeing the traditional markets to become a part of the global\nmarket that is substantially coupled to the traditional financial instruments\nlike the currencies, stocks, and commodities.\n  The main contribution is an extensive demonstration that structural\nself-organization in the cryptocurrency markets has caused the same to attain\ncomplexity characteristics that are nearly indistinguishable from the Forex\nmarket at the level of individual time-series. However, the cross-correlations\nbetween the exchange rates on cryptocurrency platforms differ from it. The\ncryptocurrency market is less synchronized and the information flows more\nslowly, which results in more frequent arbitrage opportunities. The methodology\nused in the review allows the latter to be detected, and lead-lag relationships\nto be discovered.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15403v2"
    },
    {
        "title": "Predicting S&P500 Index direction with Transfer Learning and a Causal\n  Graph as main Input",
        "authors": [
            "Djoumbissie David Romain"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  We propose a unified multi-tasking framework to represent the complex and\nuncertain causal process of financial market dynamics, and then to predict the\nmovement of any type of index with an application on the monthly direction of\nthe S&P500 index. our solution is based on three main pillars: (i) the use of\ntransfer learning to share knowledge and feature (representation, learning)\nbetween all financial markets, increase the size of the training sample and\npreserve the stability between training, validation and test sample. (ii) The\ncombination of multidisciplinary knowledge (Financial economics, behavioral\nfinance, market microstructure and portfolio construction theories) to\nrepresent a global top-down dynamics of any financial market, through a graph.\n(iii) The integration of forward looking unstructured data, different types of\ncontexts (long, medium and short term) through latent variables/nodes and then,\nuse a unique VAE network (parameter sharing) to learn simultaneously their\ndistributional representation. We obtain Accuracy, F1-score, and Matthew\nCorrelation of 74.3 %, 67 % and 0.42 above the industry and other benchmark on\n12 years test period which include three unstable and difficult sub-period to\npredict.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.13113v3"
    },
    {
        "title": "TailCoR",
        "authors": [
            "Slađana Babić",
            "Christophe Ley",
            "Lorenzo Ricci",
            "David Veredas"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Economic and financial crises are characterised by unusually large events.\nThese tail events co-move because of linear and/or nonlinear dependencies. We\nintroduce TailCoR, a metric that combines (and disentangles) these linear and\nnon-linear dependencies. TailCoR between two variables is based on the tail\ninter quantile range of a simple projection. It is dimension-free, it performs\nwell in small samples, and no optimisations are needed.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.14817v1"
    },
    {
        "title": "A Reinforcement Learning Based Encoder-Decoder Framework for Learning\n  Stock Trading Rules",
        "authors": [
            "Mehran Taghian",
            "Ahmad Asadi",
            "Reza Safabakhsh"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  A wide variety of deep reinforcement learning (DRL) models have recently been\nproposed to learn profitable investment strategies. The rules learned by these\nmodels outperform the previous strategies specially in high frequency trading\nenvironments. However, it is shown that the quality of the extracted features\nfrom a long-term sequence of raw prices of the instruments greatly affects the\nperformance of the trading rules learned by these models. Employing a neural\nencoder-decoder structure to extract informative features from complex input\ntime-series has proved very effective in other popular tasks like neural\nmachine translation and video captioning in which the models face a similar\nproblem. The encoder-decoder framework extracts highly informative features\nfrom a long sequence of prices along with learning how to generate outputs\nbased on the extracted features. In this paper, a novel end-to-end model based\non the neural encoder-decoder framework combined with DRL is proposed to learn\nsingle instrument trading strategies from a long sequence of raw prices of the\ninstrument. The proposed model consists of an encoder which is a neural\nstructure responsible for learning informative features from the input\nsequence, and a decoder which is a DRL model responsible for learning\nprofitable strategies based on the features extracted by the encoder. The\nparameters of the encoder and the decoder structures are learned jointly, which\nenables the encoder to extract features fitted to the task of the decoder DRL.\nIn addition, the effects of different structures for the encoder and various\nforms of the input sequences on the performance of the learned strategies are\ninvestigated. Experimental results showed that the proposed model outperforms\nother state-of-the-art models in highly dynamic environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.03867v1"
    },
    {
        "title": "A Fast Evidential Approach for Stock Forecasting",
        "authors": [
            "Tianxiang Zhan",
            "Fuyuan Xiao"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Within the framework of evidence theory, the confidence functions of\ndifferent information can be combined into a combined confidence function to\nsolve uncertain problems. The Dempster combination rule is a classic method of\nfusing different information. This paper proposes a similar confidence function\nfor the time point in the time series. The Dempster combination rule can be\nused to fuse the growth rate of the last time point, and finally a relatively\naccurate forecast data can be obtained. Stock price forecasting is a concern of\neconomics. The stock price data is large in volume, and more accurate forecasts\nare required at the same time. The classic methods of time series, such as\nARIMA, cannot balance forecasting efficiency and forecasting accuracy at the\nsame time. In this paper, the fusion method of evidence theory is applied to\nstock price prediction. Evidence theory deals with the uncertainty of stock\nprice prediction and improves the accuracy of prediction. At the same time, the\nfusion method of evidence theory has low time complexity and fast prediction\nprocessing speed.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.05204v2"
    },
    {
        "title": "Loss of structural balance in stock markets",
        "authors": [
            "E. Ferreira",
            "S. Orbe",
            "J. Ascorbebeitia",
            "B. Álvarez Pereira",
            "E. Estrada"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We use rank correlations as distance functions to establish the\ninterconnectivity between stock returns, building weighted signed networks for\nthe stocks of seven European countries, the US and Japan. We establish the\ntheoretical relationship between the level of balance in a network and stock\npredictability, studying its evolution from 2005 to the third quarter of 2020.\nWe find a clear balance-unbalance transition for six of the nine countries,\nfollowing the August 2011 Black Monday in the US, when the Economic Policy\nUncertainty index for this country reached its highest monthly level before the\nCOVID-19 crisis. This sudden loss of balance is mainly caused by a\nreorganization of the market networks triggered by a group of low\ncapitalization stocks belonging to the non-financial sector. After the\ntransition, the stocks of companies in these groups become all negatively\ncorrelated between them and with most of the rest of the stocks in the market.\nThe implied change in the network topology is directly related to a decrease in\nstocks predictability, a finding with novel important implications for asset\nallocation and portfolio hedging strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.06254v1"
    },
    {
        "title": "Applying Convolutional Neural Networks for Stock Market Trends\n  Identification",
        "authors": [
            "Ekaterina Zolotareva"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In this paper we apply a specific type ANNs - convolutional neural networks\n(CNNs) - to the problem of finding start and endpoints of trends, which are the\noptimal points for entering and leaving the market. We aim to explore long-term\ntrends, which last several months, not days. The key distinction of our model\nis that its labels are fully based on expert opinion data. Despite the various\nmodels based solely on stock price data, some market experts still argue that\ntraders are able to see hidden opportunities. The labelling was done via the\nGUI interface, which means that the experts worked directly with images, not\nnumerical data. This fact makes CNN the natural choice of algorithm. The\nproposed framework requires the sequential interaction of three CNN submodels,\nwhich identify the presence of a changepoint in a window, locate it and finally\nrecognize the type of new tendency - upward, downward or flat. These submodels\nhave certain pitfalls, therefore the calibration of their hyperparameters is\nthe main direction of further research. The research addresses such issues as\nimbalanced datasets and contradicting labels, as well as the need for specific\nquality metrics to keep up with practical applicability. This paper is the full\ntext of the research, presented at the 20th International Conference on\nArtificial Intelligence and Soft Computing Web System (ICAISC 2021)\n",
        "pdf_link": "http://arxiv.org/pdf/2104.13948v1"
    },
    {
        "title": "Should You Take Investment Advice From WallStreetBets? A Data-Driven\n  Approach",
        "authors": [
            "Tolga Buz",
            "Gerard de Melo"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Reddit's WallStreetBets (WSB) community has come to prominence in light of\nits notable role in affecting the stock prices of what are now referred to as\nmeme stocks. Yet very little is known about the reliability of the highly\nspeculative investment advice disseminated on WSB. This paper analyses WSB data\nspanning from January 2019 to April 2021 in order to assess how successful an\ninvestment strategy relying on the community's recommendations could have been.\nWe detect buy and sell advice and identify the community's most popular stocks,\nbased on which we define a WSB portfolio. Our evaluation shows that this\nportfolio has grown approx. 200% over the last three years and approx. 480%\nover the last year, significantly outperforming the S&P500. The average\nshort-term accuracy of buy and sell signals, in contrast, is not found to be\nsignificantly better than randomly or equally distributed buy decisions within\nthe same time frame. However, we present a technique for estimating whether\nposts are proactive as opposed to reactive and show that by focusing on a\nsubset of more promising buy signals, a trader could have made investments\nyielding higher returns than the broader market or the strategy of trusting all\nposted buy signals. Lastly, the analysis is also conducted specifically for the\nperiod before 2021 in order to factor out the effects of the GameStop hype of\nJanuary 2021 - the results confirm the conclusions and suggest that the 2021\nhype merely amplified pre-existing characteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02728v1"
    },
    {
        "title": "Characterization of the probability and information entropy of a process\n  with an exponentially increasing sample space and its application to the\n  Broad Money Supply",
        "authors": [
            "Laurence F Lacey"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  There is a random variable (X) with a determined outcome (i.e., X = x0),\np(x0) = 1. Consider x0 to have a discrete uniform distribution over the integer\ninterval [1, s], where the size of the sample space (s) = 1, in the initial\nstate, such that p(x0) = 1. What is the probability of x0 and the associated\ninformation entropy (H), as s increases exponentially? If the sample space\nexpansion occurs at an exponential rate (rate constant = lambda) with time (t)\nand applying time scaling, such that T = lambda x t, gives: p(x0|T)=exp(-T) and\nH(T)=T. The characterization has also been extended to include exponential\nexpansion by means of simultaneous, independent processes, as well as the more\ngeneral multi-exponential case. The methodology was applied to the expansion of\nthe broad money supply of US$ over the period 2001-2019, as a real-world\nexample. At any given time, the information entropy is related to the rate at\nwhich the sample space is expanding. In the context of the expansion of the\nbroad money supply, the information entropy could be considered to be related\nto the \"velocity\" of the expansion of the money supply.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.14193v1"
    },
    {
        "title": "Constraint-Based Inference of Heuristics for Foreign Exchange Trade\n  Model Optimization",
        "authors": [
            "Nikolay Ivanov",
            "Qiben Yan"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The Foreign Exchange (Forex) is a large decentralized market, on which\ntrading analysis and algorithmic trading are popular. Research efforts have\nbeen focusing on proof of efficiency of certain technical indicators. We\ndemonstrate, however, that the values of indicator functions are not\nreproducible and often reduce the number of trade opportunities, compared to\nprice-action trading.\n  In this work, we develop two dataset-agnostic Forex trading heuristic\ntemplates with high rate of trading signals. In order to determine most optimal\nparameters for the given heuristic prototypes, we perform a machine learning\nsimulation of 10 years of Forex price data over three low-margin instruments\nand 6 different OHLC granularities. As a result, we develop a specific and\nreproducible list of most optimal trade parameters found for each\ninstrument-granularity pair, with 118 pips of average daily profit for the\noptimized configuration.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.14194v1"
    },
    {
        "title": "Mapping the NFT revolution: market trends, trade networks and visual\n  features",
        "authors": [
            "Matthieu Nadini",
            "Laura Alessandretti",
            "Flavio Di Giacinto",
            "Mauro Martino",
            "Luca Maria Aiello",
            "Andrea Baronchelli"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Non Fungible Tokens (NFTs) are digital assets that represent objects like\nart, collectible, and in-game items. They are traded online, often with\ncryptocurrency, and are generally encoded within smart contracts on a\nblockchain. Public attention towards NFTs has exploded in 2021, when their\nmarket has experienced record sales, but little is known about the overall\nstructure and evolution of its market. Here, we analyse data concerning 6.1\nmillion trades of 4.7 million NFTs between June 23, 2017 and April 27, 2021,\nobtained primarily from Ethereum and WAX blockchains. First, we characterize\nstatistical properties of the market. Second, we build the network of\ninteractions, show that traders typically specialize on NFTs associated with\nsimilar objects and form tight clusters with other traders that exchange the\nsame kind of objects. Third, we cluster objects associated to NFTs according to\ntheir visual features and show that collections contain visually homogeneous\nobjects. Finally, we investigate the predictability of NFT sales using simple\nmachine learning algorithms and find that sale history and, secondarily, visual\nfeatures are good predictors for price. We anticipate that these findings will\nstimulate further research on NFT production, adoption, and trading in\ndifferent contexts.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.00647v4"
    },
    {
        "title": "Price graphs: Utilizing the structural information of financial time\n  series for stock prediction",
        "authors": [
            "Junran Wu",
            "Ke Xu",
            "Xueyuan Chen",
            "Shangzhe Li",
            "Jichang Zhao"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Great research efforts have been devoted to exploiting deep neural networks\nin stock prediction. While long-range dependencies and chaotic property are\nstill two major issues that lower the performance of state-of-the-art deep\nlearning models in forecasting future price trends. In this study, we propose a\nnovel framework to address both issues. Specifically, in terms of transforming\ntime series into complex networks, we convert market price series into graphs.\nThen, structural information, referring to associations among temporal points\nand the node weights, is extracted from the mapped graphs to resolve the\nproblems regarding long-range dependencies and the chaotic property. We take\ngraph embeddings to represent the associations among temporal points as the\nprediction model inputs. Node weights are used as a priori knowledge to enhance\nthe learning of temporal attention. The effectiveness of our proposed framework\nis validated using real-world stock data, and our approach obtains the best\nperformance among several state-of-the-art benchmarks. Moreover, in the\nconducted trading simulations, our framework further obtains the highest\ncumulative profits. Our results supplement the existing applications of complex\nnetwork methods in the financial realm and provide insightful implications for\ninvestment applications regarding decision support in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.02522v5"
    },
    {
        "title": "A News-based Machine Learning Model for Adaptive Asset Pricing",
        "authors": [
            "Liao Zhu",
            "Haoxuan Wu",
            "Martin T. Wells"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The paper proposes a new asset pricing model -- the News Embedding UMAP\nSelection (NEUS) model, to explain and predict the stock returns based on the\nfinancial news. Using a combination of various machine learning algorithms, we\nfirst derive a company embedding vector for each basis asset from the financial\nnews. Then we obtain a collection of the basis assets based on their company\nembedding. After that for each stock, we select the basis assets to explain and\npredict the stock return with high-dimensional statistical methods. The new\nmodel is shown to have a significantly better fitting and prediction power than\nthe Fama-French 5-factor model.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.07103v1"
    },
    {
        "title": "Probabilistic Forecasting of Imbalance Prices in the Belgian Context",
        "authors": [
            "Jonathan Dumas",
            "Ioannis Boukas",
            "Miguel Manuel de Villena",
            "Sébastien Mathieu",
            "Bertrand Cornélusse"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Forecasting imbalance prices is essential for strategic participation in the\nshort-term energy markets. A novel two-step probabilistic approach is proposed,\nwith a particular focus on the Belgian case. The first step consists of\ncomputing the net regulation volume state transition probabilities. It is\nmodeled as a matrix computed using historical data. This matrix is then used to\ninfer the imbalance prices since the net regulation volume can be related to\nthe level of reserves activated and the corresponding marginal prices for each\nactivation level are published by the Belgian Transmission System Operator one\nday before electricity delivery. This approach is compared to a deterministic\nmodel, a multi-layer perceptron, and a widely used probabilistic technique,\nGaussian Processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.07361v1"
    },
    {
        "title": "MegazordNet: combining statistical and machine learning standpoints for\n  time series forecasting",
        "authors": [
            "Angelo Garangau Menezes",
            "Saulo Martiello Mastelini"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Forecasting financial time series is considered to be a difficult task due to\nthe chaotic feature of the series. Statistical approaches have shown solid\nresults in some specific problems such as predicting market direction and\nsingle-price of stocks; however, with the recent advances in deep learning and\nbig data techniques, new promising options have arises to tackle financial time\nseries forecasting. Moreover, recent literature has shown that employing a\ncombination of statistics and machine learning may improve accuracy in the\nforecasts in comparison to single solutions. Taking into consideration the\nmentioned aspects, in this work, we proposed the MegazordNet, a framework that\nexplores statistical features within a financial series combined with a\nstructured deep learning model for time series forecasting. We evaluated our\napproach predicting the closing price of stocks in the S&P 500 using different\nmetrics, and we were able to beat single statistical and machine learning\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.01017v1"
    },
    {
        "title": "Clustering Structure of Microstructure Measures",
        "authors": [
            "Liao Zhu",
            "Ningning Sun",
            "Martin T. Wells"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper builds the clustering model of measures of market microstructure\nfeatures which are popular in predicting stock returns. In a 10-second\ntime-frequency, we study the clustering structure of different measures to find\nout the best ones for predicting. In this way, we can predict more accurately\nwith a limited number of predictors, which removes the noise and makes the\nmodel more interpretable.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.02283v3"
    },
    {
        "title": "Financial Return Distributions: Past, Present, and COVID-19",
        "authors": [
            "Marcin Wątorek",
            "Jarosław Kwapień",
            "Stanisław Drożdż"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We analyze the price return distributions of currency exchange rates,\ncryptocurrencies, and contracts for differences (CFDs) representing stock\nindices, stock shares, and commodities. Based on recent data from the years\n2017--2020, we model tails of the return distributions at different time scales\nby using power-law, stretched exponential, and $q$-Gaussian functions. We focus\non the fitted function parameters and how they change over the years by\ncomparing our results with those from earlier studies and find that, on the\ntime horizons of up to a few minutes, the so-called \"inverse-cubic power-law\"\nstill constitutes an appropriate global reference. However, we no longer\nobserve the hypothesized universal constant acceleration of the market time\nflow that was manifested before in an ever faster convergence of empirical\nreturn distributions towards the normal distribution. Our results do not\nexclude such a scenario but, rather, suggest that some other short-term\nprocesses related to a current market situation alter market dynamics and may\nmask this scenario. Real market dynamics is associated with a continuous\nalternation of different regimes with different statistical properties. An\nexample is the COVID-19 pandemic outburst, which had an enormous yet short-time\nimpact on financial markets. We also point out that two factors -- speed of the\nmarket time flow and the asset cross-correlation magnitude -- while related\n(the larger the speed, the larger the cross-correlations on a given time\nscale), act in opposite directions with regard to the return distribution\ntails, which can affect the expected distribution convergence to the normal\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.06659v1"
    },
    {
        "title": "Stock price prediction using BERT and GAN",
        "authors": [
            "Priyank Sonkiya",
            "Vikas Bajpai",
            "Anukriti Bansal"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The stock market has been a popular topic of interest in the recent past. The\ngrowth in the inflation rate has compelled people to invest in the stock and\ncommodity markets and other areas rather than saving. Further, the ability of\nDeep Learning models to make predictions on the time series data has been\nproven time and again. Technical analysis on the stock market with the help of\ntechnical indicators has been the most common practice among traders and\ninvestors. One more aspect is the sentiment analysis - the emotion of the\ninvestors that shows the willingness to invest. A variety of techniques have\nbeen used by people around the globe involving basic Machine Learning and\nNeural Networks. Ranging from the basic linear regression to the advanced\nneural networks people have experimented with all possible techniques to\npredict the stock market. It's evident from recent events how news and\nheadlines affect the stock markets and cryptocurrencies. This paper proposes an\nensemble of state-of-the-art methods for predicting stock prices. Firstly\nsentiment analysis of the news and the headlines for the company Apple Inc,\nlisted on the NASDAQ is performed using a version of BERT, which is a\npre-trained transformer model by Google for Natural Language Processing (NLP).\nAfterward, a Generative Adversarial Network (GAN) predicts the stock price for\nApple Inc using the technical indicators, stock indexes of various countries,\nsome commodities, and historical prices along with the sentiment scores.\nComparison is done with baseline models like - Long Short Term Memory (LSTM),\nGated Recurrent Units (GRU), vanilla GAN, and Auto-Regressive Integrated Moving\nAverage (ARIMA) model.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.09055v1"
    },
    {
        "title": "Dynamic functional time-series forecasts of foreign exchange implied\n  volatility surfaces",
        "authors": [
            "Han Lin Shang",
            "Fearghal Kearney"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  This paper presents static and dynamic versions of univariate, multivariate,\nand multilevel functional time-series methods to forecast implied volatility\nsurfaces in foreign exchange markets. We find that dynamic functional principal\ncomponent analysis generally improves out-of-sample forecast accuracy. More\nspecifically, the dynamic univariate functional time-series method shows the\ngreatest improvement. Our models lead to multiple instances of statistically\nsignificant improvements in forecast accuracy for daily EUR-USD, EUR-GBP, and\nEUR-JPY implied volatility surfaces across various maturities, when benchmarked\nagainst established methods. A stylised trading strategy is also employed to\ndemonstrate the potential economic benefits of our proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14026v1"
    },
    {
        "title": "The Adaptive Multi-Factor Model and the Financial Market",
        "authors": [
            "Liao Zhu"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Modern evolvements of the technologies have been leading to a profound\ninfluence on the financial market. The introduction of constituents like\nExchange-Traded Funds, and the wide-use of advanced technologies such as\nalgorithmic trading, results in a boom of the data which provides more\nopportunities to reveal deeper insights. However, traditional statistical\nmethods always suffer from the high-dimensional, high-correlation, and\ntime-varying instinct of the financial data. In this dissertation, we focus on\ndeveloping techniques to stress these difficulties. With the proposed\nmethodologies, we can have more interpretable models, clearer explanations, and\nbetter predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14410v2"
    },
    {
        "title": "Long-term, Short-term and Sudden Event: Trading Volume Movement\n  Prediction with Graph-based Multi-view Modeling",
        "authors": [
            "Liang Zhao",
            "Wei Li",
            "Ruihan Bao",
            "Keiko Harimoto",
            " YunfangWu",
            "Xu Sun"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Trading volume movement prediction is the key in a variety of financial\napplications. Despite its importance, there is few research on this topic\nbecause of its requirement for comprehensive understanding of information from\ndifferent sources. For instance, the relation between multiple stocks, recent\ntransaction data and suddenly released events are all essential for\nunderstanding trading market. However, most of the previous methods only take\nthe fluctuation information of the past few weeks into consideration, thus\nyielding poor performance. To handle this issue, we propose a graphbased\napproach that can incorporate multi-view information, i.e., long-term stock\ntrend, short-term fluctuation and sudden events information jointly into a\ntemporal heterogeneous graph. Besides, our method is equipped with deep\ncanonical analysis to highlight the correlations between different perspectives\nof fluctuation for better prediction. Experiment results show that our method\noutperforms strong baselines by a large margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.11318v1"
    },
    {
        "title": "Intra-Day Price Simulation with Generative Adversarial Modelling of the\n  Order Flow",
        "authors": [
            "Ye-Sheen Lim",
            "Denise Gorse"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Intra-day price variations in financial markets are driven by the sequence of\norders, called the order flow, that is submitted at high frequency by traders.\nThis paper introduces a novel application of the Sequence Generative\nAdversarial Networks framework to model the order flow, such that random\nsequences of the order flow can then be generated to simulate the intra-day\nvariation of prices. As a benchmark, a well-known parametric model from the\nquantitative finance literature is selected. The models are fitted, and then\nmultiple random paths of the order flow sequences are sampled from each model.\nModel performances are then evaluated by using the generated sequences to\nsimulate price variations, and we compare the empirical regularities between\nthe price variations produced by the generated and real sequences. The\nempirical regularities considered include the distribution of the price\nlog-returns, the price volatility, and the heavy-tail of the log-returns\ndistributions. The results show that the order sequences from the generative\nmodel are better able to reproduce the statistical behaviour of real price\nvariations than the sequences from the benchmark.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.13905v1"
    },
    {
        "title": "Stock Index Prediction using Cointegration test and Quantile Loss",
        "authors": [
            "Jaeyoung Cheong",
            "Heejoon Lee",
            "Minjung Kang"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Recent researches on stock prediction using deep learning methods has been\nactively studied. This is the task to predict the movement of stock prices in\nthe future based on historical trends. The approach to predicting the movement\nbased solely on the pattern of the historical movement of it on charts, not on\nfundamental values, is called the Technical Analysis, which can be divided into\nunivariate and multivariate methods in the regression task. According to the\nlatter approach, it is important to select different factors well as inputs to\nenhance the performance of the model. Moreover, its performance can depend on\nwhich loss is used to train the model. However, most studies tend to focus on\nbuilding the structures of models, not on how to select informative factors as\ninputs to train them. In this paper, we propose a method that can get better\nperformance in terms of returns when selecting informative factors using the\ncointegration test and learning the model using quantile loss. We compare the\ntwo RNN variants with quantile loss with only five factors obtained through the\ncointegration test among the entire 15 stock index factors collected in the\nexperiment. The Cumulative return and Sharpe ratio were used to evaluate the\nperformance of trained models. Our experimental results show that our proposed\nmethod outperforms the other conventional approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.15045v1"
    },
    {
        "title": "Representation of probability distributions with implied volatility and\n  biological rationale",
        "authors": [
            "Felix Polyakov"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Economic and financial theories and practice essentially deal with uncertain\nfuture. Humans encounter uncertainty in different kinds of activity, from\nsensory-motor control to dynamics in financial markets, what has been subject\nof extensive studies. Representation of uncertainty with normal or lognormal\ndistribution is a common feature of many of those studies. For example,\nproposed Bayessian integration of Gaussian multisensory input in the brain or\nlog-normal distribution of future asset price in renowned Black-Scholes-Merton\n(BSM) model for pricing contingent claims.\n  Standard deviation of log(future asset price) scaled by square root of time\nin the BSM model is called implied volatility. Actually, log(future asset\nprice) is not normally distributed and traders account for that to avoid\nlosses. Nevertheless the BSM formula derived under the assumption of constant\nvolatility remains a major uniform framework for pricing options in financial\nmarkets. I propose that one of the reasons for such a high popularity of the\nBSM formula could be its ability to translate uncertainty measured with implied\nvolatility into price in a way that is compatible with human intuition for\nmeasuring uncertainty.\n  The present study deals with mathematical relationship between uncertainty\nand the BSM implied volatility. Examples for a number of common probability\ndistributions are presented. Overall, this work proposes that representation of\nvarious probability distributions in terms of the BSM implied volatility\nprofile may be meaningful in both biological and financial worlds. Necessary\nbackground from financial mathematics is provided in the text.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.03517v1"
    },
    {
        "title": "Protecting Retail Investors from Order Book Spoofing using a GRU-based\n  Detection Model",
        "authors": [
            "Jean-Noël Tuccella",
            "Philip Nadler",
            "Ovidiu Şerban"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Market manipulation is tackled through regulation in traditional markets\nbecause of its detrimental effect on market efficiency and many participating\nfinancial actors. The recent increase of private retail investors due to new\nlow-fee platforms and new asset classes such as decentralised digital\ncurrencies has increased the number of vulnerable actors due to lack of\ninstitutional sophistication and strong regulation. This paper proposes a\nmethod to detect illicit activity and inform investors on spoofing attempts, a\nwell-known market manipulation technique. Our framework is based on a highly\nextendable Gated Recurrent Unit (GRU) model and allows the inclusion of market\nvariables that can explain spoofing and potentially other illicit activities.\nThe model is tested on granular order book data, in one of the most unregulated\nmarkets prone to spoofing with a large number of non-institutional traders. The\nresults show that the model is performing well in an early detection context,\nallowing the identification of spoofing attempts soon enough to allow investors\nto react. This is the first step to a fully comprehensive model that will\nprotect investors in various unregulated trading environments and regulators to\nidentify illicit activity.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.03687v1"
    },
    {
        "title": "Embracing advanced AI/ML to help investors achieve success: Vanguard\n  Reinforcement Learning for Financial Goal Planning",
        "authors": [
            "Shareefuddin Mohammed",
            "Rusty Bealer",
            "Jason Cohen"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  In the world of advice and financial planning, there is seldom one right\nanswer. While traditional algorithms have been successful in solving linear\nproblems, its success often depends on choosing the right features from a\ndataset, which can be a challenge for nuanced financial planning scenarios.\nReinforcement learning is a machine learning approach that can be employed with\ncomplex data sets where picking the right features can be nearly impossible. In\nthis paper, we will explore the use of machine learning for financial\nforecasting, predicting economic indicators, and creating a savings strategy.\nVanguard ML algorithm for goals-based financial planning is based on deep\nreinforcement learning that identifies optimal savings rates across multiple\ngoals and sources of income to help clients achieve financial success. Vanguard\nlearning algorithms are trained to identify market indicators and behaviors too\ncomplex to capture with formulas and rules, instead, it works to model the\nfinancial success trajectory of investors and their investment outcomes as a\nMarkov decision process. We believe that reinforcement learning can be used to\ncreate value for advisors and end-investors, creating efficiency, more\npersonalized plans, and data to enable customized solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.12003v1"
    },
    {
        "title": "Ask \"Who\", Not \"What\": Bitcoin Volatility Forecasting with Twitter Data",
        "authors": [
            "M. Eren Akbiyik",
            "Mert Erkul",
            "Killian Kaempf",
            "Vaiva Vasiliauskaite",
            "Nino Antulov-Fantulin"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Understanding the variations in trading price (volatility), and its response\nto exogenous information, is a well-researched topic in finance. In this study,\nwe focus on finding stable and accurate volatility predictors for a relatively\nnew asset class of cryptocurrencies, in particular Bitcoin, using deep learning\nrepresentations of public social media data obtained from Twitter. For our\nexperiments, we extracted semantic information and user statistics from over 30\nmillion Bitcoin-related tweets, in conjunction with 15-minute frequency price\ndata over a horizon of 144 days. Using this data, we built several deep\nlearning architectures that utilized different combinations of the gathered\ninformation. For each model, we conducted ablation studies to assess the\ninfluence of different components and feature sets over the prediction\naccuracy. We found statistical evidences for the hypotheses that: (i) temporal\nconvolutional networks perform significantly better than both classical\nautoregressive models and other deep learning-based architectures in the\nliterature, and (ii) tweet author meta-information, even detached from the\ntweet itself, is a better predictor of volatility than the semantic content and\ntweet volume statistics. We demonstrate how different information sets gathered\nfrom social media can be utilized in different architectures and how they\naffect the prediction results. As an additional contribution, we make our\ndataset public for future research.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.14317v2"
    },
    {
        "title": "Stock Price Prediction Using Time Series, Econometric, Machine Learning,\n  and Deep Learning Models",
        "authors": [
            "Ananda Chatterjee",
            "Hrisav Bhowmick",
            "Jaydip Sen"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  For a long-time, researchers have been developing a reliable and accurate\npredictive model for stock price prediction. According to the literature, if\npredictive models are correctly designed and refined, they can painstakingly\nand faithfully estimate future stock values. This paper demonstrates a set of\ntime series, econometric, and various learning-based models for stock price\nprediction. The data of Infosys, ICICI, and SUN PHARMA from the period of\nJanuary 2004 to December 2019 was used here for training and testing the models\nto know which model performs best in which sector. One time series model\n(Holt-Winters Exponential Smoothing), one econometric model (ARIMA), two\nmachine Learning models (Random Forest and MARS), and two deep learning-based\nmodels (simple RNN and LSTM) have been included in this paper. MARS has been\nproved to be the best performing machine learning model, while LSTM has proved\nto be the best performing deep learning model. But overall, for all three\nsectors - IT (on Infosys data), Banking (on ICICI data), and Health (on SUN\nPHARMA data), MARS has proved to be the best performing model in sales\nforecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.01137v1"
    },
    {
        "title": "Testing macroecological theories in cryptocurrency market: neutral\n  models can not describe diversity patterns and their variation",
        "authors": [
            "Edgardo Brigatti",
            "Estevan Augusto Amazonas Mendes"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We develop an analysis of the cryptocurrency market borrowing methods and\nconcepts from ecology. This approach makes it possible to identify specific\ndiversity patterns and their variation, in close analogy with ecological\nsystems, and to characterize the cryptocurrency market in an effective way. At\nthe same time, it shows how non-biological systems can have an important role\nin contrasting different ecological theories and in testing the use of neutral\nmodels. The study of the cryptocurrencies abundance distribution and the\nevolution of the community structure strongly indicates that these statistical\npatterns are not consistent with neutrality. In particular, the necessity to\nincrease the temporal change in community composition when the number of\ncryptocurrencies grows, suggests that their interactions are not necessarily\nweak. The analysis of the intraspecific and interspecific interdependency\nsupports this fact and demonstrates the presence of a market sector influenced\nby mutualistic relations. These latest findings challenge the hypothesis of\nweakly interacting symmetric species, the postulate at the heart of neutral\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.02067v3"
    },
    {
        "title": "A Review on Graph Neural Network Methods in Financial Applications",
        "authors": [
            "Jianian Wang",
            "Sheng Zhang",
            "Yanghua Xiao",
            "Rui Song"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  With multiple components and relations, financial data are often presented as\ngraph data, since it could represent both the individual features and the\ncomplicated relations. Due to the complexity and volatility of the financial\nmarket, the graph constructed on the financial data is often heterogeneous or\ntime-varying, which imposes challenges on modeling technology. Among the graph\nmodeling technologies, graph neural network (GNN) models are able to handle the\ncomplex graph structure and achieve great performance and thus could be used to\nsolve financial tasks. In this work, we provide a comprehensive review of GNN\nmodels in recent financial context. We first categorize the commonly-used\nfinancial graphs and summarize the feature processing step for each node. Then\nwe summarize the GNN methodology for each graph type, application in each area,\nand propose some potential research areas.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.15367v2"
    },
    {
        "title": "Complexity and Persistence of Price Time Series of the European\n  Electricity Spot Market",
        "authors": [
            "Chengyuan Han",
            "Hannes Hilger",
            "Eva Mix",
            "Philipp C. Böttcher",
            "Mark Reyers",
            "Christian Beck",
            "Dirk Witthaut",
            "Leonardo Rydin Gorjão"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The large variability of renewable power sources is a central challenge in\nthe transition to a sustainable energy system. Electricity markets are central\nfor the coordination of electric power generation. These markets rely evermore\non short-term trading to facilitate the balancing of power generation and\ndemand and to enable systems integration of small producers. Electricity prices\nin these spot markets show pronounced fluctuations, featuring extreme peaks as\nwell as occasional negative prices. In this article, we analyse electricity\nprice time series from the European EPEX market, in particular the hourly\nday-ahead, hourly intraday, and 15-min intraday market prices. We quantify the\nfluctuations, correlations, and extreme events and reveal different time scales\nin the dynamics of the market. The short-term fluctuations show remarkably\ndifferent characteristics for time scales below and above 12 hours.\nFluctuations are strongly correlated and persistent below 12 hours, which\ncontributes to extreme price events and a strong multifractal behaviour. On\nlonger time scales, they get anti-correlated and price time series revert to\ntheir mean, witnessed by a stark decrease of the Hurst coefficient after 12\nhours. The long-term behaviour is strongly influenced by the evolution of a\nlarge-scale weather patterns with a typical time scale of four days. We\nelucidate this dependence in detail using a classification into circulation\nweather types. The separation in time scales enables a superstatistical\ntreatment, which confirms the characteristic time scale of four days, and\nmotivates the use of $q$-Gaussian distributions as the best fit to the empiric\ndistribution of electricity prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.03031v1"
    },
    {
        "title": "Generative Adversarial Network (GAN) and Enhanced Root Mean Square Error\n  (ERMSE): Deep Learning for Stock Price Movement Prediction",
        "authors": [
            "Ashish Kumar",
            "Abeer Alsadoon",
            "P. W. C. Prasad",
            "Salma Abdullah",
            "Tarik A. Rashid",
            "Duong Thu Hang Pham",
            "Tran Quoc Vinh Nguyen"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  The prediction of stock price movement direction is significant in financial\ncircles and academic. Stock price contains complex, incomplete, and fuzzy\ninformation which makes it an extremely difficult task to predict its\ndevelopment trend. Predicting and analysing financial data is a nonlinear,\ntime-dependent problem. With rapid development in machine learning and deep\nlearning, this task can be performed more effectively by a purposely designed\nnetwork. This paper aims to improve prediction accuracy and minimizing\nforecasting error loss through deep learning architecture by using Generative\nAdversarial Networks. It was proposed a generic model consisting of Phase-space\nReconstruction (PSR) method for reconstructing price series and Generative\nAdversarial Network (GAN) which is a combination of two neural networks which\nare Long Short-Term Memory (LSTM) as Generative model and Convolutional Neural\nNetwork (CNN) as Discriminative model for adversarial training to forecast the\nstock market. LSTM will generate new instances based on historical basic\nindicators information and then CNN will estimate whether the data is predicted\nby LSTM or is real. It was found that the Generative Adversarial Network (GAN)\nhas performed well on the enhanced root mean square error to LSTM, as it was\n4.35% more accurate in predicting the direction and reduced processing time and\nRMSE by 78 secs and 0.029, respectively. This study provides a better result in\nthe accuracy of the stock index. It seems that the proposed system concentrates\non minimizing the root mean square error and processing time and improving the\ndirection prediction accuracy, and provides a better result in the accuracy of\nthe stock index.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.03946v1"
    },
    {
        "title": "Cryptocurrency Market Consolidation in 2020--2021",
        "authors": [
            "Jarosław Kwapień",
            "Marcin Wątorek",
            "Stanisław Drożdż"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Time series of price returns for 80 of the most liquid cryptocurrencies\nlisted on Binance are investigated for the presence of detrended\ncross-correlations. A spectral analysis of the detrended correlation matrix and\na topological analysis of the minimal spanning trees calculated based on this\nmatrix are applied for different positions of a moving window. The\ncryptocurrencies become more strongly cross-correlated among themselves than\nthey used to be before. The average cross-correlations increase with time on a\nspecific time scale in a way that resembles the Epps effect amplification when\ngoing from past to present. The minimal spanning trees also change their\ntopology and, for the short time scales, they become more centralized with\nincreasing maximum node degrees, while for the long time scales they become\nmore distributed, but also more correlated at the same time. Apart from the\ninter-market dependencies, the detrended cross-correlations between the\ncryptocurrency market and some traditional markets, like the stock markets,\ncommodity markets, and Forex, are also analyzed. The cryptocurrency market\nshows higher levels of cross-correlations with the other markets during the\nsame turbulent periods, in which it is strongly cross-correlated itself.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06552v1"
    },
    {
        "title": "Stock Movement Prediction Based on Bi-typed Hybrid-relational Market\n  Knowledge Graph via Dual Attention Networks",
        "authors": [
            "Yu Zhao",
            "Huaming Du",
            "Ying Liu",
            "Shaopeng Wei",
            "Xingyan Chen",
            "Fuzhen Zhuang",
            "Qing Li",
            "Ji Liu",
            "Gang Kou"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Stock Movement Prediction (SMP) aims at predicting listed companies' stock\nfuture price trend, which is a challenging task due to the volatile nature of\nfinancial markets. Recent financial studies show that the momentum spillover\neffect plays a significant role in stock fluctuation. However, previous studies\ntypically only learn the simple connection information among related companies,\nwhich inevitably fail to model complex relations of listed companies in the\nreal financial market. To address this issue, we first construct a more\ncomprehensive Market Knowledge Graph (MKG) which contains bi-typed entities\nincluding listed companies and their associated executives, and\nhybrid-relations including the explicit relations and implicit relations.\nAfterward, we propose DanSmp, a novel Dual Attention Networks to learn the\nmomentum spillover signals based upon the constructed MKG for stock prediction.\nThe empirical experiments on our constructed datasets against nine SOTA\nbaselines demonstrate that the proposed DanSmp is capable of improving stock\nprediction with the constructed MKG.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.04965v2"
    },
    {
        "title": "COVID-19 impact on the international trade",
        "authors": [
            "Célestin Coquidé",
            "José Lages",
            "Leonardo Ermann",
            "Dima L. Shepelyansky"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Using the United Nations Comtrade database, we perform the Google matrix\nanalysis of the multiproduct World Trade Network (WTN) for the years 2018-2020\ncomprising the emergence of the COVID-19 as a global pandemic. The applied\nalgorithms -- the PageRank, the CheiRank and the reduced Google matrix -- take\ninto account the multiplicity of the WTN links providing new insights on the\ninternational trade comparing to the usual import-export analysis. These\nalgorithms establish new rankings and trade balances of countries and products\nconsidering every countries on equal grounds, independently of their wealth,\nand every products on the basis of their relative exchanged volumes. In\ncomparison with the pre-COVID-19 period, significant changes in these metrics\noccur for the year 2020 highlighting a major rewiring of the international\ntrade flows induced by the COVID-19 pandemic crisis. We define a new\nPageRank-CheiRank product trade balance, either export or import oriented,\nwhich is significantly perturbed by the pandemic.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.07737v1"
    },
    {
        "title": "Option Volume Imbalance as a predictor for equity market returns",
        "authors": [
            "Nikolas Michael",
            "Mihai Cucuringu",
            "Sam Howison"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We investigate the use of the normalized imbalance between option volumes\ncorresponding to positive and negative market views, as a predictor for\ndirectional price movements in the spot market. Via a nonlinear analysis, and\nusing a decomposition of aggregated volumes into five distinct market\nparticipant classes, we find strong signs of predictability of excess market\novernight returns. The strongest signals come from Market-Maker volumes. Among\nother findings, we demonstrate that most of the predictability stems from\nhigh-implied-volatility option contracts, and that the informational content of\nput option volumes is greater than that of call options.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.09319v1"
    },
    {
        "title": "Machine Learning for Stock Prediction Based on Fundamental Analysis",
        "authors": [
            "Yuxuan Huang",
            "Luiz Fernando Capretz",
            "Danny Ho"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Application of machine learning for stock prediction is attracting a lot of\nattention in recent years. A large amount of research has been conducted in\nthis area and multiple existing results have shown that machine learning\nmethods could be successfully used toward stock predicting using stocks\nhistorical data. Most of these existing approaches have focused on short term\nprediction using stocks historical price and technical indicators. In this\npaper, we prepared 22 years worth of stock quarterly financial data and\ninvestigated three machine learning algorithms: Feed-forward Neural Network\n(FNN), Random Forest (RF) and Adaptive Neural Fuzzy Inference System (ANFIS)\nfor stock prediction based on fundamental analysis. In addition, we applied RF\nbased feature selection and bootstrap aggregation in order to improve model\nperformance and aggregate predictions from different models. Our results show\nthat RF model achieves the best prediction results, and feature selection is\nable to improve test performance of FNN and ANFIS. Moreover, the aggregated\nmodel outperforms all baseline models as well as the benchmark DJIA index by an\nacceptable margin for the test period. Our findings demonstrate that machine\nlearning models could be used to aid fundamental analysts with decision-making\nregarding stock investment.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05702v1"
    },
    {
        "title": "A 2D Levy-flight model for the complex dynamics of real-life financial\n  markets",
        "authors": [
            "Hediye Yarahmadi",
            "Abbas Ali Saberi"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We report on the emergence of scaling laws in the temporal evolution of the\ndaily closing values of the S\\&P 500 index prices and its modeling based on the\nL\\'evy flights in two dimensions (2D). The efficacy of our proposed model is\nverified and validated by using the extreme value statistics in random matrix\ntheory. We find that the random evolution of each pair of stocks in a 2D price\nspace is a scale-invariant complex trajectory whose tortuosity is governed by a\n$2/3$ geometric law between the gyration radius $R_g(t)$ and the total length\n$\\ell(t)$ of the path, i.e., $R_g(t)\\sim\\ell(t)^{2/3}$. We construct a Wishart\nmatrix containing all stocks up to a specific variable period and look at its\nspectral properties over 30 years. In contrast to the standard random matrix\ntheory, we find that the distribution of eigenvalues has a power-law tail with\na decreasing exponent over time -- a quantitative indicator of the temporal\ncorrelations. We find that the time evolution of the distance of a 2D L\\'evy\nflights with index $\\alpha=3/2$ from origin generates the same empirical\nspectral properties. The statistics of the largest eigenvalues of the model and\nthe observations are in perfect agreement.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12067v1"
    },
    {
        "title": "HiSA-SMFM: Historical and Sentiment Analysis based Stock Market\n  Forecasting Model",
        "authors": [
            "Ishu Gupta",
            "Tarun Kumar Madan",
            "Sukhman Singh",
            "Ashutosh Kumar Singh"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  One of the pillars to build a country's economy is the stock market. Over the\nyears, people are investing in stock markets to earn as much profit as possible\nfrom the amount of money that they possess. Hence, it is vital to have a\nprediction model which can accurately predict future stock prices. With the\nhelp of machine learning, it is not an impossible task as the various machine\nlearning techniques if modeled properly may be able to provide the best\nprediction values. This would enable the investors to decide whether to buy,\nsell or hold the share. The aim of this paper is to predict the future of the\nfinancial stocks of a company with improved accuracy. In this paper, we have\nproposed the use of historical as well as sentiment data to efficiently predict\nstock prices by applying LSTM. It has been found by analyzing the existing\nresearch in the area of sentiment analysis that there is a strong correlation\nbetween the movement of stock prices and the publication of news articles.\nTherefore, in this paper, we have integrated these factors to predict the stock\nprices more accurately.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08143v1"
    },
    {
        "title": "DeepTrust: A Reliable Financial Knowledge Retrieval Framework For\n  Explaining Extreme Pricing Anomalies",
        "authors": [
            "Pok Wah Chan"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Extreme pricing anomalies may occur unexpectedly without a trivial cause, and\nequity traders typically experience a meticulous process to source disparate\ninformation and analyze its reliability before integrating it into the trusted\nknowledge base. We introduce DeepTrust, a reliable financial knowledge\nretrieval framework on Twitter to explain extreme price moves at speed, while\nensuring data veracity using state-of-the-art NLP techniques. Our proposed\nframework consists of three modules, specialized for anomaly detection,\ninformation retrieval and reliability assessment. The workflow starts with\nidentifying anomalous asset price changes using machine learning models trained\nwith historical pricing data, and retrieving correlated unstructured data from\nTwitter using enhanced queries with dynamic search conditions. DeepTrust\nextrapolates information reliability from tweet features, traces of generative\nlanguage model, argumentation structure, subjectivity and sentiment signals,\nand refine a concise collection of credible tweets for market insights. The\nframework is evaluated on two self-annotated financial anomalies, i.e., Twitter\nand Facebook stock price on 29 and 30 April 2021. The optimal setup outperforms\nthe baseline classifier by 7.75% and 15.77% on F0.5-scores, and 10.55% and\n18.88% on precision, respectively, proving its capability in screening\nunreliable information precisely. At the same time, information retrieval and\nreliability assessment modules are analyzed individually on their effectiveness\nand causes of limitations, with identified subjective and objective factors\nthat influence the performance. As a collaborative project with Refinitiv, this\nframework paves a promising path towards building a scalable commercial\nsolution that assists traders to reach investment decisions on pricing\nanomalies with authenticated knowledge from social media platforms in\nreal-time.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08144v1"
    },
    {
        "title": "On the dependence structure of the trade/no trade sequence of illiquid\n  assets",
        "authors": [
            "Hamdi Raïssi"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In this paper, we propose to consider the dependence structure of the\ntrade/no trade categorical sequence of individual illiquid stocks returns. The\nframework considered here is wide as constant and time-varying zero returns\nprobability are allowed. The ability of our approach in highlighting illiquid\nstock's features is underlined for a variety of situations. More specifically,\nwe show that long-run effects for the trade/no trade categorical sequence may\nbe spuriously detected in presence of a non-constant zero returns probability.\nMonte Carlo experiments, and the analysis of stocks taken from the Chilean\nfinancial market, illustrate the usefulness of the tools developed in the\npaper.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08223v1"
    },
    {
        "title": "Rough volatility: fact or artefact?",
        "authors": [
            "Rama Cont",
            "Purba Das"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We investigate the statistical evidence for the use of `rough' fractional\nprocesses with Hurst exponent $H< 0.5$ for the modeling of volatility of\nfinancial assets, using a model-free approach. We introduce a non-parametric\nmethod for estimating the roughness of a function based on discrete sample,\nusing the concept of normalized $p$-th variation along a sequence of\npartitions. We investigate the finite sample performance of our estimator for\nmeasuring the roughness of sample paths of stochastic processes using detailed\nnumerical experiments based on sample paths of fractional Brownian motion and\nother fractional processes. We then apply this method to estimate the roughness\nof realized volatility signals based on high-frequency observations. Detailed\nnumerical experiments based on stochastic volatility models show that, even\nwhen the instantaneous volatility has diffusive dynamics with the same\nroughness as Brownian motion, the realized volatility exhibits rough behaviour\ncorresponding to a Hurst exponent significantly smaller than $0.5$. Comparison\nof roughness estimates for realized and instantaneous volatility in fractional\nvolatility models with different values of Hurst exponent shows that,\nirrespective of the roughness of the spot volatility process, realized\nvolatility always exhibits `rough' behaviour with an apparent Hurst index\n$\\hat{H}<0.5$. These results suggest that the origin of the roughness observed\nin realized volatility time-series lies in the microstructure noise rather than\nthe volatility process itself.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.13820v3"
    },
    {
        "title": "Calibration window selection based on change-point detection for\n  forecasting electricity prices",
        "authors": [
            "Julia Nasiadka",
            "Weronika Nitka",
            "Rafał Weron"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We employ a recently proposed change-point detection algorithm, the\nNarrowest-Over-Threshold (NOT) method, to select subperiods of past\nobservations that are similar to the currently recorded values. Then,\ncontrarily to the traditional time series approach in which the most recent\n$\\tau$ observations are taken as the calibration sample, we estimate\nautoregressive models only for data in these subperiods. We illustrate our\napproach using a challenging dataset - day-ahead electricity prices in the\nGerman EPEX SPOT market - and observe a significant improvement in forecasting\naccuracy compared to commonly used approaches, including the Autoregressive\nHybrid Nearest Neighbors (ARHNN) method.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.00872v1"
    },
    {
        "title": "Electricity Price Forecasting: The Dawn of Machine Learning",
        "authors": [
            "Arkadiusz Jędrzejewski",
            "Jesus Lago",
            "Grzegorz Marcjasz",
            "Rafał Weron"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Electricity price forecasting (EPF) is a branch of forecasting on the\ninterface of electrical engineering, statistics, computer science, and finance,\nwhich focuses on predicting prices in wholesale electricity markets for a whole\nspectrum of horizons. These range from a few minutes (real-time/intraday\nauctions and continuous trading), through days (day-ahead auctions), to weeks,\nmonths or even years (exchange and over-the-counter traded futures and forward\ncontracts). Over the last 25 years, various methods and computational tools\nhave been applied to intraday and day-ahead EPF. Until the early 2010s, the\nfield was dominated by relatively small linear regression models and\n(artificial) neural networks, typically with no more than two dozen inputs. As\ntime passed, more data and more computational power became available. The\nmodels grew larger to the extent where expert knowledge was no longer enough to\nmanage the complex structures. This, in turn, led to the introduction of\nmachine learning (ML) techniques in this rapidly developing and fascinating\narea. Here, we provide an overview of the main trends and EPF models as of\n2022.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.00883v1"
    },
    {
        "title": "Variational Heteroscedastic Volatility Model",
        "authors": [
            "Zexuan Yin",
            "Paolo Barucca"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We propose Variational Heteroscedastic Volatility Model (VHVM) -- an\nend-to-end neural network architecture capable of modelling heteroscedastic\nbehaviour in multivariate financial time series. VHVM leverages recent advances\nin several areas of deep learning, namely sequential modelling and\nrepresentation learning, to model complex temporal dynamics between different\nasset returns. At its core, VHVM consists of a variational autoencoder to\ncapture relationships between assets, and a recurrent neural network to model\nthe time-evolution of these dependencies. The outputs of VHVM are time-varying\nconditional volatilities in the form of covariance matrices. We demonstrate the\neffectiveness of VHVM against existing methods such as Generalised\nAutoRegressive Conditional Heteroscedasticity (GARCH) and Stochastic Volatility\n(SV) models on a wide range of multivariate foreign currency (FX) datasets.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05806v1"
    },
    {
        "title": "Discovering material information using hierarchical Reformer model on\n  financial regulatory filings",
        "authors": [
            "Francois Mercier",
            "Makesh Narsimhan"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Most applications of machine learning for finance are related to forecasting\ntasks for investment decisions. Instead, we aim to promote a better\nunderstanding of financial markets with machine learning techniques. Leveraging\nthe tremendous progress in deep learning models for natural language\nprocessing, we construct a hierarchical Reformer ([15]) model capable of\nprocessing a large document level dataset, SEDAR, from canadian financial\nregulatory filings. Using this model, we show that it is possible to predict\ntrade volume changes using regulatory filings. We adapt the pretraining task of\nHiBERT ([36]) to obtain good sentence level representations using a large\nunlabelled document dataset. Finetuning the model to successfully predict trade\nvolume changes indicates that the model captures a view from financial markets\nand processing regulatory filings is beneficial. Analyzing the attention\npatterns of our model reveals that it is able to detect some indications of\nmaterial information without explicit training, which is highly relevant for\ninvestors and also for the market surveillance mandate of financial regulators.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05979v1"
    },
    {
        "title": "Heterogeneous rarity patterns drive price dynamics in NFT collections",
        "authors": [
            "Amin Mekacher",
            "Alberto Bracci",
            "Matthieu Nadini",
            "Mauro Martino",
            "Laura Alessandretti",
            "Luca Maria Aiello",
            "Andrea Baronchelli"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We quantify Non Fungible Token (NFT) rarity and investigate how it impacts\nmarket behaviour by analysing a dataset of 3.7M transactions collected between\nJanuary 2018 and June 2022, involving 1.4M NFTs distributed across 410\ncollections. First, we consider the rarity of an NFT based on the set of\nhuman-readable attributes it possesses and show that most collections present\nheterogeneous rarity patterns, with few rare NFTs and a large number of more\ncommon ones. Then, we analyze market performance and show that, on average,\nrarer NFTs: (i) sell for higher prices, (ii) are traded less frequently, (iii)\nguarantee higher returns on investment (ROIs), and (iv) are less risky, i.e.,\nless prone to yield negative returns. We anticipate that these findings will be\nof interest to researchers as well as NFT creators, collectors, and traders.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10243v4"
    },
    {
        "title": "Forecasting Electricity Prices",
        "authors": [
            "Katarzyna Maciejowska",
            "Bartosz Uniejewski",
            "Rafał Weron"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Forecasting electricity prices is a challenging task and an active area of\nresearch since the 1990s and the deregulation of the traditionally monopolistic\nand government-controlled power sectors. Although it aims at predicting both\nspot and forward prices, the vast majority of research is focused on short-term\nhorizons which exhibit dynamics unlike in any other market. The reason is that\npower system stability calls for a constant balance between production and\nconsumption, while being weather (both demand and supply) and business activity\n(demand only) dependent. The recent market innovations do not help in this\nrespect. The rapid expansion of intermittent renewable energy sources is not\noffset by the costly increase of electricity storage capacities and\nmodernization of the grid infrastructure. On the methodological side, this\nleads to three visible trends in electricity price forecasting research as of\n2022. Firstly, there is a slow, but more noticeable with every year, tendency\nto consider not only point but also probabilistic (interval, density) or even\npath (also called ensemble) forecasts. Secondly, there is a clear shift from\nthe relatively parsimonious econometric (or statistical) models towards more\ncomplex and harder to comprehend, but more versatile and eventually more\naccurate statistical/machine learning approaches. Thirdly, statistical error\nmeasures are nowadays regarded as only the first evaluation step. Since they\nmay not necessarily reflect the economic value of reducing prediction errors,\nmore and more often, they are complemented by case studies comparing profits\nfrom scheduling or trading strategies based on price forecasts obtained from\ndifferent models.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.11735v1"
    },
    {
        "title": "Forecasting foreign exchange rates with regression networks tuned by\n  Bayesian optimization",
        "authors": [
            "Linwei Li",
            "Paul-Amaury Matt",
            "Christian Heumann"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The article is concerned with the problem of multi-step financial time series\nforecasting of Foreign Exchange (FX) rates. To address this problem, we\nintroduce a regression network termed RegPred Net. The exchange rate to\nforecast is treated as a stochastic process. It is assumed to follow a\ngeneralization of Brownian motion and the mean-reverting process referred to as\nthe generalized Ornstein-Uhlenbeck (OU) process, with time-dependent\ncoefficients. Using past observed values of the input time series, these\ncoefficients can be regressed online by the cells of the first half of the\nnetwork (Reg). The regressed coefficients depend only on - but are very\nsensitive to - a small number of hyperparameters required to be set by a global\noptimization procedure for which, Bayesian optimization is an adequate\nheuristic. Thanks to its multi-layered architecture, the second half of the\nregression network (Pred) can project time-dependent values for the OU process\ncoefficients and generate realistic trajectories of the time series.\nPredictions can be easily derived in the form of expected values estimated by\naveraging values obtained by Monte Carlo simulation. The forecasting accuracy\non a 100 days horizon is evaluated for several of the most important FX rates\nsuch as EUR/USD, EUR/CNY, and EUR/GBP. Our experimental results show that the\nRegPred Net significantly outperforms ARMA, ARIMA, LSTMs, and Autoencoder-LSTM\nmodels in terms of metrics measuring the absolute error (RMSE) and correlation\nbetween predicted and actual values (Pearson R, R-squared, MDA). Compared to\nblack-box deep learning models such as LSTM, RegPred Net has better\ninterpretability, simpler structure, and fewer parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12914v3"
    },
    {
        "title": "Sequence-Based Target Coin Prediction for Cryptocurrency Pump-and-Dump",
        "authors": [
            "Sihao Hu",
            "Zhen Zhang",
            "Shengliang Lu",
            "Bingsheng He",
            "Zhao Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  With the proliferation of pump-and-dump schemes (P&Ds) in the cryptocurrency\nmarket, it becomes imperative to detect such fraudulent activities in advance\nto alert potentially susceptible investors. In this paper, we focus on\npredicting the pump probability of all coins listed in the target exchange\nbefore a scheduled pump time, which we refer to as the target coin prediction\ntask. Firstly, we conduct a comprehensive study of the latest 709 P&D events\norganized in Telegram from Jan. 2019 to Jan. 2022. Our empirical analysis\nreveals some interesting patterns of P&Ds, such as that pumped coins exhibit\nintra-channel homogeneity and inter-channel heterogeneity. Here channel refers\na form of group in Telegram that is frequently used to coordinate P&D events.\nThis observation inspires us to develop a novel sequence-based neural network,\ndubbed SNN, which encodes a channel's P&D event history into a sequence\nrepresentation via the positional attention mechanism to enhance the prediction\naccuracy. Positional attention helps to extract useful information and\nalleviates noise, especially when the sequence length is long. Extensive\nexperiments verify the effectiveness and generalizability of proposed methods.\nAdditionally, we release the code and P&D dataset on GitHub:\nhttps://github.com/Bayi-Hu/Pump-and-Dump-Detection-on-Cryptocurrency, and\nregularly update the dataset.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12929v2"
    },
    {
        "title": "Cluster-based Regression using Variational Inference and Applications in\n  Financial Forecasting",
        "authors": [
            "Udai Nagpal",
            "Krishan Nagpal"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  This paper describes an approach to simultaneously identify clusters and\nestimate cluster-specific regression parameters from the given data. Such an\napproach can be useful in learning the relationship between input and output\nwhen the regression parameters for estimating output are different in different\nregions of the input space. Variational Inference (VI), a machine learning\napproach to obtain posterior probability densities using optimization\ntechniques, is used to identify clusters of explanatory variables and\nregression parameters for each cluster. From these results, one can obtain both\nthe expected value and the full distribution of predicted output. Other\nadvantages of the proposed approach include the elegant theoretical solution\nand clear interpretability of results. The proposed approach is well-suited for\nfinancial forecasting where markets have different regimes (or clusters) with\ndifferent patterns and correlations of market changes in each regime. In\nfinancial applications, knowledge about such clusters can provide useful\ninsights about portfolio performance and identify the relative importance of\nvariables in different market regimes. An illustrative example of predicting\none-day S&P change is considered to illustrate the approach and compare the\nperformance of the proposed approach with standard regression without clusters.\nDue to the broad applicability of the problem, its elegant theoretical\nsolution, and the computational efficiency of the proposed algorithm, the\napproach may be useful in a number of areas extending beyond the financial\ndomain.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00605v3"
    },
    {
        "title": "Cross Cryptocurrency Relationship Mining for Bitcoin Price Prediction",
        "authors": [
            "Panpan Li",
            "Shengbo Gong",
            "Shaocong Xu",
            "Jiajun Zhou",
            "Yu Shanqing",
            "Qi Xuan"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Blockchain finance has become a part of the world financial system, most\ntypically manifested in the attention to the price of Bitcoin. However, a great\ndeal of work is still limited to using technical indicators to capture Bitcoin\nprice fluctuation, with little consideration of historical relationships and\ninteractions between related cryptocurrencies. In this work, we propose a\ngeneric Cross-Cryptocurrency Relationship Mining module, named C2RM, which can\neffectively capture the synchronous and asynchronous impact factors between\nBitcoin and related Altcoins. Specifically, we utilize the Dynamic Time Warping\nalgorithm to extract the lead-lag relationship, yielding Lead-lag Variance\nKernel, which will be used for aggregating the information of Altcoins to form\nrelational impact factors. Comprehensive experimental results demonstrate that\nour C2RM can help existing price prediction methods achieve significant\nperformance improvement, suggesting the effectiveness of Cross-Cryptocurrency\ninteractions on benefitting Bitcoin price prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.00974v1"
    },
    {
        "title": "A Multivariate Hawkes Process Model for Stablecoin-Cryptocurrency\n  Depegging Event Dynamics",
        "authors": [
            "Connor Oxenhorn"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Stablecoins, digital assets pegged to a specific currency or commodity value,\nare heavily involved in transactions of major cryptocurrencies. The effects of\ndeviations from their desired fixed values (depeggings) on the cryptocurrencies\nfor which they are frequently used in transactions are therefore of interest to\nstudy. We propose a model for this phenomenon using a multivariate\nmutually-exciting Hawkes process, and present a numerical example applying this\nmodel to Tether (USDT) and Bitcoin (BTC).\n",
        "pdf_link": "http://arxiv.org/pdf/2205.06338v1"
    },
    {
        "title": "Univariate and Multivariate LSTM Model for Short-Term Stock Market\n  Prediction",
        "authors": [
            "Vishal Kuber",
            "Divakar Yadav",
            "Arun Kr Yadav"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Designing robust and accurate prediction models has been a viable research\narea since a long time. While proponents of a well-functioning market\npredictors believe that it is difficult to accurately predict market prices but\nmany scholars disagree. Robust and accurate prediction systems will not only be\nhelpful to the businesses but also to the individuals in making their financial\ninvestments. This paper presents an LSTM model with two different input\napproaches for predicting the short-term stock prices of two Indian companies,\nReliance Industries and Infosys Ltd. Ten years of historic data (2012-2021) is\ntaken from the yahoo finance website to carry out analysis of proposed\napproaches. In the first approach, closing prices of two selected companies are\ndirectly applied on univariate LSTM model. For the approach second, technical\nindicators values are calculated from the closing prices and then collectively\napplied on Multivariate LSTM model. Short term market behaviour for upcoming\ndays is evaluated. Experimental outcomes revel that approach one is useful to\ndetermine the future trend but multivariate LSTM model with technical\nindicators found to be useful in accurately predicting the future price\nbehaviours.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.06673v1"
    },
    {
        "title": "Probabilistic forecasting of German electricity imbalance prices",
        "authors": [
            "Michał Narajewski"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The exponential growth of renewable energy capacity has brought much\nuncertainty to electricity prices and to electricity generation. To address\nthis challenge, the energy exchanges have been developing further trading\npossibilities, especially the intraday and balancing markets. For an energy\ntrader participating in both markets, the forecasting of imbalance prices is of\nparticular interest. Therefore, in this manuscript we conduct a very short-term\nprobabilistic forecasting of imbalance prices, contributing to the scarce\nliterature in this novel subject. The forecasting is performed 30 minutes\nbefore the delivery, so that the trader might still choose the trading place.\nThe distribution of the imbalance prices is modelled and forecasted using\nmethods well-known in the electricity price forecasting literature: lasso with\nbootstrap, gamlss, and probabilistic neural networks. The methods are compared\nwith a naive benchmark in a meaningful rolling window study. The results\nprovide evidence of the efficiency between the intraday and balancing markets\nas the sophisticated methods do not substantially overperform the intraday\ncontinuous price index. On the other hand, they significantly improve the\nempirical coverage. The analysis was conducted on the German market, however it\ncould be easily applied to any other market of similar structure.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.11439v1"
    },
    {
        "title": "The DEBS 2022 Grand Challenge: Detecting Trading Trends in Financial\n  Tick Data",
        "authors": [
            "Sebastian Frischbier",
            "Jawad Tahir",
            "Christoph Doblander",
            "Arne Hormann",
            "Ruben Mayer",
            "Hans-Arno Jacobsen"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  The DEBS Grand Challenge (GC) is an annual programming competition open to\npractitioners from both academia and industry. The GC 2022 edition focuses on\nreal-time complex event processing of high-volume tick data provided by Infront\nFinancial Technology GmbH. The goal of the challenge is to efficiently compute\nspecific trend indicators and detect patterns in these indicators like those\nused by real-life traders to decide on buying or selling in financial markets.\nThe data set Trading Data used for benchmarking contains 289 million tick\nevents from approximately 5500+ financial instruments that had been traded on\nthe three major exchanges Amsterdam (NL), Paris (FR), and Frankfurt am Main\n(GER) over the course of a full week in 2021. The data set is made publicly\navailable. In addition to correctness and performance, submissions must\nexplicitly focus on reusability and practicability. Hence, participants must\naddress specific nonfunctional requirements and are asked to build upon\nopen-source platforms. This paper describes the required scenario and the data\nset Trading Data, defines the queries of the problem statement, and explains\nthe enhancements made to the evaluation platform Challenger that handles data\ndistribution, dynamic subscriptions, and remote evaluation of the submissions.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.13237v1"
    },
    {
        "title": "Distributional neural networks for electricity price forecasting",
        "authors": [
            "Grzegorz Marcjasz",
            "Michał Narajewski",
            "Rafał Weron",
            "Florian Ziel"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We present a novel approach to probabilistic electricity price forecasting\nwhich utilizes distributional neural networks. The model structure is based on\na deep neural network that contains a so-called probability layer. The\nnetwork's output is a parametric distribution with 2 (normal) or 4 (Johnson's\nSU) parameters. In a forecasting study involving day-ahead electricity prices\nin the German market, our approach significantly outperforms state-of-the-art\nbenchmarks, including LASSO-estimated regressions and deep neural networks\ncombined with Quantile Regression Averaging. The obtained results not only\nemphasize the importance of higher moments when modeling volatile electricity\nprices, but also -- given that probabilistic forecasting is the essence of risk\nmanagement -- provide important implications for managing portfolios in the\npower sector.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.02832v2"
    },
    {
        "title": "Variations on two-parameter families of forecasting functions:\n  seasonal/nonseasonal Models, comparison to the exponential smoothing and\n  ARIMA models, and applications to stock market data",
        "authors": [
            "Nabil Kahouadji"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We introduce twenty four two-parameter families of advanced time series\nforecasting functions using a new and nonparametric approach. We also introduce\nthe concept of powering and derive nonseasonal and seasonal models with\nexamples in education, sales, finance and economy. We compare the performance\nof our twenty four models to both Holt--Winters and ARIMA models for both\nnonseasonal and seasonal times series. We show in particular that our models\nnot only do not require a decomposition of a seasonal time series into trend,\nseasonal and random components, but leads also to substantially lower sum of\nabsolute error and a higher number of closer forecasts than both Holt--Winters\nand ARIMA models. Finally, we apply and compare the performance of our twenty\nfour models using five-year stock market data of 467 companies of the S&P500.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.04882v2"
    },
    {
        "title": "Multifractal cross-correlations of bitcoin and ether trading\n  characteristics in the post-COVID-19 time",
        "authors": [
            "Marcin Wątorek",
            "Jarosław Kwapień",
            "Stanisław Drożdż"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Unlike price fluctuations, the temporal structure of cryptocurrency trading\nhas seldom been a subject of systematic study. In order to fill this gap, we\nanalyse detrended correlations of the price returns, the average number of\ntrades in time unit, and the traded volume based on high-frequency data\nrepresenting two major cryptocurrencies: bitcoin and ether. We apply the\nmultifractal detrended cross-correlation analysis, which is considered the most\nreliable method for identifying nonlinear correlations in time series. We find\nthat all the quantities considered in our study show an unambiguous\nmultifractal structure from both the univariate (auto-correlation) and\nbivariate (cross-correlation) perspectives. We looked at the bitcoin--ether\ncross-correlations in simultaneously recorded signals, as well as in\ntime-lagged signals, in which a time series for one of the cryptocurrencies is\nshifted with respect to the other. Such a shift suppresses the\ncross-correlations partially for short time scales, but does not remove them\ncompletely. We did not observe any qualitative asymmetry in the results for the\ntwo choices of a leading asset. The cross-correlations for the simultaneous and\nlagged time series became the same in magnitude for the sufficiently long\nscales.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.01445v1"
    },
    {
        "title": "A Hawkes model with CARMA(p,q) intensity",
        "authors": [
            "Lorenzo Mercuri",
            "Andrea Perchiazzo",
            "Edit Rroji"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In this paper we introduce a new model named CARMA(p,q)-Hawkes process as the\nHawkes model with exponential kernel implies a strictly decreasing behaviour of\nthe autocorrelation function and empirically evidences reject the monotonicity\nassumption on the autocorrelation function. The proposed model is a Hawkes\nprocess where the intensity follows a Continuous Time Autoregressive Moving\nAverage (CARMA) process and specifically is able to reproduce more realistic\ndependence structures. We also study the conditions of stationarity and\npositivity for the intensity and the strong mixing property for the increments.\nFurthermore we compute the likelihood, present a simulation method and discuss\nan estimation method based on the autocorrelation function. A simulation and\nestimation exercise highlights the main features of the CARMA(p,q)-Hawkes.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.02659v3"
    },
    {
        "title": "New drugs and stock market: how to predict pharma market reaction to\n  clinical trial announcements",
        "authors": [
            "Semen Budennyy",
            "Alexey Kazakov",
            "Elizaveta Kovtun",
            "Leonid Zhukov"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Pharmaceutical companies operate in a strictly regulated and highly risky\nenvironment in which a single slip can lead to serious financial implications.\nAccordingly, the announcements of clinical trial results tend to determine the\nfuture course of events, hence being closely monitored by the public. In this\nwork, we provide statistical evidence for the result promulgation influence on\nthe public pharma market value. Whereas most works focus on retrospective\nimpact analysis, the present research aims to predict the numerical values of\nannouncement-induced changes in stock prices. For this purpose, we develop a\npipeline that includes a BERT-based model for extracting sentiment polarity of\nannouncements, a Temporal Fusion Transformer for forecasting the expected\nreturn, a graph convolution network for capturing event relationships, and\ngradient boosting for predicting the price change. The challenge of the problem\nlies in inherently different patterns of responses to positive and negative\nannouncements, reflected in a stronger and more pronounced reaction to the\nnegative news. Moreover, such phenomenon as the drop in stocks after the\npositive announcements affirms the counterintuitiveness of the price behavior.\nImportantly, we discover two crucial factors that should be considered while\nworking within a predictive framework. The first factor is the drug portfolio\nsize of the company, indicating the greater susceptibility to an announcement\nin the case of small drug diversification. The second one is the network effect\nof the events related to the same company or nosology. All findings and\ninsights are gained on the basis of one of the biggest FDA (the Food and Drug\nAdministration) announcement datasets, consisting of 5436 clinical trial\nannouncements from 681 companies over the last five years.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.07248v2"
    },
    {
        "title": "Signature-based validation of real-world economic scenarios",
        "authors": [
            "Hervé Andrès",
            "Alexandre Boumezoued",
            "Benjamin Jourdain"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Motivated by insurance applications, we propose a new approach for the\nvalidation of real-world economic scenarios. This approach is based on the\nstatistical test developed by Chevyrev and Oberhauser (2022) and relies on the\nnotions of signature and maximum mean distance. This test allows to check\nwhether two samples of stochastic processes paths come from the same\ndistribution. Our contribution is to apply this test to a variety of stochastic\nprocesses exhibiting different pathwise properties (H{\\\"o}lder regularity,\nautocorrelation, regime switches) and which are relevant for the modelling of\nstock prices and stock volatility as well as of inflation in view of actuarial\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.07251v3"
    },
    {
        "title": "The Efficient Market Hypothesis for Bitcoin in the context of neural\n  networks",
        "authors": [
            "Mike Kraehenbuehl",
            "Joerg Osterrieder"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  This study examines the weak form of the efficient market hypothesis for\nBitcoin using a feedforward neural network. Due to the increasing popularity of\ncryptocurrencies in recent years, the question has arisen, as to whether market\ninefficiencies could be exploited in Bitcoin. Several studies we refer to here\ndiscuss this topic in the context of Bitcoin using either statistical tests or\nmachine learning methods, mostly relying exclusively on data from Bitcoin\nitself. Results regarding market efficiency vary from study to study. In this\nstudy, however, the focus is on applying various asset-related input features\nin a neural network. The aim is to investigate whether the prediction accuracy\nimproves when adding equity stock indices (S&P 500, Russell 2000), currencies\n(EURUSD), 10 Year US Treasury Note Yield as well as Gold&Silver producers index\n(XAU), in addition to using Bitcoin returns as input feature. As expected, the\nresults show that more features lead to higher training performance from 54.6%\nprediction accuracy with one feature to 61% with six features. On the test set,\nwe observe that with our neural network methodology, adding additional asset\nclasses, no increase in prediction accuracy is achieved. One feature set is\nable to partially outperform a buy-and-hold strategy, but the performance drops\nagain as soon as another feature is added. This leads us to the partial\nconclusion that weak market inefficiencies for Bitcoin cannot be detected using\nneural networks and the given asset classes as input. Therefore, based on this\nstudy, we find evidence that the Bitcoin market is efficient in the sense of\nthe efficient market hypothesis during the sample period. We encourage further\nresearch in this area, as much depends on the sample period chosen, the input\nfeatures, the model architecture, and the hyperparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.07254v1"
    },
    {
        "title": "Deep Reinforcement Learning for Cryptocurrency Trading: Practical\n  Approach to Address Backtest Overfitting",
        "authors": [
            "Berend Jelmer Dirk Gort",
            "Xiao-Yang Liu",
            "Xinghang Sun",
            "Jiechao Gao",
            "Shuaiyu Chen",
            "Christina Dan Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Designing profitable and reliable trading strategies is challenging in the\nhighly volatile cryptocurrency market. Existing works applied deep\nreinforcement learning methods and optimistically reported increased profits in\nbacktesting, which may suffer from the false positive issue due to overfitting.\nIn this paper, we propose a practical approach to address backtest overfitting\nfor cryptocurrency trading using deep reinforcement learning. First, we\nformulate the detection of backtest overfitting as a hypothesis test. Then, we\ntrain the DRL agents, estimate the probability of overfitting, and reject the\noverfitted agents, increasing the chance of good trading performance. Finally,\non 10 cryptocurrencies over a testing period from 05/01/2022 to 06/27/2022\n(during which the crypto market crashed two times), we show that the less\noverfitted deep reinforcement learning agents have a higher return than that of\nmore overfitted agents, an equal weight strategy, and the S&P DBM Index (market\nbenchmark), offering confidence in possible deployment to a real market.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.05559v6"
    },
    {
        "title": "Anomaly Detection on Financial Time Series by Principal Component\n  Analysis and Neural Networks",
        "authors": [
            "Stéphane Crépey",
            "Lehdili Noureddine",
            "Nisrine Madhar",
            "Maud Thomas"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  A major concern when dealing with financial time series involving a wide\nvariety ofmarket risk factors is the presence of anomalies. These induce a\nmiscalibration of the models used toquantify and manage risk, resulting in\npotential erroneous risk measures. We propose an approachthat aims to improve\nanomaly detection in financial time series, overcoming most of the\ninherentdifficulties. Valuable features are extracted from the time series by\ncompressing and reconstructingthe data through principal component analysis. We\nthen define an anomaly score using a feedforwardneural network. A time series\nis considered to be contaminated when its anomaly score exceeds agiven cutoff\nvalue. This cutoff value is not a hand-set parameter but rather is calibrated\nas a neuralnetwork parameter throughout the minimization of a customized loss\nfunction. The efficiency of theproposed approach compared to several well-known\nanomaly detection algorithms is numericallydemonstrated on both synthetic and\nreal data sets, with high and stable performance being achievedwith the PCA NN\napproach. We show that value-at-risk estimation errors are reduced when\ntheproposed anomaly detection model is used with a basic imputation approach to\ncorrect the anomaly.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.11686v2"
    },
    {
        "title": "Forecasting Cryptocurrencies Log-Returns: a LASSO-VAR and Sentiment\n  Approach",
        "authors": [
            "Federico D'Amario",
            "Milos Ciganovic"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Cryptocurrencies have become a trendy topic recently, primarily due to their\ndisruptive potential and reports of unprecedented returns. In addition,\nacademics increasingly acknowledge the predictive power of Social Media in many\nfields and, more specifically, for financial markets and economics. In this\npaper, we leverage the predictive power of Twitter and Reddit sentiment\ntogether with Google Trends indexes and volume to forecast the log returns of\nten cryptocurrencies. Specifically, we consider $Bitcoin$, $Ethereum$,\n$Tether$, $Binance Coin$, $Litecoin$, $Enjin Coin$, $Horizen$, $Namecoin$,\n$Peercoin$, and $Feathercoin$. We evaluate the performance of LASSO-VAR using\ndaily data from January 2018 to January 2022. In a 30 days recursive forecast,\nwe can retrieve the correct direction of the actual series more than 50% of the\ntime. We compare this result with the main benchmarks, and we see a 10%\nimprovement in Mean Directional Accuracy (MDA). The use of sentiment and\nattention variables as predictors increase significantly the forecast accuracy\nin terms of MDA but not in terms of Root Mean Squared Errors. We perform a\nGranger causality test using a post-double LASSO selection for high-dimensional\nVARs. Results show no \"causality\" from Social Media sentiment to\ncryptocurrencies returns\n",
        "pdf_link": "http://arxiv.org/pdf/2210.00883v1"
    },
    {
        "title": "New financial ratios based on the compositional data methodology",
        "authors": [
            "Salvador Linares-Mustarós",
            "Maria Àngels Farreras-Noguer",
            "Núria Arimany-Serrat",
            "Germà Coenders"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Due to their type of mathematical construction, the use of standard financial\nratios in studies analysing the financial health of a group of firms leads to a\nseries of statistical problems that can invalidate the results obtained. These\nproblems are originated by the asymmetry of financial ratios. The present\narticle justifies the use of a new methodology using compositional data (CoDa)\nto analyse the financial statements of a sector, improving analyses using\nconventional ratios since the new methodology enables statistical techniques to\nbe applied without encountering any serious drawbacks such as skewness and\noutliers, and without the results depending on the arbitrary choice as to which\nof the accounting figures is the numerator of the ratio and which is the\ndenominator. An example with data of the wine sector is provided. The results\nshow that when using CoDa, outliers and skewness are much reduced and results\nare invariant to numerator and denominator permutation.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.11138v1"
    },
    {
        "title": "Modelling the Bitcoin prices and the media attention to Bitcoin via the\n  jump-type processes",
        "authors": [
            "Ekaterina Morozova",
            "Vladimir Panov"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In this paper, we present a new bivariate model for the joint description of\nthe Bitcoin prices and the media attention to Bitcoin. Our model is based on\nthe class of the L\\'evy processes and is able to realistically reproduce the\njump-type dynamics of the considered time series. We focus on the low-frequency\nsetup, which is for the L\\'evy - based models essentially more difficult than\nthe high-frequency case. We design a semiparametric estimation procedure for\nthe statistical inference on the parameters and the L\\'evy measures of the\nconsidered processes. We show that the dynamics of the market attention can be\neffectively modelled by the L\\'evy processes with finite L\\'evy measures, and\npropose a data-driven procedure for the description of the Bitcoin prices.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.13824v1"
    },
    {
        "title": "The Art NFTs and Their Marketplaces",
        "authors": [
            "Lanqing Du",
            "Michelle Kim",
            "Jinwook Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Non-Fungible Tokens (NFTs) are crypto assets with a unique digital identifier\nfor ownership, powered by blockchain technology. Technically speaking, anything\ndigital could be minted and sold as an NFT, which provides proof of ownership\nand authenticity of a digital file. For this reason, it helps us distinguish\nbetween the originals and their copies, making it possible to trade them. This\npaper focuses on art NFTs that change how artists can sell their products. It\nalso changes how the art trade market works since NFT technology cuts out the\nmiddleman. Recently, the utility of NFTs has become an essential issue in the\nNFT ecosystem, which refers to the owners' usefulness, profitability, and\nbenefits. Using recent major art NFT marketplace datasets, we summarize and\ninterpret the current market trends and patterns in a way that brings insight\ninto the future art market. Numerical examples are presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.14942v1"
    },
    {
        "title": "Monitoring the Dynamic Networks of Stock Returns",
        "authors": [
            "Elena Farahbakhsh Touli",
            "Hoang Nguyen",
            "Olha Bodnar"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In this paper, we study the connection between the companies in the Swedish\ncapital market. We consider 28 companies included in the determination of the\nmarket index OMX30. The network structure of the market is constructed using\ndifferent methods to determine the distance between the companies. We use\nhierarchical clustering methods to find the relation among the companies in\neach window. Next, we obtain one-dimensional time series of the distances\nbetween the clustering trees that reflect the changes in the relationship\nbetween the companies in the market over time. The method of statistical\nprocess control, namely the Shewhart control chart, is applied to those time\nseries to detect abnormal changes in the financial market.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.16679v1"
    },
    {
        "title": "Inflexible Multi-Asset Hedging of incomplete market",
        "authors": [
            "Ruochen Xiao",
            "Qiaochu Feng",
            "Ruxin Deng"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Models trained under assumptions in the complete market usually don't take\neffect in the incomplete market. This paper solves the hedging problem in\nincomplete market with three sources of incompleteness: risk factor,\nilliquidity, and discrete transaction dates. A new jump-diffusion model is\nproposed to describe stochastic asset prices. Three neutral networks, including\nRNN, LSTM, Mogrifier-LSTM are used to attain hedging strategies with MSE Loss\nand Huber Loss implemented and compared.As a result, Mogrifier-LSTM is the\nfastest model with the best results under MSE and Huber Loss.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00948v2"
    },
    {
        "title": "Evaluating Impact of Social Media Posts by Executives on Stock Prices",
        "authors": [
            "Anubhav Sarkar",
            "Swagata Chakraborty",
            "Sohom Ghosh",
            "Sudip Kumar Naskar"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Predicting stock market movements has always been of great interest to\ninvestors and an active area of research. Research has proven that popularity\nof products is highly influenced by what people talk about. Social media like\nTwitter, Reddit have become hotspots of such influences. This paper\ninvestigates the impact of social media posts on close price prediction of\nstocks using Twitter and Reddit posts. Our objective is to integrate sentiment\nof social media data with historical stock data and study its effect on closing\nprices using time series models. We carried out rigorous experiments and deep\nanalysis using multiple deep learning based models on different datasets to\nstudy the influence of posts by executives and general people on the close\nprice. Experimental results on multiple stocks (Apple and Tesla) and\ndecentralised currencies (Bitcoin and Ethereum) consistently show improvements\nin prediction on including social media data and greater improvements on\nincluding executive posts.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.01287v2"
    },
    {
        "title": "Efficient Integration of Multi-Order Dynamics and Internal Dynamics in\n  Stock Movement Prediction",
        "authors": [
            "Thanh Trung Huynh",
            "Minh Hieu Nguyen",
            "Thanh Tam Nguyen",
            "Phi Le Nguyen",
            "Matthias Weidlich",
            "Quoc Viet Hung Nguyen",
            "Karl Aberer"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Advances in deep neural network (DNN) architectures have enabled new\nprediction techniques for stock market data. Unlike other multivariate\ntime-series data, stock markets show two unique characteristics: (i)\n\\emph{multi-order dynamics}, as stock prices are affected by strong\nnon-pairwise correlations (e.g., within the same industry); and (ii)\n\\emph{internal dynamics}, as each individual stock shows some particular\nbehaviour. Recent DNN-based methods capture multi-order dynamics using\nhypergraphs, but rely on the Fourier basis in the convolution, which is both\ninefficient and ineffective. In addition, they largely ignore internal dynamics\nby adopting the same model for each stock, which implies a severe information\nloss.\n  In this paper, we propose a framework for stock movement prediction to\novercome the above issues. Specifically, the framework includes temporal\ngenerative filters that implement a memory-based mechanism onto an LSTM network\nin an attempt to learn individual patterns per stock. Moreover, we employ\nhypergraph attentions to capture the non-pairwise correlations. Here, using the\nwavelet basis instead of the Fourier basis, enables us to simplify the message\npassing and focus on the localized convolution. Experiments with US market data\nover six years show that our framework outperforms state-of-the-art methods in\nterms of profit and stability. Our source code and data are available at\n\\url{https://github.com/thanhtrunghuynh93/estimate}.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.07400v2"
    },
    {
        "title": "DSLOB: A Synthetic Limit Order Book Dataset for Benchmarking Forecasting\n  Algorithms under Distributional Shift",
        "authors": [
            "Defu Cao",
            "Yousef El-Laham",
            "Loc Trinh",
            "Svitlana Vyetrenko",
            "Yan Liu"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  In electronic trading markets, limit order books (LOBs) provide information\nabout pending buy/sell orders at various price levels for a given security.\nRecently, there has been a growing interest in using LOB data for resolving\ndownstream machine learning tasks (e.g., forecasting). However, dealing with\nout-of-distribution (OOD) LOB data is challenging since distributional shifts\nare unlabeled in current publicly available LOB datasets. Therefore, it is\ncritical to build a synthetic LOB dataset with labeled OOD samples serving as a\ntestbed for developing models that generalize well to unseen scenarios. In this\nwork, we utilize a multi-agent market simulator to build a synthetic LOB\ndataset, named DSLOB, with and without market stress scenarios, which allows\nfor the design of controlled distributional shift benchmarking. Using the\nproposed synthetic dataset, we provide a holistic analysis on the forecasting\nperformance of three different state-of-the-art forecasting methods. Our\nresults reflect the need for increased researcher efforts to develop algorithms\nwith robustness to distributional shifts in high-frequency time series data.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.11513v1"
    },
    {
        "title": "Understanding Cryptocoins Trends Correlations",
        "authors": [
            "Pasquale De Rosa",
            "Valerio Schiavoni"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Crypto-coins (also known as cryptocurrencies) are tradable digital assets.\nNotable examples include Bitcoin, Ether and Litecoin. Ownerships of cryptocoins\nare registered on distributed ledgers (i.e., blockchains). Secure encryption\ntechniques guarantee the security of the transactions (transfers of coins\nacross owners), registered into the ledger. Cryptocoins are exchanged for\nspecific trading prices. While history has shown the extreme volatility of such\ntrading prices across all different sets of crypto-assets, it remains unclear\nwhat and if there are tight relations between the trading prices of different\ncryptocoins. Major coin exchanges (i.e., Coinbase) provide trend correlation\nindicators to coin owners, suggesting possible acquisitions or sells. However,\nthese correlations remain largely unvalidated. In this paper, we shed lights on\nthe trend correlations across a large variety of cryptocoins, by investigating\ntheir coin-price correlation trends over a period of two years. Our\nexperimental results suggest strong correlation patterns between main coins\n(Ethereum, Bitcoin) and alt-coins. We believe our study can support forecasting\ntechniques for time-series modeling in the context of crypto-coins. We release\nour dataset and code to reproduce our analysis to the research community.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.01267v1"
    },
    {
        "title": "A machine learning approach to support decision in insider trading\n  detection",
        "authors": [
            "Piero Mazzarisi",
            "Adele Ravagnani",
            "Paola Deriu",
            "Fabrizio Lillo",
            "Francesca Medda",
            "Antonio Russo"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Identifying market abuse activity from data on investors' trading activity is\nvery challenging both for the data volume and for the low signal to noise\nratio. Here we propose two complementary unsupervised machine learning methods\nto support market surveillance aimed at identifying potential insider trading\nactivities. The first one uses clustering to identify, in the vicinity of a\nprice sensitive event such as a takeover bid, discontinuities in the trading\nactivity of an investor with respect to his/her own past trading history and on\nthe present trading activity of his/her peers. The second unsupervised approach\naims at identifying (small) groups of investors that act coherently around\nprice sensitive events, pointing to potential insider rings, i.e. a group of\nsynchronised traders displaying strong directional trading in rewarding\nposition in a period before the price sensitive event. As a case study, we\napply our methods to investor resolved data of Italian stocks around takeover\nbids.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.05912v1"
    },
    {
        "title": "Multi-step-ahead Stock Price Prediction Using Recurrent Fuzzy Neural\n  Network and Variational Mode Decomposition",
        "authors": [
            "Hamid Nasiri",
            "Mohammad Mehdi Ebadzadeh"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Financial time series prediction, a growing research topic, has attracted\nconsiderable interest from scholars, and several approaches have been\ndeveloped. Among them, decomposition-based methods have achieved promising\nresults. Most decomposition-based methods approximate a single function, which\nis insufficient for obtaining accurate results. Moreover, most existing\nresearches have concentrated on one-step-ahead forecasting that prevents stock\nmarket investors from arriving at the best decisions for the future. This study\nproposes two novel methods for multi-step-ahead stock price prediction based on\nthe issues outlined. DCT-MFRFNN, a method based on discrete cosine transform\n(DCT) and multi-functional recurrent fuzzy neural network (MFRFNN), uses DCT to\nreduce fluctuations in the time series and simplify its structure and MFRFNN to\npredict the stock price. VMD-MFRFNN, an approach based on variational mode\ndecomposition (VMD) and MFRFNN, brings together their advantages. VMD-MFRFNN\nconsists of two phases. The input signal is decomposed to several IMFs using\nVMD in the decomposition phase. In the prediction and reconstruction phase,\neach of the IMFs is given to a separate MFRFNN for prediction, and predicted\nsignals are summed to reconstruct the output. Three financial time series,\nincluding Hang Seng Index (HSI), Shanghai Stock Exchange (SSE), and Standard &\nPoor's 500 Index (SPX), are used for the evaluation of the proposed methods.\nExperimental results indicate that VMD-MFRFNN surpasses other state-of-the-art\nmethods. VMD-MFRFNN, on average, shows 35.93%, 24.88%, and 34.59% decreases in\nRMSE from the second-best model for HSI, SSE, and SPX, respectively. Also,\nDCT-MFRFNN outperforms MFRFNN in all experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.14687v1"
    },
    {
        "title": "A Comparative Predicting Stock Prices using Heston and Geometric\n  Brownian Motion Models",
        "authors": [
            "H. T. Shehzad",
            "M. A. Anwar",
            "M. Razzaq"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper presents a novel approach to predicting stock prices using\ntechnical analysis. By utilizing Ito's lemma and Euler-Maruyama methods, the\nresearchers develop Heston and Geometric Brownian Motion models that take into\naccount volatility, interest rate, and historical stock prices to generate\npredictions. The results of the study demonstrate that these models are\neffective in accurately predicting stock prices and outperform commonly used\nstatistical indicators. The authors conclude that this technical analysis-based\nmethod offers a promising solution for stock market prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.07796v1"
    },
    {
        "title": "Application of supervised learning models in the Chinese futures market",
        "authors": [
            "Fuquan Tang"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Based on the characteristics of the Chinese futures market, this paper builds\na supervised learning model to predict the trend of futures prices and then\ndesigns a trading strategy based on the prediction results. The Precision,\nRecall and F1-score of the classification problem show that our model can meet\nthe accuracy requirements for the classification of futures price movements in\nterms of test data. The backtest results show that our trading system has an\nupward trending return curve with low capital retracement.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.04581v1"
    },
    {
        "title": "Stock Trend Prediction: A Semantic Segmentation Approach",
        "authors": [
            "Shima Nabiee",
            "Nader Bagherzadeh"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Market financial forecasting is a trending area in deep learning. Deep\nlearning models are capable of tackling the classic challenges in stock market\ndata, such as its extremely complicated dynamics as well as long-term temporal\ncorrelation. To capture the temporal relationship among these time series,\nrecurrent neural networks are employed. However, it is difficult for recurrent\nmodels to learn to keep track of long-term information. Convolutional Neural\nNetworks have been utilized to better capture the dynamics and extract features\nfor both short- and long-term forecasting. However, semantic segmentation and\nits well-designed fully convolutional networks have never been studied for\ntime-series dense classification. We present a novel approach to predict\nlong-term daily stock price change trends with fully 2D-convolutional\nencoder-decoders. We generate input frames with daily prices for a time-frame\nof T days. The aim is to predict future trends by pixel-wise classification of\nthe current price frame. We propose a hierarchical CNN structure to encode\nmultiple price frames to multiscale latent representation in parallel using\nAtrous Spatial Pyramid Pooling blocks and take that temporal coarse feature\nstacks into account in the decoding stages. Our hierarchical structure of CNNs\nmakes it capable of capturing both long and short-term temporal relationships\neffectively. The effect of increasing the input time horizon via incrementing\nparallel encoders has been studied with interesting and substantial changes in\nthe output segmentation masks. We achieve overall accuracy and AUC of %78.18\nand 0.88 for joint trend prediction over the next 20 days, surpassing other\nsemantic segmentation approaches. We compared our proposed model with several\ndeep models specifically designed for technical analysis and found that for\ndifferent output horizons, our proposed models outperformed other models.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09323v1"
    },
    {
        "title": "Cryptocurrency Price Prediction using Twitter Sentiment Analysis",
        "authors": [
            "Haritha GB",
            "Sahana N. B"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The cryptocurrency ecosystem has been the centre of discussion on many social\nmedia platforms, following its noted volatility and varied opinions. Twitter is\nrapidly being utilised as a news source and a medium for bitcoin discussion.\nOur algorithm seeks to use historical prices and sentiment of tweets to\nforecast the price of Bitcoin. In this study, we develop an end-to-end model\nthat can forecast the sentiment of a set of tweets (using a Bidirectional\nEncoder Representations from Transformers - based Neural Network Model) and\nforecast the price of Bitcoin (using Gated Recurrent Unit) using the predicted\nsentiment and other metrics like historical cryptocurrency price data, tweet\nvolume, a user's following, and whether or not a user is verified. The\nsentiment prediction gave a Mean Absolute Percentage Error of 9.45%, an average\nof real-time data, and test data. The mean absolute percent error for the price\nprediction was 3.6%.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.09397v1"
    },
    {
        "title": "Forecasting Large Realized Covariance Matrices: The Benefits of Factor\n  Models and Shrinkage",
        "authors": [
            "Rafael Alves",
            "Diego S. de Brito",
            "Marcelo C. Medeiros",
            "Ruy M. Ribeiro"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We propose a model to forecast large realized covariance matrices of returns,\napplying it to the constituents of the S\\&P 500 daily. To address the curse of\ndimensionality, we decompose the return covariance matrix using standard\nfirm-level factors (e.g., size, value, and profitability) and use sectoral\nrestrictions in the residual covariance matrix. This restricted model is then\nestimated using vector heterogeneous autoregressive (VHAR) models with the\nleast absolute shrinkage and selection operator (LASSO). Our methodology\nimproves forecasting precision relative to standard benchmarks and leads to\nbetter estimates of minimum variance portfolios.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.16151v1"
    },
    {
        "title": "Why Topological Data Analysis Detects Financial Bubbles?",
        "authors": [
            "Samuel W. Akingbade",
            "Marian Gidea",
            "Matteo Manzi",
            "Vahid Nateghi"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We present a heuristic argument for the propensity of Topological Data\nAnalysis (TDA) to detect early warning signals of critical transitions in\nfinancial time series. Our argument is based on the Log-Periodic Power Law\nSingularity (LPPLS) model, which characterizes financial bubbles as\nsuper-exponential growth (or decay) of an asset price superimposed with\noscillations increasing in frequency and decreasing in amplitude when\napproaching a critical transition (tipping point). We show that whenever the\nLPPLS model is fitting with the data, TDA generates early warning signals. As\nan application, we illustrate this approach on a sample of positive and\nnegative bubbles in the Bitcoin historical price.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.06877v1"
    },
    {
        "title": "Online Ensemble of Models for Optimal Predictive Performance with\n  Applications to Sector Rotation Strategy",
        "authors": [
            "Jiaju Miao",
            "Pawel Polak"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Asset-specific factors are commonly used to forecast financial returns and\nquantify asset-specific risk premia. Using various machine learning models, we\ndemonstrate that the information contained in these factors leads to even\nlarger economic gains in terms of forecasts of sector returns and the\nmeasurement of sector-specific risk premia. To capitalize on the strong\npredictive results of individual models for the performance of different\nsectors, we develop a novel online ensemble algorithm that learns to optimize\npredictive performance. The algorithm continuously adapts over time to\ndetermine the optimal combination of individual models by solely analyzing\ntheir most recent prediction performance. This makes it particularly suited for\ntime series problems, rolling window backtesting procedures, and systems of\npotentially black-box models. We derive the optimal gain function, express the\ncorresponding regret bounds in terms of the out-of-sample R-squared measure,\nand derive optimal learning rate for the algorithm. Empirically, the new\nensemble outperforms both individual machine learning models and their simple\naverages in providing better measurements of sector risk premia. Moreover, it\nallows for performance attribution of different factors across various sectors,\nwithout conditioning on a specific model. Finally, by utilizing monthly\npredictions from our ensemble, we develop a sector rotation strategy that\nsignificantly outperforms the market. The strategy remains robust against\nvarious financial factors, periods of financial distress, and conservative\ntransaction costs. Notably, the strategy's efficacy persists over time,\nexhibiting consistent improvement throughout an extended backtesting period and\nyielding substantial profits during the economic turbulence of the COVID-19\npandemic.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09947v1"
    },
    {
        "title": "Recurrent neural network based parameter estimation of Hawkes model on\n  high-frequency financial data",
        "authors": [
            "Kyungsub Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This study examines the use of a recurrent neural network for estimating the\nparameters of a Hawkes model based on high-frequency financial data, and\nsubsequently, for computing volatility. Neural networks have shown promising\nresults in various fields, and interest in finance is also growing. Our\napproach demonstrates significantly faster computational performance compared\nto traditional maximum likelihood estimation methods while yielding comparable\naccuracy in both simulation and empirical studies. Furthermore, we demonstrate\nthe application of this method for real-time volatility measurement, enabling\nthe continuous estimation of financial volatility as new price data keeps\ncoming from the market.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.11883v1"
    },
    {
        "title": "Deep learning models for price forecasting of financial time series: A\n  review of recent advancements: 2020-2022",
        "authors": [
            "Cheng Zhang",
            "Nilam Nur Amir Sjarif",
            "Roslina Ibrahim"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Accurately predicting the prices of financial time series is essential and\nchallenging for the financial sector. Owing to recent advancements in deep\nlearning techniques, deep learning models are gradually replacing traditional\nstatistical and machine learning models as the first choice for price\nforecasting tasks. This shift in model selection has led to a notable rise in\nresearch related to applying deep learning models to price forecasting,\nresulting in a rapid accumulation of new knowledge. Therefore, we conducted a\nliterature review of relevant studies over the past three years with a view to\naiding researchers and practitioners in the field. This review delves deeply\ninto deep learning-based forecasting models, presenting information on model\narchitectures, practical applications, and their respective advantages and\ndisadvantages. In particular, detailed information is provided on advanced\nmodels for price forecasting, such as Transformers, generative adversarial\nnetworks (GANs), graph neural networks (GNNs), and deep quantum neural networks\n(DQNNs). The present contribution also includes potential directions for future\nresearch, such as examining the effectiveness of deep learning models with\ncomplex structures for price forecasting, extending from point prediction to\ninterval prediction using deep learning models, scrutinising the reliability\nand validity of decomposition ensembles, and exploring the influence of data\nvolume on model performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04811v2"
    },
    {
        "title": "What is mature and what is still emerging in the cryptocurrency market?",
        "authors": [
            "Stanisław Drożdż",
            "Jarosław Kwapień",
            "Marcin Wątorek"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In relation to the traditional financial markets, the cryptocurrency market\nis a recent invention and the trading dynamics of all its components are\nreadily recorded and stored. This fact opens up a unique opportunity to follow\nthe multidimensional trajectory of its development since inception up to the\npresent time. Several main characteristics commonly recognized as financial\nstylized facts of mature markets were quantitatively studied here. In\nparticular, it is shown that the return distributions, volatility clustering\neffects, and even temporal multifractal correlations for a few\nhighest-capitalization cryptocurrencies largely follow those of the\nwell-established financial markets. The smaller cryptocurrencies are somewhat\ndeficient in this regard, however. They are also not as highly cross-correlated\namong themselves and with other financial markets as the large\ncryptocurrencies. Quite generally, the volume V impact on price changes R\nappears to be much stronger on the cryptocurrency market than in the mature\nstock markets, and scales as $R(V) \\sim V^{\\alpha}$ with $\\alpha \\gtrsim 1$.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.05751v1"
    },
    {
        "title": "Copula Variational LSTM for High-dimensional Cross-market Multivariate\n  Dependence Modeling",
        "authors": [
            "Jia Xu",
            "Longbing Cao"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We address an important yet challenging problem - modeling high-dimensional\ndependencies across multivariates such as financial indicators in heterogeneous\nmarkets. In reality, a market couples and influences others over time, and the\nfinancial variables of a market are also coupled. We make the first attempt to\nintegrate variational sequential neural learning with copula-based dependence\nmodeling to characterize both temporal observable and latent variable-based\ndependence degrees and structures across non-normal multivariates. Our\nvariational neural network WPVC-VLSTM models variational sequential dependence\ndegrees and structures across multivariate time series by variational long\nshort-term memory networks and regular vine copula. The regular vine copula\nmodels nonnormal and long-range distributional couplings across multiple\ndynamic variables. WPVC-VLSTM is verified in terms of both technical\nsignificance and portfolio forecasting performance. It outperforms benchmarks\nincluding linear models, stochastic volatility models, deep neural networks,\nand variational recurrent networks in cross-market portfolio forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.08778v1"
    },
    {
        "title": "Accounting statement analysis at industry level. A gentle introduction\n  to the compositional approach",
        "authors": [
            "Germà Coenders",
            "Núria Arimany Serrat"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Compositional data are contemporarily defined as positive vectors, the ratios\namong whose elements are of interest to the researcher. Financial statement\nanalysis by means of accounting ratios a.k.a. financial ratios fulfils this\ndefinition to the letter. Compositional data analysis solves the major problems\nin statistical analysis of standard financial ratios at industry level, such as\nskewness, non-normality, non-linearity, outliers, and dependence of the results\non the choice of which accounting figure goes to the numerator and to the\ndenominator of the ratio. Despite this, compositional applications to financial\nstatement analysis are still rare. In this article, we present some\ntransformations within compositional data analysis that are particularly useful\nfor financial statement analysis. We show how to compute industry or\nsub-industry means of standard financial ratios from a compositional\nperspective by means of geometric means. We show how to visualise firms in an\nindustry with a compositional principal-component-analysis biplot; how to\nclassify them into homogeneous financial performance profiles with\ncompositional cluster analysis; and how to introduce financial ratios as\nvariables in a statistical model, for instance to relate financial performance\nand firm characteristics with compositional regression models. We show an\napplication to the accounting statements of Spanish wineries using the\ndecomposition of return on equity by means of DuPont analysis, and a\nstep-by-step tutorial to the compositional freeware CoDaPack.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.16842v5"
    },
    {
        "title": "Liquidity takers behavior representation through a contrastive learning\n  approach",
        "authors": [
            "Ruihua Ruan",
            "Emmanuel Bacry",
            "Jean-François Muzy"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Thanks to the access to the labeled orders on the CAC40 data from Euronext,\nwe are able to analyze agents' behaviors in the market based on their placed\norders. In this study, we construct a self-supervised learning model using\ntriplet loss to effectively learn the representation of agent market orders. By\nacquiring this learned representation, various downstream tasks become\nfeasible. In this work, we utilize the K-means clustering algorithm on the\nlearned representation vectors of agent orders to identify distinct behavior\ntypes within each cluster.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.05987v2"
    },
    {
        "title": "FinGPT: Open-Source Financial Large Language Models",
        "authors": [
            "Hongyang Yang",
            "Xiao-Yang Liu",
            "Christina Dan Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Large language models (LLMs) have shown the potential of revolutionizing\nnatural language processing tasks in diverse domains, sparking great interest\nin finance. Accessing high-quality financial data is the first challenge for\nfinancial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken\nadvantage of their unique data accumulation, such privileged access calls for\nan open-source alternative to democratize Internet-scale financial data.\n  In this paper, we present an open-source large language model, FinGPT, for\nthe finance sector. Unlike proprietary models, FinGPT takes a data-centric\napproach, providing researchers and practitioners with accessible and\ntransparent resources to develop their FinLLMs. We highlight the importance of\nan automatic data curation pipeline and the lightweight low-rank adaptation\ntechnique in building FinGPT. Furthermore, we showcase several potential\napplications as stepping stones for users, such as robo-advising, algorithmic\ntrading, and low-code development. Through collaborative efforts within the\nopen-source AI4Finance community, FinGPT aims to stimulate innovation,\ndemocratize FinLLMs, and unlock new opportunities in open finance. Two\nassociated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT}\nand \\url{https://github.com/AI4Finance-Foundation/FinNLP}\n",
        "pdf_link": "http://arxiv.org/pdf/2306.06031v1"
    },
    {
        "title": "Improved Financial Forecasting via Quantum Machine Learning",
        "authors": [
            "Sohum Thakkar",
            "Skander Kazdaghli",
            "Natansh Mathur",
            "Iordanis Kerenidis",
            "André J. Ferreira-Martins",
            "Samurai Brito"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Quantum algorithms have the potential to enhance machine learning across a\nvariety of domains and applications. In this work, we show how quantum machine\nlearning can be used to improve financial forecasting. First, we use classical\nand quantum Determinantal Point Processes to enhance Random Forest models for\nchurn prediction, improving precision by almost 6%. Second, we design quantum\nneural network architectures with orthogonal and compound layers for credit\nrisk assessment, which match classical performance with significantly fewer\nparameters. Our results demonstrate that leveraging quantum ideas can\neffectively enhance the performance of machine learning, both today as\nquantum-inspired classical ML solutions, and even more in the future, with the\nadvent of better quantum hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12965v2"
    },
    {
        "title": "Stock Price Prediction using Dynamic Neural Networks",
        "authors": [
            "David Noel"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper will analyze and implement a time series dynamic neural network to\npredict daily closing stock prices. Neural networks possess unsurpassed\nabilities in identifying underlying patterns in chaotic, non-linear, and\nseemingly random data, thus providing a mechanism to predict stock price\nmovements much more precisely than many current techniques. Contemporary\nmethods for stock analysis, including fundamental, technical, and regression\ntechniques, are conversed and paralleled with the performance of neural\nnetworks. Also, the Efficient Market Hypothesis (EMH) is presented and\ncontrasted with Chaos theory using neural networks. This paper will refute the\nEMH and support Chaos theory. Finally, recommendations for using neural\nnetworks in stock price prediction will be presented.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12969v1"
    },
    {
        "title": "Estimating the roughness exponent of stochastic volatility from discrete\n  observations of the integrated variance",
        "authors": [
            "Xiyue Han",
            "Alexander Schied"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We consider the problem of estimating the roughness of the volatility process\nin a stochastic volatility model that arises as a nonlinear function of\nfractional Brownian motion with drift. To this end, we introduce a new\nestimator that measures the so-called roughness exponent of a continuous\ntrajectory, based on discrete observations of its antiderivative. The estimator\nhas a very simple form and can be computed with great efficiency on large data\nsets. It is not derived from distributional assumptions but from strictly\npathwise considerations. We provide conditions on the underlying trajectory\nunder which our estimator converges in a strictly pathwise sense. Then we\nverify that these conditions are satisfied by almost every sample path of\nfractional Brownian motion (with drift). As a consequence, we obtain strong\nconsistency theorems in the context of a large class of rough volatility\nmodels, such as the rough fractional volatility model and the rough Bergomi\nmodel. We also demonstrate that our estimator is robust with respect to proxy\nerrors between the integrated and realized variance, and that it can be applied\nto estimate the roughness exponent directly from the price trajectory.\nNumerical simulations show that our estimation procedure performs well after\npassing to a scale-invariant modification of our estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.02582v3"
    },
    {
        "title": "Thailand Asset Value Estimation Using Aerial or Satellite Imagery",
        "authors": [
            "Supawich Puengdang",
            "Worawate Ausawalaithong",
            "Phiratath Nopratanawong",
            "Narongdech Keeratipranon",
            "Chayut Wongkamthong"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Real estate is a critical sector in Thailand's economy, which has led to a\ngrowing demand for a more accurate land price prediction approach. Traditional\nmethods of land price prediction, such as the weighted quality score (WQS), are\nlimited due to their reliance on subjective criteria and their lack of\nconsideration for spatial variables. In this study, we utilize aerial or\nsatellite imageries from Google Map API to enhance land price prediction models\nfrom the dataset provided by Kasikorn Business Technology Group (KBTG). We\npropose a similarity-based asset valuation model that uses a Siamese-inspired\nNeural Network with pretrained EfficientNet architecture to assess the\nsimilarity between pairs of lands. By ensembling deep learning and tree-based\nmodels, we achieve an area under the ROC curve (AUC) of approximately 0.81,\noutperforming the baseline model that used only tabular data. The appraisal\nprices of nearby lands with similarity scores higher than a predefined\nthreshold were used for weighted averaging to predict the reasonable price of\nthe land in question. At 20\\% mean absolute percentage error (MAPE), we improve\nthe recall from 59.26\\% to 69.55\\%, indicating a more accurate and reliable\napproach to predicting land prices. Our model, which is empowered by a more\ncomprehensive view of land use and environmental factors from aerial or\nsatellite imageries, provides a more precise, data-driven, and adaptive\napproach for land valuation in Thailand.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08650v2"
    },
    {
        "title": "Exploring the Bitcoin Mesoscale",
        "authors": [
            "Nicolò Vallarano",
            "Tiziano Squartini",
            "Claudio J. Tessone"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The open availability of the entire history of the Bitcoin transactions opens\nup the possibility to study this system at an unprecedented level of detail.\nThis contribution is devoted to the analysis of the mesoscale structural\nproperties of the Bitcoin User Network (BUN), across its entire history (i.e.\nfrom 2009 to 2017). What emerges from our analysis is that the BUN is\ncharacterized by a core-periphery structure a deeper analysis of which reveals\na certain degree of bow-tieness (i.e. the presence of a Strongly-Connected\nComponent, an IN- and an OUT-component together with some tendrils attached to\nthe IN-component). Interestingly, the evolution of the BUN structural\norganization experiences fluctuations that seem to be correlated with the\npresence of bubbles, i.e. periods of price surge and decline observed\nthroughout the entire Bitcoin history: our results, thus, further confirm the\ninterplay between structural quantities and price movements observed in\nprevious analyses.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14409v1"
    },
    {
        "title": "A Common Shock Model for multidimensional electricity intraday price\n  modelling with application to battery valuation",
        "authors": [
            "Thomas Deschatre",
            "Xavier Warin"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In this paper, we propose a multidimensional statistical model of intraday\nelectricity prices at the scale of the trading session, which allows all\nproducts to be simulated simultaneously. This model, based on Poisson measures\nand inspired by the Common Shock Poisson Model, reproduces the Samuelson effect\n(intensity and volatility increases as time to maturity decreases). It also\nreproduces the price correlation structure, highlighted here in the data, which\ndecreases as two maturities move apart. This model has only three parameters\nthat can be estimated using a moment method that we propose here. We\ndemonstrate the usefulness of the model on a case of storage valuation by\ndynamic programming over a trading session.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.16619v1"
    },
    {
        "title": "Methods for Acquiring and Incorporating Knowledge into Stock Price\n  Prediction: A Survey",
        "authors": [
            "Liping Wang",
            "Jiawei Li",
            "Lifan Zhao",
            "Zhizhuo Kou",
            "Xiaohan Wang",
            "Xinyi Zhu",
            "Hao Wang",
            "Yanyan Shen",
            "Lei Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Predicting stock prices presents a challenging research problem due to the\ninherent volatility and non-linear nature of the stock market. In recent years,\nknowledge-enhanced stock price prediction methods have shown groundbreaking\nresults by utilizing external knowledge to understand the stock market. Despite\nthe importance of these methods, there is a scarcity of scholarly works that\nsystematically synthesize previous studies from the perspective of external\nknowledge types. Specifically, the external knowledge can be modeled in\ndifferent data structures, which we group into non-graph-based formats and\ngraph-based formats: 1) non-graph-based knowledge captures contextual\ninformation and multimedia descriptions specifically associated with an\nindividual stock; 2) graph-based knowledge captures interconnected and\ninterdependent information in the stock market. This survey paper aims to\nprovide a systematic and comprehensive description of methods for acquiring\nexternal knowledge from various unstructured data sources and then\nincorporating it into stock price prediction models. We also explore fusion\nmethods for combining external knowledge with historical price features.\nMoreover, this paper includes a compilation of relevant datasets and delves\ninto potential future research directions in this domain.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.04947v1"
    },
    {
        "title": "AI-Assisted Investigation of On-Chain Parameters: Risky Cryptocurrencies\n  and Price Factors",
        "authors": [
            "Abdulrezzak Zekiye",
            "Semih Utku",
            "Fadi Amroush",
            "Oznur Ozkasap"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Cryptocurrencies have become a popular and widely researched topic of\ninterest in recent years for investors and scholars. In order to make informed\ninvestment decisions, it is essential to comprehend the factors that impact\ncryptocurrency prices and to identify risky cryptocurrencies. This paper\nfocuses on analyzing historical data and using artificial intelligence\nalgorithms on on-chain parameters to identify the factors affecting a\ncryptocurrency's price and to find risky cryptocurrencies. We conducted an\nanalysis of historical cryptocurrencies' on-chain data and measured the\ncorrelation between the price and other parameters. In addition, we used\nclustering and classification in order to get a better understanding of a\ncryptocurrency and classify it as risky or not. The analysis revealed that a\nsignificant proportion of cryptocurrencies (39%) disappeared from the market,\nwhile only a small fraction (10%) survived for more than 1000 days. Our\nanalysis revealed a significant negative correlation between cryptocurrency\nprice and maximum and total supply, as well as a weak positive correlation\nbetween price and 24-hour trading volume. Moreover, we clustered\ncryptocurrencies into five distinct groups using their on-chain parameters,\nwhich provides investors with a more comprehensive understanding of a\ncryptocurrency when compared to those clustered with it. Finally, by\nimplementing multiple classifiers to predict whether a cryptocurrency is risky\nor not, we obtained the best f1-score of 76% using K-Nearest Neighbor.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08554v1"
    },
    {
        "title": "Combining predictive distributions of electricity prices: Does\n  minimizing the CRPS lead to optimal decisions in day-ahead bidding?",
        "authors": [
            "Weronika Nitka",
            "Rafał Weron"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Probabilistic price forecasting has recently gained attention in power\ntrading because decisions based on such predictions can yield significantly\nhigher profits than those made with point forecasts alone. At the same time,\nmethods are being developed to combine predictive distributions, since no model\nis perfect and averaging generally improves forecasting performance. In this\narticle we address the question of whether using CRPS learning, a novel\nweighting technique minimizing the continuous ranked probability score (CRPS),\nleads to optimal decisions in day-ahead bidding. To this end, we conduct an\nempirical study using hourly day-ahead electricity prices from the German EPEX\nmarket. We find that increasing the diversity of an ensemble can have a\npositive impact on accuracy. At the same time, the higher computational cost of\nusing CRPS learning compared to an equal-weighted aggregation of distributions\nis not offset by higher profits, despite significantly more accurate\npredictions.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.15443v1"
    },
    {
        "title": "Predicting Financial Market Trends using Time Series Analysis and\n  Natural Language Processing",
        "authors": [
            "Ali Asgarov"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Forecasting financial market trends through time series analysis and natural\nlanguage processing poses a complex and demanding undertaking, owing to the\nnumerous variables that can influence stock prices. These variables encompass a\nspectrum of economic and political occurrences, as well as prevailing public\nattitudes. Recent research has indicated that the expression of public\nsentiments on social media platforms such as Twitter may have a noteworthy\nimpact on the determination of stock prices. The objective of this study was to\nassess the viability of Twitter sentiments as a tool for predicting stock\nprices of major corporations such as Tesla, Apple. Our study has revealed a\nrobust association between the emotions conveyed in tweets and fluctuations in\nstock prices. Our findings indicate that positivity, negativity, and\nsubjectivity are the primary determinants of fluctuations in stock prices. The\ndata was analyzed utilizing the Long-Short Term Memory neural network (LSTM)\nmodel, which is currently recognized as the leading methodology for predicting\nstock prices by incorporating Twitter sentiments and historical stock prices\ndata. The models utilized in our study demonstrated a high degree of\nreliability and yielded precise outcomes for the designated corporations. In\nsummary, this research emphasizes the significance of incorporating public\nopinions into the prediction of stock prices. The application of Time Series\nAnalysis and Natural Language Processing methodologies can yield significant\nscientific findings regarding financial market patterns, thereby facilitating\ninformed decision-making among investors. The results of our study indicate\nthat the utilization of Twitter sentiments can serve as a potent instrument for\nforecasting stock prices, and ought to be factored in when formulating\ninvestment strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.00136v1"
    },
    {
        "title": "GPT-InvestAR: Enhancing Stock Investment Strategies through Annual\n  Report Analysis with Large Language Models",
        "authors": [
            "Udit Gupta"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Annual Reports of publicly listed companies contain vital information about\ntheir financial health which can help assess the potential impact on Stock\nprice of the firm. These reports are comprehensive in nature, going up to, and\nsometimes exceeding, 100 pages. Analysing these reports is cumbersome even for\na single firm, let alone the whole universe of firms that exist. Over the\nyears, financial experts have become proficient in extracting valuable\ninformation from these documents relatively quickly. However, this requires\nyears of practice and experience. This paper aims to simplify the process of\nassessing Annual Reports of all the firms by leveraging the capabilities of\nLarge Language Models (LLMs). The insights generated by the LLM are compiled in\na Quant styled dataset and augmented by historical stock price data. A Machine\nLearning model is then trained with LLM outputs as features. The walkforward\ntest results show promising outperformance wrt S&P500 returns. This paper\nintends to provide a framework for future work in this direction. To facilitate\nthis, the code has been released as open source.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.03079v1"
    },
    {
        "title": "Dynamic relationship between XRP price and correlation tensor spectra of\n  the transaction network",
        "authors": [
            "Abhijit Chakraborty",
            "Tetsuo Hatsuda",
            "Yuichi Ikeda"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The emergence of cryptoassets has sparked a paradigm shift in the world of\nfinance and investment, ushering in a new era of digital assets with profound\nimplications for the future of currency and asset management. A recent study\nshowed that during the bubble period around the year, 2018, the price of\ncryptoasset, XRP has a strong anti correlation with the largest singular values\nof the correlation tensors obtained from the weekly XRP transaction networks.\nIn this study, we provide a detailed analysis of the method of correlation\ntensor spectra for XRP transaction networks. We calculate and compare the\ndistribution of the largest singular values of the correlation tensor using the\nrandom matrix theory with the largest singular values of the empirical\ncorrelation tensor. We investigate the correlation between the XRP price and\nthe largest singular values for a period spanning two years. We also uncover\nthe distinct dependence between XRP price and the singular values for bubble\nand non-bubble periods. The significance of time evolution of singular values\nis shown by comparison with the evolution of singular values of the reshuffled\ncorrelation tensor. Furthermore, we identify a set of driver nodes in the\ntransaction networks that drives the market during the bubble period using the\nsingular vectors.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.05935v1"
    },
    {
        "title": "Estimating Stable Fixed Points and Langevin Potentials for Financial\n  Dynamics",
        "authors": [
            "Tobias Wand",
            "Timo Wiedemann",
            "Jan Harren",
            "Oliver Kamps"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The Geometric Brownian Motion (GBM) is a standard model in quantitative\nfinance, but the potential function of its stochastic differential equation\n(SDE) cannot include stable nonzero prices. This article generalises the GBM to\nan SDE with polynomial drift of order q and shows via model selection that q=2\nis most frequently the optimal model to describe the data. Moreover, Markov\nchain Monte Carlo ensembles of the accompanying potential functions show a\nclear and pronounced potential well, indicating the existence of a stable\nprice.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.12082v2"
    },
    {
        "title": "Boosting Stock Price Prediction with Anticipated Macro Policy Changes",
        "authors": [
            "Md Sabbirul Haque",
            "Md Shahedul Amin",
            "Jonayet Miah",
            "Duc Minh Cao",
            "Ashiqul Haque Ahmed"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Prediction of stock prices plays a significant role in aiding the\ndecision-making of investors. Considering its importance, a growing literature\nhas emerged trying to forecast stock prices with improved accuracy. In this\nstudy, we introduce an innovative approach for forecasting stock prices with\ngreater accuracy. We incorporate external economic environment-related\ninformation along with stock prices. In our novel approach, we improve the\nperformance of stock price prediction by taking into account variations due to\nfuture expected macroeconomic policy changes as investors adjust their current\nbehavior ahead of time based on expected future macroeconomic policy changes.\nFurthermore, we incorporate macroeconomic variables along with historical stock\nprices to make predictions. Results from this strongly support the inclusion of\nfuture economic policy changes along with current macroeconomic information. We\nconfirm the supremacy of our method over the conventional approach using\nseveral tree-based machine-learning algorithms. Results are strongly conclusive\nacross various machine learning models. Our preferred model outperforms the\nconventional approach with an RMSE value of 1.61 compared to an RMSE value of\n1.75 from the conventional approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06278v1"
    },
    {
        "title": "A Data-driven Deep Learning Approach for Bitcoin Price Forecasting",
        "authors": [
            "Parth Daxesh Modi",
            "Kamyar Arshi",
            "Pertami J. Kunz",
            "Abdelhak M. Zoubir"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Bitcoin as a cryptocurrency has been one of the most important digital coins\nand the first decentralized digital currency. Deep neural networks, on the\nother hand, has shown promising results recently; however, we require huge\namount of high-quality data to leverage their power. There are some techniques\nsuch as augmentation that can help us with increasing the dataset size, but we\ncannot exploit them on historical bitcoin data. As a result, we propose a\nshallow Bidirectional-LSTM (Bi-LSTM) model, fed with feature engineered data\nusing our proposed method to forecast bitcoin closing prices in a daily time\nframe. We compare the performance with that of other forecasting methods, and\nshow that with the help of the proposed feature engineering method, a shallow\ndeep neural network outperforms other popular price forecasting models.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.06280v1"
    },
    {
        "title": "Multi-Label Topic Model for Financial Textual Data",
        "authors": [
            "Moritz Scherrmann"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper presents a multi-label topic model for financial texts like ad-hoc\nannouncements, 8-K filings, finance related news or annual reports. I train the\nmodel on a new financial multi-label database consisting of 3,044 German ad-hoc\nannouncements that are labeled manually using 20 predefined, economically\nmotivated topics. The best model achieves a macro F1 score of more than 85%.\nTranslating the data results in an English version of the model with similar\nperformance. As application of the model, I investigate differences in stock\nmarket reactions across topics. I find evidence for strong positive or negative\nmarket reactions for some topics, like announcements of new Large Scale\nProjects or Bankruptcy Filings, while I do not observe significant price\neffects for some other topics. Furthermore, in contrast to previous studies,\nthe multi-label structure of the model allows to analyze the effects of\nco-occurring topics on stock market reactions. For many cases, the reaction to\na specific topic depends heavily on the co-occurrence with other topics. For\nexample, if allocated capital from a Seasoned Equity Offering (SEO) is used for\nrestructuring a company in the course of a Bankruptcy Proceeding, the market\nreacts positively on average. However, if that capital is used for covering\nunexpected, additional costs from the development of new drugs, the SEO implies\nnegative reactions on average.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.07598v1"
    },
    {
        "title": "Short-term Volatility Estimation for High Frequency Trades using\n  Gaussian processes (GPs)",
        "authors": [
            "Leonard Mushunje",
            "Maxwell Mashasha",
            "Edina Chandiwana"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  The fundamental theorem behind financial markets is that stock prices are\nintrinsically complex and stochastic. One of the complexities is the volatility\nassociated with stock prices. Volatility is a tendency for prices to change\nunexpectedly [1]. Price volatility is often detrimental to the return\neconomics, and thus, investors should factor it in whenever making investment\ndecisions, choices, and temporal or permanent moves. It is, therefore, crucial\nto make necessary and regular short and long-term stock price volatility\nforecasts for the safety and economics of investors returns. These forecasts\nshould be accurate and not misleading. Different models and methods, such as\nARCH GARCH models, have been intuitively implemented to make such forecasts.\nHowever, such traditional means fail to capture the short-term volatility\nforecasts effectively. This paper, therefore, investigates and implements a\ncombination of numeric and probabilistic models for short-term volatility and\nreturn forecasting for high-frequency trades. The essence is that one-day-ahead\nvolatility forecasts were made with Gaussian Processes (GPs) applied to the\noutputs of a Numerical market prediction (NMP) model. Firstly, the stock price\ndata from NMP was corrected by a GP. Since it is not easy to set price limits\nin a market due to its free nature and randomness, a Censored GP was used to\nmodel the relationship between the corrected stock prices and returns.\nForecasting errors were evaluated using the implied and estimated data.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.10935v1"
    },
    {
        "title": "Deep State-Space Model for Predicting Cryptocurrency Price",
        "authors": [
            "Shalini Sharma",
            "Angshul Majumdar",
            "Emilie Chouzenoux",
            "Victor Elvira"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Our work presents two fundamental contributions. On the application side, we\ntackle the challenging problem of predicting day-ahead crypto-currency prices.\nOn the methodological side, a new dynamical modeling approach is proposed. Our\napproach keeps the probabilistic formulation of the state-space model, which\nprovides uncertainty quantification on the estimates, and the function\napproximation ability of deep neural networks. We call the proposed approach\nthe deep state-space model. The experiments are carried out on established\ncryptocurrencies (obtained from Yahoo Finance). The goal of the work has been\nto predict the price for the next day. Benchmarking has been done with both\nstate-of-the-art and classical dynamical modeling techniques. Results show that\nthe proposed approach yields the best overall results in terms of accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14731v1"
    },
    {
        "title": "Deep Learning and NLP in Cryptocurrency Forecasting: Integrating\n  Financial, Blockchain, and Social Media Data",
        "authors": [
            "Vincent Gurgul",
            "Stefan Lessmann",
            "Wolfgang Karl Härdle"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We introduce novel approaches to cryptocurrency price forecasting, leveraging\nMachine Learning (ML) and Natural Language Processing (NLP) techniques, with a\nfocus on Bitcoin and Ethereum. By analysing news and social media content,\nprimarily from Twitter and Reddit, we assess the impact of public sentiment on\ncryptocurrency markets. A distinctive feature of our methodology is the\napplication of the BART MNLI zero-shot classification model to detect bullish\nand bearish trends, significantly advancing beyond traditional sentiment\nanalysis. Additionally, we systematically compare a range of pre-trained and\nfine-tuned deep learning NLP models against conventional dictionary-based\nsentiment analysis methods. Another key contribution of our work is the\nadoption of local extrema alongside daily price movements as predictive\ntargets, reducing trading frequency and portfolio volatility. Our findings\ndemonstrate that integrating textual data into cryptocurrency price forecasting\nnot only improves forecasting accuracy but also consistently enhances the\nprofitability and Sharpe ratio across various validation scenarios,\nparticularly when applying deep learning NLP techniques. The entire codebase of\nour experiments is made available via an online repository:\nhttps://anonymous.4open.science/r/crypto-forecasting-public\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14759v2"
    },
    {
        "title": "Twitter Permeability to financial events: an experiment towards a model\n  for sensing irregularities",
        "authors": [
            "Ana Fernández Vilas",
            "Rebeca P. Díaz Redondo",
            "Keeley Crockett",
            "Majdi Owda",
            "Lewis Evans"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  There is a general consensus of the good sensing and novelty characteristics\nof Twitter as an information media for the complex financial market. This paper\ninvestigates the permeability of Twittersphere, the total universe of Twitter\nusers and their habits, towards relevant events in the financial market.\nAnalysis shows that a general purpose social media is permeable to\nfinancial-specific events and establishes Twitter as a relevant feeder for\ntaking decisions regarding the financial market and event fraudulent activities\nin that market. However, the provenance of contributions, their different\nlevels of credibility and quality and even the purpose or intention behind them\nshould to be considered and carefully contemplated if Twitter is used as a\nsingle source for decision taking. With the overall aim of this research, to\ndeploy an architecture for real-time monitoring of irregularities in the\nfinancial market, this paper conducts a series of experiments on the level of\npermeability and the permeable features of Twitter in the event of one of these\nirregularities. To be precise, Twitter data is collected concerning an event\ncomprising of a specific financial action on the 27th January 2017:{~ }the\nannouncement about the merge of two companies Tesco PLC and Booker Group PLC,\nlisted in the main market of the London Stock Exchange (LSE), to create the\nUK's Leading Food Business. The experiment attempts to answer five key research\nquestions which aim to characterize the features of Twitter permeability to the\nfinancial market. The experimental results confirm that a far-impacting\nfinancial event, such as the merger considered, caused apparent disturbances in\nall the features considered, that is, information volume, content and sentiment\nas well as geographical provenance. Analysis shows that despite, Twitter not\nbeing a specific financial forum, it is permeable to financial events.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.11530v1"
    },
    {
        "title": "The irruption of cryptocurrencies into Twitter cashtags: a classifying\n  solution",
        "authors": [
            "Ana Fernández Vilas",
            "Rebeca Díaz Redondo",
            "Antón Lorenzo García"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  There is a consensus about the good sensing characteristics of Twitter to\nmine and uncover knowledge in financial markets, being considered a relevant\nfeeder for taking decisions about buying or holding stock shares and even for\ndetecting stock manipulation. Although Twitter hashtags allow to aggregate\ntopic-related content, a specific mechanism for financial information also\nexists: Cashtag. However, the irruption of cryptocurrencies has resulted in a\nsignificant degradation on the cashtag-based aggregation of posts.\nUnfortunately, Twitter' users may use homonym tickers to refer to\ncryptocurrencies and to companies in stock markets, which means that filtering\nby cashtag may result on both posts referring to stock companies and\ncryptocurrencies. This research proposes automated classifiers to distinguish\nconflicting cashtags and, so, their container tweets by analyzing the\ndistinctive features of tweets referring to stock companies and\ncryptocurrencies. As experiment, this paper analyses the interference between\ncryptocurrencies and company tickers in the London Stock Exchange (LSE),\nspecifically, companies in the main and alternative market indices FTSE-100 and\nAIM-100. Heuristic-based as well as supervised classifiers are proposed and\ntheir advantages and drawbacks, including their ability to self-adapt to\nTwitter usage changes, are discussed. The experiment confirms a significant\ndistortion in collected data when colliding or homonym cashtags exist, i.e.,\nthe same \\$ acronym to refer to company tickers and cryptocurrencies. According\nto our results, the distinctive features of posts including cryptocurrencies or\ncompany tickers support accurate classification of colliding tweets (homonym\ncashtags) and Independent Models, as the most detached classifiers from\ntraining data, have the potential to be trans-applicability (in different stock\nmarkets) while retaining performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.11531v1"
    },
    {
        "title": "Hawkes-based cryptocurrency forecasting via Limit Order Book data",
        "authors": [
            "Raffaele Giuseppe Cestari",
            "Filippo Barchi",
            "Riccardo Busetto",
            "Daniele Marazzina",
            "Simone Formentin"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Accurately forecasting the direction of financial returns poses a formidable\nchallenge, given the inherent unpredictability of financial time series. The\ntask becomes even more arduous when applied to cryptocurrency returns, given\nthe chaotic and intricately complex nature of crypto markets. In this study, we\npresent a novel prediction algorithm using limit order book (LOB) data rooted\nin the Hawkes model, a category of point processes. Coupled with a continuous\noutput error (COE) model, our approach offers a precise forecast of return\nsigns by leveraging predictions of future financial interactions. Capitalizing\non the non-uniformly sampled structure of the original time series, our\nstrategy surpasses benchmark models in both prediction accuracy and cumulative\nprofit when implemented in a trading environment. The efficacy of our approach\nis validated through Monte Carlo simulations across 50 scenarios. The research\ndraws on LOB measurements from a centralized cryptocurrency exchange where the\nstablecoin Tether is exchanged against the U.S. dollar.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.16190v1"
    },
    {
        "title": "Enhancing Profitability and Investor Confidence through Interpretable AI\n  Models for Investment Decisions",
        "authors": [
            "Sahar Arshad",
            "Seemab Latif",
            "Ahmad Salman",
            "Rabia Latif"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Financial forecasting plays an important role in making informed decisions\nfor financial stakeholders, specifically in the stock exchange market. In a\ntraditional setting, investors commonly rely on the equity research department\nfor valuable reports on market insights and investment recommendations. The\nequity research department, however, faces challenges in effectuating\ndecision-making do to the demanding cognitive effort required for analyzing the\ninherently volatile nature of market dynamics. Furthermore, financial\nforecasting systems employed by analysts pose potential risks in terms of\ninterpretability and gaining the trust of all stakeholders. This paper presents\nan interpretable decision-making model leveraging the SHAP-based explainability\ntechnique to forecast investment recommendations. The proposed solution not\nonly provides valuable insights into the factors that influence forecasted\nrecommendations but also caters the investors of varying types, including those\ninterested in daily and short-term investment opportunities. To ascertain the\nefficacy of the proposed model, a case study is devised that demonstrates a\nnotable enhancement in investor's portfolio value, employing our trading\nstrategies. The results highlight the significance of incorporating\ninterpretability in forecasting models to boost stakeholders' confidence and\nfoster transparency in the stock exchange domain.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.16223v2"
    },
    {
        "title": "On the Three Demons in Causality in Finance: Time Resolution,\n  Nonstationarity, and Latent Factors",
        "authors": [
            "Xinshuai Dong",
            "Haoyue Dai",
            "Yewen Fan",
            "Songyao Jin",
            "Sathyamoorthy Rajendran",
            "Kun Zhang"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Financial data is generally time series in essence and thus suffers from\nthree fundamental issues: the mismatch in time resolution, the time-varying\nproperty of the distribution - nonstationarity, and causal factors that are\nimportant but unknown/unobserved. In this paper, we follow a causal perspective\nto systematically look into these three demons in finance. Specifically, we\nreexamine these issues in the context of causality, which gives rise to a novel\nand inspiring understanding of how the issues can be addressed. Following this\nperspective, we provide systematic solutions to these problems, which hopefully\nwould serve as a foundation for future research in the area.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05414v2"
    },
    {
        "title": "Multi-relational Graph Diffusion Neural Network with Parallel Retention\n  for Stock Trends Classification",
        "authors": [
            "Zinuo You",
            "Pengju Zhang",
            "Jin Zheng",
            "John Cartlidge"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Stock trend classification remains a fundamental yet challenging task, owing\nto the intricate time-evolving dynamics between and within stocks. To tackle\nthese two challenges, we propose a graph-based representation learning approach\naimed at predicting the future movements of multiple stocks. Initially, we\nmodel the complex time-varying relationships between stocks by generating\ndynamic multi-relational stock graphs. This is achieved through a novel edge\ngeneration algorithm that leverages information entropy and signal energy to\nquantify the intensity and directionality of inter-stock relations on each\ntrading day. Then, we further refine these initial graphs through a stochastic\nmulti-relational diffusion process, adaptively learning task-optimal edges.\nSubsequently, we implement a decoupled representation learning scheme with\nparallel retention to obtain the final graph representation. This strategy\nbetter captures the unique temporal features within individual stocks while\nalso capturing the overall structure of the stock graph. Comprehensive\nexperiments conducted on real-world datasets from two US markets (NASDAQ and\nNYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the\neffectiveness of our method. Our approach consistently outperforms\nstate-of-the-art baselines in forecasting next trading day stock trends across\nthree test periods spanning seven years. Datasets and code have been released\n(https://github.com/pixelhero98/MGDPR).\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05430v1"
    },
    {
        "title": "An adaptive network-based approach for advanced forecasting of\n  cryptocurrency values",
        "authors": [
            "Ali Mehrban",
            "Pegah Ahadian"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper describes an architecture for predicting the price of\ncryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy\nInference System (ANFIS). Historical data of cryptocurrencies and indexes that\nare considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D),\nand Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach\nthe data are hybrid and backpropagation algorithms, as well as grid partition,\nsubtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which\nare used in data clustering. The architectural performance designed in this\npaper has been compared with different inputs and neural network models in\nterms of statistical evaluation criteria. Finally, the proposed method can\npredict the price of digital currencies in a short time.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.05441v2"
    },
    {
        "title": "Application of Machine Learning in Stock Market Forecasting: A Case\n  Study of Disney Stock",
        "authors": [
            "Dengxin Huang"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This document presents a stock market analysis conducted on a dataset\nconsisting of 750 instances and 16 attributes donated in 2014-10-23. The\nanalysis includes an exploratory data analysis (EDA) section, feature\nengineering, data preparation, model selection, and insights from the analysis.\nThe Fama French 3-factor model is also utilized in the analysis. The results of\nthe analysis are presented, with linear regression being the best-performing\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10903v1"
    },
    {
        "title": "Forecasting Cryptocurrency Staking Rewards",
        "authors": [
            "Sauren Gupta",
            "Apoorva Hathi Katharaki",
            "Yifan Xu",
            "Bhaskar Krishnamachari",
            "Rajarshi Gupta"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This research explores a relatively unexplored area of predicting\ncryptocurrency staking rewards, offering potential insights to researchers and\ninvestors. We investigate two predictive methodologies: a) a straightforward\nsliding-window average, and b) linear regression models predicated on\nhistorical data. The findings reveal that ETH staking rewards can be forecasted\nwith an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day\nlook-aheads respectively, using a 7-day sliding-window average approach.\nAdditionally, we discern diverse prediction accuracies across various\ncryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is\nidentified as superior to the moving-window average for perdicting in the short\nterm for XTZ and ATOM. The results underscore the generally stable and\npredictable nature of staking rewards for most assets, with MATIC presenting a\nnoteworthy exception.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10931v1"
    },
    {
        "title": "A Novel Decision Ensemble Framework: Customized Attention-BiLSTM and\n  XGBoost for Speculative Stock Price Forecasting",
        "authors": [
            "Riaz Ud Din",
            "Salman Ahmed",
            "Saddam Hussain Khan"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Forecasting speculative stock prices is essential for effective investment\nrisk management that drives the need for the development of innovative\nalgorithms. However, the speculative nature, volatility, and complex sequential\ndependencies within financial markets present inherent challenges which\nnecessitate advanced techniques. This paper proposes a novel framework, CAB-XDE\n(customized attention BiLSTM-XGB decision ensemble), for predicting the daily\nclosing price of speculative stock Bitcoin-USD (BTC-USD). CAB-XDE framework\nintegrates a customized bi-directional long short-term memory (BiLSTM) with the\nattention mechanism and the XGBoost algorithm. The customized BiLSTM leverages\nits learning capabilities to capture the complex sequential dependencies and\nspeculative market trends. Additionally, the new attention mechanism\ndynamically assigns weights to influential features, thereby enhancing\ninterpretability, and optimizing effective cost measures and volatility\nforecasting. Moreover, XGBoost handles nonlinear relationships and contributes\nto the proposed CAB-XDE framework robustness. Additionally, the weight\ndetermination theory-error reciprocal method further refines predictions. This\nrefinement is achieved by iteratively adjusting model weights. It is based on\ndiscrepancies between theoretical expectations and actual errors in individual\ncustomized attention BiLSTM and XGBoost models to enhance performance. Finally,\nthe predictions from both XGBoost and customized attention BiLSTM models are\nconcatenated to achieve diverse prediction space and are provided to the\nensemble classifier to enhance the generalization capabilities of CAB-XDE. The\nproposed CAB-XDE framework is empirically validated on volatile Bitcoin market,\nsourced from Yahoo Finance and outperforms state-of-the-art models with a MAPE\nof 0.0037, MAE of 84.40, and RMSE of 106.14.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.11621v1"
    },
    {
        "title": "Coarse graining correlation matrices according to macrostructures:\n  Financial markets as a paradigm",
        "authors": [
            "M. Mijaíl Martínez-Ramos",
            "Parisa Majari",
            "Andres R. Cruz-Hernández",
            "Hirdesh K. Pharasi",
            "Manan Vyas"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We analyze correlation structures in financial markets by coarse graining the\nPearson correlation matrices according to market sectors to obtain Guhr\nmatrices using Guhr's correlation method according to Ref. [P. Rinn {\\it et.\nal.}, Europhysics Letters 110, 68003 (2015)]. We compare the results for the\nevolution of market states and the corresponding transition matrices with those\nobtained using Pearson correlation matrices. The behavior of market states is\nfound to be similar for both the coarse grained and Pearson matrices. However,\nthe number of relevant variables is reduced by orders of magnitude.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.05364v2"
    },
    {
        "title": "MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive\n  and Dynamic Stock Investment Prediction",
        "authors": [
            "Hao Qian",
            "Hongting Zhou",
            "Qian Zhao",
            "Hao Chen",
            "Hongxiang Yao",
            "Jingwei Wang",
            "Ziqi Liu",
            "Fei Yu",
            "Zhiqiang Zhang",
            "Jun Zhou"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The stock market is a crucial component of the financial system, but\npredicting the movement of stock prices is challenging due to the dynamic and\nintricate relations arising from various aspects such as economic indicators,\nfinancial reports, global news, and investor sentiment. Traditional sequential\nmethods and graph-based models have been applied in stock movement prediction,\nbut they have limitations in capturing the multifaceted and temporal influences\nin stock price movements. To address these challenges, the Multi-relational\nDynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a\ndiscrete dynamic graph to comprehensively capture multifaceted relations among\nstocks and their evolution over time. The representation generated from the\ngraph offers a complete perspective on the interrelationships among stocks and\nassociated entities. Additionally, the power of the Transformer structure is\nleveraged to encode the temporal evolution of multiplex relations, providing a\ndynamic and effective approach to predicting stock investment. Further, our\nproposed MDGNN framework achieves the best performance in public datasets\ncompared with state-of-the-art (SOTA) stock investment methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06633v1"
    },
    {
        "title": "Large (and Deep) Factor Models",
        "authors": [
            "Bryan Kelly",
            "Boris Kuznetsov",
            "Semyon Malamud",
            "Teng Andrea Xu"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We open up the black box behind Deep Learning for portfolio optimization and\nprove that a sufficiently wide and arbitrarily deep neural network (DNN)\ntrained to maximize the Sharpe ratio of the Stochastic Discount Factor (SDF) is\nequivalent to a large factor model (LFM): A linear factor pricing model that\nuses many non-linear characteristics. The nature of these characteristics\ndepends on the architecture of the DNN in an explicit, tractable fashion. This\nmakes it possible to derive end-to-end trained DNN-based SDFs in closed form\nfor the first time. We evaluate LFMs empirically and show how various\narchitectural choices impact SDF performance. We document the virtue of depth\ncomplexity: With enough data, the out-of-sample performance of DNN-SDF is\nincreasing in the NN depth, saturating at huge depths of around 100 hidden\nlayers.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06635v1"
    },
    {
        "title": "Transformers with Attentive Federated Aggregation for Time Series Stock\n  Forecasting",
        "authors": [
            "Chu Myaet Thwal",
            "Ye Lin Tun",
            "Kitae Kim",
            "Seong-Bae Park",
            "Choong Seon Hong"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Recent innovations in transformers have shown their superior performance in\nnatural language processing (NLP) and computer vision (CV). The ability to\ncapture long-range dependencies and interactions in sequential data has also\ntriggered a great interest in time series modeling, leading to the widespread\nuse of transformers in many time series applications. However, being the most\ncommon and crucial application, the adaptation of transformers to time series\nforecasting has remained limited, with both promising and inconsistent results.\nIn contrast to the challenges in NLP and CV, time series problems not only add\nthe complexity of order or temporal dependence among input sequences but also\nconsider trend, level, and seasonality information that much of this data is\nvaluable for decision making. The conventional training scheme has shown\ndeficiencies regarding model overfitting, data scarcity, and privacy issues\nwhen working with transformers for a forecasting task. In this work, we propose\nattentive federated transformers for time series stock forecasting with better\nperformance while preserving the privacy of participating enterprises.\nEmpirical results on various stock data from the Yahoo! Finance website\nindicate the superiority of our proposed scheme in dealing with the above\nchallenges and data heterogeneity in federated learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06638v1"
    },
    {
        "title": "DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation",
        "authors": [
            "Yuan Gao",
            "Haokun Chen",
            "Xiang Wang",
            "Zhicai Wang",
            "Xue Wang",
            "Jinyang Gao",
            "Bolin Ding"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Machine learning models have demonstrated remarkable efficacy and efficiency\nin a wide range of stock forecasting tasks. However, the inherent challenges of\ndata scarcity, including low signal-to-noise ratio (SNR) and data homogeneity,\npose significant obstacles to accurate forecasting. To address this issue, we\npropose a novel approach that utilizes artificial intelligence-generated\nsamples (AIGS) to enhance the training procedures. In our work, we introduce\nthe Diffusion Model to generate stock factors with Transformer architecture\n(DiffsFormer). DiffsFormer is initially trained on a large-scale source domain,\nincorporating conditional guidance so as to capture global joint distribution.\nWhen presented with a specific downstream task, we employ DiffsFormer to\naugment the training procedure by editing existing samples. This editing step\nallows us to control the strength of the editing process, determining the\nextent to which the generated data deviates from the target domain. To evaluate\nthe effectiveness of DiffsFormer augmented training, we conduct experiments on\nthe CSI300 and CSI800 datasets, employing eight commonly used machine learning\nmodels. The proposed method achieves relative improvements of 7.2% and 27.8% in\nannualized return ratio for the respective datasets. Furthermore, we perform\nextensive experiments to gain insights into the functionality of DiffsFormer\nand its constituent components, elucidating how they address the challenges of\ndata scarcity and enhance the overall model performance. Our research\ndemonstrates the efficacy of leveraging AIGS and the DiffsFormer architecture\nto mitigate data scarcity in stock forecasting tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.06656v1"
    },
    {
        "title": "Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and\n  IV Models for GBP/USD and EUR/GBP Pairs",
        "authors": [
            "Narayan Tondapu"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In this study, we examine the fluctuation in the value of the Great Britain\nPound (GBP). We focus particularly on its relationship with the United States\nDollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15,\n2018, to June 15, 2023, we apply various mathematical models to assess their\neffectiveness in predicting the 20-day variation in the pairs' daily returns.\nOur analysis involves the implementation of Exponentially Weighted Moving\nAverage (EWMA), Generalized Autoregressive Conditional Heteroskedasticity\n(GARCH) models, and Implied Volatility (IV) models. To evaluate their\nperformance, we compare the accuracy of their predictions using Root Mean\nSquare Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the\nintricacies of GARCH models, examining their statistical characteristics when\napplied to the provided dataset. Our findings suggest the existence of\nasymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for\nthe GBP/USD pair. Additionally, we observe that GARCH-type models better fit\nthe data when assuming residuals follow a standard t-distribution rather than a\nstandard normal distribution. Furthermore, we investigate the efficacy of\ndifferent forecasting techniques within GARCH-type models. Comparing rolling\nwindow forecasts to expanding window forecasts, we find no definitive\nsuperiority in either approach across the tested scenarios. Our experiments\nreveal that for the GBP/USD pair, the most accurate volatility forecasts stem\nfrom the utilization of GARCH models employing a rolling window methodology.\nConversely, for the EUR/GBP pair, optimal forecasts are derived from GARCH\nmodels and Ordinary Least Squares (OLS) models incorporating the annualized\nimplied volatility of the exchange rate as an independent variable.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.07435v1"
    },
    {
        "title": "Do Weibo platform experts perform better at predicting stock market?",
        "authors": [
            "Ziyuan Ma",
            "Conor Ryan",
            "Jim Buckley",
            "Muslim Chochlov"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Sentiment analysis can be used for stock market prediction. However, existing\nresearch has not studied the impact of a user's financial background on\nsentiment-based forecasting of the stock market using artificial neural\nnetworks. In this work, a novel combination of neural networks is used for the\nassessment of sentiment-based stock market prediction, based on the financial\nbackground of the population that generated the sentiment. The state-of-the-art\nlanguage processing model Bidirectional Encoder Representations from\nTransformers (BERT) is used to classify the sentiment and a Long-Short Term\nMemory (LSTM) model is used for time-series based stock market prediction. For\nevaluation, the Weibo social networking platform is used as a sentiment data\ncollection source. Weibo users (and their comments respectively) are divided\ninto Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor\n(UFA) groups according to their background information, as collected by Weibo.\nThe Hong Kong Hang Seng index is used to extract historical stock market change\ndata. The results indicate that stock market prediction learned from the AFA\ngroup users is 39.67% more precise than that learned from the UFA group users\nand shows the highest accuracy (87%) when compared to existing approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00772v1"
    },
    {
        "title": "Regional inflation analysis using social network data",
        "authors": [
            "Vasilii Chsherbakov",
            "Ilia Karpov"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Inflation is one of the most important macroeconomic indicators that have a\ngreat impact on the population of any country and region. Inflation is\ninfluenced by range of factors, one of which is inflation expectations. Many\ncentral banks take this factor into consideration while implementing monetary\npolicy within the inflation targeting regime. Nowadays, a lot of people are\nactive users of the Internet, especially social networks. There is a hypothesis\nthat people search, read, and discuss mainly only those issues that are of\nparticular interest to them. It is logical to assume that the dynamics of\nprices may also be in the focus of user discussions. So, such discussions could\nbe regarded as an alternative source of more rapid information about inflation\nexpectations. This study is based on unstructured data from Vkontakte social\nnetwork to analyze upward and downward inflationary trends (on the example of\nthe Omsk region). The sample of more than 8.5 million posts was collected\nbetween January 2010 and May 2022. The authors used BERT neural networks to\nsolve the problem. These models demonstrated better results than the benchmarks\n(e.g., logistic regression, decision tree classifier, etc.). It makes possible\nto define pro-inflationary and disinflationary types of keywords in different\ncontexts and get their visualization with SHAP method. This analysis provides\nadditional operational information about inflationary processes at the regional\nlevel The proposed approach can be scaled for other regions. At the same time\nthe limitation of the work is the time and power costs for the initial training\nof similar models for all regions of Russia.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00774v2"
    },
    {
        "title": "Detecting Anomalous Events in Object-centric Business Processes via\n  Graph Neural Networks",
        "authors": [
            "Alessandro Niro",
            "Michael Werner"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Detecting anomalies is important for identifying inefficiencies, errors, or\nfraud in business processes. Traditional process mining approaches focus on\nanalyzing 'flattened', sequential, event logs based on a single case notion.\nHowever, many real-world process executions exhibit a graph-like structure,\nwhere events can be associated with multiple cases. Flattening event logs\nrequires selecting a single case identifier which creates a gap with the real\nevent data and artificially introduces anomalies in the event logs.\nObject-centric process mining avoids these limitations by allowing events to be\nrelated to different cases. This study proposes a novel framework for anomaly\ndetection in business processes that exploits graph neural networks and the\nenhanced information offered by object-centric process mining. We first\nreconstruct and represent the process dependencies of the object-centric event\nlogs as attributed graphs and then employ a graph convolutional autoencoder\narchitecture to detect anomalous events. Our results show that our approach\nprovides promising performance in detecting anomalies at the activity type and\nattributes level, although it struggles to detect anomalies in the temporal\norder of events.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00775v1"
    },
    {
        "title": "Ploutos: Towards interpretable stock movement prediction with financial\n  large language model",
        "authors": [
            "Hanshuang Tong",
            "Jun Li",
            "Ning Wu",
            "Ming Gong",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Recent advancements in large language models (LLMs) have opened new pathways\nfor many domains. However, the full potential of LLMs in financial investments\nremains largely untapped. There are two main challenges for typical deep\nlearning-based methods for quantitative finance. First, they struggle to fuse\ntextual and numerical information flexibly for stock movement prediction.\nSecond, traditional methods lack clarity and interpretability, which impedes\ntheir application in scenarios where the justification for predictions is\nessential. To solve the above challenges, we propose Ploutos, a novel financial\nLLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen\ncontains multiple primary experts that can analyze different modal data, such\nas text and numbers, and provide quantitative strategies from different\nperspectives. Then PloutosGPT combines their insights and predictions and\ngenerates interpretable rationales. To generate accurate and faithful\nrationales, the training strategy of PloutosGPT leverage rearview-mirror\nprompting mechanism to guide GPT-4 to generate rationales, and a dynamic token\nweighting mechanism to finetune LLM by increasing key tokens weight. Extensive\nexperiments show our framework outperforms the state-of-the-art methods on both\nprediction accuracy and interpretability.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00782v1"
    },
    {
        "title": "Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes:\n  Functional and Augmented Data Structures in Financial Forecasting",
        "authors": [
            "Narayan Tondapu"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In this paper, we explore the application of Gaussian Processes (GPs) for\npredicting mean-reverting time series with an underlying structure, using\nrelatively unexplored functional and augmented data structures. While many\nconventional forecasting methods concentrate on the short-term dynamics of time\nseries data, GPs offer the potential to forecast not just the average\nprediction but the entire probability distribution over a future trajectory.\nThis is particularly beneficial in financial contexts, where accurate\npredictions alone may not suffice if incorrect volatility assessments lead to\ncapital losses. Moreover, in trade selection, GPs allow for the forecasting of\nmultiple Sharpe ratios adjusted for transaction costs, aiding in\ndecision-making. The functional data representation utilized in this study\nenables longer-term predictions by leveraging information from previous years,\neven as the forecast moves away from the current year's training data.\nAdditionally, the augmented representation enriches the training set by\nincorporating multiple targets for future points in time, facilitating\nlong-term predictions. Our implementation closely aligns with the methodology\noutlined in, which assessed effectiveness on commodity futures. However, our\ntesting methodology differs. Instead of real data, we employ simulated data\nwith similar characteristics. We construct a testing environment to evaluate\nboth data representations and models under conditions of increasing noise, fat\ntails, and inappropriate kernels-conditions commonly encountered in practice.\nBy simulating data, we can compare our forecast distribution over time against\na full simulation of the actual distribution of our test set, thereby reducing\nthe inherent uncertainty in testing time series models on real data. We enable\nfeature prediction through augmentation and employ sub-sampling to ensure the\nfeasibility of GPs.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00796v1"
    },
    {
        "title": "Jump detection in high-frequency order prices",
        "authors": [
            "Markus Bibinger",
            "Nikolaus Hautsch",
            "Alexander Ristig"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We propose methods to infer jumps of a semi-martingale, which describes\nlong-term price dynamics based on discrete, noisy, high-frequency observations.\nDifferent to the classical model of additive, centered market microstructure\nnoise, we consider one-sided microstructure noise for order prices in a limit\norder book. We develop methods to estimate, locate and test for jumps using\nlocal order statistics. We provide a local test and show that we can\nconsistently estimate price jumps. The main contribution is a global test for\njumps. We establish the asymptotic properties and optimality of this test. We\nderive the asymptotic distribution of a maximum statistic under the null\nhypothesis of no jumps based on extreme value theory. We prove consistency\nunder the alternative hypothesis. The rate of convergence for local\nalternatives is determined and shown to be much faster than optimal rates for\nthe standard market microstructure noise model. This allows the identification\nof smaller jumps. In the process, we establish uniform consistency for spot\nvolatility estimation under one-sided microstructure noise. A simulation study\nsheds light on the finite-sample implementation and properties of our new\nstatistics and draws a comparison to a popular method for market microstructure\nnoise. We showcase how our new approach helps to improve jump detection in an\nempirical analysis of intra-daily limit order book data.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.00819v1"
    },
    {
        "title": "Temporal Graph Networks for Graph Anomaly Detection in Financial\n  Networks",
        "authors": [
            "Yejin Kim",
            "Youngbin Lee",
            "Minyoung Choe",
            "Sungju Oh",
            "Yongjae Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper explores the utilization of Temporal Graph Networks (TGN) for\nfinancial anomaly detection, a pressing need in the era of fintech and\ndigitized financial transactions. We present a comprehensive framework that\nleverages TGN, capable of capturing dynamic changes in edges within financial\nnetworks, for fraud detection. Our study compares TGN's performance against\nstatic Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph\nneural network baselines using DGraph dataset for a realistic financial\ncontext. Our results demonstrate that TGN significantly outperforms other\nmodels in terms of AUC metrics. This superior performance underlines TGN's\npotential as an effective tool for detecting financial fraud, showcasing its\nability to adapt to the dynamic and complex nature of modern financial systems.\nWe also experimented with various graph embedding modules within the TGN\nframework and compared the effectiveness of each module. In conclusion, we\ndemonstrated that, even with variations within TGN, it is possible to achieve\ngood performance in the anomaly detection task.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.00060v1"
    },
    {
        "title": "Postprocessing of point predictions for probabilistic forecasting of\n  day-ahead electricity prices: The benefits of using isotonic distributional\n  regression",
        "authors": [
            "Arkadiusz Lipiecki",
            "Bartosz Uniejewski",
            "Rafał Weron"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Operational decisions relying on predictive distributions of electricity\nprices can result in significantly higher profits compared to those based\nsolely on point forecasts. However, the majority of models developed in both\nacademic and industrial settings provide only point predictions. To address\nthis, we examine three postprocessing methods for converting point forecasts of\nday-ahead electricity prices into probabilistic ones: Quantile Regression\nAveraging, Conformal Prediction, and the recently introduced Isotonic\nDistributional Regression. We find that while the latter demonstrates the most\nvaried behavior, it contributes the most to the ensemble of the three\npredictive distributions, as measured by Shapley values. Remarkably, the\nperformance of the combination is superior to that of state-of-the-art\nDistributional Deep Neural Networks over two 4.5-year test periods from the\nGerman and Spanish power markets, spanning the COVID pandemic and the war in\nUkraine.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.02270v2"
    },
    {
        "title": "Stock Recommendations for Individual Investors: A Temporal Graph Network\n  Approach with Mean-Variance Efficient Sampling",
        "authors": [
            "Youngbin Lee",
            "Yejin Kim",
            "Javier Sanz-Cruzado",
            "Richard McCreadie",
            "Yongjae Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Recommender systems can be helpful for individuals to make well-informed\ndecisions in complex financial markets. While many studies have focused on\npredicting stock prices, even advanced models fall short of accurately\nforecasting them. Additionally, previous studies indicate that individual\ninvestors often disregard established investment theories, favoring their\npersonal preferences instead. This presents a challenge for stock\nrecommendation systems, which must not only provide strong investment\nperformance but also respect these individual preferences. To create effective\nstock recommender systems, three critical elements must be incorporated: 1)\nindividual preferences, 2) portfolio diversification, and 3) the temporal\ndynamics of the first two. In response, we propose a new model, Portfolio\nTemporal Graph Network Recommender PfoTGNRec, which can handle time-varying\ncollaborative signals and incorporates diversification-enhancing sampling. On\nreal-world individual trading data, our approach demonstrates superior\nperformance compared to state-of-the-art baselines, including cutting-edge\ndynamic embedding models and existing stock recommendation models. Indeed, we\nshow that PfoTGNRec is an effective solution that can balance customer\npreferences with the need to suggest portfolios with high Return-on-Investment.\nThe source code and data are available at\nhttps://github.com/youngandbin/PfoTGNRec.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.07223v3"
    },
    {
        "title": "Unveiling the Impact of Macroeconomic Policies: A Double Machine\n  Learning Approach to Analyzing Interest Rate Effects on Financial Markets",
        "authors": [
            "Anoop Kumar",
            "Suresh Dodda",
            "Navin Kamuni",
            "Rajeev Kumar Arora"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This study examines the effects of macroeconomic policies on financial\nmarkets using a novel approach that combines Machine Learning (ML) techniques\nand causal inference. It focuses on the effect of interest rate changes made by\nthe US Federal Reserve System (FRS) on the returns of fixed income and equity\nfunds between January 1986 and December 2021. The analysis makes a distinction\nbetween actively and passively managed funds, hypothesizing that the latter are\nless susceptible to changes in interest rates. The study contrasts gradient\nboosting and linear regression models using the Double Machine Learning (DML)\nframework, which supports a variety of statistical learning techniques. Results\nindicate that gradient boosting is a useful tool for predicting fund returns;\nfor example, a 1% increase in interest rates causes an actively managed fund's\nreturn to decrease by -11.97%. This understanding of the relationship between\ninterest rates and fund performance provides opportunities for additional\nresearch and insightful, data-driven advice for fund managers and investors\n",
        "pdf_link": "http://arxiv.org/pdf/2404.07225v1"
    },
    {
        "title": "An End-to-End Structure with Novel Position Mechanism and Improved EMD\n  for Stock Forecasting",
        "authors": [
            "Chufeng Li",
            "Jianyong Chen"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  As a branch of time series forecasting, stock movement forecasting is one of\nthe challenging problems for investors and researchers. Since Transformer was\nintroduced to analyze financial data, many researchers have dedicated\nthemselves to forecasting stock movement using Transformer or attention\nmechanisms. However, existing research mostly focuses on individual stock\ninformation but ignores stock market information and high noise in stock data.\nIn this paper, we propose a novel method using the attention mechanism in which\nboth stock market information and individual stock information are considered.\nMeanwhile, we propose a novel EMD-based algorithm for reducing short-term noise\nin stock data. Two randomly selected exchange-traded funds (ETFs) spanning over\nten years from US stock markets are used to demonstrate the superior\nperformance of the proposed attention-based method. The experimental analysis\ndemonstrates that the proposed attention-based method significantly outperforms\nother state-of-the-art baselines. Code is available at\nhttps://github.com/DurandalLee/ACEFormer.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.07969v1"
    },
    {
        "title": "BERT vs GPT for financial engineering",
        "authors": [
            "Edward Sharkey",
            "Philip Treleaven"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The paper benchmarks several Transformer models [4], to show how these models\ncan judge sentiment from a news event. This signal can then be used for\ndownstream modelling and signal identification for commodity trading. We find\nthat fine-tuned BERT models outperform fine-tuned or vanilla GPT models on this\ntask. Transformer models have revolutionized the field of natural language\nprocessing (NLP) in recent years, achieving state-of-the-art results on various\ntasks such as machine translation, text summarization, question answering, and\nnatural language generation. Among the most prominent transformer models are\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-trained Transformer (GPT), which differ in their architectures and\nobjectives.\n  A CopBERT model training data and process overview is provided. The CopBERT\nmodel outperforms similar domain specific BERT trained models such as FinBERT.\nThe below confusion matrices show the performance on CopBERT & CopGPT\nrespectively. We see a ~10 percent increase in f1_score when compare CopBERT vs\nGPT4 and 16 percent increase vs CopGPT. Whilst GPT4 is dominant It highlights\nthe importance of considering alternatives to GPT models for financial\nengineering tasks, given risks of hallucinations, and challenges with\ninterpretability. We unsurprisingly see the larger LLMs outperform the BERT\nmodels, with predictive power. In summary BERT is partially the new XGboost,\nwhat it lacks in predictive power it provides with higher levels of\ninterpretability. Concluding that BERT models might not be the next XGboost\n[2], but represent an interesting alternative for financial engineering tasks,\nthat require a blend of interpretability and accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.12990v1"
    },
    {
        "title": "FinRobot: An Open-Source AI Agent Platform for Financial Applications\n  using Large Language Models",
        "authors": [
            "Hongyang Yang",
            "Boyu Zhang",
            "Neng Wang",
            "Cheng Guo",
            "Xiaoli Zhang",
            "Likun Lin",
            "Junlin Wang",
            "Tianyu Zhou",
            "Mao Guan",
            "Runjia Zhang",
            "Christina Dan Wang"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  As financial institutions and professionals increasingly incorporate Large\nLanguage Models (LLMs) into their workflows, substantial barriers, including\nproprietary data and specialized knowledge, persist between the finance sector\nand the AI community. These challenges impede the AI community's ability to\nenhance financial tasks effectively. Acknowledging financial analysis's\ncritical role, we aim to devise financial-specialized LLM-based toolchains and\ndemocratize access to them through open-source initiatives, promoting wider AI\nadoption in financial decision-making. In this paper, we introduce FinRobot, a\nnovel open-source AI agent platform supporting multiple financially specialized\nAI agents, each powered by LLM. Specifically, the platform consists of four\nmajor layers: 1) the Financial AI Agents layer that formulates Financial\nChain-of-Thought (CoT) by breaking sophisticated financial problems down into\nlogical sequences; 2) the Financial LLM Algorithms layer dynamically configures\nappropriate model application strategies for specific tasks; 3) the LLMOps and\nDataOps layer produces accurate models by applying training/fine-tuning\ntechniques and using task-relevant data; 4) the Multi-source LLM Foundation\nModels layer that integrates various LLMs and enables the above layers to\naccess them directly. Finally, FinRobot provides hands-on for both\nprofessional-grade analysts and laypersons to utilize powerful AI techniques\nfor advanced financial analysis. We open-source FinRobot at\n\\url{https://github.com/AI4Finance-Foundation/FinRobot}.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.14767v2"
    },
    {
        "title": "Identifying Extreme Events in the Stock Market: A Topological Data\n  Analysis",
        "authors": [
            "Anish Rai",
            "Buddha Nath Sharma",
            "Salam Rabindrajit Luwang",
            "Md. Nurujjaman",
            "Sushovan Majhi"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper employs Topological Data Analysis (TDA) to detect extreme events\n(EEs) in the stock market at a continental level. Previous approaches, which\nanalyzed stock indices separately, could not detect EEs for multiple time\nseries in one go. TDA provides a robust framework for such analysis and\nidentifies the EEs during the crashes for different indices. The TDA analysis\nshows that $L^1$, $L^2$ norms and Wasserstein distance ($W_D$) of the world\nleading indices rise abruptly during the crashes, surpassing a threshold of\n$\\mu+4*\\sigma$ where $\\mu$ and $\\sigma$ are the mean and the standard deviation\nof norm or $W_D$, respectively. Our study identified the stock index crashes of\nthe 2008 financial crisis and the COVID-19 pandemic across continents as EEs.\nGiven that different sectors in an index behave differently, a sector-wise\nanalysis was conducted during the COVID-19 pandemic for the Indian stock\nmarket. The sector-wise results show that after the occurrence of EE, we have\nobserved strong crashes surpassing $\\mu+2*\\sigma$ for an extended period for\nthe banking sector. While for the pharmaceutical sector, no significant spikes\nwere noted. Hence, TDA also proves successful in identifying the duration of\nshocks after the occurrence of EEs. This also indicates that the Banking sector\ncontinued to face stress and remained volatile even after the crash. This study\ngives us the applicability of TDA as a powerful analytical tool to study EEs in\nvarious fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.16052v1"
    },
    {
        "title": "Probabilistic models and statistics for electronic financial markets in\n  the digital age",
        "authors": [
            "Markus Bibinger"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The scope of this manuscript is to review some recent developments in\nstatistics for discretely observed semimartingales which are motivated by\napplications for financial markets. Our journey through this area stops to take\ncloser looks at a few selected topics discussing recent literature. We moreover\nhighlight and explain the important role played by some classical concepts of\nprobability and statistics. We focus on three main aspects: Testing for jumps;\nrough fractional stochastic volatility; and limit order microstructure noise.\nWe review jump tests based on extreme value theory and complement the\nliterature proposing new statistical methods. They are based on asymptotic\ntheory of order statistics and the R\\'{e}nyi representation. The second stage\nof our journey visits a recent strand of research showing that volatility is\nrough. We further investigate this and establish a minimax lower bound\nexploring frontiers to what extent the regularity of latent volatility can be\nrecovered in a more general framework. Finally, we discuss a stochastic\nboundary model with one-sided microstructure noise for high-frequency limit\norder prices and its probabilistic and statistical foundation.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.07388v1"
    },
    {
        "title": "Predicting Customer Goals in Financial Institution Services: A\n  Data-Driven LSTM Approach",
        "authors": [
            "Andrew Estornell",
            "Stylianos Loukas Vasileiou",
            "William Yeoh",
            "Daniel Borrajo",
            "Rui Silva"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In today's competitive financial landscape, understanding and anticipating\ncustomer goals is crucial for institutions to deliver a personalized and\noptimized user experience. This has given rise to the problem of accurately\npredicting customer goals and actions. Focusing on that problem, we use\nhistorical customer traces generated by a realistic simulator and present two\nsimple models for predicting customer goals and future actions -- an LSTM model\nand an LSTM model enhanced with state-space graph embeddings. Our results\ndemonstrate the effectiveness of these models when it comes to predicting\ncustomer goals and actions.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19399v1"
    },
    {
        "title": "Modelling financial volume curves with hierarchical Poisson processes",
        "authors": [
            "Creighton Heaukulani",
            "Abhinav Pandey",
            "Lancelot F. James"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Modeling the trading volume curves of financial instruments throughout the\nday is of key interest in financial trading applications. Predictions of these\nso-called volume profiles guide trade execution strategies, for example, a\ncommon strategy is to trade a desired quantity across many orders in line with\nthe expected volume curve throughout the day so as not to impact the price of\nthe instrument. The volume curves (for each day) are naturally grouped by stock\nand can be further gathered into higher-level groupings, such as by industry.\nIn order to model such admixtures of volume curves, we introduce a hierarchical\nPoisson process model for the intensity functions of admixtures of inhomogenous\nPoisson processes, which represent the trading times of the stock throughout\nthe day. The model is based on the hierarchical Dirichlet process, and an\nefficient Markov Chain Monte Carlo (MCMC) algorithm is derived following the\nslice sampling framework for Bayesian nonparametric mixture models. We\ndemonstrate the method on datasets of different stocks from the Trade and Quote\nrepository maintained by Wharton Research Data Services, including the most\nliquid stock on the NASDAQ stock exchange, Apple, demonstrating the scalability\nof the approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19402v1"
    },
    {
        "title": "Temporal distribution of clusters of investors and their application in\n  prediction with expert advice",
        "authors": [
            "Wojciech Wisniewski",
            "Yuri Kalnishkan",
            "David Lindsay",
            "Siân Lindsay"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Financial organisations such as brokers face a significant challenge in\nservicing the investment needs of thousands of their traders worldwide. This\ntask is further compounded since individual traders will have their own risk\nappetite and investment goals. Traders may look to capture short-term trends in\nthe market which last only seconds to minutes, or they may have longer-term\nviews which last several days to months. To reduce the complexity of this task,\nclient trades can be clustered. By examining such clusters, we would likely\nobserve many traders following common patterns of investment, but how do these\npatterns vary through time? Knowledge regarding the temporal distributions of\nsuch clusters may help financial institutions manage the overall portfolio of\nrisk that accumulates from underlying trader positions. This study contributes\nto the field by demonstrating that the distribution of clusters derived from\nthe real-world trades of 20k Foreign Exchange (FX) traders (from 2015 to 2017)\nis described in accordance with Ewens' Sampling Distribution. Further, we show\nthat the Aggregating Algorithm (AA), an on-line prediction with expert advice\nalgorithm, can be applied to the aforementioned real-world data in order to\nimprove the returns of portfolios of trader risk. However we found that the AA\n'struggles' when presented with too many trader ``experts'', especially when\nthere are many trades with similar overall patterns. To help overcome this\nchallenge, we have applied and compared the use of Statistically Validated\nNetworks (SVN) with a hierarchical clustering approach on a subset of the data,\ndemonstrating that both approaches can be used to significantly improve results\nof the AA in terms of profitability and smoothness of returns.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19403v1"
    },
    {
        "title": "International Trade Flow Prediction with Bilateral Trade Provisions",
        "authors": [
            "Zijie Pan",
            "Stepan Gordeev",
            "Jiahui Zhao",
            "Ziyi Meng",
            "Caiwen Ding",
            "Sandro Steinbach",
            "Dongjin Song"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper presents a novel methodology for predicting international\nbilateral trade flows, emphasizing the growing importance of Preferential Trade\nAgreements (PTAs) in the global trade landscape. Acknowledging the limitations\nof traditional models like the Gravity Model of Trade, this study introduces a\ntwo-stage approach combining explainable machine learning and factorization\nmodels. The first stage employs SHAP Explainer for effective variable\nselection, identifying key provisions in PTAs, while the second stage utilizes\nFactorization Machine models to analyze the pairwise interaction effects of\nthese provisions on trade flows. By analyzing comprehensive datasets, the paper\ndemonstrates the efficacy of this approach. The findings not only enhance the\npredictive accuracy of trade flow models but also offer deeper insights into\nthe complex dynamics of international trade, influenced by specific bilateral\ntrade provisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.13698v1"
    },
    {
        "title": "Machine Learning-based Relative Valuation of Municipal Bonds",
        "authors": [
            "Preetha Saha",
            "Jingrao Lyu",
            "Dhruv Desai",
            "Rishab Chauhan",
            "Jerinsh Jeyapaulraj",
            "Philip Sommer",
            "Dhagash Mehta"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The trading ecosystem of the Municipal (muni) bond is complex and unique.\nWith nearly 2\\% of securities from over a million securities outstanding\ntrading daily, determining the value or relative value of a bond among its\npeers is challenging. Traditionally, relative value calculation has been done\nusing rule-based or heuristics-driven approaches, which may introduce human\nbiases and often fail to account for complex relationships between the bond\ncharacteristics. We propose a data-driven model to develop a supervised\nsimilarity framework for the muni bond market based on CatBoost algorithm. This\nalgorithm learns from a large-scale dataset to identify bonds that are similar\nto each other based on their risk profiles. This allows us to evaluate the\nprice of a muni bond relative to a cohort of bonds with a similar risk profile.\nWe propose and deploy a back-testing methodology to compare various benchmarks\nand the proposed methods and show that the similarity-based method outperforms\nboth rule-based and heuristic-based methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.02273v1"
    },
    {
        "title": "Enhancing Causal Discovery in Financial Networks with Piecewise Quantile\n  Regression",
        "authors": [
            "Cameron Cornell",
            "Lewis Mitchell",
            "Matthew Roughan"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Financial networks can be constructed using statistical dependencies found\nwithin the price series of speculative assets. Across the various methods used\nto infer these networks, there is a general reliance on predictive modelling to\ncapture cross-correlation effects. These methods usually model the flow of\nmean-response information, or the propagation of volatility and risk within the\nmarket. Such techniques, though insightful, don't fully capture the broader\ndistribution-level causality that is possible within speculative markets. This\npaper introduces a novel approach, combining quantile regression with a\npiecewise linear embedding scheme - allowing us to construct causality networks\nthat identify the complex tail interactions inherent to financial markets.\nApplying this method to 260 cryptocurrency return series, we uncover\nsignificant tail-tail causal effects and substantial causal asymmetry. We\nidentify a propensity for coins to be self-influencing, with comparatively\nsparse cross variable effects. Assessing all link types in conjunction, Bitcoin\nstands out as the primary influencer - a nuance that is missed in conventional\nlinear mean-response analyses. Our findings introduce a comprehensive framework\nfor modelling distributional causality, paving the way towards more holistic\nrepresentations of causality in financial markets.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.12210v1"
    },
    {
        "title": "Causal Hierarchy in the Financial Market Network -- Uncovered by the\n  Helmholtz-Hodge-Kodaira Decomposition",
        "authors": [
            "Tobias Wand",
            "Oliver Kamps",
            "Hiroshi Iyetomi"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Granger causality can uncover the cause and effect relationships in financial\nnetworks. However, such networks can be convoluted and difficult to interpret,\nbut the Helmholtz-Hodge-Kodaira decomposition can split them into a rotational\nand gradient component which reveals the hierarchy of Granger causality flow.\nUsing Kenneth French's business sector return time series, it is revealed that\nduring the Covid crisis, precious metals and pharmaceutical products are causal\ndrivers of the financial network. Moreover, the estimated Granger causality\nnetwork shows a high connectivity during crisis which means that the research\npresented here can be especially useful to better understand crises in the\nmarket by revealing the dominant drivers of the crisis dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.12839v1"
    },
    {
        "title": "StockTime: A Time Series Specialized Large Language Model Architecture\n  for Stock Price Prediction",
        "authors": [
            "Shengkun Wang",
            "Taoran Ji",
            "Linhan Wang",
            "Yanshen Sun",
            "Shang-Ching Liu",
            "Amit Kumar",
            "Chang-Tien Lu"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The stock price prediction task holds a significant role in the financial\ndomain and has been studied for a long time. Recently, large language models\n(LLMs) have brought new ways to improve these predictions. While recent\nfinancial large language models (FinLLMs) have shown considerable progress in\nfinancial NLP tasks compared to smaller pre-trained language models (PLMs),\nchallenges persist in stock price forecasting. Firstly, effectively integrating\nthe modalities of time series data and natural language to fully leverage these\ncapabilities remains complex. Secondly, FinLLMs focus more on analysis and\ninterpretability, which can overlook the essential features of time series\ndata. Moreover, due to the abundance of false and redundant information in\nfinancial markets, models often produce less accurate predictions when faced\nwith such input data. In this paper, we introduce StockTime, a novel LLM-based\narchitecture designed specifically for stock price data. Unlike recent FinLLMs,\nStockTime is specifically designed for stock price time series data. It\nleverages the natural ability of LLMs to predict the next token by treating\nstock prices as consecutive tokens, extracting textual information such as\nstock correlations, statistical trends and timestamps directly from these stock\nprices. StockTime then integrates both textual and time series data into the\nembedding space. By fusing this multimodal data, StockTime effectively predicts\nstock prices across arbitrary look-back periods. Our experiments demonstrate\nthat StockTime outperforms recent LLMs, as it gives more accurate predictions\nwhile reducing memory usage and runtime costs.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08281v1"
    },
    {
        "title": "LSR-IGRU: Stock Trend Prediction Based on Long Short-Term Relationships\n  and Improved GRU",
        "authors": [
            "Peng Zhu",
            "Yuante Li",
            "Yifan Hu",
            "Qinyuan Liu",
            "Dawei Cheng",
            "Yuqi Liang"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Stock price prediction is a challenging problem in the field of finance and\nreceives widespread attention. In recent years, with the rapid development of\ntechnologies such as deep learning and graph neural networks, more research\nmethods have begun to focus on exploring the interrelationships between stocks.\nHowever, existing methods mostly focus on the short-term dynamic relationships\nof stocks and directly integrating relationship information with temporal\ninformation. They often overlook the complex nonlinear dynamic characteristics\nand potential higher-order interaction relationships among stocks in the stock\nmarket. Therefore, we propose a stock price trend prediction model named\nLSR-IGRU in this paper, which is based on long short-term stock relationships\nand an improved GRU input. Firstly, we construct a long short-term relationship\nmatrix between stocks, where secondary industry information is employed for the\nfirst time to capture long-term relationships of stocks, and overnight price\ninformation is utilized to establish short-term relationships. Next, we improve\nthe inputs of the GRU model at each step, enabling the model to more\neffectively integrate temporal information and long short-term relationship\ninformation, thereby significantly improving the accuracy of predicting stock\ntrend changes. Finally, through extensive experiments on multiple datasets from\nstock markets in China and the United States, we validate the superiority of\nthe proposed LSR-IGRU model over the current state-of-the-art baseline models.\nWe also apply the proposed model to the algorithmic trading system of a\nfinancial company, achieving significantly higher cumulative portfolio returns\ncompared to other baseline methods. Our sources are released at\nhttps://github.com/ZP1481616577/Baselines_LSR-IGRU.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08282v2"
    },
    {
        "title": "Comparative Study of Long Short-Term Memory (LSTM) and Quantum Long\n  Short-Term Memory (QLSTM): Prediction of Stock Market Movement",
        "authors": [
            "Tariq Mahmood",
            "Ibtasam Ahmad",
            "Malik Muhammad Zeeshan Ansar",
            "Jumanah Ahmed Darwish",
            "Rehan Ahmad Khan Sherwani"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In recent years, financial analysts have been trying to develop models to\npredict the movement of a stock price index. The task becomes challenging in\nvague economic, social, and political situations like in Pakistan. In this\nstudy, we employed efficient models of machine learning such as long short-term\nmemory (LSTM) and quantum long short-term memory (QLSTM) to predict the Karachi\nStock Exchange (KSE) 100 index by taking monthly data of twenty-six economic,\nsocial, political, and administrative indicators from February 2004 to December\n2020. The comparative results of LSTM and QLSTM predicted values of the KSE 100\nindex with the actual values suggested QLSTM a potential technique to predict\nstock market trends.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08297v1"
    },
    {
        "title": "Price predictability in limit order book with deep learning model",
        "authors": [
            "Kyungsub Lee"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This study explores the prediction of high-frequency price changes using deep\nlearning models. Although state-of-the-art methods perform well, their\ncomplexity impedes the understanding of successful predictions. We found that\nan inadequately defined target price process may render predictions meaningless\nby incorporating past information. The commonly used three-class problem in\nasset price prediction can generally be divided into volatility and directional\nprediction. When relying solely on the price process, directional prediction\nperformance is not substantial. However, volume imbalance improves directional\nprediction performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.14157v1"
    },
    {
        "title": "Leveraging Fundamental Analysis for Stock Trend Prediction for Profit",
        "authors": [
            "John Phan",
            "Hung-Fu Chang"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This paper investigates the application of machine learning models, Long\nShort-Term Memory (LSTM), one-dimensional Convolutional Neural Networks (1D\nCNN), and Logistic Regression (LR), for predicting stock trends based on\nfundamental analysis. Unlike most existing studies that predominantly utilize\ntechnical or sentiment analysis, we emphasize the use of a company's financial\nstatements and intrinsic value for trend forecasting. Using a dataset of 269\ndata points from publicly traded companies across various sectors from 2019 to\n2023, we employ key financial ratios and the Discounted Cash Flow (DCF) model\nto formulate two prediction tasks: Annual Stock Price Difference (ASPD) and\nDifference between Current Stock Price and Intrinsic Value (DCSPIV). These\ntasks assess the likelihood of annual profit and current profitability,\nrespectively. Our results demonstrate that LR models outperform CNN and LSTM\nmodels, achieving an average test accuracy of 74.66% for ASPD and 72.85% for\nDCSPIV. This study contributes to the limited literature on integrating\nfundamental analysis into machine learning for stock prediction, offering\nvaluable insights for both academic research and practical investment\nstrategies. By leveraging fundamental data, our approach highlights the\npotential for long-term stock trend prediction, supporting portfolio managers\nin their decision-making processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.03913v1"
    },
    {
        "title": "Evaluating Financial Relational Graphs: Interpretation Before Prediction",
        "authors": [
            "Yingjie Niu",
            "Lanxin Lu",
            "Rian Dolphin",
            "Valerio Poti",
            "Ruihai Dong"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Accurate and robust stock trend forecasting has been a crucial and\nchallenging task, as stock price changes are influenced by multiple factors.\nGraph neural network-based methods have recently achieved remarkable success in\nthis domain by constructing stock relationship graphs that reflect internal\nfactors and relationships between stocks. However, most of these methods rely\non predefined factors to construct static stock relationship graphs due to the\nlack of suitable datasets, failing to capture the dynamic changes in stock\nrelationships. Moreover, the evaluation of relationship graphs in these methods\nis often tied to the performance of neural network models on downstream tasks,\nleading to confusion and imprecision. To address these issues, we introduce the\nSPNews dataset, collected based on S\\&P 500 Index stocks, to facilitate the\nconstruction of dynamic relationship graphs. Furthermore, we propose a novel\nset of financial relationship graph evaluation methods that are independent of\ndownstream tasks. By using the relationship graph to explain historical\nfinancial phenomena, we assess its validity before constructing a graph neural\nnetwork, ensuring the graph's effectiveness in capturing relevant financial\nrelationships. Experimental results demonstrate that our evaluation methods can\neffectively differentiate between various financial relationship graphs,\nyielding more interpretable results compared to traditional approaches. We make\nour source code publicly available on GitHub to promote reproducibility and\nfurther research in this area.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.07216v1"
    },
    {
        "title": "Distilling Analysis from Generative Models for Investment Decisions",
        "authors": [
            "Chung-Chi Chen",
            "Hiroya Takamura",
            "Ichiro Kobayashi",
            "Yusuke Miyao"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Professionals' decisions are the focus of every field. For example,\npoliticians' decisions will influence the future of the country, and stock\nanalysts' decisions will impact the market. Recognizing the influential role of\nprofessionals' perspectives, inclinations, and actions in shaping\ndecision-making processes and future trends across multiple fields, we propose\nthree tasks for modeling these decisions in the financial market. To facilitate\nthis, we introduce a novel dataset, A3, designed to simulate professionals'\ndecision-making processes. While we find current models present challenges in\nforecasting professionals' behaviors, particularly in making trading decisions,\nthe proposed Chain-of-Decision approach demonstrates promising improvements. It\nintegrates an opinion-generator-in-the-loop to provide subjective analysis\nbased on each news item, further enhancing the proposed tasks' performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.07225v1"
    },
    {
        "title": "Volatility Forecasting in Global Financial Markets Using TimeMixer",
        "authors": [
            "Alex Li"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Predicting volatility in financial markets, including stocks, index ETFs,\nforeign exchange, and cryptocurrencies, remains a challenging task due to the\ninherent complexity and non-linear dynamics of these time series. In this\nstudy, I apply TimeMixer, a state-of-the-art time series forecasting model, to\npredict the volatility of global financial assets. TimeMixer utilizes a\nmultiscale-mixing approach that effectively captures both short-term and\nlong-term temporal patterns by analyzing data across different scales. My\nempirical results reveal that while TimeMixer performs exceptionally well in\nshort-term volatility forecasting, its accuracy diminishes for longer-term\npredictions, particularly in highly volatile markets. These findings highlight\nTimeMixer's strength in capturing short-term volatility, making it highly\nsuitable for practical applications in financial risk management, where precise\nshort-term forecasts are critical. However, the model's limitations in\nlong-term forecasting point to potential areas for further refinement.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.09062v1"
    },
    {
        "title": "A Hierarchical conv-LSTM and LLM Integrated Model for Holistic Stock\n  Forecasting",
        "authors": [
            "Arya Chakraborty",
            "Auhona Basu"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The financial domain presents a complex environment for stock market\nprediction, characterized by volatile patterns and the influence of\nmultifaceted data sources. Traditional models have leveraged either\nConvolutional Neural Networks (CNN) for spatial feature extraction or Long\nShort-Term Memory (LSTM) networks for capturing temporal dependencies, with\nlimited integration of external textual data. This paper proposes a novel\nTwo-Level Conv-LSTM Neural Network integrated with a Large Language Model (LLM)\nfor comprehensive stock advising. The model harnesses the strengths of\nConv-LSTM for analyzing time-series data and LLM for processing and\nunderstanding textual information from financial news, social media, and\nreports. In the first level, convolutional layers are employed to identify\nlocal patterns in historical stock prices and technical indicators, followed by\nLSTM layers to capture the temporal dynamics. The second level integrates the\noutput with an LLM that analyzes sentiment and contextual information from\ntextual data, providing a holistic view of market conditions. The combined\napproach aims to improve prediction accuracy and provide contextually rich\nstock advising.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12807v1"
    },
    {
        "title": "Functional Clustering of Discount Functions for Behavioral Investor\n  Profiling",
        "authors": [
            "Annamaria Porreca",
            "Viviana Ventre",
            "Roberta Martino",
            "Salvador Cruz Rambaud",
            "Fabrizio Maturo"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Classical finance models are based on the premise that investors act\nrationally and utilize all available information when making portfolio\ndecisions. However, these models often fail to capture the anomalies observed\nin intertemporal choices and decision-making under uncertainty, particularly\nwhen accounting for individual differences in preferences and consumption\npatterns. Such limitations hinder traditional finance theory's ability to\naddress key questions like: How do personal preferences shape investment\nchoices? What drives investor behaviour? And how do individuals select their\nportfolios? One prominent contribution is Pompian's model of four Behavioral\nInvestor Types (BITs), which links behavioural finance studies with Keirsey's\ntemperament theory, highlighting the role of personality in financial\ndecision-making. Yet, traditional parametric models struggle to capture how\nthese distinct temperaments influence intertemporal decisions, such as how\nindividuals evaluate trade-offs between present and future outcomes. To address\nthis gap, the present study employs Functional Data Analysis (FDA) to\nspecifically investigate temporal discounting behaviours revealing nuanced\npatterns in how different temperaments perceive and manage uncertainty over\ntime. Our findings show heterogeneity within each temperament, suggesting that\ninvestor profiles are far more diverse than previously thought. This refined\nclassification provides deeper insights into the role of temperament in shaping\nintertemporal financial decisions, offering practical implications for\nfinancial advisors to better tailor strategies to individual risk preferences\nand decision-making styles.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.16307v1"
    },
    {
        "title": "Zero-Coupon Treasury Yield Curve with VIX as Stochastic Volatility",
        "authors": [
            "Jihyun Park",
            "Andrey Sarantsev"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  We study a multivariate autoregressive stochastic volatility model for the\nfirst 3 principal components (level, slope, curvature) of 10 series of\nzero-coupon Treasury bond rates with maturities from 1 to 10 years. We fit this\nmodel using monthly data from 1990. Next, we prove long-term stability for this\ndiscrete-time model and its continuous-time version. Unlike classic models with\nhidden stochastic volatility, here it is observed as VIX: the volatility index\nfor the S\\&P 500 stock market index. It is surprising that this volatility,\ncreated for the stock market, also works for Treasury bonds. Since total\nreturns of zero-coupon bonds can be easily found from these principal\ncomponents, we prove long-term stability for total returns in discrete time.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.03699v3"
    },
    {
        "title": "Enhancing literature review with LLM and NLP methods. Algorithmic\n  trading case",
        "authors": [
            "Stanisław Łaniewski",
            "Robert Ślepaczuk"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This study utilizes machine learning algorithms to analyze and organize\nknowledge in the field of algorithmic trading. By filtering a dataset of 136\nmillion research papers, we identified 14,342 relevant articles published\nbetween 1956 and Q1 2020. We compare traditional practices-such as\nkeyword-based algorithms and embedding techniques-with state-of-the-art topic\nmodeling methods that employ dimensionality reduction and clustering. This\ncomparison allows us to assess the popularity and evolution of different\napproaches and themes within algorithmic trading. We demonstrate the usefulness\nof Natural Language Processing (NLP) in the automatic extraction of knowledge,\nhighlighting the new possibilities created by the latest iterations of Large\nLanguage Models (LLMs) like ChatGPT. The rationale for focusing on this topic\nstems from our analysis, which reveals that research articles on algorithmic\ntrading are increasing at a faster rate than the overall number of\npublications. While stocks and main indices comprise more than half of all\nassets considered, certain asset classes, such as cryptocurrencies, exhibit a\nmuch stronger growth trend. Machine learning models have become the most\npopular methods in recent years. The study demonstrates the efficacy of LLMs in\nrefining datasets and addressing intricate questions about the analyzed\narticles, such as comparing the efficiency of different models. Our research\nshows that by decomposing tasks into smaller components and incorporating\nreasoning steps, we can effectively tackle complex questions supported by case\nanalyses. This approach contributes to a deeper understanding of algorithmic\ntrading methodologies and underscores the potential of advanced NLP techniques\nin literature reviews.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05013v1"
    },
    {
        "title": "Utilizing RNN for Real-time Cryptocurrency Price Prediction and Trading\n  Strategy Optimization",
        "authors": [
            "Shamima Nasrin Tumpa",
            "Kehelwala Dewage Gayan Maduranga"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This study explores the use of Recurrent Neural Networks (RNN) for real-time\ncryptocurrency price prediction and optimized trading strategies. Given the\nhigh volatility of the cryptocurrency market, traditional forecasting models\noften fall short. By leveraging RNNs' capability to capture long-term patterns\nin time-series data, this research aims to improve accuracy in price prediction\nand develop effective trading strategies. The project follows a structured\napproach involving data collection, preprocessing, and model refinement,\nfollowed by rigorous backtesting for profitability and risk assessment. This\nwork contributes to both the academic and practical fields by providing a\nrobust predictive model and optimized trading strategies that address the\nchallenges of cryptocurrency trading.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05829v1"
    },
    {
        "title": "Approaching multifractal complexity in decentralized cryptocurrency\n  trading",
        "authors": [
            "Marcin Wątorek",
            "Marcin Królczyk",
            "Jarosław Kwapień",
            "Tomasz Stanisz",
            "Stanisław Drożdż"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Multifractality is a concept that helps compactly grasping the most essential\nfeatures of the financial dynamics. In its fully developed form, this concept\napplies to essentially all mature financial markets and even to more liquid\ncryptocurrencies traded on the centralized exchanges. A new element that adds\ncomplexity to cryptocurrency markets is the possibility of decentralized\ntrading. Based on the extracted tick-by-tick transaction data from the\nUniversal Router contract of the Uniswap decentralized exchange, from June 6,\n2023, to June 30, 2024, the present study using Multifractal Detrended\nFluctuation Analysis (MFDFA) shows that even though liquidity on these new\nexchanges is still much lower compared to centralized exchanges convincing\ntraces of multifractality are already emerging on this new trading as well. The\nresulting multifractal spectra are however strongly left-side asymmetric which\nindicates that this multifractality comes primarily from large fluctuations and\nsmall ones are more of the uncorrelated noise type. What is particularly\ninteresting here is the fact that multifractality is more developed for time\nseries representing transaction volumes than rates of return. On the level of\nthese larger events a trace of multifractal cross-correlations between the two\ncharacteristics is also observed.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05951v1"
    },
    {
        "title": "Filling in Missing FX Implied Volatilities with Uncertainties: Improving\n  VAE-Based Volatility Imputation",
        "authors": [
            "Achintya Gopal"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Missing data is a common problem in finance and often requires methods to\nfill in the gaps, or in other words, imputation. In this work, we focused on\nthe imputation of missing implied volatilities for FX options. Prior work has\nused variational autoencoders (VAEs), a neural network-based approach, to solve\nthis problem; however, using stronger classical baselines such as Heston with\njumps can significantly outperform their results. We show that simple\nmodifications to the architecture of the VAE lead to significant imputation\nperformance improvements (e.g., in low missingness regimes, nearly cutting the\nerror by half), removing the necessity of using $\\beta$-VAEs. Further, we\nmodify the VAE imputation algorithm in order to better handle the uncertainty\nin data, as well as to obtain accurate uncertainty estimates around imputed\nvalues.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05998v1"
    },
    {
        "title": "The Role of AI in Financial Forecasting: ChatGPT's Potential and\n  Challenges",
        "authors": [
            "Shuochen Bi",
            "Tingting Deng",
            "Jue Xiao"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The outlook for the future of artificial intelligence (AI) in the financial\nsector, especially in financial forecasting, the challenges and implications.\nThe dynamics of AI technology, including deep learning, reinforcement learning,\nand integration with blockchAIn and the Internet of Things, also highlight the\ncontinued improvement in data processing capabilities. Explore how AI is\nreshaping financial services with precisely tAIlored services that can more\nprecisely meet the diverse needs of individual investors. The integration of AI\nchallenges regulatory and ethical issues in the financial sector, as well as\nthe implications for data privacy protection. Analyze the limitations of\ncurrent AI technology in financial forecasting and its potential impact on the\nfuture financial industry landscape, including changes in the job market, the\nemergence of new financial institutions, and user interface innovations.\nEmphasizing the importance of increasing investor understanding and awareness\nof AI and looking ahead to future trends in AI tools for user experience to\ndrive wider adoption of AI in financial decision making. The huge potential,\nchallenges, and future directions of AI in the financial sector highlight the\ncritical role of AI technology in driving transformation and innovation in the\nfinancial sector\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13562v1"
    },
    {
        "title": "A Deep Learning Approach to Predict the Fall [of Price] of\n  Cryptocurrency Long Before its Actual Fall",
        "authors": [
            "Anika Tahsin Meem"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  In modern times, the cryptocurrency market is one of the world's most rapidly\nrising financial markets. The cryptocurrency market is regarded to be more\nvolatile and illiquid than traditional markets such as equities, foreign\nexchange, and commodities. The risk of this market creates an uncertain\ncondition among the investors. The purpose of this research is to predict the\nmagnitude of the risk factor of the cryptocurrency market. Risk factor is also\ncalled volatility. Our approach will assist people who invest in the\ncryptocurrency market by overcoming the problems and difficulties they\nexperience. Our approach starts with calculating the risk factor of the\ncryptocurrency market from the existing parameters. In twenty elements of the\ncryptocurrency market, the risk factor has been predicted using different\nmachine learning algorithms such as CNN, LSTM, BiLSTM, and GRU. All of the\nmodels have been applied to the calculated risk factor parameter. A new model\nhas been developed to predict better than the existing models. Our proposed\nmodel gives the highest RMSE value of 1.3229 and the lowest RMSE value of\n0.0089. Following our model, it will be easier for investors to trade in\ncomplicated and challenging financial assets like bitcoin, Ethereum, dogecoin,\netc. Where the other existing models, the highest RMSE was 14.5092, and the\nlower was 0.02769. So, the proposed model performs much better than models with\nproper generalization. Using our approach, it will be easier for investors to\ntrade in complicated and challenging financial assets like Bitcoin, Ethereum,\nand Dogecoin.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13615v3"
    },
    {
        "title": "Probabilistic Predictions of Option Prices Using Multiple Sources of\n  Data",
        "authors": [
            "Worapree Maneesoonthorn",
            "David T. Frazier",
            "Gael M. Martin"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  A new modular approximate Bayesian inferential framework is proposed that\nenables fast calculation of probabilistic predictions of future option prices.\nWe exploit multiple information sources, including daily spot returns,\nhigh-frequency spot data and option prices. A benefit of this modular Bayesian\napproach is that it allows us to work with the theoretical option pricing\nmodel, without needing to specify an arbitrary statistical model that links the\ntheoretical prices to their observed counterparts. We show that our approach\nproduces accurate probabilistic predictions of option prices in realistic\nscenarios and, despite not explicitly modelling pricing errors, the method is\nshown to be robust to their presence. Predictive accuracy based on the Heston\nstochastic volatility model, with predictions produced via rapid real-time\nupdates, is illustrated empirically for short-maturity options.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.00658v1"
    },
    {
        "title": "Time Series Feature Redundancy Paradox: An Empirical Study Based on\n  Mortgage Default Prediction",
        "authors": [
            "Chengyue Huang",
            "Yahe Yang"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  With the widespread application of machine learning in financial risk\nmanagement, conventional wisdom suggests that longer training periods and more\nfeature variables contribute to improved model performance. This paper,\nfocusing on mortgage default prediction, empirically discovers a phenomenon\nthat contradicts traditional knowledge: in time series prediction, increased\ntraining data timespan and additional non-critical features actually lead to\nsignificant deterioration in prediction effectiveness. Using Fannie Mae's\nmortgage data, the study compares predictive performance across different time\nwindow lengths (2012-2022) and feature combinations, revealing that shorter\ntime windows (such as single-year periods) paired with carefully selected key\nfeatures yield superior prediction results. The experimental results indicate\nthat extended time spans may introduce noise from historical data and outdated\nmarket patterns, while excessive non-critical features interfere with the\nmodel's learning of core default factors. This research not only challenges the\ntraditional \"more is better\" approach in data modeling but also provides new\ninsights and practical guidance for feature selection and time window\noptimization in financial risk prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.00034v1"
    },
    {
        "title": "Econophysics, Statistical Mechanics Approach to",
        "authors": [
            "Victor M. Yakovenko"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  This is a review article for Encyclopedia of Complexity and System Science,\nto be published by Springer http://refworks.springer.com/complexity/. The paper\nreviews statistical models for money, wealth, and income distributions\ndeveloped in the econophysics literature since late 1990s.\n",
        "pdf_link": "http://arxiv.org/pdf/0709.3662v4"
    },
    {
        "title": "Likelihood-based inference for correlated diffusions",
        "authors": [
            "Konstantinos Kalogeropoulos",
            "Petros Dellaportas",
            "Gareth O. Roberts"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  We address the problem of likelihood based inference for correlated diffusion\nprocesses using Markov chain Monte Carlo (MCMC) techniques. Such a task\npresents two interesting problems. First, the construction of the MCMC scheme\nshould ensure that the correlation coefficients are updated subject to the\npositive definite constraints of the diffusion matrix. Second, a diffusion may\nonly be observed at a finite set of points and the marginal likelihood for the\nparameters based on these observations is generally not available. We overcome\nthe first issue by using the Cholesky factorisation on the diffusion matrix. To\ndeal with the likelihood unavailability, we generalise the data augmentation\nframework of Roberts and Stramer (2001 Biometrika 88(3):603-621) to\nd-dimensional correlated diffusions including multivariate stochastic\nvolatility models. Our methodology is illustrated through simulation based\nexperiments and with daily EUR /USD, GBP/USD rates together with their implied\nvolatilities.\n",
        "pdf_link": "http://arxiv.org/pdf/0711.1595v1"
    },
    {
        "title": "Quantitative comparisons between finitary posterior distributions and\n  Bayesian posterior distributions",
        "authors": [
            "Federico Bassetti"
        ],
        "category": "q-fin.ST",
        "published_year": "2008",
        "summary": "  The main object of Bayesian statistical inference is the determination of\nposterior distributions. Sometimes these laws are given for quantities devoid\nof empirical value. This serious drawback vanishes when one confines oneself to\nconsidering a finite horizon framework. However, assuming infinite\nexchangeability gives rise to fairly tractable {\\it a posteriori} quantities,\nwhich is very attractive in applications. Hence, with a view to a\nreconciliation between these two aspects of the Bayesian way of reasoning, in\nthis paper we provide quantitative comparisons between posterior distributions\nof finitary parameters and posterior distributions of allied parameters\nappearing in usual statistical models.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.1201v1"
    },
    {
        "title": "An information theoretic approach to statistical dependence: copula\n  information",
        "authors": [
            "Rafael S. Calsaverini",
            "Renato Vicente"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  We discuss the connection between information and copula theories by showing\nthat a copula can be employed to decompose the information content of a\nmultivariate distribution into marginal and dependence components, with the\nlatter quantified by the mutual information. We define the information excess\nas a measure of deviation from a maximum entropy distribution. The idea of\nmarginal invariant dependence measures is also discussed and used to show that\nempirical linear correlation underestimates the amplitude of the actual\ncorrelation in the case of non-Gaussian marginals. The mutual information is\nshown to provide an upper bound for the asymptotic empirical log-likelihood of\na copula. An analytical expression for the information excess of T-copulas is\nprovided, allowing for simple model identification within this family. We\nillustrate the framework in a financial data set.\n",
        "pdf_link": "http://arxiv.org/pdf/0911.4207v1"
    },
    {
        "title": "Cumulant Approach of Arbitrary Truncated Levy Flight",
        "authors": [
            "Dmitry V. Vinogradov"
        ],
        "category": "q-fin.ST",
        "published_year": "2010",
        "summary": "  The problem of an arbitrary truncated Levy flight description using the\nmethod of cumulant approach has been solved. The set of cumulants of the\ntruncated Levy distribution given the assumption of arbitrary truncation has\nbeen found. The influence of truncation shape on the truncated Levy flight\nproperties in the Gaussian and the Levy regimes has been investigated.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.2489v5"
    },
    {
        "title": "Forecasting day-ahead electricity prices in Europe: the importance of\n  considering market integration",
        "authors": [
            "Jesus Lago",
            "Fjo De Ridder",
            "Peter Vrancx",
            "Bart De Schutter"
        ],
        "category": "q-fin.ST",
        "published_year": "2017",
        "summary": "  Motivated by the increasing integration among electricity markets, in this\npaper we propose two different methods to incorporate market integration in\nelectricity price forecasting and to improve the predictive performance. First,\nwe propose a deep neural network that considers features from connected markets\nto improve the predictive accuracy in a local market. To measure the importance\nof these features, we propose a novel feature selection algorithm that, by\nusing Bayesian optimization and functional analysis of variance, evaluates the\neffect of the features on the algorithm performance. In addition, using market\nintegration, we propose a second model that, by simultaneously predicting\nprices from two markets, improves the forecasting accuracy even further. As a\ncase study, we consider the electricity market in Belgium and the improvements\nin forecasting accuracy when using various French electricity features. We show\nthat the two proposed models lead to improvements that are statistically\nsignificant. Particularly, due to market integration, the predictive accuracy\nis improved from 15.7% to 12.5% sMAPE (symmetric mean absolute percentage\nerror). In addition, we show that the proposed feature selection algorithm is\nable to perform a correct assessment, i.e. to discard the irrelevant features.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07061v3"
    },
    {
        "title": "A Blockchain Transaction Graph based Machine Learning Method for Bitcoin\n  Price Prediction",
        "authors": [
            "Xiao Li",
            "Weili Wu"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Bitcoin, as one of the most popular cryptocurrency, is recently attracting\nmuch attention of investors. Bitcoin price prediction task is consequently a\nrising academic topic for providing valuable insights and suggestions. Existing\nbitcoin prediction works mostly base on trivial feature engineering, that\nmanually designs features or factors from multiple areas, including Bticoin\nBlockchain information, finance and social media sentiments. The feature\nengineering not only requires much human effort, but the effectiveness of the\nintuitively designed features can not be guaranteed. In this paper, we aim to\nmining the abundant patterns encoded in bitcoin transactions, and propose\nk-order transaction graph to reveal patterns under different scope. We propose\nthe transaction graph based feature to automatically encode the patterns. A\nnovel prediction method is proposed to accept the features and make price\nprediction, which can take advantage from particular patterns from different\nhistory period. The results of comparison experiments demonstrate that the\nproposed method outperforms the most recent state-of-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.09667v1"
    },
    {
        "title": "Inferring short-term volatility indicators from Bitcoin blockchain",
        "authors": [
            "Nino Antulov-Fantulin",
            "Dijana Tolic",
            "Matija Piskorec",
            "Zhang Ce",
            "Irena Vodenska"
        ],
        "category": "q-fin.ST",
        "published_year": "2018",
        "summary": "  In this paper, we study the possibility of inferring early warning indicators\n(EWIs) for periods of extreme bitcoin price volatility using features obtained\nfrom Bitcoin daily transaction graphs. We infer the low-dimensional\nrepresentations of transaction graphs in the time period from 2012 to 2017\nusing Bitcoin blockchain, and demonstrate how these representations can be used\nto predict extreme price volatility events. Our EWI, which is obtained with a\nnon-negative decomposition, contains more predictive information than those\nobtained with singular value decomposition or scalar value of the total Bitcoin\ntransaction volume.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.07856v1"
    },
    {
        "title": "Information dynamics of price and liquidity around the 2017 Bitcoin\n  markets crash",
        "authors": [
            "Vaiva Vasiliauskaite",
            "Fabrizio Lillo",
            "Nino Antulov-Fantulin"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  We study the information dynamics between the largest Bitcoin exchange\nmarkets during the bubble in 2017-2018. By analysing high-frequency\nmarket-microstructure observables with different information theoretic measures\nfor dynamical systems, we find temporal changes in information sharing across\nmarkets. In particular, we study the time-varying components of predictability,\nmemory, and synchronous coupling, measured by transfer entropy, active\ninformation storage, and multi-information. By comparing these empirical\nfindings with several models we argue that some results could relate to\nintra-market and inter-market regime shifts, and changes in direction of\ninformation flow between different market observables.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.09057v1"
    },
    {
        "title": "Causal Analysis of Generic Time Series Data Applied for Market\n  Prediction",
        "authors": [
            "Anton Kolonin",
            "Ali Raheman",
            "Mukul Vishwas",
            "Ikram Ansari",
            "Juan Pinzon",
            "Alice Ho"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We explore the applicability of the causal analysis based on temporally\nshifted (lagged) Pearson correlation applied to diverse time series of\ndifferent natures in context of the problem of financial market prediction.\nTheoretical discussion is followed by description of the practical approach for\nspecific environment of time series data with diverse nature and sparsity, as\napplied for environments of financial markets. The data involves various\nfinancial metrics computable from raw market data such as real-time trades and\nsnapshots of the limit order book as well as metrics determined upon social\nmedia news streams such as sentiment and different cognitive distortions. The\napproach is backed up with presentation of algorithmic framework for data\nacquisition and analysis, concluded with experimental results, and summary\npointing out at the possibility to discriminate causal connections between\ndifferent sorts of real field market data with further discussion on present\nissues and possible directions of the following work.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.12928v1"
    },
    {
        "title": "Statistical inference of lead-lag at various timescales between\n  asynchronous time series from p-values of transfer entropy",
        "authors": [
            "Christian Bongiorno",
            "Damien Challet"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  Symbolic transfer entropy is a powerful non-parametric tool to detect\nlead-lag between time series. Because a closed expression of the distribution\nof Transfer Entropy is not known for finite-size samples, statistical testing\nis often performed with bootstraps whose slowness prevents the inference of\nlarge lead-lag networks between long time series. On the other hand, the\nasymptotic distribution of Transfer Entropy between two time series is known.\nIn this work, we derive the asymptotic distribution of the test for one time\nseries having a larger Transfer Entropy than another one on a target time\nseries. We then measure the convergence speed of both tests in the small sample\nsize limits via benchmarks. We then introduce Transfer Entropy between\ntime-shifted time series, which allows to measure the timescale at which\ninformation transfer is maximal and vanishes. We finally apply these methods to\ntick-by-tick price changes of several hundreds of stocks, yielding non-trivial\nstatistically validated networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.10173v1"
    },
    {
        "title": "Short-time expansion of characteristic functions in a rough volatility\n  setting with applications",
        "authors": [
            "Carsten H. Chong",
            "Viktor Todorov"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We derive a higher-order asymptotic expansion of the conditional\ncharacteristic function of the increment of an It\\^o semimartingale over a\nshrinking time interval. The spot characteristics of the It\\^o semimartingale\nare allowed to have dynamics of general form. In particular, their paths can be\nrough, that is, exhibit local behavior like that of a fractional Brownian\nmotion, while at the same time have jumps with arbitrary degree of activity.\nThe expansion result shows the distinct roles played by the different features\nof the spot characteristics dynamics. As an application of our result, we\nconstruct a nonparametric estimator of the Hurst parameter of the diffusive\nvolatility process from portfolios of short-dated options written on an\nunderlying asset.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.00830v2"
    },
    {
        "title": "Modeling Volatility and Dependence of European Carbon and Energy Prices",
        "authors": [
            "Jonathan Berrisch",
            "Sven Pappert",
            "Florian Ziel",
            "Antonia Arsova"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  We study the prices of European Emission Allowances (EUA), whereby we analyze\ntheir uncertainty and dependencies on related energy prices (natural gas, coal,\nand oil). We propose a probabilistic multivariate conditional time series model\nwith a VECM-Copula-GARCH structure which exploits key characteristics of the\ndata. Data are normalized with respect to inflation and carbon emissions to\nallow for proper cross-series evaluation. The forecasting performance is\nevaluated in an extensive rolling-window forecasting study, covering eight\nyears out-of-sample. We discuss our findings for both levels- and\nlog-transformed data, focusing on time-varying correlations, and in view of the\nRussian invasion of Ukraine.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.14311v4"
    },
    {
        "title": "Cross-Domain Shopping and Stock Trend Analysis",
        "authors": [
            "Aditya Pandey",
            "Haseeba Fathiya",
            "Nivedita Patel"
        ],
        "category": "q-fin.ST",
        "published_year": "2022",
        "summary": "  This paper presents a cross-domain trend analysis that aims to identify and\nanalyze the relationships between stock prices, stock news on Twitter, and\nusers' behaviors on e-commerce websites. The analysis is based on three\ndatasets: a US stock dataset, a stock tweets dataset, and an e-commerce\nbehavior dataset. The analysis is performed using Hadoop, Hive, and Tableau,\nallowing for efficient and scalable processing and visualizing large datasets.\n  The analysis includes trend analysis of Twitter sentiment (positive and\nnegative tweets) and correlation analysis, including the correlation between\ntweet sentiment and stocks, the correlation between stock trends and shopping\nbehavior, and the understanding of data based on different slices of time. By\ncomparing different features from the datasets over time, we hope to gain\ninsight into the factors that drive user behavior as well as the market in\ndifferent categories. The results of this analysis can provide valuable\ninsights for businesses and investors to inform decision-making.\n  We believe that our analysis can serve as a valuable starting point for\nfurther research and investigation into these topics.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.14689v1"
    },
    {
        "title": "Characterizing Financial Market Coverage using Artificial Intelligence",
        "authors": [
            "Jean Marie Tshimula",
            "D'Jeff K. Nkashama",
            "Patrick Owusu",
            "Marc Frappier",
            "Pierre-Martin Tardif",
            "Froduald Kabanza",
            "Armelle Brun",
            "Jean-Marc Patenaude",
            "Shengrui Wang",
            "Belkacem Chikhaoui"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  This paper scrutinizes a database of over 4900 YouTube videos to characterize\nfinancial market coverage. Financial market coverage generates a large number\nof videos. Therefore, watching these videos to derive actionable insights could\nbe challenging and complex. In this paper, we leverage Whisper, a\nspeech-to-text model from OpenAI, to generate a text corpus of market coverage\nvideos from Bloomberg and Yahoo Finance. We employ natural language processing\nto extract insights regarding language use from the market coverage. Moreover,\nwe examine the prominent presence of trending topics and their evolution over\ntime, and the impacts that some individuals and organizations have on the\nfinancial market. Our characterization highlights the dynamics of the financial\nmarket coverage and provides valuable insights reflecting broad discussions\nregarding recent financial events and the world economy.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.03694v1"
    },
    {
        "title": "TM-vector: A Novel Forecasting Approach for Market stock movement with a\n  Rich Representation of Twitter and Market data",
        "authors": [
            "Faraz Sasani",
            "Ramin Mousa",
            "Ali Karkehabadi",
            "Samin Dehbashi",
            "Ali Mohammadi"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  Stock market forecasting has been a challenging part for many analysts and\nresearchers. Trend analysis, statistical techniques, and movement indicators\nhave traditionally been used to predict stock price movements, but text\nextraction has emerged as a promising method in recent years. The use of neural\nnetworks, especially recurrent neural networks, is abundant in the literature.\nIn most studies, the impact of different users was considered equal or ignored,\nwhereas users can have other effects. In the current study, we will introduce\nTM-vector and then use this vector to train an IndRNN and ultimately model the\nmarket users' behaviour. In the proposed model, TM-vector is simultaneously\ntrained with both the extracted Twitter features and market information.\nVarious factors have been used for the effectiveness of the proposed\nforecasting approach, including the characteristics of each individual user,\ntheir impact on each other, and their impact on the market, to predict market\ndirection more accurately. Dow Jones 30 index has been used in current work.\nThe accuracy obtained for predicting daily stock changes of Apple is based on\nvarious models, closed to over 95\\% and for the other stocks is significant.\nOur results indicate the effectiveness of TM-vector in predicting stock market\ndirection.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.02094v1"
    },
    {
        "title": "Trend patterns statistics for assessing irreversibility in\n  cryptocurrencies: time-asymmetry versus inefficiency",
        "authors": [
            "Jessica Morales Herrera",
            "Raúl Salgado-García"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  In this paper, we present a measure of time irreversibility using trend\npattern statistics. We define the irreversibility index as the Kullback-Leibler\ndivergence between the distribution of uptrends subsequences (increasing\ntrends) and the corresponding downtrends subsequences distribution (decreasing\ntrends) in a time series. We use this index to analyze the degree of\nirreversibility in log return series over time, specifically focusing on five\ncryptocurrencies: Bitcoin, Ethereum, Ripple, Litecoin, and Bitcoin Cash. Our\nanalysis reveals a strong indication of irreversibility in all these\ncryptocurrencies and the characteristic evolves over time. We additionally\nevaluate the market efficiency for these cryptocurrencies based on a recently\nproposed information-theoretic measure. By comparing inefficiency and\nirreversibility, we explore the relationship between these statistical\nfeatures. This comparison provides insight into the non-trivial relationship\nbetween inefficiency and irreversibility.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.08612v1"
    },
    {
        "title": "Tweet Influence on Market Trends: Analyzing the Impact of Social Media\n  Sentiment on Biotech Stocks",
        "authors": [
            "C. Sarai R. Avila"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  This study investigates the relationship between tweet sentiment across\ndiverse categories: news, company opinions, CEO opinions, competitor opinions,\nand stock market behavior in the biotechnology sector, with a focus on\nunderstanding the impact of social media discourse on investor sentiment and\ndecision-making processes. We analyzed historical stock market data for ten of\nthe largest and most influential pharmaceutical companies alongside Twitter\ndata related to COVID-19, vaccines, the companies, and their respective CEOs.\nUsing VADER sentiment analysis, we examined the sentiment scores of tweets and\nassessed their relationships with stock market performance. We employed ARIMA\n(AutoRegressive Integrated Moving Average) and VAR (Vector AutoRegression)\nmodels to forecast stock market performance, incorporating sentiment covariates\nto improve predictions. Our findings revealed a complex interplay between tweet\nsentiment, news, biotech companies, their CEOs, and stock market performance,\nemphasizing the importance of considering diverse factors when modeling and\npredicting stock prices. This study provides valuable insights into the\ninfluence of social media on the financial sector and lays a foundation for\nfuture research aimed at refining stock price prediction models.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.03353v1"
    },
    {
        "title": "Detection of financial opportunities in micro-blogging data with a\n  stacked classification system",
        "authors": [
            "Francisco de Arriba-Pérez",
            "Silvia García-Méndez",
            "José A. Regueiro-Janeiro",
            "Francisco J. González-Castaño"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  Micro-blogging sources such as the Twitter social network provide valuable\nreal-time data for market prediction models. Investors' opinions in this\nnetwork follow the fluctuations of the stock markets and often include educated\nspeculations on market opportunities that may have impact on the actions of\nother investors. In view of this, we propose a novel system to detect positive\npredictions in tweets, a type of financial emotions which we term\n\"opportunities\" that are akin to \"anticipation\" in Plutchik's theory.\nSpecifically, we seek a high detection precision to present a financial\noperator a substantial amount of such tweets while differentiating them from\nthe rest of financial emotions in our system. We achieve it with a three-layer\nstacked Machine Learning classification system with sophisticated features that\nresult from applying Natural Language Processing techniques to extract valuable\nlinguistic information. Experimental results on a dataset that has been\nmanually annotated with financial emotion and ticker occurrence tags\ndemonstrate that our system yields satisfactory and competitive performance in\nfinancial opportunity detection, with precision values up to 83%. This\npromising outcome endorses the usability of our system to support investors'\ndecision making.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.07224v1"
    },
    {
        "title": "Correlations versus noise in the NFT market",
        "authors": [
            "Marcin Wątorek",
            "Paweł Szydło",
            "Jarosław Kwapień",
            "Stanisław Drożdż"
        ],
        "category": "q-fin.ST",
        "published_year": "2024",
        "summary": "  The non-fungible token (NFT) market emerges as a recent trading innovation\nleveraging blockchain technology, mirroring the dynamics of the cryptocurrency\nmarket. The current study is based on the capitalization changes and\ntransaction volumes across a large number of token collections on the Ethereum\nplatform. In order to deepen the understanding of the market dynamics, the\ncollection-collection dependencies are examined by using the multivariate\nformalism of detrended correlation coefficient and correlation matrix. It\nappears that correlation strength is lower here than that observed in\npreviously studied markets. Consequently, the eigenvalue spectra of the\ncorrelation matrix more closely follow the Marchenko-Pastur distribution,\nstill, some departures indicating the existence of correlations remain. The\ncomparison of results obtained from the correlation matrix built from the\nPearson coefficients and, independently, from the detrended cross-correlation\ncoefficients suggests that the global correlations in the NFT market arise from\nhigher frequency fluctuations. Corresponding minimal spanning trees (MSTs) for\ncapitalization variability exhibit a scale-free character while, for the number\nof transactions, they are somewhat more decentralized.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.15495v2"
    },
    {
        "title": "From Physics to Economics: An Econometric Example Using Maximum Relative\n  Entropy",
        "authors": [
            "Adom Giffin"
        ],
        "category": "q-fin.ST",
        "published_year": "2009",
        "summary": "  Econophysics, is based on the premise that some ideas and methods from\nphysics can be applied to economic situations. We intend to show in this paper\nhow a physics concept such as entropy can be applied to an economic problem. In\nso doing, we demonstrate how information in the form of observable data and\nmoment constraints are introduced into the method of Maximum relative Entropy\n(MrE). A general example of updating with data and moments is shown. Two\nspecific econometric examples are solved in detail which can then be used as\ntemplates for real world problems. A numerical example is compared to a large\ndeviation solution which illustrates some of the advantages of the MrE method.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.0401v1"
    },
    {
        "title": "Ensemble Forecasting for Intraday Electricity Prices: Simulating\n  Trajectories",
        "authors": [
            "Michał Narajewski",
            "Florian Ziel"
        ],
        "category": "q-fin.ST",
        "published_year": "2020",
        "summary": "  Recent studies concerning the point electricity price forecasting have shown\nevidence that the hourly German Intraday Continuous Market is weak-form\nefficient. Therefore, we take a novel, advanced approach to the problem. A\nprobabilistic forecasting of the hourly intraday electricity prices is\nperformed by simulating trajectories in every trading window to receive a\nrealistic ensemble to allow for more efficient intraday trading and redispatch.\nA generalized additive model is fitted to the price differences with the\nassumption that they follow a zero-inflated distribution, precisely a mixture\nof the Dirac and the Student's t-distributions. Moreover, the mixing term is\nestimated using a high-dimensional logistic regression with lasso penalty. We\nmodel the expected value and volatility of the series using i.a. autoregressive\nand no-trade effects or load, wind and solar generation forecasts and\naccounting for the non-linearities in e.g. time to maturity. Both the in-sample\ncharacteristics and forecasting performance are analysed using a rolling window\nforecasting study. Multiple versions of the model are compared to several\nbenchmark models and evaluated using probabilistic forecasting measures and\nsignificance tests. The study aims to forecast the price distribution in the\nGerman Intraday Continuous Market in the last 3 hours of trading, but the\napproach allows for application to other continuous markets, especially in\nEurope. The results prove superiority of the mixture model over the benchmarks\ngaining the most from the modelling of the volatility. They also indicate that\nthe introduction of XBID reduced the market volatility.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.01365v3"
    },
    {
        "title": "Efficient Calibration of Multi-Agent Simulation Models from Output\n  Series with Bayesian Optimization",
        "authors": [
            "Yuanlu Bai",
            "Henry Lam",
            "Svitlana Vyetrenko",
            "Tucker Balch"
        ],
        "category": "q-fin.ST",
        "published_year": "2021",
        "summary": "  Multi-agent simulation is commonly used across multiple disciplines,\nspecifically in artificial intelligence in recent years, which creates an\nenvironment for downstream machine learning or reinforcement learning tasks. In\nmany practical scenarios, however, only the output series that result from the\ninteractions of simulation agents are observable. Therefore, simulators need to\nbe calibrated so that the simulated output series resemble historical -- which\namounts to solving a complex simulation optimization problem. In this paper, we\npropose a simple and efficient framework for calibrating simulator parameters\nfrom historical output series observations. First, we consider a novel concept\nof eligibility set to bypass the potential non-identifiability issue. Second,\nwe generalize the two-sample Kolmogorov-Smirnov (K-S) test with Bonferroni\ncorrection to test the similarity between two high-dimensional distributions,\nwhich gives a simple yet effective distance metric between the output series\nsample sets. Third, we suggest using Bayesian optimization (BO) and\ntrust-region BO (TuRBO) to minimize the aforementioned distance metric.\nFinally, we demonstrate the efficiency of our framework using numerical\nexperiments both on a multi-agent financial market simulator.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.03874v2"
    },
    {
        "title": "Permutation invariant Gaussian matrix models for financial correlation\n  matrices",
        "authors": [
            "George Barnes",
            "Sanjaye Ramgoolam",
            "Michael Stephanou"
        ],
        "category": "q-fin.ST",
        "published_year": "2023",
        "summary": "  We construct an ensemble of correlation matrices from high-frequency foreign\nexchange market data, with one matrix for every day for 446 days. The matrices\nare symmetric and have vanishing diagonal elements after subtracting the\nidentity matrix. For this case, we construct the general permutation invariant\nGaussian matrix model, which has 4 parameters characterised using the\nrepresentation theory of symmetric groups. The permutation invariant polynomial\nfunctions of the symmetric, diagonally vanishing matrices have a basis labelled\nby undirected loop-less graphs. Using the expectation values of the general\nlinear and quadratic permutation invariant functions of the matrices in the\ndataset, the 4 parameters of the matrix model are determined. The model then\npredicts the expectation values of the cubic and quartic polynomials. These\npredictions are compared to the data to give strong evidence for a good overall\nfit of the permutation invariant Gaussian matrix model. The linear, quadratic,\ncubic and quartic polynomial functions are then used to define low-dimensional\nfeature vectors for the days associated to the matrices. These vectors, with\nchoices informed by the refined structure of small non-Gaussianities, are found\nto be effective as a tool for anomaly detection in market states: statistically\nsignificant correlations are established between atypical days as defined using\nthese feature vectors, and days with significant economic events as recognized\nin standard foreign exchange economic calendars. They are also shown to be\nuseful as a tool for ranking pairs of days in terms of their similarity,\nyielding a strongly statistically significant correlation with a ranking based\non a higher dimensional proxy for visual similarity.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.04569v1"
    },
    {
        "title": "A Bayesian Framework for Combining Valuation Estimates",
        "authors": [
            "Kenton K. Yee"
        ],
        "category": "q-fin.ST",
        "published_year": "2007",
        "summary": "  Obtaining more accurate equity value estimates is the starting point for\nstock selection, value-based indexing in a noisy market, and beating benchmark\nindices through tactical style rotation. Unfortunately, discounted cash flow,\nmethod of comparables, and fundamental analysis typically yield discrepant\nvaluation estimates. Moreover, the valuation estimates typically disagree with\nmarket price. Can one form a superior valuation estimate by averaging over the\nindividual estimates, including market price? This article suggests a Bayesian\nframework for combining two or more estimates into a superior valuation\nestimate. The framework justifies the common practice of averaging over several\nestimates to arrive at a final point estimate.\n",
        "pdf_link": "http://arxiv.org/pdf/0707.3482v1"
    }
]