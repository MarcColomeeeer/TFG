[
    {
        "title": "Cognitive Constructivism and the Epistemic Significance of Sharp\n  Statistical Hypotheses in Natural Sciences",
        "authors": [
            "J. M. Stern"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  This book presents our case in defense of a constructivist epistemological\nframework and the use of compatible statistical theory and inference tools. The\nbasic metaphor of decision theory is the maximization of a gambler's expected\nfortune, according to his own subjective utility, prior beliefs an learned\nexperiences. This metaphor has proven to be very useful, leading the\ndevelopment of Bayesian statistics since its XX-th century revival, rooted on\nthe work of de Finetti, Savage and others. The basic metaphor presented in this\ntext, as a foundation for cognitive constructivism, is that of an\neigen-solution, and the verification of its objective epistemic status. The\nFBST - Full Bayesian Significance Test - is the cornerstone of a set of\nstatistical tolls conceived to assess the epistemic value of such\neigen-solutions, according to their four essential attributes, namely,\nsharpness, stability, separability and composability. We believe that this\nalternative perspective, complementary to the one ofered by decision theory,\ncan provide powerful insights and make pertinent contributions in the context\nof scientific research.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.5471v8"
    },
    {
        "title": "Sweave Documentation for \"Implementing Markov chain Monte Carlo:\n  Estimating with confidence\"",
        "authors": [
            "James M. Flegal",
            "Galin L. Jones"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  This file is the Sweave documentation for the examples provided in Flegal, J.\nM. and Jones, G. L. (2010), \"Implementing Markov chain Monte Carlo: Estimating\nwith confidence\", in Handbook of Markov Chain Monte Carlo, edited by Brooks,\nS., Gelman, A., Jones, G., and Meng, X. published by Chapman & Hall/CRC Press.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.5690v1"
    },
    {
        "title": "Hotelling's test for highly correlated data",
        "authors": [
            "Peter Bubeliny"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  This paper is motivated by the analysis of gene expression sets, especially\nby finding differentially expressed gene sets between two phenotypes. Gene\n$\\log_2$ expression levels are highly correlated and, very likely, have\napproximately normal distribution. Therefore, it seems reasonable to use\ntwo-sample Hotelling's test for such data. We discover some unexpected\nproperties of the test making it different from the majority of tests\npreviously used for such data. It appears that the Hotelling's test does not\nalways reach maximal power when all marginal distributions are differentially\nexpressed. For highly correlated data its maximal power is attained when about\na half of marginal distributions are essentially different. For the case when\nthe correlation coefficient is greater than 0.5 this test is more powerful if\nonly one marginal distribution is shifted, omparing to the case when all\nmarginal distributions are equally shifted. Moreover, when the correlation\ncoefficient increases the power of Hotelling's test increases as well.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.1094v1"
    },
    {
        "title": "The two sample problem: Exact distributions, numerical solutions,\n  simulations",
        "authors": [
            "D. E. Chambers"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  The work presented in this article suggests a solution to the two sample\nproblem. Keywords: Two sample problem, Welch-Aspin solution, Fisher-Behrens\nproblem, nuisance parameter, similarity, the Linnik phenomenon.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.1787v1"
    },
    {
        "title": "Development and Initial Validation of a Scale to Measure Instructors'\n  Attitudes toward Concept-Based Teaching of Introductory Statistics in the\n  Health and Behavioral Sciences",
        "authors": [
            "Rossi A. Hassad",
            "Anthony P. M. Coxon"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  Despite more than a decade of reform efforts, students continue to experience\ndifficulty understanding and applying statistical concepts. The predominant\nfocus of reform has been on content, pedagogy, technology and assessment, with\nlittle attention to instructor characteristics. However, there is strong\ntheoretical and empirical evidence that instructors' attitudes impact the\nquality of teaching and learning. The objective of this study was to develop\nand initially validate a scale to measure instructors' attitudes toward\nreform-oriented (or concept-based) teaching of introductory statistics in the\nhealth and behavioral sciences, at the tertiary level. This scale will be\nreferred to as FATS (Faculty Attitudes Toward Statistics). Data were obtained\nfrom 227 instructors (USA and international), and analyzed using factor\nanalysis, multidimensional scaling and hierarchical cluster analysis. The\noverall scale consists of five sub-scales with a total of 25 items, and an\noverall alpha of 0.89. Construct validity was established. Specifically, the\noverall scale, and subscales (except perceived difficulty) plausibly\ndifferentiated between low-reform and high-reform practice instructors.\nStatistically significant differences in attitude were observed with respect to\nage, but not gender, employment status, membership status in professional\norganizations, ethnicity, highest academic qualification, and degree\nconcentration. This scale can be considered a reliable and valid measure of\ninstructors' attitudes toward reform-oriented (concept-based or constructivist)\nteaching of introductory statistics in the health and behavioral sciences at\nthe tertiary level. These five dimensions influence instructors' attitudes.\nAdditional studies are required to confirm these structural and psychometric\nproperties.\n",
        "pdf_link": "http://arxiv.org/pdf/1007.3210v1"
    },
    {
        "title": "$L_p$-nested symmetric distributions",
        "authors": [
            "Fabian Sinz",
            "Matthias Bethge"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  Tractable generalizations of the Gaussian distribution play an important role\nfor the analysis of high-dimensional data. One very general super-class of\nNormal distributions is the class of $\\nu$-spherical distributions whose random\nvariables can be represented as the product $\\x = r\\cdot \\u$ of a uniformly\ndistribution random variable $\\u$ on the $1$-level set of a positively\nhomogeneous function $\\nu$ and arbitrary positive radial random variable $r$.\nProminent subclasses of $\\nu$-spherical distributions are spherically symmetric\ndistributions ($\\nu(\\x)=\\|\\x\\|_2$) which have been further generalized to the\nclass of $L_p$-spherically symmetric distributions ($\\nu(\\x)=\\|\\x\\|_p$). Both\nof these classes contain the Gaussian as a special case. In general, however,\n$\\nu$-spherical distributions are computationally intractable since, for\ninstance, the normalization constant or fast sampling algorithms are unknown\nfor an arbitrary $\\nu$. In this paper we introduce a new subclass of\n$\\nu$-spherical distributions by choosing $\\nu$ to be a nested cascade of\n$L_p$-norms. This class is still computationally tractable, but includes all\nthe aforementioned subclasses as a special case. We derive a general expression\nfor $L_p$-nested symmetric distributions as well as the uniform distribution on\nthe $L_p$-nested unit sphere, including an explicit expression for the\nnormalization constant. We state several general properties of $L_p$-nested\nsymmetric distributions, investigate its marginals, maximum likelihood fitting\nand discuss its tight links to well known machine learning methods such as\nIndependent Component Analysis (ICA), Independent Subspace Analysis (ISA) and\nmixed norm regularizers. Finally, we derive a fast and exact sampling algorithm\nfor arbitrary $L_p$-nested symmetric distributions, and introduce the Nested\nRadial Factorization algorithm (NRF), which is a form of non-linear ICA.\n",
        "pdf_link": "http://arxiv.org/pdf/1008.0740v1"
    },
    {
        "title": "Extreme shock models: an alternative perspective",
        "authors": [
            "Pasquale Cirillo",
            "Jürg Hüsler"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  Extreme shock models have been introduced in Gut and H\\\"usler (1999) to study\nsystems that at random times are subject to shock of random magnitude. These\nsystems break down when some shock overcomes a given resistance level. In this\npaper we propose an alternative approach to extreme shock models using\nreinforced urn processes. As a consequence of this we are able to look at the\nsame problem under a Bayesian nonparametric perspective, providing the\npredictive distribution of systems' defaults.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1732v1"
    },
    {
        "title": "Tests of Non-Equivalence among Absolutely Nonsingular Tensors through\n  Geometric Invariants",
        "authors": [
            "Toshio Sakata",
            "Kazumitsu Maehra",
            "Takeshi Sasaki",
            "Toshio Sumi",
            "Mitsuhiro Miyazaki",
            "Yoshitaka Watanabe"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  4x4x3 absolutely nonsingular tensors are characterized by their determinant\npolynomial. Non-quivalence among absolutely nonsingular tensors with respect to\na class of linear transformations, which do not chage the tensor rank,is\nstudied. It is shown theoretically that affine geometric invariants of the\nconstant surface of a determinant polynomial is useful to discriminate\nnon-equivalence among absolutely nonsingular tensors. Also numerical\ncaluculations are presented and these invariants are shown to be useful indeed.\nFor the caluculation of invarinats by 20-spherical design is also commented. We\nshowed that an algebraic problem in tensor data analysis can be attacked by an\naffine geometric method.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.2723v1"
    },
    {
        "title": "A brief history of the Fail Safe Number in Applied Research",
        "authors": [
            "Moritz Heene"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  Rosenthal's (1979) Fail-Safe-Number (FSN) is probably one of the best known\nstatistics in the context of meta-analysis aimed to estimate the number of\nunpublished studies in meta-analyses required to bring the meta-analytic mean\neffect size down to a statistically insignificant level. Already before\nScargle's (2000) and Schonemann & Scargle's (2008) fundamental critique on the\nclaimed stability of the basic rationale of the FSN approach, objections\nfocusing on the basic assumption of the FSN which treats the number of studies\nas unbiased with averaging null were expressed throughout the history of the\nFSN by different authors (Elashoff, 1978; Iyengar & Greenhouse, 1988a; 1988b;\nsee also Scargle, 2000). In particular, Elashoff's objection appears to be\nimportant because it was the very first critique pointing directly to the\ncentral problem of the FSN: \"R & R claim that the number of studies hidden in\nthe drawers would have to be 65,000 to achieve a mean effect size of zero when\ncombined with the 345 studies reviewed here. But surely, if we allowed the\nhidden studies to be negative, on the average no more than 345 hidden studies\nwould be necessary to obtain a zero mean effect size\" (p. 392). Thus, users of\nmeta-analysis could have been aware right from the beginning that something was\nwrong with the statistical reasoning of the FSN. In particular, from an applied\nresearch perspective, it is therefore of interest whether any of the\nfundamental objections on the FSN are reflected in standard handbooks on\nmeta-analysis as well as -and of course even more importantly- in meta-analytic\nstudies itself.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.2326v1"
    },
    {
        "title": "A Conversation with James Hannan",
        "authors": [
            "Dennis Gilliland",
            "R. V. Ramamoorthi"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  Jim Hannan is a professor who has lived an interesting life and one whose\nfundamental research in repeated games was not fully appreciated until late in\nhis career. During his service as a meteorologist in the Army in World War II,\nJim played poker and made weather forecasts. It is curious that his later\nresearch included strategies for repeated play that apply to selecting the best\nforecaster. James Hannan was born in Holyoke, Massachusetts on September 14,\n1922. He attended St. Jerome's High School and in January 1943 received the\nPh.B. from St. Michael's College in Colchester, Vermont. Jim enlisted in the US\nArmy Air Force to train and serve as a meteorologist. This took him to army\nairbases in China by the close of the war. Following discharge from the army,\nJim studied mathematics at Harvard and graduated with the M.S. in June 1947. To\nprepare for doctoral work in statistics at the University of North Carolina\nthat fall, Jim went to the University of Michigan in the summer of 1947. The\nroutine admissions' physical revealed a spot on the lung and the possibility of\ntuberculosis. This caused Jim to stay at Ann Arbor through the fall of 1947 and\nthen at a Veterans Administration Hospital in Framingham, Massachusetts to have\nhis condition followed more closely. He was discharged from the hospital in the\nspring and started his study at Chapel Hill in the fall of 1948. There he began\nresearch in compound decision theory under Herbert Robbins. Feeling the need\nfor teaching experience, Jim left Chapel Hill after two years and short of\nthesis to take a three year appointment as an instructor at Catholic University\nin Washington, DC. When told that renewal was not coming, Jim felt pressure to\nfinish his degree.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.1160v1"
    },
    {
        "title": "A Conversation with Martin Bradbury Wilk",
        "authors": [
            "Christian Genest",
            "Gordon Brackstone"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  Martin Bradbury Wilk was born on December 18, 1922, in Montr\\'{e}al,\nQu\\'{e}bec, Canada. He completed a B.Eng. degree in Chemical Engineering in\n1945 at McGill University and worked as a Research Engineer on the Atomic\nEnergy Project for the National Research Council of Canada from 1945 to 1950.\nHe then went to Iowa State College, where he completed a M.Sc. and a Ph.D.\ndegree in Statistics in 1953 and 1955, respectively. After a one-year post-doc\nwith John Tukey, he became Assistant Director of the Statistical Techniques\nResearch Group at Princeton University in 1956--1957, and then served as\nProfessor and Director of Research in Statistics at Rutgers University from\n1959 to 1963. In parallel, he also had a 14-year career at Bell Laboratories,\nMurray Hill, New Jersey. From 1956 to 1969, he was in turn Member of Technical\nStaff, Head of the Statistical Models and Methods Research Department, and\nStatistical Director in Management Sciences Research. He wrote a number of\ninfluential papers in statistical methodology during that period, notably\ntesting procedures for normality (the Shapiro--Wilk statistic) and probability\nplotting techniques for multivariate data. In 1970, Martin moved into higher\nmanagement levels of the American Telephone and Telegraph (AT&T) Company. He\noccupied various positions culminating as Assistant Vice-President and Director\nof Corporate Planning. In 1980, he returned to Canada and became the first\nprofessional statistician to serve as Chief Statistician. His accomplishments\nat Statistics Canada were numerous and contributed to a resurgence of the\ninstitution's international standing. He played a crucial role in the\nreinstatement of the Cabinet-cancelled 1986 Census.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.6517v1"
    },
    {
        "title": "Degrees of Equivalence in a Key Comparison",
        "authors": [
            "Thang H. Le",
            "Nguyen D. Do"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  In an interlaboratory key comparison, a data analysis procedure for this\ncomparison was proposed and recommended by CIPM [1, 2, 3], therein the degrees\nof equivalence of measurement standards of the laboratories participated in the\ncomparison and the ones between each two laboratories were introduced but a\ncorresponding clear and plausible measurement model was not given. Authors in\n[4] offered possible measurement models for a given comparison and a suitable\nmodel was selected out after rigorous analyzing steps for expectation values of\nthese degrees of equivalence. The systematic laboratory-effects model was then\nselected as a right one in this report. Those models were all based on the one\ntrue value existence assumption. However in the year 2008, a new version of the\nVocabulary for International Metrology (VIM) [7] was issued where the true\nvalue of a given measurement standard should be now perceived as multi true\nvalues which following a given statistics distribution. Applying this\nperception of true values of a measurement standard with combination of the\nsteps in [4], measurement models have been developed and degrees of equivalence\nhave been analyzed. The results show that although with new definition, the\nsystematic laboratory-effects model is still the reasonable one in a given key\ncomparison.\n",
        "pdf_link": "http://arxiv.org/pdf/1012.1069v2"
    },
    {
        "title": "Squaring the Circle and Cubing the Sphere: Circular and Spherical\n  Copulas",
        "authors": [
            "Michael D. Perlman",
            "Jon A. Wellner"
        ],
        "category": "stat.OT",
        "published_year": "2010",
        "summary": "  Do there exist circular and spherical copulas in $R^d$? That is, do there\nexist circularly symmetric distributions on the unit disk in $R^2$ and\nspherically symmetric distributions on the unit ball in $R^d$, $d\\ge3$, whose\none-dimensional marginal distributions are uniform? The answer is yes for $d=2$\nand 3, where the circular and spherical copulas are unique and can be\ndetermined explicitly, but no for $d\\ge4$. A one-parameter family of elliptical\nbivariate copulas is obtained from the unique circular copula in $R^2$ by\noblique coordinate transformations. Copulas obtained by a non-linear\ntransformation of a uniform distribution on the unit ball in $R^d$ are also\ndescribed, and determined explicitly for $d=2$.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0145v1"
    },
    {
        "title": "A Conversation with George G. Roussas",
        "authors": [
            "Debasis Bhattacharya",
            "Francisco J. Samaniego"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  George G. Roussas was born in the city of Marmara in central Greece, on June\n29, 1933. He received a B.A. with high honors in Mathematics from the\nUniversity of Athens in 1956, and a Ph.D. in Statistics from the University of\nCalifornia, Berkeley, in 1964. In 1964--1966, he served as Assistant Professor\nof Mathematics at the California State University, San Jose, and he was a\nfaculty member of the Department of Statistics at the University of Wisconsin,\nMadison, in 1966--1976, starting as an Assistant Professor in 1966, becoming a\nProfessor in 1972. He was a Professor of Applied Mathematics and Director of\nthe Laboratory of Applied Mathematics at the University of Patras, Greece, in\n1972--1984. He was elected Dean of the School of Physical and Mathematical\nSciences at the University of Patras in 1978, and Chancellor of the university\nin 1981. He served for about three years as Vice President-Academic Affairs of\nthe then new University of Crete, Greece, in 1981--1985. In 1984, he was a\nVisiting Professor in the Intercollege Division of Statistics at the University\nof California, Davis, and he was appointed Professor, Associate Dean and Chair\nof the Graduate Group in Statistics in the same university in 1985; he served\nin the two administrative capacities in 1985--1999. He is an elected member of\nthe International Statistical Institute since 1974, a Fellow of the Royal\nStatistical Society since 1975, a Fellow of the Institute of Mathematical\nStatistics since 1983, and a Fellow of the American Statistical Association\nsince 1986. He served as a member of the Council of the Hellenic Mathematical\nSociety, and as President of the Balkan Union of Mathematicians.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.2434v1"
    },
    {
        "title": "Baby Morse Theory in Data Analysis",
        "authors": [
            "Caren Marzban",
            "Ulvi Yurtsever"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  A methodology is proposed for inferring the topology underlying point cloud\ndata. The approach employs basic elements of Morse Theory, and is capable of\nproducing not only a point estimate of various topological quantities (e.g.,\ngenus), but it can also assess their sampling uncertainty in a probabilistic\nfashion. Several examples of point cloud data in three dimensions are utilized\nto demonstrate how the method yields interval estimates for the topology of the\ndata as a 2-dimensional surface embedded in R^3.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.2582v1"
    },
    {
        "title": "A flexible observed factor model with separate dynamics for the factor\n  volatilities and their correlation matrix",
        "authors": [
            "Yu-Cheng Ku",
            "Peter Bloomfield",
            "Robert Kohn"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  Our article considers a regression model with observed factors. The observed\nfactors have a flexible stochastic volatility structure that has separate\ndynamics for the volatilities and the correlation matrix. The correlation\nmatrix of the factors is time-varying and its evolution is described by an\ninverse Wishart process. The model specifies the evolution of the observed\nvolatilities flexibly and is particularly attractive when the dimension of the\nobservations is high. A Markov chain Monte Carlo algorithm is developed to\nestimate the model. It is straightforward to use this algorithm to obtain the\npredictive distributions of future observations and to carry out model\nselection. The model is illustrated and compared to other Wishart-type factor\nmultivariate stochastic volatility models using various empirical data\nincluding monthly stock returns and portfolio weighted returns. The evidence\nsuggests that our model has better predictive performance. The paper also\nallows the idiosyncratic errors to follow individual stochastic volatility\nprocesses in order to deal with more volatile data such as daily or weekly\nstock returns.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.3259v2"
    },
    {
        "title": "Some notes on biasedness and unbiasedness of two-sample\n  Kolmogorov-Smirnov test",
        "authors": [
            "Peter Bubelíny"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  This paper deals with two-sample Kolmogorov-Smirnov test and its biasedness.\nThis test is not unbiased in general in case of different sample sizes. We\nfound out most biased distribution for some values of significance level\n$\\alpha$. Moreover we discovered that there exists number of observation and\nsignificance level $\\alpha$ such that this test is unbiased at level $\\alpha$.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.5598v1"
    },
    {
        "title": "Revealing Sub-Optimality Conditions of Strategic Decisions",
        "authors": [
            "H. Kemal Ilter"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  Conceptual view of fitness and fitness measurement of strategic decisions on\ninformation systems, technological systems and innovation are becoming more\nimportant in recent years. This paper determines some dynamics of fitness\nlandscape which are lead to termination of decision makers' research before\nreaching the global maximum in strategic decisions. These dynamics are\nspecified according to management decision making models and supported with\nsimulation results. This article determines simulation results by means of\n\"Fitness Value\" and \"Probability of Optimality\". Correlation between these two\nconcepts may be remarkable according to revealing optimal values in innovative\nand research-based decision making approaches beside sub-optimal results of\ntraditional decision making approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.0202v1"
    },
    {
        "title": "Which are the best cities for psychology research worldwide? A map\n  visualizing city ratios of observed and expected numbers of highly-cited\n  papers",
        "authors": [
            "Lutz Bornmann",
            "Loet Leydesdorff",
            "Günter Krampen"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  We present scientometric results about world-wide centers of excellence in\npsychology. Based on Web of Science data, domain-specific excellence can be\nidentified for cities where highly cited papers are published. Data refer to\nall psychology articles published in 2007 which are documented in the Social\nScience Citation Index and to their citation frequencies from 2007 to May 2011.\nVisualized are 214 cities with an article output of at least 50 in 2007.\nStatistical z tests are used for the evaluation of the degree to which an\nobserved number of top-cited papers (top-10%) for a city differs from the\nnumber expected on the basis of randomness in the selection of papers. Map\nvisualizing city ratios on significant differences between observed and\nexpected numbers of highly-cited papers point at excellence centers in cities\nat the East and West Coast of the United States as well as in Great Britain,\nGermany, the Netherlands, Ireland, Belgium, Sweden, Finland, Australia, and\nTaiwan. Furthermore, positive but non-significant differences in favor of high\ncitation rates are documented for some cities in the United States, Great\nBritain, the Netherlands, the Scandinavian and the German-speaking countries,\nBelgium, France, Spain, Israel, South Korea, and China. Scientometric results\nshow convincingly that highly-cited psychological research articles come from\nthe Anglo-American countries and some of the non-English European countries in\nwhich the number of English-language publications has increased during the last\ndecades.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.5141v1"
    },
    {
        "title": "Modern Portfolio Theory using SAS\\textregistered OR",
        "authors": [
            "Murphy Choy"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  Investment approaches in financial instruments have been varied and often\nproduce unpredictable results. Many investors in the earlier days of investment\nbanking suffered catastrophical losses due to poor strategy and lack of\nunderstanding of the financial market. With the development of investment\nbanking, many innovative investment strategies have been proposed to make\nportfolio returns higher than the overall market. One of the most famous\ntheories of portfolio creation and management is the modern portfolio theory\nproposed by Harry Markowitz. In this paper, we shall apply the theory in\ncreating a portfolio of stocks as well as managing it.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.0349v1"
    },
    {
        "title": "A Concise Resolution to the Two Envelope Paradox",
        "authors": [
            "Eric Bliss"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  In this paper, I will demonstrate a new perspective on the Two Envelope\nProblem. I hope to show with convincing clarity how the paradox results from an\ninherent problem pertaining to the interpretation of Bayesian probability.\nSpecifically, a subjective probability that is inconsistent with reality can\nmislead reasoning based on Bayesian decision theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4669v3"
    },
    {
        "title": "A Proof on Asymptotics of Wavelet Variance of a Long Memory Process by\n  Using Taylor Expansion",
        "authors": [
            "Wonsang You",
            "Wojciech Kordecki"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  A long memory process has self-similarity or scale-invariant properties in\nlow frequencies. We prove that the log of the scale-dependent wavelet variance\nfor a long memory process is asymptotically proportional to scales by using the\nTaylor expansion of wavelet variances.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4746v1"
    },
    {
        "title": "A divergence formula for regularization methods with an L2 constraint",
        "authors": [
            "Yixin Fang",
            "Yuanjia Wang",
            "Xin Huang"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  We derive a divergence formula for a group of regularization methods with an\nL2 constraint. The formula is useful for regularization parameter selection,\nbecause it provides an unbiased estimate for the number of degrees of freedom.\nWe begin with deriving the formula for smoothing splines and then extend it to\nother settings such as penalized splines, ridge regression, and functional\nlinear regression.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.3559v1"
    },
    {
        "title": "Sparse solution of overdetermined linear systems when the columns of $A$\n  are orthogonal",
        "authors": [
            "Phanindra V. Jampana",
            "Sastry S. Challa"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  In this paper, we consider the problem of obtaining the best $k$-sparse\nsolution of $Ax=y$ subject to the constraint that the columns of $A$ are\northogonal. The naive approach for obtaining a solution to this problem has\nexponential complexity and there exist $l_1$ regularization methods such as\nLasso to obtain approximate solutions. In this paper, we show that we can\nobtain an exact solution to the problem, with much less computational effort\ncompared to the brute force search when the columns of $A$ are orthogonal.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.5402v2"
    },
    {
        "title": "On individual neutrality and collective decision making",
        "authors": [
            "Mu Zhu",
            "Shangsi Wang",
            "Lu Xin"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  We derive a simple mathematical \"theory\" to show that two decision-making\nentities can work better together only if at least one of them is occasionally\nwilling to stay neutral. This provides a mathematical \"justification\" for an\nage-old cliche among marriage counselors.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.5334v1"
    },
    {
        "title": "A Fast Non-Gaussian Bayesian Matching Pursuit Method for Sparse\n  Reconstruction",
        "authors": [
            "Mudassir Masood",
            "Tareq Al-Naffouri"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  A fast matching pursuit method using a Bayesian approach is introduced for\nsparse signal recovery. This method, referred to as nGpFBMP, performs Bayesian\nestimates of sparse signals even when the signal prior is non-Gaussian or\nunknown. It is agnostic on signal statistics and utilizes a priori statistics\nof additive noise and the sparsity rate of the signal, which are shown to be\neasily estimated from data if not available. nGpFBMP utilizes a greedy approach\nand order-recursive updates of its metrics to find the most dominant sparse\nsupports to determine the approximate minimum mean square error (MMSE) estimate\nof the sparse signal. Simulation results demonstrate the power and robustness\nof our proposed estimator.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.4208v1"
    },
    {
        "title": "Demmartingales and the functionnal Hill process for small parameters",
        "authors": [
            "Adja Mbarka Fall",
            "Gane Samb Lo",
            "Cheikhna Hamallah Ndiaye"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  Association of random variables and Demimartingales are recent fields for\nhandling asymptotic behaviors of sums of dependent random variables. We apply\ntheir techniques to establish the asymptotic law of a demimartingale\n  We next apply the results to find the asymptotic behavior the functional Hill\nprocess for small parameters within the Extreme Value Theory (EVT) field. Such\na result would have been very hard to find whithout demimartingales techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.1487v3"
    },
    {
        "title": "Benford's law: A theoretical explanation for base 2",
        "authors": [
            "H. M. Bharath"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  In this paper, we present a possible theoretical explanation for benford's\nlaw. We develop a recursive relation between the probabilities, using simple\nintuitive ideas. We first use numerical solutions of this recursion and verify\nthat the solutions converge to the benford's law. Finally we solve the\nrecursion analytically to yeild the benford's law for base 2.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.7008v1"
    },
    {
        "title": "A paradox on the spectral representation of stationary random processes",
        "authors": [
            "Mohammad Mohammadi",
            "Adel Mohammadpour",
            "Afshin Parvardeh"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  In this note our aim is to show a paradox in the spectral representation of\nstationary random processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.6339v2"
    },
    {
        "title": "Perceptive Statistical Variability Indicators",
        "authors": [
            "Kalman Ziha"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  The concepts of variability and uncertainty, both epistemic and alleatory,\ncame from experience and coexist with different connotations. Therefore this\narticle attempts to express their relation by analytic means firstly setting\nsights on their differences and then on their common characteristics. Inspired\nwith the definition of average number of equally probable events based on\nentropy concept in probability theory, the article introduced two related\nperceptive statistical measures which indicate the same variability as the\nbasic probability distribution. First is the equivalent number of a\nhypothetical distribution with one sure and all the other impossible outcomes\nwhich indicates variability. Second is the appropriate equivalent number of a\nhypothetical distribution with all equal probabilities which indicates\ninvariability. The article interprets the common properties of variability and\nuncertainty on theoretical distributions and on ocean-wide wind wave\ndirectional properties by using the long term observations compiled in the\nGlobal Wave Statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1301.0765v2"
    },
    {
        "title": "On the asymptotics of Ajtai-Komlós-Tusnády statistics",
        "authors": [
            "L. Rejtő",
            "G. Tusnády"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  In our days there is a widespread analysis of Wasserstein distances between\ntheoretical and empirical measures. One of the first investigation of the topic\nis given in the paper written by Ajtai, Koml\\'os and Tusn\\'ady in $1984.$\n  Interestingly, all the neighboring questions posed by that paper were settled\nalready without the original one. In this paper we are going to delineate the\nlimit behavior of the original statistics with the help of computer\nsimulations. At the same time we kept an eye on theoretical grasping of the\nproblem. Based on our computer simulations our opinion is that the limit\ndistribution is Gaussian.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.1023v3"
    },
    {
        "title": "I hear, I forget. I do, I understand: a modified Moore-method\n  mathematical statistics course",
        "authors": [
            "Nicholas Jon Horton"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  Moore introduced a method for graduate mathematics instruction that consisted\nprimarily of individual student work on challenging proofs (Jones, 1977). Cohen\n(1982) described an adaptation with less explicit competition suitable for\nundergraduate students at a liberal arts college. This paper details an\nadaptation of this modified Moore-method to teach mathematical statistics, and\ndescribes ways that such an approach helps engage students and foster the\nteaching of statistics.\n  Groups of students worked a set of 3 difficult problems (some theoretical,\nsome applied) every two weeks. Class time was devoted to coaching sessions with\nthe instructor, group meeting time, and class presentations. R was used to\nestimate solutions empirically where analytic results were intractable, as well\nas to provide an environment to undertake simulation studies with the aim of\ndeepening understanding and complementing analytic solutions. Each group\npresented comprehensive solutions to complement oral presentations. Development\nof parallel techniques for empirical and analytic problem solving was an\nexplicit goal of the course, which also attempted to communicate ways that\nstatistics can be used to tackle interesting problems. The group problem\nsolving component and use of technology allowed students to attempt much more\nchallenging questions than they could otherwise solve.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.7445v1"
    },
    {
        "title": "A Conversation with Stephen E. Fienberg",
        "authors": [
            "Miron L. Straf",
            "Judith M. Tanur"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  The following conversation is based in part on a transcript of a 2009\ninterview funded by Pfizer Global Research-Connecticut, the American\nStatistical Association and the Department of Statistics at the University of\nConnecticut-Storrs as part of the \"Conversations with Distinguished\nStatisticians in Memory of Professor Harry O. Posten\".\n",
        "pdf_link": "http://arxiv.org/pdf/1310.2442v1"
    },
    {
        "title": "On Renyi entropy convergence of the max domain of attraction",
        "authors": [
            "Ali Saeb"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  In this paper, we prove that the Renyi entropy of linearly normalized partial\nmaxima of independent and identically distributed random variables is\nconvergent to the corresponding limit Renyi entropy when the linearly\nnormalized partial maxima converges to some nondegenerate random variable.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.0947v4"
    },
    {
        "title": "Null hypothesis significance tests: A mix-up of two different theories,\n  the basis for widespread confusion and numerous misinterpretations",
        "authors": [
            "Jesper W. Schneider"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Null hypothesis statistical significance tests (NHST) are widely used in\nquantitative research in the empirical sciences including scientometrics.\nNevertheless, since their introduction nearly a century ago significance tests\nhave been controversial. Many researchers are not aware of the numerous\ncriticisms raised against NHST. As practiced, NHST has been characterized as a\nnull ritual that is overused and too often misapplied and misinterpreted. NHST\nis in fact a patchwork of two fundamentally different classical statistical\ntesting models, often blended with some wishful quasi-Bayesian interpretations.\nThis is undoubtedly a major reason why NHST is very often misunderstood. But\nNHST also has intrinsic logical problems and the epistemic range of the\ninformation provided by such tests is much more limited than most researchers\nrecognize. In this article we introduce to the scientometric community the\ntheoretical origins of NHST, which is mostly absent from standard statistical\ntextbooks, and we discuss some of the most prevalent problems relating to the\npractice of NHST and trace these problems back to the mixup of the two\ndifferent theoretical origins. Finally, we illustrate some of the\nmisunderstandings with examples from the scientometric literature and bring\nforward some modest recommendations for a more sound practice in quantitative\ndata analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1089v1"
    },
    {
        "title": "R Markdown: Integrating A Reproducible Analysis Tool into Introductory\n  Statistics",
        "authors": [
            "Ben Baumer",
            "Mine Cetinkaya-Rundel",
            "Andrew Bray",
            "Linda Loi",
            "Nicholas J. Horton"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Nolan and Temple Lang argue that \"the ability to express statistical\ncomputations is an essential skill.\" A key related capacity is the ability to\nconduct and present data analysis in a way that another person can understand\nand replicate. The copy-and-paste workflow that is an artifact of antiquated\nuser-interface design makes reproducibility of statistical analysis more\ndifficult, especially as data become increasingly complex and statistical\nmethods become increasingly sophisticated. R Markdown is a new technology that\nmakes creating fully-reproducible statistical analysis simple and painless. It\nprovides a solution suitable not only for cutting edge research, but also for\nuse in an introductory statistics course. We present evidence that R Markdown\ncan be used effectively in introductory statistics courses, and discuss its\nrole in the rapidly-changing world of statistical computation.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.1894v1"
    },
    {
        "title": "A method for comparing chess openings",
        "authors": [
            "Jamal Munshi"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  A quantitative method is described for comparing chess openings. Test\nopenings and baseline openings are run through chess engines under controlled\nconditions and compared to evaluate the effectiveness of the test openings. The\nresults are intuitively appealing and in some cases they agree with expert\nopinion. The specific contribution of this work is the development of an\nobjective measure that may be used for the evaluation and refutation of chess\nopenings, a process that had been left to thought experiments and subjective\nconjectures and thereby to a large variety of opinion and a great deal of\ndebate.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.6791v3"
    },
    {
        "title": "Claude Bouchu, intendant de Bourgogne au 17ème siècle, a-t-il\n  inventé le mot \"statistique\"",
        "authors": [
            "Dominique Pepin"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  The objective of this paper is to examine the assertion that the word\n\"statistics\" would have been used for the first time in the 17th century, in a\nreport written by Claude Bouchu, administrator of Bourgogne. A historical and\nbibliographical analysis is carried out to judge the credibility of this\nthesis. The physical inspection of the report then makes it possible to bring a\nfinal answer.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.5557v1"
    },
    {
        "title": "Generalized probabilities in statistical theories",
        "authors": [
            "F. Holik",
            "C. Massri",
            "A. Plastino",
            "M. Sáenz"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  In this review article we present different formal frameworks for the\ndescription of generalized probabilities in statistical theories. We discuss\nthe particular cases of probabilities appearing in classical and quantum\nmechanics, possible generalizations of the approaches of A. N. Kolmogorov and\nR. T. Cox to non-commutative models, and the approach to generalized\nprobabilities based on convex sets.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0913v3"
    },
    {
        "title": "Using Google Scholar to predict self citation: A case study in Health\n  Economics",
        "authors": [
            "Richard Norman",
            "Francisco M Couto"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Metrics designed to quantify the influence of academics are increasingly used\nand easily estimable, and perhaps the most popular is the h index. Metrics such\nas this are however potentially impacted through excessive self citation. This\nwork explores the issue using a group of researchers working in a well defined\nsub field of economics, namely Health Economics. It then employs self citation\nidentification software, and identifies the characteristics that best predict\nself citation. This provides evidence regarding the scale of self citation in\nthe field, and the degree to which self citation impacts on inferences about\nthe relative influence of individual Health Economists. Using data from 545\nHealth Economists, it suggests self citation to be associated with the\ngeographical region and longevity of the Health Economist, with early career\nresearchers and researchers from mainland Europe and Australasia self citing\nmost frequently.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5241v1"
    },
    {
        "title": "A brief history of long memory: Hurst, Mandelbrot and the road to ARFIMA",
        "authors": [
            "Timothy Graves",
            "Robert B. Gramacy",
            "Nicholas Watkins",
            "Christian Franzke"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Long memory plays an important role in many fields by determining the\nbehaviour and predictability of systems; for instance, climate, hydrology,\nfinance, networks and DNA sequencing. In particular, it is important to test if\na process is exhibiting long memory since that impacts the accuracy and\nconfidence with which one may predict future events on the basis of a small\namount of historical data. A major force in the development and study of long\nmemory was the late Benoit B. Mandelbrot. Here we discuss the original\nmotivation of the development of long memory and Mandelbrot's influence on this\nfascinating field. We will also elucidate the sometimes contrasting approaches\nto long memory in different scientific communities\n",
        "pdf_link": "http://arxiv.org/pdf/1406.6018v3"
    },
    {
        "title": "The Final Solutions of Monty Hall Problem and Three Prisoners Problem",
        "authors": [
            "Shiro Ishikawa"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Recently we proposed the linguistic interpretation of quantum mechanics\n(called quantum and classical measurement theory, or quantum language), which\nwas characterized as a kind of metaphysical and linguistic turn of the\nCopenhagen interpretation. This turn from physics to language does not only\nextend quantum theory to classical systems but also yield the quantum\nmechanical world view (i.e., the philosophy of quantum mechanics, in other\nwords, quantum philosophy).And we believe that this quantum language is the\nmost powerful language to describe science. The purpose of this paper is to\ndescribe the Monty-Hall problem and the three prisoners problem in quantum\nlanguage. We of course believe that our proposal is the final solutions of the\ntwo problems. Thus in this paper, we can answer the question: \"Why have\nphilosophers continued to stick to these problems?\" And the readers will find\nthat these problems are never elementary, and they can not be solved without\nthe deep understanding of \"probability\" and \"dualism\".\n  KEY WORDS: Philosophy of probability, Fisher Maximum Likelihood Method,\nBayes' Method,The Principle of Equal (a priori) Probabilities\n",
        "pdf_link": "http://arxiv.org/pdf/1408.0963v1"
    },
    {
        "title": "Quantile of a Mixture",
        "authors": [
            "Carole Bernard",
            "Steven Vanduffel"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  In this note, we give an explicit expression for the quantile of a mixture of\ntwo random variables. We carefully examine all possible cases of discrete and\ncontinuous variables with possibly unbounded support. The result is useful for\nfinding bounds on the Value-at-Risk of risky portfolios when only partial\ninformation is available (Bernard and Vanduffel (2014)).\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4824v1"
    },
    {
        "title": "Generalized Labeled Multi-Bernoulli Approximation of Multi-Object\n  Densities",
        "authors": [
            "Francesco Papi",
            "Ba-Ngu Vo",
            "Ba-Tuong Vo",
            "Claudio Fantacci",
            "Michael Beard"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  In multi-object inference, the multi-object probability density captures the\nuncertainty in the number and the states of the objects as well as the\nstatistical dependence between the objects. Exact computation of the\nmulti-object density is generally intractable and tractable implementations\nusually require statistical independence assumptions between objects. In this\npaper we propose a tractable multi-object density approximation that can\ncapture statistical dependence between objects. In particular, we derive a\ntractable Generalized Labeled Multi-Bernoulli (GLMB) density that matches the\ncardinality distribution and the first moment of the labeled multi-object\ndistribution of interest. It is also shown that the proposed approximation\nminimizes the Kullback-Leibler divergence over a special tractable class of\nGLMB densities. Based on the proposed GLMB approximation we further demonstrate\na tractable multi-object tracking algorithm for generic measurement models.\nSimulation results for a multi-object Track-Before-Detect example using radar\nmeasurements in low signal-to-noise ratio (SNR) scenarios verify the\napplicability of the proposed approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.5294v3"
    },
    {
        "title": "From Curriculum Guidelines to Learning Objectives: A Survey of Five\n  Statistics Programs",
        "authors": [
            "Beth Chance",
            "Roxy Peck"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  The 2000 ASA Guidelines for Undergraduate Statistics majors aimed to provide\nguidance to programs with undergraduate degrees in statistics as to the content\nand skills that statistics majors should be learning. With new guidelines\nforthcoming, it is important to help programs develop an assessment cycle of\nevaluation. How do we know the students are learning what we want them to\nlearn? How do we improve the program over time? The first step in this process\nis to translate the broader Guidelines into institution-specific measurable\nlearning outcomes. This paper provides examples of how five programs did so for\nthe 2000 Guidelines. We hope they serve as illustrative examples for programs\nmoving forward with the new guidelines.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7261v1"
    },
    {
        "title": "What are the true clusters?",
        "authors": [
            "Christian Hennig"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Constructivist philosophy and Hasok Chang's active scientific realism are\nused to argue that the idea of \"truth\" in cluster analysis depends on the\ncontext and the clustering aims. Different characteristics of clusterings are\nrequired in different situations. Researchers should be explicit about on what\nrequirements and what idea of \"true clusters\" their research is based, because\nclustering becomes scientific not through uniqueness but through transparent\nand open communication. The idea of \"natural kinds\" is a human construct, but\nit highlights the human experience that the reality outside the observer's\ncontrol seems to make certain distinctions between categories inevitable.\nVarious desirable characteristics of clusterings and various approaches to\ndefine a context-dependent truth are listed, and I discuss what impact these\nideas can have on the comparison of clustering methods, and the choice of a\nclustering methods and related decisions in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/1502.02555v1"
    },
    {
        "title": "Teaching and Learning Data Visualization: Ideas and Assignments",
        "authors": [
            "Deborah Nolan",
            "Jamis Perrett"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  This article discusses how to make statistical graphics a more prominent\nelement of the undergraduate statistics curricula. The focus is on several\ndifferent types of assignments that exemplify how to incorporate graphics into\na course in a pedagogically meaningful way. These assignments include having\nstudents deconstruct and reconstruct plots, copy masterful graphs, create\none-minute visual revelations, convert tables into `pictures', and develop\ninteractive visualizations with, e.g., the virtual earth as a plotting canvas.\nIn addition to describing the goals and details of each assignment, we also\ndiscuss the broader topic of graphics and key concepts that we think warrant\ninclusion in the statistics curricula. We advocate that more attention needs to\nbe paid to this fundamental field of statistics at all levels, from\nintroductory undergraduate through graduate level courses. With the rapid rise\nof tools to visualize data, e.g., Google trends, GapMinder, ManyEyes, and\nTableau, and the increased use of graphics in the media, understanding the\nprinciples of good statistical graphics, and having the ability to create\ninformative visualizations is an ever more important aspect of statistics\neducation.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.00781v1"
    },
    {
        "title": "Replication, Communication, and the Population Dynamics of Scientific\n  Discovery",
        "authors": [
            "Richard McElreath",
            "Paul E. Smaldino"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Many published research results are false, and controversy continues over the\nroles of replication and publication policy in improving the reliability of\nresearch. Addressing these problems is frustrated by the lack of a formal\nframework that jointly represents hypothesis formation, replication,\npublication bias, and variation in research quality. We develop a mathematical\nmodel of scientific discovery that combines all of these elements. This model\nprovides both a dynamic model of research as well as a formal framework for\nreasoning about the normative structure of science. We show that replication\nmay serve as a ratchet that gradually separates true hypotheses from false, but\nthe same factors that make initial findings unreliable also make replications\nunreliable. The most important factors in improving the reliability of research\nare the rate of false positives and the base rate of true hypotheses, and we\noffer suggestions for addressing each. Our results also bring clarity to verbal\ndebates about the communication of research. Surprisingly, publication bias is\nnot always an obstacle, but instead may have positive impacts---suppression of\nnegative novel findings is often beneficial. We also find that communication of\nnegative replications may aid true discovery even when attempts to replicate\nhave diminished power. The model speaks constructively to ongoing debates about\nthe design and conduct of science, focusing analysis and discussion on precise,\ninternally consistent models, as well as highlighting the importance of\npopulation dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.02780v3"
    },
    {
        "title": "Exponentiated Extended Weibull-Power Series Class of Distributions",
        "authors": [
            "Saeid Tahmasebi",
            "Ali Akbar Jafari"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  In this paper, we introduce a new class of distributions by compounding the\nexponentiated extended Weibull family and power series family. This\ndistribution contains several lifetime models such as the complementary\nextended Weibull-power series, generalized exponential-power series,\ngeneralized linear failure rate-power series, exponentiated Weibull-power\nseries, generalized modified Weibull-power series, generalized Gompertz-power\nseries and exponentiated extended Weibull distributions as special cases. We\nobtain several properties of this new class of distributions such as Shannon\nentropy, mean residual life, hazard rate function, quantiles and moments. The\nmaximum likelihood estimation procedure via a EM-algorithm is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.08653v1"
    },
    {
        "title": "Failure and Uses of Jaynes' Principle of Transformation Groups",
        "authors": [
            "Alon Drory"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Bertand's paradox is a fundamental problem in probability that casts doubt on\nthe applicability of the indifference principle by showing that it may yield\ncontradictory results, depending on the meaning assigned to \"randomness\".\nJaynes claimed that symmetry requirements (the principle of transformation\ngroups) solve the paradox by selecting a unique solution to the problem. I show\nthat this is not the case and that every variant obtained from the principle of\nindifference can also be obtained from Jaynes' principle of transformation\ngroups. This is because the same symmetries can be mathematically implemented\nin different ways, depending on the procedure of random selection that one\nuses. I describe a simple experiment that supports a result from symmetry\narguments, but the solution is different from Jaynes'. Jaynes' method is thus\nbest seen as a tool to obtain probability distributions when the principle of\nindifference is inconvenient, but it cannot resolve ambiguities inherent in the\nuse of that principle and still depends on explicitly defining the selection\nprocedure.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.09072v1"
    },
    {
        "title": "Le Her and Other Problems in Probability Discussed by Bernoulli,\n  Montmort and Waldegrave",
        "authors": [
            "David R. Bellhouse",
            "Nicolas Fillion"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Part V of the second edition of Pierre R\\'{e}mond de Montmort's Essay\nd'analyse sur les jeux de hazard published in 1713 contains correspondence on\nprobability problems between Montmort and Nicolaus Bernoulli. This\ncorrespondence begins in 1710. The last published letter, dated November 15,\n1713, is from Montmort to Nicolaus Bernoulli. There is some discussion of the\nstrategy of play in the card game Le Her and a bit of news that Montmort's\nfriend Waldegrave in Paris was going to take care of the printing of the book.\nFrom earlier correspondence between Bernoulli and Montmort, it is apparent that\nWaldegrave had also analyzed Le Her and had come up with a mixed strategy as a\nsolution. He had also suggested working on the \"problem of the pool,\" or what\nis often called Waldegrave's problem. The Universit\\\"{a}tsbibliothek Basel\ncontains an additional forty-two letters between Bernoulli and Montmort written\nafter 1713, as well as two letters between Bernoulli and Waldegrave. The\nletters are all in French, and here we provide translations of key passages.\nThe trio continued to discuss probability problems, particularly Le Her which\nwas still under discussion when the Essay d'analyse went to print. We describe\nthe probability content of this body of correspondence and put it in its\nhistorical context. We also provide a proper identification of Waldegrave based\non manuscripts in the Archives nationales de France in Paris.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.01950v1"
    },
    {
        "title": "A Conversation with Nancy Flournoy",
        "authors": [
            "William F. Rosenberger"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Nancy Flournoy was born in Long Beach, California, on May 4, 1947. After\ngraduating from Polytechnic School in Pasadena in 1965, she earned a B.S.\n(1969) and M.S. (1971) in biostatistics from UCLA. Between her bachelors and\nmasters degrees, she worked as a Statistician I for Regional Medical Programs\nat UCLA. After receiving her master's degree, she spend three years at the\nSouthwest Laboratory for Education Research and Development in Seal Beach,\nCalifornia. Flournoy joined the Seattle team pioneering bone marrow\ntransplantation in 1973. She moved with the transplant team into the newly\nformed Fred Hutchinson Cancer Research Center in 1975 as Director of Clinical\nStatistics, where she supervised a group responsible for the design and\nanalysis of about 80 simultaneous clinical trials. To support the Clinical\nDivision, she supervised the development of an interdisciplinary shared data\nsoftware system. She recruited Leonard B. Hearne to create this database\nmanagement system in 1975 (and married him in 1978). While at the Cancer\nCenter, she was also at the University of Washington, where she received her\ndoctorate in biomathematics in 1982. She became the first female director of\nthe program in statistics at the National Science Foundation (NSF) in 1986. She\nreceived service awards from the NSF in 1988 and the National Institute of\nStatistical Science in 2006 for facilitating interdisciplinary research.\nFlournoy joined the Department of Mathematics and Statistics at American\nUniversity in 1988. She moved as department chair to the University of Missouri\nin 2002, where she became Curators' Distinguished Professor in 2012.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.03089v1"
    },
    {
        "title": "A Conversation with Richard A. Olshen",
        "authors": [
            "John A. Rice"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent\nhis early years in Chevy Chase, Maryland, but has lived most of his life in\nCalifornia. He received an A.B. in Statistics at the University of California,\nBerkeley, in 1963, and a Ph.D. in Statistics from Yale University in 1966,\nwriting his dissertation under the direction of Jimmie Savage and Frank\nAnscombe. He served as Research Staff Statistician and Lecturer at Yale in\n1966-1967. Richard accepted a faculty appointment at Stanford University in\n1967, and has held tenured faculty positions at the University of Michigan\n(1972-1975), the University of California, San Diego (1975-1989), and Stanford\nUniversity (since 1989). At Stanford, he is Professor of Health Research and\nPolicy (Biostatistics), Chief of the Division of Biostatistics (since 1998) and\nProfessor (by courtesy) of Electrical Engineering and of Statistics. At various\ntimes, he has had visiting faculty positions at Columbia, Harvard, MIT,\nStanford and the Hebrew University. Richard's research interests are in\nstatistics and mathematics and their applications to medicine and biology. Much\nof his work has concerned binary tree-structured algorithms for classification,\nregression, survival analysis and clustering. Those for classification and\nsurvival analysis have been used with success in computer-aided diagnosis and\nprognosis, especially in cardiology, oncology and toxicology. He coauthored the\n1984 book Classification and Regression Trees (with Leo Brieman, Jerome\nFriedman and Charles Stone) which gives motivation, algorithms, various\nexamples and mathematical theory for what have come to be known as CART\nalgorithms. The approaches to tree-structured clustering have been applied to\nproblems in digital radiography (with Stanford EE Professor Robert Gray) and to\nHIV genetics, the latter work including studies on single nucleotide\npolymorphisms, which has helped to shed light on the presence of hypertension\nin certain subpopulations of women.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.03131v1"
    },
    {
        "title": "Propagation of Uncertainty in Risk Analysis and Safety Integrity Level\n  Composition",
        "authors": [
            "Jens Braband",
            "Hendrik Schäbe"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  In many risk analyses the results are only given as mean values and often the\ninput data are also mean values. However the required accuracy of the result is\noften an interval of values e. g. for the derivation of a Safety Integrity\nLevel (SIL). In this paper we reason what should be the accuracy of the input\ndata of risk analyses if a particular certainty of the result is demanded. Also\nthe backside of the coin, the SIL composition is discussed. The results show\nthat common methods for risk analysis are faulty and that SIL allocation by a\nkind of SIL calculus seems infeasible without additional requirements on the\ncomposed components. A justification of a common practice for parameter scaling\nin well-constructed semi-quantitative risk analysis is also provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.07087v1"
    },
    {
        "title": "Visualizing Probabilistic Proof",
        "authors": [
            "Enrique Guerra-Pujol"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The author revisits the Blue Bus Problem, a famous thought-experiment in law\ninvolving probabilistic proof, and presents simple Bayesian solutions to\ndifferent versions of the blue bus hypothetical. In addition, the author\nexpresses his solutions in standard and visual formats, i.e. in terms of\nprobabilities and natural frequencies.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05057v1"
    },
    {
        "title": "Mere Renovation is Too Little Too Late: We Need to Rethink Our\n  Undergraduate Curriculum from the Ground Up",
        "authors": [
            "George W. Cobb"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The last half-dozen years have seen The American Statistician publish\nwell-argued and provocative calls to change our thinking about statistics and\nhow we teach it, among them Brown and Kass (2009), Nolan and Temple-Lang\n(2010), and Legler et al. (2010). Within this past year, the ASA has issued a\nnew and comprehensive set of guidelines for undergraduate programs (ASA 2014).\nAccepting (and applauding) all this as background, the current article argues\nthe need to rethink our curriculum from the ground up, and offers five\nprinciples and two caveats intended to help us along the path toward a new\nsynthesis. These principles and caveats rest on my sense of three parallel\nevolutions: the convergence of trends in the roles of mathematics, computation,\nand context within statistics education. These ongoing changes, together with\nthe articles cited above and the seminal provocation by Leo Breiman (2001) call\nfor a deep rethinking of what we teach to undergraduates. In particular,\nfollowing Brown and Kass, we should put priority on two goals, to make\nfundamental concepts accessible and to minimize prerequisites to research.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05346v1"
    },
    {
        "title": "On locating statistics in the world of finding out",
        "authors": [
            "Chris J. Wild"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  This paper attempts to situate statistics in relation to qualitative research\nmethods and other means of \"finding out\". It compares and contrasts aspects of\nqualitative research methods and statistical inquiry and attempts to answer the\nquestion of whether and how elements of qualitative research methods should be\nincluded in statistics teaching.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.05982v1"
    },
    {
        "title": "The Crisis Of Evidence: Why Probability And Statistics Cannot Discover\n  Cause",
        "authors": [
            "William M. Briggs"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Probability models are only useful at explaining the uncertainty of what we\ndo not know, and should never be used to say what we already know. Probability\nand statistical models are useless at discerning cause. Classical statistical\nprocedures, in both their frequentist and Bayesian implementations are, falsely\nimply they can speak about cause. No hypothesis test, or Bayes factor, should\never be used again. Even assuming we know the cause or partial cause for some\nset of observations, reporting via relative risk exagerates the certainty we\nhave in the future, often by a lot. This over-certainty is made much worse when\nparametetric and not predictive methods are used. Unfortunately, predictive\nmethods are rarely used; and even when they are, cause must still be an\nassumption, meaning (again) certainty in our scientific pronouncements is too\nhigh.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.07244v2"
    },
    {
        "title": "The XL-mHG Test For Enrichment: A Technical Report",
        "authors": [
            "Florian Wagner"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The minimum hypergeometric test (mHG) is a powerful nonparametric hypothesis\ntest to detect enrichment in ranked binary lists. Here, I provide a detailed\nreview of its definition, as well as the algorithms used in its implementation,\nwhich enable the efficient computation of an exact p-value. I then introduce a\ngeneralization of the mHG, termed XL-mHG, which provides additional control\nover the type of enrichment tested, and describe the precise algorithmic\nmodifications necessary to compute its test statistic and p-value. The XL-mHG\nalgorithm is a building block of GO-PCA, a recently proposed method for the\nexploratory analysis of gene expression data using prior knowledge.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.07905v2"
    },
    {
        "title": "A Conversation with Robert C. Elston",
        "authors": [
            "Gang Zheng",
            "Zhaohai Li",
            "Nancy L. Geller"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Robert C. Elston was born on February 4, 1932, in London, England. He went to\nCambridge University to study natural science from 1952-1956 and obtained B.A.,\nM.A. and Diploma in Agriculture (Dip Ag). He came to the US at age 24 to study\nanimal breeding at Cornell University and received his Ph.D. in 1959. From\n1959-1960, he was a post-doctoral fellow in biostatistics at University of\nNorth Carolina (UNC), Chapel Hill, where he studied mathematical statistics. He\nthen rose through the academic ranks in the department of biostatistics at UNC,\nbecoming a full professor in 1969. From 1979-1995, he was a professor and head\nof the Department of Biometry and Genetics at Louisiana State University\nMedical Center in New Orleans. In 1995, he moved to Case Western Reserve\nUniversity where he is a professor of epidemiology and biostatistics and served\nas chairman from 2008 to 2014. Between 1966 and 2013, he directed 42 Ph.D.\nstudents and mentored over 40 post-doctoral fellows. If one regards him as a\nfounder of a pedigree in research in genetic epidemiology, it was estimated in\n2007 that there were more than 500 progeny. Among his many honors are a NIH\nResearch Career Development Award (1966-1976), the Leadership Award from\nInternational Society of Human Genetics (1995), William Allan Award from\nAmerican Society of Human Genetics (1996), NIH MERIT Award (1998) and the\nMarvin Zelen Leadership Award, Harvard University (2004). He is a Fellow of the\nAmerican Statistical Association and the Institute of Mathematical Statistics\nas well as a Fellow of the Ohio Academy of Science. A leader in research in\ngenetic epidemiology for over 40 years, he has published over 600 research\narticles in biostatistics, genetic epidemiology and applications. He has also\ncoauthored and edited 9 books in biostatistics, population genetics and methods\nfor the analysis of genetic data.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.08500v1"
    },
    {
        "title": "A Conversation with Jerry Friedman",
        "authors": [
            "N. I. Fisher"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939.\nHe received his high school education at Yreka High School, then spent two\nyears at Chico State College before transferring to the University of\nCalifornia at Berkeley in 1959. He completed an undergraduate degree in physics\nin 1962 and a Ph.D. in high-energy particle physics in 1968 and was a\npost-doctoral research physicist at the Lawrence Berkeley Laboratory during\n1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as\nhead of the Computation Research Group, retaining this position until 2006. In\n1981, he was appointed half time as Professor in the Department of Statistics,\nStanford University, remaining half time with his SLAC appointment. He has held\nvisiting appointments at CSIRO in Sydney, CERN and the Department of Statistics\nat Berkeley, and has had a very active career as a commercial consultant. Jerry\nbecame Professor Emeritus in the Department of Statistics in 2007. Apart from\nsome 30 publications in high-energy physics early in his career, Jerry has\npublished over 70 research articles and books in statistics and computer\nscience, including co-authoring the pioneering books Classification and\nRegression Trees and The Elements of Statistical Learning. Many of his\npublications have hundreds if not thousands of citations (e.g., the CART book\nhas over 21,000). Much of his software is incorporated in commercial products,\nincluding at least one popular search engine. Many of his methods and\nalgorithms are essential inclusions in modern statistical and data mining\npackages. Honors include the following: the Rietz Lecture (1999) and the Wald\nLectures (2009); election to the American Academy of Arts and Sciences (2005)\nand the US National Academy of Sciences (2010); a Fellow of the American\nStatistical Association; Paper of the Year (JASA 1980, 1985; Technometrics\n1998, 1992); Statistician of the Year (ASA, Chicago Chapter, 1999); ACM Data\nMining Lifetime Innovation Award (2002), Emanuel & Carol Parzen Award for\nStatistical Innovation (2004); Noether Senior Lecturer (American Statistical\nAssociation, 2010); and the IEEE Computer Society Data Mining Research\nContribution Award (2012).\n",
        "pdf_link": "http://arxiv.org/pdf/1507.08502v1"
    },
    {
        "title": "A Framework for Infusing Authentic Data Experiences Within Statistics\n  Courses",
        "authors": [
            "Scott D. Grimshaw"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Working with complex data is one of the important updates to the 2014 ASA\nCurriculum Guidelines for Undergraduate Programs in Statistical Science.\nInfusing 'authentic data experiences' within courses allow students\nopportunities to learn and practice data skills as they prepare a dataset for\nanalysis. While more modest in scope than a senior-level culminating\nexperience, authentic data experiences provide an opportunity to demonstrate\nconnections between data skills and statistical skills. The result is more\npractice of data skills for undergraduate statisticians.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.08934v1"
    },
    {
        "title": "Combating anti-statistical thinking using simulation-based methods\n  throughout the undergraduate curriculum",
        "authors": [
            "Nathan Tintle",
            "Beth Chance",
            "George Cobb",
            "Soma Roy",
            "Todd Swanson",
            "Jill VanderStoep"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The use of simulation-based methods for introducing inference is growing in\npopularity for the Stat 101 course, due in part to increasing evidence of the\nmethods ability to improve students' statistical thinking. This impact comes\nfrom simulation-based methods (a) clearly presenting the overarching logic of\ninference, (b) strengthening ties between statistics and probability or\nmathematical concepts, (c) encouraging a focus on the entire research process,\n(d) facilitating student thinking about advanced statistical concepts, (e)\nallowing more time to explore, do, and talk about real research and messy data,\nand (f) acting as a firmer foundation on which to build statistical intuition.\nThus, we argue that simulation-based inference should be an entry point to an\nundergraduate statistics program for all students, and that simulation-based\ninference should be used throughout all undergraduate statistics courses. In\norder to achieve this goal and fully recognize the benefits of simulation-based\ninference on the undergraduate statistics program we will need to break free of\nhistorical forces tying undergraduate statistics curricula to mathematics,\nconsider radical and innovative new pedagogical approaches in our courses,\nfully implement assessment-driven content innovations, and embrace computation\nthroughout the curriculum.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.00543v1"
    },
    {
        "title": "Studies on properties and estimation problems for modified extension of\n  exponential distribution",
        "authors": [
            "M. A. El-Damcese",
            "Dina. A. Ramadan"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The present paper considers modified extension of the exponential\ndistribution with three parameters. We study the main properties of this new\ndistribution, with special emphasis on its median, mode and moments function\nand some characteristics related to reliability studies. For Modified-\nextension exponential distribution (MEXED) we have obtained the Bayes\nEstimators of scale and shape parameters using Lindley's approximation\n(L-approximation) under squared error loss function. But, through this\napproximation technique it is not possible to compute the interval estimates of\nthe parameters. Therefore, we also propose Gibbs sampling method to generate\nsample from the posterior distribution. On the basis of generated posterior\nsample we computed the Bayes estimates of the unknown parameters and\nconstructed 95 % highest posterior density credible intervals. A Monte Carlo\nsimulation study is carried out to compare the performance of Bayes estimators\nwith the corresponding classical estimators in terms of their simulated risk. A\nreal data set has been considered for illustrative purpose of the study.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02083v1"
    },
    {
        "title": "The Third Way Of Probability & Statistics: Beyond Testing and Estimation\n  To Importance, Relevance, and Skill",
        "authors": [
            "William M. Briggs"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  There is a third way of implementing probability models and practicing. This\nis to answer questions put in terms of observables. This eliminates frequentist\nhypothesis testing and Bayes factors and it also eliminates parameter\nestimation. The Third Way is the logical probability approach, which is to make\nstatements $\\Pr(Y \\in y | X,D,M)$ about observables of interest $Y$ taking\nvalues $y$, given probative data $X$, past observations (when present) $D$ and\nsome model (possibly deduced) $M$. Significance and the false idea that\nprobability models show causality are no more, and in their place are\nimportance and relevance. Models are built keeping on information that is\nrelevant and important to a decision maker (and not a statistician). All models\nare stated in publicly verifiable fashion, as predictions. All models must\nundergo a verification process before any trust is put into them.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02384v1"
    },
    {
        "title": "Explorations in Statistics Research: An Approach to Expose\n  Undergraduates to Authentic Data Analysis",
        "authors": [
            "Deborah Nolan",
            "Duncan Temple Lang"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The Explorations in Statistics Research workshop is a one-week NSF-funded\nsummer program that introduces undergraduate students to current research\nproblems in applied statistics. The goal of the workshop is to expose students\nto exciting, modern applied statistical research and practice, with the\nultimate aim of interesting them in seeking more training in statistics at the\nundergraduate and graduate levels. The program is explicitly designed to engage\nstudents in the connections between authentic domain problems and the\nstatistical ideas and approaches needed to address these problems, which is an\nimportant aspect of statistical thinking that is difficult to teach and\nsometimes lacking in our methodological courses and programs. Over the past\nnine years, we ran the workshop six times and a similar program in the sciences\ntwo times. We describe the program, summarize feedback from participants, and\nidentify the key features to its success. We abstract these features and\nprovide a set of recommendations for how faculty can incorporate important\nelements into their regular courses.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05541v1"
    },
    {
        "title": "Extended Dynamic Generalized Linear Models: the two-parameter\n  exponential family",
        "authors": [
            "Mariana Albi de Oliveira Souza",
            "Helio dos Santos Migon"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  We develop a Bayesian framework for estimation and prediction of dynamic\nmodels for observations from the two-parameter exponential family. Different\nlink functions are introduced to model both the mean and the precision in the\nexponential family allowing the introduction of covariates and time series\ncomponents. We explore conjugacy and analytical approximations under the class\nof partial specified models to keep the computation fast. The algorithm of\nWest, Harrison and Migon (1985) is extended to cope with the two-parameter\nexponential family models. The methodological novelties are illustrated with\ntwo applications to real data. The first, considers unemployment rates in\nBrazil and the second some macroeconomic variables for the United Kingdom.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.05914v1"
    },
    {
        "title": "A Conversation with Professor Tadeusz Caliński",
        "authors": [
            "Anthony C. Atkinson",
            "Barbara Bogacka"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Tadeusz Cali\\'{n}ski was born in Pozna\\'{n}, Poland in 1928. Despite the\nabsence of formal secondary eduction for Poles during the Second World War, he\nentered the University of Pozna\\'{n} in 1948, initially studying agronomy and\nin later years mathematics. From 1953 to 1988 he taught statistics, biometry\nand experimental design at the Agricultural University of Pozna\\'{n}. During\nthis period he founded and developed the Pozna\\'{n} inter-university school of\nmathematical statistics and biometry, which has become one of the most\nimportant schools of this type in Poland and beyond. He has supervised 24 Ph.D.\nstudents, many of whom are currently professors at a variety of universities.\nHe is now Professor Emeritus. Among many awards, in 1995 Professor Cali\\'{n}ski\nreceived the Order of Polonia Restituta for his outstanding achievements in the\nfields of Education and Science. In 2012 the Polish Statistical Society awarded\nhim The Jerzy Sp{\\l}awa-Neyman Medal for his contribution to the development of\nresearch in statistics in Poland. Professor Cali\\'{n}ski in addition has\nDoctoral Degrees honoris causa from the Agricultural University of Pozna\\'{n}\nand the Warsaw University of Life Sciences. His research interests include\nmathematical statistics and biometry, with applications to agriculture, natural\nsciences, biology and genetics. He has published over 140 articles in\nscientific journals as well as, with Sanpei Kageyama, two important books on\nthe randomization approach to the design and analysis of experiments. He has\nbeen extremely active and successful in initiating and contributing to fruitful\ninternational research cooperation between Polish statisticians and\nbiometricians and their colleagues in various countries, particularly in the\nNetherlands, France, Italy, Great Britain, Germany, Japan and Portugal. The\nconversations in addition cover the history of biometry and experimental design\nin Poland and the early influence of British statisticians.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.03056v1"
    },
    {
        "title": "A Conversation with Alan Gelfand",
        "authors": [
            "Bradley P. Carlin",
            "Amy H. Herring"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Alan E. Gelfand was born April 17, 1945, in the Bronx, New York. He attended\npublic grade schools and did his undergraduate work at what was then called\nCity College of New York (CCNY, now CUNY), excelling at mathematics. He then\nsurprised and saddened his mother by going all the way across the country to\nStanford to graduate school, where he completed his dissertation in 1969 under\nthe direction of Professor Herbert Solomon, making him an academic grandson of\nHerman Rubin and Harold Hotelling. Alan then accepted a faculty position at the\nUniversity of Connecticut (UConn) where he was promoted to tenured associate\nprofessor in 1975 and to full professor in 1980. A few years later he became\ninterested in decision theory, then empirical Bayes, which eventually led to\nthe publication of Gelfand and Smith [J. Amer. Statist. Assoc. 85 (1990)\n398-409], the paper that introduced the Gibbs sampler to most statisticians and\nrevolutionized Bayesian computing. In the mid-1990s, Alan's interests turned\nstrongly to spatial statistics, leading to fundamental contributions in\nspatially-varying coefficient models, coregionalization, and spatial boundary\nanalysis (wombling). He spent 33 years on the faculty at UConn, retiring in\n2002 to become the James B. Duke Professor of Statistics and Decision Sciences\nat Duke University, serving as chair from 2007-2012. At Duke, he has continued\nhis work in spatial methodology while increasing his impact in the\nenvironmental sciences. To date, he has published over 260 papers and 6 books;\nhe has also supervised 36 Ph.D. dissertations and 10 postdocs. This interview\nwas done just prior to a conference of his family, academic descendants, and\ncolleagues to celebrate his 70th birthday and his contributions to statistics\nwhich took place on April 19-22, 2015 at Duke University.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.03068v1"
    },
    {
        "title": "A multi-dimensional stream and its signature representation",
        "authors": [
            "Hao Ni"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The signature of a path is an essential object in the theory of rough paths.\nThe signature representation of the data stream can recover standard\nstatistics, e.g. the moments of the data stream. The classification of random\nwalks indicates the advantages of using the signature of a stream as the\nfeature set for machine learning.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.03346v1"
    },
    {
        "title": "From Statistician to Data Scientist",
        "authors": [
            "Philippe Besse",
            "Beatrice Laurent"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  According to a recent report from the European Commission, the world\ngenerates every minute 1.7 million of billions of data bytes, the equivalent of\n360,000 DVDs, and companies that build their decision-making processes by\nexploiting these data increase their productivity. The treatment and\nvalorization of massive data has consequences on the employment of graduate\nstudents in statistics. Which additional skills do students trained in\nstatistics need to acquire to become data scientists ? How to evolve training\nso that future graduates can adapt to rapid changes in this area, without\nneglecting traditional jobs and the fundamental and lasting foundation for the\ntraining? After considering the notion of big data and questioning the\nemergence of a \"new\" science: Data Science, we present the current developments\nin the training of engineers in Mathematical and Modeling at INSA Toulouse.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.07804v3"
    },
    {
        "title": "An Outline of the Bayesian Decision Theory",
        "authors": [
            "H. R. N. van Erp",
            "R. O. Linger",
            "P. H. A. J. M. van Gelder"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  In this paper we give an outline on the Bayesian Decision Theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.08307v1"
    },
    {
        "title": "Bayesian prediction for physical models with application to the\n  optimization of the synthesis of pharmaceutical products using chemical\n  kinetics",
        "authors": [
            "Antony Overstall",
            "David Woods",
            "Kieran Martin"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Quality control in industrial processes is increasingly making use of prior\nscientific knowledge, often encoded in physical models that require numerical\napproximation. Statistical prediction, and subsequent optimization, is key to\nensuring the process output meets a specification target. However, the\nnumerical expense of approximating the models poses computational challenges to\nthe identification of combinations of the process factors where there is\nconfidence in the quality of the response. Recent work in Bayesian computation\nand statistical approximation (emulation) of expensive computational models is\nexploited to develop a novel strategy for optimizing the posterior probability\nof a process meeting specification. The ensuing methodology is motivated by,\nand demonstrated on, a chemical synthesis process to manufacture a\npharmaceutical product, within which an initial set of substances evolve\naccording to chemical reactions, under certain process conditions, into a\nseries of new substances. One of these substances is a target pharmaceutical\nproduct and two are unwanted by-products. The aim is to determine the\ncombinations of process conditions and amounts of initial substances that\nmaximize the probability of obtaining sufficient target pharmaceutical product\nwhilst ensuring unwanted by-products do not exceed a given level. The\nrelationship between the factors and amounts of substances of interest is\ntheoretically described by the solution to a system of ordinary differential\nequations incorporating temperature dependence. Using data from a small\nexperiment, it is shown how the methodology can approximate the multivariate\nposterior predictive distribution of the pharmaceutical target and by-products,\nand therefore identify suitable operating values. Materials to replicate the\nanalysis can be found at www.github.com/amo105/chemicalkinetics.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.01347v4"
    },
    {
        "title": "Interactive graphics for functional data analyses",
        "authors": [
            "Julia Wrobel",
            "So Young Park",
            "Ana Maria Staicu",
            "Jeff Goldsmith"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Although there are established graphics that accompany the most common\nfunctional data analyses, generating these graphics for each dataset and\nanalysis can be cumbersome and time consuming. Often, the barriers to\nvisualization inhibit useful exploratory data analyses and prevent the\ndevelopment of intuition for a method and its application to a particular\ndataset. The refund.shiny package was developed to address these issues for\nseveral of the most common functional data analyses. After conducting an\nanalysis, the plot_shiny() function is used to generate an interactive\nvisualization environment that contains several distinct graphics, many of\nwhich are updated in response to user input. These visualizations reduce the\nburden of exploratory analyses and can serve as a useful tool for the\ncommunication of results to non-statisticians.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.04091v1"
    },
    {
        "title": "Picking Winners in Daily Fantasy Sports Using Integer Programming",
        "authors": [
            "David Scott Hunter",
            "Juan Pablo Vielma",
            "Tauhid Zaman"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  We consider the problem of selecting a portfolio of entries of fixed\ncardinality for contests with top-heavy payoff structures, i.e. most of the\nwinnings go to the top-ranked entries. This framework is general and can be\nused to model a variety of problems, such as movie studios selecting movies to\nproduce, venture capital firms picking start-up companies to invest in, or\nindividuals selecting lineups for daily fantasy sports contests, which is the\nexample we focus on here. We model the portfolio selection task as a\ncombinatorial optimization problem with a submodular objective function, which\nis given by the probability of at least one entry winning. We then show that\nthis probability can be approximated using only pairwise marginal probabilities\nof the entries winning when there is a certain structure on their joint\ndistribution. We consider a model where the entries are jointly Gaussian random\nvariables and present a closed form approximation to the objective function.\nBuilding on this, we then consider a scenario where the entries are given by\nsums of constrained resources and present an integer programming formulation to\nconstruct the entries. Our formulation uses principles based on our theoretical\nanalysis to construct entries: we maximize the expected score of an entry\nsubject to a lower bound on its variance and an upper bound on its correlation\nwith previously constructed entries. To demonstrate the effectiveness of our\ninteger programming approach, we apply it to daily fantasy sports contests that\nhave top-heavy payoff structures. We find that our approach performs well in\npractice. Using our integer programming approach, we are able to rank in the\ntop-ten multiple times in hockey and baseball contests with thousands of\ncompeting entries. Our approach can easily be extended to other problems with\nconstrained resources and a top-heavy payoff structure.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01455v3"
    },
    {
        "title": "Its All on the Square- The Importance of the Sum of Squares and Making\n  the General Linear Model Simple",
        "authors": [
            "Alexander Nussbaum",
            "Richard Seides"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Statistics is one of the most valuable of disciplines. Science is based on\nproof and it alone produces results, other approaches are not, and do not.\nStatistics is the only acceptable language of proof in science. Yet statistics\nis difficult to understand for a large percentage of those who will be\nevaluating and even doing research. Reasons for this difficulty may be that\nstatistics operates counter to the way people think, as well as the widespread\nphobia of numeracy. Adding to the difficulty is that undergraduate textbooks\ntend to make statistical tests seem to be an unorganized conglomeration of\nunrelated procedures, and this leads to a failure of students to understand\nthat all of the parametric procedures they are studying in an introductory\ncourse are ultimately doing the same thing and stem from common sources. In\nstatistics, precisely because the material is complex, the presentation must be\nsimple! This article endeavors to do just that.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.05418v1"
    },
    {
        "title": "Peter Hall's work on high-dimensional data and classification",
        "authors": [
            "Richard J. Samworth"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  In this article, I summarise Peter Hall's contributions to high-dimensional\ndata, including their geometric representations and variable selection methods\nbased on ranking. I also discuss his work on classification problems,\nconcluding with some personal reflections on my own interactions with him.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.01183v1"
    },
    {
        "title": "Bringing Order to the Chaos in the Brickyard",
        "authors": [
            "Bethany Shifflett"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  An allegory published in 1963 titled Chaos in the Brickyard spoke to the\ndecline in the quality of research. In the intervening time greater awareness\nof the issues and actions to improve research endeavors have emerged. Still,\nproblems persist. This paper is intended to clarify some of the challenges,\nparticularly with respect to quantitative research, then suggest ways to\nimprove the quality of published research. The paper highlights where feasible\nrefinements in analytical techniques can be made and provides a guide to\nfundamental principles related to data analysis in research.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.03189v1"
    },
    {
        "title": "Identifiability and testability in GRT with Individual Differences",
        "authors": [
            "Noah H. Silbert",
            "Robin D. Thomas"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Silbert and Thomas (2013) showed that failures of decisional separability are\nnot, in general, identifiable in fully parameterized $2 \\times 2$ Gaussian GRT\nmodels. A recent extension of $2 \\times 2$ GRT models (GRTwIND) was developed\nto solve this problem and a conceptually similar problem with the simultaneous\nidentifiability of means and marginal variances in GRT models. Central to the\nability of GRTwIND to solve these problems is the assumption of universal\nperception, which consists of shared perceptual distributions modified by\nattentional and global scaling parameters (Soto et al., 2015). If universal\nperception is valid, GRTwIND solves both issues. In this paper, we show that\nGRTwIND with universal perception and subject-specific failures of decisional\nseparability is mathematically, and thereby empirically, equivalent to a model\nwith decisional separability and failure of universal perception. We then\nprovide a formal proof of the fact that means and marginal variances are not,\nin general, simultaneously identifiable in $2 \\times 2$ GRT models, including\nGRTwIND. These results can be taken to delineate precisely what the assumption\nof universal perception must consist of. Based on these results and related\nrecent mathematical developments in the GRT framework, we propose that, in\naddition to requiring a fixed subset of parameters to determine the location\nand scale of any given GRT model, some subset of parameters must be set in GRT\nmodels to fix the orthogonality of the modeled perceptual dimensions, a central\nconceptual underpinning of the GRT framework. We conclude with a discussion of\nperceptual primacy and its relationship to universal perception.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.05598v2"
    },
    {
        "title": "Consider avoiding the .05 significance level",
        "authors": [
            "David Navon",
            "Yoav Cohen"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  It is suggested that some shortcomings of Null Hypothesis Significance\nTesting (NHST), viewed from the perspective of Bayesian statistics, turn benign\nonce the traditional threshold p value of .05 is substituted by a sufficiently\nsmaller value. To illustrate, the posterior probability of H0 stating P=.5,\ngiven data that just render it rejected by NHST with a p value of .05 (and a\nuniform prior), is shown here to be not much smaller than .50 for most values\nof N below 100 (and even exceeds .50 for N>=100); in contrast, with a p value\nof .001 posterior probability does not exceed .06 for N<=100 (neither .25 for\nN<9000). Yet more interesting, posterior probability becomes quite independent\nof N with a p value of .0001, hence practically satisfying the alpha postulate\n- set by Cornfield (1966) as the condition for p value being a measure of\nevidence in itself. In view of the low prospect that most researchers will soon\nconvert to use Bayesian statistics in any form, we thus suggest that\nresearchers who elect the conservative option of resorting to NHST be\nencouraged to avoid as much as possible using a p value of .05 as a threshold\nfor rejecting H0. The analysis presented here may be used to discuss afresh\nwhich level of threshold p value seems to be a reasonable, practical\nsubstitute.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.09017v1"
    },
    {
        "title": "Designing Modular Software: A Case Study in Introductory Statistics",
        "authors": [
            "Eric Hare",
            "Andee Kaplan"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Modular programming is a development paradigm that emphasizes self-contained,\nflexible, and independent pieces of functionality. This practice allows new\nfeatures to be seamlessly added when desired, and unwanted features to be\nremoved, thus simplifying the user-facing view of the software. The recent rise\nof web-based software applications has presented new challenges for designing\nan extensible, modular software system. In this paper, we outline a framework\nfor designing such a system, with a focus on reproducibility of the results. We\npresent as a case study a Shiny-based web application called intRo, that allows\nthe user to perform basic data analyses and statistical routines. Finally, we\nhighlight some challenges we encountered, and how to address them, when\ncombining modular programming concepts with reactive programming as used by\nShiny.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.02533v2"
    },
    {
        "title": "Revisiting nested group testing procedures: new results, comparisons,\n  and robustness",
        "authors": [
            "Yaakov Malinovsky",
            "Paul S. Albert"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Group testing has its origin in the identification of syphilis in the US army\nduring World War II. Much of the theoretical framework of group testing was\ndeveloped starting in the late 1950s, with continued work into the 1990s.\nRecently, with the advent of new laboratory and genetic technologies, there has\nbeen an increasing interest in group testing designs for cost saving purposes.\nIn this paper, we compare different nested designs, including Dorfman, Sterrett\nand an optimal nested procedure obtained through dynamic programming. To\nelucidate these comparisons, we develop closed-form expressions for the optimal\nSterrett procedure and provide a concise review of the prior literature for\nother commonly used procedures. We consider designs where the prevalence of\ndisease is known as well as investigate the robustness of these procedures when\nit is incorrectly assumed. This article provides a technical presentation that\nwill be of interest to researchers as well as from a pedagogical perspective.\nSupplementary material for this article is available online.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.06330v4"
    },
    {
        "title": "Sterrett Procedure for the Generalized Group Testing Problem",
        "authors": [
            "Yaakov Malinovsky"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Group testing is a useful method that has broad applications in medicine,\nengineering, and even in airport security control. Consider a finite population\nof $N$ items, where item $i$ has a probability $p_i$ to be defective. The goal\nis to identify all items by means of group testing. This is the generalized\ngroup testing problem. The optimum procedure, with respect to the expected\ntotal number of tests, is unknown even in case when all $p_i$ are equal.\n\\cite{H1975} proved that an ordered partition (with respect to $p_i$) is the\noptimal for the Dorfman procedure (procedure $D$), and obtained an optimum\nsolution (i.e., found an optimal partition) by dynamic programming. In this\npaper, we investigate the Sterrett procedure (procedure $S$). We provide close\nform expression for the expected total number of tests, which allows us to find\nthe optimum arrangement of the items in the particular group. We also show that\nan ordered partition is not optimal for the procedure $S$ or even for a\nslightly modified Dorfman procedure (procedure $D^{\\prime}$). This discovery\nimplies that finding an optimal procedure $S$ appears to be a hard\ncomputational problem. However, by using an optimal ordered partition for all\nprocedures, we show that procedure $D^{\\prime}$ is uniformly better than\nprocedure $D$, and based on numerical comparisons, procedure $S$ is uniformly\nand significantly better than procedures $D$ and $D^{\\prime}$.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04478v3"
    },
    {
        "title": "Conditional Visualization for Statistical Models: An Introduction to the\n  condvis Package in R",
        "authors": [
            "Mark O'Connell",
            "Catherine B. Hurley",
            "Katarina Domijan"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  The condvis package is for interactive visualization of sections in data\nspace, showing fitted models on the section, and observed data near the\nsection. The primary goal is the interpretation of complex models, and showing\nhow the observed data support the fitted model. There is a video accompaniment\nto this paper available at https://www.youtube.com/watch?v=rKFq7xwgdX0. This is\na preprint version of an article to appear in the Journal of Statistical\nSoftware.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00290v1"
    },
    {
        "title": "Scale and curvature effects in principal geodesic analysis",
        "authors": [
            "Drew Lazar",
            "Lizhen Lin"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  There is growing interest in using the close connection between differential\ngeometry and statistics to model smooth manifold-valued data. In particular,\nmuch work has been done recently to generalize principal component analysis\n(PCA), the method of dimension reduction in linear spaces, to Riemannian\nmanifolds. One such generalization is known as principal geodesic analysis\n(PGA). This paper, in a novel fashion, obtains Taylor expansions in scaling\nparameters introduced in the domain of objective functions in PGA. It is shown\nthis technique not only leads to better closed-form approximations of PGA but\nalso reveals the effects that scale, curvature and the distribution of data\nhave on solutions to PGA and on their differences to first-order tangent space\napproximations. This approach should be able to be applied not only to PGA but\nalso to other generalizations of PCA and more generally to other intrinsic\nstatistics on Riemannian manifolds.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.01537v2"
    },
    {
        "title": "A Devastating Example for the Halfer Rule",
        "authors": [
            "Vincent Conitzer"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  How should we update de dicto beliefs in the face of de se evidence? The\nSleeping Beauty problem divides philosophers into two camps, halfers and\nthirders. But there is some disagreement among halfers about how their position\nshould generalize to other examples. A full generalization is not always given;\none notable exception is the Halfer Rule, under which the agent updates her\nuncentered beliefs based on only the uncentered part of her evidence. In this\nbrief article, I provide a simple example for which the Halfer Rule prescribes\ncredences that, I argue, cannot be reasonably held by anyone. In particular,\nthese credences constitute an egregious violation of the Reflection Principle.\nI then discuss the consequences for halfing in general.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.05733v1"
    },
    {
        "title": "Estimating the Probability that a Function Observed with Noise is Convex",
        "authors": [
            "Nanjing Jian",
            "Shane G. Henderson"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Consider a real-valued function that can only be observed with stochastic\nnoise at a finite set of design points within a Euclidean space. We wish to\ndetermine whether there exists a convex function that goes through the true\nfunction values at the design points. We develop an asymptotically consistent\nBayesian sequential sampling procedure that estimates the posterior probability\nof this being true. In each iteration, the posterior probability is estimated\nusing Monte Carlo simulation. We offer three variance reduction methods --\nchange of measure, acceptance-rejection, and conditional Monte Carlo. Numerical\nexperiments suggest that the conditional Monte Carlo method should be\npreferred.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.04185v2"
    },
    {
        "title": "The new concepts of measurement error's regularities and effect\n  characteristics",
        "authors": [
            "Xiaoming Ye",
            "Haibo Liu",
            "Xuebin Xiao",
            "Mo Ling"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  In several literatures, the authors give a new thinking of measurement theory\nsystem based on error non-classification philosophy, which completely\noverthrows the existing measurement concept system of precision, trueness and\naccuracy. In this paper, by focusing on the issues of error's regularities and\neffect characteristics, the authors will do a thematic interpretation, and\nprove that the error's regularities actually come from different cognitive\nperspectives, are also unable to be used for classifying errors, and that the\nerror's effect characteristics actually depend on artificial condition rules of\nrepeated measurement, and are still unable to be used for classifying errors.\nThus, from the perspectives of error's regularities and effect characteristics,\nthe existing error classification philosophy is still incorrect; and an\nuncertainty concept system, which must be interpreted by the error\nnon-classification philosophy, naturally becomes the only way out of\nmeasurement theory.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.08648v3"
    },
    {
        "title": "What is the best fractional derivative to fit data?",
        "authors": [
            "Ricardo Almeida"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  The aim of this work is to show, based on concrete data observation, that the\nchoice of the fractional derivative when modelling a problem is relevant for\nthe accuracy of a method. Using the least squares fitting technique, we\ndetermine the order of the fractional differential equation that better\ndescribes the experimental data, for different types of fractional derivatives.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.00609v1"
    },
    {
        "title": "Nonparametric estimation of the conditional distribution at regression\n  boundary points",
        "authors": [
            "Srinjoy Das",
            "Dimitris N. Politis"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Nonparametric regression is a standard statistical tool with increased\nimportance in the Big Data era. Boundary points pose additional difficulties\nbut local polynomial regression can be used to alleviate them. Local linear\nregression, for example, is easy to implement and performs quite well both at\ninterior as well as boundary points. Estimating the conditional distribution\nfunction and/or the quantile function at a given regressor point is immediate\nvia standard kernel methods but problems ensue if local linear methods are to\nbe used. In particular, the distribution function estimator is not guaranteed\nto be monotone increasing, and the quantile curves can \"cross\". In the paper at\nhand, a simple method of correcting the local linear distribution estimator for\nmonotonicity is proposed, and its good performance is demonstrated via\nsimulations and real data examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.00674v1"
    },
    {
        "title": "A Mathematically Sensible Explanation of the Concept of Statistical\n  Population",
        "authors": [
            "Yiping Cheng"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  In statistics education, the concept of population is widely felt hard to\ngrasp, as a result of vague explanations in textbooks. Some textbook authors\ntherefore chose not to mention it. This paper offers a new explanation by\nproposing a new theoretical framework of population and sampling, which aims to\nachieve high mathematical sensibleness. In the explanation, the term population\nis given clear definition, and the relationship between simple random sampling\nand iid random variables are examined mathematically.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01732v1"
    },
    {
        "title": "A New Theoretical Interpretation of Measurement Error and Its\n  Uncertainty",
        "authors": [
            "Huisheng Shi",
            "Xiaoming Ye",
            "Cheng Xing",
            "Shijun Ding"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  The traditional measurement theory interprets the variance as the dispersion\nof a measured value, which is actually contrary to a general mathematical\nconcept that the variance of a constant is 0. This paper will fully demonstrate\nthat the variance in measurement theory is actually the evaluation of\nprobability interval of an error instead of the dispersion of a measured value,\npoint out the key point of mistake in the traditional interpretation, and fully\ninterpret a series of changes in conceptual logic and processing method brought\nabout by this new concept.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.03812v15"
    },
    {
        "title": "Remark On Variance Bounds",
        "authors": [
            "R. Sharma"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  It is shown that the formula for the variance of combined series yields\nsurprisingly simple proofs of some well known variance bounds.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.06292v1"
    },
    {
        "title": "Information vs. Uncertainty as the Foundation for a Science of\n  Environmental Modeling",
        "authors": [
            "Grey Nearing",
            "Hoshin Gupta"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Information accounting provides a better foundation for hypothesis testing\nthan does uncertainty quantification. A quantitative account of science is\nderived under this perspective that alleviates the need for epistemic bridge\nprinciples, solves the problem of ad hoc falsification criteria, and deals with\nverisimilitude by facilitating a general approach to process-level diagnostics.\nOur argument is that the well-known inconsistencies of both Bayesian and\nclassical statistical hypothesis tests are due to the fact that probability\ntheory is an insufficient logic of science. Information theory, as an extension\nof probability theory, is required to provide a complete logic on which to base\nquantitative theories of empirical learning. The organizing question in this\ncase becomes not whether our theories or models are more or less true, or about\nhow much uncertainty is associated with a particular model, but instead whether\nthere is any information available from experimental data that might allow us\nto improve the model. This becomes a formal hypothesis test, provides a theory\nof model diagnostics, and suggests a new approach to building dynamical systems\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.07512v1"
    },
    {
        "title": "Feasibility study on the least square method for fitting non-Gaussian\n  noise data",
        "authors": [
            "Wei Xu",
            "Wen Chen",
            "Yingjie Liang"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  This study is to investigate the feasibility of least square method in\nfitting non-Gaussian noise data. We add different levels of the two typical\nnon-Gaussian noises, L\\'evy and stretched Gaussian noises, to exact value of\nthe selected functions including linear equations, polynomial and exponential\nequations, and the maximum absolute and the mean square errors are calculated\nfor the different cases. L\\'evy and stretched Gaussian distributions have many\napplications in fractional and fractal calculus. It is observed that the\nnon-Gaussian noises are less accurately fitted than the Gaussian noise, but the\nstretched Gaussian cases appear to perform better than the L\\'evy noise cases.\nIt is stressed that the least-squares method is inapplicable to the\nnon-Gaussian noise cases when the noise level is larger than 5%.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01451v2"
    },
    {
        "title": "A Dutch Book against Sleeping Beauties Who Are Evidential Decision\n  Theorists",
        "authors": [
            "Vincent Conitzer"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  In the context of the Sleeping Beauty problem, it has been argued that\nso-called \"halfers\" can avoid Dutch book arguments by adopting evidential\ndecision theory. I introduce a Dutch book for a variant of the Sleeping Beauty\nproblem and argue that evidential decision theorists fall prey to it, whether\nthey are halfers or thirders. The argument crucially requires that an action\ncan provide evidence for what the agent would do not only at other decision\npoints where she has exactly the same information, but also at decision points\nwhere she has different but \"symmetric\" information.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.03560v1"
    },
    {
        "title": "Can rational choice guide us to correct {\\em de se} beliefs?",
        "authors": [
            "Vincent Conitzer"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Significant controversy remains about what constitute correct self-locating\nbeliefs in scenarios such as the Sleeping Beauty problem, with proponents on\nboth the \"halfer\" and \"thirder\" sides. To attempt to settle the issue, one\nnatural approach consists in creating decision variants of the problem,\ndetermining what actions the various candidate beliefs prescribe, and assessing\nwhether these actions are reasonable when we step back. Dutch book arguments\nare a special case of this approach, but other Sleeping Beauty games have also\nbeen constructed to make similar points. Building on a recent article (James\nR.~Shaw. {\\em De se} belief and rational choice. {\\em Synthese},\n190(3):491-508, 2013), I show that in general we should be wary of such\narguments, because unintuitive actions may result for reasons that are\nunrelated to the beliefs. On the other hand, I show that, when we restrict our\nattention to {\\em additive} games, then a thirder will necessarily maximize her\n{\\em ex ante} expected payout, but a halfer in some cases will not (assuming\ncausal decision theory). I conclude that this does not necessarily settle the\nissue and speculate about what might.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.06332v1"
    },
    {
        "title": "Semiparametric estimation in the normal variance-mean mixture model",
        "authors": [
            "Denis Belomestny",
            "Vladimir Panov"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  In this paper we study the problem of statistical inference on the parameters\nof the semiparametric variance-mean mixtures. This class of mixtures has\nrecently become rather popular in statistical and financial modelling. We\ndesign a semiparametric estimation procedure that first estimates the mean of\nthe underlying normal distribution and then recovers nonparametrically the\ndensity of the corresponding mixing distribution. We illustrate the performance\nof our procedure on simulated and real data.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07578v1"
    },
    {
        "title": "Data Visualization on Day One: Bringing Big Ideas into Intro Stats Early\n  and Often",
        "authors": [
            "Xiaofei Wang",
            "Cynthia Rush",
            "Nicholas Jon Horton"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  In a world awash with data, the ability to think and compute with data has\nbecome an important skill for students in many fields. For that reason,\ninclusion of some level of statistical computing in many introductory-level\ncourses has grown more common in recent years. Existing literature has\ndocumented multiple success stories of teaching statistics with R, bolstered by\nthe capabilities of R Markdown. In this article, we present an in-class data\nvisualization activity intended to expose students to R and R Markdown during\nthe first week of an introductory statistics class. The activity begins with a\nbrief lecture on exploratory data analysis in R. Students are then placed in\nsmall groups tasked with exploring a new dataset to produce three\nvisualizations that describe particular insights that are not immediately\nobvious from the data. Upon completion, students will have produced a series of\nunivariate and multivariate visualizations on a real dataset and practiced\ndescribing them.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.08544v1"
    },
    {
        "title": "Application of Bayesian Networks for Estimation of Individual\n  Psychological Characteristics",
        "authors": [
            "Alexander Litvinenko",
            "Natalya Litvinenko",
            "Orken Mamyrbayev"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  An accurate qualitative and comprehensive assessment of human potential is\none of the most important challenges in any company or collective. We apply\nBayesian networks for developing more accurate overall estimations of\npsychological characteristics of an individual, based on psychological test\nresults, which identify how much an individual possesses a certain trait.\nExamples of traits could be a stress resistance, the readiness to take a risk,\nthe ability to concentrate on certain complicated work. The most common way of\nstudying psychological characteristics of each individual is testing.\nAdditionally, the overall estimation is usually based on personal experiences\nand the subjective perception of a psychologist or a group of psychologists\nabout the investigated psychological personality traits.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.00060v3"
    },
    {
        "title": "Statistics Educational Challenge in the 21st Century",
        "authors": [
            "Subhadeep Mukhopadhyay"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  What do we teach and what should we teach? An honest answer to this question\nis painful, very painful--what we teach lags decades behind what we practice.\nHow can we reduce this `gap' to prepare a data science workforce of trained\nnext-generation statisticians? This is a challenging open problem that requires\nmany well-thought-out experiments before finding the secret sauce. My goal in\nthis article is to lay out some basic principles and guidelines (rather than\ncreating a pseudo-curriculum based on cherry-picked topics) to expedite this\nprocess for finding an `objective' solution.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04098v1"
    },
    {
        "title": "Redundancy schemes for engineering coherent systems via a\n  signature-based approach",
        "authors": [
            "Mahdi Doostparast"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  This paper proposes a signature-based approach for solving redundancy\nallocation problems when component lifetimes are not only heterogeneous but\nalso dependent. The two common schemes for allocations, that is active and\nstandby redundancies, are considered. If the component lifetimes are\nindependent, the proposed approach leads to simple manipulations. Various\nillustrative examples are also analysed. This method can be implemented for\npractical complex engineering systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07059v1"
    },
    {
        "title": "Distance Correlation: A New Tool for Detecting Association and Measuring\n  Correlation Between Data Sets",
        "authors": [
            "Donald St. P. Richards"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  The difficulties of detecting association, measuring correlation, and\nestablishing cause and effect have fascinated mankind since time immemorial.\nDemocritus, the Greek philosopher, underscored well the importance and the\ndifficulty of proving causality when he wrote, \"I would rather discover one\ncause than gain the kingdom of Persia.\" To address the difficulties of relating\ncause and effect, statisticians have developed many inferential techniques.\nPerhaps the most well-known method stems from Karl Pearson's coefficient of\ncorrelation, which Pearson introduced in the late 19th century based on ideas\nof Francis Galton.\n  I will describe in this lecture the recently-devised distance correlation\ncoefficient and describe its advantages over the Pearson and other classical\nmeasures of correlation. We will examine an application of the distance\ncorrelation coefficient to data drawn from large astrophysical databases, where\nit is desired to classify galaxies according to various types. Further, the\nlecture will analyze data arising in the ongoing national discussion of the\nrelationship between state-by-state homicide rates and the stringency of state\nlaws governing firearm ownership.\n  The lecture will also describe a remarkable singular integral which lies at\nthe core of the theory of the distance correlation coefficient. We will see\nthat this singular integral admits generalizations to the truncated Maclaurin\nexpansions of the cosine function and to the theory of spherical functions on\nsymmetric cones.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.06400v1"
    },
    {
        "title": "On the Virtues of Automated QSAR The New Kid on the Block",
        "authors": [
            "Marcelo T. de Oliveira",
            "Edson Katekawa"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Quantitative Structure-Activity Relationship (QSAR) has proved an invaluable\ntool in medicinal chemistry. Data availability at unprecedented levels through\nvarious databases have collaborated to a resurgence in the interest for QSAR.\nIn this context, rapid generation of quality predictive models is highly\ndesirable for hit identification and lead optimization. We showcase the\napplication of an automated QSAR approach, which randomly selects multiple\ntraining/test sets and utilizes machine-learning algorithms to generate\npredictive models. Results demonstrate that AutoQSAR produces models of\nimproved or similar quality to those generated by practitioners in the field\nbut in just a fraction of the time. Despite the potential of the concept to the\nbenefit of the community, the AutoQSAR opportunity has been largely\nundervalued.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.02639v1"
    },
    {
        "title": "Conducting Highly Principled Data Science: A Statistician's Job and Joy",
        "authors": [
            "Xiao-Li Meng"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Highly Principled Data Science insists on methodologies that are: (1)\nscientifically justified, (2) statistically principled, and (3) computationally\nefficient. An astrostatistics collaboration, together with some reminiscences,\nillustrates the increased roles statisticians can and should play to ensure\nthis trio, and to advance the science of data along the way.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00544v2"
    },
    {
        "title": "Statistics students' identification of inferential model elements within\n  contexts of their own invention",
        "authors": [
            "Matthew Beckman",
            "Robert delMas"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Statistical thinking partially depends upon an iterative process by which\nessential features of a problem setting are identified and mapped onto an\nabstract model or archetype, and then translated back into the context of the\noriginal problem setting (Wild and Pfannkuch 1999). Assessment in introductory\nstatistics often relies on tasks that present students with data in context and\nexpects them to choose and describe an appropriate model. This study explores\npost-secondary student responses to an alternative task that prompts students\nto clearly identify a sample, population, statistic, and parameter using a\ncontext of their own invention. The data include free text narrative responses\nof a random sample of 500 students from a sample of more than 1600 introductory\nstatistics students. Results suggest that students' responses often portrayed\nsample and population accurately. Portrayals of statistic and parameter were\nless reliable and were associated with descriptions of a wide variety of other\nconcepts. Responses frequently attributed a variable of some kind to the\nstatistic, or a study design detail to the parameter. Implications for\ninstruction and research are discussed, including a call for emphasis on a\nmodeling paradigm in introductory statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.02410v2"
    },
    {
        "title": "On optimal policy in the group testing with incomplete identification",
        "authors": [
            "Yaakov Malinovsky"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Consider a very large (infinite) population of items, where each item\nindependent from the others is defective with probability p, or good with\nprobability q=1-p. The goal is to identify N good items as quickly as possible.\nThe following group testing policy (policy A) is considered: test items\ntogether in the groups, if the test outcome of group i of size n_i is negative,\nthen accept all items in this group as good, otherwise discard the group. Then,\nmove to the next group and continue until exact N good items are found. The\ngoal is to find an optimal testing configuration, i.e., group sizes, under\npolicy A, such that the expected waiting time to obtain N good items is\nminimal. Recently, Gusev (2012) found an optimal group testing configuration\nunder the assumptions of constant group size and N=\\infty. In this note, an\noptimal solution under policy A for finite N is provided. Keywords: Dynamic\nprogramming; Optimal design; Partition problem; Shur-convexity\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03168v2"
    },
    {
        "title": "Data Science: A Three Ring Circus or a Big Tent?",
        "authors": [
            "Jennifer Bryan",
            "Hadley Wickham"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  This is part of a collection of discussion pieces on David Donoho's paper 50\nYears of Data Science, appearing in Volume 26, Issue 4 of the Journal of\nComputational and Graphical Statistics (2017).\n",
        "pdf_link": "http://arxiv.org/pdf/1712.07349v1"
    },
    {
        "title": "Data Science vs. Statistics: Two Cultures?",
        "authors": [
            "Iain Carmichael",
            "J. S. Marron"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Data science is the business of learning from data, which is traditionally\nthe business of statistics. Data science, however, is often understood as a\nbroader, task-driven and computationally-oriented version of statistics. Both\nthe term data science and the broader idea it conveys have origins in\nstatistics and are a reaction to a narrower view of data analysis. Expanding\nupon the views of a number of statisticians, this paper encourages a big-tent\nview of data analysis. We examine how evolving approaches to modern data\nanalysis relate to the existing discipline of statistics (e.g. exploratory\nanalysis, machine learning, reproducibility, computation, communication and the\nrole of theory). Finally, we discuss what these trends mean for the future of\nstatistics by highlighting promising directions for communication, education\nand research.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00371v2"
    },
    {
        "title": "Using Random Variables to Predict Experimental Outcomes",
        "authors": [
            "James D. Stein"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  We shall show in this paper that there are experiments which are Bernoulli\ntrials with success probability p > 0.5, and which have the curious feature\nthat it is possible to correctly predict the outcome with probability > p.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02426v1"
    },
    {
        "title": "Curriculum Guidelines for Undergraduate Programs in Data Science",
        "authors": [
            "Richard De Veaux",
            "Mahesh Agarwal",
            "Maia Averett",
            "Benjamin Baumer",
            "Andrew Bray",
            "Thomas Bressoud",
            "Lance Bryant",
            "Lei Cheng",
            "Amanda Francis",
            "Robert Gould",
            "Albert Y. Kim",
            "Matt Kretchmar",
            "Qin Lu",
            "Ann Moskol",
            "Deborah Nolan",
            "Roberto Pelayo",
            "Sean Raleigh",
            "Ricky J. Sethi",
            "Mutiara Sondjaja",
            "Neelesh Tiruviluamala",
            "Paul Uhlig",
            "Talitha Washington",
            "Curtis Wesley",
            "David White",
            "Ping Ye"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  The Park City Math Institute (PCMI) 2016 Summer Undergraduate Faculty Program\nmet for the purpose of composing guidelines for undergraduate programs in Data\nScience. The group consisted of 25 undergraduate faculty from a variety of\ninstitutions in the U.S., primarily from the disciplines of mathematics,\nstatistics and computer science. These guidelines are meant to provide some\nstructure for institutions planning for or revising a major in Data Science.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06814v1"
    },
    {
        "title": "Combining Empirical Likelihood and Robust Estimation Methods for Linear\n  Regression Models",
        "authors": [
            "Şenay Özdemir",
            "Olcay Arslan"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Ordinary least square (OLS), maximum likelihood (ML) and robust methods are\nthe widely used methods to estimate the parameters of a linear regression\nmodel. It is well known that these methods perform well under some\ndistributional assumptions on error terms. However, these distributional\nassumptions on the errors may not be appropriate for some data sets. In these\ncase, nonparametric methods may be considered to carry on the regression\nanalysis. Empirical likelihood (EL) method is one of these nonparametric\nmethods. The EL method maximizes a function, which is multiplication of the\nunknown probabilities corresponding to each observation, under some constraints\ninherited from the normal equations in OLS estimation method. However, it is\nwell known that the OLS method has poor performance when there are some\noutliers in the data. In this paper, we consider the EL method with robustifyed\nconstraints. The robustification of the constraints is done by using the robust\nM estimation methods for regression. We provide a small simulation study and a\nreal data example to demonstrate the capability of the robust EL method to\nhandle unusual observations in the data. The simulation and real data results\nreveal that robust constraints are needed when heavy tailedness and/or outliers\nare possible in the data.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.08812v1"
    },
    {
        "title": "Statistical analysis of the effect of the current, potential and\n  proposed rules of a game in tennis",
        "authors": [
            "G. Szigeti"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  With the aid of mathematical modelling (basic tool is the random walk with\nabsorbing barriers) we derive subsequent formulas to study the effect of\ndifferent versions of possible rules. For different rules the probability of\nwinning a game, the probability of break point occurrence, the mathematical\nexpectation of the number of rallies (points) and, the mathematical expectation\nof the number of break points in a game are expressed. We check these rules\nagainst ATP statistics for the Top-200 men players. In conclusion, we suggest a\nslight but essential modification for the rule of a tennis game, namely ,\nsecond service ( in case of a first service fault) is to be allowed only at the\nfirst three points (rallies). This would partially preserve the traditions\n(server has an advantage in the modern game) and at the same time it would\nreduce the predictability of the game, significantly increasing in this way the\nexcitement for the spectators.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.09284v1"
    },
    {
        "title": "Which Football Player Bears Most Resemblance to Messi? A Statistical\n  Analysis",
        "authors": [
            "Jiri Mazurek"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Many pundits and fans ask themselves the same question: Which football player\nbears most resemblance to Lionel Messi? Is it Chelsea's Eden Hazard? Is it\nPaulo Dybala, the heir to Messi in the national team of Argentina? Or is the\nmost alike player to Messi someone completely else? In general, the research on\nthe evaluation of players' performances originated in the context of baseball\nin the USA, but, currently, it is of great importance in almost every team\nsport on the planet. Specifically, football clubs' managers can use the data on\nplayer's similarity when looking for replacement of their players by other,\npresumably similar ones. Also, the research in the presented direction is\ncertainly interesting both for football pundits and football fans. Therefore,\nthe aim of this study is to answer the question from the title with the use of\nthe statistical analysis based on the data from ongoing league season retrieved\nfrom WhoScored (WS) database. WS provides detailed data (up to 24 parameters\nsuch as goals scored, the number of assists, shots on goal, passes, dribbles or\nfouls) for players of TOP 5 European leagues, and ranks them with respect to\ntheir overall performance. For this study, 17 parameters (criteria) most\nrelevant for an attacking player were used, and a set of 28 players, candidates\nto be 'most alike to Messi' from WS TOP 100 list were selected. After data\nnormalization and application of a proper metric function the most similar\nplayer to Lionel Messi was found.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.00967v1"
    },
    {
        "title": "Risk Factors Associated with Mortality in Game of Thrones: A\n  Longitudinal Cohort Study",
        "authors": [
            "Suveen Angraal",
            "Ambika Bhatnagar",
            "Suraj Verma",
            "Sukhman Shergill",
            "Aakriti Gupta",
            "Rohan Khera"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Objective: To assess mortality, and identify the risk factors associated with\nmortality in Game of Thrones (GoT). Design and Setting: A longitudinal cohort\nstudy in the fictional kingdom of Westeros and Essos. Participants: All the\ncharacters appearing in the GoT since airing of its first episode with screen\ntime of greater than or equal to 5 minutes. Main Outcome Measures: All-cause\nmortality. Multivariate Cox proportional hazard model was used to assess the\nrisk factors associated with mortality, represented by hazard ratios, with\nepisodes as the unit of time. Results: Of the 132 characters, followed up for a\nmedian time of 32 episodes, a total 89 (67.4%) characters died; with external\ninvasive injury as the most common cause of death, attributing to 42.4% of the\ntotal deaths. Age (in decades) was a significant risk factor for death [HR,\n1.24 (95% CI, 1.08-1.43), P=0.0001]. Although statistically non-significant,\nallegiance to house Targaryen [HR, 1.10 (95% CI, 0.32-3.77)] was associated\nwith a higher risk for mortality per episode than house Stark. Characters\nresiding in South were less likely to die than characters residing in North\n[HR, 0.58 (95% CI, 0.29-1.16), P=0.12]. Advisors showed a lower risk of\nmortality than the members of houses, with some statistical significance [HR,\n0.39 (95% CI, 0.14-1.08), P=0.07]. Conclusions: There is a high mortality rate\namong the characters in GoT. Residing in the North and being a member of a\nhouse is very dangerous in GoT. Allegiance to house Stark trended to be safer\nthan house Targaryen.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.04161v5"
    },
    {
        "title": "Differentiating the pseudo determinant",
        "authors": [
            "Andrew Holbrook"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  A class of derivatives is defined for the pseudo determinant $Det(A)$ of a\nHermitian matrix $A$. This class is shown to be non-empty and to have a unique,\ncanonical member $\\mathbf{\\nabla Det}(A)=Det(A)A^+$, where $A^+$ is the\nMoore-Penrose pseudo inverse. The classic identity for the gradient of the\ndeterminant is thus reproduced. Examples are provided, including the maximum\nlikelihood problem for the rank-deficient covariance matrix of the degenerate\nmultivariate Gaussian distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.04878v2"
    },
    {
        "title": "Elements of the Kopula (eventological copula) theory",
        "authors": [
            "Oleg Yu. Vorobyev"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  New in the probability theory and eventology theory, the concept of Kopula\n(eventological copula) is introduced. The theorem on the characterization of\nthe sets of events by Kopula is proved, which serves as the eventological\npre-image of the well-known Sclar's theorem on copulas (1959). The Kopulas of\ndoublets and triplets of events are given, as well as of some N-sets of events.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.08179v1"
    },
    {
        "title": "A Project Based Approach to Statistics and Data Science",
        "authors": [
            "David White"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  In an increasingly data-driven world, facility with statistics is more\nimportant than ever for our students. At institutions without a statistician,\nit often falls to the mathematics faculty to teach statistics courses. This\npaper presents a model that a mathematician asked to teach statistics can\nfollow. This model entails connecting with faculty from numerous departments on\ncampus to develop a list of topics, building a repository of real-world\ndatasets from these faculty, and creating projects where students interface\nwith these datasets to write lab reports aimed at consumers of statistics in\nother disciplines. The end result is students who are well prepared for\ninterdisciplinary research, who are accustomed to coping with the\nidiosyncrasies of real data, and who have sharpened their technical writing and\nspeaking skills.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.08858v1"
    },
    {
        "title": "Assessing the association between pre-course metrics of student\n  preparation and student performance in introductory statistics: Results from\n  early data on simulation-based inference vs. nonsimulation based inference",
        "authors": [
            "Nathan Tintle",
            "Jake Clark",
            "Karen Fischer",
            "Beth Chance",
            "George Cobb",
            "Soma Roy",
            "Todd Swanson",
            "Jill VanderStoep"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  The recent simulation-based inference (SBI) movement in algebra-based\nintroductory statistics courses (Stat 101) has provided preliminary evidence of\nimproved student conceptual understanding and retention. However, little is\nknown about whether these positive effects are preferentially distributed\nacross types of students entering the course. We consider how two metrics of\nStat 101 student preparation (pre-course performance on concept inventory and\nmath ACT score) may or may not be associated with end of course student\nperformance on conceptual inventories. Students across all preparation levels\ntended to show improvement in Stat 101, but more improvement was observed\nacross all student preparation levels in early versions of a SBI course.\nFurthermore, students' gains tended to be similar regardless of whether\nstudents entered the course with more preparation or less. Recent data on a\nsample of students using a current version of an SBI course showed similar\nresults, though direct comparison with non-SBI students was not possible.\nOverall, our analysis provides additional evidence that SBI curricula are\neffective at improving students' conceptual understanding of statistical ideas\npost-course regardless student preparation. Further work is needed to better\nunderstand nuances of student improvement based on other student demographics,\nprior coursework, as well as instructor and institutional variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09455v1"
    },
    {
        "title": "Theory of Cricket: Target Scores and Predictability",
        "authors": [
            "Robin de Regt",
            "Ravinder Kumar"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  We propose a model for recalculating the target score in rain affected\nmatches based on empirical data. During the development of the current stage of\nthe Cricket, different methods have been introduced to recalculate the target\nscores in interpreted games. Currently, the International Cricket Council (ICC)\nuses the Duckworth-Lewis method and have in the past strongly considered\nchanging to the VJD method. Here, we introduce a simple approach to calculate\ntarget scores in interrupted games by considering the area under a run rate\ncurve. To calculate the target we have analysed over a decades worth of\nempirical data using various statistical methods. As in the case of Duckworth-\nLewis method, we also have two parameters in our model, that is overs and\nwickets in combination. We also found that in the one day international cricket\n(ODI) wickets play a crucial role whereas in T20 cricket they do not effect the\nrun rate of the games to the same degree. Using empirical and mathematical\narguments we show that the run scoring distributions are independent of the\ninnings.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.02229v1"
    },
    {
        "title": "Resolving the Lord's Paradox",
        "authors": [
            "Priyantha Wijayatunga"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  An explanation to Lord's paradox using ordinary least square regression\nmodels is given. It is not a paradox at all, if the regression parameters are\ninterpreted as predictive or as causal with stricter conditions and be aware of\nlaws of averages. We use derivation of a super-model from a given sub-model,\nwhen its residuals can be modelled with other potential predictors as a\nsolution.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07923v1"
    },
    {
        "title": "Viewing Simpson's Paradox",
        "authors": [
            "Priyantha Wijayatunga"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Well known Simpson's paradox is puzzling and surprising for many, especially\nfor the empirical researchers and users of statistics. However there is no\nsurprise as far as mathematical details are concerned. A lot more is written\nabout the paradox but most of them are beyond the grasp of such users. This\nshort article is about explaining the phenomenon in an easy way to grasp using\nsimple algebra and geometry. The mathematical conditions under which the\nparadox can occur are made explicit and a simple geometrical illustrations is\nused to describe it. We consider the reversal of the association between two\nbinary variables, say, $X$ and $Y$ by a third binary variable, say, $Z$. We\nshow that it is always possible to define $Z$ algebraically for non-extreme\ndependence between $X$ and $Y$, therefore occurrence of the paradox depends on\nidentifying it with a practical meaning for it in a given context of interest,\nthat is up to the subject domain expert. And finally we discuss the paradox in\npredictive contexts since in literature it is argued that the paradox is\nresolved using causal reasoning.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.07940v1"
    },
    {
        "title": "BNSP: an R Package for Fitting Bayesian Semiparametric Regression Models\n  and Variable Selection",
        "authors": [
            "Georgios Papageorgiou"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  The R package BNSP provides a unified framework for semiparametric\nlocation-scale regression and stochastic search variable selection. The\nstatistical methodology that the package is built upon utilizes basis function\nexpansions to represent semiparametric covariate effects in the mean and\nvariance functions, and spike-slab priors to perform selection and\nregularization of the estimated effects. In addition to the main function that\nperforms posterior sampling, the package includes functions for assessing\nconvergence of the sampler, summarizing model fits, visualizing covariate\neffects and obtaining predictions for new responses or their means given\nfeature/covariate vectors.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10939v4"
    },
    {
        "title": "Conjectures on Optimal Nested Generalized Group Testing Algorithm",
        "authors": [
            "Yaakov Malinovsky"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Consider a finite population of $N$ items, where item $i$ has a probability\n$p_i$ to be defective. The goal is to identify all items by means of group\ntesting. This is the generalized group testing problem (hereafter GGTP). In the\ncase of $\\displaystyle p_1=\\cdots=p_{N}=p$ \\cite{YH1990} proved that the\npairwise testing algorithm is the optimal nested algorithm, with respect to the\nexpected number of tests, for all $N$ if and only if $\\displaystyle p \\in\n[1-1/\\sqrt{2},\\,(3-\\sqrt{5})/2]$ (R-range hereafter) (an optimal at the\nboundary values). In this note, we present a result that helps to define the\ngeneralized pairwise testing algorithm (hereafter GPTA) for the GGTP. We\npresent two conjectures: (1) when all $p_i, i=1,\\ldots,N$ belong to the\nR-range, GPTA is the optimal procedure among nested procedures applied to $p_i$\nof nondecreasing order; (2) if all $p_i, i=1,\\ldots,N$ belong to the R-range,\nGPTA the optimal nested procedure, i.e., minimises the expected total number of\ntests with respect to all possible testing orders in the class of nested\nprocedures. Although these conjectures are logically reasonable, we were only\nable to empirically verify the first one up to a particular level of $N$. We\nalso provide a short survey of GGTP.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.01345v3"
    },
    {
        "title": "On estimands and the analysis of adverse events in the presence of\n  varying follow-up times within the benefit assessment of therapies",
        "authors": [
            "Steffen Unkel",
            "Marjan Amiri",
            "Norbert Benda",
            "Jan Beyersmann",
            "Dietrich Knoerzer",
            "Katrin Kupas",
            "Frank Langer",
            "Friedhelm Leverkus",
            "Anja Loos",
            "Claudia Ose",
            "Tanja Proctor",
            "Claudia Schmoor",
            "Carsten Schwenke",
            "Guido Skipka",
            "Kristina Unnebrink",
            "Florian Voss",
            "Tim Friede"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  The analysis of adverse events (AEs) is a key component in the assessment of\na drug's safety profile. Inappropriate analysis methods may result in\nmisleading conclusions about a therapy's safety and consequently its\nbenefit-risk ratio. The statistical analysis of AEs is complicated by the fact\nthat the follow-up times can vary between the patients included in a clinical\ntrial. This paper takes as its focus the analysis of AE data in the presence of\nvarying follow-up times within the benefit assessment of therapeutic\ninterventions. Instead of approaching this issue directly and solely from an\nanalysis point of view, we first discuss what should be estimated in the\ncontext of safety data, leading to the concept of estimands. Although the\ncurrent discussion on estimands is mainly related to efficacy evaluation, the\nconcept is applicable to safety endpoints as well. Within the framework of\nestimands, we present statistical methods for analysing AEs with the focus\nbeing on the time to the occurrence of the first AE of a specific type. We give\nrecommendations which estimators should be used for the estimands described.\nFurthermore, we state practical implications of the analysis of AEs in clinical\ntrials and give an overview of examples across different indications. We also\nprovide a review of current practices of health technology assessment (HTA)\nagencies with respect to the evaluation of safety data. Finally, we describe\nproblems with meta-analyses of AE data and sketch possible solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.01834v2"
    },
    {
        "title": "Hyperspectral Data Analysis in R: the hsdar Package",
        "authors": [
            "Lukas W. Lehnert",
            "Hanna Meyer",
            "Wolfgang A. Obermeier",
            "Brenner Silva",
            "Bianca Regeling",
            "Jörg Bendix"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Hyperspectral remote sensing is a promising tool for a variety of\napplications including ecology, geology, analytical chemistry and medical\nresearch. This article presents the new \\hsdar package for R statistical\nsoftware, which performs a variety of analysis steps taken during a typical\nhyperspectral remote sensing approach. The package introduces a new class for\nefficiently storing large hyperspectral datasets such as hyperspectral cubes\nwithin R. The package includes several important hyperspectral analysis tools\nsuch as continuum removal, normalized ratio indices and integrates two widely\nused radiation transfer models. In addition, the package provides methods to\ndirectly use the functionality of the caret package for machine learning tasks.\nTwo case studies demonstrate the package's range of functionality: First, plant\nleaf chlorophyll content is estimated and second, cancer in the human larynx is\ndetected from hyperspectral data.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.05090v1"
    },
    {
        "title": "To Bayes or Not To Bayes? That's no longer the question!",
        "authors": [
            "Ernest Fokoue"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  This paper seeks to provide a thorough account of the ubiquitous nature of\nthe Bayesian paradigm in modern statistics, data science and artificial\nintelligence. Once maligned, on the one hand by those who philosophically hated\nthe very idea of subjective probability used in prior specification, and on the\nother hand because of the intractability of the computations needed for\nBayesian estimation and inference, the Bayesian school of thought now permeates\nand pervades virtually all areas of science, applied science, engineering,\nsocial science and even liberal arts, often in unsuspected ways. Thanks in part\nto the availability of powerful computing resources, but also to the literally\nunavoidable inherent presence of the quintessential building blocks of the\nBayesian paradigm in all walks of life, the Bayesian way of handling\nstatistical learning, estimation and inference is not only mainstream but also\nbecoming the most central approach to learning from the data. This paper\nexplores some of the most relevant elements to help to the reader appreciate\nthe pervading power and presence of the Bayesian paradigm in statistics,\nartificial intelligence and data science, with an emphasis on how the Gospel\naccording to Reverend Thomas Bayes has turned out to be the truly good news,\nand some cases the amazing saving grace, for all who seek to learn\nstatistically from the data. To further help the reader gain deeper and\ntangible practical insights into the Bayesian machinery, we point to some\ncomputational tools designed for the R Statistical Software Environment to help\nexplore Bayesian statistical learning.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11012v1"
    },
    {
        "title": "Absolutely Zero Evidence",
        "authors": [
            "Veronica J. Vieland"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Statistical analysis is often used to evaluate the evidence for or against\nscientific hypotheses, and various statistics (e.g., p-values, likelihood\nratios, Bayes factors) are interpreted as measures of evidence strength. Here I\nconsider evidence measurement from the point of view of representational\nmeasurement theory, and argue that familiar evidence statistics do not conform\nto any legitimate measurement scale type. I then consider the notion of an\nabsolute scale for evidence measurement, in a sense to be defined, focusing\nparticularly on the notion of absolute 0 evidence, which turns out to be\nsomething other than what one might have expected.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.11516v1"
    },
    {
        "title": "Canadian Crime Rates in the Penalty Box",
        "authors": [
            "Simon Demers"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Over the 1962 to 2016 period, the Canadian violent crime rate has remained\nstrongly correlated with National Hockey League (NHL) penalties. The Canadian\nproperty crime rate was similarly correlated with stolen base attempts in the\nMajor League Baseball (MLB). Of course, correlation does not imply causation or\nprove association. It is simply presented here as an observation. Curious\nreaders might be tempted to conduct additional research and ask questions in\norder to enhance the conversation, transition away from a state of confusion,\nclarify the situation, prevent false attribution, and possibly solve a problem\nthat economists call identification.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.05118v3"
    },
    {
        "title": "I can see clearly now: reinterpreting statistical significance",
        "authors": [
            "Jonathan Dushoff",
            "Morgan P. Kain",
            "Benjamin M. Bolker"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Null hypothesis significance testing remains popular despite decades of\nconcern about misuse and misinterpretation. We believe that much of the problem\nis due to language: significance testing has little to do with other meanings\nof the word \"significance\". Despite the limitations of null-hypothesis tests,\nwe argue here that they remain useful in many contexts as a guide to whether a\ncertain effect can be seen clearly in that context (e.g. whether we can clearly\nsee that a correlation or between-group difference is positive or negative). We\ntherefore suggest that researchers describe the conclusions of null-hypothesis\ntests in terms of statistical \"clarity\" rather than statistical \"significance\".\nThis simple semantic change could substantially enhance clarity in statistical\ncommunication.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.06387v1"
    },
    {
        "title": "Complementary Lipschitz continuity results for the distribution of\n  intersections or unions of independent random sets in finite discrete spaces",
        "authors": [
            "John Klein"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  We prove that intersections and unions of independent random sets in finite\nspaces achieve a form of Lipschitz continuity. More precisely, given the\ndistribution of a random set $\\Xi$, the function mapping any random set\ndistribution to the distribution of its intersection (under independence\nassumption) with $\\Xi$ is Lipschitz continuous with unit Lipschitz constant if\nthe space of random set distributions is endowed with a metric defined as the\n$L_k$ norm distance between inclusion functionals also known as commonalities.\nMoreover, the function mapping any random set distribution to the distribution\nof its union (under independence assumption) with $\\Xi$ is Lipschitz continuous\nwith unit Lipschitz constant if the space of random set distributions is\nendowed with a metric defined as the $L_k$ norm distance between hitting\nfunctionals also known as plausibilities.\n  Using the epistemic random set interpretation of belief functions, we also\ndiscuss the ability of these distances to yield conflict measures. All the\nproofs in this paper are derived in the framework of Dempster-Shafer belief\nfunctions. Let alone the discussion on conflict measures, it is straightforward\nto transcribe the proofs into the general (non necessarily epistemic) random\nset terminology.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.10859v2"
    },
    {
        "title": "Helix modelling through the Mardia-Holmes model framework and an\n  extension of the Mardia-Holmes model",
        "authors": [
            "Mai F Alfahad",
            "John T Kent",
            "Kanti V Mardia"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  For noisy two-dimensional data, which are approximately uniformly distributed\nnear the circumference of an ellipse, Mardia and Holmes (1980) developed a\nmodel to fit the ellipse. In this paper we adapt their methodology to the\nanalysis of helix data in three dimensions. If the helix axis is known, then\nthe Mardia-Holmes model for the circular case can be fitted after projecting\nthe helix data onto the plane normal to the helix axis. If the axis is unknown,\nan iterative algorithm has been developed to estimate the axis. The methodology\nis illustrated using simulated protein alpha-helices. We also give a\nmultivariate version of the Mardia-Holmes model which will be applicable for\nfitting an ellipsoid and in particular a cylinder.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.10946v1"
    },
    {
        "title": "On a generalised form of subjective probability",
        "authors": [
            "Russell J. Bowater"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  This paper is motivated by the questions of how to give the concept of\nprobability an adequate real-world meaning, and how to explain a certain type\nof phenomenon that can be found, for instance, in Ellsberg's paradox. It\nattempts to answer these questions by constructing an alternative theory to one\nthat was proposed in earlier papers on the basis of various important\ncriticisms that were raised against this earlier theory. The conceptual\nprinciples of the corresponding definition of probability are laid out and\nexplained in detail. In particular, what is required to fully specify a\nprobability distribution under this definition is not just the distribution\nfunction of the variable concerned, but also an assessment of the internal\nand/or the external strength of this function relative to other distribution\nfunctions of interest. This way of defining probability is applied to various\nexamples and problems including, perhaps most notably, to a long-running\ncontroversy concerning the distinction between Bayesian and fiducial inference.\nThe characteristics of this definition of probability are carefully evaluated\nin terms of the issues that it sets out to address.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.10972v3"
    },
    {
        "title": "Rapid Prototyping Model for Healthcare Alternative Payment Models:\n  Replicating the Federally Qualified Health Center Advanced Primary Care\n  Practice Demonstration",
        "authors": [
            "Jarrod Olson",
            "Amir Rahimi",
            "Po Hsu Allen Chen",
            "J. Elizabeth Jackson",
            "Tyler Coy",
            "Adrienne Cocci",
            "Nancy McMillan",
            "Jeff Geppert"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Innovation in healthcare payment and service delivery utilizes high cost,\nhigh risk pilots paired with traditional program evaluations. Decision-makers\nare unable to reliably forecast the impacts of pilot interventions in this\ncomplex system, complicating the feasibility assessment of proposed healthcare\nmodels. We developed and validated a Discrete Event Simulation (DES) model of\nprimary care for patients with Diabetes to allow rapid prototyping and\nassessment of models before pilot implementation. We replicated four outcomes\nfrom the Centers for Medicare and Medicaid Services Federally Qualified Health\nCenter Advanced Primary Care Practice pilot. The DES model simulates a\nsynthetic population's healthcare experience, including symptom onset,\nappointment scheduling, screening, and treatment, as well as the impact of\nphysician training. A network of detailed event modules was developed from\npeer-reviewed literature. Synthetic patients' attributes modify the probability\ndistributions for event outputs and direct them through an episode of care;\nattributes are in turn modified by patients' experiences. Our model replicates\nthe direction of the effect of physician training on the selected outcomes, and\nthe strength of the effect increases with the number of trainings. The\nsimulated effect strength replicates the pilot results for eye exams and\nnephropathy screening, but over-estimates results for HbA1c and LDL screening.\nOur model will improve decision-makers' abilities to assess the feasibility of\npilot success, with reproducible, literature-based systems models. Our model\nidentifies intervention and healthcare system components to which outcomes are\nsensitive, so these aspects can be monitored and controlled during pilot\nimplementation. More work is needed to improve replication of HbA1c and LDL\nscreening, and to elaborate sub-models related to intervention components.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.03940v1"
    },
    {
        "title": "Pragmatic hypotheses in the evolution of science",
        "authors": [
            "Luis G. Esteves",
            "Rafael Izbicki",
            "Rafael B. Stern",
            "Julio M. Stern"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  This paper introduces pragmatic hypotheses and relates this concept to the\nspiral of scientific evolution. Previous works determined a characterization of\nlogically consistent statistical hypothesis tests and showed that the modal\noperators obtained from this test can be represented in the hexagon of\noppositions. However, despite the importance of precise hypothesis in science,\nthey cannot be accepted by logically consistent tests. Here, we show that this\ndilemma can be overcome by the use of pragmatic versions of precise hypotheses.\nThese pragmatic versions allow a level of imprecision in the hypothesis that is\nsmall relative to other experimental conditions. The introduction of pragmatic\nhypotheses allows the evolution of scientific theories based on statistical\nhypothesis testing to be interpreted using the narratological structure of\nhexagonal spirals, as defined by Pierre Gallais.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.09998v1"
    },
    {
        "title": "Application of Robust Estimators in Shewhart S-Charts",
        "authors": [
            "Burak Alakent",
            "Ece C. Mutlu"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Maintaining the quality of manufactured products at a desired level is known\nto increase customer satisfaction and profitability. Shewhart control chart is\nthe most widely used in statistical process control (SPC) technique to monitor\nthe quality of products and control process variability. Based on the\nassumption of independent and normally distributed data sets, sample mean and\nstandard deviation statistics are known to be the most efficient conventional\nestimators to determine the process location and scale, respectively. On the\nother hand, there is not guarantee that the real-world process data would be\nnormally distributed: outliers may exist, and/or sampled population may be\ncontaminated. In such cases, efficiency of the conventional estimators is\nsignificantly reduced, and power of the Shewhart charts may be undesirably low,\ne.g. occasional outliers in the rational subgroups (Phase I dataset) may\ndrastically affect the sample mean and standard deviation, resulting a serious\ndelay in detection of inferior products (Phase II procedure). For more\nefficient analyses, it is required to use robust estimators against\ncontaminations. Consequently, it is determined that robust estimators are more\nefficient both against diffuse localized and symmetric-asymmetric\ncontaminations, and have higher power in detecting disturbances, compared to\nconventional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.11132v1"
    },
    {
        "title": "Comparison of plotting system outputs in beginner analysts",
        "authors": [
            "Leslie Myint",
            "Aboozar Hadavand",
            "Leah Jager",
            "Jeffrey Leek"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The R programming language is built on an ecosystem of packages, some that\nallow analysts to accomplish the same tasks. For example, there are at least\ntwo clear workflows for creating data visualizations in R: using the base\ngraphics package (referred to as \"base R\") and the ggplot2 add-on package based\non the grammar of graphics. Here we perform an empirical study of the quality\nof scientific graphics produced by beginning R users. In our experiment,\nlearners taking a data science course on the Coursera platform were randomized\nto complete identical plotting exercises in either the base R or the ggplot2\nsystem. Learners were then asked to evaluate their peers in terms of visual\ncharacteristics key to scientific cognition. We observed that graphics created\nwith the two systems rated similarly on many characteristics. However, ggplot2\ngraphics were generally judged to be more visually pleasing and, in the case of\nfaceted scientific plots, easier to understand. Our results suggest that while\nboth graphic systems are useful in the hands of beginning users, ggplot2's\nnatural faceting system may be easier to use by beginning users for displaying\nmore complex relationships.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.01829v1"
    },
    {
        "title": "Synthesis of High-Resolution Load Profiles with Minimal Data",
        "authors": [
            "Thomas Schnake",
            "David Bauer"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  For the estimation of a new energy supply system it is an important to have\nhigh-resolution energy load profile. Such a profile is in general either not\npresent or very costly to obtain. We will therefore present a method which\nsynthesizes load profiles from minimal given data, but with maximal resolution.\nThe general initial data setting includes month integrals and load profiles a\nfew days. The resulting time series features all important properties to\nrepresent a real energy profile.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.06625v1"
    },
    {
        "title": "Three issues impeding communication of statistical methodology for\n  incomplete data",
        "authors": [
            "John C. Galati"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  We identify three issues permeating the literature on statistical methodology\nfor incomplete data written for non-specialist statisticians and other\ninvestigators. The first is a mathematical defect in the notation Yobs, Ymis\nused to partition the data into observed and missing components. The second are\nissues concerning the notation `P(R|Yobs, Ymis)=P(R|Yobs)' used for\ncommunicating the definition of missing at random (MAR). And the third is the\nframing of ignorability by emulating complete-data methods exactly, rather than\ntreating the question of ignorability on its own merits. These issues have been\npresent in the literature for a long time, and have simple remedies. The\npurpose of this paper is to raise awareness of these issues, and to explain how\nthey can be remedied.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.08880v4"
    },
    {
        "title": "Modeling the Health Expenditure in Japan, 2011. A Healthy Life Years\n  Lost Methodology",
        "authors": [
            "Christos H Skiadas",
            "Charilaos Skiadas"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The Healthy Life Years Lost Methodology (HLYL) is introduced to model and\nestimate the Health Expenditure in Japan in 2011. The HLYL theory and\nestimation methods are presented in our books in the Springer Series on\nDemographic Methods and Population Analysis vol. 45 and 46 titled: Exploring\nthe Health State of a Population by Dynamic Modeling Methods and Demography and\nHealth Issues: Population Aging, Mortality and Data Analysis. Special\napplications appear in Chapters of these books as in The Health-Mortality\nApproach in Estimating the Healthy Life Years Lost Compared to the Global\nBurden of Disease Studies and Applications in World, USA and Japan and in\nEstimation of the Healthy Life Expectancy in Italy Through a Simple Model Based\non Mortality Rate by Skiadas and Arezzo. Here further to present the main part\nof the methodology with more details and illustrations, we develop and extend a\nlife table important to estimate the healthy life years lost along with the\nfitting to the health expenditure in the related case. The application results\nare quite promising and important to support decision makers and health\nagencies with a powerful tool to improve the health expenditure allocation and\nthe future predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11565v1"
    },
    {
        "title": "Randomization and Fair Judgment in Law and Science",
        "authors": [
            "Julio Michael Stern",
            "Marcos Antonio Simplicio",
            "Marcos Vinicius M. Silva",
            "Roberto A. Castellanos Pfeiffer"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Randomization procedures are used in legal and statistical applications,\naiming to shield important decisions from spurious influences. This article\ngives an intuitive introduction to randomization and examines some intended\nconsequences of its use related to truthful statistical inference and fair\nlegal judgment. This article also presents an open-code Java implementation for\na cryptographically secure, statistically reliable, transparent, traceable, and\nfully auditable randomization tool.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06709v4"
    },
    {
        "title": "The use of multiple models within an organisation",
        "authors": [
            "Chris J Dent",
            "Michael Goldstein",
            "Andrew Wright",
            "Henry P. Wynn"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Organisations, whether in government, industry or commerce, are required to\nmake decisions in a complex and uncertain environment. The way models are used\nis intimately connected to the way organisations make decisions and the context\nin which they make them. Typically, in a complex organisation, multiple related\nmodels will often be used in support of a decision. For example, engineering\nmodels might be combined with financial models and macro-economic models in\norder to decide whether to invest in new production capability. Different parts\nof a complex organisation might operate their own related models which might\nthen be presented to a central decision maker. Yet in practice, there is little\nawareness of the practical challenges of using models in a robust way to\nsupport decision making. There is significant scope to improve decision making\nthough an enhanced understanding of the role and limitations of modelling and\nthrough the application of cutting edge methodologies and organisational best\npractice. This report is in the form of a 'white paper', whose purpose is to\nidentify key issues for consideration whist postulating tentative approaches to\nthese issues that might be worthy of further exploration, focussing on both\ntechnical and organisational aspects. It begins with a framework for\nconsideration of how model-based decisions are made in organisations. It then\nlooks more closely at the questions of uncertainty and multiple models. It then\npostulates some technical statistical and organisational approaches for\nmanaging some of these issues. Finally, it considers the way forward, and the\npossible focus for further work.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.11813v1"
    },
    {
        "title": "Revealing the Beauty behind the Sleeping Beauty Problem",
        "authors": [
            "Ioannis Mariolis"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  A large number of essays address the Sleeping Beauty problem, which\nundermines the validity of Bayesian inference and Bas Van Fraassen's\n'Reflection Principle'. In this study a straightforward analysis of the problem\nbased on probability theory is presented. The key difference from previous\nworks is that apart from the random experiment imposed by the problem's\ndescription, a different one is also considered, in order to negate the\nconfusion on the involved conditional probabilities. The results of the\nanalysis indicate that no inconsistency takes place, whereas both Bayesian\ninference and 'Reflection Principle' are valid.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.3803v1"
    },
    {
        "title": "Tree Oriented Data Analysis",
        "authors": [
            "Sean Skwerer"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Complex data objects arise in many areas of modern science including\nevolutionary biology, nueroscience, dynamics of gene expression and medical\nimaging. Object oriented data analysis (OODA) is the statistical analysis of\ndatasets of complex objects. Data analysis of tree data objects is an exciting\nresearch area with interesting questions and challenging problems. This thesis\nfocuses on tree oriented statistical methodologies, and algorithms for solving\nrelated mathematical optimization problems.\n  This research is motivated by the goal of analyzing a data set of images of\nhuman brain arteries. The approach we take here is to use a novel\nrepresentation of brain artery systems as points in phylogenetic treespace. The\ntreespace property of unique global geodesics leads to a notion of geometric\ncenter called a Fr\\'echet mean. For a sample of data points, the Fr\\'echet\nfunction is the sum of squared distances from a point to the data points, and\nthe Fr\\'echet mean is the minimizer of the Fr\\'echet function.\n  In this thesis we use properties of the Fr\\'echet function to develop an\nalgorithmic system for computing Fr\\'echet means. Properties of the Fr\\'echet\nfunction are also used to show a sticky law of large numbers which describes a\nsurprising stability of the topological tree structure of sample Fr\\'echet\nmeans at that of the population Fr\\'echet mean. We also introduce\nnon-parametric regression of brain artery tree structure as a response variable\nto age based on weighted Fr\\'echet means.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.5501v2"
    },
    {
        "title": "Coherent combination of probabilistic outputs for group decision making:\n  an algebraic approach",
        "authors": [
            "Manuele Leonelli",
            "Eva Riccomagno",
            "Jim Q. Smith"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Current decision support systems address domains that are heterogeneous in\nnature and becoming progressively larger. Such systems often require the input\nof expert judgement about a variety of different fields and an intensive\ncomputational power to produce the scores necessary to rank the available\npolicies. Recently, integrating decision support systems have been introduced\nto enable a formal Bayesian multi-agent decision analysis to be distributed and\nconsequently efficient. In such systems, where different panels of experts\noversee disjoint but correlated vectors of variables, each expert group needs\nto deliver only certain summaries of the variables under their jurisdiction to\nproperly derive an overall score for the available policies. Here we present an\nalgebraic approach that makes this methodology feasible for a wide range of\nmodelling contexts and that enables us to identify the summaries needed for\nsuch a combination of judgements. We are also able to demonstrate that\ncoherence, in a sense we formalize here, is still guaranteed when panels only\nshare a partial specification of their model with other panel members. We\nillustrate this algebraic approach by applying it to a specific class of\nBayesian networks and demonstrate how we can use it to derive closed form\nformulae for the computations of the joint moments of variables that determine\nthe score of different policies.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02070v1"
    },
    {
        "title": "Estimation of Inverse Weibull Distribution Under Type-I Hybrid Censoring",
        "authors": [
            "Mohammad Kazemi",
            "Mina Azizpour"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  The hybrid censoring is a mixture of Type I and Type II censoring schemes.\nThis paper presents the statistical inferences of the Inverse Weibull\ndistribution when the data are Type-I hybrid censored. First we consider the\nmaximum likelihood estimators of the unknown parameters. It is observed that\nthe maximum likelihood estimators can not be obtained in closed form. We\nfurther obtain the Bayes estimators and the corresponding highest posterior\ndensity credible intervals of the unknown parameters under the assumption of\nindependent gamma priors using the importance sampling procedure. We also\ncompute the approximate Bayes estimators using Lindley's approximation\ntechnique. We have performed a simulation study and a real data analysis in\norder to compare the proposed Bayes estimators with the maximum likelihood\nestimators.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05769v2"
    },
    {
        "title": "Estimation of P(X > Y ) for Weibull distribution based on hybrid\n  censored samples",
        "authors": [
            "Akbar Asgharzadeh",
            "Mohammad Kazemi",
            "Debasis Kundu"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  A Hybrid censoring scheme is mixture of Type-I and Type-II censoring schemes.\nBased on hybrid censored samples, this paper deals with the in- ference on R =\nP(X > Y ), when X and Y are two independent Weibull distributions with\ndifferent scale parameters, but having the same shape pa- rameter. The maximum\nlikelihood estimator (MLE), and the approximate MLE (AMLE) of R are obtained.\nThe asymptotic distribution of the maxi- mum likelihood estimator of R is\nobtained. Based on the asymptotic distribu- tion, the confidence interval of R\ncan be derived. Two bootstrap confidence intervals are also proposed. We\nconsider the Bayesian estimate of R, and propose the corresponding credible\ninterval for R. Monte Carlo simulations are performed to compare the different\nproposed methods. Analysis of a real data set has also been presented for\nillustrative purposes.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.05804v1"
    },
    {
        "title": "Cognitive Transfer Outcomes for a Simulation-Based Introductory\n  Statistics Curriculum",
        "authors": [
            "Matthew D. Beckman",
            "Robert C. delMas",
            "Joan Garfield"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Cognitive transfer is the ability to apply learned skills and knowledge to\nnew applications and contexts. This investigation evaluates cognitive transfer\noutcomes for a tertiary-level introductory statistics course using the CATALST\ncurriculum, which exclusively used simulation-based methods to develop\nfoundations of statistical inference. A common assessment instrument\nadministered at the end of each course measured learning outcomes for students.\nCATALST students showed evidence of both near and far transfer outcomes while\nscoring as high, or higher on the assessed learning objectives, when compared\nwith peers enrolled in similar courses that emphasized parametric inferential\nmethods (e.g. the t-test).\n",
        "pdf_link": "http://arxiv.org/pdf/1707.06537v1"
    },
    {
        "title": "We are not alone ! (at least, most of us). Homonymy in large scale\n  social groups",
        "authors": [
            "Arthur Charpentier",
            "Baptiste Coulmont"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  This article brings forward an estimation of the proportion of homonyms in\nlarge scale groups based on the distribution of first names and last names in a\nsubset of these groups. The estimation is based on the generalization of the\n\"birthday paradox problem\". The main results is that, in societies such as\nFrance or the United States, identity collisions (based on first + last names)\nare frequent. The large majority of the population has at least one homonym.\nBut in smaller settings, it is much less frequent : even if small groups of a\nfew thousand people have at least one couple of homonyms, only a few\nindividuals have an homonym.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07607v1"
    },
    {
        "title": "A Case Study of Promoting Informal Inferential Reasoning in Learning\n  Sampling Distribution for High School Students",
        "authors": [
            "Geovani Debby Setyani",
            "Yosep Dwi Kristanto"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Drawing inference from data is an important skill for students to understand\ntheir everyday life, so that the sampling distribution as a central topic in\nstatistical inference is necessary to be learned by the students. However,\nlittle is known about how to teach the topic for high school students,\nespecially in Indonesian context. Therefore, the present study provides a\nteaching experiment to support the students' informal inferential reasoning in\nunderstanding the sampling distribution, as well as the students' perceptions\ntoward the teaching experiment. The subjects in the present study were three\n11th-grader of one private school in Yogyakarta majoring in mathematics and\nnatural science. The method of data collection was direct observation of\nsampling distribution learning process, interviews, and documentation. The\npresent study found that that informal inferential reasoning with problem-based\nlearning using contextual problems and real data could support the students to\nunderstand the sampling distribution, and they also gave positive responses\nabout their learning experience.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04384v1"
    },
    {
        "title": "Online Statistics Teaching and Learning",
        "authors": [
            "Jim Albert",
            "Mine Cetinkaya-Rundel",
            "Jingchen Hu"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  For statistics courses at all levels, teaching and learning online poses\nchallenges in different aspects. Particular online challenges include how to\neffectively and interactively conduct exploratory data analyses, how to\nincorporate statistical programming, how to include individual or team\nprojects, and how to present mathematical derivations efficiently and\neffectively.\n  This article draws from the authors' experience with seven different online\nstatistics courses to address some of the aforementioned challenges. One course\nis an online exploratory data analysis course taught at Bowling Green State\nUniversity. A second course is an upper level Bayesian statistics course taught\nat Vassar College and shared among 10 liberal arts colleges through a hybrid\nmodel. We alo describes a five-course MOOC specialization on Coursera, offered\nby Duke University.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09700v1"
    },
    {
        "title": "A Conversation with Donald B. Rubin",
        "authors": [
            "Fan Li",
            "Fabrizia Mealli"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Donald Bruce Rubin is John L. Loeb Professor of Statistics at Harvard\nUniversity. He has made fundamental contributions to statistical methods for\nmissing data, causal inference, survey sampling, Bayesian inference, computing\nand applications to a wide range of disciplines, including psychology,\neducation, policy, law, economics, epidemiology, public health and other social\nand biomedical sciences.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.1789v3"
    },
    {
        "title": "Validating Sample Average Approximation Solutions with Negatively\n  Dependent Batches",
        "authors": [
            "Jiajie Chen",
            "Cong Han Lim",
            "Peter Z. G. Qian",
            "Jeff Linderoth",
            "Stephen J. Wright"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Sample-average approximations (SAA) are a practical means of finding\napproximate solutions of stochastic programming problems involving an extremely\nlarge (or infinite) number of scenarios. SAA can also be used to find estimates\nof a lower bound on the optimal objective value of the true problem which, when\ncoupled with an upper bound, provides confidence intervals for the true optimal\nobjective value and valuable information about the quality of the approximate\nsolutions. Specifically, the lower bound can be estimated by solving multiple\nSAA problems (each obtained using a particular sampling method) and averaging\nthe obtained objective values. State-of-the-art methods for lower-bound\nestimation generate batches of scenarios for the SAA problems independently. In\nthis paper, we describe sampling methods that produce negatively dependent\nbatches, thus reducing the variance of the sample-averaged lower bound\nestimator and increasing its usefulness in defining a confidence interval for\nthe optimal objective value. We provide conditions under which the new sampling\nmethods can reduce the variance of the lower bound estimator, and present\ncomputational results to verify that our scheme can reduce the variance\nsignificantly, by comparison with the traditional Latin hypercube approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.7208v2"
    },
    {
        "title": "The Two-envelope Problem: An Informed Choice",
        "authors": [
            "Jeffrey Brian Tyler"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The host of a game presents two indistinguishable envelopes to an agent. One\nof the envelopes is randomly selected and allocated to the agent. The agent is\ninformed that the monetary content of one of the envelopes is twice that of the\nother. The dilemma is under which conditions it would be beneficial to switch\nthe allocated envelope for the complementary one. The objective of his or her\nenvelope-switching strategy is to determine the benefit of switching the\nallocated envelope and its content for the expected content of the\ncomplementary envelope.\n  The agent, upon revealing the content of the allocated envelope, must\nconsider the events that are likely to have taken place as a result of the\nhost's activities. The preceding approach is in stark contrast to considering\nthe agent's reasoning for a particular outcome that seeks to derive a strategy\nbased on the relative contents of the presented envelopes. However, it is the\nformer reasoning that seeks to identify what the initial amounts could have\nbeen, as a result of the observed amount, that facilitates the identification\nof an appropriate switching strategy.\n  Knowledge of the content and allocation process is essential for the agent to\nderive a successful switching strategy, as is the distribution function from\nwhich the host sampled the initial amount that is assigned to the first\nenvelope.\n  For every play of the game, once the agent is afforded the opportunity of\nsighting the content of the randomly allocated envelope, he or she can\ndetermine the expected benefit of switching.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.04570v1"
    },
    {
        "title": "PUMA criterion = MODE criterion",
        "authors": [
            "Dave Zachariah",
            "Petre Stoica",
            "Magnus Jansson"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  We show that the recently proposed (enhanced) PUMA estimator for array\nprocessing minimizes the same criterion function as the well-established MODE\nestimator. (PUMA = principal-singular-vector utilization for modal analysis,\nMODE = method of direction estimation.)\n",
        "pdf_link": "http://arxiv.org/pdf/1701.04583v1"
    },
    {
        "title": "HyperTools: A Python toolbox for visualizing and manipulating\n  high-dimensional data",
        "authors": [
            "Andrew C. Heusser",
            "Kirsten Ziman",
            "Lucy L. W. Owen",
            "Jeremy R. Manning"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Data visualizations can reveal trends and patterns that are not otherwise\nobvious from the raw data or summary statistics. While visualizing\nlow-dimensional data is relatively straightforward (for example, plotting the\nchange in a variable over time as (x,y) coordinates on a graph), it is not\nalways obvious how to visualize high-dimensional datasets in a similarly\nintuitive way. Here we present HypeTools, a Python toolbox for visualizing and\nmanipulating large, high-dimensional datasets. Our primary approach is to use\ndimensionality reduction techniques (Pearson, 1901; Tipping & Bishop, 1999) to\nembed high-dimensional datasets in a lower-dimensional space, and plot the data\nusing a simple (yet powerful) API with many options for data manipulation [e.g.\nhyperalignment (Haxby et al., 2011), clustering, normalizing, etc.] and plot\nstyling. The toolbox is designed around the notion of data trajectories and\npoint clouds. Just as the position of an object moving through space can be\nvisualized as a 3D trajectory, HyperTools uses dimensionality reduction\nalgorithms to create similar 2D and 3D trajectories for time series of\nhigh-dimensional observations. The trajectories may be plotted as interactive\nstatic plots or visualized as animations. These same dimensionality reduction\nand alignment algorithms can also reveal structure in static datasets (e.g.\ncollections of observations or attributes). We present several examples\nshowcasing how using our toolbox to explore data through trajectories and\nlow-dimensional embeddings can reveal deep insights into datasets across a wide\nvariety of domains.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.08290v1"
    },
    {
        "title": "Using a \"Study of Studies\" to help statistics students assess research\n  findings",
        "authors": [
            "Azka Javaid",
            "Xiaofei Wang",
            "Nicholas J Horton"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  One learning goal of the introductory statistics course is to develop the\nability to make sense of research findings in published papers. The Atlantic\nmagazine regularly publishes a feature called \"Study of Studies\" that\nsummarizes multiple articles published in a particular domain. We describe a\nclassroom activity to develop this capacity using the \"Study of Studies.\" In\nthis activity, students read capsule summaries of twelve research papers\nrelated to restaurants and dining that was published in April 2015. The\nselected papers report on topics such as how seating arrangement, server\nposture, plate color and size, and the use of background music relate to\nrevenue, ambiance, and perceived food quality. The students are assigned one of\nthe twelve papers to read and critique as part of a small group. Their group\ncritiques are shared with the class and the instructor.\n  A pilot study was conducted during the 2015-2016 academic year at Amherst\nCollege. Students noted that key details were not included in the published\nsummary. They were generally skeptical of the published conclusions. The\nstudents often provided additional summarization of information from the\njournal articles that better describe the results. By independently assessing\nand comparing the original study conclusions with the capsule summary in the\n\"Study of Studies,\" students can practice developing judgment and assessing the\nvalidity of statistical results.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.08438v1"
    },
    {
        "title": "Enriching students' conceptual understanding of confidence intervals: An\n  interactive trivia-based classroom activity",
        "authors": [
            "Xiaofei Wang",
            "Nicholas G. Reich",
            "Nicholas J. Horton"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Confidence intervals provide a way to determine plausible values for a\npopulation parameter. They are omnipresent in research articles involving\nstatistical analyses. Appropriately, a key statistical literacy learning\nobjective is the ability to interpret and understand confidence intervals in a\nwide range of settings. As instructors, we devote a considerable amount of time\nand effort to ensure that students master this topic in introductory courses\nand beyond. Yet, studies continue to find that confidence intervals are\ncommonly misinterpreted and that even experts have trouble calibrating their\nindividual confidence levels. In this article, we present a ten-minute trivia\ngame-based activity that addresses these misconceptions by exposing students to\nconfidence intervals from a personal perspective. We describe how the activity\ncan be integrated into a statistics course as a one-time activity or with\nrepetition at intervals throughout a course, discuss results of using the\nactivity in class, and present possible extensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.08452v1"
    },
    {
        "title": "Nonparametric Spherical Regression Using Diffeomorphic Mappings",
        "authors": [
            "Michael Rosenthal",
            "Wei Wu",
            "Eric Klassen",
            "Anuj Srivastava"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Spherical regression explores relationships between variables on spherical\ndomains. We develop a nonparametric model that uses a diffeomorphic map from a\nsphere to itself. The restriction of this mapping to diffeomorphisms is natural\nin several settings. The model is estimated in a penalized maximum-likelihood\nframework using gradient-based optimization. Towards that goal, we specify a\nfirst-order roughness penalty using the Jacobian of diffeomorphisms. We compare\nthe prediction performance of the proposed model with state-of-the-art methods\nusing simulated and real data involving cloud deformations, wind directions,\nand vector-cardiograms. This model is found to outperform others in capturing\nrelationships between spherical variables.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.00823v1"
    },
    {
        "title": "MH370 Burst Frequency Offset Analysis and Implications on Descent Rate\n  at End-of-Flight",
        "authors": [
            "Ian D. Holland"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Malaysian Airlines flight MH370 veered off course unexpectedly during a\nscheduled trip from Kuala Lumpur to Beijing on the 7th of March 2014. MH370 was\ntracked via military radar into the Malacca Straits and, after disappearing\nfrom radar, was subsequently believed to have turned south towards the southern\nIndian Ocean before crashing approximately 6 hours later. This article\ndiscusses specifically the analysis of burst frequency offset (BFO) metadata\nfrom the SATCOM messages. It is shown that the BFOs corresponding to the last\ntwo SATCOM messages from the plane at 00:19:29Z and 00:19:37Z 8th March 2014\nsuggest that flight MH370 was rapidly descending and accelerating downwards\nwhen message exchange with the ground station ceased.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.02432v3"
    },
    {
        "title": "Social Big Data Analytics of Consumer Choices: A Two Sided Online\n  Platform Perspective",
        "authors": [
            "Meisam Hejazi Nia"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  This dissertation examines three distinct big data analytics problems related\nto the social aspects of consumers' choices. The main goal of this line of\nresearch is to help two sided platform firms to target their marketing policies\ngiven the great heterogeneity among their customers. In three essays, I\ncombined structural modeling and machine learning approaches to first\nunderstand customers' responses to intrinsic and extrinsic factors, using\nunique data sets I scraped from the web, and then explore methods to optimize\ntwo sided platforms' firms' reactions accordingly. The first essay examines\n\"social learning\" in the mobile app store context, controlling for intrinsic\nvalue of hedonic and utilitarian mobile apps, price, advertising, and number of\noptions available. The second essay investigates bidders' anticipated winner\nand loser regret in the context of the eBay online auction platform. Using a\nlarge data set from eBay and empirical Bayesian estimation method, I quantify\nthe bidders' anticipation of regret in various product categories, and\ninvestigate the role of experience in explaining the bidders' regret and\nlearning behaviors. The third essay investigates the effects of Gamification\nincentive mechanisms in an online platform for user generated content. I use an\nensemble method over LDA, mixed normal and k-mean clustering methods to segment\nusers into competitors, collaborators, achievers, explorers and uninterested\nusers. These findings help the Gamification platform to target its users. The\nsimulation counterfactual analysis suggests that a two sided platform can\nincrease the number of user contributions, by making earning badges more\ndifficult.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.07074v1"
    },
    {
        "title": "J.B.S. Haldane Could Have Done Better",
        "authors": [
            "Claus Vogl"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  In a review on the contribution of J.B.S. Haldane to the development of the\nBayes factor hypothesis test (arXiv:1511.08180), Etz and Wagenmakers focus on\nHaldane's proposition of a mixture prior in a genetic example (Haldane 1932, A\nnote on inverse probability. Mathematical Proceedings of the Cambridge\nPhilosophical Society, 28, 55-61.). As Haldane never followed up on these\nideas, it is difficult to gauge his motivation and intentions. I argue that\ncontrary to Haldane's stated intention of replacing flat priors with more\nreasonable assumptions, he actually chose in this example an unreasonable flat\nprior. Considering the information available to Haldane, I derive a superior\nprior and compare to Haldane's flat prior. Haldane's main intent with his\narticle seems to have been to explore the different parameter regions of the\nbinomial and the conjugate beta. Furthermore, I agree with Etz and Wagenmakers\nthat Haldane serendipitously adopted a mixture prior comprising a point mass\nand smooth distribution in his genetic example.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.08261v1"
    },
    {
        "title": "Scientific progress despite irreproducibility: A seeming paradox",
        "authors": [
            "Richard M. Shiffrin",
            "Katy Borner",
            "Stephen M. Stigler"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  It appears paradoxical that science is producing outstanding new results and\ntheories at a rapid rate at the same time that researchers are identifying\nserious problems in the practice of science that cause many reports to be\nirreproducible and invalid. Certainly the practice of science needs to be\nimproved and scientists are now pursuing this goal. However, in this\nperspective we argue that this seeming paradox is not new, has always been part\nof the way science works, and likely will remain so. We first introduce the\nparadox. We then review a wide range of challenges that appear to make\nscientific success difficult. Next, we describe the factors that make science\nwork-in the past, present, and presumably also in the future. We then suggest\nthat remedies for the present practice of science need to be applied\nselectively so as not to slow progress, and illustrate with a few examples. We\nconclude with arguments that communication of science needs to emphasize not\njust problems but the enormous successes and benefits that science has brought\nand is now bringing to all elements of modern society.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.01946v1"
    },
    {
        "title": "Replacing P values with frequentist posterior probabilities - as\n  possible parameter values must have uniform base-rate prior probabilities by\n  definition in a random sampling model",
        "authors": [
            "Huw Llewelyn"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Possible parameter values in a random sampling model are shown by definition\nto have uniform base-rate prior probabilities. This allows a frequentist\nposterior probability distribution to be calculated for such possible parameter\nvalues conditional solely on actual study observations. If the likelihood\nprobability distribution of a random selection is modelled with a symmetrical\ncontinuous function then the frequentist posterior probability of something\nequal to or more extreme than the null hypothesis will be equal to the P-value;\notherwise the P value would be an approximation. An idealistic probability of\nreplication based on an assumption of perfect study methodological\nreproducibility can be used as the upper bound of a realistic probability of\nreplication that may be affected by various confounding factors. Bayesian\ndistributions can be combined with these frequentist distributions. The\nidealistic frequentist posterior probability of replication may be easier than\nthe P-value for non-statisticians to understand and to interpret.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.07284v5"
    },
    {
        "title": "Variance decompositions for extensive-form games",
        "authors": [
            "Alex Cloud",
            "Eric Laber"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Quantitative measures of randomness in games are useful for game design and\nhave implications for gambling law. We treat the outcome of a game as a random\nvariable and derive a closed-form expression and estimator for the variance in\nthe outcome attributable to a player of the game. We analyze poker hands to\nshow that randomness in the cards dealt has little influence on the outcomes of\neach hand. A simple example is given to demonstrate how variance decompositions\ncan be used to measure other interesting properties of games.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.04834v1"
    },
    {
        "title": "Review of: Mixed Effects Models and Extensions in Ecology with R",
        "authors": [
            "J. Andrew Royle"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  This is a review of the book \"Mixed Effects Models and Extensions in Ecology\nwith R\" by Zuur, Ieno, Walker, Saveliev and Smith (2009, Springer). I was asked\nto review this book for The American Statistician in 2010. After I wrote the\nreview, the invitation was revoked. This is the review.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.6995v1"
    },
    {
        "title": "Causal influence in linear response models",
        "authors": [
            "Andrea Auconi",
            "Andrea Giansanti",
            "Edda Klipp"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  The intuition of causation is so fundamental that almost every research study\nin life sciences refers to this concept. However a widely accepted formal\ndefinition of causal influence between observables is still missing. In the\nframework of linear Langevin networks without feedbacks (linear response\nmodels) we developed a measure of causal influence based on a decomposition of\ninformation flows over time. We discuss its main properties and compare it with\nother information measures like the Transfer Entropy. Finally we outline some\ndifficulties of the extension to a general definition of causal influence for\ncomplex systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03073v1"
    },
    {
        "title": "On $p$-values",
        "authors": [
            "Laurie Davies"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Models are consistently treated as approximations and all procedures are\nconsistent with this. They do not treat the model as being true. In this\ncontext $p$-values are one measure of approximation, a small $p$-value\nindicating a poor approximation. Approximation regions are defined and\ndistinguished from confidence regions.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.06168v1"
    },
    {
        "title": "Stop the tests: Opinion bias and statistical tests",
        "authors": [
            "André C. R. Martins"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  When statisticians quarrel about hypothesis testing, the debate usually focus\non which method is the correct one. The fundamental question of whether we\nshould test hypothesis at all tends to be forgotten. This lack of debate has\nits roots on our desire to have ideas we believe and defend. But cognitive\nexperiments have been showing that, when we do choose ideas, we become prey to\na large number of biases. Several of our biases can be grouped together in a\nsingle description, an opinion bias. This opinion bias is nothing more than our\ndesire to believe in something and to defend it. Also, despite our feelings,\nbelieving has no solid logical or philosophical grounds. In this paper, I will\nshow that if we combine the fact that even logic can never prove an idea right\nor wrong and the problems our brains cause when we pick ideas, hypothesis\ntesting and its terminology are a recipe for disaster. Testing should have no\nplace when we are thinking about hypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.06545v1"
    },
    {
        "title": "BayesVarSel: Bayesian Testing, Variable Selection and model averaging in\n  Linear Models using R",
        "authors": [
            "Gonzalo Garcia-Donato",
            "Anabel Forte"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  This paper introduces the R package BayesVarSel which implements objective\nBayesian methodology for hypothesis testing and variable selection in linear\nmodels. The package computes posterior probabilities of the competing\nhypotheses/models and provides a suite of tools, specifically proposed in the\nliterature, to properly summarize the results. Additionally, \\ourpack\\ is armed\nwith functions to compute several types of model averaging estimations and\npredictions with weights given by the posterior probabilities. BayesVarSel\ncontains exact algorithms to perform fast computations in problems of small to\nmoderate size and heuristic sampling methods to solve large problems. The\nsoftware is intended to appeal to a broad spectrum of users, so the interface\nhas been carefully designed to be highly intuititive and is inspired by the\nwell-known lm function. The issue of prior inputs is carefully addressed. In\nthe default usage (fully automatic for the user)BayesVarSel implements the\ncriteria-based priors proposed by Bayarri et al (2012), but the advanced user\nhas the possibility of using several other popular priors in the literature.\nThe package is available through the Comprehensive R Archive Network, CRAN. We\nillustrate the use of BayesVarSel with several data examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.08118v1"
    },
    {
        "title": "mBART: Multidimensional Monotone BART",
        "authors": [
            "Hugh A. Chipman",
            "Edward I. George",
            "Robert E. McCulloch",
            "Thomas S. Shively"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  For the discovery of regression relationships between Y and a large set of p\npotential predictors x 1 , . . . , x p , the flexible nonparametric nature of\nBART (Bayesian Additive Regression Trees) allows for a much richer set of\npossibilities than restrictive parametric approaches. However, subject matter\nconsiderations sometimes warrant a minimal assumption of monotonicity in at\nleast some of the predictors. For such contexts, we introduce mBART, a\nconstrained version of BART that can flexibly incorporate monotonicity in any\npredesignated subset of predictors using a multivariate basis of monotone\ntrees, while avoiding the further confines of a full parametric form. For such\nmonotone relationships, mBART provides (i) function estimates that are smoother\nand more interpretable, (ii) better out-of-sample predictive performance, and\n(iii) less post-data uncertainty. While many key aspects of the unconstrained\nBART model carry over directly to mBART, the introduction of monotonicity\nconstraints necessitates a fundamental rethinking of how the model is\nimplemented. In particular, the original BART Markov Chain Monte Carlo\nalgorithm relied on a conditional conjugacy that is no longer available in a\nmonotonically constrained space. Various simulated and real examples\ndemonstrate the wide ranging potential of mBART.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.01619v3"
    },
    {
        "title": "Building Communication Skills in a Theoretical Statistics Course",
        "authors": [
            "Amy Wagaman"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  The traditional theoretical statistics course which develops the theoretical\nunderpinnings of the discipline (usually following a probability course) is\nundergoing near-continuous revision in the statistics community. In particular,\nrecent versions of this course have incorporated more and more computation. We\ntake a look at a different aspect of the revision - building student\ncommunication skills in the course, in both written and verbal forms, to allow\nstudents to demonstrate their ability to explain statistical concepts. Two\nseparate projects are discussed, both of which were engaged in by a class of\nsize 17 in Spring 2015. The first project had a computational aspect (performed\nusing R), a statistical theory component, and a writing component, and was\nbased on the historical German tank problem. The second project involved a\nclass presentation and written report summarizing, critiquing, and/or\nexplaining an article selected from The American Statistician.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.02252v1"
    },
    {
        "title": "A Graph Downsampling Technique Based On Graph Fourier Transform",
        "authors": [
            "Nileshkumar Vaishnav",
            "Aditya Tatu"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  In this paper, we provide a Graph Fourier Transform based approach to\ndownsample signals on graphs. For bandlimited signals on a graph, a test is\nprovided to identify whether signal reconstruction is possible from the given\ndownsampled signal. Moreover, if the signal is not bandlimited, we provide a\nquality measure for comparing different downsampling schemes. Using this\nquality measure, we propose a greedy downsampling algorithm. Most of the\nprevailing approaches consider undirected graphs, and exploit the topological\nproperties of the graph in order to downsample the grid, while the proposed\nmethod exploits spectral properties of graph signals, and is applicable to\ndirected graphs, undirected graphs, and graphs with negative edge-weights. We\nprovide several experiments demonstrating our downsampling scheme, and compare\nour quality measure with measures like normalized cuts.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07542v1"
    },
    {
        "title": "Data learning from big data",
        "authors": [
            "José L. Torrecilla",
            "Juan Romo"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Technology is generating a huge and growing availability of observa tions of\ndiverse nature. This big data is placing data learning as a central scientific\ndiscipline. It includes collection, storage, preprocessing, visualization and,\nessentially, statistical analysis of enormous batches of data. In this paper,\nwe discuss the role of statistics regarding some of the issues raised by big\ndata in this new paradigm and also propose the name of data learning to\ndescribe all the activities that allow to obtain relevant knowledge from this\nnew source of information.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.03971v1"
    },
    {
        "title": "Teaching computational reproducibility for neuroimaging",
        "authors": [
            "K. Jarrod Millman",
            "Matthew Brett",
            "Ross Barnowski",
            "Jean-Baptiste Poline"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  We describe a project-based introduction to reproducible and collaborative\nneuroimaging analysis. Traditional teaching on neuroimaging usually consists of\na series of lectures that emphasize the big picture rather than the foundations\non which the techniques are based. The lectures are often paired with practical\nworkshops in which students run imaging analyses using the graphical interface\nof specific neuroimaging software packages. Our experience suggests that this\ncombination leaves the student with a superficial understanding of the\nunderlying ideas, and an informal, inefficient, and inaccurate approach to\nanalysis. To address these problems, we based our course around a substantial\nopen-ended group project. This allowed us to teach: (a) computational tools to\nensure computationally reproducible work, such as the Unix command line,\nstructured code, version control, automated testing, and code review and (b) a\nclear understanding of the statistical techniques used for a basic analysis of\na single run in an MRI scanner. The emphasis we put on the group project showed\nthe importance of standard computational tools for accuracy, efficiency, and\ncollaboration. The projects were broadly successful in engaging students in\nworking reproducibly on real scientific questions. We propose that a course on\nthis model should be the foundation for future programs in neuroimaging. We\nbelieve it will also serve as a model for teaching efficient and reproducible\nresearch in other fields of computational science.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.06145v1"
    },
    {
        "title": "A Constructive Algebraic Proof of Student's Theorem",
        "authors": [
            "Yiping Cheng"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Student's theorem is an important result in statistics which states that for\nnormal population, the sample variance is independent from the sample mean and\nhas a chi-square distribution. The existing proofs of this theorem either\noverly rely on advanced tools such as moment generating functions, or fail to\nexplicitly construct an orthogonal matrix used in the proof. This paper\nprovides an elegant explicit construction of that matrix, making the algebraic\nproof complete. The constructive algebraic proof proposed here is thus very\nsuitable for being included in textbooks.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08031v1"
    },
    {
        "title": "Nonparametric Confidence Regions for Veronese-Whitney Means and\n  Antimeans on Planar Kendall Shape Spaces",
        "authors": [
            "Yunfan Wang",
            "Vic Patrangenaru"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  In this paper after a brief revision of VW-means, which are extrinsic means\non real and complex projective spaces, relative to the Veronese-Whitney\nembeddings, we give two examples of sample VW means computations on planar\nKendall shape spaces. Here we derive large sample and pivotal nonparametric\nbootstrap confidence regions for VW-antimeans, using VW-anti-covariance\nmatrices, and their sample counterparts\n",
        "pdf_link": "http://arxiv.org/pdf/1806.08683v2"
    },
    {
        "title": "Discussion on Using Stacking to Average Bayesian Predictive\n  Distributions by Yao et al",
        "authors": [
            "William Weimin Yoo"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  I begin by summarizing key ideas of the paper under discussion. Then I will\ntalk about a graphical modeling perspective, posterior contraction rates and\nalternative methods of aggregation. Moreover, I will also discuss possible\napplications of the stacking method to other problems, in particular,\naggregating (sub)posterior distributions in distributed computing.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.11427v1"
    },
    {
        "title": "Characterization of Sine- Skewed von Mises Distribution",
        "authors": [
            "Mohammad Ahsanullah",
            "M. Z. Anis"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The von Mises distribution is one of the most important distribution in\nstatistics to deal with circular data. In this paper we will consider some\nbasic properties and characterizations of the sine skewed von Mises\ndistribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.02579v1"
    },
    {
        "title": "Applications of band-limited extrapolation to forecasting of weather and\n  financial time series",
        "authors": [
            "Nicholas James Rowe"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  This paper describes the practical application of causal extrapolation of\nsequences for the purpose of forecasting. The methods and proofs have been\napplied to simulations to measure the range which data can be accurately\nextrapolated. Real world data from the Australian Stock exchange and the\nAustralian Bureau of Meteorology have been tested and compared with simple\nlinear extrapolation of the same data. In a majority of the tested scenarios\ncasual extrapolation has been proved to be the more effective forecaster.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.09501v1"
    },
    {
        "title": "A note on Fibonacci Sequences of Random Variables",
        "authors": [
            "Ismihan Bayramoglu"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The focus of this paper is the random sequences in the form $\\{X_{0},X_{1},$\n$X_{n}=X_{n-2}+X_{n-1},n=2,3,..\\dot{\\}},$ referred to as Fibonacci Random\nSequence (FRS). The initial random variables $X_{0}$ and $X_{1}$ are assumed to\nbe absolutely continuous with joint probability density function (pdf)\n$f_{X_{0},X_{1}}.$ The FRS is completely determined by $X_{0}$ and $X_{1}$ and\nthe members of Fibonacci sequence $\\digamma\n\\equiv\\{0,1,1,2,3,5,8,13,21,34,55,89,144,...\\}.$ We examine the distributional\nand limit properties of the random sequence $X_{n},n=0,1,2,...$ .\n",
        "pdf_link": "http://arxiv.org/pdf/1902.09790v1"
    },
    {
        "title": "A First Course in Data Science",
        "authors": [
            "Donghui Yan",
            "Gary E. Davis"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Data science is a discipline that provides principles, methodology and\nguidelines for the analysis of data for tools, values, or insights. Driven by a\nhuge workforce demand, many academic institutions have started to offer degrees\nin data science, with many at the graduate, and a few at the undergraduate\nlevel. Curricula may differ at different institutions, because of varying\nlevels of faculty expertise, and different disciplines (such as Math, computer\nscience, and business etc) in developing the curriculum. The University of\nMassachusetts Dartmouth started offering degree programs in data science from\nFall 2015, at both the undergraduate and the graduate level. Quite a few\narticles have been published that deal with graduate data science courses, much\nless so dealing with undergraduate ones. Our discussion will focus on\nundergraduate course structure and function, and specifically, a first course\nin data science. Our design of this course centers around a concept called the\ndata science life cycle. That is, we view tasks or steps in the practice of\ndata science as forming a process, consisting of states that indicate how it\ncomes into life, how different tasks in data science depend on or interact with\nothers until the birth of a data product or the reach of a conclusion.\nNaturally, different pieces of the data science life cycle then form individual\nparts of the course. Details of each piece are filled up by concepts,\ntechniques, or skills that are popular in industry. Consequently, the design of\nour course is both \"principled\" and practical. A significant feature of our\ncourse philosophy is that, in line with activity theory, the course is based on\nthe use of tools to transform real data in order to answer strongly motivated\nquestions related to the data.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03121v1"
    },
    {
        "title": "A response to critiques of \"The reproducibility of research and the\n  misinterpretation of p-values\"",
        "authors": [
            "David Colquhoun"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  I proposed (8, 1, 3) that p values should be supplemented by an estimate of\nthe false positive risk (FPR). FPR was defined as the probability that, if you\nclaim that there is a real effect on the basis of p value from a single\nunbiased experiment, that you will be mistaken and the result has occurred by\nchance. This is a Bayesian quantity and that means that there is an infinitude\nof ways to calculate it. My choice of a way to estimate FPR was, therefore,\narbitrary. I maintain that it is a reasonable way, and has the advantage of\nbeing mathematically simpler than other proposals and easier to understand than\nother methods. This might make it more easily accepted by users. As always, not\nevery statistician agrees. This paper is a response to a critique of my 2017\npaper (1) by Arandjelovic (2)\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08338v3"
    },
    {
        "title": "Statistical methods research done as science rather than mathematics",
        "authors": [
            "James S. Hodges"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  This paper is about how we study statistical methods. As an example, it uses\nthe random regressions model, in which the intercept and slope of\ncluster-specific regression lines are modeled as a bivariate random effect.\nMaximizing this model's restricted likelihood often gives a boundary value for\nthe random effect correlation or variances. We argue that this is a problem;\nthat it is a problem because our discipline has little understanding of how\ncontemporary models and methods map data to inferential summaries; that we lack\nsuch understanding, even for models as simple as this, because of a\nnear-exclusive reliance on mathematics as a means of understanding; and that\nmath alone is no longer sufficient. We then argue that as a discipline, we can\nand should break open our black-box methods by mimicking the five steps that\nmolecular biologists commonly use to break open Nature's black boxes: design a\nsimple model system, formulate hypotheses using that system, test them in\nexperiments on that system, iterate as needed to reformulate and test\nhypotheses, and finally test the results in an \"in vivo\" system. We demonstrate\nthis by identifying conditions under which the random-regressions restricted\nlikelihood is likely to be maximized at a boundary value. Resistance to this\napproach seems to arise from a view that it lacks the certainty or intellectual\nheft of mathematics, perhaps because simulation experiments in our literature\nrarely do more than measure a new method's operating characteristics in a small\nrange of situations. We argue that such work can make useful contributions\nincluding, as in molecular biology, the findings themselves and sometimes the\ndesigns used in the five steps; that these contributions have as much practical\nvalue as mathematical results; and that therefore they merit publication as\nmuch as the mathematical results our discipline esteems so highly.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08381v1"
    },
    {
        "title": "Many perspectives on Deborah Mayo's \"Statistical Inference as Severe\n  Testing: How to Get Beyond the Statistics Wars\"",
        "authors": [
            "Andrew Gelman",
            "Brian Haig",
            "Christian Hennig",
            "Art Owen",
            "Robert Cousins",
            "Stan Young",
            "Christian Robert",
            "Corey Yanofsky",
            "E. J. Wagenmakers",
            "Ron Kenett",
            "Daniel Lakeland"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The new book by philosopher Deborah Mayo is relevant to data science for\ntopical reasons, as she takes various controversial positions regarding\nhypothesis testing and statistical practice, and also as an entry point to\nthinking about the philosophy of statistics. The present article is a slightly\nexpanded version of a series of informal reviews and comments on Mayo's book.\nWe hope this discussion will introduce people to Mayo's ideas along with other\nperspectives on the topics she addresses.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08876v2"
    },
    {
        "title": "An illustration of the risk of borrowing information via a shared\n  likelihood",
        "authors": [
            "P. Richard Hahn"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  A concrete, stylized example illustrates that inferences may be degraded,\nrather than improved, by incorporating supplementary data via a joint\nlikelihood. In the example, the likelihood is assumed to be correctly\nspecified, as is the prior over the parameter of interest; all that is\nnecessary for the joint modeling approach to suffer is misspecification of the\nprior over a nuisance parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.09715v1"
    },
    {
        "title": "A score function for Bayesian cluster analysis",
        "authors": [
            "John Noble",
            "Łukasz Rajkowski"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  We propose a score function for Bayesian clustering. The function is\nparameter free and captures the interplay between the within cluster variance\nand the between cluster entropy of a clustering. It can be used to choose the\nnumber of clusters in well-established clustering methods such as hierarchical\nclustering or $K$-means algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.10209v1"
    },
    {
        "title": "Visualisation of Brain Statistics with R-packages ggseg and ggseg3d",
        "authors": [
            "Athanasia M. Mowinckel",
            "Didac Vidal-Piñeiro"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  There is an increased emphasis on visualizing neuroimaging results in more\nintuitive ways. Common statistical tools for dissemination, such as bar charts,\nlack the spatial dimension that is inherent in neuroimaging data. Here we\npresent two packages for the statistical software R, ggseg and ggseg3d, that\nintegrate this spatial component. The ggseg and ggseg3d packages visualize\npre-defined brain segmentations as both 2D polygons and 3D meshes,\nrespectively. Both packages are integrated with other well-established\nR-packages, allowing great flexibility. In this tutorial, we present the main\ndata and functions in the ggseg and ggseg3d packages for brain atlas\nvisualization. The main highlighted functions are able to display brain\nsegmentation plots in R. Further, the accompanying ggsegExtra-package includes\na wider collection of atlases, and is intended for community-based efforts to\ndevelop more compatible atlases to ggseg and ggseg3d. Overall, the\nggseg-packages facilitate parcellation-based visualizations in R, improve and\nease the dissemination of the results, and increase the efficiency of the\nworkflows.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.08200v1"
    },
    {
        "title": "Resolving the induction problem: Can we state with complete confidence\n  via induction that the sun rises forever?",
        "authors": [
            "Youngjo Lee"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Induction is a form of reasoning that moves from a particular example to a\ngeneral rule. However, establishing the truth of a general proposition is\nproblematic because it is always possible that a conflicting observation will\noccur. This is known as the induction problem. The sunrise problem, first\nintroduced by Laplace (1814), is a quintessential example of the induction\nproblem. However, in his solution, a zero probability is always assigned to the\ngeneral proposition that the sun will rise forever, regardless of the number of\nobservations made. This is a problem of induction: the acceptance of a general\nproposition can never be attained via induction. In this paper, we study why\nthis happens and show how scientific theory enables us to overcome such a\ndifficulty. A likelihood-based epistemology is proposed, which allows a\nresolution in agreement with the evidence by providing a new updating rule not\nthe Bayes rule. The study shows that through induction, one can rationally\naccept a general proposition.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.04110v2"
    },
    {
        "title": "Error control in the numerical posterior distribution in the Bayesian UQ\n  analysis of a semilinear evolution PDE",
        "authors": [
            "Maria L. Daza-Torres",
            "J. Cricelio Montesinos-López",
            "Marcos A. Capistrán",
            "J. Andrés Christen",
            "Heikki Haario"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  We elaborate on results obtained in \\cite{christen2018} for controlling the\nnumerical posterior error for Bayesian UQ problems, now considering forward\nmaps arising from the solution of a semilinear evolution partial differential\nequation. Results in \\cite{christen2018} demand an estimate for the absolute\nglobal error (AGE) of the numeric forward map. Our contribution is a numerical\nmethod for computing the AGE for semilinear evolution PDEs and shows the\npotential applicability of \\cite{christen2018} in this important wide range\nfamily of PDEs. Numerical examples are given to illustrate the efficiency of\nthe proposed method, obtaining numerical posterior distributions for unknown\nparameters that are nearly identical to the corresponding theoretical\nposterior, by keeping their Bayes factor close to 1.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.04977v2"
    },
    {
        "title": "When black box algorithms are (not) appropriate: a principled\n  prediction-problem ontology",
        "authors": [
            "Jordan Rodu",
            "Michael Baiocchi"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  In the 1980s a new, extraordinarily productive way of reasoning about\nalgorithms emerged. In this paper, we introduce the term \"outcome reasoning\" to\nrefer to this form of reasoning. Though outcome reasoning has come to dominate\nareas of data science, it has been under-discussed and its impact\nunder-appreciated. For example, outcome reasoning is the primary way we reason\nabout whether ``black box'' algorithms are performing well. In this paper we\nanalyze outcome reasoning's most common form (i.e., as \"the common task\nframework\") and its limitations. We discuss why a large class of\nprediction-problems are inappropriate for outcome reasoning. As an example, we\nfind the common task framework does not provide a foundation for the deployment\nof an algorithm in a real world situation. Building off of its core features,\nwe identify a class of problems where this new form of reasoning can be used in\ndeployment. We purposefully develop a novel framework so both technical and\nnon-technical people can discuss and identify key features of their prediction\nproblem and whether or not it is suitable for outcome reasoning.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.07648v5"
    },
    {
        "title": "Linear Regression under Special Relativity",
        "authors": [
            "Si Hyung Joo"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  This study investigated the problem posed by using ordinary least squares\n(OLS) to estimate parameters of simple linear regression under a specific\ncontext of special relativity, where an independent variable is restricted to\nan open interval, (-c, c). It is found that the OLS estimate for the slope\ncoefficient is not invariant under Lorentz velocity transformation.\nAccordingly, an alternative estimator for the parameters of linear regression\nunder special relativity is proposed. This estimator can be considered a\ngeneralization of the OLS estimator under special relativity; when c approaches\nto infinity, the proposed estimator and its variance converges to the OLS\nestimator and its variance, respectively. The variance of the proposed\nestimator is larger than that of the OLS estimator, which implies that\nhypothesis testing using the OLS estimator and its variance may result in a\nliberal test under special relativity.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.14292v1"
    },
    {
        "title": "Removing Gaussian Noise by Optimization of Weights in Non-Local Means",
        "authors": [
            "Qiyu Jin",
            "Ion Grama",
            "Quansheng Liu"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  A new image denoising algorithm to deal with the additive Gaussian white\nnoise model is given. Like the non-local means method, the filter is based on\nthe weighted average of the observations in a neighborhood, with weights\ndepending on the similarity of local patches. But in contrast to the non-local\nmeans filter, instead of using a fixed Gaussian kernel, we propose to choose\nthe weights by minimizing a tight upper bound of mean square error. This\napproach makes it possible to define the weights adapted to the function at\nhand, mimicking the weights of the oracle filter. Under some regularity\nconditions on the target image, we show that the obtained estimator converges\nat the usual optimal rate. The proposed algorithm is parameter free in the\nsense that it automatically calculates the bandwidth of the smoothing kernel;\nit is fast and its implementation is straightforward. The performance of the\nnew filter is illustrated by numerical simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.5640v1"
    },
    {
        "title": "Experimental design for Partially Observed Markov Decision Processes",
        "authors": [
            "Leifur Thorbergsson",
            "Giles Hooker"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  This paper deals with the question of how to most effectively conduct\nexperiments in Partially Observed Markov Decision Processes so as to provide\ndata that is most informative about a parameter of interest. Methods from\nMarkov decision processes, especially dynamic programming, are introduced and\nthen used in an algorithm to maximize a relevant Fisher Information. The\nalgorithm is then applied to two POMDP examples. The methods developed can also\nbe applied to stochastic dynamical systems, by suitable discretization, and we\nconsequently show what control policies look like in the Morris-Lecar Neuron\nmodel, and simulation results are presented. We discuss how parameter\ndependence within these methods can be dealt with by the use of priors, and\ndevelop tools to update control policies online. This is demonstrated in\nanother stochastic dynamical system describing growth dynamics of DNA template\nin a PCR model.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.4019v4"
    },
    {
        "title": "J. B. S. Haldane's Contribution to the Bayes Factor Hypothesis Test",
        "authors": [
            "Alexander Etz",
            "Eric-Jan Wagenmakers"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  This article brings attention to some historical developments that gave rise\nto the Bayes factor for testing a point null hypothesis against a composite\nalternative. In line with current thinking, we find that the conceptual\ninnovation - to assign prior mass to a general law - is due to a series of\nthree articles by Dorothy Wrinch and Sir Harold Jeffreys (1919, 1921, 1923).\nHowever, our historical investigation also suggests that in 1932 J. B. S.\nHaldane made an important contribution to the development of the Bayes factor\nby proposing the use of a mixture prior comprising a point mass and a\ncontinuous probability density. Jeffreys was aware of Haldane's work and it may\nhave inspired him to pursue a more concrete statistical implementation for his\nconceptual ideas. It thus appears that Haldane may have played a much bigger\nrole in the statistical development of the Bayes factor than has hitherto been\nassumed.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08180v4"
    },
    {
        "title": "A shiny update to an old experiment game",
        "authors": [
            "Robert B. Gramacy"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Games can be a powerful tool for learning about statistical methodology.\nEffective game design involves a fine balance between caricature and realism,\nto simultaneously illustrate salient concepts in a controlled setting and serve\nas a testament to real-world applicability. Striking that balance is\nparticularly challenging in response surface and design domains, where\nreal-world scenarios often play out over long time scales, during which\ntheories are revised, model and inferential techniques are improved, and\nknowledge is updated. Here I present a game, borrowing liberally from one first\nplayed over forty years ago, that attempts to achieve that balance while\nreinforcing a cascade of topics in modern nonparametric response surfaces,\nsequential design and optimization. The game embeds a blackbox simulation\nwithin a shiny app whose interface is designed to simulate a realistic\ninformation-availability setting, while offering a stimulating, competitive\nenvironment wherein students can try out new methodology, and ultimately\nappreciate its power and limitations. Interface, rules, timing with course\nmaterial, and evaluation are described, along with a \"case study\" involving a\ncohort of students at Virginia Tech.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00613v2"
    },
    {
        "title": "Emanuel Parzen: A Memorial, and a Model With the Two Kernels That He\n  Championed",
        "authors": [
            "Grace Wahba"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Manny Parzen passed away in February 2016, and this article is written partly\nas a memorial and appreciation. Manny made important contributions to several\nareas, but the two that influenced me most were his contributions to kernel\ndensity estimation and to Reproducing Kernel Hilbert Spaces, the two kernels of\nthe title. Some fond memories of Manny as a PhD advisor begin this memorial,\nfollowed by a discussion of Manny's influence on density estimation and RKHS\nmethods. A picture gallery of trips comes next, followed by the technical part\nof the article. Here our goal is to show how risk models can be built using\nRKHS penalized likelihood methods where subjects have personal (sample)\ndensities which can be used as {\\it attributes} in such models.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.05555v1"
    },
    {
        "title": "How sure are we? Two approaches to statistical inference",
        "authors": [
            "Michael Wood"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Suppose you are told that taking a statin will reduce your risk of a heart\nattack or stroke by 3% in the next ten years, or that women have better\nemotional intelligence than men. You may wonder how accurate the 3% is, or how\nconfident we should be about the assertion about women's emotional\nintelligence, bearing in mind that these conclusions are only based on samples\nof data? My aim here is to present two statistical approaches to questions like\nthese. Approach 1 is often called null hypothesis testing but I prefer the\nphrase \"baseline hypothesis\": this is the standard approach in many areas of\ninquiry but is fraught with problems. Approach 2 can be viewed as a\ngeneralisation of the idea of confidence intervals, or as the application of\nBayes' theorem. Unlike Approach 1, Approach 2 provides a tentative estimate of\nthe probability of hypotheses of interest. For both approaches, I explain, from\nfirst principles, building only on \"common sense\" statistical concepts like\naverages and randomness, both how to derive answers, and the rationale behind\nthe answers. This is achieved by using computer simulation methods (resampling\nand bootstrapping using a spreadsheet available on the web) which avoid the use\nof probability distributions (t, normal, etc). Such a minimalist, but\nreasonably rigorous, analysis is particularly useful in a discipline like\nstatistics which is widely used by people who are not specialists. My intended\naudience includes both statisticians, and users of statistical methods who are\nnot statistical experts.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.06214v1"
    },
    {
        "title": "Scientific discovery in a model-centric framework: Reproducibility,\n  innovation, and epistemic diversity",
        "authors": [
            "Berna Devezer",
            "Luis G. Nardin",
            "Bert Baumgaertner",
            "Erkan Buzbas"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Consistent confirmations obtained independently of each other lend\ncredibility to a scientific result. We refer to results satisfying this\nconsistency as reproducible and assume that reproducibility is a desirable\nproperty of scientific discovery. Yet seemingly science also progresses despite\nirreproducible results, indicating that the relationship between\nreproducibility and other desirable properties of scientific discovery is not\nwell understood. These properties include early discovery of truth, persistence\non truth once it is discovered, and time spent on truth in a long-term\nscientific inquiry. We build a mathematical model of scientific discovery that\npresents a viable framework to study its desirable properties including\nreproducibility. In this framework, we assume that scientists adopt a\nmodel-centric approach to discover the true model generating data in a\nstochastic process of scientific discovery. We analyze the properties of this\nprocess using Markov chain theory, Monte Carlo methods, and agent-based\nmodeling. We show that the scientific process may not converge to truth even if\nscientific results are reproducible and that irreproducible results do not\nnecessarily imply untrue results. The proportion of different research\nstrategies represented in the scientific population, scientists' choice of\nmethodology, the complexity of truth, and the strength of signal contribute to\nthis counter-intuitive finding. Important insights include that innovative\nresearch speeds up the discovery of scientific truth by facilitating the\nexploration of model space and epistemic diversity optimizes across desirable\nproperties of scientific discovery.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.10118v3"
    },
    {
        "title": "Statistical testing in a Linear Probability Space",
        "authors": [
            "Christopher M Rembold"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Imagine that you could calculate of posttest probabilities, i.e. Bayes\ntheorem with simple addition. This is possible if we stop thinking of\nprobabilities as ranging from 0 to 1.0. There is a naturally occurring linear\nprobability space when data are transformed into the logarithm of the odds\nratio (log10 odds). In this space, probabilities are replaced by W (Weight)\nwhere W=log10(probability/(1-probability)). I would like to argue the multiple\nbenefits of performing statistical testing in a linear probability space: 1)\nStatistical testing is accurate in linear probability space but not in other\nspaces. 2) Effect size is called Impact (I) and is the difference in means\nbetween two treatments (I=Wmean2-Wmean1). 3) Bayes theorem is simply\nWposttest=Wpretest+Itest. 4) Significance (p value) is replaced by Certainty\n(C) which is the W of the p value. Methods to transform data into and out of\nlinear probability space are described.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.01491v1"
    },
    {
        "title": "Data-driven discovery of coordinates and governing equations",
        "authors": [
            "Kathleen Champion",
            "Bethany Lusch",
            "J. Nathan Kutz",
            "Steven L. Brunton"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The discovery of governing equations from scientific data has the potential\nto transform data-rich fields that lack well-characterized quantitative\ndescriptions. Advances in sparse regression are currently enabling the\ntractable identification of both the structure and parameters of a nonlinear\ndynamical system from data. The resulting models have the fewest terms\nnecessary to describe the dynamics, balancing model complexity with descriptive\nability, and thus promoting interpretability and generalizability. This\nprovides an algorithmic approach to Occam's razor for model discovery. However,\nthis approach fundamentally relies on an effective coordinate system in which\nthe dynamics have a simple representation. In this work, we design a custom\nautoencoder to discover a coordinate transformation into a reduced space where\nthe dynamics may be sparsely represented. Thus, we simultaneously learn the\ngoverning equations and the associated coordinate system. We demonstrate this\napproach on several example high-dimensional dynamical systems with\nlow-dimensional behavior. The resulting modeling framework combines the\nstrengths of deep neural networks for flexible representation and sparse\nidentification of nonlinear dynamics (SINDy) for parsimonious models. It is the\nfirst method of its kind to place the discovery of coordinates and models on an\nequal footing.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.02107v2"
    },
    {
        "title": "The Contribution Plot: Decomposition and Graphical Display of the RV\n  Coefficient, with Application to Genetic and Brain Imaging Biomarkers of\n  Alzheimer's Disease",
        "authors": [
            "JinCheol Choi",
            "Donghuan Lu",
            "Mirza Faisal Beg",
            "Jinko Graham",
            "Brad McNeney"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Alzheimer's disease (AD) is a chronic neurodegenerative disease that causes\nmemory loss and decline in cognitive abilities. AD is the sixth leading cause\nof death in the United States, affecting an estimated 5 million Americans. To\nassess the association between multiple genetic variants and multiple\nmeasurements of structural changes in the brain a recent study of AD used a\nmultivariate measure of linear dependence, the RV coefficient. The authors\ndecomposed the RV coefficient into contributions from individual variants and\ndisplayed these contributions graphically. We investigate the properties of\nsuch a `contribution plot' in terms of an underlying linear model, and discuss\nestimation of the components of the plot when the correlation signal may be\nsparse. The contribution plot is applied to simulated data and to genomic and\nbrain imaging data from the Alzheimer's Disease Neuroimaging Initiative.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.04330v1"
    },
    {
        "title": "Statistical witchhunts: Science, justice & the p-value crisis",
        "authors": [
            "Spencer Wheatley",
            "Didier Sornette"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  We provide accessible insight into the current 'replication crisis' in\n'statistical science', by revisiting the old metaphor of 'court trial as\nhypothesis test'. Inter alia, we define and diagnose harmful statistical\nwitch-hunting both in justice and science, which extends to the replication\ncrisis itself, where a hunt on p-values is currently underway.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05662v2"
    },
    {
        "title": "Governance on Social Media Data: Different Focuses between Government\n  and Internet Company",
        "authors": [
            "Wenting Yu",
            "Fei Shen",
            "Chen Min"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  How governments and Internet companies regulate user data on social media\nattracts public attention. This study tried to answer two questions: What kind\nof countries send more requests for Facebook user data? What kind of countries\nget more requests replies from Facebook? We aim to figure out how a country's\neconomic, political and social factors affect its government requests for user\ndata and Facebook's responses rate to those requests. Results show that\ncountries with higher GDP per capita, a higher level of human freedom and a\nlower level of rule of law send more requests for user data; while Facebook\ntends to reply to government requests from countries with a higher level of\nhuman freedom and a lower level of political stability. In conclusion,\ngovernments and Facebook show different focuses on governance on social media\ndata.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.11242v1"
    },
    {
        "title": "Methods of Estimation for the Three-Parameter Reflected Weibull\n  Distribution",
        "authors": [
            "Fateme Maleki Jebeli",
            "Einolah Deiri"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  In this paper, we propose methods for the estimation of parameters for the\nthree-parameter Reflected Weibull distribution. The Moment estimator , Maximum\nlikelihood estimator and Location and Scale Parameters free maximum likelihood\nestimator. The Location and Scale Parameters free maximum likelihood estimator\nis based on a data transformation, which avoids the problem of unbounded\nlikelihood estimator. Through Mont Carlo simulations, we further show that the\nLocation and Scale Parameters free maximum likelihood estimator performs better\nthan methods moment and maximum likelihood estimator in terms of bias and root\nmean squared error. Finally, two examples based on real data sets are presented\nto illustrate methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.13207v1"
    },
    {
        "title": "Fault Diagnosis using Clustering. What Statistical Test to use for\n  Hypothesis Testing?",
        "authors": [
            "Nagdev Amruthnath",
            "Tarun Gupta"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Predictive maintenance and condition-based monitoring systems have seen\nsignificant prominence in recent years to minimize the impact of machine\ndowntime on production and its costs. Predictive maintenance involves using\nconcepts of data mining, statistics, and machine learning to build models that\nare capable of performing early fault detection, diagnosing the faults and\npredicting the time to failure. Fault diagnosis has been one of the core areas\nwhere the actual failure mode of the machine is identified. In fluctuating\nenvironments such as manufacturing, clustering techniques have proved to be\nmore reliable compared to supervised learning methods. One of the fundamental\nchallenges of clustering is developing a test hypothesis and choosing an\nappropriate statistical test for hypothesis testing. Most statistical analyses\nuse some underlying assumptions of the data which most real-world data is\nincapable of satisfying those assumptions. This paper is dedicated to\novercoming the following challenge by developing a test hypothesis for fault\ndiagnosis application using clustering technique and performing PERMANOVA test\nfor hypothesis testing.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.13365v1"
    },
    {
        "title": "A Proof of First Digit Law from Laplace Transform",
        "authors": [
            "Mingshu Cong",
            "Bo-Qiang Ma"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The first digit law, also known as Benford's law or the significant digit\nlaw, is an empirical phenomenon that the leading digit of numbers from real\nworld sources favors small ones in a form $\\log(1+{1}/{d})$, where $d=1, 2,\n..., 9$. Such a law keeps elusive for over one hundred years because it was\nobscure whether this law is due to the logical consequence of the number system\nor some mysterious mechanism of the nature. We provide a simple and elegant\nproof of this law from the application of the Laplace transform, which is an\nimportant tool of mathematical methods in physics. We reveal that the first\ndigit law is originated from the basic property of the number system, thus it\nshould be attributed as a basic mathematical knowledge for wide applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04670v1"
    },
    {
        "title": "Karl Pearson and the Logic of Science: Renouncing Causal Understanding\n  (the Bride) and Inverted Spinozism",
        "authors": [
            "Julio Michael Stern"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Karl Pearson is the leading figure of XX century statistics. He and his\nco-workers crafted the core of the theory, methods and language of frequentist\nor classical statistics -- the prevalent inductive logic of contemporary\nscience. However, before working in statistics, K.Pearson had other interests\nin life, namely, in this order, philosophy, physics, and biological heredity.\nKey concepts of his philosophical and epistemological system of anti-Spinozism\n(a form of transcendental idealism) are carried over to his subsequent works on\nthe logic of scientific discovery. This article's main goal is to analyze\nK.Pearson early philosophical and theological ideas and to investigate how the\nsame ideas came to influence contemporary science, either directly or\nindirectly -- by the use of variant theories, methods and dialects of\nstatistics, corresponding to variant statistical inference procedures and their\nspecific belief calculi.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.06346v1"
    },
    {
        "title": "The maximum likelihood degree of a chemical reaction at the equilibrium",
        "authors": [
            "Simone Camosso"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The complexity of a maximum likelihood estimation is measured by its maximum\nlikelihood degree ($ML$ degree). In this paper we study the maximum likelihood\nproblem associated to chemical networks composed by one single chemical\nreaction under the equilibrium assumption.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.01219v1"
    },
    {
        "title": "A new approach of chain sampling inspection plan",
        "authors": [
            "Harsh Tripathi",
            "Mahendra Saha"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  To develop decision rules regarding acceptance or rejection of production\nlots based on sample data is the purpose of acceptance sampling inspection\nplan. Dependent sampling procedures cumulate results from several preceding\nproduction lots when testing is expensive or destructive. This chaining of past\nlots reduce the sizes of the required samples, essential for acceptance or\nrejection of production lots. In this article, a new approach for chaining the\npast lot(s) results proposed, named as modified chain group acceptance sampling\ninspection plan, requires a smaller sample size than the commonly used sampling\ninspection plan, such as group acceptance sampling inspection plan and single\nacceptance sampling inspection plan. A comparison study has been done between\nthe proposed and group acceptance sampling inspection plan as well as single\nacceptance sampling inspection plan. A example has been given to illustrate the\nproposed plan in a good manner.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.03784v2"
    },
    {
        "title": "Data Science in Biomedicine",
        "authors": [
            "Yovaninna Alarcón-Soto",
            "Jenifer Espasandín-Domínguez",
            "Ipek Guler",
            "Mercedes Conde-Amboage",
            "Francisco Gude-Sampedro",
            "Klaus Langohr",
            "Carmen Cadarso-Suárez",
            "Guadalupe Gómez-Melis"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  We highlight the role of Data Science in Biomedicine. Our manuscript goes\nfrom the general to the particular, presenting a global definition of Data\nScience and showing the trend for this discipline together with the terms of\ncloud computing and big data. In addition, since Data Science is mostly related\nto areas like economy or business, we describe its importance in biomedicine.\nBiomedical Data Science (BDS) presents the challenge of dealing with data\ncoming from a range of biological and medical research, focusing on\nmethodologies to advance the biomedical science discoveries, in an\ninterdisciplinary context.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.04486v1"
    },
    {
        "title": "Justifying the Norms of Inductive Inference",
        "authors": [
            "Olav Benjamin Vassend"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Bayesian inference is limited in scope because it cannot be applied in\nidealized contexts where none of the hypotheses under consideration is true and\nbecause it is committed to always using the likelihood as a measure of\nevidential favoring, even when that is inappropriate. The purpose of this paper\nis to study inductive inference in a very general setting where finding the\ntruth is not necessarily the goal and where the measure of evidential favoring\nis not necessarily the likelihood. I use an accuracy argument to argue for\nprobabilism and I develop a new kind of argument to argue for two general\nupdating rules, both of which are reasonable in different contexts. One of the\nupdating rules has standard Bayesian updating, Bissiri et al's (2016) general\nBayesian updating, Douven's (2016) IBE-based updating, and Vassend's (2019a)\nquasi-Bayesian updating as special cases. The other updating rule is novel.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.06523v1"
    },
    {
        "title": "An introduction to Bent Jorgensen's ideas",
        "authors": [
            "Gauss M. Cordeiro",
            "Rodrigo Labouriau",
            "Denise A. Botter"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  We briefly expose some key aspects of the theory and use of dispersion\nmodels, for which Bent Jorgensen played a crucial role as a driving force and\nan inspiration source. Starting with the general notion of dispersion models,\nbuilt using minimalistic mathematical assumptions, we specialize in two classes\nof families of distributions with different statistical flavors: exponential\ndispersion and proper dispersion models. The construction of dispersion models\ninvolves the solution of integral equations that are, in general, untractable.\nThese difficulties disappear when a more mathematical structure is assumed: it\nreduces to the calculation of a moment generating function or of a\nRiemann-Stieltjes integral for the exponential dispersion and the proper\ndispersion models, respectively. A new technique for constructing dispersion\nmodels based on characteristic functions is introduced turning the integral\nequations above into a tractable convolution equation and yielding examples of\ndispersion models that are neither proper dispersion nor exponential dispersion\nmodels. A corollary is that the cardinality of regular and non-regular\ndispersion models are both large.\n  Some selected applications are discussed including exponential families\nnon-linear models (for which generalized linear models are particular cases)\nand several models for clustered and dependent data based on a latent Levy\nprocess.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.09155v3"
    },
    {
        "title": "Causal screening for dynamical systems",
        "authors": [
            "Søren Wengel Mogensen"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Many classical algorithms output graphical representations of causal\nstructures by testing conditional independence among a set of random variables.\nIn dynamical systems, local independence can be used analogously as a testable\nimplication of the underlying data-generating process. We suggest some\ninexpensive methods for causal screening which provide output with a sound\ncausal interpretation under the assumption of ancestral faithfulness. The\npopular model class of linear Hawkes processes is used to provide an example of\na dynamical causal model. We argue that for sparse causal graphs the output\nwill often be close to complete. We give examples of this framework and apply\nit to a challenging biological system.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.13186v2"
    },
    {
        "title": "Think-aloud interviews: A tool for exploring student statistical\n  reasoning",
        "authors": [
            "Alex Reinhart",
            "Ciaran Evans",
            "Amanda Luby",
            "Josue Orellana",
            "Mikaela Meyer",
            "Jerzy Wieczorek",
            "Peter Elliott",
            "Philipp Burckhardt",
            "Rebecca Nugent"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Think-aloud interviews have been a valuable but underused tool in statistics\neducation research. Think-alouds, in which students narrate their reasoning in\nreal time while solving problems, differ in important ways from other types of\ncognitive interviews and related education research methods. Beyond the uses\nalready found in the statistics literature -- mostly validating the wording of\nstatistical concept inventory questions and studying student misconceptions --\nwe suggest other possible use cases for think-alouds and summarize\nbest-practice guidelines for designing think-aloud interview studies. Using\nexamples from our own experiences studying the local student body for our\nintroductory statistics courses, we illustrate how research goals should inform\nstudy-design decisions and what kinds of insights think-alouds can provide. We\nhope that our overview of think-alouds encourages more statistics educators and\nresearchers to begin using this method.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00535v5"
    },
    {
        "title": "The Scenario Culture",
        "authors": [
            "Edward Wheatcroft",
            "Henry Wynn",
            "Chris J. Dent",
            "Jim Q. Smith",
            "Claire L. Copeland",
            "Daniel Ralph",
            "Stan Zachary"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Scenario Analysis is a risk assessment tool that aims to evaluate the impact\nof a small number of distinct plausible future scenarios. In this paper, we\nprovide an overview of important aspects of Scenario Analysis including when it\nis appropriate, the design of scenarios, uncertainty and encouraging\ncreativity. Each of these issues is discussed in the context of climate, energy\nand legal scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.13170v1"
    },
    {
        "title": "Fourier Analysis and Benford Random Variables",
        "authors": [
            "Frank Benford"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  This paper has several major purposes. The central purpose is to describe the\n\"Benford analysis\" of a positive random variable and to summarize some results\nfrom investigations into base dependence of Benford random variables. The\nprincipal tools used to derive these results are Fourier series and Fourier\ntransforms, and a second major purpose of this paper is to present an\nintroductory exposition about these tools. My motivation for writing this paper\nis twofold. First, I think the theory of Benford random variables and the\nBenford analysis of a positive random variable are interesting and deserve to\nbe better known. Second, I think that Benford analysis provides a really\nexcellent illustration of the utility of Fourier series and transforms, and\nreveals certain interconnections between series and transforms that are not\nobvious from the usual way these subjects are introduced.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.07136v3"
    },
    {
        "title": "Bringing Visual Inference to the Classroom",
        "authors": [
            "Adam Loy"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  In the classroom, we traditionally visualize inferential concepts using\nstatic graphics or interactive apps. For example, there is a long history of\nusing apps to visualize sampling distributions. Recent developments in\nstatistical graphics have created an opportunity to bring additional\nvisualizations into the classroom to hone student understanding. Specifically,\nthe lineup protocol for visual inference provides a framework for students see\nthe difference between signal and noise by embedding a plot of observed data in\na field of null (noise) plots. Lineups have proven valuable in visualizing\nrandomization/permutation tests, diagnosing models, and even conducting valid\ninference when distributional assumptions break down. This paper provides an\noverview of how the lineup protocol for visual inference can be used to hone\nunderstanding of key statistical topics throughout the statistics curricula.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.11345v1"
    },
    {
        "title": "Demographic perspectives in research on global environmental change",
        "authors": [
            "Raya Muttarak"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Human population is at the centre of research on global environmental change.\nOn the one hand, population dynamics influence the environment and the global\nclimate system through consumption-based carbon emissions. On the other hand,\nhealth and wellbeing of the population is already being affected by climate\nchange. The knowledge on population dynamics and population heterogeneity thus\nis fundamental in improving our understanding of how population size,\ncomposition and distribution influence global environmental change and how\nthese changes affect subgroups of population differentially by demographic\ncharacteristics and spatial distribution. Existing theoretical concepts and\nmethodological tools in demography can be readily applied to the study of\npopulation and global environmental change. In the past couple of decades,\ndemographic research has enriched climate change research both in the analysis\nof the impact of population dynamics on the global climate system as well as\nthe impact of climate change on human population. What is missing in the\nliterature is the study that investigates how global environmental change\naffect current and future demographic processes and consequently population\ntrends. If global environmental change does influence fertility, mortality and\nmigration, the three key demographic components underlying population change,\npopulation estimates and forecast need to adjust from the climate feedback in\npopulation projections. Indisputably, this is the new area of research that\ndirectly requires expertise in population science and contribution from\ndemographers.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.00757v1"
    },
    {
        "title": "A Text Mining Discovery of Similarities and Dissimilarities Among Sacred\n  Scriptures",
        "authors": [
            "Younous Mofenjou Peuriekeu",
            "Victoire Djimna Noyum",
            "Cyrille Feudjio",
            "Alkan Goktug",
            "Ernest Fokoue"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  The careful examination of sacred texts gives valuable insights into human\npsychology, different ideas regarding the organization of societies as well as\ninto terms like truth and God. To improve and deepen our understanding of\nsacred texts, their comparison, and their separation is crucial. For this\npurpose, we use our data set has nine sacred scriptures. This work deals with\nthe separation of the Quran, the Asian scriptures Tao-Te-Ching, the Buddhism,\nthe Yogasutras, and the Upanishads as well as the four books from the Bible,\nnamely the Book of Proverbs, the Book of Ecclesiastes, the Book of\nEcclesiasticus, and the Book of Wisdom. These scriptures are analyzed based on\nthe natural language processing NLP creating the mathematical representation of\nthe corpus in terms of frequencies called document term matrix (DTM). After\nthis analysis, machine learning methods like supervised and unsupervised\nlearning are applied to perform classification. Here we use the Multinomial\nNaive Bayes (MNB), the Super Vector Machine (SVM), the Random Forest (RF), and\nthe K-nearest Neighbors (KNN). We obtain that among these methods MNB is able\nto predict the class of a sacred text with an accuracy of about 85.84 %.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.04421v1"
    },
    {
        "title": "Health and Demographic Surveillance Systems and the 2030 Agenda:\n  Sustainable Development Goals",
        "authors": [
            "Samuel J. Clark"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  The health and demographic surveillance system (HDSS) is an old method for\nintensively monitoring a population to assess the effects of healthcare or\nother population-level interventions - often clinical trials. The strengths of\nHDSS include very detailed descriptions of whole populations with frequent\nupdates. This often provides long time series of accurate population and health\nindicators for the HDSS study population. The primary weakness of HDSS is that\nthe data describe only the HDSS study population and cannot be generalized\nbeyond that.\n  The 2030 agenda is the ecosystem of activities - many including\npopulation-level monitoring - that relate to the United Nations (UN)\nSustainable Development Goals (SDG). With respect to the 2030 agenda, HDSS can\ncontribute by: continuing to conduct cause-and-effect studies; contributing to\ndata triangulation or amalgamation initiatives; characterizing the bias in and\ncalibrating 'big data'; and contributing more to the rapid training of\ndata-oriented professionals, especially in the population and health fields.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03910v1"
    },
    {
        "title": "The case for balanced hypothesis tests and equal-tailed confidence\n  intervals",
        "authors": [
            "André Gillibert",
            "Jacques Bénichou",
            "Bruno Falissard"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Introduction: there is an ongoing debate about directional inference of\ntwo-sided hypothesis tests for which some authors argue that rejecting $\\theta\n= \\theta_0$ does not allow to conclude that $\\theta > \\theta_0$ or $\\theta <\n\\theta_0$ but only that $\\theta \\neq \\theta_0$, while others argue that this is\na minor error without practical consequence.\n  Discussion: new elements are brought to the debate. It is shown that the\ndirectional interpretation of some non-directional hypothesis tests about\nReceiver Operating Characteristic (ROC) and survival curves may lead to\ninflated type III error rates with a probability of concluding that a\ndifference exists in the opposite side of the actual difference that can reach\n50% in the worst case. Some of the issues of directional tests also apply to\ntwo-sided confidence intervals (CIs). It is shown that equal-tailed CIs should\nbe preferred to shortest CIs. New assessment criteria of two-sided CIs and\nhypothesis tests are proposed to provide a reliable directional interpretation:\npartial left-sided and right-sided $\\alpha$ error rates for hypothesis tests,\nprobabilities of overestimation and underestimation $\\alpha_L$ and $\\alpha_U$\nand interval half-widths for two-sided CIs.\n  Conclusion: two-sided CIs and two-sided tests are interpreted directionally.\nThis implies that directional interpretation be taken in account in the\ndevelopment and evaluation of confidence intervals and tests.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.12581v1"
    },
    {
        "title": "A Review of Containerization for Interactive and Reproducible Analysis",
        "authors": [
            "Gregory J. Hunt",
            "Johann A. Gagnon-Bartsch"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  In recent decades the analysis of data has become increasingly computational.\nCorrespondingly, this has changed how scientific and statistical work is\nshared. For example, it is now commonplace for underlying analysis code and\ndata to be proffered alongside journal publications and conference talks.\nUnfortunately, sharing code faces several challenges. First, it is often\ndifficult to take code from one computer and run it on another. Code\nconfiguration, version, and dependency issues often make this challenging.\nSecondly, even if the code runs, it is often hard to understand or interact\nwith the analysis. This makes it difficult to assess the code and its findings,\nfor example, in a peer review process. In this review we describe the\ncombination of two computing technologies that help make analyses shareable,\ninteractive, and completely reproducible. These technologies are (1) analysis\ncontainerization, which leverages virtualization to fully encapsulate analysis,\ndata, code and dependencies into an interactive and shareable format, and (2)\ncode notebooks, a literate programming format for interacting with analyses.\nThe fusion of these two technologies offers significant advantages over using\neither individually. This review surveys how the combination enhances the\naccessibility and reproducibility of code, analyses, and ideas.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.16004v2"
    },
    {
        "title": "Matlab routines for centrality in directed acyclic graphs",
        "authors": [
            "Richard S. J. Tol"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  New Matlab functions for network centrality are introduced. Instead of the\nmean distance, the generalized mean distance is used. If closer relationships\nare prioritized, this closeness measure is also defined for unconnected graphs.\nInstead of distance to all nodes, distance to selected nodes is considered.\nBesides the vertical in- and out-closeness measures, horizontal cross-closeness\nis proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.07711v1"
    },
    {
        "title": "It is Time to Stop Teaching Frequentism to Non-statisticians",
        "authors": [
            "William M. Briggs"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  We should cease teaching frequentist statistics to undergraduates and switch\nto Bayes. Doing so will reduce the amount of confusion and over-certainty rife\namong users of statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.2590v1"
    },
    {
        "title": "Conditional quantile estimation through optimal quantization",
        "authors": [
            "Isabelle Charlier",
            "Davy Paindaveine",
            "Jérôme Saracco"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  In this paper, we use quantization to construct a nonparametric estimator of\nconditional quantiles of a scalar response $Y$ given a d-dimensional vector of\ncovariates $X$. First we focus on the population level and show how optimal\nquantization of $X$, which consists in discretizing $X$ by projecting it on an\nappropriate grid of $N$ points, allows to approximate conditional quantiles of\n$Y$ given $X$. We show that this is approximation is arbitrarily good as $N$\ngoes to infinity and provide a rate of convergence for the approximation error.\nThen we turn to the sample case and define an estimator of conditional\nquantiles based on quantization ideas. We prove that this estimator is\nconsistent for its fixed-$N$ population counterpart. The results are\nillustrated on a numerical example. Dominance of our estimators over local\nconstant/linear ones and nearest neighbor ones is demonstrated through\nextensive simulations in the companion paper Charlier et al.(2014b).\n",
        "pdf_link": "http://arxiv.org/pdf/1405.2781v1"
    },
    {
        "title": "The Sociotype, a New Conceptual Construct on Human Social Networks:\n  Application in Mental Health and Quality of Life",
        "authors": [
            "R. del Moral",
            "J. Navarro",
            "Y. Lopez-del Hoyo",
            "J. D. Gomez-Quintero",
            "J. Garcia-Campayo",
            "P. C. Marijuan"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  The present work discusses the pertinence of a 'sociotype' construct, both\ntheoretically and empirically oriented. The term, based on the conceptual chain\ngenotype-phenotype-sociotype, suggests an evolutionary preference in the human\nspecies for some determined averages of social relationships. This core pattern\nor 'sociotype' has been explored herein for the networking relationships of\nyoung people--165 university students filling in a 20-items questionnaire on\ntheir social interactions. In spite that this is a preliminary study,\ninteresting results have been obtained on gender conversation time, mental\nhealth, sociability level, and satisfaction with personal relationships. This\nsociotype hypothesis could be a timely enterprise for mental health and quality\nof life policies.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.4136v1"
    },
    {
        "title": "Using Board Games and Mathematica to Teach the Fundamentals of Finite\n  Stationary Markov Chains",
        "authors": [
            "Roger Bilisoly"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Markov chains are an important example for a course on stochastic processes\nbecause simple board games can be used to illustrate the fundamental concepts.\nFor example, a looping board game (like Monopoly) consists of all recurrent\nstates, and a game where players win by reaching a final square (like Chutes\nand Ladders) consists of all transient states except for the last one. With the\navailability of computer algebra packages, these games can be analyzed. For\nexample, the mean times in transient states and the stationary probabilities\nfor recurrent states are easily computed. This article analyzes some simple\nboard games with Mathematica, and indicates how this can be extended to more\ncomplex situations.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.1107v1"
    },
    {
        "title": "Data Science in Statistics Curricula: Preparing Students to \"Think with\n  Data\"",
        "authors": [
            "Johanna Hardin",
            "Roger Hoerl",
            "Nicholas J. Horton",
            "Deborah Nolan"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  A growing number of students are completing undergraduate degrees in\nstatistics and entering the workforce as data analysts. In these positions,\nthey are expected to understand how to utilize databases and other data\nwarehouses, scrape data from Internet sources, program solutions to complex\nproblems in multiple languages, and think algorithmically as well as\nstatistically. These data science topics have not traditionally been a major\ncomponent of undergraduate programs in statistics. Consequently, a curricular\nshift is needed to address additional learning outcomes. The goal of this paper\nis to motivate the importance of data science proficiency and to provide\nexamples and resources for instructors to implement data science in their own\nstatistics curricula. We provide case studies from seven institutions. These\nvaried approaches to teaching data science demonstrate curricular innovations\nto address new needs. Also included here are examples of assignments designed\nfor courses that foster engagement of undergraduates with data and data\nscience.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.3127v3"
    },
    {
        "title": "A Conversation with Howell Tong",
        "authors": [
            "Kung-Sik Chan",
            "Qiwei Yao"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  The following conversation is partly based on an interview that took place in\nthe Hong Kong University of Science and Technology in July 2013.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4515v1"
    },
    {
        "title": "Precinct Size Matters - The Large Precinct Bias in US Presidential\n  Elections",
        "authors": [
            "Glenn Webb"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Examination of precinct level data in US presidential elections reveals a\ncorrelation of large precincts and increased fraction of Republican votes. The\nlarge precinct bias is analyzed with respect to voter heterogeneity and voter\ninconvenience as precinct size increases. The analysis shows that voter\ninconvenience is a significant factor in election outcomes in certain states,\nand may significantly disadvantage Democratic candidates.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.8868v1"
    },
    {
        "title": "The Compass for Statistical Researchers",
        "authors": [
            "Daniele Durante",
            "Davide Vidotto",
            "Sabrina Vettori"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  We have hiked many miles alongside several professors as we traversed our\nstatistical path -- a regime switching trail which changed direction following\na class on the foundations of our discipline. As we play the game of research\nin that limbo between student and academic, one thing among Prof. Bernardi's\nteachings has never been more clear: to draw a route in the research map you\nnot only need to know your destination, but you must also understand where you\nare and how you arrived there.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.01908v1"
    },
    {
        "title": "Robust Hypothesis Testing with $α$-Divergence",
        "authors": [
            "Gökhan Gül",
            "Abdelhak M. Zoubir"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  A robust minimax test for two composite hypotheses, which are determined by\nthe neighborhoods of two nominal distributions with respect to a set of\ndistances - called $\\alpha-$divergence distances, is proposed. Sion's minimax\ntheorem is adopted to characterize the saddle value condition. Least favorable\ndistributions, the robust decision rule and the robust likelihood ratio test\nare derived. If the nominal probability distributions satisfy a symmetry\ncondition, the design procedure is shown to be simplified considerably. The\nparameters controlling the degree of robustness are bounded from above and the\nbounds are shown to be resulting from a solution of a set of equations. The\nsimulations performed evaluate and exemplify the theoretical derivations.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.05019v3"
    },
    {
        "title": "Impugning Randomness, Convincingly",
        "authors": [
            "Yuri Gurevich",
            "Grant Olney Passmore"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  John organized a state lottery and his wife won the main prize. You may feel\nthat the event of her winning wasn't particularly random, but how would you\nargue that in a fair court of law? Traditional probability theory does not even\nhave the notion of random events. Algorithmic information theory does, but it\nis not applicable to real-world scenarios like the lottery one. We attempt to\nrectify that.\n",
        "pdf_link": "http://arxiv.org/pdf/1601.00665v1"
    },
    {
        "title": "Mandelbrot's 1/f fractional renewal models of 1963-67: The non-ergodic\n  missing link between change points and long range dependence",
        "authors": [
            "Nicholas Wynn Watkins"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  The problem of 1/f noise has been with us for about a century. Because it is\nso often framed in Fourier spectral language, the most famous solutions have\ntended to be the stationary long range dependent (LRD) models such as\nMandelbrot's fractional Gaussian noise. In view of the increasing importance to\nphysics of non-ergodic fractional renewal models, I present preliminary results\nof my research into the history of Mandelbrot's very little known work in that\narea from 1963-67. I speculate about how the lack of awareness of this work in\nthe physics and statistics communities may have affected the development of\ncomplexity science, and I discuss the differences between the Hurst effect, 1/f\nnoise and LRD, concepts which are often treated as equivalent.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.00738v1"
    },
    {
        "title": "On point processes in multitarget tracking",
        "authors": [
            "Ronald Mahler"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  The finite-set statistics (FISST) approach to multitarget tracking was\nintroduced in the mid-1990s. Its current extended form dates from 2001. In\n2008, an \"elementary\" alternative to FISST was proposed, based on \"finite point\nprocesses\" rather than RFS's. This was accompanied by single-sensor and\nmultisensor versions of a claimed generalization of the PHD filter, the\n\"iFilter.\" Then in 2013 in the Journal of Advances in Information Fusion (JAIF)\nand elsewhere, the same author went on to claim that the FISST\np.g.fl./functional derivative approach is actually \"due to\" (a \"corollary\" of)\na 50-year-old pure-mathematics paper by Moyal; and described a \"point process\"\np.g.fl./functional derivative approach to multitarget tracking supposedly based\non it. In this paper it is shown that: (1)non-RFS point processes are a\nphenomenologically erroneous foundation for multitarget tracking; (2) nearly\nevery equation, concept, discussion, derivation, and methodology in the JAIF\npaper originally appeared in FISST publications, without being so attributed;\n(3) FISST cannot possibly be \"due to Moyal\"; (4) the \"point process\" approach\ndescribed in JAIF differs from FISST only in regard to terminology and\nnotation, and thus in this sense appears to be an obscured, phenomenologically\nerroneous, and improperly attributed copy of FISST. It is also shown that the\nderivations of the single-sensor and multisensory iFilter appear to have had\nmajor errors, as did a subsequent recasting of the multisensor iFilter as a\n\"traffic mapping filter.\"\n",
        "pdf_link": "http://arxiv.org/pdf/1603.02373v2"
    },
    {
        "title": "Dynamic Data in the Statistics Classroom",
        "authors": [
            "Johanna Hardin"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  The call for using real data in the classroom has long meant using datasets\nwhich are culled, cleaned, and wrangled prior to any student working with the\nobservations. However, an important part of teaching statistics should include\nactually retrieving data from the Internet. Nowadays, there are many different\nsources of data that are continually updated by the organization hosting the\ndata website. The R tools to download such dynamic data have improved in such a\nway to make accessing the data possible even in an introductory statistics\nclass. We provide five full analyses on dynamic data as well as an additional\nnine sources of dynamic data that can be brought into the classroom. The goal\nof our work is to demonstrate that using dynamic data can have a short learning\ncurve, even for introductory students or faculty unfamiliar with the landscape.\nThe examples provided are unlikely to create expert data scrapers, but they\nshould help motivate students and faculty toward more engaged use of online\ndata sources.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.04912v4"
    },
    {
        "title": "Reduced Perplexity: A simplified perspective on assessing probabilistic\n  forecasts",
        "authors": [
            "Kenric P. Nelson"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  A simple, intuitive approach to the assessment of probabilistic inferences is\nintroduced. The Shannon information metrics are translated to the probability\ndomain. The translation shows that the negative logarithmic score and the\ngeometric mean are equivalent measures of the accuracy of a probabilistic\ninference. Thus there is both a quantitative reduction in perplexity, which is\nthe inverse of the geometric mean of the probabilities, as good inference\nalgorithms reduce the uncertainty and a qualitative reduction due to the\nincreased clarity between the original set of probabilistic forecasts and their\ncentral tendency, the geometric mean. Further insight is provided by showing\nthat the R\\'enyi and Tsallis entropy functions translated to the probability\ndomain are both the weighted generalized mean of the distribution. The\ngeneralized mean of probabilistic forecasts forms a spectrum of performance\nmetrics referred to as a Risk Profile. The arithmetic mean is used to measure\nthe decisiveness, while the -2/3 mean is used to measure the robustness.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.08830v5"
    },
    {
        "title": "Understanding Convolutional Neural Networks",
        "authors": [
            "Jayanth Koushik"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Convoulutional Neural Networks (CNNs) exhibit extraordinary performance on a\nvariety of machine learning tasks. However, their mathematical properties and\nbehavior are quite poorly understood. There is some work, in the form of a\nframework, for analyzing the operations that they perform. The goal of this\nproject is to present key results from this theory, and provide intuition for\nwhy CNNs work.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.09081v1"
    },
    {
        "title": "Multiple Object Tracking in Unknown Backgrounds with Labeled Random\n  Finite Sets",
        "authors": [
            "Yuthika Punchihewa",
            "Ba-Tuong Vo",
            "Ba-Ngu Vo",
            "Du Yong Kim"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  This paper proposes an on-line multiple object tracking algorithm that can\noperate in unknown background. In a majority of multiple object tracking\napplications, model parameters for background processes such as clutter and\ndetection are unknown and vary with time, hence the ability of the algorithm to\nadaptively learn the these parameters is essential in practice. In this work,\nwe detail how the Generalized Labeled Multi Bernouli (GLMB) filter a tractable\nand provably Bayes optimal multi-object tracker can be tailored to learn\nclutter and detection parameters on the fly while tracking. Provided that these\nbackground model parameters do not fluctuate rapidly compared to the data rate,\nthe proposed algorithm can adapt to the unknown background yielding better\ntracking performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01584v3"
    },
    {
        "title": "Fiducial on a string",
        "authors": [
            "Gunnar Taraldsen",
            "Bo Henry Lindqvist"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  The fiducial argument of Fisher (1973) has been described as his biggest\nblunder, but the recent review of Hannig et al. (2016) demonstrates the current\nand increasing interest in this brilliant idea. This short note analyses an\nexample introduced by Seidenfeld (1992) where the fiducial distribution is\nrestricted to a string.\n  Keywords and phrases: Bayesian and fiducial inference, Restrictions on\nparameters, Uncertainty quantification, Epistemic probability, Statistics on a\nmanifold.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.03805v1"
    },
    {
        "title": "A network flow approach to visualising the roles of covariates in random\n  forests",
        "authors": [
            "Benjamin R. Fitzpatrick",
            "Kerrie Mengersen"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  We propose novel applications of parallel coordinates plots and Sankey\ndiagrams to represent the hierarchies of interacting covariate effects in\nrandom forests. Each visualisation summarises the frequencies of all of the\npaths through all of the trees in a random forest. Visualisations of the roles\nof covariates in random forests include: ranked bar or dot charts depicting\nscalar metrics of the contributions of individual covariates to the predictive\naccuracy of the random forest; line graphs depicting various summaries of the\neffect of varying a particular covariate on the predictions from the random\nforest; heatmaps of metrics of the strengths of interactions between all pairs\nof covariates; and parallel coordinates plots for each response class depicting\nthe distributions of the values of all covariates among the observations most\nrepresentative of those predicted to belong that class. Together these\nvisualisations facilitate substantial insights into the roles of covariates in\na random forest but do not communicate the frequencies of the hierarchies of\ncovariates effects across the random forest or the orders in which covariates\noccur in these hierarchies. Our visualisations address these gaps. We\ndemonstrate our visualisations using a random forest fitted to publicly\navailable data and provide a software implementation in the form of an R\npackage.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.08702v1"
    },
    {
        "title": "A Conversation with Jon Wellner",
        "authors": [
            "Moulinath Banerjee",
            "Richard J. Samworth"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Jon August Wellner was born in Portland, Oregon, in August 1945. He received\nhis Bachelor's degree from the University of Idaho in 1968 and his PhD degree\nfrom the University of Washington in 1975. From 1975 until 1983 he was an\nAssistant Professor and Associate Professor at the University of Rochester. In\n1983 he returned to the University of Washington, and has remained at the UW as\na faculty member since that time. Over the course of a long and distinguished\ncareer, Jon has made seminal contributions to a variety of areas including\nempirical processes, semiparametric theory, and shape-constrained inference,\nand has co-authored a number of extremely influential books. He has been\nhonored as the Le Cam lecturer by both the IMS (2015) and the French\nStatistical Society (2017). He is a Fellow of the IMS, the ASA, and the AAAS,\nand an elected member of the International Statistical Institute. He has served\nas co-Editor of Annals of Statistics (2001--2003) and Editor of Statistical\nScience (2010--2013), and President of IMS (2016--2017). In 2010 he was made a\nKnight of the Order of the Netherlands Lion. In his free time, Jon enjoys\nmountain climbing and backcountry skiing in the Cascades and British Columbia.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.05014v1"
    },
    {
        "title": "On the mathematics of the free-choice paradigm",
        "authors": [
            "Peter Selinger",
            "Kristopher Tapp"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Chen and Risen pointed out a logical flaw affecting the conclusions of a\nnumber of past experiments that used the free-choice paradigm to measure\nchoice-induced attitude change. They went on to design and implement a\nfree-choice experiment that used a novel type of control group in order to\navoid this logical pitfall. In this paper, we describe a method by which a\nfree-choice experiment can be correctly conducted even without a control group.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.06961v1"
    },
    {
        "title": "On Some Integral Means",
        "authors": [
            "Fariba Khoshnasib-Zeinabad",
            "Mohammadhossein Mehrabi"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Harmonic, Geometric, Arithmetic, Heronian and Contraharmonic means have been\nstudied by many mathematicians. In 2003, H. Evens studied these means from\ngeometrical point of view and established some of the inequalities between them\nin using a circle and its radius. In 1961, E. Beckenback and R. Bellman\nintroduced several inequalities corresponding to means. In this paper, we will\nintroduce the concept of mean functions and integral means and give bounds on\nsome of these mean functions and integral means.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.00377v2"
    },
    {
        "title": "Data scraping, ingestation, and modeling: bringing data from cars.com\n  into the intro stats class",
        "authors": [
            "Sarah McDonald",
            "Nicholas Jon Horton"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  New tools have made it much easier for students to develop skills to work\nwith interesting data sets as they begin to extract meaning from data. To fully\nappreciate the statistical analysis cycle, students benefit from repeated\nexperiences collecting, ingesting, wrangling, analyzing data and communicating\nresults. How can we bring such opportunities into the classroom? We describe a\nclassroom activity, originally developed by Danny Kaplan (Macalester College),\nin which students can expand upon statistical problem solving by hand-scraping\ndata from cars.com, ingesting these data into R, then carrying out analyses of\nthe relationships between price, mileage, and model year for a selected type of\ncar.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.02952v1"
    },
    {
        "title": "Game time: statistical contests in the classroom",
        "authors": [
            "Sam Doerken",
            "Martin Schumacher",
            "Franz Baumdicker"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  We describe a contest in variable selection which was part of a statistics\ncourse for graduate students. In particular, the possibility to create a\ncontest themselves offered an additional challenge for more advanced students.\nSince working with data is becoming more important in teaching statistics, we\ngreatly encourage other instructors to try the same.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04338v1"
    },
    {
        "title": "Perspective from the Literature on the Role of Expert Judgment in\n  Scientific and Statistical Research and Practice",
        "authors": [
            "Naomi C Brownstein"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  This article, produced as a result of the Symposium on Statistical Inference,\nis an introduction to the literature on the function of expertise, judgment,\nand choice in the practice of statistics and scientific research. In\nparticular, expert judgment plays a critical role in conducting Frequentist\nhypothesis tests and Bayesian models, especially in selection of appropriate\nprior distributions for model parameters. The subtlety of interpreting results\nis also discussed. Finally, external recommendations are collected for how to\nmore effectively encourage proper use of judgment in statistics. The paper\nsynthesizes the literature for the purpose of creating a single reference and\ninciting more productive discussions on how to improve the future of statistics\nand science.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.04721v1"
    },
    {
        "title": "Inter-Rater: Software for analysis of inter-rater reliability by\n  permutating pairs of multiple users",
        "authors": [
            "Daniel J. Arenas"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Inter-Rater quantifies the reliability between multiple raters who evaluate a\ngroup of subjects. It calculates the group quantity, Fleiss kappa, and it\nimproves on existing software by keeping information about each user and\nquantifying how each user agreed with the rest of the group. This is\naccomplished through permutations of user pairs. The software was written in\nPython, can be run in Linux, and the code is deposited in Zenodo and GitHub.\nThis software can be used for evaluation of inter-rater reliability in\nsystematic reviews, medical diagnosis algorithms, education applications, and\nothers.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05731v1"
    },
    {
        "title": "Benchmarking in cluster analysis: A white paper",
        "authors": [
            "Iven Van Mechelen",
            "Anne-Laure Boulesteix",
            "Rainer Dangl",
            "Nema Dean",
            "Isabelle Guyon",
            "Christian Hennig",
            "Friedrich Leisch",
            "Douglas Steinley"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Note: A revised version of this is now published. Please cite and read (it's\nopen access): Van Mechelen, I., Boulesteix, A.-L., Dangl, R., Dean, N., Hennig,\nC., Leisch, F., Steinley, D., Warrens, M. J. (2023). A white paper on good\nresearch practices in benchmarking: The case of cluster analysis. WIREs Data\nMining and Knowledge Discovery, e1511. https://doi.org/10.1002/widm.1511\n  To achieve scientific progress in terms of building a cumulative body of\nknowledge, careful attention to benchmarking is of the utmost importance. This\nmeans that proposals of new methods of data pre-processing, new data-analytic\ntechniques, and new methods of output post-processing, should be extensively\nand carefully compared with existing alternatives, and that existing methods\nshould be subjected to neutral comparison studies. To date, benchmarking and\nrecommendations for benchmarking have been frequently seen in the context of\nsupervised learning. Unfortunately, there has been a dearth of guidelines for\nbenchmarking in an unsupervised setting, with the area of clustering as an\nimportant subdomain. To address this problem, discussion is given to the\ntheoretical conceptual underpinnings of benchmarking in the field of cluster\nanalysis by means of simulated as well as empirical data. Subsequently, the\npracticalities of how to address benchmarking questions in clustering are dealt\nwith, and foundational recommendations are made.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10496v3"
    },
    {
        "title": "The ASCCR Frame for Learning Essential Collaboration Skills",
        "authors": [
            "Eric A. Vance",
            "Heather S. Smith"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Statistics and data science are especially collaborative disciplines that\ntypically require practitioners to interact with many different people or\ngroups. Consequently, interdisciplinary collaboration skills are part of the\npersonal and professional skills essential for success as an applied\nstatistician or data scientist. These skills are learnable and teachable, and\nlearning and improving collaboration skills provides a way to enhance one's\npractice of statistics and data science. To help individuals learn these skills\nand organizations to teach them, we have developed a framework covering five\nessential components of statistical collaboration: Attitude, Structure,\nContent, Communication, and Relationship. We call this the ASCCR Frame. This\nframework can be incorporated into formal training programs in the classroom or\non the job and can also be used by individuals through self-study. We show how\nthis framework can be applied specifically to statisticians and data scientists\nto improve their collaboration skills and their interdisciplinary impact. We\nbelieve that the ASCCR Frame can help organize and stimulate research and\nteaching in interdisciplinary collaboration and call on individuals and\norganizations to begin generating evidence regarding its effectiveness.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03578v5"
    },
    {
        "title": "Surrogate Modeling of Stochastic Functions - Application to\n  computational Electromagnetic Dosimetry",
        "authors": [
            "Soumaya Azzi",
            "Yuanyuan Huang",
            "Bruno Sudret",
            "Joe Wiart"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Metamodeling of complex numerical systems has recently attracted the interest\nof the mathematical programming community. Despite the progress in high\nperformance computing, simulations remain costly, as a matter of fact, the\nassessment of the exposure to radio frequency electromagnetic fields is\ncomputationally prohibitive since one simulation can require hours. Moreover,\nin many engineering problems, carrying out deterministic numerical operations\nwithout considering uncertainties can lead to unreliable designs. In this paper\nwe focus on the surrogate modeling of a particular type of computational models\ncalled stochastic simulators. In contrast to deterministic simulators which\nyield a unique output for each set of input parameters, stochastic simulators\ninherently contain some sources of randomness and the output at a given point\nis a probability density function. Characterizing the stochastic simulators is\neven more time consuming. This paper represents stochastic simulators as a\nstochastic process and describes a metamodeling approach based on the\nKarhunen-Lo\\`eve spectral decomposition.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04079v1"
    },
    {
        "title": "Openness and Reproducibility: Insights from a Model-Centric Approach",
        "authors": [
            "Bert Baumgaertner",
            "Berna Devezer",
            "Erkan O. Buzbas",
            "Luis G. Nardin"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  This paper investigates the conceptual relationship between openness and\nreproducibility using a model-centric approach, heavily informed by probability\ntheory and statistics. We first clarify the concepts of reliability,\nauditability, replicability, and reproducibility--each of which denotes a\npotential scientific objective. Then we advance a conceptual analysis to\ndelineate the relationship between open scientific practices and these\nobjectives. Using the notion of an idealized experiment, we identify which\ncomponents of an experiment need to be reported and which need to be repeated\nto achieve the relevant objective. The model-centric framework we propose aims\nto contribute precision and clarity to the discussions surrounding the\nso-called reproducibility crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04525v2"
    },
    {
        "title": "Batch Self Organizing maps for distributional data using adaptive\n  distances",
        "authors": [
            "Antonio Irpino",
            "Francisco De Carvalho",
            "Rosanna Verde",
            "Antonio Balzanella"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  The paper deals with a Batch Self Organizing Map algorithm (DBSOM) for data\ndescribed by distributional-valued variables. This kind of variables is\ncharacterized to take as values one-dimensional probability or frequency\ndistributions on a numeric support. The objective function optimized in the\nalgorithm depends on the choice of the distance measure. According to the\nnature of the date, the $L_2$ Wasserstein distance is proposed as one of the\nmost suitable metrics to compare distributions. It is widely used in several\ncontexts of analysis of distributional data. Conventional batch SOM algorithms\nconsider that all variables are equally important for the training of the SOM.\nHowever, it is well known that some variables are less relevant than others for\nthis task. In order to take into account the different contribution of the\nvariables we propose an adaptive version of the DBSOM algorithm that tackles\nthis problem with an additional step: a relevance weight is automatically\nlearned for each distributional-valued variable. Moreover, since the $L_2$\nWasserstein distance allows a decomposition into two components: one related to\nthe means and one related to the size and shape of the distributions, also\nrelevance weights are automatically learned for each of the measurement\ncomponents to emphasize the importance of the different estimated parameters of\nthe distributions. Examples of real and synthetic datasets of distributional\ndata illustrate the usefulness of the proposed DBSOM algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.06980v3"
    },
    {
        "title": "Quantifying Privacy in Nuclear Warhead Authentication Protocols",
        "authors": [
            "Ruaridh. R Macdonald",
            "R. Scott Kemp"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  International verification of nuclear warheads is a practical problem in\nwhich the protection of secret warhead information is of paramount importance.\nWe propose a measure that would enable a weapon owner to evaluate the privacy\nof a proposed protocol in a technology-neutral fashion. We show the problem is\nreducible to `natural' and `corrective' learning. The natural learning can be\ncomputed without assumptions about the inspector, while the corrective learning\naccounts for the inspector's prior knowledge. The natural learning provides the\nwarhead owner a useful lower bound on the information leaked by the proposed\nprotocol. Using numerical examples, we demonstrate that the proposed measure\ncorrelates better with the accuracy of a maximum a posteriori probability\nestimate than alternative measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10375v1"
    },
    {
        "title": "On some properties of the new Sine-skewed Cardioid Distribution",
        "authors": [
            "Cherif Mamadou Moctar Traoré",
            "Moumouni Diallo",
            "Gane Samb Lo",
            "Mouhamad Ahsanullah",
            "Okereke Lois Chinwendu"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  The new Sine Skewed Cardioid (ssc) distribution been just introduced and\ncharacterized by Ahsanullah (2018). Here, we study the asymptotic properties of\nits tails by determining its extreme value domain, the characteristic function,\nthe moments and likelihood estimators of the two parameters, the asymptotic\nnormality of the moments estimators and the random generation of data from the\n\\textit{ssc} distribution. Finally, we proceed to a simulation study to show\nthe performance of the random generation method and the quality of the moments\nestimation of the parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10603v1"
    },
    {
        "title": "Key Factor Not to Drop Out is to Attend the Lecture",
        "authors": [
            "Hideo Hirose"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  In addition to the learning check testing results performed at each lectures,\nwe have extended the factors to find the key dropping out factors. Among them\nare, the number of successes in the learning check testing, the number of\nattendances to the follow-up program classes, and etc. Then, we have found key\nfactors strongly related to the students at risk. They are the following. 1)\nBadly failed students (score range is 0-39 in the final examination) tend to be\nabsent for the regular classes and fail in the learning check testing even if\nthey attended, and they are very reluctant to attend the follow-up program\nclasses. 2) Successful students (score range is 60-100 in the final\nexamination) attend classes and get good scores in every learning check\ntesting. 3) Failed students but not so badly (score range is 40-59 in the final\nexamination) reveal both sides of features appeared in score range of 0-39 and\nscore range of 60-100. Therefore, it is crucial to attend the lectures in order\nnot to drop out. Students who failed in learning check testing more than half\nout of all testing times almost absolutely failed in the final examination,\nwhich could cause the drop out. Also, students who were successful to learning\ncheck testing more than two third out of all testing times took better score in\nthe final examination.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.03696v1"
    },
    {
        "title": "Robust Adaptive Control Charts",
        "authors": [
            "Gejza Dohnal"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  In statistical process control, procedures are applied that require\nrelatively strict conditions for their use. If such assumptions are violated,\nthese methods become inefficient, leading to increased incidence of false\nsignals. Therefore, a robust version of control charts is sought to be less\nsensitive with respect to a breach of normality and independence in\nmeasurements. Robust control charts, however, usually increase the delay in the\ndetection of assignable causes. This negative effect can, to some extent, be\nremoved with the aid of an adaptive approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.03701v1"
    },
    {
        "title": "Variability in the interpretation of Dutch probability phrases - a risk\n  for miscommunication",
        "authors": [
            "Sanne J. W. Willems",
            "Casper J. Albers",
            "Ionica Smeets"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Verbal probability phrases are often used to express estimated risk. In this\nstudy, focus was on the numerical interpretation of 29 Dutch probability and\nfrequency phrases, including several complementary phrases to test (a)symmetry\nin their interpretation. Many of these phrases had not been studied before. The\nphrases were presented in the context of ordinary situations. The survey was\ndistributed among both statisticians and non-statisticians with Dutch as their\nnative language.\n  The responses from 881 participants showed a large variability in the\ninterpretation of Dutch phrases, and the neutral contexts seemed to have no\nstructural influence. Furthermore, the results demonstrated an asymmetry in the\ninterpretation of Dutch complementary phrases. The large variability of\ninterpretations was found among both statisticians and non-statisticians, and\namong males and females, however, no structural differences were found between\nthe groups.\n  Concluding, there is a large variability in the interpretation of verbal\nprobability phrases, even within sub-populations. Therefore, verbal probability\nexpressions may be a risk for miscommunication.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.09686v1"
    },
    {
        "title": "Rediscovering a little known fact about the t-test and the F-test:\n  Algebraic, Geometric, Distributional and Graphical Considerations",
        "authors": [
            "Jennifer A. Sinnott",
            "Steven N. MacEachern",
            "Mario Peruggia"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  We discuss the role that the null hypothesis should play in the construction\nof a test statistic used to make a decision about that hypothesis. To construct\nthe test statistic for a point null hypothesis about a binomial proportion, a\ncommon recommendation is to act as if the null hypothesis is true. We argue\nthat, on the surface, the one-sample t-test of a point null hypothesis about a\nGaussian population mean does not appear to follow the recommendation. We show\nhow simple algebraic manipulations of the usual t-statistic lead to an\nequivalent test procedure consistent with the recommendation. We provide\ngeometric intuition regarding this equivalence and we consider extensions to\ntesting nested hypotheses in Gaussian linear models. We discuss an application\nto graphical residual diagnostics where the form of the test statistic makes a\npractical difference. By examining the formulation of the test statistic from\nmultiple perspectives in this familiar example, we provide simple, concrete\nillustrations of some important issues that can guide the formulation of\neffective solutions to more complex statistical problems.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.08703v5"
    },
    {
        "title": "A review of problem- and team-based methods for teaching statistics in\n  Higher Education",
        "authors": [
            "Elinor Jones",
            "Tom Palmer"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The teaching of statistics in higher education in the UK is still largely\nlecture-based. This is despite recommendations such as those given by the\nAmerican Statistical Association's GAISE report that more emphasis should be\nplaced on active learning strategies where students take more responsibility\nfor their own learning. One possible model is that of collaborative learning,\nwhere students learn in groups through carefully crafted `problems', which has\nlong been suggested as a strategy for teaching statistics.\n  In this article, we review two specific approaches that fall under the\ncollaborative learning model: problem- and team-based learning. We consider the\nevidence for changing to this model of teaching in statistics, as well as give\npractical suggestions on how this could be implemented in typical statistics\nclasses in Higher Education.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00256v3"
    },
    {
        "title": "A Step by Step Mathematical Derivation and Tutorial on Kalman Filters",
        "authors": [
            "Hamed Masnadi-Shirazi",
            "Alireza Masnadi-Shirazi",
            "Mohammad-Amir Dastgheib"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  We present a step by step mathematical derivation of the Kalman filter using\ntwo different approaches. First, we consider the orthogonal projection method\nby means of vector-space optimization. Second, we derive the Kalman filter\nusing Bayesian optimal filtering. We provide detailed proofs for both methods\nand each equation is expanded in detail.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03558v1"
    },
    {
        "title": "A Bayesian Statistics Course for Undergraduates: Bayesian Thinking,\n  Computing, and Research",
        "authors": [
            "Jingchen Hu"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  We propose a semester-long Bayesian statistics course for undergraduate\nstudents with calculus and probability background. We cultivate students'\nBayesian thinking with Bayesian methods applied to real data problems. We\nleverage modern Bayesian computing techniques not only for implementing\nBayesian methods, but also to deepen students' understanding of the methods.\nCollaborative case studies further enrich students' learning and provide\nexperience to solve open-ended applied problems. The course has an emphasis on\nundergraduate research, where accessible academic journal articles are read,\ndiscussed, and critiqued in class. With increased confidence and familiarity,\nstudents take the challenge of reading, implementing, and sometimes extending\nmethods in journal articles for their course projects.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.05818v3"
    },
    {
        "title": "baymedr: An R Package and Web Application for the Calculation of Bayes\n  Factors for Superiority, Equivalence, and Non-Inferiority Designs",
        "authors": [
            "Maximilian Linde",
            "Don van Ravenzwaaij"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Clinical trials often seek to determine the superiority, equivalence, or\nnon-inferiority of an experimental condition (e.g., a new drug) compared to a\ncontrol condition (e.g., a placebo or an already existing drug). The use of\nfrequentist statistical methods to analyze data for these types of designs is\nubiquitous even though they have several limitations. Bayesian inference\nremedies many of these shortcomings and allows for intuitive interpretations.\nIn this article, we outline the frequentist conceptualization of superiority,\nequivalence, and non-inferiority designs and discuss its disadvantages.\nSubsequently, we explain how Bayes factors can be used to compare the relative\nplausibility of competing hypotheses. We present baymedr, an R package and web\napplication, that provides user-friendly tools for the computation of Bayes\nfactors for superiority, equivalence, and non-inferiority designs. Instructions\non how to use baymedr are provided and an example illustrates how already\nexisting results can be reanalyzed with baymedr.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.11616v3"
    },
    {
        "title": "What Does the \"Mean\" Really Mean?",
        "authors": [
            "Nozer D. Singpurwalla",
            "Boya Lai"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  The arithmetic average of a collection of observed values of a homogeneous\ncollection of quantities is often taken to be the most representative\nobservation. There are several arguments supporting this choice the moment of\ninertia being the most familiar. But what does this mean?\n  In this note, we bring forth the Kolmogorov-Nagumo point of view that the\narithmetic average is a special case of a sequence of functions of a special\nkind, the quadratic and the geometric means being some of the other cases. The\nmedian fails to belong to this class of functions. The Kolmogorov-Nagumo\ninterpretation is the most defensible and the most definitive one for the\narithmetic average, but its essence boils down to the fact that this average is\nmerely an abstraction which has meaning only within its mathematical set-up.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.01973v1"
    },
    {
        "title": "Bernoulli Trials With Skewed Propensities for Certification and\n  Validation",
        "authors": [
            "Nozer D. Singpurwalla",
            "Boya Lai"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  The impetus for writing this paper are the well publicized media reports that\nsoftware failure was the cause of the two recent mishaps of the Boeing 737 Max\naircraft. The problem considered here though, is a specific one, in the sense\nthat it endeavors to address the general matter of conditions under which an\nitem such as a drug, a material specimen, or a complex, system can be certified\nfor use based on a large number of Bernoulli trials, all successful. More\nbroadly, the paper is an attempt to answer the old and honorable philosophical\nquestion, namely,\" when can empirical testing on its own validate a law of\nnature?\" Our message is that the answer depends on what one starts with,\nnamely, what is one's prior distribution, what unknown does this prior\ndistribution endow, and what has been observed as data.\n  The paper is expository in that it begins with a historical overview, and\nends with some new ideas and proposals for addressing the question posed. In\nthe sequel, it also articulates on Popper's notion of \"propensity\" and its role\nin providing a proper framework for Bayesian inference under Bernoulli trials,\nas well as the need to engage with posterior distributions that are\nsubjectively specified; that is, without a recourse to the usual Bayesian prior\nto posterior iteration.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03098v1"
    },
    {
        "title": "How and Why Did Probability Theory Come About?",
        "authors": [
            "Nozer D. Singpurwalla",
            "Boya Lai"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  This paper is a top down historical perspective on the several phases in the\ndevelopment of probability from its prehistoric origins to its modern day\nevolution, as one of the key methodologies in artificial intelligence, data\nscience, and machine learning. It is written in honor of Barry Arnold's\nbirthday for his many contributions to statistical theory and methodology.\nDespite the fact that much of Barry's work is technical, a descriptive document\nto mark his achievements should not be viewed as being out of line. Barry's\ndissertation adviser at Stanford (he received a Ph.D. in Statistics there) was\na philosopher of Science who dug deep in the foundations and roots of\nprobability, and it is this breadth of perspective is what Barry has inherent.\nThe paper is based on lecture materials compiled by the first author from\nvarious published sources, and over a long period of time. The material below\ngives a limited list of references, because the cast of characters is many, and\ntheir contributions are a part of the historical heritage of those of us who\nare interested in probability, statistics, and the many topics they have\nspawned.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03152v1"
    },
    {
        "title": "Bayes' Theorem under Conditional Independence",
        "authors": [
            "Jun Hu",
            "Xianggui Qu"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  In this article we provide a substantial discussion on the statistical\nconcept of conditional independence, which is not routinely mentioned in most\nelementary statistics and mathematical statistics textbooks. Under the\nassumption of conditional independence, an extended version of Bayes' Theorem\nis then proposed with illustrations from both hypothetical and real-world\nexamples of disease diagnosis.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03970v1"
    },
    {
        "title": "An R Autograder for PrairieLearn",
        "authors": [
            "Dirk Eddelbuettel",
            "Alton Barbehenn"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  We describe how we both use and extend the PrarieLearn framework by taking\nadvantage of its built-in support for external auto-graders. By using a custom\nDocker container, we can match our course requirements perfectly. Moreover, by\nrelying on the flexibility of the interface we can customize our Docker\ncontainer. A specific extension for unit testing is described which creates\ncontext-dependent difference between student answers and reference solution\nproviding a more comprehensive response at test time.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.06500v1"
    },
    {
        "title": "Ramsey's contributions to probability and legal theory",
        "authors": [
            "F. E. Guerra-Pujol"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Review of Cheryl Misak, Frank Ramsey: A Sheer Excess of Powers (Oxford\nUniversity Press, 2020).\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13518v2"
    },
    {
        "title": "Winning by hiding behind others: An analysis of speed skating data",
        "authors": [
            "Genki Ichinose",
            "Daiki Miyagawa",
            "Junji Ito",
            "Naoki Masuda"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  In some athletic races, such as cycling and types of speed skating races,\nathletes have to complete a relatively long distance at a high speed in the\npresence of direct opponents. To win such a race, athletes are motivated to\nhide behind others to suppress energy consumption before a final moment of the\nrace. This situation seems to produce a social dilemma: players want to hide\nbehind others, whereas if a group of players attempts to do so, they may all\nlose to other players that overtake them. To support that speed skaters are\ninvolved in such a social dilemma, we analyzed video footage data for 14 mass\nstart skating races to find that skaters that hid behind others to avoid air\nresistance for a long time before the final lap tended to win. Furthermore, the\nfinish rank of the skaters in mass start races was independent of the record of\nthe same skaters in time-trial races measured in the absence of direct\nopponents. The results suggest that how to strategically cope with a skater's\ndilemma may be a key determinant for winning long-distance and high-speed races\nwith direct opponents.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06464v2"
    },
    {
        "title": "Popper's falsification and corroboration from the statistical\n  perspectives",
        "authors": [
            "Youngjo Lee",
            "Yudi Pawitan"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  The role of probability appears unchallenged as the key measure of\nuncertainty, used among other things for practical induction in the empirical\nsciences. Yet, Popper was emphatic in his rejection of inductive probability\nand of the logical probability of hypotheses; furthermore, for him, the degree\nof corroboration cannot be a probability. Instead he proposed a deductive\nmethod of testing. In many ways this dialectic tension has many parallels in\nstatistics, with the Bayesians on logico-inductive side vs the non-Bayesians or\nthe frequentists on the other side. Simplistically Popper seems to be on the\nfrequentist side, but recent synthesis on the non-Bayesian side might direct\nthe Popperian views to a more nuanced destination. Logical probability seems\nperfectly suited to measure partial evidence or support, so what can we use if\nwe are to reject it? For the past 100 years, statisticians have also developed\na related concept called likelihood, which has played a central role in\nstatistical modelling and inference. Remarkably, this Fisherian concept of\nuncertainty is largely unknown or at least severely under-appreciated in\nnon-statistical literature. As a measure of corroboration, the likelihood\nsatisfies the Popperian requirement that it is not a probability. Our aim is to\nintroduce the likelihood and its recent extension via a discussion of two\nwell-known logical fallacies in order to highlight that its lack of recognition\nmay have led to unnecessary confusion in our discourse about falsification and\ncorroboration of hypotheses. We highlight the 100 years of development of\nlikelihood concepts. The year 2021 will mark the 100-year anniversary of the\nlikelihood, so with this paper we wish it a long life and increased\nappreciation in non-statistical literature.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00238v1"
    },
    {
        "title": "A Bayesian Redesign of the First Probability/Statistics Course",
        "authors": [
            "Jim Albert"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  The traditional calculus-based introduction to statistical inference consists\nof a semester of probability followed by a semester of frequentist inference.\nCobb (2015) challenges the statistical education community to rethink the\nundergraduate statistics curriculum. In particular, he suggests that we should\nfocus on two goals: making fundamental concepts accessible and minimizing\nprerequisites to research. Using five underlying principles of Cobb, we\ndescribe a new calculus-based introduction to statistics based on\nsimulation-based Bayesian computation.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.04180v1"
    },
    {
        "title": "The Effects of Vacant Lot Greening and the Impact of Land Use and\n  Business Presence on Crime",
        "authors": [
            "Jesse Cui",
            "Shane T. Jensen",
            "John Macdonald"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  We examine the effect of the Philadelphia LandCare (PLC) vacant lot greening\ninitiative on crime and the extent to which surrounding land uses and business\ntypes moderate this intervention. We rely on a propensity score matching\nanalysis to account for substantial differences in demographic, economic, land\nuse, and business characteristics between greened and ungreened vacant lots. We\nestimate larger and more significant crime reductions around vacant lots that\nare greened in our matched pairs analysis compared to unmatched analyses. The\neffects of vacant lot greening on crime are larger in areas with high\nresidential and low commercial land use and are moderated by the presence of\ndifferent types of nearby businesses.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.06065v2"
    },
    {
        "title": "Dynamics of ternary statistical experiments with equilibrium state",
        "authors": [
            "M. L. Bertotti",
            "S. O. Dovgyi",
            "D. Koroliouk"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  We study the scenarios of the dynamics of ternary statistical experiments,\nmodeled employing difference equations. The important features are a balance\ncondition and the existence of a steady-state (equilibrium). We give a\nclassification of scenarios of the model evolution which are significantly\ndifferent between them, depending on the domain of the values of the model\nbasic parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.06543v1"
    },
    {
        "title": "Incertitudes et mesures",
        "authors": [
            "Romain Legrand"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Educational guide focused on the statistical treatment of measurement\nuncertainties. The conditions of application of current practices are detailed\nand precised: mean values, central limit theorem, linear regression. The last\ntwo chapters are devoted to an introduction to the Bayesian inference and a\nseries of application cases: machine failure date, elimination of a background\nnoise, linear adjustment with elimination of outliers.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.06701v1"
    },
    {
        "title": "Reproducible Research: A Retrospective",
        "authors": [
            "Roger D. Peng",
            "Stephanie C. Hicks"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Rapid advances in computing technology over the past few decades have spurred\ntwo extraordinary phenomena in science: large-scale and high-throughput data\ncollection coupled with the creation and implementation of complex statistical\nalgorithms for data analysis. Together, these two phenomena have brought about\ntremendous advances in scientific discovery but have also raised two serious\nconcerns, one relatively new and one quite familiar. The complexity of modern\ndata analyses raises questions about the reproducibility of the analyses,\nmeaning the ability of independent analysts to re-create the results claimed by\nthe original authors using the original data and analysis techniques. While\nseemingly a straightforward concept, reproducibility of analyses is typically\nthwarted by the lack of availability of the data and computer code that were\nused in the analyses. A much more general concern is the replicability of\nscientific findings, which concerns the frequency with which scientific claims\nare confirmed by completely independent investigations. While the concepts of\nreproduciblity and replicability are related, it is worth noting that they are\nfocused on quite different goals and address different aspects of scientific\nprogress. In this review, we will discuss the origins of reproducible research,\ncharacterize the current status of reproduciblity in public health research,\nand connect reproduciblity to current concerns about replicability of\nscientific findings. Finally, we describe a path forward for improving both the\nreproducibility and replicability of public health research in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12210v1"
    },
    {
        "title": "Asymmetry approach to study for chemotherapy treatment and devices\n  failure times data using modified Power function distribution with some\n  modified estimators",
        "authors": [
            "Azam Zaka",
            "Ahmad Saeed Akhter",
            "Riffat Jabeen"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  In order to improve the already existing models that are used extensively in\nbio sciences and applied sciences research, a new class of Weighted Power\nfunction distribution (WPFD) has been proposed with its various properties and\ndifferent modifications to be more applicable in real life. We have provided\nthe mathematical derivations for the new distribution including moments,\nincomplete moments, conditional moments, inverse moments, mean residual\nfunction, vitality function, order statistics, mills ratio, information\nfunction, Shannon entropy, Bonferroni and Lorenz curves and quantile function.\nWe have also characterized the WPFD, based on doubly truncated mean. The aim of\nthe study is to increase the application of the Power function distribution.\nThe main feature of the proposed distribution is that there is no induction of\nparameters as compare to the other generalization of the distributions, which\nare complexed having many parameters. We have used R programming to estimate\nthe parameters of the new class of WPFD using Maximum Likelihood Method (MLM),\nPercentile Estimators (P.E) and their modified estimators. After analyzing the\ndata, we conclude that the proposed model WPFD performs better in the data sets\nwhile compared to different competitor models.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.13646v1"
    },
    {
        "title": "Dealing with multiple testing: To adjust or not to adjust",
        "authors": [
            "Yudi Pawitan",
            "Arvid Sjölander"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Multiple testing problems arise naturally in scientific studies because of\nthe need to capture or convey more information with more variables. The\nliterature is enormous, but the emphasis is primarily methodological, providing\nnumerous methods with their mathematical justification and practical\nimplementation. Our aim is to highlight the logical issues involved in the\napplication of multiple testing adjustment.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.02205v1"
    },
    {
        "title": "Likelihood-based solution to the Monty Hall puzzle and a related\n  3-prisoner paradox",
        "authors": [
            "Yudi Pawitan"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  The Monty Hall puzzle has been solved and dissected in many ways, but always\nusing probabilistic arguments, so it is considered a probability puzzle. In\nthis paper the puzzle is set up as an orthodox statistical problem involving an\nunknown parameter, a probability model and an observation. This means we can\ncompute a likelihood function, and the decision to switch corresponds to\nchoosing the maximum likelihood solution. One advantage of the likelihood-based\nsolution is that the reasoning applies to a single game, unaffected by the\nfuture plan of the host. I also describe an earlier version of the puzzle in\nterms of three prisoners: two to be executed and one released. Unlike the goats\nand the car, these prisoners have consciousness, so they can think about\nexchanging punishments. When two of them do that, however, we have a paradox,\nwhere it is advantageous for both to exchange their punishment with each other.\nOverall, the puzzle and the paradox are useful examples of statistical\nthinking, so they are excellent teaching topics.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.02211v1"
    },
    {
        "title": "On statistical deficiency: Why the test statistic of the matching method\n  is hopelessly underpowered and uniquely informative",
        "authors": [
            "Michael C. Nelson"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  The random variate m is, in combinatorics, a basis for comparing\npermutations, as well as the solution to a centuries-old riddle involving the\nmishandling of hats. In statistics, m is the test statistic for a disused null\nhypothesis statistical test (NHST) of association, the matching method. In this\npaper, I show that the matching method has an absolute and relatively low limit\non its statistical power. I do so first by reinterpreting Rae's theorem, which\ndescribes the joint distributions of m with several rank correlation statistics\nunder a true null. I then derive this property solely from m's unconditional\nsampling distribution, on which basis I develop the concept of a deficient\nstatistic: a statistic that is insufficient and inconsistent and inefficient\nwith respect to its parameter. Finally, I demonstrate an application for m that\nmakes use of its deficiency to qualify the sampling error in a jointly\nestimated sample correlation.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08104v1"
    },
    {
        "title": "Does preregistration improve the credibility of research findings?",
        "authors": [
            "Mark Rubin"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Preregistration entails researchers registering their planned research\nhypotheses, methods, and analyses in a time-stamped document before they\nundertake their data collection and analyses. This document is then made\navailable with the published research report to allow readers to identify\ndiscrepancies between what the researchers originally planned to do and what\nthey actually ended up doing. This historical transparency is supposed to\nfacilitate judgments about the credibility of the research findings. The\npresent article provides a critical review of 17 of the reasons behind this\nargument. The article covers issues such as HARKing, multiple testing,\np-hacking, forking paths, optional stopping, researchers' biases, selective\nreporting, test severity, publication bias, and replication rates. It is\nconcluded that preregistration's historical transparency does not facilitate\njudgments about the credibility of research findings when researchers provide\ncontemporary transparency in the form of (a) clear rationales for current\nhypotheses and analytical approaches, (b) public access to research data,\nmaterials, and code, and (c) demonstrations of the robustness of research\nconclusions to alternative interpretations and analytical approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.10513v1"
    },
    {
        "title": "Causes of Misleading Statistics and Research Results Irreproducibility:\n  A Concise Review",
        "authors": [
            "Farzan Shenavarmasouleh",
            "Hamid R. Arabnia"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Bad statistics make research papers unreproducible and misleading. For the\nmost part, the reasons for such misusage of numerical data have been found and\naddressed years ago by experts and proper practical solutions have been\npresented instead. Yet, we still see numerous instances of statistical\nfallacies in modern researches which without a doubt play a significant role in\nthe research reproducibility crisis. In this paper, we review different bad\npractices that impact the research process from its beginning to its very end.\nAdditionally, we briefly propose open science as a universal methodology that\ncan facilitate the entire research life cycle.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.12095v1"
    },
    {
        "title": "The P-T Probability Framework for Semantic Communication, Falsification,\n  Confirmation, and Bayesian Reasoning",
        "authors": [
            "Chenguang Lu"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Many researchers want to unify probability and logic by defining logical\nprobability or probabilistic logic reasonably. This paper tries to unify\nstatistics and logic so that we can use both statistical probability and\nlogical probability at the same time. For this purpose, this paper proposes the\nP-T probability framework, which is assembled with Shannon's statistical\nprobability framework for communication, Kolmogorov's probability axioms for\nlogical probability, and Zadeh's membership functions used as truth functions.\nTwo kinds of probabilities are connected by an extended Bayes' theorem, with\nwhich we can convert a likelihood function and a truth function from one to\nanother. Hence, we can train truth functions (in logic) by sampling\ndistributions (in statistics). This probability framework was developed in the\nauthor's long-term studies on semantic information, statistical learning, and\ncolor vision. This paper first proposes the P-T probability framework and\nexplains different probabilities in it by its applications to semantic\ninformation theory. Then, this framework and the semantic information methods\nare applied to statistical learning, statistical mechanics, hypothesis\nevaluation (including falsification), confirmation, and Bayesian reasoning.\nTheoretical applications illustrate the reasonability and practicability of\nthis framework. This framework is helpful for interpretable AI. To interpret\nneural networks, we need further study.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.00992v1"
    },
    {
        "title": "New definitions (measures) of skewness, mean and dispersion of fuzzy\n  numbers -- by way of a new representation as parameterized curves",
        "authors": [
            "Jan Schneider"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  We give a geometrically motivated measure of skewness, define a mean value\ntriangle number, and dispersion (in that order) of a fuzzy number without\nreference or seeking analogy to the namesake but parallel concepts in\nprobability theory. These measures come about by way of a new representation of\nfuzzy numbers as parameterized curves respectively their associated tangent\nbundle. Importantly skewness and dispersion are given as functions of $\\alpha$\n(the degree of membership) and such may be given separately and pointwise at\neach $\\alpha$-level, as well as overall. This allows for e.g., when a\nmathematical model is formulated in fuzzy numbers, to run optimization programs\nlevel-wise thereby encapsuling with deliberate accuracy the involved membership\nfunctions' characteristics while increasing the computational complexity by\nonly a multiplicative factor compared to the same program formulated in real\nvariables and parameters. As an example the work offers a contribution to the\nrecently very popular fuzzy mean-variance-skewness portfolio optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.01041v1"
    },
    {
        "title": "The causal foundations of applied probability and statistics",
        "authors": [
            "Sander Greenland"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Statistical science (as opposed to mathematical statistics) involves far more\nthan probability theory, for it requires realistic causal models of data\ngenerators - even for purely descriptive goals. Statistical decision theory\nrequires more causality: Rational decisions are actions taken to minimize costs\nwhile maximizing benefits, and thus require explication of causes of loss and\ngain. Competent statistical practice thus integrates logic, context, and\nprobability into scientific inference and decision using narratives filled with\ncausality. This reality was seen and accounted for intuitively by the founders\nof modern statistics, but was not well recognized in the ensuing statistical\ntheory (which focused instead on the causally inert properties of probability\nmeasures). Nonetheless, both statistical foundations and basic statistics can\nand should be taught using formal causal models. The causal view of statistical\nscience fits within a broader information-processing framework which\nilluminates and unifies frequentist, Bayesian, and related probability-based\nfoundations of statistics. Causality theory can thus be seen as a key component\nconnecting computation to contextual information, not extra-statistical but\ninstead essential for sound statistical training and applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.02677v5"
    },
    {
        "title": "The Agnostic Structure of Data Science Methods",
        "authors": [
            "Domenico Napoletani",
            "Marco Panza",
            "Daniele Struppa"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  In this paper we argue that data science is a coherent and novel approach to\nempirical problems that, in its most general form, does not build understanding\nabout phenomena. Within the new type of mathematization at work in data\nscience, mathematical methods are not selected because of any relevance for a\nproblem at hand; mathematical methods are applied to a specific problem only by\n'forcing', i.e. on the basis of their ability to reorganize the data for\nfurther analysis and the intrinsic richness of their mathematical structure. In\nparticular, we argue that deep learning neural networks are best understood\nwithin the context of forcing optimization methods. We finally explore the\nbroader question of the appropriateness of data science methods in solving\nproblems. We argue that this question should not be interpreted as a search for\na correspondence between phenomena and specific solutions found by data science\nmethods; rather, it is the internal structure of data science methods that is\nopen to precise forms of understanding.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.12150v2"
    },
    {
        "title": "The general conformable fractional grey system model and its\n  applications",
        "authors": [
            "Wanli Xie",
            "Mingyong Pang",
            "Wen-Ze Wu",
            "Chong Liu",
            "Caixia Liu"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Grey system theory is an important mathematical tool for describing uncertain\ninformation in the real world. It has been used to solve the uncertainty\nproblems specially caused by lack of information. As a novel theory, the theory\ncan deal with various fields and plays an important role in modeling the small\nsample problems. But many modeling mechanisms of grey system need to be\nanswered, such as why grey accumulation can be successfully applied to grey\nprediction model? What is the key role of grey accumulation? Some scholars have\nalready given answers to a certain extent. In this paper, we explain the role\nfrom the perspective of complex networks. Further, we propose generalized\nconformable accumulation and difference, and clarify its physical meaning in\nthe grey model. We use our newly proposed fractional accumulation and\ndifference to our generalized conformable fractional grey model, or GCFGM(1,1),\nand employ practical cases to verify that GCFGM(1,1) has higher accuracy\ncompared to traditional models.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.01114v2"
    },
    {
        "title": "The potential stickiness of pandemic-induced behavior changes in the\n  United States",
        "authors": [
            "Deborah Salon",
            "Matthew Wigginton Conway",
            "Denise Capasso da Silva",
            "Rishabh Singh Chauhan",
            "Sybil Derrible",
            "Kouros Mohammadian",
            "Sara Khoeini",
            "Nathan Parker",
            "Laura Mirtich",
            "Ali Shamshiripour",
            "Ehsan Rahimi",
            "Ram Pendyala"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Human behavior is notoriously difficult to change, but a disruption of the\nmagnitude of the COVID-19 pandemic has the potential to bring about long-term\nbehavioral changes. During the pandemic, people have been forced to experience\nnew ways of interacting, working, learning, shopping, traveling, and eating\nmeals. A critical question going forward is how these experiences have actually\nchanged preferences and habits in ways that might persist after the pandemic\nends. Many observers have suggested theories about what the future will bring,\nbut concrete evidence has been lacking. We present evidence on how much U.S.\nadults expect their own post-pandemic choices to differ from their pre-pandemic\nlifestyles in the areas of telecommuting, restaurant patronage, air travel,\nonline shopping, transit use, car commuting, uptake of walking and biking, and\nhome location. The analysis is based on a nationally-representative survey\ndataset collected between July and October 2020. Key findings include that the\nnew normal will feature a doubling of telecommuting, reduced air travel, and\nimproved quality of life for some.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14708v2"
    },
    {
        "title": "Each student with her/his own data: understanding sampling distributions",
        "authors": [
            "Mariela Sued",
            "Marina Valdora"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Sampling distribution, a foundational concept in statistics, is difficult to\nunderstand, since we usually have only one realization of the estimator of\ninterest. In this work, we present an innovative method for helping university\nstudents understand the variability of an estimator. In our approach, each\nstudent uses a different data set, getting diverse estimations. Then, sharing\nthe results, we can empirically study the sampling distribution. After some\n\"handmade\" experiences, we have built a web page to deliver a personalized data\nset for each student. Through this web page, we can also reformulate the role\nof the student in the classroom, inviting him/her to became an active player,\nsubmitting the solution to different problems and checking whether they are\ncorrect. In this work we present a recent experience in such direction.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02727v2"
    },
    {
        "title": "Phase transition in a power-law uniform hypergraph",
        "authors": [
            "Mingao Yuan"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  We propose a power-law $m$-uniform random hypergraph on $n$ vertexes. In this\nhypergraph, each vertex is independently assigned a random weight from a\npower-law distribution with exponent $\\alpha\\in(0,\\infty)$ and the hyperedge\nprobabilities are defined as functions of the random weights. We characterize\nthe number of hyperedge and the number of loose 2-cycle. There is a phase\ntransition phenomenon for the number of hyperedge at $\\alpha=1$. Interestingly,\nfor the number of loose 2-cycle, phase transition occurs at both $\\alpha=1$ and\n$\\alpha=2$.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.04296v2"
    },
    {
        "title": "Comments on \"Two Cultures\": What have changed over 20 years?",
        "authors": [
            "Xuming He",
            "Jingshen Wang"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Twenty years ago Breiman (2001) called to our attention a significant\ncultural division in modeling and data analysis between the stochastic data\nmodels and the algorithmic models. Out of his deep concern that the statistical\ncommunity was so deeply and \"almost exclusively\" committed to the former,\nBreiman warned that we were losing our abilities to solve many real-world\nproblems. Breiman was not the first, and certainly not the only statistician,\nto sound the alarm; we may refer to none other than John Tukey who wrote almost\n60 years ago \"data analysis is intrinsically an empirical science.\" However,\nthe bluntness and timeliness of Breiman's article made it uniquely influential.\nIt prepared us for the data science era and encouraged a new generation of\nstatisticians to embrace a more broadly defined discipline. Some might argue\nthat \"The cultural division between these two statistical learning frameworks\nhas been growing at a steady pace in recent years\", to quote Mukhopadhyay and\nWang (2020). In this commentary, we focus on some of the positive changes over\nthe past 20 years and offer an optimistic outlook for our profession.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.07315v1"
    },
    {
        "title": "An Introduction to DoSStoolkit",
        "authors": [
            "Rohan Alexander",
            "Samantha-Jo Caetano",
            "Haoluan Chen",
            "Michael Chong",
            "Annie Collins",
            "Shirley Deng",
            "Isaac Ehrlich",
            "Paul Hodgetts",
            "Yena Joo",
            "Marija Pejcinovska",
            "Mariam Walaa",
            "Matthew Wankiewicz"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  We describe a series of interactive, student-developed, self-paced, modules\nfor learning R. We detail the components of this resource, and the pedagogical\nunderpinning. We discuss the development of this resource, and avenues for\nfuture work. Our resource is available as an R package: DoSStoolkit.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.09347v1"
    },
    {
        "title": "Statistical issues in Serial Killer Nurse cases",
        "authors": [
            "Richard D. Gill",
            "Norman Fenton",
            "David Lagnado"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  We study statistical aspects of the case of the British nurse Ben Geen,\nconvicted of 2 counts of murder and 15 of grievous bodily harm following events\nat Horton General Hospital (in the town of Banbury, Oxfordshire, UK) during\nDecember 2013-February 2014. We draw attention to parallels with the cases of\nnurses Lucia de Berk (the Netherlands) and Daniela Poggiali (Italy), in both of\nwhich an initial conviction for multiple murders of patients was overturned\nafter reopening of the case. We pay most attention to the investigative\nprocesses by which data, and not just statistical data, is generated; namely,\nthe identification of past cases in which the nurse under suspicion might have\nbeen involved. We argue that the investigation and prosecution of such cases is\nvulnerable to many cognitive biases and errors of reasoning about uncertainty,\ncomplicated by the fact that fact-finders have to determine not only whether a\nparticular person was guilty of certain crimes, but whether any crimes were\ncommitted by anybody at all. The paper includes some new statistical findings\non the Ben Geen case and suggests further avenues for investigation. The\nexperiences recounted here have contributed to the writing of the hand-book\nGreen et al. (2022), Healthcare Serial Killer or Coincidence? Statistical\nIssues in Investigation of Suspected Medical Misconduct, commissioned by the\nRoyal Statistical Society, Statistics and the Law section. Submitted to MDPI\nLaws. This version: 5 August, 2022.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.00758v11"
    },
    {
        "title": "An educator's perspective of the tidyverse",
        "authors": [
            "Mine Çetinkaya-Rundel",
            "Johanna Hardin",
            "Benjamin S. Baumer",
            "Amelia McNamara",
            "Nicholas J. Horton",
            "Colin Rundel"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Computing makes up a large and growing component of data science and\nstatistics courses. Many of those courses, especially when taught by faculty\nwho are statisticians by training, teach R as the programming language. A\nnumber of instructors have opted to build much of their teaching around use of\nthe tidyverse. The tidyverse, in the words of its developers, \"is a collection\nof R packages that share a high-level design philosophy and low-level grammar\nand data structures, so that learning one package makes it easier to learn the\nnext\". These shared principles have led to the widespread adoption of the\ntidyverse ecosystem. A large part of this usage is because the tidyverse tools\nhave been intentionally designed to ease the learning process and make it\neasier for users to learn new functions as they engage with additional pieces\nof the larger ecosystem. Moreover, the functionality offered by the packages\nwithin the tidyverse spans the entire data science cycle, which includes data\nimport, visualisation, wrangling, modeling, and communication. We believe the\ntidyverse provides an effective and efficient pathway for undergraduate\nstudents at all levels and majors to gain computational skills and thinking\nneeded throughout the data science cycle. In this paper, we introduce the\ntidyverse from an educator's perspective. We provide a brief introduction to\nthe tidyverse, demonstrate how foundational statistics and data science tasks\nare accomplished with the tidyverse, and discuss the strengths of the\ntidyverse, particularly in the context of teaching and learning.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.03510v4"
    },
    {
        "title": "Lottery paradox, DNA evidence and other stories: How to accept uncertain\n  statements",
        "authors": [
            "Yudi Pawitan"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  I think we can agree that dealing with uncertainty is not easy. Probability\nis the main tool for dealing with uncertainty, and we know there are many\nprobability-related puzzles and paradoxes. Here I describe a rather\nidiosyncratic selection that highlights the problem of accepting uncertain\nstatements. Without going into a formal decision theory, there are simple\nintuitive rational bases for doing that, for instance based on high probability\nalone. The lottery paradox shows the logical problem of accepting uncertain\nstatements based on high probability. The DNA evidence story is an example of\nthe use probabilistic reasoning in court, where philosophical differences\nbetween the schools of inference -- the frequentist, Bayesian and likelihood\nschools -- lead to substantial differences in the quantification of evidence.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.03971v2"
    },
    {
        "title": "Rank Energy Statistics in the Context of Change Point Detection",
        "authors": [
            "Amanda Ng"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  In this paper, I propose a general procedure for multivariate\ndistribution-free nonparametric testing derived from the concept of ranks that\nare based upon measure transportation in the context of multiple change point\nanalysis. I will use this algorithm to estimate both the number of change\npoints and their locations within an observed multivariate time series. In this\npaper, the change point problem is observed in a general setting in which both\nthe given distribution and number of change points are unknown, rather than\nassume the observed time series follows a specific distribution or contains\nonly one change point as many works in this area of study assume. The intention\nof this is to develop a technique for accurately identifying the changes in a\ndistribution while making as few suppositions as possible. The rank energy\nstatistic used here is based on energy statistics and has the potential to\ndetect any change in a distribution. I present the properties of this new\nalgorithm, which can be used to analyze various datasets, including\nhierarchical clustering, testing multivariate normality, gene selection, and\nmicroarray data analysis. This algorithm has also been implemented in the R\npackage recp, which is available on GitHub.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.04903v2"
    },
    {
        "title": "Which Type of Statistical Uncertainty Helps Evidence-Based Policymaking?\n  An Insight from a Survey Experiment in Ireland",
        "authors": [
            "Akisato Suzuki"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Which type of statistical uncertainty -- statistical (in)significance with a\np-value, or a Bayesian probability -- enables people to see the continuous\nnature of uncertainty more clearly in a policymaking context? An original\nsurvey experiment used a hypothetical scenario, where participants from Ireland\nwere asked whether to introduce a new bus line to reduce traffic jams, given a\nresearch report estimating its effectiveness. The treatments were uncertainty\ninformation: statistical significance with a p-value of 2%, statistical\ninsignificance with a p-value of 25%, the 95% probability that the estimate is\ncorrect, and the 68% probability that the estimate is correct. In the case of\nlower uncertainty, both significance and Bayesian frameworks resulted in a\nlarge proportion of participants adopting the policy (0.82 and 0.91\nrespectively). In the case of higher uncertainty, the significance framework\nled a much smaller proportion of participants to adopt the policy (0.39 against\n0.83). The findings suggest participants saw the continuous nature of\nuncertainty more clearly in the Bayesian framework than in the significance\nframework.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.05100v4"
    },
    {
        "title": "The Current State of Undergraduate Bayesian Education and\n  Recommendations for the Future",
        "authors": [
            "Mine Dogucu",
            "Jingchen Hu"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  As a result of the increased emphasis on mis- and over-use of $p$-values in\nscientific research and the rise in popularity of Bayesian statistics, Bayesian\neducation is becoming more important at the undergraduate level. With the\nadvances in computing tools, Bayesian statistics is also becoming more\naccessible for the undergraduates. This study focuses on analyzing Bayesian\ncourses for the undergraduates. We explored whether an undergraduate Bayesian\ncourse is offered in our sample of 152 high-ranking research universities and\nliberal arts colleges. For each identified Bayesian course, we examined how it\nfits into the institution's undergraduate curricula, such as majors and\nprerequisites. Through a series of course syllabi analyses, we explored the\ntopics covered and their popularity in these courses, and the adopted teaching\nand learning tools, such as software. This paper presents our findings on the\ncurrent practices of teaching full Bayesian courses at the undergraduate level.\nBased on our findings, we provide recommendations for programs that may\nconsider offering Bayesian courses to their students.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.00848v2"
    },
    {
        "title": "Interactive Probing of Multivariate Time Series Prediction Models: A\n  Case of Freight Rate Analysis",
        "authors": [
            "Haonan Xu",
            "Haotian Li",
            "Yong Wang"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  We present an interactive probing tool to create, modify and analyze what-if\nscenarios for multivariate time series models. The solution is applied to\nfreight trading, where analysts can carry out sensitivity analysis on freight\nrates by changing demand and supply-related econometric variables and observing\ntheir resultant effects on freight indexes. We utilize various visualization\ntechniques to enable intuitive scenario creation, alteration, and comprehension\nof time series inputs and model predictions. Our tool proved to be useful to\nthe industry practitioners, demonstrated by a case study where freight traders\nare given hypothetical market scenarios and successfully generated quantitative\nfreight index projection with confidence.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.10869v1"
    },
    {
        "title": "Framework for Accessible and Inclusive Teaching Materials for Statistics\n  and Data Science Courses",
        "authors": [
            "Mine Dogucu",
            "Alicia A. Johnson",
            "Miles Ott"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Despite rapid growth in the data science workforce, people of color, women,\nthose with disabilities, and others remain underrepresented in, underserved by,\nand sometimes excluded from the field. This pattern prevents equal opportunity\nfor individuals, while also creating products and policies that perpetuate\ninequality. Thus, for statistics and data science educators of the next\ngeneration, accessibility and inclusion should be of utmost importance in our\nprograms and courses. In this paper, we discuss how we developed an\naccessibility and inclusion framework, hence a structure for holding ourselves\naccountable to these principles, for the writing of a statistics textbook. We\nshare our experiences in setting accessibility and inclusion goals, the tools\nwe used to achieve these goals, and recommendations for other educators. We\nprovide examples for instructors that can be implemented in their own courses.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.06355v1"
    },
    {
        "title": "A generalised matching distribution for the problem of coincidences",
        "authors": [
            "Ben O'Neill"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  This paper examines the classical matching distribution arising in the\n\"problem of coincidences\". We generalise the classical matching distribution\nwith a preliminary round of allocation where items are correctly matched with\nsome fixed probability, and remaining non-matched items are allocated using\nsimple random sampling without replacement. Our generalised matching\ndistribution is a convolution of the classical matching distribution and the\nbinomial distribution. We examine the properties of this latter distribution\nand show how its probability functions can be computes. We also show how to use\nthe distribution for matching tests and inferences of matching ability.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.12442v1"
    },
    {
        "title": "An empirical exploration of the diversified R ecosystem",
        "authors": [
            "Tian-Yuan Huang",
            "Zhilan Lou"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Born in the late 20s, R is one of the most popular software for statistical\ncomputing and graphics. With the development of information technology and the\nadvent of the big data era, great changes have taken place in the R ecosystem.\nBased on the meta information of the Comprehensive R Archive Network (CRAN) and\nthe bibliometric data of literature citing R, we discovered that while R is\ninitiated by statistics, its development is benefited greatly from computer\nscience and the main user group in academics come from various disciplines such\nas agricultural science, biological science, environmental science and medical\nscience. In addition, we displayed the collaboration patterns among R\ndevelopers and analyze the possible effects of collaboration in the R\ncommunity.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.04982v5"
    },
    {
        "title": "Effect of Measurement Errors on the Multivariate CUSUM CoDa Control\n  Chart for the Manufacturing Process",
        "authors": [
            "Muhammad Imran",
            "Jinsheng Sun",
            "Fatima Sehar Zaidi",
            "Zameer Abbas",
            "Hafiz Zafar Nazir"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Control charts, one of the main tools in Statistical Process Control (SPC),\nhave been widely adopted in manufacturing sectors as an effective strategy for\nmalfunction detection throughout the previous decades. Measurement errors\n(M.E's) are involved in the quality characteristic of interest. The authors\nexplored the impact of a linear covariate error model on the multivariate\ncumulative sum (CUSUM) control charts for a specific kind of data known as\ncompositional data(CoDa). The average run length ARL is used to assess the\nperformance of the proposed chart. The results indicate that M.E's\nsignificantly affects the multivariate CUSUM-CoDa control charts. The authors\nhave used the Markov chain method to study the impact of different involved\nparameters using four different cases for the variance-covariance matrix (i.e.\nuncorrelated with equal variances, negatively correlated with equal variances,\nuncorrelated with unequal variances, positively correlated with unequal\nvariances). The authors concluded that the ARL of the multivariate CUSUM-CoDa\nchart increase with an increase in the value of error variance-covariance\nmatrix, while the ARL decreases with an increase in the subgroup size m or the\nconstant powering b. For the implementation of the proposal, two illustrated\nexamples have been reported for multivariate CUSUM-CoDa control charts in the\npresence of M.E's. One deals with the manufacturing process of uncoated aspirin\ntablets, and the other is based on monitoring machines in the muesli\nmanufacturing process.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.12318v2"
    },
    {
        "title": "Teaching modeling in introductory statistics: A comparison of formula\n  and tidyverse syntaxes",
        "authors": [
            "Amelia McNamara"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  There are many pedagogical considerations for incorporating programming into\na statistics course. When using the programming language R, one consideration\nis the particular R syntax that will be used. This paper reports on a\nhead-to-head comparison run in a pair of introductory statistics labs, one\nconducted fully in the formula syntax, the other in tidyverse. Analysis of pre-\nand post-survey data show minimal differences between the two labs, with\nstudents reporting a positive experience regardless of section. Analysis of\ndata from YouTube and RStudio Cloud show interesting distinctions. The formula\nsection appeared to watch a larger proportion of pre-lab YouTube videos, but\nspend less time computing on RStudio Cloud. Conversely, the tidyverse section\nwatched a smaller proportion of the videos and spent more time computing.\nAnalysis of lab materials showed tidyverse labs tended to be slightly longer in\nterms of lines in the provided RMarkdown materials and minutes of the\nassociated YouTube videos. The tidyverse labs exposed students to more distinct\nR functions, but reused functions more frequently. Both labs relied on a\nrelatively small vocabulary of consistent functions, which can provide a\nstarting point for instructors interested in teaching introductory statistics\nin R. The instructor experience of teaching in the two syntaxes diverged\nprimarily when discussing relationships between categorical variables, as well\nas when working with summary statistics for numeric variables. This work\nprovides additional evidence for instructors looking to choose between syntaxes\nfor introductory statistics teaching.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.12960v5"
    },
    {
        "title": "Mathematical models of confirmation bias",
        "authors": [
            "Rose D Baker"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Confirmation bias is a cognitive bias that adversely affects management\ndecisions, and mathematical modelling is an aid to its detailed understanding.\nBias in opinion update about the value of a parameter is modelled here assuming\nthat observations are discounted depending on their distance from prior\nopinion. The models allow belief persistence, attitude polarization, and the\nirrational primacy effect to be explored. A general framework for exploring\nlarge-sample properties of these models is given, and an attempt made to\nclassify the models. An interesting result is that in some models the influence\nof an observation always increases with distance from the prior opinion,\nwhereas in others observations greatly at odds with prior opinion are given\nvery little weight. The models could be useful to those exploring these\nphenomena in detail.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03072v1"
    },
    {
        "title": "Revisiting the secondary climate attributes for transportation\n  infrastructure management: A Redux and Update for 2020",
        "authors": [
            "Tao Liao",
            "Paul Kepley",
            "Indraneel Kumar",
            "Samuel Labi"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Environmental conditions in various regions can have a severely negative\nimpact on the longevity and durability of the civil engineering\ninfrastructures. In 2018, a published paper used 1971 to 2010 NOAA data from\nthe contiguous United States to examine the temporal changes in secondary\nclimate attributes (freeze-thaw cycles and freeze index) using the climate\nnormals from two time windows, 1971-2000 and 1981-2010. That paper investigated\nwhether there have been statistically significant changes in climate attribute\nlevels across the two time windows, and used GIS-based interpolation methods to\ndevelop isarithmic maps of the climate attributes to facilitate their\ninterpretation and application. This paper updates that study. In this study,\nwe use NOAA climatic data from 1991 to 2020 to construct continuous surface\nmaps showing the values of the primary and secondary climate attributes at the\n48 continental states. The new maps provide an updated picture of the freeze\nindex values and freeze-thaw cycles for the most recent climate condition.\nThese new values provide a better picture of the freezing season\ncharacteristics of the United States, and will provide information necessary\nfor better winter maintenance procedures and infrastructure design to\naccommodate regional climate differences.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.12480v1"
    },
    {
        "title": "Length L-function for Network-Constrained Point Data",
        "authors": [
            "Zidong Fang",
            "Ci Song",
            "Hua Shu",
            "Jie Chen",
            "Tianyu Liu",
            "Xi Wang",
            "Xiao Chen",
            "Tao Pei"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Network constrained points are referred to as points restricted to road\nnetworks, such as taxi pick up and drop off locations. A significant pattern of\nnetwork constrained points is referred to as an aggregation; e.g., the\naggregation of pick up points may indicate a high taxi demand in a particular\narea. Although the network K function using the shortest path network distance\nhas been proposed to detect point aggregation, its statistical unit is still\nradius based. R neighborhood, in particular, has inconsistent network length\nowing to the complex configuration of road networks which cause unfair counts\nand identification errors in networks (e.g., the length of the r neighborhood\nlocated at an intersection is longer than that on straight roads, which may\ninclude more points). In this study, we derived the length L function for\nnetwork constrained points to identify the aggregation by designing a novel\nneighborhood as the statistical unit; the total length of this is consistent\nthroughout the network. Compared to the network K function, our method can\ndetect a true to life aggregation scale, identify the aggregation with higher\nnetwork density, as well as identify the aggregations that the network K\nfunction cannot. We validated our method using taxi trips pick up location data\nwithin Zhongguancun Area in Beijing, analyzing differences in maximal\naggregation between workdays and weekends to understand taxi demand in the\nmorning and evening peak.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.17262v1"
    },
    {
        "title": "Causal inference: critical developments, past and future",
        "authors": [
            "Erica EM Moodie",
            "David A Stephens"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Causality is a subject of philosophical debate and a central scientific issue\nwith a long history. In the statistical domain, the study of cause and effect\nbased on the notion of `fairness' in comparisons dates back several hundred\nyears, and yet statistical concepts and developments that form the area of\ncausal inference are only decades old. In this paper, we review core tenets and\nmethods of causal inference and key developments in the history of the field.\nWe highlight connections with traditional `associational' statistical methods,\nincluding estimating equations and semiparametric theory, and point to current\ntopics of active research in this crucial area of our field.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.02231v1"
    },
    {
        "title": "The loss value of multilinear regression",
        "authors": [
            "Helmut Kahl"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Determinant formulas are presented for: a certain positive semidefinite,\nhermitian matrix; the loss value of multilinear regression; the multiple linear\nregression coefficient.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.02686v4"
    },
    {
        "title": "Six Statistical Senses",
        "authors": [
            "Radu V. Craiu",
            "Ruobin Gong",
            "Xiao-Li Meng"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  This article proposes a set of categories, each one representing a particular\ndistillation of important statistical ideas. Each category is labeled a \"sense\"\nbecause we think of these as essential in helping every statistical mind\nconnect in constructive and insightful ways with statistical theory,\nmethodologies, and computation, toward the ultimate goal of building\nstatistical phronesis. The illustration of each sense with statistical\nprinciples and methods provides a sensical tour of the conceptual landscape of\nstatistics, as a leading discipline in the data science ecosystem.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05313v2"
    },
    {
        "title": "Computational Statistics and Data Science in the Twenty-first Century",
        "authors": [
            "Andrew J. Holbrook",
            "Akihiko Nishimura",
            "Xiang Ji",
            "Marc A. Suchard"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Data science has arrived, and computational statistics is its engine. As the\nscale and complexity of scientific and industrial data grow, the discipline of\ncomputational statistics assumes an increasingly central role among the\nstatistical sciences. An explosion in the range of real-world applications\nmeans the development of more and more specialized computational methods, but\nfive Core Challenges remain. We provide a high-level introduction to\ncomputational statistics by focusing on its central challenges, present recent\nmodel-specific advances and preach the ever-increasing role of non-sequential\ncomputational paradigms such as multi-core, many-core and quantum computing.\nData science is bringing major changes to computational statistics, and these\nchanges will shape the trajectory of the discipline in the 21st century.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.05530v1"
    },
    {
        "title": "Comment on \"The statistics wars and intellectual conflicts of interest\"\n  by D. Mayo",
        "authors": [
            "Philip B. Stark"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  While P-values are widely abused, they are a useful tool for many purposes;\nbanning them is analogous to banning scalpels because most people do not know\nhow to perform surgery. Many reported P-values are not genuine P-values, for a\nvariety of reasons. Perhaps the most widespread and pernicious problem is the\nType III error of testing a statistical hypothesis that has little or no\nconnection to the scientific hypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.07903v1"
    },
    {
        "title": "Minimizing Fleet Size and Improving Bike Allocation of Bike Sharing\n  under Future Uncertainty",
        "authors": [
            "Mingzhuang Hua",
            "Xuewu Chen",
            "Jingxu Chen",
            "Yu Jiang"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  As a rapidly expanding service, bike sharing is facing severe problems of\nbike over-supply and demand fluctuation in many Chinese cities. This study\ndevelops a large-scale method to determine the minimum fleet size under\nuncertainty, based on the bike sharing data of millions of trips in Nanjing. It\nis found that the algorithm of minimizing fleet size under the\nincomplete-information scenario is effective in handling future uncertainty.\nFor a dockless bike sharing system, supplying 14.5% of the original fleet could\nmeet 96.8% of trip demands. Meanwhile, the results suggest that providing a\nintegrated service platform that integrates multiple companies can\nsignificantly reduce the total fleet size by 44.6%. Moreover, in view of the\nCOVID-19 pandemic, this study proposes a social distancing policy that\nmaintains a suitable usage interval. These findings provide useful insights for\nimproving the resource efficiency and operational service of bike sharing and\nshared mobility.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.08603v1"
    },
    {
        "title": "Research on spatial information transmission efficiency and capability\n  of safe evacuation signs",
        "authors": [
            "Ruiwen Fan",
            "Zhangyin Dai",
            "Shixiang Tian",
            "Ting Xia a",
            "Hui Zhou",
            "Congbao Huang"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  As an indispensable spatial direction information indicator for emergency\nevacuation, the spatial relationship between safety evacuation signs and\nevacuees will affect the response time of evacuees and the evacuation\nefficiency. This paper takes 2 kinds of common safety evacuation signs,\nhangtag-type and embedded, as the research object and designs space direction\ninformation transmission efficiency and capability simulation experiment and\nfire drill, the efficiency and capability of spatial direction information\ntransmission of safety evacuation signs are studied. The results show that the\nspace angle of the hangtag-type safety evacuation sign is inversely\nproportional to the information transmission efficiency and capability of the\nspace direction, and the fire drill also confirms this conclusion. When the\nspatial angle of the embedded safety evacuation sign is 5{\\deg}, the spatial\ndirection information transmission efficiency and capability increase.\nSimultaneously, the average escape time of the participants in the fire drill\nwas lower, and the percentage of choosing unfamiliarity exports increased. The\nevolution of spatial angle has no significant effect on the intention of the\nresponse of subjects of different genders; when choosing the direction, males\nare more easily affected by the change of spatial angle than females; the\nconfidence level of females' choice is more easily affected by spatial angle.\nIn addition, according to the research results, the corresponding\nthree-dimensional structure safety evacuation signs are designed. The\nfunctional structure of the safety evacuation signs is perfected, which can\neffectively improve the efficiency of fire emergency evacuation.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10525v1"
    },
    {
        "title": "Foundations for NLP-assisted formative assessment feedback for\n  short-answer tasks in large-enrollment classes",
        "authors": [
            "Susan Lloyd",
            "Matthew Beckman",
            "Dennis Pearl",
            "Rebecca Passonneau",
            "Zhaohui Li",
            "Zekun Wang"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Research suggests \"write-to-learn\" tasks improve learning outcomes, yet\nconstructed-response methods of formative assessment become unwieldy with large\nclass sizes. This study evaluates natural language processing algorithms to\nassist this aim. Six short-answer tasks completed by 1,935 students were scored\nby several human raters, using a detailed rubric, and an algorithm. Results\nindicate substantial inter-rater agreement using quadratic weighted kappa for\nrater pairs (each QWK > 0.74) and group consensus (Fleiss Kappa = 0.68).\nAdditionally, intra-rater agreement was estimated for one rater who had scored\n178 responses seven years prior (QWK = 0.89). With compelling rater agreement,\nthe study then pilots cluster analysis of response text toward enabling\ninstructors to ascribe meaning to clusters as a means for scalable formative\nassessment.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.02829v1"
    },
    {
        "title": "Adjust Pearson's $r$ to Measure Arbitrary Monotone Dependence",
        "authors": [
            "Xinbo Ai"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Pearson's r, the most widely-used correlation coefficient, is traditionally\nregarded as exclusively capturing linear dependence, leading to its\ndiscouragement in contexts involving nonlinear relationships. However, recent\nresearch challenges this notion, suggesting that Pearson's r should not be\nruled out a priori for measuring nonlinear monotone relationships. Pearson's r\nis essentially a scaled covariance, rooted in the renowned Cauchy-Schwarz\nInequality. Our findings reveal that different scaling bounds yield\ncoefficients with different capture ranges, and interestingly, tighter bounds\nactually expand these ranges. We derive a tighter inequality than\nCauchy-Schwarz Inequality, leverage it to refine Pearson's r, and propose a new\ncorrelation coefficient, i.e., rearrangement correlation. This coefficient is\nable to capture arbitrary monotone relationships, both linear and nonlinear\nones. It reverts to Pearson's r in linear scenarios. Simulation experiments and\nreal-life investigations show that the rearrangement correlation is more\naccurate in measuring nonlinear monotone dependence than the three classical\ncorrelation coefficients, and other recently proposed dependence measures.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.04571v7"
    },
    {
        "title": "Examining the role of context in statistical literacy outcomes using an\n  isomorphic assessment instrument",
        "authors": [
            "Sayali Phadke",
            "Matthew D Beckman",
            "Kari Lock Morgan"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  The central role of statistical literacy has been discussed extensively,\nemphasizing its importance as a learning outcome and in promoting a citizenry\ncapable of interacting with the world in an informed and critical manner. Our\nwork contributes to the growing literature on assessing and improving people's\nstatistical literacy vis-a-vis contexts important in their professional and\npersonal lives. We consider the measurement of contextualized statistics\nliteracy - statistical literacy as applied to relevant contexts. We discuss the\ndevelopment of an isomorphic instrument modifying an existing assessment,\ndesign of a pilot study, and results which conclude that 1) the isomorphic\nassessment has comparable psychometric properties, and 2) test takers have\nlower statistical literacy scores on an assessment that incorporates relevant\ncontexts.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.05608v1"
    },
    {
        "title": "A Journey from Wild to Textbook Data to Reproducibly Refresh the Wages\n  Data from the National Longitudinal Survey of Youth Database",
        "authors": [
            "Dewi Amaliah",
            "Dianne Cook",
            "Emi Tanaka",
            "Kate Hyde",
            "Nicholas Tierney"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Textbook data is essential for teaching statistics and data science methods\nbecause they are clean, allowing the instructor to focus on methodology.\nIdeally textbook data sets are refreshed regularly, especially when they are\nsubsets taken from an on-going data collection. It is also important to use\ncontemporary data for teaching, to imbue the sense that the methodology is\nrelevant today. This paper describes the trials and tribulations of refreshing\na textbook data set on wages, extracted from the National Longitudinal Survey\nof Youth (NLSY79) in the early 1990s. The data is useful for teaching modeling\nand exploratory analysis of longitudinal data. Subsets of NLSY79, including the\nwages data, can be found in supplementary files from numerous textbooks and\nresearch articles. The NLSY79 database has been continuously updated through to\n2018, so new records are available. Here we describe our journey to refresh the\nwages data, and document the process so that the data can be regularly updated\ninto the future. Our journey was difficult because the steps and decisions\ntaken to get from the raw data to the wages textbook subset have not been\nclearly articulated. We have been diligent to provide a reproducible workflow\nfor others to follow, which also hopefully inspires more attempts at refreshing\ndata for teaching. Three new data sets and the code to produce them are\nprovided in the open source R package called `yowie`.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.06417v1"
    },
    {
        "title": "Three principles for modernizing an undergraduate regression analysis\n  course",
        "authors": [
            "Maria Tackett"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  As data have become more prevalent in academia, industry, and daily life, it\nis imperative that undergraduate students are equipped with the skills needed\nto analyze data in the modern environment. In recent years there has been a lot\nof work innovating introductory statistics courses and developing introductory\ndata science courses; however, there has been less work beyond the first\ncourse. This paper describes innovations to Regression Analysis taught at Duke\nUniversity, a course focused on application that serves a diverse undergraduate\nstudent population of statistics and data science majors along with non-majors.\nThree principles guiding the modernization of the course are presented with\ndetails about how these principles align with the necessary skills of practice\noutlined in recent statistics and data science curriculum guidelines. The paper\nincludes pedagogical strategies, motivated by the innovations in introductory\ncourses, that make it feasible to implement skills for the practice of modern\nstatistics and data science alongside fundamental statistical concepts. The\npaper concludes with the impact of these changes, challenges, and next steps\nfor the course. Portions of in-class activities and assignments are included in\nthe paper, with full sample assignments and resources for finding data in the\nsupplemental materials.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.11026v2"
    },
    {
        "title": "The paradoxical nature of easily improvable evidence",
        "authors": [
            "Maria Chikina",
            "Wesley Pegden"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Established frameworks to understand problems with reproducibility in science\nbegin with the relationship between our understanding of the prior probability\nof a claim and the statistical certainty that should be demanded of it, and\nexplore the ways in which independent investigations, biases in study design\nand publication bias interact with these considerations.\n  We propose a complementary perspective; namely, that to improve\nreproducibility in science, our interpretation of the persuasiveness of\nevidence (e.g., statistical significance thresholds) should be responsive to\nour understanding of the effort that would be required to improve that\nevidence. We will quantify this notion in some formal settings. Indeed, we will\ndemonstrate that even simplistic models of evidence publication can exhibit an\nimprovable evidence paradox, where the publication of easily improvable\nevidence in favor of a claim can best seen as evidence the claim is false.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.14016v1"
    },
    {
        "title": "A validation of the short-form classroom community scale for\n  undergraduate mathematics and statistics students",
        "authors": [
            "Maria Tackett",
            "Shira Viel",
            "Kim Manturuk"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  This study examines Cho and Demmans Epp's short-form adaptation of Rovai's\nwell-known Classroom Community Scale (CCS-SF) as a measure of classroom\ncommunity among introductory undergraduate math and statistics students. A\nseries of statistical analyses were conducted to investigate the validity of\nthe CCS-SF for this new population. Data were collected from 351 students\nenrolled in 21 online classes, offered for credit in Fall 2020 and Spring 2021\nat a private university in the United States. Further confirmatory analysis was\nconducted with data from 128 undergraduates enrolled in 13 in-person and hybrid\nclasses, offered for credit in Fall 2021 at the same institution. Following\nRovai's original 20-item CCS, the 8-item CCS-SF yields two interpretable\nfactors, connectedness and learning. This study confirms the two-factor\nstructure of the CCS-SF, and concludes that it is a valid measure of classroom\ncommunity among undergraduate students enrolled in remote, hybrid, and\nin-person introductory mathematics and statistics courses.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.00590v3"
    },
    {
        "title": "Process, Population, and Sample: the Researcher's Interest",
        "authors": [
            "Charles W. Champ",
            "Andrew V. Sills"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  A case is made that researchers are interested in studying processes. Often\nthe inferences they are interested in making are about the process and its\nassociated population. On other occasions, a researcher may be interested in\nmaking an inference about the collection of individuals the process has\ngenerated. We will call the statistical methods employed by the researcher to\nmake such inferences about the process/population ``estimation methods.'' The\nstatistical methods used in making an inference about the collection of\nindividuals generated we call ``prediction methods.'' Methods for obtaining\ninterval estimates of a parameter and prediction intervals for a statistic are\ngiven. The analytical and enumerative methods discussed in Deming (1953) are\nsimply estimation and prediction methods, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.08376v1"
    },
    {
        "title": "The Inference Framework",
        "authors": [
            "Nicholas Carrara"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  The following three sections and appendices are taken from my thesis \"The\nFoundations of Inference and its Application to Fundamental Physics\" from 2021,\nin which I construct a theory of entropic inference from first principles. The\nmajority of these chapters are not original, but are a collection of various\nsources through the history of the subject. The first section deals with\ndeductive reasoning, which is inference in the presence of complete\ninformation. The second section expands on the deductive system by constructing\na theory of inductive inference, a theory of probabilities, which is inference\nin the presence of incomplete information. Finally, section three develops a\nmeans of updating these probabilities in the presence of new information that\ncomes in the form of constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.08785v1"
    },
    {
        "title": "Data science transfer pathways from associate's to bachelor's programs",
        "authors": [
            "Benjamin S. Baumer",
            "Nicholas J. Horton"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  A substantial fraction of students who complete their college education at a\npublic university in the United States begin their journey at one of the 935\npublic two-year colleges. While the number of four-year colleges offering\nbachelor's degrees in data science continues to increase, data science\ninstruction at many two-year colleges lags behind. A major impediment is the\nrelative paucity of introductory data science courses that serve multiple\nstudent audiences and can easily transfer. In addition, the lack of pre-defined\ntransfer pathways (or articulation agreements) for data science creates a\ngrowing disconnect that leaves students who want to study data science at a\ndisadvantage. We describe opportunities and barriers to data science transfer\npathways. Five points of curricular friction merit attention: 1) a first course\nin data science, 2) a second course in data science, 3) a course in scientific\ncomputing, data science workflow, and/or reproducible computing, 4) lab\nsciences, and 5) navigating communication, ethics, and application domain\nrequirements in the context of general education and liberal arts course\nmappings. We catalog existing transfer pathways, efforts to align curricula\nacross institutions, obstacles to overcome with minimally-disruptive solutions,\nand approaches to foster these pathways. Improvements in these areas are\ncritically important to ensure that a broad and diverse set of students are\nable to engage and succeed in undergraduate data science programs.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.12528v2"
    },
    {
        "title": "A Comparison of Reproducibility Guidelines and Its Implications on\n  Undergraduate Statistical Education",
        "authors": [
            "Siqi Zheng"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  In this paper, we replicated a Bayesian educational research project, which\nexplores the association between broadband access and online course enrollment\nin the US. We summarized key findings from our replication and compared them\nwith the original project. Based on my replication experience, we aim to\ndemonstrate the challenges of research reproduction, even when codes and data\nare shared openly and the quality of the materials on GitHub are high.\nMoreover, we investigate the implicit presumptions of the researchers' level of\nknowledge and discuss how such presumptions may add difficulty to the\nreproduction of scientific research. Finally, we hope this article sheds light\non the design of reproducibility criterion and opens up a space to explore what\nshould be taught in undergraduate statistics education.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.16350v1"
    },
    {
        "title": "An exposition of possibility and probability",
        "authors": [
            "Ben O'Neill"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  This paper considers the notion of possible events which are insignificant in\nprobabilistic analysis (i.e. events that have zero probability). The paper\ndiscusses the method of modal logic based on \"possible worlds\" and discusses a\nmathematical framework for the concepts of possibility, impossibility and\ncertainty that are sometimes (incorrectly) thought to be defined with respect\nto probability. The relationship between possibility and probability is\nexplored for general probability spaces and for refinements of these spaces\nconditional on other events, with particular focus on the properties of events\nhaving zero probability. We derive conditions under which possibility and\nsignificance diverge and conditions under which they can be reconciled as\nequivalent ideas within certain contexts. We also apply this analysis to\ndiscuss issues in possibility and probability in the multinomial model.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.03060v1"
    },
    {
        "title": "The Second Competition on Spatial Statistics for Large Datasets",
        "authors": [
            "Sameh Abdulah",
            "Faten Alamri",
            "Pratik Nag",
            "Ying Sun",
            "Hatem Ltaief",
            "David E. Keyes",
            "Marc G. Genton"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  In the last few decades, the size of spatial and spatio-temporal datasets in\nmany research areas has rapidly increased with the development of data\ncollection technologies. As a result, classical statistical methods in spatial\nstatistics are facing computational challenges. For example, the kriging\npredictor in geostatistics becomes prohibitive on traditional hardware\narchitectures for large datasets as it requires high computing power and memory\nfootprint when dealing with large dense matrix operations. Over the years,\nvarious approximation methods have been proposed to address such computational\nissues, however, the community lacks a holistic process to assess their\napproximation efficiency. To provide a fair assessment, in 2021, we organized\nthe first competition on spatial statistics for large datasets, generated by\nour {\\em ExaGeoStat} software, and asked participants to report the results of\nestimation and prediction. Thanks to its widely acknowledged success and at the\nrequest of many participants, we organized the second competition in 2022\nfocusing on predictions for more complex spatial and spatio-temporal processes,\nincluding univariate nonstationary spatial processes, univariate stationary\nspace-time processes, and bivariate stationary spatial processes. In this\npaper, we describe in detail the data generation procedure and make the\nvaluable datasets publicly available for a wider adoption. Then, we review the\nsubmitted methods from fourteen teams worldwide, analyze the competition\noutcomes, and assess the performance of each team.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.03119v1"
    },
    {
        "title": "Near-peer mentoring in data science: Two experiences at Stanford\n  University",
        "authors": [
            "Chiara Sabatti",
            "Qian Zhao"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Universities have been expanding the data science programs for undergraduate\nstudents, with the simultaneous goal of reaching and retaining students from\nunderrepresented groups in the data science workforce. The set of new programs\nalso offer opportunities to involve graduate students, fostering their growth\nas future leaders in data science education. We describe two programs that use\nthe near peer mentoring structure to provide pathways for graduate students to\ndevelop teaching and mentoring skills, while providing research and learning\nopportunities for undergraduate students from diverse backgrounds. In the Data\nScience for Social Good Summer program, graduate students mentor a group of\nundergraduate fellows as they tackle a data science project with positive\nsocial impact. In the Inclusive Mentoring in Data Science course, graduate\nstudents participate in workshops on effective and inclusive mentorship\nstrategies. In an experiential learning framework, they are paired with\nundergraduate students from non-R1 schools, who they mentor through weekly\none-on-one on-line meetings. These initiatives offer a prototype of future\nprograms that serve the dual goal of providing both hands-on mentoring\nexperience for graduate students and research opportunities for undergraduate\nstudents, in a high-touch inclusive and encouraging environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.08637v2"
    },
    {
        "title": "A historical view on the maximum entropy",
        "authors": [
            "Seyedeh Azadeh Fallah Mortezanejad"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  How to find unknown distributions is questioned in many pieces of research.\nThere are several ways to figure them out, but the main question is which acts\nmore reasonably than others. In this paper, we focus on the maximum entropy\nprinciple as a suitable method of discovering the unknown distribution, which\nrecommends some prior information based on the available data set. We explain\nits features by reviewing some papers. Furthermore, we recommend some articles\nto study around the generalized maximum entropy issue, which is more suitable\nwhen autocorrelation data exists. Then, we list the beneficial features of the\nmaximum entropy as a result. Finally, some disadvantages of entropy are\nexpressed to have a complete look at the maximum entropy principle, and we list\nits drawbacks as the final step.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.14602v3"
    },
    {
        "title": "Learning to forecast: The probabilistic time series forecasting\n  challenge",
        "authors": [
            "Johannes Bracher",
            "Nils Koster",
            "Fabian Krüger",
            "Sebastian Lerch"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  We report on a course project in which students submit weekly probabilistic\nforecasts of two weather variables and one financial variable. This real-time\nformat allows students to engage in practical forecasting, which requires a\ndiverse set of skills in data science and applied statistics. We describe the\ncontext and aims of the course, and discuss design parameters like the\nselection of target variables, the forecast submission process, the evaluation\nof forecast performance, and the feedback provided to students. Furthermore, we\ndescribe empirical properties of students' probabilistic forecasts, as well as\nsome lessons learned on our part.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.16171v2"
    },
    {
        "title": "Challenges and Successes of Emergency Online Teaching in Statistics\n  Courses",
        "authors": [
            "Analisa Flores",
            "Lauren Cappiello",
            "Isaac Quintanilla Salinas"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  As the COVID-19 pandemic took hold in early months of 2020, education at all\nlevels was pushed to emergency fully remote, online formats. This emergency\nshift affected all aspects of teaching and learning with very little notice and\noften with limited resources. Educators were required to convert entire courses\nonline and shift to remote instructional approaches practically overnight.\nStudents found themselves enrolled in online courses without choice and\nstruggling to adjust to their new learning environments. This article\nhighlights some of the challenges and successes of teaching emergency online\nundergraduate statistics courses. In particular, we discuss challenges and\nsuccesses related to (1) technology, (2) classroom community and feedback, and\n(3) student-content engagement. We also reflect on the opportunity to continue\nto enhance and enrich the learning experiences of our students by utilizing\nsome of the lessons learned from emergency online teaching as new permanent\nonline statistics courses are developed and/or moved back into the classroom.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.01476v1"
    },
    {
        "title": "Motivation literally. Construction and expression of educational\n  aspirations on Parcoursup",
        "authors": [
            "Marie-Paule Couto",
            "Marion Valarcher"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  This paper analyses the framing and expression of French high school\nstudents' aspirations. It sheds new light on the inequalities in tracking\nbetween academic versus technological and vocational track. Through the\nanalysis of a national survey and a corpus of cover letters written by\napplicants for a sociology degree, it shows that, due to the lack of means,\nteachers mainly have two types of guidance support strategies.Teachers use to\ntarget and concentrate their supporting on ``good students'' in vocational\ntracks, while, in academic tracks, they delegate some steps of the tracking\nprocedures to families. These different strategies have effects on the way high\nschool students internalise school prescriptions and restitute them in cover\nletters. Through the close support they benefit from teachers, ``good\nstudents'' in vocational tracks strongly internalise the instructions and their\nplace in the school hierarchy. In academic tracks, students' expression of the\naspirations is much more dependent of their familial capital.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.08256v1"
    },
    {
        "title": "Against normality testing",
        "authors": [
            "Paolo Frumento"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  I reject the following null hypothesis: {H0: your data are normal}. Such\ndrastic decision is motivated by theoretical reasons, and applies to your\ncurrent data, the past ones, and the future ones. While this situation may\nappear embarrassing, it does not invalidate any of your results. Moreover, it\nallows to save time and energy that are currently spent in vain by performing\nthe following unnecessary tasks: (i) carrying out normality tests; (ii)\npretending to do something if normality is rejected; and (iii) arguing about\nnormality with Referee #2.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.11536v1"
    },
    {
        "title": "Reimagining Doctoral Training in Statistics: Is There a Role for a\n  Professional Doctorate?",
        "authors": [
            "Camden L. Lopez"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Modern demands of the statistics profession call for reimagining statistics\ntraining. The discipline needs to attract and develop students who are\neffective as real-world problem solvers, interdisciplinary collaborators,\ncommunicators, leaders, and teachers. Demand for statistics professionals with\nbroad technical and non-technical skills has grown in a variety of settings,\nbut especially in business and industry. Academic curricula, though, remain\nprimarily oriented around a narrow, technical conception of statistics.\nAdvanced graduate-level training essentially is limited to research doctorate\n(PhD) programs which tend to prioritize theoretical and methodological research\nover development of effective applied statisticians. Other professions, such as\nthose of physicians and surgeons, have training oriented around a professional\ndoctorate, as opposed to a research doctorate. The statistics profession should\nconsider not only changes to PhD curricula, but also the potential for a\nprofessional doctorate, drawing ideas from the curricula of other professional\ndegrees such as the MD.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.08282v2"
    },
    {
        "title": "Successful and sustainable undergraduate research in data science",
        "authors": [
            "Audrey E Hendricks"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Undergraduate research experiences hold many potential benefits. Students can\nlearn about new areas opening up previously unknown paths in academia and\nindustry. The hands-on experience often provides a deeper understanding of what\nscience, research, and data analysis is and, importantly, is not. While\nnumerous studies have provided information about the benefits and challenges of\nundergraduate research, many still find it difficult to start an undergraduate\nresearch group. Here, we provide a roadmap and resources to help faculty of all\nlevels create and sustain an undergraduate research group in quantitative areas\nsuch as statistics, informatics, and data science. While we focus on\nundergraduate research in data science, many of the recommendations may be\ngenerally useful to research mentoring of all levels and other domains.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10792v3"
    },
    {
        "title": "Using online student focus groups in the development of new educational\n  resources",
        "authors": [
            "Gian Carlo Diluvi",
            "Sonja Isberg",
            "Bruce Dunham",
            "Nancy Heckman",
            "Melissa Lee"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Educational resources, such as web apps and self-directed tutorials, have\nbecome popular tools for teaching and active learning. Ideally, students - the\nintended users of these resources - should be involved in the resource\ndevelopment stage. However, in practice students often only interact with fully\ndeveloped resources, when it might be too late to incorporate changes. Previous\nwork has addressed this by involving students in the development of new\nresources via in-person focus groups and interviews. In these, the resource\ndevelopers observe students interacting with the resource. This allows\ndevelopers to incorporate their observations and students' direct feedback into\nfurther development of the resource. However, as a result of the COVID-19\npandemic, carrying out in-person focus groups became infeasible due to social\ndistancing restrictions. Instead, online meetings and classes became\nubiquitous. In this work, we describe a fully-online methodology to evaluate\nnew resources in development. Specifically, our methodology consists of\ncarrying out student focus groups via online video conferencing software. We\nassessed two educational resources for introductory statistics using our\nmethodology and found that the online setting allowed us to obtain rich,\ndetailed information from the students. We also found online focus groups to be\nmore efficient: students and researchers did not need to travel and scheduling\nwas not restricted by the availability of physical space. Our findings suggest\nthat online focus groups are an attractive alternative to in-person focus\ngroups for student assessment of resources in development, even now that\npandemic restrictions are being eased.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.00149v1"
    },
    {
        "title": "The Design and Implementation of a Bayesian Data Analysis Lesson for\n  Pre-Service Mathematics and Science Teachers",
        "authors": [
            "Mine Dogucu",
            "Sibel Kazak",
            "Joshua Rosenberg"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  With the rise of the popularity of Bayesian methods and accessible computer\nsoftware, teaching and learning about Bayesian methods are expanding. However,\nmost educational opportunities are geared toward statistics and data science\nstudents and are less available in the broader STEM fields. In addition, there\nare fewer opportunities at the K-12 level. With the indirect aim of introducing\nBayesian methods at the K-12 level, we have developed a Bayesian Data Analysis\nactivity and implemented it with 35 mathematics and science pre-service\nteachers. In this manuscript, we describe the activity, the web app supporting\nthe activity, and pre-service teachers' perceptions of the activity. Lastly, we\ndiscuss future directions for preparing K-12 teachers in teaching and learning\nabout Bayesian methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.01276v1"
    },
    {
        "title": "Communication of Statistics and Evidence in Times of Crisis",
        "authors": [
            "Claudia R Schneider",
            "John R Kerr",
            "Sarah Dryhurst",
            "John A D Aston"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  This review provides an overview of concepts relating to the communication of\nstatistical and empirical evidence in times of crisis, with a special focus on\nCOVID-19. In it, we consider topics relating both to the communication of\nnumbers -- such as the role of format, context, comparisons, and visualization\n-- and the communication of evidence more broadly -- such as evidence quality,\nthe influence of changes in available evidence, transparency, and repeated\ndecision making. A central focus is on the communication of the inherent\nuncertainties in statistical analysis, especially in rapidly changing\ninformational environments during crises. We present relevant literature on\nthese topics and draw connections to the communication of statistics and\nempirical evidence during the COVID-19 pandemic and beyond. We finish by\nsuggesting some considerations for those faced with communicating statistics\nand evidence in times of crisis.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.01789v1"
    },
    {
        "title": "Smart Sports Predictions via Hybrid Simulation: NBA Case Study",
        "authors": [
            "Ignacio Erazo"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Increased data availability has stimulated the interest in studying sports\nprediction problems via analytical approaches; in particular, with machine\nlearning and simulation. We characterize several models that have been proposed\nin the literature, all of which suffer from the same drawback: they cannot\nincorporate rational decision-making and strategies from teams/players\neffectively. We tackle this issue by proposing hybrid simulation logic that\nincorporates teams as agents, generalizing the models/methodologies that have\nbeen proposed in the past. We perform a case study on the NBA with two goals:\ni) study the quality of predictions when using only one predictive variable,\nand ii) study how much historical data should be kept to maximize prediction\naccuracy. Results indicate that there is an optimal range of data quantity and\nthat studying what data and variables to include is of extreme importance.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09918v2"
    },
    {
        "title": "Using Auxiliary Data to Guide the Recruitment of Sites for Randomized\n  Controlled Trials",
        "authors": [
            "Robert B. Olsen",
            "Maria L. Vasquez-Rossi"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Sampling methods such as Stratified Random Sampling can be used to select\nrepresentative samples of schools for randomized controlled trials of\neducational interventions. However, these methods may still yield external\nvalidity bias when participation by schools is voluntary and participation\ndecisions are associated with unobserved variables. This paper offers a new\nsampling method called Stratified Random Sampling with Quotas. Under this\nmethod, quotas are set to avoid including too many schools of a particular\ntype, as defined by auxiliary variables that are unobserved in the sampling\nframe, but whose population distribution can be estimated from external data.\nOur simulations find that when the auxiliary variables affect whether or not a\nschool participates in the study, quotas set based on those variables reduce\nexternal validity bias. These results suggest that when auxiliary data are\navailable on strong impact moderators for the target population, these data can\nbe used to address non-ignorable self-selection by schools into randomized\ncontrolled trials.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.12237v1"
    },
    {
        "title": "Downscaling Epidemiological Time Series Data for Improving Forecasting\n  Accuracy: An Algorithmic Approach",
        "authors": [
            "Mahadee Al Mobin",
            "Md. Kamrujjaman"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Data scarcity and discontinuity are common occurrences in the healthcare and\nepidemiological dataset and often need help in forming an educative decision\nand forecasting the upcoming scenario. Often, these data are stored as\nmonthly/yearly aggregate where the prevalent forecasting tools like\nAutoregressive Integrated Moving Average (ARIMA), Seasonal Autoregressive\nIntegrated Moving Average (SARIMA), and TBATS often fail to provide\nsatisfactory results. Artificial data synthesis methods have been proven to be\na powerful tool for tackling these challenges. The paper aims to propose a\ndownscaling data algorithm based on the underlying distribution. Our findings\nshow that the synthesized data is in agreement with the original data in terms\nof trend, seasonality, and residuals, and the synthesized data provides a\nstable foothold for the forecasting tools to generate a much more accurate\nforecast of the situation.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.12305v1"
    },
    {
        "title": "Onset of a conceptual outline map to get a hold on the jungle of cluster\n  analysis",
        "authors": [
            "Iven Van Mechelen",
            "Christian Hennig",
            "Henk A. L. Kiers"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  The domain of cluster analysis is a meeting point for a very rich\nmultidisciplinary encounter, with cluster-analytic methods being studied and\ndeveloped in discrete mathematics, numerical analysis, statistics, data\nanalysis, data science, and computer science (including machine learning, data\nmining, and knowledge discovery), to name but a few. The other side of the\ncoin, however, is that the domain suffers from a major accessibility problem as\nwell as from the fact that it is rife with division across many pretty isolated\nislands. As a way out, the present paper offers a thorough and in-depth review\nof the clustering domain as a whole under the form of an outline map based on\nan overarching conceptual framework and a common language. With this framework\nwe wish to contribute to structuring the clustering domain, to characterizing\nmethods that have often been developed and studied in quite different contexts,\nto identifying links between methods, and to introducing a frame of reference\nfor optimally setting up cluster analyses in data-analytic practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13406v6"
    },
    {
        "title": "The Saltbox-Roof Probability Distribution",
        "authors": [
            "Ludger O. Suarez-Burgoa"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  The saltbox-roof parametric probability distribution is a special case of the\ntriangular distribution, where only one side is truncated. Here it is presented\nas a single and independent distribution, where the explicit equations are\ndefined for its probability density--, the cumulative distribution--, and the\ninverse of the cumulative distribution (quantile--) functions as also its\nrandom generator. Four parameters are necessary to define it: the lower and the\nupper limits, the mode, and a shape parameter. Also, the saltbox-roof\ndistribution degenerates into the uniform distribution, into a kind of a\ntrapezoidal distribution and into other special cases of the general triangular\ndistribution, all of them which are related to the domain of the shape\nparameter within the mode. The mean, median, and the variance are also here\nexpressed by explicit equations. The function equations have been verified with\ntheorems of truncated distributions. Three application examples are exposed.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03108v1"
    },
    {
        "title": "Measurable Taylor's Theorem: An Elementary Proof",
        "authors": [
            "Gianluca Viggiano"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  The Taylor expansion is a widely used and powerful tool in all branches of\nMathematics, both pure and applied. In Probability and Mathematical Statistics,\nhowever, a stronger version of Taylor's classical theorem is often needed, but\nonly tacitly assumed. In this note, we provide an elementary proof of this\nmeasurable Taylor's theorem, which guarantees that the interpolating point in\nthe Lagrange form of the remainder can be chosen to depend measurably on the\nindependent variable.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.04486v1"
    },
    {
        "title": "Over-Measurement Paradox: Suspension of Thermonuclear Research Center\n  and Need to Update Standards",
        "authors": [
            "Hector Reyes",
            "Saeid Tizpaz-Niari",
            "Vladik Kreinovich"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  In general, the more measurements we perform, the more information we gain\nabout the system and thus, the more adequate decisions we will be able to make.\nHowever, in situations when we perform measurements to check for safety, the\nsituation is sometimes opposite: the more additional measurements we perform\nbeyond what is required, the worse the decisions will be: namely, the higher\nthe chance that a perfectly safe system will be erroneously classified as\nunsafe and therefore, unnecessary additional features will be added to the\nsystem design. This is not just a theoretical possibility: exactly this\nphenomenon is one of the reasons why the construction of a world-wide\nthermonuclear research center has been suspended. In this paper, we show that\nthe reason for this paradox is in the way the safety standards are formulated\nnow -- what was a right formulation when sensors were much more expensive is no\nlonger adequate now when sensors and measurements are much cheaper. We also\npropose how to modify the safety standards so as to avoid this paradox and make\nsure that additional measurements always lead to better solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.08041v1"
    },
    {
        "title": "Comparison of open-source software for producing directed acyclic graphs",
        "authors": [
            "Amy J. Pitts",
            "Charlotte R. Fowler"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Many software packages have been developed to assist researchers in drawing\ndirected acyclic graphs (DAGs), each with unique functionality and usability.\nWe examine five of the most common software to generate DAGs: TikZ, DAGitty,\nggdag, dagR, and igraph. For each package, we provide a general description of\nthe its background, analysis and visualization capabilities, and\nuser-friendliness. Additionally in order to compare packages, we produce two\nDAGs in each software, the first featuring a simple confounding structure,\nwhile the second includes a more complex structure with three confounders and a\nmediator. We provide recommendations for when to use each software depending on\nthe user's needs.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.12006v3"
    },
    {
        "title": "Orderings of extremes among dependent extended Weibull random variables",
        "authors": [
            "Ramkrishna Jyoti Samanta",
            "Sangita Das",
            "N. Balakrishnan"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  In this work, we consider two sets of dependent variables\n$\\{X_{1},\\ldots,X_{n}\\}$ and $\\{Y_{1},\\ldots,Y_{n}\\}$, where $X_{i}\\sim\nEW(\\alpha_{i},\\lambda_{i},k_{i})$ and $Y_{i}\\sim EW(\\beta_{i},\\mu_{i},l_{i})$,\nfor $i=1,\\ldots, n$, which are coupled by Archimedean copulas having different\ngenerators. Also, let $N_{1}$ and $N_{2}$ be two non-negative integer-valued\nrandom variables, independent of $X_{i}'$s and $Y_{i}'$s, respectively. We then\nestablish different inequalities between two extremes, namely, $X_{1:n}$ and\n$Y_{1:n}$ and $X_{n:n}$ and $Y_{n:n}$, in terms of the usual stochastic, star,\nLorenz, hazard rate, reversed hazard rate and dispersive orders. We also\nestablish some ordering results between $X_{1:{N_{1}}}$ and $Y_{1:{N_{2}}}$ and\n$X_{{N_{1}}:{N_{1}}}$ and $Y_{{N_{2}}:{N_{2}}}$ in terms of the usual\nstochastic order. Several examples and counterexamples are presented for\nillustrating all the results established here. Some of the results here extend\nthe existing results of Barmalzan et al. (2020).\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00590v1"
    },
    {
        "title": "Multiple Correspondence and Proportional Analysis of Vaccination Rate\n  Among Healthcare Personnel of MINSA",
        "authors": [
            "Luz B. Valenzuela-Narvaez",
            "Wladimir A. Carlosviza-Amanqui",
            "Fred Torres-Cruz"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  DataProAnalytica is a powerful application for analyzing vaccination data in\nhealth care professionals. Through visualizations and multiple correspondence\nanalysis, it uncovers meaningful relationships between variables and\ncategories. The results provide valuable information for improving vaccination\nstrategies. While there are limitations, the potential of DataProAnalytica to\nimprove accuracy and functionality makes it a promising tool for future\nresearch and decision making in any other research topic.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.05876v1"
    },
    {
        "title": "Peru Mining: Analysis and Forecast of Mining Production in Peru Using\n  Time Series and Data Science Techniques",
        "authors": [
            "Yhack Bryan Aycaya-Paco",
            "Lindell Dennis Vilca-Mamani",
            "Fred Torres-Cruz"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Peruvian mining plays a crucial role in the country's economy, being one of\nthe main producers and exporters of minerals worldwide. In this project, an\napplication was developed in RStudio that utilizes statistical analysis and\ntime series modeling techniques to understand and forecast mineral extraction\nin different departments of Peru. The application includes an interactive map\nthat allows users to explore Peruvian geography and obtain detailed statistics\nby clicking on each department. Additionally, bar charts, pie charts, and\nfrequency polygons were implemented to visualize and analyze the data. Using\nthe ARIMA model, predictions were made on the future extraction of minerals,\nenabling informed decision-making in planning and resource management within\nthe mining sector. The application provides an interactive and accessible tool\nto explore the Peruvian mining industry, comprehend trends, and make accurate\nforecasts. These predictions for 2027 in total annual production are as\nfollows: Copper = 2,694,957 MT, Gold = 72,817.47 kg Fine, Zinc = 1,369,649 MT,\nSilver = 3,083,036 MT, Lead = 255,443 MT, Iron = 15,776,609 MT, Tin = 29,542\nMT, Molybdenum = 35,044.66 MT, and Cadmium = 724 MT. These predictions, based\non historical data, provide valuable information for strategic decision-making\nand contribute to the sustainable development of the mining industry in Peru.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.06293v1"
    },
    {
        "title": "Evaluation of Active Affiliates to the SIS Multidimensional Analysis in\n  R Shiny",
        "authors": [
            "Nadine ACeituno-Moya",
            "Fred Torres-Cruz"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  This article presents a study that uses multiple linear regression analysis\nto examine the factors influencing the number of people affiliated with\ndifferent insurance plans within the Comprehensive Health Insurance (SIS)\nsystem in Peru.The study highlights the importance of multiple linear\nregression analysis in understanding the factors that affect SIS Comprehensive\nHealth Insurance affiliates. It also showcases the value of utilizing\ninteractive tools like RShiny to enhance data analysis, providing a dynamic and\nparticipatory experience for researchers and users interested in the subject.To\nfacilitate the analysis and visualization of SIS-related data, the researchers\ndeveloped an interactive application using RShiny. This tool allows for the\neasy loading, visualization, and analysis of data in a user-friendly and\npractical manner. By providing an interactive platform, users can effectively\nexplore and understand the factors that impact SIS affiliates.The results of\nthe analysis indicate that the selected variables have a significant positive\ninfluence on the total number of affiliates. This suggests that the specific\ninsurance plan examined in this study has a favorable effect on the enrollment\nof individuals in SIS. Additionally, the data shows a linear trend, supporting\nthe use of a linear regression model to describe this relationship.\n  Active affiliates,Comprehensive health insurance SIS,Data\nVisualization,Multiple Linear Regression Analysis,RShiny\n",
        "pdf_link": "http://arxiv.org/pdf/2307.07058v1"
    },
    {
        "title": "Prediction of breast cancer with 98% accuracy",
        "authors": [
            "Condori Condori Nelyda Ayde",
            "Mamani Mamani Ilma Magda",
            "Cruz Paredes Soledad Epifania",
            "Torres-Cruz Fred"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Abstract Cancer is a tumor that affects people worldwide, with a higher\nincidence in females but not excluding males. It ranks among the top five\ndeadliest types of cancer, particularly prevalent in less developed countries\nwith deficient healthcare programs. Finding the best algorithm for effective\nbreast cancer prediction with minimal error is crucial. In this scientific\narticle, we employed the SMOTE method in conjunction with the R package Shiny\nto enhance the algorithms and improve prediction accuracy. We classified the\ntumor types as benign and malignant (B/M). Various algorithms were analyzed\nusing a Kaggle dataset, and our study identified the superior algorithm as\nlogistic regression. We evaluated algorithm performance using confusion\nmatrices to visualize results and the ROC Curve to obtain a comprehensive\nmeasure of performance. Additionally, we calculated precision by dividing the\nnumber of correct predictions by the total predictions Keywords Breast cancer,\nSmote, Benign, Malignant.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.07571v1"
    },
    {
        "title": "DataXploreFines: Generalized Data for Informed Decision, Making, An\n  Interactive Shiny Application for Data Analysis and Visualization",
        "authors": [
            "Torres Cruz",
            "Fred Garcia Jimenez",
            "Angel Raul Quispe Bravo",
            "Eder Ander"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  This article presents DataXploreFines, an innovative Shiny application that\nrevolutionizes data exploration, analysis, and visualization. The application\noffers functionalities for data loading, management, summarization, basic\ngraphs, advanced analysis, and contact. Users can upload their datasets in\npopular formats like CSV or Excel, explore the data structure, perform\nmanipulations, and obtain statistical summaries. DataXploreFines provides a\nwide range of interactive visualizations, including histograms, scatter plots,\nbar charts, and line graphs, enabling users to identify patterns and trends.\nAdditionally, the application offers statistical tools such as time series\nanalysis using ARIMA and SARIMA models, forecasting, and Ljung-Box statistic.\nIts user-friendly interface empowers individuals from various domains,\nincluding beginners in statistics, to make informed decisions.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.11056v1"
    },
    {
        "title": "Comparative analysis using classification methods versus early stage\n  diabetes",
        "authors": [
            "Alca-Vilca Gabriel Anthony",
            "Carpio-Vargas Eloy"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  In this research work, a comparative analysis was carried out using\nclassification methods such as: Discriminant Analysis and Logistic Regression\nto subsequently predict whether a person may have the presence of early stage\ndiabetes. For this purpose, use was made of a database of the UC IRVINE\nplatform of the year 2020 where specific variables that influence diabetes were\nused for a better result. Likewise in terms of methodology, the corresponding\nanalysis was performed for each of the 3 classification methods and then take\nthem to a comparative table and analyze the results obtained. Finally we can\nadd that the majority of the studies carried out applying the classification\nmethods to the diseases can be clearly seen that there is a certain attachment\nand more use of the logistic regression classification method, on the other\nhand, in the results we could see significant differences in terms of the 2\nclassification methods that were applied, which was valuable information for\nlater drawing final conclusions.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.12296v1"
    },
    {
        "title": "CATASTROAGRI -- Interactive data analysis and visualization application\n  with a future projection for catastrophic agricultural insurance",
        "authors": [
            "Marizol Lizbeth Serrano Quispe",
            "Fred Torres Cruz"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  CATASTROAGRI is an application developed to load, analyze and interactively\nvisualize relevant data on catastrophic agricultural insurance. It also focuses\non the analysis of an ARIMA (0,1,1) (0,1,1) model to identify and estimate\npatterns in the agricultural data of the Puno Region, it presents a decreasing\ntrend because there is a significant relationship between successive values of\nthe time series, We can also state that it is not stationary because the mean\nand variance do not remain constant over time and the series has periods, and\nit is observed that the cases are decreasing and increasing over the years,\nespecially the amount to indemnify due to the behavior of the climate in the\nhighlands. The results of the analysis show that agricultural insurance plays\nan important role in protecting farmers against losses caused by adverse\nclimatic events. The importance of concentrating resources and indemnities on\nthe most affected crops and in the provinces with the highest agricultural\nproduction is emphasized. The results of the users' evaluation showed a high\nlevel of satisfaction, as well as ease of use.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.12844v1"
    },
    {
        "title": "Time Series Analysis Applied to Notifications of Work Accidents",
        "authors": [
            "Tony Gabriel-Fernandez Cayo",
            "Fred Torres-Cruz"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Time series analysis applied to occupational accident reports is a powerful\ntool for understanding the evolution of occupational accidents over time. It\nprovides valuable information to make informed decisions. In this study, data\nfrom reports of work accidents collected from the MINISTRY OF LABOR AND\nEMPLOYMENT PROMOTION. MTPE were analyzed by time series. Significant patterns\nand trends in accident reporting have been identified, leading to more\neffective prevention strategies and better health and safety management.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.13748v1"
    },
    {
        "title": "Prediction of depression status in college students using a Naive Bayes\n  classifier based machine learning model",
        "authors": [
            "Fred Torres Cruz",
            "Evelyn Eliana Coaquira Flores",
            "Sebastian Jarom Condori Quispe"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  This study presents a machine learning model based on the Naive Bayes\nclassifier for predicting the level of depression in university students, the\nobjective was to improve prediction accuracy using a machine learning model\ninvolving 70% training data and 30% validation data based on the Naive Bayes\nclassifier, the collected data includes factors associated with depression from\n519 university students, the results showed an accuracy of 78.03%, high\nsensitivity in detecting positive cases of depression, especially at moderate\nand severe levels, and significant specificity in correctly classifying\nnegative cases, these findings highlight the effectiveness of the model in\nearly detection and treatment of depression, benefiting vulnerable sectors and\ncontributing to the improvement of mental health in the student population.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14371v1"
    },
    {
        "title": "The Mastery Rubric for Statistics and Data Science: promoting coherence\n  and consistency in data science education and training",
        "authors": [
            "Rochelle E. Tractenberg",
            "Donna LaLonde",
            "Suzanne Thornton"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Consensus based publications of both competencies and undergraduate\ncurriculum guidance documents targeting data science instruction for higher\neducation have recently been published. Recommendations for curriculum features\nfrom diverse sources may not result in consistent training across programs. A\nMastery Rubric was developed that prioritizes the promotion and documentation\nof formal growth as well as the development of independence needed for the 13\nrequisite knowledge, skills, and abilities for professional practice in\nstatistics and data science, SDS. The Mastery Rubric, MR, driven curriculum can\nemphasize computation, statistics, or a third discipline in which the other\nwould be deployed or, all three can be featured. The MR SDS supports each of\nthese program structures while promoting consistency with international,\nconsensus based, curricular recommendations for statistics and data science,\nand allows 'statistics', 'data science', and 'statistics and data science'\ncurricula to consistently educate students with a focus on increasing learners\nindependence. The Mastery Rubric construct integrates findings from the\nlearning sciences, cognitive and educational psychology, to support teachers\nand students through the learning enterprise. The MR SDS will support higher\neducation as well as the interests of business, government, and academic work\nforce development, bringing a consistent framework to address challenges that\nexist for a domain that is claimed to be both an independent discipline and\npart of other disciplines, including computer science, engineering, and\nstatistics. The MR-SDS can be used for development or revision of an evaluable\ncurriculum that will reliably support the preparation of early e.g.,\nundergraduate degree programs, middle e.g., upskilling and training programs,\nand late e.g., doctoral level training practitioners.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.08004v1"
    },
    {
        "title": "Potential Outcomes and Decision Theoretic Foundations for Statistical\n  Causality: Response to Richardson and Robins",
        "authors": [
            "A. Philip Dawid"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  I thank Thomas Richardson and James Robins for their discussion of my paper,\nand discuss the similarities and differences between their approach to causal\nmodelling, based on single world intervention graphs, and my own\ndecision-theoretic approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.02234v2"
    },
    {
        "title": "Monografía de Estadística Bayesiana",
        "authors": [
            "Arturo Erdely",
            "Eduardo Gutiérrez-Peña"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Course notes about an introduction to Bayesian Statistics. First, an\nexplanation of the bayesian paradigm is motivated and explained in detail\n(first three chapters). Then, a brief introduction to the basics about Decision\nTheory in chapter four, which is self contained, with the purpose of\nintroducing parametrica bayesian inference as a decision problem in chapter\nfive.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.06601v1"
    },
    {
        "title": "Creating Community in a Data Science Classroom",
        "authors": [
            "David Kane"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  A community is a collection of people who know and care about each other. The\nvast majority of college courses are not communities. This is especially true\nof statistics and data science courses, both because our classes are larger and\nbecause we are more likely to lecture. However, it is possible to create a\ncommunity in your classroom. This article offers an idiosyncratic set of\npractices for creating community. I have used these techniques successfully in\nfirst and second semester statistics courses with enrollments ranging from 40\nto 120. The key steps are knowing names, cold calling, classroom seating, a\nshallow learning curve, Study Halls, Recitations and rotating-one-on-one final\nproject presentations.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.06983v1"
    },
    {
        "title": "How does international guidance for statistical practice align with the\n  ASA Ethical Guidelines?",
        "authors": [
            "Rochelle E. Tractenberg",
            "Jennifer Park"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Gillikin (2017) defines a 'practice standard' as a document to 'define the\nway the profession's body of knowledge is ethically translated into day-to-day\nactivities' (Gillikin 2017, p. 1). Such documents fulfill three objectives:\nthey 1) define the profession; 2) communicate uniform standards to\nstakeholders; and 3) reduce conflicts between personal and professional conduct\n(Gillikin, 2017 p. 2). However, there are many guidelines - this is due to\ndifferent purposes that guidance writers may have, as well as to the fact that\nthere are different audiences for the many guidance documents. The existence of\ndiverse statements do not necessarily make it clear that there are\ncommonalities; and while some statements are explicitly aspirational,\nprofessionals as well as the public need to know that ethically-trained\npractitioners follow accepted practice standards. This paper applies the\nmethodological approach described in Tractenberg (2023) and demonstrated in\nPark and Tractenberg (2023) to study alignment among international guidance for\nofficial statistics, and between these guidance documents and the ASA Ethical\nGuidelines for Statistical Practice functioning as an ethical practice standard\n(Tractenberg, 2022-A, 2022-B; after Gillikin 2017). In the spirit of exchanging\nexperiences and lessons learned, we discuss how our findings could inform\ncloser examination, clarification, and, if beneficial, possible revision of\nguidance in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.08713v4"
    },
    {
        "title": "Classroom Community amid Covid-19: A Mixed-Methods Study of\n  Undergraduate Students in Introductory Mathematics and Statistics",
        "authors": [
            "Shira Viel",
            "Maria Tackett",
            "Sarwari Das",
            "Joseph Choo"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  A strong sense of classroom community is associated with many positive\nlearning outcomes and is a critical contributor to undergraduate students'\npersistence in STEM, particularly for women and students of color. This chapter\ndescribes a mixed-methods investigation into the relationship between classroom\ncommunity and course attributes in introductory undergraduate mathematics and\nstatistics courses, mediated by student demographics. The project was motivated\nby and conducted amid the Covid-19 pandemic: data were collected from online\ncourses in the 2021-21 academic year and from hybrid and in-person courses in\nthe 2021-22 academic year. Quantitative data was gathered from both students\nand instructors and analyzed using structural equation modeling. The primary\ninstrument was the validated Classroom Community Scale - Short Form. These\nquantitative results are complemented and contextualized by thematic and\ntextual analyses of focus group data, gathered using a newly developed protocol\npiloted during the 2021-22 academic year. All data comes from a highly\nselective private university in the United States. Preliminary practical\nimplications of the study include the value of synchronous participation in\nfostering connectedness and the importance of attending to students' personal\nidentities in understanding their experiences of belonging.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.11739v2"
    },
    {
        "title": "A review of Design of Experiments courses offered to undergraduate\n  students at American universities",
        "authors": [
            "Alan R. Vazquez",
            "Xiaocong Xuan"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Design of Experiments (DoE) is a relevant class to undergraduate students in\nthe sciences, because it teaches them how to plan, conduct, and analyze\nexperiments. In the literature on DoE, there are several contributions to its\npedagogy, such as easy-to-use class experiments, virtual experiments, and\nsoftware to construct experimental designs. However, there are virtually no\nsystematic evaluations of the actual DoE pedagogy. To address this issue, we\nbuild the first database of DoE courses offered to undergraduate students in\nthe United States. The database has records on courses offered from 2019 to\n2022 at the best universities in the US News Best National Universities ranking\nof 2022. Specifically, it has data on 18 general and content-specific features\nof 206 courses. To study the DoE pedagogy, we analyze the database using\ndescriptive statistics and text mining. Based on our analysis, we provide\ninstructors with recommendations and teaching material to enhance their DoE\ncourses. The database and material are included in the supplement of this\narticle.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.16961v2"
    },
    {
        "title": "The concept of probability, crisis in statistics, and the unbearable\n  lightness of Bayesing",
        "authors": [
            "Boris Čulina"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Education in statistics, the application of statistics in scientific\nresearch, and statistics itself as a scientific discipline are in crisis.\nWithin science, the main cause of the crisis is the insufficiently clarified\nconcept of probability. This article aims to separate the concept of\nprobability which is scientifically based from other concepts that do not have\nthis characteristic. The scientifically based concept of probability is\nKolmogorovs concept of probability models together with the conditions of their\napplicability. Bayesian statistics is based on the subjective concept of\nprobability, and as such can only have a heuristic value in searching for the\ntruth, but it cannot and must not replace the truth. The way out of the crisis\nshould take Kolmogorov and Bayesian analysis as elements, each of which has a\nwell-defined and limited use. Only together with qualitative analysis and other\ntypes of quantitative analysis, and combined with experiments, they can\ncontribute to reaching correct conclusions.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00002v1"
    },
    {
        "title": "Statistics for Machine Learning with Mathematica Applications",
        "authors": [
            "M. M. Hammad"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  In recent years, the field of statistics has experienced a surge in interest\nand application, largely due to significant advances in computer technology.\nThis progress has led to remarkable developments in statistics methods and\nalgorithms, enabling their widespread adoption across various disciplines. Key\nareas benefiting from these advancements include machine learning, economics,\nfinance, geophysics, molecular modeling, computational systems biology,\noperations research, and engineering. For example, in machine learning,\nstatistics forms the foundation for algorithms used in regression,\nclassification, clustering, and deep learning to analyze vast datasets and make\npredictions. Mathematica, among other tools, has played a significant role in\nenabling the integration of statistics and computer technology, facilitating\ndeeper exploration of data-driven insights and groundbreaking discoveries\nacross diverse domains. With a rich library of functions, Mathematica allows\nusers to calculate measures of central tendency, dispersion, and correlation,\nas well as perform hypothesis testing and estimation. Moreover, it supports\nprobability distributions, making simulations and probabilistic modeling tasks\nmore accessible. This monograph presents the main theorems in mathematical\nstatistics, ranging from basic descriptive statistics to sophisticated\ninferential techniques. In addition, we have created, more than 200 manipulates\ncover different scenarios in statistics, more than 500 light Mathematica codes\n(examples) and 25 programs (procedures) that follow the principles of testing\nhypotheses and estimation theory. The code will run as-is with no code from\nprior algorithms or third parties required beyond the installation of\nMathematica.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00004v1"
    },
    {
        "title": "Data Science at the Singularity",
        "authors": [
            "David Donoho"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  A purported `AI Singularity' has been in the public eye recently. Mass media\nand US national political attention focused on `AI Doom' narratives hawked by\nsocial media influencers. The European Commission is announcing initiatives to\nforestall `AI Extinction'. In my opinion, `AI Singularity' is the wrong\nnarrative for what's happening now; recent happenings signal something else\nentirely. Something fundamental to computation-based research really changed in\nthe last ten years. In certain fields, progress is dramatically more rapid than\npreviously, as the fields undergo a transition to frictionless reproducibility\n(FR). This transition markedly changes the rate of spread of ideas and\npractices, affects mindsets, and erases memories of much that came before.\n  The emergence of frictionless reproducibility follows from the maturation of\n3 data science principles in the last decade. Those principles involve data\nsharing, code sharing, and competitive challenges, however implemented in the\nparticularly strong form of frictionless open services. Empirical Machine\nLearning (EML) is todays leading adherent field, and its consequent rapid\nchanges are responsible for the AI progress we see. Still, other fields can and\ndo benefit when they adhere to the same principles.\n  Many rapid changes from this maturation are misidentified. The advent of FR\nin EML generates a steady flow of innovations; this flow stimulates outsider\nintuitions that there's an emergent superpower somewhere in AI. This opens the\nway for PR to push worrying narratives: not only `AI Extinction', but also the\nsupposed monopoly of big tech on AI research. The helpful narrative observes\nthat the superpower of EML is adherence to frictionless reproducibility\npractices; these practices are responsible for the striking progress in AI that\nwe see everywhere.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.00865v1"
    },
    {
        "title": "Philosophy within Data Science Ethics Courses",
        "authors": [
            "Sara Colando",
            "Johanna Hardin"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  There is wide agreement that ethical considerations are a valuable aspect of\na data science curriculum, and to that end, many data science programs offer\ncourses in data science ethics. There are not always, however, explicit\nconnections between data science ethics and the centuries-old work on ethics\nwithin the discipline of philosophy. Here, we present a framework for bringing\ntogether key data science practices with ethics topics. The ethics topics were\ncollated from sixteen data science ethics courses with public-facing syllabi\nand reading lists. We encourage individuals who are teaching data science\nethics to engage with the philosophical literature and its connection to\ncurrent data science practices, which are rife with potentially morally charged\ndecision points.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.02444v3"
    },
    {
        "title": "Interview with Adrian Raftery",
        "authors": [
            "Leontine Alkema",
            "Thomas Brendan Murphy",
            "Adrian E. Raftery"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Professor Adrian E. Raftery is the Boeing International Professor of\nStatistics and Sociology, and an adjunct professor of Atmospheric Sciences, at\nthe University of Washington in Seattle. He was born in Dublin, Ireland, and\nobtained a B.A. in Mathematics and an M.Sc. in Statistics and Operations\nResearch at Trinity College Dublin. He obtained a doctorate in mathematical\nstatistics from the Universit\\'e Pierre et Marie Curie under the supervision of\nPaul Deheuvels. He was a lecturer in statistics at Trinity College Dublin, and\nthen an associate and full professor of statistics and sociology at the\nUniversity of Washington. He was the founding Director of the Center for\nStatistics and Social Sciences.\n  Professor Raftery has published over 200 articles in peer-reviewed\nstatistical, sociological and other journals. His research focuses on Bayesian\nmodel selection and Bayesian model averaging, model-based clustering, inference\nfor deterministic models, and the development of new statistical methods for\ndemography, sociology, and the environmental and health sciences.\n  He is a member of the United States National Academy of Sciences, a Fellow of\nthe American Academy of Arts and Sciences, an Honorary Member of the Royal\nIrish Academy, a member of the Washington State Academy of Sciences, a Fellow\nof the American Statistical Association, a Fellow of the Institute of\nMathematical Statistics, and an elected Member of the Sociological Research\nAssociation. He has won multiple awards for his research. He was Coordinating\nand Applications Editor of the Journal of the American Statistical Association\nand Editor of Sociological Methodology. He was identified as the world's most\ncited researcher in mathematics for the period 1995-2005.\n  Thirty-three students have obtained Ph.D.'s working under Raftery's\nsupervision, of whom 21 hold or have held tenure-track university faculty\npositions.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.11095v2"
    },
    {
        "title": "Unraveling the Skillsets of Data Scientists: Text Mining Analysis of\n  Dutch University Master Programs in Data Science and Artificial Intelligence",
        "authors": [
            "Mathijs J. Mol",
            "Barbara Belfi",
            "Zsuzsa Bakk"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  The growing demand for data scientists in the global labor market and the\nNetherlands has led to a rise in data science and artificial intelligence (AI)\nmaster programs offered by universities. However, there is still a lack of\nclarity regarding the specific skillsets of data scientists. This study aims to\naddress this issue by employing Correlated Topic Modeling (CTM) to analyse the\ncontent of 41 master programs offered by seven Dutch universities. We assess\nthe differences and similarities in the core skills taught by these programs,\ndetermine the subject-specific and general nature of the skills, and provide a\ncomparison between the different types of universities offering these programs.\nOur findings reveal that research, data processing, statistics and ethics are\nthe predominant skills taught in Dutch data science and AI master programs,\nwith general universities emphasizing research skills and technical\nuniversities focusing more on IT and electronic skills. This study contributes\nto a better understanding of the diverse skillsets of data scientists, which is\nessential for employers, universities, and prospective students.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.14726v1"
    },
    {
        "title": "A Conversation with A. Philip Dawid",
        "authors": [
            "Vladimir Vovk",
            "Glenn Shafer"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Beginning in the 1970s, Alexander Philip Dawid has been a leading contributor\nto the foundations of statistics and especially to the development and\napplication of Bayesian statistics. He is also known for his work on causality,\nespecially his notation for conditional independence and his critique of the\noveruse of counterfactuals, and for his contributions to forensic statistics.\n  Dawid was born in Lancashire, England, on February 1, 1946. His family moved\nto London soon afterwards, and he attended the City of London School from 1956\nto 1963. He studied mathematics at Cambridge, earning a BA (Bachelor of Arts)\ndegree in 1966. After earning a Diploma in Mathematical Statistics in the\nacademic year 1966-1967, he studied for a PhD at Imperial, then at UCL, where\nhe became a Lecturer in Statistics in 1969. In 1978, he left UCL for a position\nas Professor of Statistics in the Department of Mathematics, The City\nUniversity, London, where he served as Head of Statistics Section and Director\nof the Statistical Laboratory. He returned to the Department of Statistics at\nUCL in 1981, serving as Head of Department from 1983 to 1993. He moved to the\nUniversity of Cambridge in 2007, becoming Professor of Statistics and Fellow of\nDarwin College. He has continued his work in mathematical statistics after\nretiring from Cambridge in 2013 and was elected Fellow of the Royal Society in\n2018.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.00632v1"
    },
    {
        "title": "Human-Centric and Integrative Lighting Asset Management in Public\n  Libraries: Qualitative Insights and Challenges from a Swedish Field Study",
        "authors": [
            "Jing Lin",
            "Per Olof Hedekvist",
            "Nina Mylly",
            "Math Bollen",
            "Jingchun Shen",
            "Jiawei Xiong",
            "Christofer Silfvenius"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Traditional lighting source reliability evaluations, often covering just half\nof a lamp's volume, can misrepresent real-world performance. To overcome these\nlimitations,adopting advanced asset management strategies for a more holistic\nevaluation is crucial. This paper investigates human-centric and integrative\nlighting asset management in Swedish public libraries. Through field\nobservations, interviews, and gap analysis, the study highlights a disparity\nbetween current lighting conditions and stakeholder expectations, with issues\nlike eye strain suggesting significant improvement potential. We propose a\nshift towards more dynamic lighting asset management and reliability\nevaluations, emphasizing continuous enhancement and comprehensive training in\nhuman-centric and integrative lighting principles.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.11000v2"
    },
    {
        "title": "A computed 95% confidence interval does cover the true value with\n  probability 0.95 if epistemically interpreted",
        "authors": [
            "Dan Hedlin"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Suppose the lifetime of a large sample of batteries in routine use is\nmeasured. A confidence interval is computed to 394 plus/minus 1.96 times 4.6\ndays. The standard interpretation is that if we repeatedly draw samples and\ncompute confidence intervals, about 95% of the intervals will cover the unknown\ntrue lifetime. What can be said about the particular interval 394 plus/minus\n1.96 times 4.6 has not been clear. We clarify this by using an epistemic\ninterpretation of probability. The conclusion is that a realised (computed)\nconfidence interval covers the parameter with the probability given by the\nconfidence level is a valid statement, unless there are relevant and\nrecognisable subsets of the sample.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.10000v1"
    },
    {
        "title": "The PORTSEA (Portuguese School of Extremes and Applications) and a few\n  personal scientific achievements",
        "authors": [
            "M. Ivette Gomes"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  The Portuguese School of Extremes and Applications is nowadays well\nrecognised by the international scientific community, and in my opinion, the\norganisation of a NATO Advanced Study Institute on Statistical Extremes and\nApplications, which took place at Vimeiro in the summer of 1983, was a landmark\nfor the international recognition of the group. The dynamic of publication has\nbeen very high and the topics under investigation in the area of Extremes have\nbeen quite diverse. In this article, attention will be paid essentially to some\nof the scientific achievements of the author in this field.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.14414v2"
    },
    {
        "title": "A Systematic Literature Review of Undergraduate Data Science Education\n  Research",
        "authors": [
            "Mine Dogucu",
            "Sinem Demirci",
            "Harry Bendekgey",
            "Federica Zoe Ricci",
            "Catalina M. Medina"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  The presence of data science has been profound in the scientific community in\nalmost every discipline. An important part of the data science education\nexpansion has been at the undergraduate level. We conducted a systematic\nliterature review to (1) portray current evidence and knowledge gaps in\nself-proclaimed undergraduate data science education research and (2) inform\npolicymakers and the data science education community about what educators may\nencounter when searching for literature using the general keyword 'data science\neducation.' While open-access publications that target a broader audience of\ndata science educators and include multiple examples of data science programs\nand courses are a strength, significant knowledge gaps remain. The\nundergraduate data science literature that we identified often lacks empirical\ndata, research questions and reproducibility. Certain disciplines are less\nvisible. We recommend that we should (1) cherish data science as an\ninterdisciplinary field; (2) adopt a consistent set of keywords/terminology to\nensure data science education literature is easily identifiable; (3) prioritize\ninvestments in empirical studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.03387v2"
    },
    {
        "title": "An analysis of the NCAA college football playoff team selections using\n  an Elo ratings model",
        "authors": [
            "Benjamin Lucas"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  In December 2023 the Florida State Seminoles became the first Power 5 school\nto have an undefeated season and miss selection for the College Football\nPlayoff. In order to assess this decision, we employed an Elo ratings model to\nrank the teams and found that the selection committee's decision was justified\nand that Florida State were not one of the four best teams in college football\nin that season (ranking only 11th!). We extended this analysis to all other\nyears of the CFP and found that the top four teams by Elo ratings differ\ngreatly from the four teams selected in almost every year of the CFP's\nexistence. Furthermore, we found that there have been more egregious\nnon-selections including when Alabama was ranked first by Elo ratings in 2022\nand were not selected. The analysis suggests that the current criteria are too\nsubjective and a ratings model should be implemented to provide transparency\nfor the sport, its teams, and its fans.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.03862v1"
    },
    {
        "title": "Why Name Popularity is a Good Test of Historicity",
        "authors": [
            "Luuk van de Weghe",
            "Jason Wilson"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Are name statistics in the Gospels and Acts a good test of historicity? Kamil\nGregor and Brian Blais, in a recent article in The Journal for the Study of the\nHistorical Jesus, argue that the sample of name occurrences in the Gospels and\nActs is too small to be determinative and that several statistical anomalies\nweigh against a positive verdict. Unfortunately, their conclusions result\ndirectly from improper testing and questionable data selection. Chi-squared\ngoodness-of-fit testing establishes that name occurrences in the Gospels and\nActs fit into their historical context at least as good as those in the works\nof Josephus. Additionally, they fit better than occurrences derived from\nancient fictional sources and occurrences from modern, well-researched\nhistorical novels.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.14883v3"
    },
    {
        "title": "An Investigation into Distance Measures in Cluster Analysis",
        "authors": [
            "Zoe Shapcott"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  This report provides an exploration of different distance measures that can\nbe used with the $K$-means algorithm for cluster analysis. Specifically, we\ninvestigate the Mahalanobis distance, and critically assess any benefits it may\nhave over the more traditional measures of the Euclidean, Manhattan and Maximum\ndistances. We perform this by first defining the metrics, before considering\ntheir advantages and drawbacks as discussed in literature regarding this area.\nWe apply these distances, first to some simulated data and then to subsets of\nthe Dry Bean dataset [1], to explore if there is a better quality detectable\nfor one metric over the others in these cases. One of the sections is devoted\nto analysing the information obtained from ChatGPT in response to prompts\nrelating to this topic.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.13664v1"
    },
    {
        "title": "What's So Hard about the Monty Hall Problem?",
        "authors": [
            "Rafael C. Alvarado"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  The Monty Hall problem is notorious for its deceptive simplicity. Although\ntoday it is widely used as a provocative thought experiment to introduce\nBayesian thinking to students of probability, in the not so distant past it was\nrejected by established mathematicians. This essay provides some historical\nbackground to the problem and explains why it is considered so\ncounter-intuitive to many. It is argued that the main barrier to understanding\nthe problem is the back-grounding of the concept of dependence in probability\ntheory as it is commonly taught. To demonstrate this, a Bayesian solution is\nprovided and augmented with a probabilistic graphical model (PGM) inspired by\nthe work of Pearl (1988, 1998). Although the Bayesian approach produces the\ncorrect answer, without a representation of the dependency structure of events\nimplied by the problem, the salient fact that motivates the problem's solution\nremains hidden.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.00884v2"
    },
    {
        "title": "Expected Points Above Average: A Novel NBA Player Metric Based on\n  Bayesian Hierarchical Modeling",
        "authors": [
            "Benjamin Williams",
            "Erin M. Schliep",
            "Bailey Fosdick",
            "Ryan Elmore"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Team and player evaluation in professional sport is extremely important given\nthe financial implications of success/failure. It is especially critical to\nidentify and retain elite shooters in the National Basketball Association\n(NBA), one of the premier basketball leagues worldwide because the ultimate\ngoal of the game is to score more points than one's opponent. To this end we\npropose two novel basketball metrics: \"expected points\" for team-based\ncomparisons and \"expected points above average (EPAA)\" as a player-evaluation\ntool. Both metrics leverage posterior samples from Bayesian hierarchical\nmodeling framework to cluster teams and players based on their shooting\npropensities and abilities. We illustrate the concepts for the top 100 shot\ntakers over the last decade and offer our metric as an additional metric for\nevaluating players.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.10453v1"
    },
    {
        "title": "The Epistemology behind Covariate Adjustment",
        "authors": [
            "Grayson L. Baird",
            "Stephen L. Bieber"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  It is often asserted that to control for the effects of confounders, one\nshould include the confounding variables of concern in a statistical model as a\ncovariate. Conversely, it is also asserted that control can only be concluded\nby design, where the results from an analysis can only be interpreted as\nevidence of an effect because the design controlled for the cause. To suggest\notherwise is said to be a fallacy of cum hoc ergo propter hoc. Obviously, these\ntwo assertions create a conundrum: How can the effect of confounder be\ncontrolled for with analysis instead of by design without committing cum hoc\nergo propter hoc? The present manuscript answers this conundrum.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17224v1"
    },
    {
        "title": "Guidelines and Best Practices to Share Deidentified Data and Code",
        "authors": [
            "Nicholas J. Horton",
            "Sara Stoudt"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  In 2022, the Journal of Statistics and Data Science Education (JSDSE)\ninstituted augmented requirements for authors to post deidentified data and\ncode underlying their papers. These changes were prompted by an increased focus\non reproducibility and open science (NASEM 2019). A recent review of data\navailability practices noted that \"such policies help increase the\nreproducibility of the published literature, as well as make a larger body of\ndata available for reuse and re-analysis\" (PLOS ONE, 2024). JSDSE values\naccessibility as it endeavors to share knowledge that can improve educational\napproaches to teaching statistics and data science. Because institution,\nenvironment, and students differ across readers of the journal, it is\nespecially important to facilitate the transfer of a journal article's findings\nto new contexts. This process may require digging into more of the details,\nincluding the deidentified data and code. Our goal is to provide our readers\nand authors with a review of why the requirements for code and data sharing\nwere instituted, summarize ongoing trends and developments in open science,\ndiscuss options for data and code sharing, and share advice for authors.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.18232v2"
    },
    {
        "title": "Fractal dimension, and the problems traps of its estimation",
        "authors": [
            "Carlos Sevcik"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  This chapter deals with error and uncertainty in data. Treats their measuring\nmethods and meaning. It shows that uncertainty is a natural property of many\ndata sets. Uncertainty is fundamental for the survival os living species,\nUncertainty of the \"chaos\" type occurs in many systems, is fundamental to\nunderstand these systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.19885v3"
    },
    {
        "title": "The Need for a Recurring Large-Scale Benchmarking Survey to Continually\n  Evaluate Sampling Methods and Administration Modes: Lessons from the 2022\n  Collaborative Midterm Survey",
        "authors": [
            "Peter K. Enns",
            "Colleen L. Barry",
            "James N. Druckman",
            "Sergio Garcia-Rios",
            "David C. Wilson",
            "Jonathon P. Schuldt"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  As survey methods adapt to technological and societal changes, a growing body\nof research seeks to understand the tradeoffs associated with various sampling\nmethods and administration modes. We show how the NSF-funded 2022 Collaborative\nMidterm Survey (CMS) can be used as a dynamic and transparent framework for\nevaluating which sampling approaches - or combination of approaches - are best\nsuited for various research goals. The CMS is ideally suited for this purpose\nbecause it includes almost 20,000 respondents interviewed using two\nadministration modes (phone and online) and data drawn from random digit\ndialing, random address-based sampling, a probability-based panel, two\nnonprobability panels, and two nonprobability marketplaces. The analysis\nconsiders three types of population benchmarks (election data, administrative\nrecords, and large government surveys) and focuses on the national-level\nestimates as well as oversamples in three states (California, Florida, and\nWisconsin). In addition to documenting how each of the survey strategies\nperformed, we develop a strategy to assess how different combinations of\napproaches compare to different population benchmarks in order to guide\nresearchers combining sampling methods and sources. We conclude by providing\nspecific recommendations to public opinion and election survey researchers and\ndemonstrating how our approach could be applied to a large government survey\nconducted at regular intervals to provide ongoing guidance to researchers,\ngovernment, businesses, and nonprofits regarding the most appropriate survey\nsampling and administration methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.06090v1"
    },
    {
        "title": "More than Formulas -- Integrity, Communication, Computing and\n  Reproducibility in Statistics Education",
        "authors": [
            "Eva Furrer",
            "Annina Cincera",
            "Reinhard Furrer"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  This paper introduces a novel course design in the Master Program in\nBiostatistics at the University of Zurich that integrates computing skills,\neffective communication, reproducibility, and scientific integrity within one\ncourse. Utilizing a flipped classroom model, the course aims to equip students\nwith the necessary competencies to handle real-world data analysis challenges\nand effective statistical practice in general. The curriculum includes\npractical tools such as version control with Git, dynamic reporting, unit\ntesting and containerization to foster reproducibility, and integrity in\nstatistical practice. Feedback gathered from both staff and students\npost-implementation indicates that the course significantly enhances student\nreadiness for professional and academic environments, demonstrating the\neffectiveness of this educational approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.08835v1"
    },
    {
        "title": "Toward a Complete Criterion for Value of Information in Insoluble\n  Decision Problems",
        "authors": [
            "Ryan Carey",
            "Sanghack Lee",
            "Robin J. Evans"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  In a decision problem, observations are said to be material if they must be\ntaken into account to perform optimally. Decision problems have an underlying\n(graphical) causal structure, which may sometimes be used to evaluate certain\nobservations as immaterial. For soluble graphs - ones where important past\nobservations are remembered - there is a complete graphical criterion; one that\nrules out materiality whenever this can be done on the basis of the graphical\nstructure alone. In this work, we analyse a proposed criterion for insoluble\ngraphs. In particular, we prove that some of the conditions used to prove\nimmateriality are necessary; when they are not satisfied, materiality is\npossible. We discuss possible avenues and obstacles to proving necessity of the\nremaining conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.09883v1"
    },
    {
        "title": "How Books Tell a History of Statistics in Portugal: Works of Foreigners,\n  Estrangeirados, and Others",
        "authors": [
            "Dinis Pestana",
            "Rui Santos"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Foreigners and \"estrangeirados\", an expression meaning \"people going to a\nforeign country [\"estrangeiro\"] getting there further education\", had a leading\nrole in the development of Mathematical Statistics in Portugal. In what\nconcerns Statistics, \"estrangeirados\" in the nineteenth century were mainly\nliberal intellectuals exiled for political reasons. From 1930 onwards, the\nresearch funding authority sent university professors abroad, and hired foreign\nresearchers to stay in Portuguese institutions, and some of them were\ninstrumental in the importation of new concepts and methods of inferential\nstatistics. After 1970, there was a huge program of sending young researchers\nabroad for doctoral studies. At the same time, many new universities and\npolytechnic institutes have been created in Portugal. After that, aside from\nforeigners who choose to have a research career in those institutions and the\n\"estrangeirados\" who had returned and created programs of doctoral studies,\nothers, who hadn't the opportunity of studying abroad, began to play a decisive\nrole in the development of Statistics in Portugal. The publication of handbooks\non Probability and Statistics, thesis and core papers in Portuguese scientific\njournals, and also of works for the layman, reveals how Statistics progressed\nfrom descriptive to a mathematical discipline used for inference in all fields\nof knowledge, from natural sciences to methodology of scientific research.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19433v2"
    },
    {
        "title": "Computational methods to simultaneously compare the predictive values of\n  two diagnostic tests with missing data: EM-SEM algorithms and multiple\n  imputation",
        "authors": [
            "Jose Antonio Roldan-Nofuentes"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Predictive values are measures of the clinical accuracy of a binary\ndiagnostic test, and depend on the sensitivity and the specificity of the test\nand on the disease prevalence among the population being studied. This article\nstudies hypothesis tests to simultaneously compare the predictive values of two\nbinary diagnostic tests in the presence of missing data. The hypothesis tests\nwere solved applying two computational methods: the EM and SEM algorithms and\nmultiple imputation. Simulation experiments were carried out to study the sizes\nand the power of the hypothesis tests, giving some general rules of\napplication. Two R programmes were written to apply each method, and they are\navailable as supplementary material for the manuscript. The results were\napplied to the diagnosis of Alzheimer's disease.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.21190v2"
    },
    {
        "title": "Comparison of the likelihood ratios of two diagnostic tests subject to a\n  paired design: confidence intervals and sample size",
        "authors": [
            "Jose Antonio Roldan-Nofuentes",
            "Saad Bouh Sidaty-Regad"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Positive and negative likelihood ratios are parameters which are used to\nassess and compare the effectiveness of binary diagnostic tests. Both\nparameters only depend on the sensitivity and specificity of the diagnostic\ntest and are equivalent to a relative risk. This article studies the comparison\nof the likelihood ratios of two binary diagnostic tests subject to a paired\ndesign through confidence intervals. Six approximate confidence intervals are\npresented for the ratio of the likelihood ratios, and simulation experiments\nare carried out to study the coverage probabilities and the average lengths of\nthe intervals considered, and some general rules of application are proposed. A\nmethod is also proposed to determine the sample size necessary to estimate the\nratio between the likelihood ratios with a determined precision. The results\nwere applied to the diagnosis of coronary artery disease.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.21382v1"
    },
    {
        "title": "Asymptotic confidence intervals for the difference and the ratio of the\n  weighted kappa coefficients of two diagnostic tests subject to a paired\n  design",
        "authors": [
            "Jose Antonio Roldan-Nofuentes",
            "Saad bouh Sidaty-regad"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  The weighted kappa coefficient of a binary diagnostic test is a measure of\nthe beyond-chance agreement between the diagnostic test and the gold standard,\nand depends on the sensitivity and specificity of the diagnostic test, on the\ndisease prevalence and on the relative importance between the false positives\nand the false negatives. This article studies the comparison of the weighted\nkappa coefficients of two binary diagnostic tests subject to a paired design\nthrough confidence intervals. Three asymptotic confidence intervals are studied\nfor the difference between the parameters and five other intervals for the\nratio. Simulation experiments were carried out to study the coverage\nprobabilities and the average lengths of the intervals, giving some general\nrules for application. A method is also proposed to calculate the sample size\nnecessary to compare the two weighted kappa coefficients through a confidence\ninterval. A program in R has been written to solve the problem studied and it\nis available as supplementary material. The results were applied to a real\nexample of the diagnosis of malaria.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.21387v1"
    },
    {
        "title": "How to survive the Squid Games using probability theory",
        "authors": [
            "Elena Moltchanova",
            "Miguel Moyers-González",
            "Geertrui Van de Voorde",
            "José Felipe Voloch",
            "Philipp Wacker"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  In this paper, we consider how probability theory can be used to determine\nthe survival strategy in two of the ``Squid Game\" and ``Squid Game: The\nChallenge\" challenges: the Hopscotch and the Warships. We show how Hopscotch\ncan be easily tackled with the knowledge of the binomial distribution, taught\nin introductory statistics courses, while Warships is a much more complex\nproblem, which can be tackled at different levels.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.05263v1"
    },
    {
        "title": "Oral exams in introductory statistics class with non-native English\n  speakers",
        "authors": [
            "Eric Yanchenko"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Oral exams are a powerful tool for educators to gauge student's learning.\nThis is particularly important in introductory statistics classes where many\nstudents struggle to grasp a deep meaning of topics like $p$-values, confidence\nintervals, hypothesis testing, and more. These challenges are only heightened\nin a context where students are learning in a second language. In this paper, I\nshare my experience administering oral exams to an introductory statistics\nclass of non-native English speakers at a Japanese university. I explain the\ncontext of the university and course that the exam was given in, before sharing\ndetails about the two exams. Despite the challenges the students (and I myself)\nfaced, the exams seemed to truly test their statistical knowledge and not\nmerely their English proficiency, as I found little relationship between a\nstudent's English ability and performance. I close with encouragements and\nrecommendations for practitioners hoping to implement these exams, all while\nkeeping an eye towards the unique difficulties faced by students not learning\nin their mother tongue.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.16613v2"
    },
    {
        "title": "A comparative study of self-starting CUSUM control charts for location\n  shifts",
        "authors": [
            "Konstantinos Bourazas"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  In recent years, self-starting methods have garnered increasing attention in\nStatistical Process Control and Monitoring (SPC/M), as they offer real-time\ndisorder detection without the need for a calibration phase (Phase I). This\nstudy focuses on evaluating parametric self-starting CUSUM-type control charts,\nspecifically the Bayesian Predictive Ratio CUSUM (PRC) developed by Bourazas et\nal. (2023) and the frequentist alternative self-starting CUSUM proposed by\nHawkins and Olwell (1998). The performance of these methods is thoroughly\nexamined through an extensive simulation study under various scenarios\ninvolving a change in the mean of Normal data. Additionally, a prior\nsensitivity analysis for PRC is conducted. The work ands with concluding\nremarks summarizing the findings.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.12736v1"
    },
    {
        "title": "Developing Consistency Among Undergraduate Graders Scoring Open-Ended\n  Statistics Tasks",
        "authors": [
            "Matthew D. Beckman",
            "Sean Burke",
            "Jack Fiochetta",
            "Benjamin Fry",
            "Susan E. Lloyd",
            "Luke Patterson",
            "Elle Tang"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Undergraduate graders are frequently important contributors to the teaching\nteam in post-secondary education settings. This study set out to investigate\nagreement for a team of undergraduate graders as they acquired training and\nexperience for scoring responses to open-ended tasks. Results demonstrate\ncompelling evidence that undergraduate students can develop the ability to\nestablish and sustain substantial agreement with an instructor, especially when\nequipped with proper training and a high-quality scoring rubric.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.18062v1"
    },
    {
        "title": "Frequentist Statistics as Internalist Reliabilism",
        "authors": [
            "Hanti Lin"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  There has long been an impression that reliabilism implies externalism and\nthat frequentist statistics, due to its reliabilist nature, is inherently\nexternalist. I argue, however, that frequentist statistics can plausibly be\nunderstood as a form of internalist reliabilism -- internalist in the\nconventional sense, yet reliabilist in certain unconventional and intriguing\nways. Crucially, in developing the thesis that reliabilism does not imply\nexternalism, my aim is not to stretch the meaning of `reliabilism' merely to\nsever the implication. Instead, it is to gain a deeper understanding of\nfrequentist statistics, which stands as one of the most sustained attempts by\nscientists to develop an epistemology for their own use.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.08547v3"
    },
    {
        "title": "Towards Socially Responsible Forecasting: Identifying and Typifying\n  Forecasting Harms",
        "authors": [
            "Bahman Rostami-Tabar",
            "Travis Greene",
            "Galit Shmueli",
            "Rob J. Hyndman"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Data-driven organizations around the world routinely use forecasting methods\nto improve their planning and decision-making capabilities. Although much\nresearch exists on the harms resulting from traditional machine learning\napplications, little has specifically focused on the ethical impact of time\nseries forecasting. Yet forecasting raises unique ethical issues due to the way\nit is used in different organizational contexts, supports different goals, and\ninvolves different data processing, model development, and evaluation\npipelines. These differences make it difficult to apply machine learning harm\ntaxonomies to common forecasting contexts. We leverage multiple interviews with\nexpert industry practitioners and academic researchers to remedy this knowledge\ngap by cataloguing and analysing under-explored domains, applications, and\nscenarios where forecasting may cause harm, with the goal of developing a novel\ntaxonomy of forecasting-specific harms. Inspired by the Microsoft Azure\ntaxonomy for responsible innovation, we combined a human-led inductive coding\nscheme with an AI-driven analysis centered on the extraction of key taxonomies\nof harm in forecasting. The taxonomy is designed to guide researchers and\npractitioners and encourage ethical reflection on the impact of their decisions\nduring the forecasting process. A secondary objective is to create a research\nagenda focused on possible forecasting-related measures to mitigate harm. Our\nwork extends the growing literature on machine learning harms by identifying\nunique forms of harm that may occur in forecasting.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.16531v2"
    },
    {
        "title": "Internalist Reliabilism in Statistics and Machine Learning: Thoughts on\n  Jun Otsuka's Thinking about Statistics",
        "authors": [
            "Hanti Lin"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Otsuka (2023) argues for a correspondence between data science and\ntraditional epistemology: Bayesian statistics is internalist; classical\n(frequentist) statistics is externalist, owing to its reliabilist nature; model\nselection is pragmatist; and machine learning is a version of virtue\nepistemology. Where he sees diversity, I see an opportunity for unity. In this\narticle, I argue that classical statistics, model selection, and machine\nlearning share a foundation that is reliabilist in an unconventional sense that\naligns with internalism. Hence a unification under internalist reliabilism.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.02367v1"
    },
    {
        "title": "My Statistics is Better than Yours",
        "authors": [
            "Simon Benhaïem"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Statistical schools-such as Bayesianism and Frequentism-are often presented\nas competing frameworks, each claiming technical rigour and superiority.\nFrequentism emphasizes objective inferences through repeated sampling, while\nBayesianism incorporates prior beliefs and updates them with new evidence.\nDespite their strengths, neither school proves universally applicable, and the\npursuit of a single \"correct\" statistical framework is ultimately misguided.\nInstead, this essay advocates for a context-dependent approach to statistical\nnorms, drawing on Douglas (2004)'s concept of \"operational objectivity\". The\nidea is that by aligning the context of the research question with the value\njudgments inherent to its field, a certain statistical paradigm is warranted.\nThe essay delves into the decision-theoretic foundations of Bayesianism,\nevaluates its descriptive limitations as highlighted by the Ellsberg paradox,\nand examines the difficulty of comparing normative systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.10296v2"
    },
    {
        "title": "Russian roulette: Why you can have a deterministic potential-outcome\n  framework, or an asymmetric utility function, but not both",
        "authors": [
            "Andrew Gelman",
            "Jonas Mikhaeil"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  It has been proposed in medical decision analysis to express the ``first do\nno harm'' principle as an asymmetric utility function in which the loss from\nkilling a patient would count more than the gain from saving a life. Such a\nutility depends on unrealized potential outcomes, and we show how this yields a\nparadoxical decision recommendation in a simple hypothetical example involving\ngames of Russian roulette. The problem is resolved if we allow the potential\noutcomes to be random variables. This leads us to conclude that, if you are\ninterested in this sort of asymmetric utility function, you need to move to the\nstochastic potential outcome framework. We discuss the implications of the\nchoice of parameterization in this setting.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.12233v1"
    },
    {
        "title": "An Undergraduate Course on the Statistical Principles of Research Study\n  Design",
        "authors": [
            "Lee Kennedy-Shaffer"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  The undergraduate curriculum in statistics and data science is undergoing\nchanges to accommodate new methods, newly interested students, and the changing\nrole of statistics in society. Because of this, it is more important than ever\nthat students understand the role of study design and how to formulate\nmeaningful scientific and statistical research questions. While the traditional\nDesign of Experiments course is still extremely valuable for students heading\nto industry and research careers, a broader study design course that\nincorporates survey sampling, observational studies, and the basics of causal\ninference with randomized experiment design is particularly useful for students\nwith a wide range of applied interests. Here, I describe such a course at a\nsmall liberal arts college, along with ways to adapt it to meet different\nstudent and instructor background and interests. The course serves as a\nvaluable bridge to advanced statistical coursework, meets key statistical\nliteracy and communication learning goals, and can be tailored to the desired\nlevel of computational and mathematical fluency. Through reading, discussing,\nand critiquing actual published research studies, students learn that\nstatistics is a living discipline with real consequences and become better\nconsumers and producers of scientific research and data-driven insights.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.20175v1"
    },
    {
        "title": "A Mathematical Lens for Teaching Data Science",
        "authors": [
            "Johanna Hardin"
        ],
        "category": "stat.OT",
        "published_year": "2025",
        "summary": "  Using the National Academies report, {\\em Data Science for Undergraduates:\nOpportunities and Options}, we connect data science curricula to the more\nfamiliar pedagogy used by many mathematical scientists. We use their list of\n``data acumen\" components to ground a discussion, which hopes to connect data\nscience curricula to the more familiar pedagogy used by many mathematical\nscientists.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.02126v1"
    },
    {
        "title": "A Bureaucratic Theory of Statistics",
        "authors": [
            "Benjamin Recht"
        ],
        "category": "stat.OT",
        "published_year": "2025",
        "summary": "  This commentary proposes a framework for understanding the role of statistics\nin policy-making, regulation, and bureaucratic systems. I introduce the concept\nof \"ex ante policy,\" describing statistical rules and procedures designed\nbefore data collection to govern future actions. Through examining examples,\nparticularly clinical trials, I explore how ex ante policy serves as a calculus\nof bureaucracy, providing numerical foundations for governance through clear,\ntransparent rules. The ex ante frame obviates heated debates about inferential\ninterpretations of probability and statistical tests, p-values, and rituals. I\nconclude by calling for a deeper appreciation of statistics' bureaucratic\nfunction and suggesting new directions for research in policy-oriented\nstatistical methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.03457v1"
    },
    {
        "title": "Boundary detection in disease mapping studies",
        "authors": [
            "Duncan Lee",
            "Richard Mitchell"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  In disease mapping, the aim is to estimate the spatial pattern in disease\nrisk over an extended geographical region, so that areas with elevated risks\ncan be identified. A Bayesian hierarchical approach is typically used to\nproduce such maps, which models the risk surface with a set of spatially smooth\nrandom effects. However, in complex urban settings there are likely to be\nboundaries in the risk surface, which separate populations that are\ngeographically adjacent but have very different risk profiles. Therefore this\npaper proposes an approach for detecting such risk boundaries, and tests its\neffectiveness by simulation. Finally, the model is applied to lung cancer\nincidence data in Greater Glasgow, Scotland, between 2001 and 2005.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.1879v1"
    },
    {
        "title": "Reading Theorie Analytique des Probabilites",
        "authors": [
            "Christian P. Robert"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  This note is an extended read of my read of Laplace's book Theorie Analytique\ndes Probabilites, when considered from a Bayesian viewpoint but without\nhistorical nor comparative pretentions. A deeper analysis is provided in Dale\n(1999).\n",
        "pdf_link": "http://arxiv.org/pdf/1203.6249v1"
    },
    {
        "title": "Vine Constructions of Levy Copulas",
        "authors": [
            "Oliver Grothe",
            "Stephan Nicklas"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  Levy copulas are the most general concept to capture jump dependence in\nmultivariate Levy processes. They translate the intuition and many features of\nthe copula concept into a time series setting. A challenge faced by both,\ndistributional and Levy copulas, is to find flexible but still applicable\nmodels for higher dimensions. To overcome this problem, the concept of pair\ncopula constructions has been successfully applied to distributional copulas.\nIn this paper, we develop the pair construction for Levy copulas (PLCC).\nSimilar to pair constructions of distributional copulas, the pair construction\nof a d-dimensional Levy copula consists of d(d-1)/2 bivariate dependence\nfunctions. We show that only d-1 of these bivariate functions are Levy copulas,\nwhereas the remaining functions are distributional copulas. Since there are no\nrestrictions concerning the choice of the copulas, the proposed pair\nconstruction adds the desired flexibility to Levy copula models. We discuss\nestimation and simulation in detail and apply the pair construction in a\nsimulation study.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.4309v2"
    },
    {
        "title": "Biologists meet statisticians: A workshop for young scientists to foster\n  interdisciplinary team work",
        "authors": [
            "Benjamin Hofner",
            "Lea Vaas",
            "John-Philip Lawo",
            "Tina Müller",
            "Johannes Sikorski",
            "Dirk Repsilber"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  Life science and statistics have necessarily become essential partners. The\nneed to plan complex, structured experiments, involving elaborated designs, and\nthe need to analyse datasets in the era of systems biology and high throughput\ntechnologies has to build upon professional statistical expertise. On the other\nhand, conducting such analyses and also developing improved or new methods,\nalso for novel kinds of data, has to build upon solid biological understanding\nand practise. However, the meeting of scientists of both fields is often\nhampered by a variety of communicative hurdles - which are based on\nfield-specific working languages and cultural differences.\n  As a step towards a better mutual understanding, we developed a workshop\nconcept bringing together young experimental biologists and statisticians, to\nwork as pairs and learn to value each others competences and practise\ninterdisciplinary communication in a casual atmosphere. The first\nimplementation of our concept was a cooperation of the German Region of the\nInternational Biometrical Society and the Leibnitz Institute DSMZ-German\nCollection of Microorganisms and Cell Cultures (short: DSMZ), Braunschweig,\nGermany. We collected feedback in form of three questionnaires, oral comments,\nand gathered experiences for the improvement of this concept. The long-term\nchallenge for both disciplines is the establishment of systematic schedules and\nstrategic partnerships which use the proposed workshop concept to foster mutual\nunderstanding, to seed the necessary interdisciplinary cooperation network, and\nto start training the indispensable communication skills at the earliest\npossible phase of education.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.5597v1"
    },
    {
        "title": "Breaking Monotony with Meaning: Motivation in Crowdsourcing Markets",
        "authors": [
            "Dana Chandler",
            "Adam Kapelner"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  We conduct the first natural field experiment to explore the relationship\nbetween the \"meaningfulness\" of a task and worker effort. We employed about\n2,500 workers from Amazon's Mechanical Turk (MTurk), an online labor market, to\nlabel medical images. Although given an identical task, we experimentally\nmanipulated how the task was framed. Subjects in the meaningful treatment were\ntold that they were labeling tumor cells in order to assist medical\nresearchers, subjects in the zero-context condition (the control group) were\nnot told the purpose of the task, and, in stark contrast, subjects in the\nshredded treatment were not given context and were additionally told that their\nwork would be discarded. We found that when a task was framed more\nmeaningfully, workers were more likely to participate. We also found that the\nmeaningful treatment increased the quantity of output (with an insignificant\nchange in quality) while the shredded treatment decreased the quality of output\n(with no change in quantity). We believe these results will generalize to other\nshort-term labor markets. Our study also discusses MTurk as an exciting\nplatform for running natural field experiments in economics.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.0962v1"
    },
    {
        "title": "The anti-Bayesian moment and its passing",
        "authors": [
            "Andrew Gelman",
            "Christian P. Robert"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  The present article is the reply to the discussion of our earlier \"Not only\ndefended but also applied\" (arXiv:1006.5366, to appear in The American\nStatistician) that arose from our memory of a particularly intemperate\nanti-Bayesian statement in Feller's beautiful and classic book on probability\ntheory. We felt that it was worth exploring the very extremeness of Feller's\nwords, along with similar anti-Bayesian remarks by others, in order to better\nunderstand the background underlying controversies that still exist regarding\nthe foundations of statistics. We thank the four discussants of our article for\ntheir contributions to our understanding of these controversies as they have\nexisted in the past and persist today.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.7225v1"
    },
    {
        "title": "From Ordinary Differential Equations to Structural Causal Models: the\n  deterministic case",
        "authors": [
            "Joris M. Mooij",
            "Dominik Janzing",
            "Bernhard Schölkopf"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  We show how, and under which conditions, the equilibrium states of a\nfirst-order Ordinary Differential Equation (ODE) system can be described with a\ndeterministic Structural Causal Model (SCM). Our exposition sheds more light on\nthe concept of causality as expressed within the framework of Structural Causal\nModels, especially for cyclic models.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7920v1"
    },
    {
        "title": "On moment indeterminacy of the Benini income distribution",
        "authors": [
            "Christian Kleiber"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  The Benini distribution is a lognormal-like distribution generalizing the\nPareto distribution. Like the Pareto and the lognormal distributions it was\noriginally proposed for modeling economic size distributions, notably the size\ndistribution of personal income. This paper explores a probabilistic property\nof the Benini distribution, showing that it is not determined by the sequence\nof its moments although all the moments are finite. It also provides explicit\nexamples of distributions possessing the same set of moments. Related\ndistributions are briefly explored.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.0364v1"
    },
    {
        "title": "Meeting Student Needs for Multivariate Data Analysis: A Case Study in\n  Teaching a Multivariate Data Analysis Course with No Pre-requisites",
        "authors": [
            "Amy S. Wagaman"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  Modern students encounter big, messy data sets long before setting foot in\nour classrooms. Many of our students need to develop skills in exploratory data\nanalysis and multivariate analysis techniques for their jobs after college, but\nthese topics are not covered in introductory statistics courses. This case\nstudy describes my experience in designing and teaching a course on\nmultivariate data analysis with no pre-requisites, using real data, active\nlearning, and other activities to help students tackle the material.\n",
        "pdf_link": "http://arxiv.org/pdf/1310.7141v1"
    },
    {
        "title": "Statistical inference for exponential functionals of Lévy processes",
        "authors": [
            "Denis Belomestny",
            "Vladimir Panov"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  In this paper, we consider the exponential functional\n\\(A_{\\infty}=\\int_0^\\infty e^{-\\xi_s}ds\\) of a L{\\'e}vy process \\(\\xi_s\\) and\naim to estimate the characteristics of \\(\\xi_{s}\\) from the distribution of\n\\(A_{\\infty}\\). We present a new approach, which allows to statistically infer\non the L{\\'e}vy triplet of \\(\\xi_{t}\\), and study the theoretical properties of\nthe proposed estimators. The suggested algorithms are illustrated with\nnumerical simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.4731v1"
    },
    {
        "title": "A mobile web for enhancing statistics and mathematics education",
        "authors": [
            "Jamie Lentin",
            "Anna H. Jonsdottir",
            "David Stern",
            "Victoria Mokua",
            "Gunnar Stefansson"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  A freely available educational application (a mobile website) is presented.\nThis provides access to educational material and drilling on selected topics\nwithin mathematics and statistics with an emphasis on tablets and mobile\nphones. The application adapts to the student's performance, selecting from\neasy to difficult questions, or older material etc. These adaptations are based\non statistical models and analyses of data from testing precursors of the\nsystem within several courses, from calculus and introductory statistics\nthrough multiple linear regression. The application can be used in both on-line\nand off-line modes. The behavior of the application is determined by\nparameters, the effects of which can be estimated statistically. Results\npresented include analyses of how the internal algorithms relate to passing a\ncourse and general incremental improvement in knowledge during a semester.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.5004v2"
    },
    {
        "title": "The two envelopes paradox in non-Bayesian and Bayesian statistics",
        "authors": [
            "Shiro Ishikawa"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  The purpose of this paper is to clarify the (non-Bayesian and Bayesian)\ntwo-envelope problems in terms of quantum language (or, measurement theory),\nwhich was recently proposed as a linguistic turn of quantum mechanics (with the\nCopenhagen interpretation). The two envelopes paradox is only a kind of high\nschool student's probability puzzle, and it may be exaggerated to say that this\nis an unsolved problem. However, since we are convinced that quantum language\nis just statistics of the future, we believe that there is no clear answer\nwithout the description by quantum language. In this sense, the readers are to\nfind that quantum language provides the final answer (i.e., the easiest and\ndeepest understanding) to the two envelope-problems in both non-Bayesian and\nBayesian statistics. Also, we add the discussion about St. Petersburg\ntwo-envelope paradox.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.4916v4"
    },
    {
        "title": "Nonparametric estimation of the division rate of an age dependent\n  branching process",
        "authors": [
            "Marc Hoffmann",
            "Adélaïde Olivier"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  We study the nonparametric estimation of the branching rate $B(x)$ of a\nsupercritical Bellman-Harris population: a particle with age $x$ has a random\nlifetime governed by $B(x)$; at its death time, it gives rise to $k \\geq 2$\nchildren with lifetimes governed by the same division rate and so on. We\nobserve in continuous time the process over $[0,T]$. Asymptotics are taken as\n$T \\rightarrow \\infty$; the data are stochastically dependent and one has to\nface simultaneously censoring, bias selection and non-ancillarity of the number\nof observations. In this setting, under appropriate ergodicity properties, we\nconstruct a kernel-based estimator of $B(x)$ that achieves the rate of\nconvergence $\\exp(-\\lambda_B \\frac{\\beta}{2\\beta+1}T)$, where $\\lambda_B$ is\nthe Malthus parameter and $\\beta >0$ is the smoothness of the function $B(x)$\nin a vicinity of $x$. We prove that this rate is optimal in a minimax sense and\nwe relate it explicitly to classical nonparametric models such as density\nestimation observed on an appropriate (parameter dependent) scale. We also shed\nsome light on the fact that estimation with kernel estimators based on data\nalive at time $T$ only is not sufficient to obtain optimal rates of\nconvergence, a phenomenon which is specific to nonparametric estimation and\nthat has been observed in other related growth-fragmentation models.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.5936v2"
    },
    {
        "title": "Embracing Data Science",
        "authors": [
            "Adam Loy"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Statistics is running the risk of appearing irrelevant to today's\nundergraduate students. Today's undergraduate students are familiar with data\nscience projects and they judge statistics against what they have seen.\nStatistics, especially at the introductory level, should take inspiration from\ndata science so that the discipline is not seen as somehow lesser than data\nscience. This article provides a brief overview of data science, outlines ideas\nfor how introductory courses could take inspiration from data science, and\nprovides a reference to materials for developing stand-alone data science\ncourses.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.00858v1"
    },
    {
        "title": "Generalised Reichenbachian Common Cause Systems",
        "authors": [
            "Claudio Mazzola"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  The principle of the common cause claims that if an improbable coincidence\nhas occurred, there must exist a common cause. This is generally taken to mean\nthat positive correlations between non-causally related events should disappear\nwhen conditioning on the action of some underlying common cause. The extended\ninterpretation of the principle, by contrast, urges that common causes should\nbe called for in order to explain positive deviations between the estimated\ncorrelation of two events and the expected value of their correlation. The aim\nof this paper is to provide the extended reading of the principle with a\ngeneral probabilistic model, capturing the simultaneous action of a system of\nmultiple common causes. To this end, two distinct models are elaborated, and\nthe necessary and sufficient conditions for their existence are determined.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.06109v1"
    },
    {
        "title": "Data-Mining Research in Education",
        "authors": [
            "Jiechao Cheng"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  As an interdisciplinary discipline, data mining (DM) is popular in education\narea especially when examining students' learning performances. It focuses on\nanalyzing educational related data to develop models for improving learners'\nlearning experiences and enhancing institutional effectiveness. Therefore, DM\ndoes help education institutions provide high-quality education for its\nlearners. Applying data mining in education also known as educational data\nmining (EDM), which enables to better understand how students learn and\nidentify how improve educational outcomes. Present paper is designed to justify\nthe capabilities of data mining approaches in the filed of education. The\nlatest trends on EDM research are introduced in this review. Several specific\nalgorithms, methods, applications and gaps in the current literature and future\ninsights are discussed here.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.10117v2"
    },
    {
        "title": "Explanation and exact formula of Zipfs law evaluated from rank-share\n  combinatorics",
        "authors": [
            "A Shyklo"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  This work proves that ranks and shares are statistically dependent on one\nanother, based on simple combinatorics. It presents a formula for rank-share\ndistribution and illustrates that Zipfs law, is descended from expected values\nof various ranks in the new distribution. All conclusions, formulas and charts\npresented here were tested against publicly available statistical data in\ndifferent areas. The correlation coefficient between the calculated values and\nstatistical numbers provided by Bureau of Labor Statistics was 0.99899.\nMonte-Carlo simulations were performed as additional evidence.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07890v1"
    },
    {
        "title": "Econométrie et Machine Learning",
        "authors": [
            "Arthur Charpentier",
            "Emmanuel Flachaire",
            "Antoine Ly"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Econometrics and machine learning seem to have one common goal: to construct\na predictive model, for a variable of interest, using explanatory variables (or\nfeatures). However, these two fields developed in parallel, thus creating two\ndifferent cultures, to paraphrase Breiman (2001). The first was to build\nprobabilistic models to describe economic phenomena. The second uses algorithms\nthat will learn from their mistakes, with the aim, most often to classify\n(sounds, images, etc.). Recently, however, learning models have proven to be\nmore effective than traditional econometric techniques (with a price to pay\nless explanatory power), and above all, they manage to manage much larger data.\nIn this context, it becomes necessary for econometricians to understand what\nthese two cultures are, what opposes them and especially what brings them\ncloser together, in order to appropriate tools developed by the statistical\nlearning community to integrate them into Econometric models.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.06992v2"
    },
    {
        "title": "Aggregating incoherent agents who disagree",
        "authors": [
            "Richard Pettigrew"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  In this paper, we explore how we should aggregate the degrees of belief of of\na group of agents to give a single coherent set of degrees of belief, when at\nleast some of those agents might be probabilistically incoherent. There are a\nnumber of way of aggregating degrees of belief, and there are a number of ways\nof fixing incoherent degrees of belief. When we have picked one of each, should\nwe aggregate first and then fix, or fix first and then aggregate? Or should we\ntry to do both at once? And when do these different procedures agree with one\nanother? In this paper, we focus particularly on the final question.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.03981v1"
    },
    {
        "title": "Discussions on non-probabilistic convex modelling for uncertain problems",
        "authors": [
            "Ni Bingyu",
            "Jiang Chao",
            "Huang Zhiliang"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Non-probabilistic convex model utilizes a convex set to quantify the\nuncertainty domain of uncertain-but-bounded parameters, which is very effective\nfor structural uncertainty analysis with limited or poor-quality experimental\ndata. To overcome the complexity and diversity of the formulations of current\nconvex models, in this paper, a unified framework for construction of the\nnon-probabilistic convex models is proposed. By introducing the correlation\nanalysis technique, the mathematical expression of a convex model can be\nconveniently formulated once the correlation matrix of the uncertain parameters\nis created. More importantly, from the theoretic analysis level, an evaluation\ncriterion for convex modelling methods is proposed, which can be regarded as a\ntest standard for validity verification of subsequent newly proposed convex\nmodelling methods. And from the practical application level, two model\nassessment indexes are proposed, by which the adaptabilities of different\nconvex models to a specific uncertain problem with given experimental samples\ncan be estimated. Four numerical examples are investigated to demonstrate the\neffectiveness of the present study.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.05755v1"
    },
    {
        "title": "Limits on Inferring the Past",
        "authors": [
            "Nathaniel Rupprecht",
            "Dervis Vural"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Here we define and study the properties of retrodictive inference. We derive\nequations relating retrodiction entropy and thermodynamic entropy, and as a\nspecial case, show that under equilibrium conditions, the two are identical. We\ndemonstrate relations involving the KL-divergence and retrodiction probability,\nand bound the time rate of change of retrodiction entropy. As a specific case,\nwe invert various Langevin processes, inferring the initial condition of \\(N\\)\nparticles given their final positions at some later time. We evaluate the\nretrodiction entropy for Langevin dynamics exactly for special cases, and find\nthat one's ability to infer the initial state of a system can exhibit two\npossible qualitative behaviors depending on the potential energy landscape,\neither decreasing indefinitely, or asymptotically approaching a fixed value. We\nalso study how well we can retrodict points that evolve based on the logistic\nmap. We find singular changes in the retrodictivity near bifurcations.\nCounterintuitively, the transition to chaos is accompanied by maximal\nretrodictability.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.10420v2"
    },
    {
        "title": "A Mathematical Approach to Comply with Ethical Constraints in\n  Compassionate Use Treatments",
        "authors": [
            "F. Thomas Bruss"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Patients who are seriously ill may ask doctors to treat them with unapproved\nmedication, about which not much is known, or else with known medication in a\nhigh dosage. Apart from strict legal constraints such cases may involve\ndifficult ethical questions as e.g. how long a series of treatments of\ndifferent patients should be continued. Similar questions also arise in less\nserious situations. A physician trusts that a certain combination of freely\navailable drugs are efficient against a specific disease and tries to help\npatients and to follow at the same time the primum-non-nocere principle.\n  The objective of this paper is to contribute to the research on such\nquestions in the form of mathematical models. Arguing in a step-to-step\napproach, we will show that certain sequential optimisation problems comply in\na natural way with the true spirit of major ethical principles in medicine. We\nthen suggest protocols and associate algorithms to find optimal, or\napproximately optimal, treatment strategies. Although the contribution may\nsometimes be difficult to apply in medical practice, the author thinks that the\nrational behind the approach offers a valuable alternative for finding decision\nsupport and should attract attention.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.04628v1"
    },
    {
        "title": "The rationality of irrationality in the Monty Hall problem",
        "authors": [
            "Torsten Enßlin",
            "Margret Westerkamp"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  The rational solution of the Monty Hall problem unsettles many people. Most\npeople, including the authors, think it feels wrong to switch the initial\nchoice of one of the three doors, despite having fully accepted the\nmathematical proof for its superiority. Many people, if given the choice to\nswitch, think the chances are fifty-fifty between their options, but still\nstrongly prefer to stay with their initial choice. Is there some sense behind\nthese irrational feelings?\n  We entertain the possibility that intuition solves the problem of how to\nbehave in a real game show, not in the abstract textbook version of the Monty\nHall problem. A real showmaster sometimes plays evil, either to make the show\nmore interesting, to save money, or because he is in a bad mood. A moody\nshowmaster erases any information advantage the guest could extract by him\nopening other doors which drives the chance of the car being behind the chosen\ndoor towards fifty percent. Furthermore, the showmaster could try to read or\nmanipulate the guest's strategy to the guest's disadvantage. Given this, the\npreference to stay with the initial choice turns out to be a very rational\ndefense strategy of the show's guest against the threat of being manipulated by\nits host. Thus, the intuitive feelings most people have about the Monty Hall\nproblem coincide with what would be a rational strategy for a real-world game\nshow. Although these investigations are mainly intended to be an entertaining\nmathematical commentary on an information-theoretic puzzle, they touch on\ninteresting psychological questions.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.04948v4"
    },
    {
        "title": "Benford or not Benford: new results on digits beyond the first",
        "authors": [
            "Stéphane Blondeau da Silva"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  In this paper, we will see that the proportion of d as p th digit, where p >\n1 and d $\\in$ 0, 9, in data (obtained thanks to the hereunder developed model)\nis more likely to follow a law whose probability distribution is determined by\na specific upper bound, rather than the generalization of Benford's Law to\ndigits beyond the first one. These probability distributions fluctuate around\ntheoretical values determined by Hill in 1995. Knowing beforehand the value of\nthe upper bound can be a way to find a better adjusted law than Hill's one.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.01291v1"
    },
    {
        "title": "Can everyday AI be ethical. Fairness of Machine Learning Algorithms",
        "authors": [
            "Philippe Besse",
            "Celine Castets-Renard",
            "Aurelien Garivier",
            "Jean-Michel Loubes"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  Combining big data and machine learning algorithms, the power of automatic\ndecision tools induces as much hope as fear. Many recently enacted European\nlegislation (GDPR) and French laws attempt to regulate the use of these tools.\nLeaving aside the well-identified problems of data confidentiality and\nimpediments to competition, we focus on the risks of discrimination, the\nproblems of transparency and the quality of algorithmic decisions. The detailed\nperspective of the legal texts, faced with the complexity and opacity of the\nlearning algorithms, reveals the need for important technological disruptions\nfor the detection or reduction of the discrimination risk, and for addressing\nthe right to obtain an explanation of the auto- matic decision. Since trust of\nthe developers and above all of the users (citizens, litigants, customers) is\nessential, algorithms exploiting personal data must be deployed in a strict\nethical framework. In conclusion, to answer this need, we list some ways of\ncontrols to be developed: institutional control, ethical charter, external\naudit attached to the issue of a label.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.01729v1"
    },
    {
        "title": "Bounds on Bayes Factors for Binomial A/B Testing",
        "authors": [
            "Maciej Skorski"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Bayes factors, in many cases, have been proven to bridge the classic -value\nbased significance testing and bayesian analysis of posterior odds. This paper\ndiscusses this phenomena within the binomial A/B testing setup (applicable for\nexample to conversion testing). It is shown that the bayes factor is controlled\nby the \\emph{Jensen-Shannon divergence} of success ratios in two tested groups,\nwhich can be further bounded by the Welch statistic. As a result, bayesian\nsample bounds almost match frequentionist's sample bounds. The link between\nJensen-Shannon divergence and Welch's test as well as the derivation are an\nelegant application of tools from information geometry.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.00049v1"
    },
    {
        "title": "A fresh look at introductory data science",
        "authors": [
            "Mine Çetinkaya-Rundel",
            "Victoria Ellison"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  The proliferation of vast quantities of available datasets that are large and\ncomplex in nature has challenged universities to keep up with the demand for\ngraduates trained in both the statistical and the computational set of skills\nrequired to effectively plan, acquire, manage, analyze, and communicate the\nfindings of such data. To keep up with this demand, attracting students early\non to data science as well as providing them a solid foray into the field\nbecomes increasingly important. We present a case study of an introductory\nundergraduate course in data science that is designed to address these needs.\nOffered at Duke University, this course has no pre-requisites and serves a wide\naudience of aspiring statistics and data science majors as well as humanities,\nsocial sciences, and natural sciences students. We discuss the unique set of\nchallenges posed by offering such a course and in light of these challenges, we\npresent a detailed discussion into the pedagogical design elements, content,\nstructure, computational infrastructure, and the assessment methodology of the\ncourse. We also offer a repository containing all teaching materials that are\nopen-source, along with supplemental materials and the R code for reproducing\nthe figures found in the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.00315v1"
    },
    {
        "title": "Opening practice: supporting Reproducibility and Critical spatial data\n  science",
        "authors": [
            "Chris Brunsdon",
            "Alexis Comber"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  This paper reflects on a number of trends towards a more open and\nreproducible approach to geographic and spatial data science over recent years.\nIn particular it considers trends towards Big Data, and the impacts this is\nhaving on spatial data analysis and modelling. It identifies a turn in academia\ntowards coding as a core analytic tool, and away from proprietary software\ntools offering 'black boxes' where the internal workings of the analysis are\nnot revealed. It is argued that this closed form software is problematic, and\nconsiders a number of ways in which issues identified in spatial data analysis\n(such as the MAUP) could be overlooked when working with closed tools, leading\nto problems of interpretation and possibly inappropriate actions and policies\nbased on these. In addition, this paper and considers the role that\nreproducible and open spatial science may play in such an approach, taking into\naccount the issues raised. It highlights the dangers of failing to account for\nthe geographical properties of data, now that all data are spatial (they are\ncollected somewhere), the problems of a desire for n=all observations in data\nscience and it identifies the need for a critical approach. This is one in\nwhich openness, transparency, sharing and reproducibility provide a mantra for\ndefensible and robust spatial data science.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.03256v1"
    },
    {
        "title": "A novel entropy recurrence quantification analysis",
        "authors": [
            "G. Corso",
            "T. L. Prado",
            "G. Z. dos S. Lima",
            "S. R. Lopes"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  The growing study of time series, especially those related to nonlinear\nsystems, has challenged the methodologies to characterize and classify\ndynamical structures of a signal. Here we conceive a new diagnostic tool for\ntime series based on the concept of information entropy, in which the\nprobabilities are associated to microstates defined from the recurrence phase\nspace. Recurrence properties can properly be studied using recurrence plots, a\nmethodology based on binary matrices where trajec- tories in phase space of\ndynamical systems are evaluated against other embedded trajectory. Our novel\nentropy methodology has several advantages compared to the traditional\nrecurrence entropy defined in the literature, namely, the correct evaluation of\nthe chaoticity level of the signal, the weak dependence on parameters, correct\nevaluation of periodic time series properties and more sensitivity to noise\nlevel of time series. Furthermore, the new entropy quantifier developed in this\nmanuscript also fixes inconsistent results of the traditional recurrence\nentropy concept, reproducing classical results with novel insights.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.00944v1"
    },
    {
        "title": "Confidence biases and learning among intuitive Bayesians",
        "authors": [
            "Louis Lévy-Garboua",
            "Muniza Askari",
            "Marco Gazel"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  We design a double-or-quits game to compare the speed of learning one's\nspecific ability with the speed of rising confidence as the task gets\nincreasingly difficult. We find that people on average learn to be\noverconfident faster than they learn their true ability and we present an\nintuitive-Bayesian model of confidence which integrates confidence biases and\nlearning. Uncertainty about one's true ability to perform a task in isolation\ncan be responsible for large and stable confidence biases, namely limited\ndiscrimination, the hard--easy effect, the Dunning--Kruger effect, conservative\nlearning from experience and the overprecision phenomenon (without\nunderprecision) if subjects act as Bayesian learners who rely only on\nsequentially perceived performance cues and contrarian illusory signals induced\nby doubt. Moreover, these biases are likely to persist since the Bayesian\naggregation of past information consolidates the accumulation of errors and the\nperception of contrarian illusory signals generates conservatism and\nunder-reaction to events. Taken together, these two features may explain why\nintuitive Bayesians make systematically wrong predictions of their own\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02748v1"
    },
    {
        "title": "TDEFSI: Theory Guided Deep Learning Based Epidemic Forecasting with\n  Synthetic Information",
        "authors": [
            "Lijing Wang",
            "Jiangzhuo Chen",
            "Madhav Marathe"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Influenza-like illness (ILI) places a heavy social and economic burden on our\nsociety. Traditionally, ILI surveillance data is updated weekly and provided at\na spatially coarse resolution. Producing timely and reliable high-resolution\nspatiotemporal forecasts for ILI is crucial for local preparedness and optimal\ninterventions. We present TDEFSI (Theory Guided Deep Learning Based Epidemic\nForecasting with Synthetic Information), an epidemic forecasting framework that\nintegrates the strengths of deep neural networks and high-resolution\nsimulations of epidemic processes over networks. TDEFSI yields accurate\nhigh-resolution spatiotemporal forecasts using low-resolution time series data.\nDuring the training phase, TDEFSI uses high-resolution simulations of epidemics\nthat explicitly model spatial and social heterogeneity inherent in urban\nregions as one component of training data. We train a two-branch recurrent\nneural network model to take both within-season and between-season\nlow-resolution observations as features, and output high-resolution detailed\nforecasts. The resulting forecasts are not just driven by observed data but\nalso capture the intricate social, demographic and geographic attributes of\nspecific urban regions and mathematical theories of disease propagation over\nnetworks. We focus on forecasting the incidence of ILI and evaluate TDEFSI's\nperformance using synthetic and real-world testing datasets at the state and\ncounty levels in the USA. The results show that, at the state level, our method\nachieves comparable/better performance than several state-of-the-art methods.\nAt the county level, TDEFSI outperforms the other methods. The proposed method\ncan be applied to other infectious diseases as well.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04663v1"
    },
    {
        "title": "Can visualization alleviate dichotomous thinking? Effects of visual\n  representations on the cliff effect",
        "authors": [
            "Jouni Helske",
            "Satu Helske",
            "Matthew Cooper",
            "Anders Ynnerman",
            "Lonni Besançon"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Common reporting styles for statistical results in scientific articles, such\nas p-values and confidence intervals (CI), have been reported to be prone to\ndichotomous interpretations, especially with respect to the null hypothesis\nsignificance testing framework. For example when the p-value is small enough or\nthe CIs of the mean effects of a studied drug and a placebo are not\noverlapping, scientists tend to claim significant differences while often\ndisregarding the magnitudes and absolute differences in the effect sizes. This\ntype of reasoning has been shown to be potentially harmful to science.\nTechniques relying on the visual estimation of the strength of evidence have\nbeen recommended to reduce such dichotomous interpretations but their\neffectiveness has also been challenged. We ran two experiments on researchers\nwith expertise in statistical analysis to compare several alternative\nrepresentations of confidence intervals and used Bayesian multilevel models to\nestimate the effects of the representation styles on differences in\nresearchers' subjective confidence in the results. We also asked the\nrespondents' opinions and preferences in representation styles. Our results\nsuggest that adding visual information to classic CI representation can\ndecrease the tendency towards dichotomous interpretations - measured as the\n`cliff effect': the sudden drop in confidence around p-value 0.05 - compared\nwith classic CI visualization and textual representation of the CI with\np-values. All data and analyses are publicly available at\nhttps://github.com/helske/statvis.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07671v4"
    },
    {
        "title": "Network impact on persistence in a finite population dynamic diffusion\n  model: application to an emergent seed exchange network",
        "authors": [
            "Pierre Barbillon",
            "Mathieu Thomas",
            "Isabelle Goldringer",
            "Frédéric Hospital",
            "Stéphane Robin"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Dynamic extinction colonisation models (also called contact processes) are\nwidely studied in epidemiology and in metapopulation theory. Contacts are\nusually assumed to be possible only through a network of connected patches.\nThis network accounts for a spatial landscape or a social organisation of\ninteractions. Thanks to social network literature, heterogeneous networks of\ncontacts can be considered. A major issue is to assess the influence of the\nnetwork in the dynamic model. Most work with this common purpose uses\ndeterministic models or an approximation of a stochastic\nExtinction-Colonisation model (sEC) which are relevant only for large networks.\nWhen working with a limited size network, the induced stochasticity is\nessential and has to be taken into account in the conclusions. Here, a rigorous\nframework is proposed for limited size networks and the limitations of the\ndeterministic approximation are exhibited. This framework allows exact\ncomputations when the number of patches is small. Otherwise, simulations are\nused and enhanced by adapted simulation techniques when necessary. A\nsensitivity analysis was conducted to compare four main topologies of networks\nin contrasting settings to determine the role of the network. A challenging\ncase was studied in this context: seed exchange of crop species in the R\\'eseau\nSemences Paysannes (RSP), an emergent French farmers' organisation. A\nstochastic Extinction-Colonisation model was used to characterize the\nconsequences of substantial changes in terms of RSP's social organisation on\nthe ability of the system to maintain crop varieties.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4287v1"
    },
    {
        "title": "A Coin-Tossing Conundrum",
        "authors": [
            "James Stein",
            "Leonard M. Wapner"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  It is shown that an equiprobability hypothesis leads to a scenario in which\nit is possible to predict the outcome of a single toss of a fair coin with a\nsuccess probability greater than 50%. We discuss whether this hypothesis might\nbe independent of the usual hypotheses governing probability, as well as\nwhether this hypothesis might be assumed as a result of the Principle of\nIndifference. Also discussed are ways to implement or circumvent the\nhypothesis.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.01298v1"
    },
    {
        "title": "Apocalypse Now? Reviving the Doomsday Argument",
        "authors": [
            "Fergus Simpson"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Whether the fate of our species can be forecast from its past has been the\ntopic of considerable controversy. One refutation of the so-called Doomsday\nArgument is based on the premise that we are more likely to exist in a universe\ncontaining a greater number of observers. Here we present a Bayesian\nreformulation of the Doomsday Argument which is immune to this effect. By\nmarginalising over the spatial configuration of observers, we find that any\npreference for a larger total number of observers has no impact on the inferred\nlocal number. Our results remain unchanged when we adopt either the\nSelf-Indexing Assumption (SIA) or the Self-Sampling Assumption (SSA).\nFurthermore the median value of our posterior distribution is found to be in\nagreement with the frequentist forecast. Humanity's prognosis for the coming\ncentury is well approximated by a global catastrophic risk of 0.2% per year.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03072v1"
    },
    {
        "title": "A Guide to Teaching Data Science",
        "authors": [
            "Stephanie C. Hicks",
            "Rafael A. Irizarry"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Demand for data science education is surging and traditional courses offered\nby statistics departments are not meeting the needs of those seeking training.\nThis has led to a number of opinion pieces advocating for an update to the\nStatistics curriculum. The unifying recommendation is computing should play a\nmore prominent role. We strongly agree with this recommendation, but advocate\nthe main priority is to bring applications to the forefront as proposed by\nNolan and Speed (1999). We also argue that the individuals tasked with\ndeveloping data science courses should not only have statistical training, but\nalso have experience analyzing data with the main objective of solving\nreal-world problems. Here, we share a set of general principles and offer a\ndetailed guide derived from our successful experience developing and teaching a\ngraduate-level, introductory data science course centered entirely on case\nstudies. We argue for the importance of statistical thinking, as defined by\nWild and Pfannkuck (1999) and describe how our approach teaches students three\nkey skills needed to succeed in data science, which we refer to as creating,\nconnecting, and computing. This guide can also be used for statisticians\nwanting to gain more practical knowledge about data science before embarking on\nteaching an introductory course.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.07140v2"
    },
    {
        "title": "CMS Sematrix: A Tool to Aid the Development of Clinical Quality Measures\n  (CQMs)",
        "authors": [
            "Michael A. Schwemmer",
            "Po-Hsu Chen",
            "Mithun Balakrishna",
            "Amy Leibrand",
            "Aaron Leonard",
            "Nancy J. McMillan",
            "Jeffrey J. Geppert"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  As part of the effort to improve quality and to reduce national healthcare\ncosts, the Centers for Medicare and Medicaid Services (CMS) are responsible for\ncreating and maintaining an array of clinical quality measures (CQMs) for\nassessing healthcare structure, process, outcome, and patient experience across\nvarious conditions, clinical specialties, and settings. The development and\nmaintenance of CQMs involves substantial and ongoing evaluation of the evidence\non the measure's properties: importance, reliability, validity, feasibility,\nand usability. As such, CMS conducts monthly environmental scans of the\npublished clinical and health service literature. Conducting time consuming,\nexhaustive evaluations of the ever-changing healthcare literature presents one\nof the largest challenges to an evidence-based approach to healthcare quality\nimprovement. Thus, it is imperative to leverage automated techniques to aid CMS\nin the identification of clinical and health services literature relevant to\nCQMs. Additionally, the estimated labor hours and related cost savings of using\nCMS Sematrix compared to a traditional literature review are roughly 818 hours\nand 122,000 dollars for a single monthly environmental scan.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.01918v1"
    },
    {
        "title": "A Combinatorial Solution to Causal Compatibility",
        "authors": [
            "Thomas C. Fraser"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Within the field of causal inference, it is desirable to learn the structure\nof causal relationships holding between a system of variables from the\ncorrelations that these variables exhibit; a sub-problem of which is to certify\nwhether or not a given causal hypothesis is compatible with the observed\ncorrelations. A particularly challenging setting for assessing causal\ncompatibility is in the presence of partial information; i.e. when some of the\nvariables are hidden/latent. This paper introduces the possible worlds\nframework as a method for deciding causal compatibility in this difficult\nsetting. We define a graphical object called a possible worlds diagram, which\ncompactly depicts the set of all possible observations. From this construction,\nwe demonstrate explicitly, using several examples, how to prove causal\nincompatibility. In fact, we use these constructions to prove causal\nincompatibility where no other techniques have been able to. Moreover, we prove\nthat the possible worlds framework can be adapted to provide a complete\nsolution to the possibilistic causal compatibility problem. Even more, we also\ndiscuss how to exploit graphical symmetries and cross-world consistency\nconstraints in order to implement a hierarchy of necessary compatibility tests\nthat we prove converges to sufficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.07091v2"
    },
    {
        "title": "Discussion of \"Nonparametric generalized fiducial inference for survival\n  functions under censoring\"",
        "authors": [
            "G. Taraldsen",
            "B. H. Lindqvist"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The following discussion is inspired by the paper Nonparametric generalized\nfiducial inference for survival functions under censoring by Cui and Hannig.\nThe discussion consists of comments on the results, but also indicates it's\nimportance more generally in the context of fiducial inference. A two page\nintroduction to fiducial inference is given to provide a context.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.09929v1"
    },
    {
        "title": "AVaN Pack: An Analytical/Numerical Solution for Variance-Based\n  Sensitivity Analysis",
        "authors": [
            "Eduardo Vasconcelos",
            "Adriano Souza",
            "Kelvin Dias"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Sensitivity analysis is an important concept to analyze the influences of\nparameters in a system, an equation or a collection of data. The methods used\nfor sensitivity analysis are divided into deterministic and statistical\ntechniques. Generally, deterministic techniques analyze fixed points of a model\nwhilst stochastic techniques analyze a range of values. Deterministic methods\nfail in analyze the entire range of input values and stochastic methods\ngenerate outcomes with random errors. In this manuscript, we are interested in\nstochastic methods, mainly in variance-based techniques such as Variance and\nSobol indices, since this class of techniques is largely used on literature.\nThe objective of this manuscript is to present an analytical solution for\nvariance based sensitive analysis. As a result of this research, two small\nprograms were developed in Javascript named as AVaN Pack (Analysis of Variance\nthrough Numerical solution). These programs allow users to find the\ncontribution of each individual parameter in any function by means of a\nmathematical solution, instead of sampling-based ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11369v1"
    },
    {
        "title": "A generalization of the symmetrical and optimal\n  probability-to-possibility transformations",
        "authors": [
            "Esteve del Acebo",
            "Yousef Alizadeh-Q",
            "Sayyed Ali Hossayni"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Possibility and probability theories are alternative and complementary ways\nto deal with uncertainty, which has motivated over the last years an interest\nfor the study of ways to transform probability distributions into possibility\ndistributions and conversely. This paper studies the advantages and\nshortcomings of two well-known discrete probability to possibility\ntransformations: the optimal transformation and the symmetrical transformation,\nand presents a novel parametric family of probability to possibility\ntransformations which generalizes them and alleviate their shortcomings,\nshowing a big potential for practical application. The paper also introduces a\nnovel fuzzy measure of specificity for probability distributions based on the\nconcept of fuzzy subsethood and presents a empirical validation of the\ngeneralized transformation usefulness applying it to the text authorship\nattribution problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.00007v1"
    },
    {
        "title": "Exponential moving average versus moving exponential average",
        "authors": [
            "Frank Klinker"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  In this note we discuss the mathematical tools to define trend indicators\nwhich are used to describe market trends. We explain the relation between\naverages and moving averages on the one hand and the so called exponential\nmoving average (EMA) on the other hand. We present a lot of examples and give\nthe definition of the most frequently used trend indicator, the MACD, and\ndiscuss its properties.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.04237v1"
    },
    {
        "title": "Integrating data science ethics into an undergraduate major: A case\n  study",
        "authors": [
            "Benjamin S. Baumer",
            "Randi L. Garcia",
            "Albert Y. Kim",
            "Katherine M. Kinnaird",
            "Miles Q. Ott"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  We present a programmatic approach to incorporating ethics into an\nundergraduate major in statistical and data sciences. We discuss\ndepartmental-level initiatives designed to meet the National Academy of\nSciences recommendation for integrating ethics into the curriculum from\ntop-to-bottom as our majors progress from our introductory courses to our\nsenior capstone course, as well as from side-to-side through co-curricular\nprogramming. We also provide six examples of data science ethics modules used\nin five different courses at our liberal arts college, each focusing on a\ndifferent ethical consideration. The modules are designed to be portable such\nthat they can be flexibly incorporated into existing courses at different\nlevels of instruction with minimal disruption to syllabi. We connect our\nefforts to a growing body of literature on the teaching of data science ethics,\npresent assessments of our effectiveness, and conclude with next steps and\nfinal thoughts.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.07649v6"
    },
    {
        "title": "The Product Life Cycle of Durable Goods",
        "authors": [
            "Joachim Kaldasch"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  A dynamic model of the product lifecycle of (nearly) homogeneous durables in\npolypoly markets is established. It describes the concurrent evolution of the\nunit sales and price of durable goods. The theory is based on the idea that the\nsales dynamics is determined by a meeting process of demanded with supplied\nproduct units. Taking advantage from the Bass model for first purchase and a\nlogistic model for repurchase the entire product lifecycle of a durable can be\nestablished. For the case of a fast growing supply the model suggests that the\nmean price of the good decreases according to a logistic law. Both, the\nestablished unit sales and price evolution are in agreement with the empirical\ndata studied in this paper. The presented approach discusses further the\ninterference of the diffusion process with the supply dynamics. The model\npredicts the occurrence of lost sales in the initial stages of the lifecycle\ndue to supply constraints. They are the origin for a retarded market\npenetration. The theory suggests that the imitation rate B indicating social\ncontagion in the Bass model has its maximum magnitude for the case of a large\namount of available units at introduction and a fast output increase. The\nempirical data of the investigated samples are in qualitative agreement with\nthis prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.0828v2"
    },
    {
        "title": "Technical Report: Energy Evaluation of preamble Sampling MAC Protocols\n  for Wireless Sensor Networks",
        "authors": [
            "Giorgio Corbellini",
            "Cedric Abgrall",
            "Emilio Calvanese Strinati",
            "Andrzej Duda"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  The paper presents a simple probabilistic analysis of the energy consumption\nin preamble sampling MAC protocols. We validate the analytical results with\nsimulations. We compare the classical MAC protocols (B-MAC and X-MAC) with\nLAMAC, a method proposed in a companion paper. Our analysis highlights the\nenergy savings achievable with LA-MAC with respect to B-MAC and X-MAC. It also\nshows that LA-MAC provides the best performance in the considered case of high\ndensity networks under traffic congestion.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.6531v1"
    },
    {
        "title": "Statistical Robust Chinese Remainder Theorem for Multiple Numbers",
        "authors": [
            "Hanshen Xiao",
            "Nan Du",
            "Zhikang T. Wang",
            "Guoqiang Xiao"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Generalized Chinese Remainder Theorem (CRT) is a well-known approach to solve\nambiguity resolution related problems. In this paper, we study the robust CRT\nreconstruction for multiple numbers from a view of statistics. To the best of\nour knowledge, it is the first rigorous analysis on the underlying statistical\nmodel of CRT-based multiple parameter estimation. To address the problem, two\nnovel approaches are established. One is to directly calculate a conditional\nmaximum a posteriori probability (MAP) estimation of the residue clustering,\nand the other is based on a generalized wrapped Gaussian mixture model to\niteratively search for MAP of both estimands and clustering. Residue error\ncorrecting codes are introduced to improve the robustness further. Experimental\nresults show that the statistical schemes achieve much stronger robustness\ncompared to state-of-the-art deterministic schemes, especially in heavy-noise\nscenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.00225v1"
    },
    {
        "title": "Explicit formulas for the joint third and fourth central moments of the\n  multinomial distribution",
        "authors": [
            "Frédéric Ouimet"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  We give the first explicit formulas for the joint third and fourth central\nmoments of the multinomial distribution, by differentiating the moment\ngenerating function. A general formula for the joint factorial moments was\npreviously given in Mosimann (1962).\n",
        "pdf_link": "http://arxiv.org/pdf/2006.09059v2"
    },
    {
        "title": "Spatial variation in the basic reproduction number of COVID-19: A\n  systematic review",
        "authors": [
            "Renate Thiede",
            "Nada Abdelatif",
            "Inger Fabris-Rotelli",
            "Raeesa Manjoo-Docrat",
            "Jennifer Holloway",
            "Charl Janse van Rensburg",
            "Pravesh Debba",
            "Nontembeko Dudeni-Tlhone",
            "Zaid Kimmie",
            "Alize le Roux"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  OBJECTIVES: Estimates of the basic reproduction number (R0) of COVID-19 vary\nacross countries. This paper aims to characterise the spatial variability in R0\nacross the first six months of the global COVID-19 outbreak, and to explore\nsocial factors that impact R0 estimates at national and regional level.\n  METHODS: We searched PubMed, LitCOVID and the WHO COVID-19 database from\nJanuary to June 2020. Peer-reviewed English-language papers were included that\nprovided R0 estimates. For each study, the value of the estimate, country under\nstudy and publication month were extracted. The median R0 value was calculated\nper country, and the median and variance were calculated per region. For each\ncountry with an R0 estimate, the Human Development Index (HDI), Sustainable\nMobility Index (SMI), median age, population density and development status\nwere obtained from external sources.\n  RESULTS: A total of 81 studies were included in the analysis. These studies\nprovided at least one estimate of R0, along with sufficient methodology to\nexplain how the value was calculated. Values of R0 ranged between 0.48 and\n14.8, and between 0.48 and 6.7 when excluding outliers.\n  CONCLUSIONS: This systematic review provides a comprehensive overview of the\nestimates of the basic reproduction number of COVID-19 globally and highlights\nthe spatial heterogeneity in R0. Higher values were recorded in more developed\ncountries, and countries with an older population or more sustainable mobility.\nCountries with higher population density had lower R0 estimates. For most\nregions, variability in R0 spiked initially before reducing and stabilising as\nmore estimates became available.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.06301v1"
    },
    {
        "title": "Kill The Math and Let the Introductory Course Be Born",
        "authors": [
            "David Kane"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Our introductory classes in statistics and data science use too much\nmathematics. The key causal effect which our students want our classes to have\nis to improve their future performance and opportunities. The more professional\ntheir computing skills (in the context of data analysis), the greater their\nlikely success. Introductory courses should feature almost no\nmathematical/statistical formulas beyond simple algebra.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.14390v1"
    },
    {
        "title": "Bridging Breiman's Brook: From Algorithmic Modeling to Statistical\n  Learning",
        "authors": [
            "Lucas Mentch",
            "Giles Hooker"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  In 2001, Leo Breiman wrote of a divide between \"data modeling\" and\n\"algorithmic modeling\" cultures. Twenty years later this division feels far\nmore ephemeral, both in terms of assigning individuals to camps, and in terms\nof intellectual boundaries. We argue that this is largely due to the \"data\nmodelers\" incorporating algorithmic methods into their toolbox, particularly\ndriven by recent developments in the statistical understanding of Breiman's own\nRandom Forest methods. While this can be simplistically described as \"Breiman\nwon\", these same developments also expose the limitations of the\nprediction-first philosophy that he espoused, making careful statistical\nanalysis all the more important. This paper outlines these exciting recent\ndevelopments in the random forest literature which, in our view, occurred as a\nresult of a necessary blending of the two ways of thinking Breiman originally\ndescribed. We also ask what areas statistics and statisticians might currently\noverlook.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12328v1"
    },
    {
        "title": "A Bayesian treatment of the German tank problem",
        "authors": [
            "Cory M. Simon"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  The German tank problem has an interesting historical background and is an\nengaging problem of statistical estimation for the classroom. The objective is\nto estimate the size of a population of tanks inscribed with sequential serial\nnumbers, from a random sample. In this tutorial article, we outline the\nBayesian approach to the German tank problem, (i) whose solution assigns a\nprobability to each tank population size, thereby quantifying uncertainty, and\n(ii) which provides an opportunity to incorporate prior information and/or\nbeliefs about the tank population size into the solution. We illustrate with an\nexample. Finally, we survey problems in other contexts that resemble the German\ntank problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.00046v2"
    },
    {
        "title": "A Brief History of the Statistics Department of the University of\n  California at Berkeley",
        "authors": [
            "Terry Speed",
            "Jim Pitman",
            "John Rice"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  The early history of our department was dominated by Jerzy Neyman\n(1894-1981), while the next phase was largely in the hands of Neyman's\nstudents, with Erich Lehmann (1917-2009) being a central, long-lived and\nmuch-loved member of this group. We are very fortunate in having Constance\nReid's biography \"Neyman -- From Life\" and Erich's \"Reminiscences of a\nStatistician: The Company I Kept\" and other historical material documenting the\nfounding and growth of the department, and the people in it. In what follows,\nwe will draw heavily from these sources, describing what seems to us to be a\nremarkable success story: one person starting \"a cell of statistical research\nand teaching ... not being hampered by any existing traditions and routines\"\nand seeing that cell grow rapidly into a major force in academic statistics\nworldwide. That it has remained so for (at least) the half-century after its\nfounding is a testament to the strength of Neyman's model for a department of\nstatistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.6450v1"
    },
    {
        "title": "Using an Online Learning Environment to Teach an Undergraduate\n  Statistics Course: the tutor-web",
        "authors": [
            "Anna Helga Jonsdottir",
            "Gunnar Stefansson"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  A learning environment, the tutor-web (http://tutor-web.net), has been\ndeveloped and used for educational research. The system is accessible and free\nto use for anyone having access to the Web. It is based on open source software\nand the teaching material is licensed under the Creative Commons\nAttribution-ShareAlike License. The system has been used for computer-assisted\neducation in statistics and mathematics. It offers a unique way to structure\nand link together teaching material and includes interactive quizzes with the\nprimary purpose of increasing learning rather than mere evaluation.\n  The system was used in a course on basic statistics in the University of\nIceland, spring 2013. A randomized trial was conducted to investigate the\ndifference in learning between students doing regular homework and students\nusing the system. The difference between the groups was not found to be\nsignificant.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.0308v1"
    },
    {
        "title": "Network Analysis with the Enron Email Corpus",
        "authors": [
            "Johanna Hardin",
            "Ghassan Sarkis",
            "P. C. Urc"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  We use the Enron email corpus to study relationships in a network by applying\nsix different measures of centrality. Our results came out of an in-semester\nundergraduate research seminar. The Enron corpus is well suited to statistical\nanalyses at all levels of undergraduate education. Through this note's focus on\ncentrality, students can explore the dependence of statistical models on\ninitial assumptions and the interplay between centrality measures and\nhierarchical ranking, and they can use completed studies as springboards for\nfuture research. The Enron corpus also presents opportunities for research into\nmany other areas of analysis, including social networks, clustering, and\nnatural language processing.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.2759v3"
    },
    {
        "title": "Creating, Automating, and Assessing Online Homework in Introductory\n  Statistics and Mathematics Classes",
        "authors": [
            "Karen Santoro",
            "Roger Bilisoly"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Although textbook publishers offer course management systems, they do so to\npromote brand loyalty, and while an open source tool such as WeBWorK is\npromising, it requires administrative and IT buy-in. So supported in part by a\nCollege Access Challenge Grant from the Department of Education, we\ncollaborated with other instructors to create online homework sets for three\nclasses: Elementary Algebra, Intermediate Algebra, and Statistics for\nBehavioral Sciences I. After experimentation, some of these question pools are\nnow created by Mathematica programs that can generate data sets from specified\ndistributions, generate random polynomials that factor in a given way, create\nimage files of histograms, scatterplots, and so forth. These programs produce\nfiles that can be read by the software package, Respondus, which then uploads\nthe questions into Blackboard Learn, the course management system used by the\nConnecticut State University system. Finally, we summarize five classes worth\nof student performance data along with lessons learned while working on this\nproject.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.03215v1"
    },
    {
        "title": "The Problem Of Grue Isn't",
        "authors": [
            "William M. Briggs"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The so-called problem of grue was introduced by Nelson Goodman in 1954 as a\n\"riddle\" about induction, a riddle which has been widely thought to cast doubt\non the validity and rationality of induction. That unnecessary doubt in turn is\npartly responsible for the reluctance to adopt the view that probability is\npart of logic. Several authors have pointed out deficiencies in grue;\nnevertheless, the \"problem\" still excites. Here, adapted from Groarke, is\npresented the basis of grue, along with another simple demonstration that the\n\"problem\" makes no sense and is brought about by a misunderstanding of\ncausation.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.03811v1"
    },
    {
        "title": "Iterated Integrals and Population Time Series Analysis",
        "authors": [
            "Chad Giusti",
            "Darrick Lee"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  One of the core advantages topological methods for data analysis provide is\nthat the language of (co)chains can be mapped onto the semantics of the data,\nproviding a natural avenue for human understanding of the results. Here, we\ndescribe such a semantic structure on Chen's classical iterated integral\ncochain model for paths in Euclidean space. Specifically, in the context of\npopulation time series data, we observe that iterated integrals provide a\nmodel-free measure of pairwise influence that can be used for causality\ninference. Along the way, we survey recent results and applications, review the\ncurrent standard methods for causality inference, and briefly provide our\noutlook on generalizations to go beyond time series data.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03558v2"
    },
    {
        "title": "Learning as We Go: An Examination of the Statistical Accuracy of COVID19\n  Daily Death Count Predictions",
        "authors": [
            "Roman Marchant",
            "Noelle I. Samia",
            "Ori Rosen",
            "Martin A. Tanner",
            "Sally Cripps"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  This paper provides a formal evaluation of the predictive performance of a\nmodel (and its various updates) developed by the Institute for Health Metrics\nand Evaluation (IHME) for predicting daily deaths attributed to COVID19 for\neach state in the United States. The IHME models have received extensive\nattention in social and mass media, and have influenced policy makers at the\nhighest levels of the United States government. For effective policy making the\naccurate assessment of uncertainty, as well as accurate point predictions, are\nnecessary because the risks inherent in a decision must be taken into account,\nespecially in the present setting of a novel disease affecting millions of\nlives. To assess the accuracy of the IHME models, we examine both forecast\naccuracy as well as the predictive performance of the 95% prediction intervals\nprovided by the IHME models. We find that the initial IHME model underestimates\nthe uncertainty surrounding the number of daily deaths substantially.\nSpecifically, the true number of next day deaths fell outside the IHME\nprediction intervals as much as 70% of the time, in comparison to the expected\nvalue of 5%. In addition, we note that the performance of the initial model\ndoes not improve with shorter forecast horizons. Regarding the updated models,\nour analyses indicate that the later models do not show any improvement in the\naccuracy of the point estimate predictions. In fact, there is some evidence\nthat this accuracy has actually decreased over the initial models. Moreover,\nwhen considering the updated models, while we observe a larger percentage of\nstates having actual values lying inside the 95% prediction intervals (PI), our\nanalysis suggests that this observation may be attributed to the widening of\nthe PIs. The width of these intervals calls into question the usefulness of the\npredictions to drive policy making and resource allocation.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.04734v4"
    },
    {
        "title": "The rule of conditional probability is valid in quantum theory [Comment\n  on Gelman & Yao's \"Holes in Bayesian Statistics\"]",
        "authors": [
            "P. G. L. Porta Mana"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  In a recent manuscript, Gelman & Yao (2020) claim that \"the usual rules of\nconditional probability fail in the quantum realm\" and that \"probability theory\nisn't true (quantum physics)\" and purport to support these statements with the\nexample of a quantum double-slit experiment. The present comment recalls some\nrelevant literature in quantum theory and shows that (i) Gelman & Yao's\nstatements are false; in fact, the quantum example confirms the rules of\nprobability theory; (ii) the particular inequality found in the quantum example\ncan be shown to appear also in very non-quantum examples, such as drawing from\nan urn; thus there is nothing peculiar to quantum theory in this matter. A\ncouple of wrong or imprecise statements about quantum theory in the cited\nmanuscript are also corrected.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.08160v3"
    },
    {
        "title": "Review of Low Voltage Load Forecasting: Methods, Applications, and\n  Recommendations",
        "authors": [
            "Stephen Haben",
            "Siddharth Arora",
            "Georgios Giasemidis",
            "Marcus Voss",
            "Danica Vukadinovic Greetham"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  The increased digitalisation and monitoring of the energy system opens up\nnumerous opportunities to decarbonise the energy system. Applications on low\nvoltage, local networks, such as community energy markets and smart storage\nwill facilitate decarbonisation, but they will require advanced control and\nmanagement. Reliable forecasting will be a necessary component of many of these\nsystems to anticipate key features and uncertainties. Despite this urgent need,\nthere has not yet been an extensive investigation into the current\nstate-of-the-art of low voltage level forecasts, other than at the smart meter\nlevel. This paper aims to provide a comprehensive overview of the landscape,\ncurrent approaches, core applications, challenges and recommendations. Another\naim of this paper is to facilitate the continued improvement and advancement in\nthis area. To this end, the paper also surveys some of the most relevant and\npromising trends. It establishes an open, community-driven list of the known\nlow voltage level open datasets to encourage further research and development.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.00006v2"
    },
    {
        "title": "Discovery of Bayes' Table at Tunbridge Wells",
        "authors": [
            "David C. Schneider",
            "Roy Thompson"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  In 1755 Thomas Bayes expressed an interest in the problem of combining\nrepeated measurements of the location of a star. Bayes described a tandem\nset-up of a ball thrown on a table, followed by repeated throws of a second\nball. Bayes' table has long been taken as a billiard table, for which there is\nno evidence. We report the discovery of Bayes' table, a bowling green located\nhalf a km uphill (SE) from the meeting house where Bayes served as minister for\ntwo decades. Bayes' drawing shows a rectangular space marked off in yards,\nwhich allows calculation of an interval measurement of uncertainty. The Bayes\nrule interval from 2.5% to 97.5% is from 0.56 - 0.42 = 0.12 perches equivalent\nto 0.61 m. The discovery of Bayes' table establishes the physical basis for\nBayes' symmetrical probability model, a fixed parameter binomial ({\\theta} =\n0.5). The discovery establishes Bayes as the founder of statistical science,\ndefined as the application of mathematics to scientific measurement.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.05145v1"
    },
    {
        "title": "Mr. Bayes and the classics: a suggested interpretation",
        "authors": [
            "Marcio Alves Diniz",
            "David Richard Bellhouse"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  The main hypothesis about Thomas Bayes's intentions to write his famous Essay\non probability is that he wanted to refute the arguments of David Hume against\nthe reliability of the occurrence of miracles, published in 1748. In this paper\nwe argue that it was not Bayes's intention to rebut Hume but that his interest\non the \"inverse problem\" came about as result of his study of the second\nedition of Abraham De Moivre's book, The Doctrine of Chances, published in\n1738.\n  A possible communication of Bayes's breakthrough might have annoyed De\nMoivre, leading to a response written for Bayes in the third edition of De\nMoivre's book, published in 1756. Among other points, the response claims that\nDe Moivre was the first to solve the mentioned inverse problem. Under this\nperspective Richard Price's letter, written as preface to Bayes's essay, has a\nnew interpretation, appearing also as a defense of Bayes premiership on a\nsatisfactory or proper solution.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.03819v1"
    },
    {
        "title": "Physical, subjective and analogical probability",
        "authors": [
            "Russell J. Bowater"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  The aim of this paper is to show that the concept of probability is best\nunderstood by dividing this concept into two different types of probability,\nnamely physical probability and analogical probability. Loosely speaking, a\nphysical probability is a probability that applies to the outcomes of an\nexperiment that have been judged as being equally likely on the basis of\nphysical symmetry. Physical probabilities are arguably in some sense\n'objective' and possess all the standard properties of the concept of\nprobability. On the other hand, an analogical probability is defined by making\nan analogy between the uncertainty surrounding an event of interest and the\nuncertainty surrounding an event that has a physical probability. Analogical\nprobabilities are undeniably subjective probabilities and are not obliged to\nhave all the standard mathematical properties possessed by physical\nprobabilities, e.g. they may not have the property of additivity or obey the\nstandard definition of conditional probability. Nevertheless, analogical\nprobabilities have extra properties, which are not possessed by physical\nprobabilities, that assist in their direct elicitation, general derivation,\ncomparison and justification. More specifically, these properties facilitate\nthe application of analogical probability to real-world problems that can not\nbe adequately resolved by using only physical probability, e.g. probabilistic\ninference about hypotheses on the basis of observed data. Careful definitions\nare given of the concepts that are introduced and, where appropriate, examples\nof the application of these concepts are presented for additional clarity.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.10159v1"
    },
    {
        "title": "Race and ethnicity data for first, middle, and last names",
        "authors": [
            "Evan T. R. Rosenman",
            "Santiago Olivella",
            "Kosuke Imai"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  We provide the largest compiled publicly available dictionaries of first,\nmiddle, and last names for the purpose of imputing race and ethnicity using,\nfor example, Bayesian Improved Surname Geocoding (BISG). The dictionaries are\nbased on the voter files of six Southern states that collect self-reported\nracial data upon voter registration. Our data cover a much larger scope of\nnames than any comparable dataset, containing roughly one million first names,\n1.1 million middle names, and 1.4 million surnames. Individuals are categorized\ninto five mutually exclusive racial and ethnic groups -- White, Black,\nHispanic, Asian, and Other -- and racial/ethnic counts by name are provided for\nevery name in each dictionary. Counts can then be normalized row-wise or\ncolumn-wise to obtain conditional probabilities of race given name or name\ngiven race. These conditional probabilities can then be deployed for imputation\nin a data analytic task for which ground truth racial and ethnic data is not\navailable.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.12443v1"
    },
    {
        "title": "Risk management in the use of published statistical results for policy\n  decisions",
        "authors": [
            "Duncan Ermini Leaf"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Statistical inferential results generally come with a measure of reliability\nfor decision-making purposes. For a policy implementer, the value of\nimplementing published policy research depends critically upon this\nreliability. For a policy researcher, the value of policy implementation may\ndepend weakly or not at all upon the policy's outcome. Some researchers might\nbenefit from overstating the reliability of statistical results. Implementers\nmay find it difficult or impossible to determine whether researchers are\noverstating reliability. This information asymmetry between researchers and\nimplementers can lead to an adverse selection problem where, at best, the full\nbenefits of a policy are not realized or, at worst, a policy is deemed too\nrisky to implement at any scale. Researchers can remedy this by guaranteeing\nthe policy outcome. Researchers can overcome their own risk aversion and wealth\nconstraints by exchanging risks with other researchers or offering only partial\ninsurance. The problem and remedy are illustrated using a confidence interval\nfor the success probability of a binomial policy outcome.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03205v2"
    },
    {
        "title": "Using R for teaching and research",
        "authors": [
            "M. Isabel Parra",
            "Eva L. Sanjuán",
            "M. Carmen Robustillo",
            "Mario M. Pizarro"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  R is a language and environment for statistical computing and graphics, which\nprovides a wide variety of statistical tools (modeling, statistical testing,\ntime series analysis, classification problems, machine learning, ...), together\nwith amazing graphical techniques and the great advantage that it is highly\nextensible. Nowadays, there is no doubt that it is the software par excellence\nin statistical courses for any level, for theoretical and applied subjects\nalike. Besides, it has become an almost essential tool for every research work\nthat involves any kind of analysis or data visualization. Furthermore, it is\none of the most employed programming languages for general purposes. The goal\nof this work is helping to share ideas and resources to improve teaching and/or\nresearch using the statistical software R. We will cover its benefits, show how\nto get started and where to locate specific resources, and will make\ninteresting recommendations for using R, according to our experience. For the\nclassroom we will develop a curricular and assessment infrastructure to support\nboth dissemination and evaluation, while for research we will offer a broader\napproach to quantitative studies that provides an excellent support for work in\nscience and technology.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.12200v1"
    },
    {
        "title": "Extending the Bayesian Framework from Information to Action",
        "authors": [
            "Vasileios Basios",
            "Yukio-Pegio Gunji",
            "Pier-Francesco Moretti"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  In this review, we examine an extended Bayesian inference method and its\nrelation to biological information processing. We discuss the idea of combining\ntwo modes of Bayesian inference. The first is the standard Bayesian inference,\nwhich contracts probability space. The second is its inverse, which extends and\nenriches the probability space of latent and observable variables. Their\ncombination has been observed that, greatly, facilitates discovery. Moreover,\nthis dual search during the updating process elucidates a crucial difference\nbetween biological and artificial information processing. The latter is\nrestricted due to nonlinearities, while the former utilizes it. This duality is\nubiquitous in biological information process dynamics (`flee-or-fight',\n`explore-or-exploit' etc.) as is the role of fractality and chaos in its\nunderlying nonequilibrium, nonlinear dynamics. We also propose a new\nexperimental set up that stems from testing these ideas.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.00025v1"
    },
    {
        "title": "J. B. S. Haldane's Rule of Succession",
        "authors": [
            "Eric-Jan Wagenmakers",
            "Sandy Zabell",
            "Quentin F. Gronau"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  After Bayes, the oldest Bayesian account of enumerative induction is given by\nLaplace's so-called rule of succession: if all $n$ observed instances of a\nphenomenon to date exhibit a given character, the probability that the next\ninstance of that phenomenon will also exhibit the character is\n$\\frac{n+1}{n+2}$. Laplace's rule however has the apparently counterintuitive\nmathematical consequence that the corresponding \"universal generalization\"\n(every future observation of this type will also exhibit that character) has\nzero probability. In 1932, the British scientist J. B. S. Haldane proposed an\nalternative rule giving a universal generalization the positive probability\n$\\frac{n+1}{n+2} \\times \\frac{n+3}{n+2}$. A year later Harold Jeffreys proposed\nessentially the same rule in the case of a finite population. A related variant\nrule results in a predictive probability of $\\frac{n+1}{n+2} \\times\n\\frac{n+4}{n+3}$. These arguably elegant adjustments of the original Laplacean\nform have the advantage that they give predictions better aligned with\nintuition and common sense. In this paper we discuss J. B. S. Haldane's rule\nand its variants, placing them in their historical context, and relating them\nto subsequent philosophical discussions.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.09489v1"
    },
    {
        "title": "The fiducial-Bayes fusion: A general theory of statistical inference",
        "authors": [
            "Russell J. Bowater"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  An overview is presented of a general theory of statistical inference that is\nreferred to as the fiducial-Bayes fusion. This theory combines organic fiducial\ninference and Bayesian inference. The aim is that the reader is given a clear\nsummary of the conceptual framework of the fiducial-Bayes fusion as well as\npointers to further reading about its more technical aspects. Particular\nattention is paid to the issue of how much importance should be attached to the\nrole of Bayesian inference within this framework. The appendix contains a\nsubstantive example of the application of the theory of the fiducial-Bayes\nfusion, which supplements various other examples of the application of this\ntheory that are referenced in the paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.01533v1"
    },
    {
        "title": "Facilitating the Integration of Ethical Reasoning into Quantitative\n  Courses: Stakeholder Analysis, Ethical Practice Standards, and Case Studies",
        "authors": [
            "Rochelle E. Tractenberg",
            "Suzanne Thorton"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Case studies are typically used to teach 'ethics', but in quantitative\ncourses it can seem distracting, for both instructor and learner, to introduce\na case analysis. Moreover, case analyses are typically focused on issues\nrelating to people: obtaining consent, dealing with research team members,\nand/or potential institutional policy violations. While relevant to some\nresearch, not all students in quantitative courses plan to become researchers,\nand ethical practice is an essential topic for students of of mathematics,\nstatistics, data science, and computing regardless of whether or not the\nlearner intends to do research. Ethical reasoning is a way of thinking that\nrequires the individual to assess what they know about a potential ethical\nproblem (their prerequisite knowledge), and in some cases, how behaviors they\nobserve, are directed to perform, or have performed, diverge from what they\nknow to be ethical behavior. Ethical reasoning is a learnable, improvable set\nof knowledge, skills, and abilities that enable learners to recognize what they\ndo and do not know about what constitutes 'ethical practice' of a discipline,\nand in some cases, to contemplate alternative decisions about how to first\nrecognize, and then proceed past, or respond to, such divergences. A\nstakeholder analysis is part of prerequisite knowledge, and can be used whether\nthere is or is not an actual case or situation to react to. In courses with\nmainly quantitative content, a stakeholder analysis is a useful tool for\ninstruction and assessment. It can be used to both integrate authentic ethical\ncontent and encourage careful quantitative thought. It is a mistake to treat\n'training in ethical practice' and 'training in responsible conduct of\nresearch' as the same thing. This paper discusses how to introduce ethical\nreasoning, stakeholder analysis, and ethical practice standards authentically\nin quantitative courses.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.01973v2"
    },
    {
        "title": "Generative AI for Data Science 101: Coding Without Learning To Code",
        "authors": [
            "Jacob Bien",
            "Gourab Mukherjee"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Should one teach coding in a required introductory statistics and data\nscience class for non-major students? Many professors advise against it,\nconsidering it a distraction from the important and challenging statistical\ntopics that need to be covered. By contrast, other professors argue that the\nability to interact flexibly with data will inspire students with a lasting\nlove of the subject and a continued commitment to the material beyond the\nintroductory course. With the release of large language models that write code,\nwe saw an opportunity for a middle ground, which we tried in Fall 2023 in a\nrequired introductory data science course in our school's full-time MBA\nprogram. We taught students how to write English prompts to the artificial\nintelligence tool Github Copilot that could be turned into R code and executed.\nIn this short article, we report on our experience using this new approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.17647v2"
    },
    {
        "title": "Malaria incidence and prevalence: An ecological analysis through Six\n  Sigma approach",
        "authors": [
            "Md. Al-Amin",
            "Kesava Chandran Vijaya Bhaskar",
            "Walaa Enab",
            "Reza Kamali Miab",
            "Jennifer Slavin",
            "Nigar Sultana"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Malaria is the leading cause of death globally, especially in sub-Saharan\nAfrican countries claiming over 400,000 deaths globally each year, underscoring\nthe critical need for continued efforts to combat this preventable and\ntreatable disease. The objective of this study is to provide statistical\nguidance on the optimal preventive and control measures against malaria. Data\nhave been collected from reliable sources, such as World Health Organization,\nUNICEF, Our World in Data, and STATcompiler. Data were categorized according to\nthe factors and sub-factors related to deaths caused by malaria. These factors\nand sub-factors were determined based on root cause analysis and data sources.\nUsing JMP 16 Pro software, both linear and multiple linear regression were\nconducted to analyze the data. The analyses aimed to establish a linear\nrelationship between the dependent variable (malaria deaths in the overall\npopulation) and independent variables, such as life expectancy, malaria\nprevalence in children, net usage, indoor residual spraying usage, literate\npopulation, and population with inadequate sanitation in each selected sample\ncountry. The statistical analysis revealed that using insecticide treated nets\n(ITNs) by children and individuals significantly decreased the death count, as\n1,000 individuals sleeping under ITNs could reduce the death count by eight.\nBased on the statistical analysis, this study suggests more rigorous research\non the usage of ITNs.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.02233v1"
    },
    {
        "title": "Exploring the impact of gamification on engagement in a statistics\n  classroom",
        "authors": [
            "Eilidh Jack",
            "Craig Alexander",
            "Elinor M Jones"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  In recent years, the integration of gamification into educational settings\nhas garnered significant attention as a means to enhance student engagement and\nlearning outcomes. By leveraging gamified elements such as points and\nleaderboards, educators aim to promote active participation, motivation, and\ndeeper understanding among students. This study investigates the effect of\ngamification on student engagement in a flipped statistics classroom\nenvironment. The findings suggest that gamification strategies, when\neffectively implemented, can have a positive impact on student motivation and\nengagement. This paper concludes with recommendations for educators, potential\nchallenges such as superficial engagement and demotivation, and future\ndirections for research to address these challenges and further explore the\npotential of gamification in fostering student success.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.18313v2"
    },
    {
        "title": "To democratize research with sensitive data, we should make synthetic\n  data more accessible",
        "authors": [
            "Erik-Jan van Kesteren"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  For over 30 years, synthetic data has been heralded as a promising solution\nto make sensitive datasets accessible. However, despite much research effort\nand several high-profile use-cases, the widespread adoption of synthetic data\nas a tool for open, accessible, reproducible research with sensitive data is\nstill a distant dream. In this opinion, Erik-Jan van Kesteren, head of the\nODISSEI Social Data Science team, argues that in order to progress towards\nwidespread adoption of synthetic data as a privacy enhancing technology, the\ndata science research community should shift focus away from developing better\nsynthesis methods: instead, it should develop accessible tools, educate peers,\nand publish small-scale case studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.17271v1"
    },
    {
        "title": "Alternative proof for the bias of the hot hand statistic of streak\n  length one",
        "authors": [
            "Maximilian Janisch"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  For a sequence of $n$ random variables taking values $0$ or $1$, the hot hand\nstatistic of streak length $k$ counts what fraction of the streaks of length\n$k$, that is, $k$ consecutive variables taking the value $1$, among the $n$\nvariables are followed by another $1$. Since this statistic does not use the\nexpected value of how many streaks of length $k$ are observed, but instead uses\nthe realization of the number of streaks present in the data, it may be a\nbiased estimator of the conditional probability of a fixed random variable\ntaking value $1$ if it is preceded by a streak of length $k$, as was first\nstudied and observed explicitly in [Miller and Sanjurjo, 2018]. In this short\nnote, we suggest an alternative proof for an explicit formula of the\nexpectation of the hot hand statistic for the case of streak length one. This\nformula was obtained through a different argument in [Miller and Sanjurjo,\n2018] and [Rinott and Bar-Hillel, 2015].\n",
        "pdf_link": "http://arxiv.org/pdf/2407.10577v1"
    },
    {
        "title": "The Future of Data Science Education",
        "authors": [
            "Brian Wright",
            "Peter Alonzi",
            "Ali Riveria"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  The definition of Data Science is a hotly debated topic. For many, the\ndefinition is a simple shortcut to Artificial Intelligence or Machine Learning.\nHowever, there is far more depth and nuance to the field of Data Science than a\nsimple shortcut can provide. The School of Data Science at the University of\nVirginia has developed a novel model for the definition of Data Science. This\nmodel is based on identifying a unified understanding of the data work done\nacross all areas of Data Science. It represents a generational leap forward in\nhow we understand and teach Data Science. In this paper we will present the\ncore features of the model and explain how it unifies various concepts going\nfar beyond the analytics component of AI. From this foundation we will present\nour Undergraduate Major curriculum in Data Science and demonstrate how it\nprepares students to be well-rounded Data Science team members and leaders. The\npaper will conclude with an in-depth overview of the Foundations of Data\nScience course designed to introduce students to the field while also\nimplementing proven STEM oriented pedagogical methods. These include, for\nexample, specifications grading, active learning lectures, guest lectures from\nindustry experts and weekly gamification labs.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.11824v1"
    },
    {
        "title": "The Impact of Foreign Players in the English Premier League: A\n  Mathematical Analys",
        "authors": [
            "Amit K Chattopadhyay",
            "A. Abdul",
            "Sudhir Jain"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  We undertake extensive analysis of English Premier League data over the\nperiod 2009/10 to 2017/18 to identify and rank key factors affecting the\neconomic and footballing performances of the teams. Alternative end-of-season\nleague tables are generated by re-ranking the teams based on five different\ndescriptors - total expenditure, total funds spent on players, total funds\nspent on foreign players, the ratio of foreign to British players and the\noverall profit. The unequal distribution of resources and expenditure between\nthe clubs is analyzed through Lorenz curves. A comparative analysis of the\ndifferences between the alternative tables and the conventional end-of-season\nleague table establishes the most likely factors to influence the performances\nof the teams that we also rank using Principal Component Analysis. We find that\nthe top teams in the league are also those that tend to have the highest\nexpenditure overall, for all players, including foreign players; they also have\nthe highest ratios of foreign to British players. Our statistical and machine\nlearning study also indicates that successful performance on the field may not\nguarantee healthy profits at the end of the season.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.19285v1"
    },
    {
        "title": "Unified Inductive Logic: From Formal Learning to Statistical Inference\n  to Supervised Learning",
        "authors": [
            "Hanti Lin"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  While the traditional conception of inductive logic is Carnapian, I develop a\nPeircean alternative and use it to unify formal learning theory, statistics,\nand a significant part of machine learning: supervised learning. Some crucial\nstandards for evaluating non-deductive inferences have been assumed separately\nin those areas, but can actually be justified by a unifying principle.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.02969v1"
    },
    {
        "title": "Equity in the Use of ChatGPT for the Classroom: A Comparison of the\n  Accuracy and Precision of ChatGPT 3.5 vs. ChatGPT4 with Respect to Statistics\n  and Data Science Exams",
        "authors": [
            "Monnie McGee",
            "Bivin Sadler"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  A college education historically has been seen as method of moving upward\nwith regards to income brackets and social status. Indeed, many colleges\nrecognize this connection and seek to enroll talented low income students.\nWhile these students might have their education, books, room, and board paid;\nthere are other items that they might be expected to use that are not part of\nmost college scholarship packages. One of those items that has recently\nsurfaced is access to generative AI platforms. The most popular of these\nplatforms is ChatGPT, and it has a paid version (ChatGPT4) and a free version\n(ChatGPT3.5). We seek to explore differences in the free and paid versions in\nthe context of homework questions and data analyses as might be seen in a\ntypical introductory statistics course. We determine the extent to which\nstudents who cannot afford newer and faster versions of generative AI programs\nwould be disadvantaged in terms of writing such projects and learning these\nmethods.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.13116v1"
    },
    {
        "title": "The Landscape of College-level Data Visualization Courses, and the\n  Benefits of Incorporating Statistical Thinking",
        "authors": [
            "Zach Branson",
            "Monica Paz Parra",
            "Ronald Yurko"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Data visualization is a core part of statistical practice and is ubiquitous\nin many fields. Although there are numerous books on data visualization,\ninstructors in statistics and data science may be unsure how to teach data\nvisualization, because it is such a broad discipline. To give guidance on\nteaching data visualization from a statistical perspective, we make two\ncontributions. First, we conduct a survey of data visualization courses at top\ncolleges and universities in the United States, in order to understand the\nlandscape of data visualization courses. We find that most courses are not\ntaught by statistics and data science departments and do not focus on\nstatistical topics, especially those related to inference. Instead, most\ncourses focus on visual storytelling, aesthetic design, dashboard design, and\nother topics specialized for other disciplines. Second, we outline three\nteaching principles for incorporating statistical inference in data\nvisualization courses, and provide several examples that demonstrate how\ninstructors can follow these principles. The dataset from our survey allows\nothers to explore the diversity of data visualization courses, and our teaching\nprinciples give guidance to instructors and departments who want to encourage\nstatistical thinking via data visualization. In this way, statistics-related\ndepartments can provide a valuable perspective on data visualization that is\nunique to current course offerings.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.16402v1"
    },
    {
        "title": "Towards Strong AI: Transformational Beliefs and Scientific Creativity",
        "authors": [
            "Samuel J. Eschker",
            "Chuanhai Liu"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Strong artificial intelligence (AI) is envisioned to possess general\ncognitive abilities and scientific creativity comparable to human intelligence,\nencompassing both knowledge acquisition and problem-solving. While remarkable\nprogress has been made in weak AI, the realization of strong AI remains a topic\nof intense debate and critical examination. In this paper, we explore pivotal\ninnovations in the history of astronomy and physics, focusing on the discovery\nof Neptune and the concept of scientific revolutions as perceived by\nphilosophers of science. Building on these insights, we introduce a simple\ntheoretical and statistical framework of weak beliefs, termed the\nTransformational Belief (TB) framework, designed as a foundation for modeling\nscientific creativity. Through selected illustrative examples in statistical\nscience, we demonstrate the TB framework's potential as a promising foundation\nfor understanding, analyzing, and even fostering creativity -- paving the way\ntoward the development of strong AI. We conclude with reflections on future\nresearch directions and potential advancements.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.19938v1"
    },
    {
        "title": "Generative AI Takes a Statistics Exam: A Comparison of Performance\n  between ChatGPT3.5, ChatGPT4, and ChatGPT4o-mini",
        "authors": [
            "Monnie McGee",
            "Bivin Sadler"
        ],
        "category": "stat.OT",
        "published_year": "2025",
        "summary": "  Many believe that use of generative AI as a private tutor has the potential\nto shrink access and achievement gaps between students and schools with\nabundant resources versus those with fewer resources. Shrinking the gap is\npossible only if paid and free versions of the platforms perform with the same\naccuracy. In this experiment, we investigate the performance of GPT versions\n3.5, 4.0, and 4o-mini on the same 16-question statistics exam given to a class\nof first-year graduate students. While we do not advocate using any generative\nAI platform to complete an exam, the use of exam questions allows us to explore\naspects of ChatGPT's responses to typical questions that students might\nencounter in a statistics course. Results on accuracy indicate that GPT 3.5\nwould fail the exam, GPT4 would perform well, and GPT4o-mini would perform\nsomewhere in between. While we acknowledge the existence of other Generative\nAI/LLMs, our discussion concerns only ChatGPT because it is the most widely\nused platform on college campuses at this time. We further investigate\ndifferences among the AI platforms in the answers for each problem using\nmethods developed for text analytics, such as reading level evaluation and\ntopic modeling. Results indicate that GPT3.5 and 4o-mini have characteristics\nthat are more similar than either of them have with GPT4.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.09171v1"
    },
    {
        "title": "Maximum lilkelihood estimation in the $β$-model",
        "authors": [
            "Alessandro Rinaldo",
            "Sonja Petrović",
            "Stephen E. Fienberg"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  We study maximum likelihood estimation for the statistical model for\nundirected random graphs, known as the $\\beta$-model, in which the degree\nsequences are minimal sufficient statistics. We derive necessary and sufficient\nconditions, based on the polytope of degree sequences, for the existence of the\nmaximum likelihood estimator (MLE) of the model parameters. We characterize in\na combinatorial fashion sample points leading to a nonexistent MLE, and\nnonestimability of the probability parameters under a nonexistent MLE. We\nformulate conditions that guarantee that the MLE exists with probability\ntending to one as the number of nodes increases.\n",
        "pdf_link": "http://arxiv.org/pdf/1105.6145v4"
    },
    {
        "title": "Cooperative spectrum sensing over unreliable reporting channel",
        "authors": [
            "Amanda de Paula",
            "Cristiano Panazio"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  This article aims to analyze a cooperative spectrum sensing scheme using a\ncentralized approach with unreliable reporting channel. The spectrum sensing is\napplied to a cognitive radio system, where each cognitive radio performs a\nsimple energy detection and send the decision to a fusion center through a\nreporting channel. When the decisions are available at the fusion center, a\nn-out-of-K rule is applied. The impact of the choice of the parameter n in the\ncognitive radio system performance is analyzed in the case where the reporting\nchannel introduces errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.3940v1"
    },
    {
        "title": "Stable mixed graphs",
        "authors": [
            "Kayvan Sadeghi"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  In this paper, we study classes of graphs with three types of edges that\ncapture the modified independence structure of a directed acyclic graph (DAG)\nafter marginalisation over unobserved variables and conditioning on selection\nvariables using the $m$-separation criterion. These include MC, summary, and\nancestral graphs. As a modification of MC graphs, we define the class of\nribbonless graphs (RGs) that permits the use of the $m$-separation criterion.\nRGs contain summary and ancestral graphs as subclasses, and each RG can be\ngenerated by a DAG after marginalisation and conditioning. We derive simple\nalgorithms to generate RGs, from given DAGs or RGs, and also to generate\nsummary and ancestral graphs in a simple way by further extension of the\nRG-generating algorithm. This enables us to develop a parallel theory on these\nthree classes and to study the relationships between them as well as the use of\neach class.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.4168v3"
    },
    {
        "title": "Markov Equivalences for Subclasses of Loopless Mixed Graphs",
        "authors": [
            "Kayvan Sadeghi"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  In this paper we discuss four problems regarding Markov equivalences for\nsubclasses of loopless mixed graphs. We classify these four problems as finding\nconditions for internal Markov equivalence, which is Markov equivalence within\na subclass, for external Markov equivalence, which is Markov equivalence\nbetween subclasses, for representational Markov equivalence, which is the\npossibility of a graph from a subclass being Markov equivalent to a graph from\nanother subclass, and finding algorithms to generate a graph from a certain\nsubclass that is Markov equivalent to a given graph. We particularly focus on\nthe class of maximal ancestral graphs and its subclasses, namely regression\ngraphs, bidirected graphs, undirected graphs, and directed acyclic graphs, and\npresent novel results for representational Markov equivalence and algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.4539v1"
    },
    {
        "title": "Uncertainties and Ambiguities in Percentiles and how to Avoid Them",
        "authors": [
            "Michael Schreiber"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  The recently proposed fractional scoring scheme is used to attribute\npublications to percentile rank classes. It is shown that in this way\nuncertainties and ambiguities in the evaluation of percentile ranks do not\noccur. Using the fractional scoring the total score of all papers exactly\nreproduces the theoretical value.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.3588v1"
    },
    {
        "title": "In praise of the referee",
        "authors": [
            "Nicolas Chopin",
            "Andrew Gelman",
            "Kerrie L. Mengersen",
            "Christian P. Robert"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  There has been a lively debate in many fields, including statistics and\nrelated applied fields such as psychology and biomedical research, on possible\nreforms of the scholarly publishing system. Currently, referees contribute so\nmuch to improve scientific papers, both directly through constructive criticism\nand indirectly through the threat of rejection. We discuss ways in which new\napproaches to journal publication could continue to make use of the valuable\nefforts of peer reviewers.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.4304v1"
    },
    {
        "title": "Networks with time structure from time series",
        "authors": [
            "Tomomichi Nakamura",
            "Toshihiro Tanizawa"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  We propose a method of constructing a network, in which its time structure is\ndirectly incorporated, based on a deterministic model from a time series. To\nconstruct such a network, we transform a linear model containing terms with\ndifferent time delays into network topology. The terms in the model are\ntranslated into temporal nodes of the network. On each link connecting these\nnodes, we assign a positive real number representing the strength of\nrelationship, or the \"distance,\" between nodes specified by the parameters of\nthe model. The method is demonstrated by a known system and applied to two\nactual time series.\n",
        "pdf_link": "http://arxiv.org/pdf/1205.4811v1"
    },
    {
        "title": "Propagation of initial errors on the parameters for linear and Gaussian\n  state space models",
        "authors": [
            "Salima El Kolei"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  For linear and Gaussian state space models parametrized by $\\theta_0 \\in\n\\Theta \\subset \\mathbb{R}^r, r \\geq 1$ corresponding to the vector of\nparameters of the model, the Kalman filter gives exactly the solution for the\noptimal filtering under weak assumptions. This result supposes that $\\theta_0$\nis perfectly known. In most real applications, this assumption is not realistic\nsince $\\theta_0$ is unknown and has to be estimated. In this paper, we analysis\nthe Kalman filter for a biased estimator of $\\theta_0$. We show the propagation\nof this bias on the estimation of the hidden state. We give an expression of\nthis propagation for linear and Gaussian state space models and we extend this\nresult for almost linear models estimated by the Extended Kalman filter. An\nillustration is given for the autoregressive process with measurement noises\nwidely studied in econometrics to model economic and financial data.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.3518v1"
    },
    {
        "title": "A Data Science Course for Undergraduates: Thinking with Data",
        "authors": [
            "Ben Baumer"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Data science is an emerging interdisciplinary field that combines elements of\nmathematics, statistics, computer science, and knowledge in a particular\napplication domain for the purpose of extracting meaningful information from\nthe increasingly sophisticated array of data available in many settings. These\ndata tend to be non-traditional, in the sense that they are often live, large,\ncomplex, and/or messy. A first course in statistics at the undergraduate level\ntypically introduces students with a variety of techniques to analyze small,\nneat, and clean data sets. However, whether they pursue more formal training in\nstatistics or not, many of these students will end up working with data that is\nconsiderably more complex, and will need facility with statistical computing\ntechniques. More importantly, these students require a framework for thinking\nstructurally about data. We describe an undergraduate course in a liberal arts\nenvironment that provides students with the tools necessary to apply data\nscience. The course emphasizes modern, practical, and useful skills that cover\nthe full data analysis spectrum, from asking an interesting question to\nacquiring, managing, manipulating, processing, querying, analyzing, and\nvisualizing data, as well communicating findings in written, graphical, and\noral forms.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.05570v1"
    },
    {
        "title": "Arbitrariness of peer review: A Bayesian analysis of the NIPS experiment",
        "authors": [
            "Olivier Francois"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  The principle of peer review is central to the evaluation of research, by\nensuring that only high-quality items are funded or published. But peer review\nhas also received criticism, as the selection of reviewers may introduce biases\nin the system. In 2014, the organizers of the ``Neural Information Processing\nSystems\\rq\\rq{} conference conducted an experiment in which $10\\%$ of submitted\nmanuscripts (166 items) went through the review process twice. Arbitrariness\nwas measured as the conditional probability for an accepted submission to get\nrejected if examined by the second committee. This number was equal to $60\\%$,\nfor a total acceptance rate equal to $22.5\\%$. Here we present a Bayesian\nanalysis of those two numbers, by introducing a hidden parameter which measures\nthe probability that a submission meets basic quality criteria. The standard\nquality criteria usually include novelty, clarity, reproducibility, correctness\nand no form of misconduct, and are met by a large proportions of submitted\nitems. The Bayesian estimate for the hidden parameter was equal to $56\\%$\n($95\\%$CI: $ I = (0.34, 0.83)$), and had a clear interpretation. The result\nsuggested the total acceptance rate should be increased in order to decrease\narbitrariness estimates in future review processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1507.06411v1"
    },
    {
        "title": "Teaching Data Science",
        "authors": [
            "Robert J. Brunner",
            "Edward J. Kim"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  We describe an introductory data science course, entitled Introduction to\nData Science, offered at the University of Illinois at Urbana-Champaign. The\ncourse introduced general programming concepts by using the Python programming\nlanguage with an emphasis on data preparation, processing, and presentation.\nThe course had no prerequisites, and students were not expected to have any\nprogramming experience. This introductory course was designed to cover a wide\nrange of topics, from the nature of data, to storage, to visualization, to\nprobability and statistical analysis, to cloud and high performance computing,\nwithout becoming overly focused on any one subject. We conclude this article\nwith a discussion of lessons learned and our plans to develop new data science\ncourses.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.07397v1"
    },
    {
        "title": "Progress on a Conjecture Regarding the Triangular Distribution",
        "authors": [
            "Hien D Nguyen",
            "Geoffrey J McLachlan"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Triangular distributions are a well-known class of distributions that are\noften used as an elementary example of a probability model. Maximum likelihood\nestimation of the mode parameter of the triangular distribution over the unit\ninterval can be performed via an order statistics-based method. It had been\nconjectured that such a method can be conducted using only a constant number of\nlikelihood function evaluations, on average, as the sample size becomes large.\nWe prove two theorems that validate this conjecture. Graphical and numerical\nresults are presented to supplement our proofs.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04807v2"
    },
    {
        "title": "Putting Down Roots: A Graphical Exploration of Community Attachment",
        "authors": [
            "Andee Kaplan",
            "Eric Hare"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  In this paper, we explore the relationships that individuals have with their\ncommunities. This work was prepared as part of the ASA Data Expo '13 sponsored\nby the Graphics Section and the Computing Section, using data provided by the\nKnight Foundation Soul of the Community survey. The Knight Foundation in\ncooperation with Gallup surveyed 43,000 people over three years in 26\ncommunities across the United States with the intention of understanding the\nassociation between community attributes and the degree of attachment people\nfeel towards their community. These include the different facets of both urban\nand rural communities, the impact of quality education, and the trend in the\nperceived economic conditions of a community over time. The goal of our work is\nto facilitate understanding of why people feel attachment to their communities\nthrough the use of an interactive and web-based visualization. We will explain\nthe development and use of web-based interactive graphics, including an\noverview of the R package Shiny and the JavaScript library D3, focusing on the\nchoices made in producing the visualizations and technical aspects of how they\nwere created. Then we describe the stories about community attachment that\nunfolded from our analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04843v1"
    },
    {
        "title": "Do Reichenbachian Common Cause Systems of Arbitrary Finite Size Exist?",
        "authors": [
            "Claudio Mazzola",
            "Peter Evans"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  The principle of common cause asserts that positive correlations between\ncausally unrelated events ought to be explained through the action of some\nshared causal factors. Reichenbachian common cause systems are probabilistic\nstructures aimed at accounting for cases where correlations of the aforesaid\nsort cannot be explained through the action of a single common cause. The\nexistence of Reichenbachian common cause systems of arbitrary finite size for\neach pair of non-causally correlated events was allegedly demonstrated by\nHofer-Szab\\'o and R\\'edei in 2006. This paper shows that their proof is\nlogically deficient, and we propose an improved proof.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.00352v1"
    },
    {
        "title": "Rethinking probabilistic prediction in the wake of the 2016 U.S.\n  presidential election",
        "authors": [
            "Harry Crane",
            "Ryan Martin"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  To many statisticians and citizens, the outcome of the most recent U.S.\npresidential election represents a failure of data-driven methods on the\ngrandest scale. This impression has led to much debate and discussion about how\nthe election predictions went awry -- Were the polls inaccurate? Were the\nmodels wrong? Did we misinterpret the probabilities? -- and how they went right\n-- Perhaps the analyses were correct even though the predictions were wrong,\nthat's just the nature of probabilistic forecasting. With this in mind, we\nanalyze the election outcome with respect to a core set of effectiveness\nprinciples. Regardless of whether and how the election predictions were right\nor wrong, we argue that they were ineffective in conveying the extent to which\nthe data was informative of the outcome and the level of uncertainty in making\nthese assessments. Among other things, our analysis sheds light on the\nshortcomings of the classical interpretations of probability and its\ncommunication to consumers in the form of predictions. We present here an\nalternative approach, based on a notion of validity, which offers two immediate\ninsights for predictive inference. First, the predictions are more\nconservative, arguably more realistic, and come with certain guarantees on the\nprobability of an erroneous prediction. Second, our approach easily and\nnaturally reflects the (possibly substantial) uncertainty about the model by\noutputting plausibilities instead of probabilities. Had these simple steps been\ntaken by the popular prediction outlets, the election outcome may not have been\nso shocking.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01171v1"
    },
    {
        "title": "Stem-ming the Tide: Predicting STEM attrition using student transcript\n  data",
        "authors": [
            "Lovenoor Aulck",
            "Rohan Aras",
            "Lysia Li",
            "Coulter L'Heureux",
            "Peter Lu",
            "Jevin West"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Science, technology, engineering, and math (STEM) fields play growing roles\nin national and international economies by driving innovation and generating\nhigh salary jobs. Yet, the US is lagging behind other highly industrialized\nnations in terms of STEM education and training. Furthermore, many economic\nforecasts predict a rising shortage of domestic STEM-trained professions in the\nUS for years to come. One potential solution to this deficit is to decrease the\nrates at which students leave STEM-related fields in higher education, as\ncurrently over half of all students intending to graduate with a STEM degree\neventually attrite. However, little quantitative research at scale has looked\nat causes of STEM attrition, let alone the use of machine learning to examine\nhow well this phenomenon can be predicted. In this paper, we detail our efforts\nto model and predict dropout from STEM fields using one of the largest known\ndatasets used for research on students at a traditional campus setting. Our\nresults suggest that attrition from STEM fields can be accurately predicted\nwith data that is routinely collected at universities using only information on\nstudents' first academic year. We also propose a method to model student STEM\nintentions for each academic term to better understand the timing of STEM\nattrition events. We believe these results show great promise in using machine\nlearning to improve STEM retention in traditional and non-traditional campus\nsettings.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.09344v1"
    },
    {
        "title": "Implementation of the Bin Hierarchy Method for restoring a smooth\n  function from a sampled histogram",
        "authors": [
            "Olga Goulko",
            "Alexander Gaenko",
            "Emanuel Gull",
            "Nikolay Prokof'ev",
            "Boris Svistunov"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  We present $\\texttt{BHM}$, a tool for restoring a smooth function from a\nsampled histogram using the bin hierarchy method. The theoretical background of\nthe method is presented in [arXiv:1707.07625]. The code automatically generates\na smooth polynomial spline with the minimal acceptable number of knots from the\ninput data. It works universally for any sufficiently regular shaped\ndistribution and any level of data quality, requiring almost no external\nparameter specification. It is particularly useful for large-scale numerical\ndata analysis. This paper explains the details of the implementation and the\nuse of the program.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.04316v1"
    },
    {
        "title": "An innovating Statistical Learning Tool based on Partial Differential\n  Equations, intending livestock Data Assimilation",
        "authors": [
            "Hélène Flourent",
            "Emmanuel Frénod",
            "Vincent Sincholle"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The realistic modeling intended to quantify precisely some biological\nmechanisms is a task requiering a lot of a priori knowledge and generally\nleading to heavy mathematical models. On the other hand, the structure of the\nclassical Machine Learning algorithms, such as Neural Networks, limits their\nflexibility and the possibility to take into account the existence of complex\nunderlying phenomena, such as delay, saturation and accumulation. The aim of\nthis paper is to reach a compromise between precision, parsimony and\nflexibility to design an efficient biomimetic predictive tool extracting\nknowledge from livestock data. To achieve this, we build a Mathematical Model\nbased on Partial Differential Equations (PDE) embarking the mathematical\nexpression of biological determinants. We made the hypothesis that all the\nphysico-chemical phenomena occurring in animal body can be summarized by the\nevolution of a global information. Therefore the developed PDE system describes\nthe evolution and the action of an information circulating in an Avatar of the\nReal Animal. This Avatar outlines the dynamics of the biological reactions of\nanimal body in the framework of a specific problem. Each PDE contains\nparameters corresponding to biological-like factors which can be learnt from\ndata by the developed Statistical Learning Tool.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.12423v3"
    },
    {
        "title": "Restoring a smooth function from its noisy integrals",
        "authors": [
            "Olga Goulko",
            "Nikolay Prokof'ev",
            "Boris Svistunov"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Numerical (and experimental) data analysis often requires the restoration of\na smooth function from a set of sampled integrals over finite bins. We present\nthe bin hierarchy method that efficiently computes the maximally smooth\nfunction from the sampled integrals using essentially all the information\ncontained in the data. We perform extensive tests with different classes of\nfunctions and levels of data quality, including Monte Carlo data suffering from\na severe sign problem and physical data for the Green's function of the\nFr\\\"ohlich polaron.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07625v2"
    },
    {
        "title": "A Fourier-invariant method for locating point-masses and computing their\n  attributes",
        "authors": [
            "Charles K. Chui",
            "Hrushikesh N. Mhaskar"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Motivated by the interest of observing the growth of cancer cells among\nnormal living cells and exploring how galaxies and stars are truly formed, the\nobjective of this paper is to introduce a rigorous and effective method for\ncounting point-masses, determining their spatial locations, and computing their\nattributes. Based on computation of Hermite moments that are Fourier-invariant,\nour approach facilitates the processing of both spatial and Fourier data in any\ndimension.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09319v1"
    },
    {
        "title": "Inverse Ising techniques to infer underlying mechanisms from data",
        "authors": [
            "Hong-Li Zeng",
            "Erik Aurell"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  As a problem in data science the inverse Ising (or Potts) problem is to infer\nthe parameters of a Gibbs-Boltzmann distributions of an Ising (or Potts) model\nfrom samples drawn from that distribution. The algorithmic and computational\ninterest stems from the fact that this inference task cannot be done\nefficiently by the maximum likelihood criterion, since the normalizing constant\nof the distribution (the partition function) can not be calculated exactly and\nefficiently. The practical interest on the other hand flows from several\noutstanding applications, of which the most well known has been predicting\nspatial contacts in protein structures from tables of homologous protein\nsequences. Most applications to date have been to data that has been produced\nby a dynamical process which, as far as it is known, cannot be expected to\nsatisfy detailed balance. There is therefore no a priori reason to expect the\ndistribution to be of the Gibbs-Boltzmann type, and no a priori reason to\nexpect that inverse Ising (or Potts) techniques should yield useful\ninformation. In this review we discuss two types of problems where progress\nnevertheless can be made. We find that depending on model parameters there are\nphases where, in fact, the distribution is close to Gibbs-Boltzmann\ndistribution, a non-equilibrium nature of the under-lying dynamics\nnotwithstanding. We also discuss the relation between inferred Ising model\nparameters and parameters of the underlying dynamics.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.05222v1"
    },
    {
        "title": "Connections between statistical practice in elementary particle physics\n  and the severity concept as discussed in Mayo's Statistical Inference as\n  Severe Testing",
        "authors": [
            "Robert D. Cousins"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  For many years, philosopher-of-statistics Deborah Mayo has been advocating\nthe concept of severe testing as a key part of hypothesis testing. Her recent\nbook, Statistical Inference as Severe Testing, is a comprehensive exposition of\nher arguments in the context of a historical study of many threads of\nstatistical inference, both frequentist and Bayesian. Her foundational point of\nview is called error statistics, emphasizing frequentist evaluation of the\nerrors called Type I and Type II in the Neyman-Pearson theory of frequentist\nhypothesis testing. Since the field of elementary particle physics (also known\nas high energy physics) has strong traditions in frequentist inference, one\nmight expect that something like the severity concept was independently\ndeveloped in the field. Indeed, I find that, at least operationally\n(numerically), we high-energy physicists have long interpreted data in ways\nthat map directly onto severity. Whether or not we subscribe to Mayo's\nphilosophical interpretations of severity is a more complicated story that I do\nnot address here.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.09713v2"
    },
    {
        "title": "Solution for the Indefinite Integral of the Standard Normal Probability\n  Density Function",
        "authors": [
            "Joram Soch"
        ],
        "category": "stat.OT",
        "published_year": "2015",
        "summary": "  Conventional wisdom assumes that the indefinite integral of the probability\ndensity function for the standard normal distribution cannot be expressed in\nfinite elementary terms. While this is true, there is an expression for this\nanti-derivative in infinite elementary terms that, when being differentiated,\ndirectly yields the standard normal density function. We derive this function\nusing infinite partial integration and review its relation to the cumulative\ndistribution function for the standard normal distribution and the error\nfunction.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.04858v3"
    },
    {
        "title": "SPEW: Synthetic Populations and Ecosystems of the World",
        "authors": [
            "Shannon Gallagher",
            "Lee Richardson",
            "Samuel L. Ventura",
            "William F. Eddy"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  Agent-based models (ABMs) simulate interactions between autonomous agents in\nconstrained environments over time. ABMs are often used for modeling the spread\nof infectious diseases. In order to simulate disease outbreaks or other\nphenomena, ABMs rely on \"synthetic ecosystems,\" or information about agents and\ntheir environments that is representative of the real world. Previous\napproaches for generating synthetic ecosystems have some limitations: they are\nnot open-source, cannot be adapted to new or updated input data sources, and do\nnot allow for alternative methods for sampling agent characteristics and\nlocations. We introduce a general framework for generating Synthetic\nPopulations and Ecosystems of the World (SPEW), implemented as an open-source R\npackage. SPEW allows researchers to choose from a variety of sampling methods\nfor agent characteristics and locations when generating synthetic ecosystems\nfor any geographic region. SPEW can produce synthetic ecosystems for any agent\n(e.g. humans, mosquitoes, etc), provided that appropriate data is available. We\nanalyze the accuracy and computational efficiency of SPEW given different\nsampling methods for agent characteristics and locations and provide a suite of\ndiagnostics to screen our synthetic ecosystems. SPEW has generated over five\nbillion human agents across approximately 100,000 geographic regions in about\n70 countries, available online.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.02383v1"
    },
    {
        "title": "A geometer's view of the the Cramér-Rao bound on estimator variance",
        "authors": [
            "Anthony D. Blaom"
        ],
        "category": "stat.OT",
        "published_year": "2017",
        "summary": "  The classical Cram\\'er-Rao inequality gives a lower bound for the variance of\na unbiased estimator of an unknown parameter, in some statistical model of a\nrandom process. In this note we rewrite the statment and proof of the bound\nusing contemporary geometric language.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.01598v2"
    },
    {
        "title": "On approximations via convolution-defined mixture models",
        "authors": [
            "Hien D. Nguyen",
            "Geoffrey J. McLachlan"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  An often-cited fact regarding mixing or mixture distributions is that their\ndensity functions are able to approximate the density function of any unknown\ndistribution to arbitrary degrees of accuracy, provided that the mixing or\nmixture distribution is sufficiently complex. This fact is often not made\nconcrete. We investigate and review theorems that provide approximation bounds\nfor mixing distributions. Connections between the approximation bounds of\nmixing distributions and estimation bounds for the maximum likelihood estimator\nof finite mixtures of location- scale distributions are reviewed.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.03974v3"
    },
    {
        "title": "First digit law from Laplace transform",
        "authors": [
            "Mingshu Cong",
            "Congqiao Li",
            "Bo-Qiang Ma"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The occurrence of digits 1 through 9 as the leftmost nonzero digit of numbers\nfrom real-world sources is distributed unevenly according to an empirical law,\nknown as Benford's law or the first digit law. It remains obscure why a variety\nof data sets generated from quite different dynamics obey this particular law.\nWe perform a study of Benford's law from the application of the Laplace\ntransform, and find that the logarithmic Laplace spectrum of the digital\nindicator function can be approximately taken as a constant. This particular\nconstant, being exactly the Benford term, explains the prevalence of Benford's\nlaw. The slight variation from the Benford term leads to deviations from\nBenford's law for distributions which oscillate violently in the inverse\nLaplace space. We prove that the whole family of completely monotonic\ndistributions can satisfy Benford's law within a small bound. Our study\nsuggests that Benford's law originates from the way that we write numbers, thus\nshould be taken as a basic mathematical knowledge.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.00352v1"
    },
    {
        "title": "An efficient surrogate-aided importance sampling framework for\n  reliability analysis",
        "authors": [
            "Wang-Sheng Liu",
            "Sai Hung Cheung",
            "Wen-Jun Cao"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Surrogates in lieu of expensive-to-evaluate performance functions can\naccelerate the reliability analysis greatly. This paper proposes a new\ntwo-stage framework for surrogate-aided reliability analysis named Surrogates\nfor Importance Sampling (S4IS). In the first stage, a coarse surrogate is built\nto gain the information about failure regions; the second stage zooms into the\nimportant regions and improves the accuracy of the failure probability\nestimator by adaptively selecting support points therein. The learning\nfunctions are proposed to guide the selection of support points such that the\nexploration and exploitation can be dynamically balanced. As a generic\nframework, S4IS has the potential to incorporate different types of surrogates\n(Gaussian Processes, Support Vector Machines, Neural Network, etc.). The\neffectiveness and efficiency of S4IS is validated by five illustrative\nexamples, which involve system reliability, highly nonlinear limit-state\nfunction, small failure probability and moderately high dimensionality. The\nimplementation of S4IS is made available to download at\nhttps://github.com/RobinSeaside/S4IS.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10489v1"
    },
    {
        "title": "Markov properties for mixed graphs",
        "authors": [
            "Kayvan Sadeghi",
            "Steffen Lauritzen"
        ],
        "category": "stat.OT",
        "published_year": "2011",
        "summary": "  In this paper, we unify the Markov theory of a variety of different types of\ngraphs used in graphical Markov models by introducing the class of loopless\nmixed graphs, and show that all independence models induced by $m$-separation\non such graphs are compositional graphoids. We focus in particular on the\nsubclass of ribbonless graphs which as special cases include undirected graphs,\nbidirected graphs, and directed acyclic graphs, as well as ancestral graphs and\nsummary graphs. We define maximality of such graphs as well as a pairwise and a\nglobal Markov property. We prove that the global and pairwise Markov properties\nof a maximal ribbonless graph are equivalent for any independence model that is\na compositional graphoid.\n",
        "pdf_link": "http://arxiv.org/pdf/1109.5909v5"
    },
    {
        "title": "Cramer-Rao-Induced Bounds for CANDECOMP/PARAFAC tensor decomposition",
        "authors": [
            "Petr Tichavsky",
            "Anh Huy Phan",
            "Zbynek Koldovsky"
        ],
        "category": "stat.OT",
        "published_year": "2012",
        "summary": "  This paper presents a Cramer-Rao lower bound (CRLB) on the variance of\nunbiased estimates of factor matrices in Canonical Polyadic (CP) or\nCANDECOMP/PARAFAC (CP) decompositions of a tensor from noisy observations,\n(i.e., the tensor plus a random Gaussian i.i.d. tensor). A novel expression is\nderived for a bound on the mean square angular error of factors along a\nselected dimension of a tensor of an arbitrary dimension. The expression needs\nless operations for computing the bound, O(NR^6), than the best existing\nstate-of-the art algorithm, O(N^3R^6) operations, where N and R are the tensor\norder and the tensor rank. Insightful expressions are derived for tensors of\nrank 1 and rank 2 of arbitrary dimension and for tensors of arbitrary dimension\nand rank, where two factor matrices have orthogonal columns.\n  The results can be used as a gauge of performance of different approximate CP\ndecomposition algorithms, prediction of their accuracy, and for checking\nstability of a given decomposition of a tensor (condition whether the CRLB is\nfinite or not). A novel expression is derived for a Hessian matrix needed in\npopular damped Gauss-Newton method for solving the CP decomposition of tensors\nwith missing elements. Beside computing the CRLB for these tensors the\nexpression may serve for design of damped Gauss-Newton algorithm for the\ndecomposition.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.3215v2"
    },
    {
        "title": "Distributed Hypothesis Testing over a Noisy Channel: Error-exponents\n  Trade-off",
        "authors": [
            "Sreejith Sreekumar",
            "Deniz Gündüz"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  A two-terminal distributed binary hypothesis testing problem over a noisy\nchannel is studied. The two terminals, called the observer and the decision\nmaker, each has access to $n$ independent and identically distributed samples,\ndenoted by $\\mathbf{U}$ and $\\mathbf{V}$, respectively. The observer\ncommunicates to the decision maker over a discrete memoryless channel, and the\ndecision maker performs a binary hypothesis test on the joint probability\ndistribution of $(\\mathbf{U},\\mathbf{V})$ based on $\\mathbf{V}$ and the noisy\ninformation received from the observer. The trade-off between the exponents of\nthe type I and type II error probabilities is investigated. Two inner bounds\nare obtained, one using a separation-based scheme that involves type-based\ncompression and unequal error-protection channel coding, and the other using a\njoint scheme that incorporates type-based hybrid coding. The separation-based\nscheme is shown to recover the inner bound obtained by Han and Kobayashi for\nthe special case of a rate-limited noiseless channel, and also the one obtained\nby the authors previously for a corner point of the trade-off. Finally, we show\nvia an example that the joint scheme achieves a strictly tighter bound than the\nseparation-based scheme for some points of the error-exponents trade-off.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.07521v5"
    },
    {
        "title": "A Conceptual Introduction to Markov Chain Monte Carlo Methods",
        "authors": [
            "Joshua S. Speagle"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Markov Chain Monte Carlo (MCMC) methods have become a cornerstone of many\nmodern scientific analyses by providing a straightforward approach to\nnumerically estimate uncertainties in the parameters of a model using a\nsequence of random samples. This article provides a basic introduction to MCMC\nmethods by establishing a strong conceptual understanding of what problems MCMC\nmethods are trying to solve, why we want to use them, and how they work in\ntheory and in practice. To develop these concepts, I outline the foundations of\nBayesian inference, discuss how posterior distributions are used in practice,\nexplore basic approaches to estimate posterior-based quantities, and derive\ntheir link to Monte Carlo sampling and MCMC. Using a simple toy problem, I then\ndemonstrate how these concepts can be used to understand the benefits and\ndrawbacks of various MCMC approaches. Exercises designed to highlight various\nconcepts are also included throughout the article.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.12313v2"
    },
    {
        "title": "Machine Learning for high speed channel optimization",
        "authors": [
            "Jiayi He",
            "Aravind Sampath Kumar",
            "Arun Chada",
            "Bhyrav Mutnury",
            "James Drewniak"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Design of printed circuit board (PCB) stack-up requires the consideration of\ncharacteristic impedance, insertion loss and crosstalk. As there are many\nparameters in a PCB stack-up design, the optimization of these parameters needs\nto be efficient and accurate. A less optimal stack-up would lead to expensive\nPCB material choices in high speed designs. In this paper, an efficient global\noptimization method using parallel and intelligent Bayesian optimization is\nproposed for the stripline design.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.04317v1"
    },
    {
        "title": "Online detection of cascading change-points",
        "authors": [
            "Rui Zhang",
            "Yao Xie",
            "Rui Yao",
            "Feng Qiu"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  We propose an online detection procedure for cascading failures in the\nnetwork from sequential data, which can be modeled as multiple correlated\nchange-points happening during a short period. We consider a temporal diffusion\nnetwork model to capture the temporal dynamic structure of multiple\nchange-points and develop a sequential Shewhart procedure based on the\ngeneralized likelihood ratio statistics based on the diffusion network model\nassuming unknown post-change distribution parameters. We also tackle the\ncomputational complexity posed by the unknown propagation. Numerical\nexperiments demonstrate the good performance for detecting cascade failures.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.05610v4"
    },
    {
        "title": "New plans orthogonal through the block factor",
        "authors": [
            "Sunanda Bagchi"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  In the present paper we construct plans orthogonal through the block factor\n(POTBs). We describe procedures for adding blocks as well as factors to an\ninitial plan and thus generate a bigger plan. Using these procedures we\nconstruct POTBs for symmetrical experiments with factors having three or more\nlevels. We also construct a series of plans inter-class orthogonal through the\nblock factor for two-level factors.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.12891v1"
    },
    {
        "title": "A Note on Taylor's Expansion and Mean Value Theorem With Respect to a\n  Random Variable",
        "authors": [
            "Yifan Yang",
            "Xiaoyu Zhou"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  We introduce a stochastic version of Taylor's expansion and Mean Value\nTheorem, originally proved by Aliprantis and Border (1999), and extend them to\na multivariate case. For a univariate case, the theorem asserts that \"suppose a\nreal-valued function $f$ has a continuous derivative $f'$ on a closed interval\n$I$ and $X$ is a random variable on a probability space $(\\Omega, \\mathcal{F},\nP)$. Fix $a \\in I$, there exists a \\textit{random variable} $\\xi$ such that\n$\\xi(\\omega) \\in I$ for every $\\omega \\in \\Omega$ and $f(X(\\omega)) = f(a) +\nf'(\\xi(\\omega))(X(\\omega) - a)$.\" The proof is not trivial. By applying these\nresults in statistics, one may simplify some details in the proofs of the Delta\nmethod or the asymptotic properties for a maximum likelihood estimator. In\nparticular, when mentioning \"there exists $\\theta ^ *$ between $\\hat{\\theta}$\n(a maximum likelihood estimator) and $\\theta_0$ (the true value)\", a stochastic\nversion of Mean Value Theorem guarantees $\\theta ^ *$ is a random variable (or\na random vector).\n",
        "pdf_link": "http://arxiv.org/pdf/2102.10429v1"
    },
    {
        "title": "Divergence vs. Decision P-values: A Distinction Worth Making in Theory\n  and Keeping in Practice",
        "authors": [
            "Sander Greenland"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  There are two distinct definitions of 'P-value' for evaluating a proposed\nhypothesis or model for the process generating an observed dataset. The\noriginal definition starts with a measure of the divergence of the dataset from\nwhat was expected under the model, such as a sum of squares or a deviance\nstatistic. A P-value is then the ordinal location of the measure in a reference\ndistribution computed from the model and the data, and is treated as a\nunit-scaled index of compatibility between the data and the model. In the other\ndefinition, a P-value is a random variable on the unit interval whose\nrealizations can be compared to a cutoff alpha to generate a decision rule with\nknown error rates under the model and specific alternatives. It is commonly\nassumed that realizations of such decision P-values always correspond to\ndivergence P-values. But this need not be so: Decision P-values can violate\nintuitive single-sample coherence criteria where divergence P-values do not. It\nis thus argued that divergence and decision P-values should be carefully\ndistinguished in teaching, and that divergence P-values are the relevant choice\nwhen the analysis goal is to summarize evidence rather than implement a\ndecision rule.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.02478v3"
    },
    {
        "title": "The phenotypic expansion and its boundaries",
        "authors": [
            "Geoffroy Berthelot"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  The development of sport performances in the future is a subject of myth and\ndisagreement among experts. As arguments favoring and opposing such methodology\nwere discussed, other publications empirically showed that the past development\nof performances followed a non linear trend. Other works, while deeply\nexploring the conditions leading to world records, highlighted that performance\nis tied to the economical and geopolitical context. Here we investigated the\nfollowing human boundaries: development of performances with time in Olympic\nand non-Olympic events, development of sport performances with aging among\nhumans and others species (greyhounds, thoroughbreds, mice). Development of\nperformances from a broader point of view (demography & lifespan) in a specific\nsub-system centered on primary energy was also investigated. We show that the\nphysiological developments are limited with time. Three major and direct\ndeterminants of sport performance are age, technology and climatic conditions\n(temperature). However, all observed developments are related to the\ninternational context including the efficient use of primary energies. This\nlast parameter is a major indirect propeller of performance development. We\nshow that when physiological and societal performance indicators such as\nlifespan and population density depend on primary energies, the energy source,\ncompetition and mobility are key parameters for achieving long term sustainable\ntrajectories. Otherwise, the vast majority (98.7%) of the studied trajectories\nreaches 0 before 15 generations, due to the consumption of fossil energy and a\nlow mobility rate. This led us to consider that in the present turbulent\neconomical context and given the upcoming energy crisis, societal and physical\nperformances are not expected to grow continuously.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.7474v1"
    },
    {
        "title": "Statistique et Big Data Analytics; Volumétrie, L'Attaque des Clones",
        "authors": [
            "Philippe Besse",
            "Nathalie Villa-Vialaneix"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  This article assumes acquired the skills and expertise of a statistician in\nunsupervised (NMF, k-means, SVD) and supervised learning (regression, CART,\nrandom forest). What skills and knowledge do a statistician must acquire to\nreach the \"Volume\" scale of big data? After a quick overview of the different\nstrategies available and especially of those imposed by Hadoop, the algorithms\nof some available learning methods are outlined in order to understand how they\nare adapted to the strong stresses of the Map-Reduce functionalities\n",
        "pdf_link": "http://arxiv.org/pdf/1405.6676v2"
    },
    {
        "title": "Marginalization and Conditioning for LWF Chain Graphs",
        "authors": [
            "Kayvan Sadeghi"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  In this paper, we deal with the problem of marginalization over and\nconditioning on two disjoint subsets of the node set of chain graphs (CGs) with\nthe LWF Markov property. For this purpose, we define the class of chain mixed\ngraphs (CMGs) with three types of edges and, for this class, provide a\nseparation criterion under which the class of CMGs is stable under\nmarginalization and conditioning and contains the class of LWF CGs as its\nsubclass. We provide a method for generating such graphs after marginalization\nand conditioning for a given CMG or a given LWF CG. We then define and study\nthe class of anterial graphs, which is also stable under marginalization and\nconditioning and contains LWF CGs, but has a simpler structure than CMGs.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.7129v4"
    },
    {
        "title": "Fisher, Neyman-Pearson or NHST? A Tutorial for Teaching Data Testing",
        "authors": [
            "Jose D. Perezgonzalez"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  Despite frequent calls for the overhaul of null hypothesis significance\ntesting (NHST), this controversial procedure remains ubiquitous in behavioral,\nsocial and biomedical teaching and research. Little change seems possible once\nthe procedure becomes well ingrained in the minds and current practice of\nresearchers; thus, the optimal opportunity for such change is at the time the\nprocedure is taught, be this at undergraduate or at postgraduate levels. This\npaper presents a tutorial for the teaching of data testing procedures, often\nreferred to as hypothesis testing theories. The first procedure introduced is\nthe approach to data testing followed by Fisher (tests of significance); the\nsecond is the approach followed by Neyman and Pearson (tests of acceptance);\nthe final procedure is the incongruent combination of the previous two theories\ninto the current approach (NSHT). For those researchers sticking with the\nlatter, two compromise solutions on how to improve NHST conclude the tutorial.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.07408v1"
    },
    {
        "title": "An overview and perspective on social network monitoring",
        "authors": [
            "William H. Woodall",
            "Meng J. Zhao",
            "Kamran Paynabar",
            "Ross Sparks",
            "James D. Wilson"
        ],
        "category": "stat.OT",
        "published_year": "2016",
        "summary": "  In this expository paper we give an overview of some statistical methods for\nthe monitoring of social networks. We discuss the advantages and limitations of\nvarious methods as well as some relevant issues. One of our primary\ncontributions is to give the relationships between network monitoring methods\nand monitoring methods in engineering statistics and public health\nsurveillance. We encourage researchers in the industrial process monitoring\narea to work on developing and comparing the performance of social network\nmonitoring methods. We also discuss some of the issues in social network\nmonitoring and give a number of research ideas.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.09453v1"
    },
    {
        "title": "Optimal spectral shrinkage and PCA with heteroscedastic noise",
        "authors": [
            "William Leeb",
            "Elad Romanov"
        ],
        "category": "stat.OT",
        "published_year": "2018",
        "summary": "  This paper studies the related problems of prediction, covariance estimation,\nand principal component analysis for the spiked covariance model with\nheteroscedastic noise. We consider an estimator of the principal components\nbased on whitening the noise, and we derive optimal singular value and\neigenvalue shrinkers for use with these estimated principal components.\nUnderlying these methods are new asymptotic results for the high-dimensional\nspiked model with heteroscedastic noise, and consistent estimators for the\nrelevant population parameters. We extend previous analysis on out-of-sample\nprediction to the setting of predictors with whitening. We demonstrate certain\nadvantages of noise whitening. Specifically, we show that in a certain\nasymptotic regime, optimal singular value shrinkage with whitening converges to\nthe best linear predictor, whereas without whitening it converges to a\nsuboptimal linear predictor. We prove that for generic signals, whitening\nimproves estimation of the principal components, and increases a natural\nsignal-to-noise ratio of the observations. We also show that for rank one\nsignals, our estimated principal components achieve the asymptotic minimax\nrate.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.02201v4"
    },
    {
        "title": "Projective Decomposition and Matrix Equivalence up to Scale",
        "authors": [
            "Max Robinson"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  A data matrix may be seen simply as a means of organizing observations into\nrows ( e.g., by measured object) and into columns ( e.g., by measured variable)\nso that the observations can be analyzed with mathematical tools. As a\nmathematical object, a matrix defines a linear mapping between points\nrepresenting weighted combinations of its rows (the row vector space) and\npoints representing weighted combinations of its columns (the column vector\nspace). From this perspective, a data matrix defines a relationship between the\ninformation that labels its rows and the information that labels its columns,\nand numerical methods are used to analyze this relationship. A first step is to\nnormalize the data, transforming each observation from scales convenient for\nmeasurement to a common scale, on which addition and multiplication can\nmeaningfully combine the different observations. For example, z-transformation\nrescales every variable to the same scale, standardized variation from an\nexpected value, but ignores scale differences between measured objects. Here we\ndevelop the concepts and properties of projective decomposition, which applies\nthe same normalization strategy to both rows and columns by separating the\nmatrix into row- and column-scaling factors and a scale-normalized matrix. We\nshow that different scalings of the same scale-normalized matrix form an\nequivalence class, and call the scale-normalized, canonical member of the class\nits scale-invariant form that preserves all pairwise relative ratios.\nProjective decomposition therefore provides a means of normalizing the broad\nclass of ratio-scale data, in which relative ratios are of primary interest,\nonto a common scale without altering the ratios of interest, and simultaneously\naccounting for scale effects for both organizations of the matrix values. Both\nof these properties distinguish it from z-transformation.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.01336v1"
    },
    {
        "title": "Approaching Ethical Guidelines for Data Scientists",
        "authors": [
            "Ursula Garzcarek",
            "Detlef Steuer"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  The goal of this article is to inspire data scientists to participate in the\ndebate on the impact that their professional work has on society, and to become\nactive in public debates on the digital world as data science professionals.\nHow do ethical principles (e.g., fairness, justice, beneficence, and\nnon-maleficence) relate to our professional lives? What lies in our\nresponsibility as professionals by our expertise in the field? More\nspecifically this article makes an appeal to statisticians to join that debate,\nand to be part of the community that establishes data science as a proper\nprofession in the sense of Airaksinen, a philosopher working on professional\nethics. As we will argue, data science has one of its roots in statistics and\nextends beyond it. To shape the future of statistics, and to take\nresponsibility for the statistical contributions to data science, statisticians\nshould actively engage in the discussions. First the term data science is\ndefined, and the technical changes that have led to a strong influence of data\nscience on society are outlined. Next the systematic approach from CNIL is\nintroduced. Prominent examples are given for ethical issues arising from the\nwork of data scientists. Further we provide reasons why data scientists should\nengage in shaping morality around and to formulate codes of conduct and codes\nof practice for data science. Next we present established ethical guidelines\nfor the related fields of statistics and computing machinery. Thereafter\nnecessary steps in the community to develop professional ethics for data\nscience are described. Finally we give our starting statement for the debate:\nData science is in the focal point of current societal development. Without\nbecoming a profession with professional ethics, data science will fail in\nbuilding trust in its interaction with and its much needed contributions to\nsociety!\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04824v1"
    },
    {
        "title": "Fitting A Mixture Distribution to Data: Tutorial",
        "authors": [
            "Benyamin Ghojogh",
            "Aydin Ghojogh",
            "Mark Crowley",
            "Fakhri Karray"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  This paper is a step-by-step tutorial for fitting a mixture distribution to\ndata. It merely assumes the reader has the background of calculus and linear\nalgebra. Other required background is briefly reviewed before explaining the\nmain algorithm. In explaining the main algorithm, first, fitting a mixture of\ntwo distributions is detailed and examples of fitting two Gaussians and\nPoissons, respectively for continuous and discrete cases, are introduced.\nThereafter, fitting several distributions in general case is explained and\nexamples of several Gaussians (Gaussian Mixture Model) and Poissons are again\nprovided. Model-based clustering, as one of the applications of mixture\ndistributions, is also introduced. Numerical simulations are also provided for\nboth Gaussian and Poisson examples for the sake of better clarification.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.06708v2"
    },
    {
        "title": "Topological Information Data Analysis",
        "authors": [
            "Pierre Baudot",
            "Monica Tapia",
            "Daniel Bennequin",
            "Jean-Marc Goaillard"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  This paper presents methods that quantify the structure of statistical\ninteractions within a given data set, and was first used in \\cite{Tapia2018}.\nIt establishes new results on the k-multivariate mutual-informations (I_k)\ninspired by the topological formulation of Information introduced in. In\nparticular we show that the vanishing of all I_k for 2\\leq k \\leq n of n random\nvariables is equivalent to their statistical independence. Pursuing the work of\nHu Kuo Ting and Te Sun Han, we show that information functions provide\nco-ordinates for binary variables, and that they are analytically independent\non the probability simplex for any set of finite variables. The maximal\npositive I_k identifies the variables that co-vary the most in the population,\nwhereas the minimal negative I_k identifies synergistic clusters and the\nvariables that differentiate-segregate the most the population. Finite data\nsize effects and estimation biases severely constrain the effective computation\nof the information topology on data, and we provide simple statistical tests\nfor the undersampling bias and the k-dependences following. We give an example\nof application of these methods to genetic expression and unsupervised\ncell-type classification. The methods unravel biologically relevant subtypes,\nwith a sample size of 41 genes and with few errors. It establishes generic\nbasic methods to quantify the epigenetic information storage and a unified\nepigenetic unsupervised learning formalism. We propose that higher-order\nstatistical interactions and non identically distributed variables are\nconstitutive characteristics of biological systems that should be estimated in\norder to unravel their significant statistical structure and diversity. The\ntopological information data analysis presented here allows to precisely\nestimate this higher-order structure characteristic of biological systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.04242v1"
    },
    {
        "title": "Incomplete Reparameterizations and Equivalent Metrics",
        "authors": [
            "Michael Betancourt"
        ],
        "category": "stat.OT",
        "published_year": "2019",
        "summary": "  Reparameterizing a probabilisitic system is common advice for improving the\nperformance of a statistical algorithm like Markov chain Monte Carlo, even\nthough in theory such reparameterizations should leave the system, and the\nperformance of any algorithm, invariant. In this paper I show how the\nreparameterizations common in practice are only incomplete reparameterizations\nwhich result in different interactions between a target probabilistic system\nand a given algorithm. I then consider how these changing interactions manifest\nin the context of Markov chain Monte Carlo algorithms defined on Riemannian\nmanifolds. In particular I show how any incomplete reparameterization is\nequivalent to modifying the metric geometry directly.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09407v1"
    },
    {
        "title": "Anna Karenina and The Two Envelopes Problem",
        "authors": [
            "R. D. Gill"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  The Anna Karenina principle is named after the opening sentence in the\neponymous novel: Happy families are all alike; every unhappy family is unhappy\nin its own way. The Two Envelopes Problem (TEP) is a much-studied paradox in\nprobability theory, mathematical economics, logic, and philosophy. Time and\nagain a new analysis is published in which an author claims finally to explain\nwhat actually goes wrong in this paradox. Each author (the present author\nincluded) emphasizes what is new in their approach and concludes that earlier\napproaches did not get to the root of the matter. We observe that though a\nlogical argument is only correct if every step is correct, an apparently\nlogical argument which goes astray can be thought of as going astray at\ndifferent places. This leads to a comparison between the literature on TEP and\na successful movie franchise: it generates a succession of sequels, and even\nprequels, each with a different director who approaches the same basic premise\nin a personal way. We survey resolutions in the literature with a view to\nsynthesis, correct common errors, and give a new theorem on order properties of\nan exchangeable pair of random variables, at the heart of most TEP variants and\ninterpretations. A theorem on asymptotic independence between the amount in\nyour envelope and the question whether it is smaller or larger shows that the\npathological situation of improper priors or infinite expectation values has\nconsequences as we merely approach such a situation.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.04008v5"
    },
    {
        "title": "Determining feature importance for actionable climate change mitigation\n  policies",
        "authors": [
            "Romit Maulik",
            "Junghwa Choi",
            "Wesley Wehde",
            "Prasanna Balaprakash"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Given the importance of public support for policy change and implementation,\npublic policymakers and researchers have attempted to understand the factors\nassociated with this support for climate change mitigation policy. In this\narticle, we compare the feasibility of using different supervised learning\nmethods for regression using a novel socio-economic data set which measures\npublic support for potential climate change mitigation policies. Following this\nmodel selection, we utilize gradient boosting regression, a well-known\ntechnique in the machine learning community, but relatively uncommon in public\npolicy and public opinion research, and seek to understand what factors among\nthe several examined in previous studies are most central to shaping public\nsupport for mitigation policies in climate change studies. The use of this\nmethod provides novel insights into the most important factors for public\nsupport for climate change mitigation policies. Using national survey data, we\nfind that the perceived risks associated with climate change are more decisive\nfor shaping public support for policy options promoting renewable energy and\nregulating pollutants. However, we observe a very different behavior related to\npublic support for increasing the use of nuclear energy where climate change\nrisk perception is no longer the sole decisive feature. Our findings indicate\nthat public support for renewable energy is inherently different from that for\nnuclear energy reliance with the risk perception of climate change, dominant\nfor the former, playing a subdued role for the latter.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.10234v1"
    },
    {
        "title": "Exploring the Effects of COVID-19 Containment Policies on Crime: An\n  Empirical Analysis of the Short-term Aftermath in Los Angeles",
        "authors": [
            "Gian Maria Campedelli",
            "Alberto Aziani",
            "Serena Favarin"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  This work investigates whether and how COVID-19 containment policies had an\nimmediate impact on crime trends in Los Angeles. The analysis is conducted\nusing Bayesian structural time-series and focuses on nine crime categories and\non the overall crime count, daily monitored from January 1st 2017 to March 28th\n2020. We concentrate on two post-intervention time windows - from March 4th to\nMarch 16th and from March 4th to March 28th 2020 - to dynamically assess the\nshort-term effects of mild and strict policies. In Los Angeles, overall crime\nhas significantly decreased, as well as robbery, shoplifting, theft, and\nbattery. No significant effect has been detected for vehicle theft, burglary,\nassault with a deadly weapon, intimate partner assault, and homicide. Results\nsuggest that, in the first weeks after the interventions are put in place,\nsocial distancing impacts more directly on instrumental and less serious\ncrimes. Policy implications are also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.11021v3"
    },
    {
        "title": "A machine learning methodology for real-time forecasting of the\n  2019-2020 COVID-19 outbreak using Internet searches, news alerts, and\n  estimates from mechanistic models",
        "authors": [
            "Dianbo Liu",
            "Leonardo Clemente",
            "Canelle Poirier",
            "Xiyu Ding",
            "Matteo Chinazzi",
            "Jessica T Davis",
            "Alessandro Vespignani",
            "Mauricio Santillana"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  We present a timely and novel methodology that combines disease estimates\nfrom mechanistic models with digital traces, via interpretable machine-learning\nmethodologies, to reliably forecast COVID-19 activity in Chinese provinces in\nreal-time. Specifically, our method is able to produce stable and accurate\nforecasts 2 days ahead of current time, and uses as inputs (a) official health\nreports from Chinese Center Disease for Control and Prevention (China CDC), (b)\nCOVID-19-related internet search activity from Baidu, (c) news media activity\nreported by Media Cloud, and (d) daily forecasts of COVID-19 activity from\nGLEAM, an agent-based mechanistic model. Our machine-learning methodology uses\na clustering technique that enables the exploitation of geo-spatial\nsynchronicities of COVID-19 activity across Chinese provinces, and a data\naugmentation technique to deal with the small number of historical disease\nactivity observations, characteristic of emerging outbreaks. Our model's\npredictive power outperforms a collection of baseline models in 27 out of the\n32 Chinese provinces, and could be easily extended to other geographies\ncurrently affected by the COVID-19 outbreak to help decision makers.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.04019v1"
    },
    {
        "title": "Leveraging Big Data Analytics in Healthcare Enhancement: Trends,\n  Challenges and Opportunities",
        "authors": [
            "Arshia Rehman",
            "Saeeda Naz",
            "Imran Razzak"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Clinicians decisions are becoming more and more evidence-based meaning in no\nother field the big data analytics so promising as in healthcare. Due to the\nsheer size and availability of healthcare data, big data analytics has\nrevolutionized this industry and promises us a world of opportunities. It\npromises us the power of early detection, prediction, prevention and helps us\nto improve the quality of life. Researchers and clinicians are working to\ninhibit big data from having a positive impact on health in the future.\nDifferent tools and techniques are being used to analyze, process, accumulate,\nassimilate and manage large amount of healthcare data either in structured or\nunstructured form. In this paper, we would like to address the need of big data\nanalytics in healthcare: why and how can it help to improve life?. We present\nthe emerging landscape of big data and analytical techniques in the five\nsub-disciplines of healthcare i.e.medical image analysis and imaging\ninformatics, bioinformatics, clinical informatics, public health informatics\nand medical signal analytics. We presents different architectures, advantages\nand repositories of each discipline that draws an integrated depiction of how\ndistinct healthcare activities are accomplished in the pipeline to facilitate\nindividual patients from multiple perspectives. Finally the paper ends with the\nnotable applications and challenges in adoption of big data analytics in\nhealthcare.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09010v1"
    },
    {
        "title": "Product risk assessment: a Bayesian network approach",
        "authors": [
            "Joshua Hunte",
            "Martin Neil",
            "Norman Fenton"
        ],
        "category": "stat.OT",
        "published_year": "2020",
        "summary": "  Product risk assessment is the overall process of determining whether a\nproduct, which could be anything from a type of washing machine to a type of\nteddy bear, is judged safe for consumers to use. There are several methods used\nfor product risk assessment, including RAPEX, which is the primary method used\nby regulators in the UK and EU. However, despite its widespread use, we\nidentify several limitations of RAPEX including a limited approach to handling\nuncertainty and the inability to incorporate causal explanations for using and\ninterpreting test data. In contrast, Bayesian Networks (BNs) are a rigorous,\nnormative method for modelling uncertainty and causality which are already used\nfor risk assessment in domains such as medicine and finance, as well as\ncritical systems generally. This article proposes a BN model that provides an\nimproved systematic method for product risk assessment that resolves the\nidentified limitations with RAPEX. We use our proposed method to demonstrate\nrisk assessments for a teddy bear and a new uncertified kettle for which there\nis no testing data and the number of product instances is unknown. We show\nthat, while we can replicate the results of the RAPEX method, the BN approach\nis more powerful and flexible.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.06698v1"
    },
    {
        "title": "Improving non-deterministic uncertainty modelling in Industry 4.0\n  scheduling",
        "authors": [
            "Ashwin Misra",
            "Ankit Mittal",
            "Vihaan Misra",
            "Deepanshu Pandey"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  The latest Industrial revolution has helped industries in achieving very high\nrates of productivity and efficiency. It has introduced data aggregation and\ncyber-physical systems to optimize planning and scheduling. Although,\nuncertainty in the environment and the imprecise nature of human operators are\nnot accurately considered for into the decision making process. This leads to\ndelays in consignments and imprecise budget estimations. This widespread\npractice in the industrial models is flawed and requires rectification. Various\nother articles have approached to solve this problem through stochastic or\nfuzzy set model methods. This paper presents a comprehensive method to\nlogically and realistically quantify the non-deterministic uncertainty through\nprobabilistic uncertainty modelling. This method is applicable on virtually all\nIndustrial data sets, as the model is self adjusting and uses\nepsilon-contamination to cater to limited or incomplete data sets. The results\nare numerically validated through an Industrial data set in Flanders, Belgium.\nThe data driven results achieved through this robust scheduling method\nillustrate the improvement in performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.05677v1"
    },
    {
        "title": "A comparative study of scoring systems by simulations",
        "authors": [
            "László Csató"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Scoring rules aggregate individual rankings by assigning some points to each\nposition in each ranking such that the total sum of points provides the overall\nranking of the alternatives. They are widely used in sports competitions\nconsisting of multiple contests. We study the tradeoff between two risks in\nthis setting: (1) the threat of early clinch when the title has been clinched\nbefore the last contest(s) of the competition take place; (2) the danger of\nwinning the competition without finishing first in any contest. In particular,\nfour historical points scoring systems of the Formula One World Championship\nare compared with the family of geometric scoring rules, recently proposed by\nan axiomatic approach. The schemes used in practice are found to be competitive\nwith respect to these goals, and the current rule seems to be a reasonable\ncompromise close to the Pareto frontier. Our results shed more light on the\nevolution of the Formula One points scoring systems and contribute to the issue\nof choosing the set of point values.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.05744v7"
    },
    {
        "title": "Statistical Analysis of Quantum Annealing",
        "authors": [
            "Xinyu Song",
            "Yazhen Wang",
            "Shang Wu",
            "Donggyu Kim"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  Quantum computers use quantum resources to carry out computational tasks and\nmay outperform classical computers in solving certain computational problems.\nSpecial-purpose quantum computers such as quantum annealers employ quantum\nadiabatic theorem to solve combinatorial optimization problems. In this paper,\nwe compare classical annealings such as simulated annealing and quantum\nannealings that are done by the D-Wave machines both theoretically and\nnumerically. We show that if the classical and quantum annealing are\ncharacterized by equivalent Ising models, then solving an optimization problem,\ni.e., finding the minimal energy of each Ising model, by the two annealing\nprocedures, are mathematically identical. For quantum annealing, we also derive\nthe probability lower-bound on successfully solving an optimization problem by\nmeasuring the system at the end of the annealing procedure. Moreover, we\npresent the Markov chain Monte Carlo (MCMC) method to realize quantum annealing\nby classical computers and investigate its statistical properties. In the\nnumerical section, we discuss the discrepancies between the MCMC based\nannealing approaches and the quantum annealing approach in solving optimization\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.06854v1"
    },
    {
        "title": "Lessons from the German Tank Problem",
        "authors": [
            "George Clark",
            "Alex Gonye",
            "Steven J Miller"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  During World War II the German army used tanks to devastating advantage. The\nAllies needed accurate estimates of their tank production and deployment. They\nused two approaches to find these values: spies, and statistics. This note\ndescribes the statistical approach. Assuming the tanks are labeled\nconsecutively starting at 1, if we observe $k$ serial numbers from an unknown\nnumber $N$ of tanks, with the maximum observed value $m$, then the best\nestimate for $N$ is $m(1 + 1/k) - 1$. This is now known as the German Tank\nProblem, and is a terrific example of the applicability of mathematics and\nstatistics in the real world. The first part of the paper reproduces known\nresults, specifically deriving this estimate and comparing its effectiveness to\nthat of the spies. The second part presents a result we have not found in print\nelsewhere, the generalization to the case where the smallest value is not\nnecessarily 1. We emphasize in detail why we are able to obtain such clean,\nclosed-form expressions for the estimates, and conclude with an appendix\nhighlighting how to use this problem to teach regression and how statistics can\nhelp us find functional relationships.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.08162v2"
    },
    {
        "title": "Deepest Voting: a new way of electing",
        "authors": [
            "Jean-Baptiste Aubin",
            "Irène Gannaz",
            "Samuela Leoni-Aubin",
            "Antoine Rolland"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  This article aims to present a unified framework for grading-based voting\nprocesses. The idea is to represent the grades of each voter on d candidates as\na point in R^d and to define the winner of the vote using the deepest point of\nthe scatter plot. The deepest point is obtained by the maximization of a depth\nfunction. Universality, unanimity, and neutrality properties are proved to be\nsatisfied. Monotonicity and Independence to Irrelevant Alternatives are also\nstudied. It is shown that usual voting processes correspond to specific choices\nof depth functions. Finally, some basic paradoxes are explored for these voting\nprocesses.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.04228v2"
    },
    {
        "title": "A nonBayesian view of Hempel's paradox of the ravens",
        "authors": [
            "Yudi Pawitan"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  In Hempel's paradox of the ravens, seeing a red pencil is considered as\nsupporting evidence that all ravens are black. Also known as the Paradox of\nConfirmation, the paradox and its many resolutions indicate that we cannot\nunderestimate the logical and statistical elements needed in the assessment of\nevidence in support of a hypothesis. Most of the previous analyses of the\nparadox are within the Bayesian framework. These analyses and Hempel himself\ngenerally accept the paradoxical conclusion; it feels paradoxical supposedly\nbecause the amount of evidence is extremely small. Here I describe a\nnonBayesian analysis of various statistical models with an accompanying\nlikelihood-based reasoning. The analysis shows that the paradox feels\nparadoxical because there are natural models where observing a red pencil has\nno relevance to the color of ravens. In general the value of the evidence\ndepends crucially on the sampling scheme and on the assumption about the\nunderlying parameters of the relevant model.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.02522v3"
    },
    {
        "title": "On rereading Savage",
        "authors": [
            "Yudi Pawitan",
            "Youngjo Lee"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  If we accept Savage's set of axioms, then all uncertainties must be treated\nlike ordinary probability. Savage espoused subjective probability, allowing,\nfor example, the probability of Donald Trump's re-election. But Savage's\nprobability also covers the objective version, such as the probability of heads\nin a fair toss of a coin. In other words, there is no distinction between\nobjective and subjective probability. Savage's system has great theoretical\nimplications; for example, prior probabilities can be elicited from subjective\npreferences, and then get updated by objective evidence, a learning step that\nforms the basis of Bayesian computations. Non-Bayesians have generally refused\nto accept the subjective aspect of probability or to allow priors in formal\nstatistical modelling. As demanded, for example, by the late Dennis Lindley,\nsince Bayesian probability is axiomatic, it is the non-Bayesians' duty to point\nout which axioms are not acceptable to them. This is not a simple request,\nsince the Bayesian axioms are not commonly covered in our professional\ntraining, even in the Bayesian statistics courses. So our aim is to provide a\nreadable exposition the Bayesian axioms from a close rereading Savage's classic\nbook.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.15012v1"
    },
    {
        "title": "Source Reconstruction for Spatio-Temporal Physical Statistical Models",
        "authors": [
            "Connie Okasaki",
            "Mevin B. Hooten",
            "Andrew M. Berdahl"
        ],
        "category": "stat.OT",
        "published_year": "2021",
        "summary": "  In many applications, a signal is deformed by well-understood dynamics before\nit can be measured. For example, when a pollutant enters a river, it\nimmediately begins dispersing, flowing, settling, and reacting. If the\npollutant enters at a single point, its concentration can be measured before it\nenters the complex dynamics of the river system. However, in the case of a\nnon-point source pollutant, it is not clear how to efficiently measure its\nsource. One possibility is to record concentration measurements in the river,\nbut this signal is masked by the fluid dynamics of the river. Specifically,\nconcentration is governed by the advection-diffusion-reaction PDE, with an\nunknown source term. We propose a method to statistically reconstruct a source\nterm from these PDE-deformed measurements. Our method is general and applies to\nany linear PDE. This method has important applications in the study of\nenvironmental DNA and non-point source pollution.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.13829v2"
    },
    {
        "title": "A framework to determine micro-level population figures using spatially\n  disaggregated population estimates",
        "authors": [
            "V. E. Irekponor",
            "M. Abdul-Rahman",
            "M. Agunbiade",
            "A. J. Bustamente"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  About half of the world population already live in urban areas. It is\nprojected that by 2050, approximately 70% of the world population will live in\ncities. In addition to this, most developing countries do not have reliable\npopulation census figures, and periodic population censuses are extremely\nresource expensive. In Africa's most populous country, Nigeria, for instance,\nthe last decennial census was conducted in 2006. The relevance of near-accurate\npopulation figures at the local levels cannot be overemphasized for a broad\nrange of applications by government agencies and non-governmental\norganizations, including the planning and delivery of services, estimating\npopulations at risk of hazards or infectious diseases, and disaster relief\noperations. Using GRID3 (Geo-Referenced Infrastructure and Demographic Data for\nDevelopment) high-resolution spatially disaggregated population data estimates,\nthis study proposed a framework for aggregating population figures at micro\nlevels within a larger geographic jurisdiction. Python, QGIS, and machine\nlearning techniques were used for data visualization, spatial analysis, and\nzonal statistics. Lagos Island, Nigeria was used as a case study to demonstrate\nhow to obtain a more precise population estimate at the lowest administrative\njurisdiction and eliminate ambiguity caused by antithetical parameters in the\ncalculations. We also demonstrated how the framework can be used as a benchmark\nfor estimating the carrying capacities of urban basic services like healthcare,\nhousing, sanitary facilities, education, water etc. The proposed framework\nwould help urban planners and government agencies to plan and manage cities\nbetter using more accurate data.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.02020v2"
    },
    {
        "title": "The Geometry of Chi-Square Degrees of Freedom",
        "authors": [
            "James Bernhard"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  In this paper, we state and prove a simple geometric interpretation of the\ndegrees of freedom of a chi-square distribution. The general geometric idea\ngoes back at least to Fisher in the 1920s, but the exact result does not appear\nto have been explicitly stated or proved prior to this paper.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.10335v1"
    },
    {
        "title": "Using maximum weighted likelihood to derive Lehmer and Hölder mean\n  families",
        "authors": [
            "Djemel Ziou"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  In this paper, we establish the links between the Lehmer and H\\\"older mean\nfamilies and maximum weighted likelihood estimator. Considering the regular\none-parameter exponential family of probability density functions, we show that\nthe maximum weighted likelihood of the parameter is a generalized weighted mean\nfamily from which Lehmer and H\\\"older mean families are derived. Some of the\noutcomes obtained provide a probabilistic interpretation of these mean families\nand could therefore broaden their uses in various applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.18366v2"
    },
    {
        "title": "Rational Aversion to Information",
        "authors": [
            "Sven Neth"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Is more information always better? Or are there some situations in which more\ninformation can make us worse off? Good (1967) argues that expected utility\nmaximizers should always accept more information if the information is\ncost-free and relevant. But Good's argument presupposes that you are certain\nyou will update by conditionalization. If we relax this assumption and allow\nagents to be uncertain about updating, these agents can be rationally required\nto reject free and relevant information. Since there are good reasons to be\nuncertain about updating, rationality can require you to prefer ignorance.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.12374v3"
    },
    {
        "title": "Automated grading workflows for providing personalized feedback to\n  open-ended data science assignments",
        "authors": [
            "Federica Zoe Ricci",
            "Catalina Mari Medina",
            "Mine Dogucu"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  Open-ended assignments - such as lab reports and semester-long projects -\nprovide data science and statistics students with opportunities for developing\ncommunication, critical thinking, and creativity skills. However, providing\ngrades and formative feedback to open-ended assignments can be very time\nconsuming and difficult to do consistently across students. In this paper, we\ndiscuss the steps of a typical grading workflow and highlight which steps can\nbe automated in an approach that we call automated grading workflow. We\nillustrate how gradetools, a new R package, implements this approach within\nRStudio to facilitate efficient and consistent grading while providing\nindividualized feedback. By outlining the motivations behind the development of\nthis package and the considerations underlying its design, we hope this article\nwill provide data science and statistics educators with ideas for improving\ntheir grading workflows, possibly developing new grading tools or considering\nuse gradetools as their grading workflow assistant.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.12924v2"
    },
    {
        "title": "Better Foundations for Subjective Probability",
        "authors": [
            "Sven Neth"
        ],
        "category": "stat.OT",
        "published_year": "2023",
        "summary": "  How do we ascribe subjective probability? In decision theory, this question\nis often addressed by representation theorems, going back to Ramsey (1926),\nwhich tell us how to define or measure subjective probability by observable\npreferences. However, standard representation theorems make strong rationality\nassumptions, in particular expected utility maximization. How do we ascribe\nsubjective probability to agents which do not satisfy these strong rationality\nassumptions? I present a representation theorem with weak rationality\nassumptions which can be used to define or measure subjective probability for\npartly irrational agents.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.09796v1"
    },
    {
        "title": "A Modified Satterthwaite (1941,1946) Effective Degrees of Freedom\n  Approximation",
        "authors": [
            "Matthias von Davier"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  This study introduces a correction to the approximation of effective degrees\nof freedom as proposed by Satterthwaite (1941, 1946), specifically addressing\nscenarios where component degrees of freedom are small. The correction is\ngrounded in analytical results concerning the moments of standard normal random\nvariables. This modification is applicable to complex variance estimates that\ninvolve both small and large degrees of freedom, offering an enhanced\napproximation of the higher moments required by Satterthwaite's framework.\nAdditionally, this correction extends and partially validates the empirically\nderived adjustment by Johnson & Rust (1992), as it is based on theoretical\nfoundations rather than simulations used to derive empirical transformation\nconstants.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.14606v5"
    },
    {
        "title": "easylayout: an R package for interactive force-directed layouts within\n  RStudio",
        "authors": [
            "Danilo Oliveira Imparato",
            "João Vitor F. Cavalcante",
            "Rodrigo J. S. Dalmolin"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Motivation\n  Network visualization is critical for effective communication in various\nfields of knowledge. Currently, a gap separates network manipulation from\nnetwork visualization in programming environments. Users often export network\ndata to be laid out in external interactive software, like Cytoscape and Gephi.\nWe argue the current R package ecosystem lacks an interactive layout engine\nwell integrated with common data analysis workflows.\n  Results\n  We present easylayout, an R package that bridges network manipulation and\nvisualization by leveraging interactive force simulations within the IDE itself\n(e.g., RStudio, VSCode). It is not yet another visualization library, but\ninstead aims to interconnect existing libraries and streamline their usage into\nthe R ecosystem. easylayout takes an igraph object and serializes it into a web\napplication integrated with the IDE's interface through a Shiny server. The app\nlays out the network by simulating attraction and repulsion forces. Simulation\nparameters can be adjusted in real-time. An editing mode allows moving and\nrotating nodes. The implementation aims for performance, so that even lower-end\ndevices are able to work with relatively large networks. Once the user finishes\ntweaking the layout, it is sent back to the R session to be plotted through\npopular libraries like ggraph, igraph or even the base package itself. The\ncurrent implementation focuses on the R ecosystem, but using web technologies\nmakes it easily portable to similar environments, like Python/Jupyter\nNotebooks. We expect this tool to reduce the time spent searching for suitable\nnetwork layouts, ultimately allowing researchers to generate more compelling\nfigures.\n  Availability and implementation\n  easylayout is freely available under an MIT license on GitHub\n(https://github.com/dalmolingroup/easylayout). The package is implemented in\nR/Shiny and JavaScript/Svelte.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.18280v1"
    },
    {
        "title": "Language all the way down: Linguistic structures in statistics education",
        "authors": [
            "Tess O'Brien"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  The ability to read, write, and speak mathematics is critical to students\nbecoming comfortable with statistical models and skills. Faster development of\nthose skills may act as encouragement to further engage with the discipline.\nVocabulary has been the focus of scholarship in existing literature on the\nlinguistics of mathematics and statistics but there are structures that go\nbeyond the content of words and symbols. Here I introduce ideas for grammar and\ndiscourse features through a sequence of examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.07569v1"
    },
    {
        "title": "Convergence to the Truth",
        "authors": [
            "Hanti Lin"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  This article reviews and develops an epistemological tradition in philosophy\nof science, called convergentism, which holds that inference methods should be\nassessed in terms of their abilities to converge to the truth. This tradition\nis compared with three competing ones: (1) explanationism, which holds that\ntheory choice should be guided by a theory's overall balance of explanatory\nvirtues, such as simplicity and fit with data; (2) instrumentalism, according\nto which scientific inference should be driven by the goal of obtaining useful\nmodels, rather than true theories; (3) Bayesianism, which features a shift of\nfocus from all-or-nothing beliefs to degrees of belief.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.11399v1"
    },
    {
        "title": "Ethical Statistical Practice and Ethical AI",
        "authors": [
            "Rochelle E. Tractenberg"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  Artificial Intelligence (AI) is a field that utilizes computing and often,\ndata and statistics, intensively together to solve problems or make\npredictions. AI has been evolving with literally unbelievable speed over the\npast few years, and this has led to an increase in social, cultural,\nindustrial, scientific, and governmental concerns about the ethical development\nand use of AI systems worldwide. The ASA has issued a statement on ethical\nstatistical practice and AI (ASA, 2024), which echoes similar statements from\nother groups. Here we discuss the support for ethical statistical practice and\nethical AI that has been established in long-standing human rights law and\nethical practice standards for computing and statistics. There are multiple\nsources of support for ethical statistical practice and ethical AI deriving\nfrom these source documents, which are critical for strengthening the\noperationalization of the \"Statement on Ethical AI for Statistics\nPractitioners\". These resources are explicated for interested readers to\nutilize to guide their development and use of AI in, and through, their\nstatistical practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.22475v1"
    },
    {
        "title": "On the Impossibility of Equating the Youden Index with Tjur's $R^2$-like\n  metrics in $2\\times2$ Tables",
        "authors": [
            "Linard Hoessly"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  In 2017, Hughes claimed an equivalence between Tjurs $R^2$ coefficient of\ndiscrimination and the Youden index for assessing diagnostic test performance\non $2\\times 2$ contingency tables. We prove an impossibility result when\naveraging over binary outcomes (0s and 1s) under any continuous real-valued\nscoring rule. Our finding clarifies the limitations of such a possible\nequivalence and highlights the distinct roles these metrics play in diagnostic\ntest assessment.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05391v2"
    },
    {
        "title": "Scientific Realism vs. Anti-Realism: Toward a Common Ground",
        "authors": [
            "Hanti Lin"
        ],
        "category": "stat.OT",
        "published_year": "2024",
        "summary": "  The debate between scientific realism and anti-realism remains at a\nstalemate, making reconciliation seem hopeless. Yet, important work remains:\nexploring a common ground, even if only to uncover deeper points of\ndisagreement and, ideally, to benefit both sides of the debate. I propose such\na common ground. Specifically, many anti-realists, such as instrumentalists,\nhave yet to seriously engage with Sober's call to justify their preferred\nversion of Ockham's razor through a positive account. Meanwhile, realists face\na similar challenge: providing a non-circular explanation of how their version\nof Ockham's razor connects to truth. The common ground I propose addresses\nthese challenges for both sides; the key is to leverage the idea that everyone\nvalues some truths and to draw on insights from scientific fields that study\nscientific inference -- namely, statistics and machine learning. This common\nground also isolates a distinctively epistemic root of the irreconcilability in\nthe realism debate.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.10643v2"
    },
    {
        "title": "How to read probability distributions as statements about process",
        "authors": [
            "Steven A. Frank"
        ],
        "category": "stat.OT",
        "published_year": "2014",
        "summary": "  Probability distributions can be read as simple expressions of information.\nEach continuous probability distribution describes how information changes with\nmagnitude. Once one learns to read a probability distribution as a measurement\nscale of information, opportunities arise to understand the processes that\ngenerate the commonly observed patterns. Probability expressions may be parsed\ninto four components: the dissipation of all information, except the\npreservation of average values, taken over the measurement scale that relates\nchanges in observed values to changes in information, and the transformation\nfrom the underlying scale on which information dissipates to alternative scales\non which probability pattern may be expressed. Information invariances set the\ncommonly observed measurement scales and the relations between them. In\nparticular, a measurement scale for information is defined by its invariance to\nspecific transformations of underlying values into measurable outputs.\nEssentially all common distributions can be understood within this simple\nframework of information invariance and measurement scale.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.5196v3"
    },
    {
        "title": "Decision Making for Inconsistent Expert Judgments Using Negative\n  Probabilities",
        "authors": [
            "J. Acacio de Barros"
        ],
        "category": "stat.OT",
        "published_year": "2013",
        "summary": "  In this paper we provide a simple random-variable example of inconsistent\ninformation, and analyze it using three different approaches: Bayesian,\nquantum-like, and negative probabilities. We then show that, at least for this\nparticular example, both the Bayesian and the quantum-like approaches have less\nnormative power than the negative probabilities one.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.4101v2"
    },
    {
        "title": "Group testing via residuation and partial geometries",
        "authors": [
            "Marcus Greferath",
            "Cornelia Roessing"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  The motivation for this paper comes from the ongoing SARS-CoV-2 Pandemic. Its\ngoal is to present a previously neglected approach to non-adaptive group\ntesting and describes it in terms of residuated pairs on partially ordered\nsets. Our investigation has the advantage, as it naturally yields an efficient\ndecision scheme (decoder) for any given testing scheme. This decoder allows to\ndetect a large amount of infection patterns. Apart from this, we devise a\nconstruction of good group testing schemes that are based on incidence matrices\nof finite partial linear spaces. The key idea is to exploit the structure of\nthese matrices and make them available as test matrices for group testing.\nThese matrices may generally be tailored for different estimated disease\nprevalence levels. As an example, we discuss the group testing schemes based on\ngeneralized quadrangles. In the context at hand, we state our results only for\nthe error-free case so far. An extension to a noisy scenario is desirable and\nwill be treated in a subsequent account on the topic.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.05876v1"
    },
    {
        "title": "Far from Asymptopia",
        "authors": [
            "Michael C. Abbott",
            "Benjamin B. Machta"
        ],
        "category": "stat.OT",
        "published_year": "2022",
        "summary": "  Inference from limited data requires a notion of measure on parameter space,\nmost explicit in the Bayesian framework as a prior. Here we demonstrate that\nJeffreys prior, the best-known uninformative choice, introduces enormous bias\nwhen applied to typical scientific models. Such models have a relevant\neffective dimensionality much smaller than the number of microscopic\nparameters. Because Jeffreys prior treats all microscopic parameters equally,\nit is from uniform when projected onto the sub-space of relevant parameters,\ndue to variations in the local co-volume of irrelevant directions. We present\nresults on a principled choice of measure which avoids this issue, leading to\nunbiased inference in complex models. This optimal prior depends on the\nquantity of data to be gathered, and approaches Jeffreys prior in the\nasymptotic limit. However, this limit cannot be justified without an impossibly\nlarge amount of data, exponential in the number of microscopic parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.03343v2"
    }
]