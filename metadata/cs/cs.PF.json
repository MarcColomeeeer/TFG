[
    {
        "title": "Faster-than-light effects and negative group delays in optics and\n  electronics, and their applications",
        "authors": [
            "Raymond Y. Chiao",
            "Jandir M. Hickmann",
            "Daniel Solli"
        ],
        "category": "cs.PF",
        "published_year": "2001",
        "summary": "  Recent manifestations of apparently faster-than-light effects confirmed our\npredictions that the group velocity in transparent optical media can exceed c.\nSpecial relativity is not violated by these phenomena. Moreover, in the\nelectronic domain, the causality principle does not forbid negative group\ndelays of analytic signals in electronic circuits, in which the peak of an\noutput pulse leaves the exit port of a circuit before the peak of the input\npulse enters the input port. Furthermore, pulse distortion for these\nsuperluminal analytic signals can be negligible in both the optical and\nelectronic domains. Here we suggest an extension of these ideas to the\nmicroelectronic domain. The underlying principle is that negative feedback can\nbe used to produce negative group delays. Such negative group delays can be\nused to cancel out the positive group delays due to transistor latency (e.g.,\nthe finite RC rise time of MOSFETS caused by their intrinsic gate capacitance),\nas well as the propagation delays due to the interconnects between transistors.\nUsing this principle, it is possible to speed up computer systems.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0103014v1"
    },
    {
        "title": "Experiences with advanced CORBA services",
        "authors": [
            "G. Milcinski",
            "M. Plesko",
            "M. Sekoranja"
        ],
        "category": "cs.PF",
        "published_year": "2001",
        "summary": "  The Common Object Request Broker Architecture (CORBA) is successfully used in\nmany control systems (CS) for data transfer and device modeling. Communication\nrates below 1 millisecond, high reliability, scalability, language independence\nand other features make it very attractive. For common types of applications\nlike error logging, alarm messaging or slow monitoring, one can benefit from\nstandard CORBA services that are implemented by third parties and save\ntremendous amount of developing time. We have started using few CORBA services\non our previous CORBA-based control system for the light source ANKA [1] and\nuse now several CORBA services for the ALMA Common Software (ACS) [2], the core\nof the control system of the Atacama Large Millimeter Array. Our experiences\nwith the interface repository (IFR), the implementation repository, the naming\nservice, the property service, telecom log service and the notify service from\ndifferent vendors are presented. Performance and scalability benchmarks have\nbeen performed.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0111034v1"
    },
    {
        "title": "Minimizing Cache Misses in Scientific Computing Using Isoperimetric\n  Bodies",
        "authors": [
            "Michael Frumkin",
            "Rob F. Van der Wijngaart"
        ],
        "category": "cs.PF",
        "published_year": "2002",
        "summary": "  A number of known techniques for improving cache performance in scientific\ncomputations involve the reordering of the iteration space. Some of these\nreorderings can be considered coverings of the iteration space with sets having\nsmall surface-to-volume ratios. Use of such sets may reduce the number of cache\nmisses in computations of local operators having the iteration space as their\ndomain. First, we derive lower bounds on cache misses that any algorithm must\nsuffer while computing a local operator on a grid. Then, we explore coverings\nof iteration spaces of structured and unstructured discretization grid\noperators which allow us to approach these lower bounds. For structured grids\nwe introduce a covering by successive minima tiles based on the interference\nlattice of the grid. We show that the covering has a small surface-to-volume\nratio and present a computer experiment showing actual reduction of the cache\nmisses achieved by using these tiles. For planar unstructured grids we show\nexistence of a covering which reduces the number of cache misses to the level\nof that of structured grids. Next, we introduce a class of multidimensional\ngrids, called starry grids in this paper. These grids represent an abstraction\nof unstructured grids used in, for example, molecular simulations and the\nsolution of partial differential equations. We show that starry grids can be\ncovered by sets having a low surface-to-volume ratio and, hence have the same\ncache efficiency as structured grids. Finally, we present a triangulation of a\nthree-dimensional cube that has the property that any local operator on the\ncorresponding grid must incur a significantly larger number of cache misses\nthan a similar operator on a structured grid of the same size.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0205062v1"
    },
    {
        "title": "Modelling Delay Jitter in Voice over IP",
        "authors": [
            "R. Ganesh",
            "B. Kaushik",
            "R. Sadhu"
        ],
        "category": "cs.PF",
        "published_year": "2003",
        "summary": "  It has been suggested in voice over IP that an appropriate choice of the\ndistribution used in modeling the delay jitters, can improve the play-out\nalgorithm. In this paper, we propose a tool using which, one can determine, at\na given instance, which distribution model best explains the jitter\ndistribution. This is done using Expectation Maximization, to choose amongst\npossible distribution models which include, the i.i.d exponential distribution,\nthe gamma distribution etc.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0301005v1"
    },
    {
        "title": "A Performance Study of Monitoring and Information Services for\n  Distributed Systems",
        "authors": [
            "Xuehai Zhang",
            "Jeffrey Freschl",
            "Jennifer M. Schopf"
        ],
        "category": "cs.PF",
        "published_year": "2003",
        "summary": "  Monitoring and information services form a key component of a distributed\nsystem, or Grid. A quantitative study of such services can aid in understanding\nthe performance limitations, advise in the deployment of the systems, and help\nevaluate future development work. To this end, we study the performance of\nthree monitoring and information services for distributed systems: the Globus\nToolkit's Monitoring and Discovery Service (MDS), the European Data Grid\nRelational Grid Monitoring Architecture (R-GMA), and Hawkeye, part of the\nCondor project. We perform experiments to test their scalability with respect\nto number of users, number of resources, and amount of data collected. Our\nstudy shows that each approach has different behaviors, often due to their\ndifferent design goals. In the four sets of experiments we conducted to\nevaluate the performance of the service components under different\ncircumstances, we found a strong advantage to caching or prefetching the data,\nas well as the need to have primary components at well connected sites due to\nhigh load seen by all systems.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0304015v1"
    },
    {
        "title": "A Monitoring System for the BaBar INFN Computing Cluster",
        "authors": [
            "M. Marzolla",
            "V. Melloni"
        ],
        "category": "cs.PF",
        "published_year": "2003",
        "summary": "  Monitoring large clusters is a challenging problem. It is necessary to\nobserve a large quantity of devices with a reasonably short delay between\nconsecutive observations. The set of monitored devices may include PCs, network\nswitches, tape libraries and other equipments. The monitoring activity should\nnot impact the performances of the system. In this paper we present PerfMC, a\nmonitoring system for large clusters. PerfMC is driven by an XML configuration\nfile, and uses the Simple Network Management Protocol (SNMP) for data\ncollection. SNMP is a standard protocol implemented by many networked\nequipments, so the tool can be used to monitor a wide range of devices. System\nadministrators can display informations on the status of each device by\nconnecting to a WEB server embedded in PerfMC. The WEB server can produce\ngraphs showing the value of different monitored quantities as a function of\ntime; it can also produce arbitrary XML pages by applying XSL Transformations\nto an internal XML representation of the cluster's status. XSL Transformations\nmay be used to produce HTML pages which can be displayed by ordinary WEB\nbrowsers. PerfMC aims at being relatively easy to configure and operate, and\nhighly efficient. It is currently being used to monitor the Italian\nReprocessing farm for the BaBar experiment, which is made of about 200 dual-CPU\nLinux machines.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0305054v1"
    },
    {
        "title": "Performance comparison between iSCSI and other hardware and software\n  solutions",
        "authors": [
            "Mathias Gug"
        ],
        "category": "cs.PF",
        "published_year": "2003",
        "summary": "  We report on our investigations on some technologies that can be used to\nbuild disk servers and networks of disk servers using commodity hardware and\nsoftware solutions. It focuses on the performance that can be achieved by these\nsystems and gives measured figures for different configurations.\n  It is divided into two parts : iSCSI and other technologies and hardware and\nsoftware RAID solutions.\n  The first part studies different technologies that can be used by clients to\naccess disk servers using a gigabit ethernet network. It covers block access\ntechnologies (iSCSI, hyperSCSI, ENBD). Experimental figures are given for\ndifferent numbers of clients and servers.\n  The second part compares a system based on 3ware hardware RAID controllers, a\nsystem using linux software RAID and IDE cards and a system mixing both\nhardware RAID and software RAID. Performance measurements for reading and\nwriting are given for different RAID levels.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0305060v1"
    },
    {
        "title": "Elements for Response Time Statistics in ERP Transaction Systems",
        "authors": [
            "Andreas Mielke"
        ],
        "category": "cs.PF",
        "published_year": "2004",
        "summary": "  We present some measurements and ideas for response time statistics in ERP\nsystems. It is shown that the response time distribution of a given transaction\nin a given system is generically a log-normal distribution or, in some\nsituations, a sum of two or more log-normal distributions. We present some\narguments for this form of the distribution based on heuristic rules for\nresponse times, and we show data from performance measurements in actual\nsystems to support the log-normal form. Deviations of the log-normal form can\noften be traced back to performance problems in the system. Consequences for\nthe interpretation of response time data and for service level agreements are\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0404035v3"
    },
    {
        "title": "Analysis of a Reputation System for Mobile Ad-Hoc Networks with Liars",
        "authors": [
            "Jochen Mundinger",
            "Jean-Yves Le Boudec"
        ],
        "category": "cs.PF",
        "published_year": "2006",
        "summary": "  The application of decentralized reputation systems is a promising approach\nto ensure cooperation and fairness, as well as to address random failures and\nmalicious attacks in Mobile Ad-Hoc Networks. However, they are potentially\nvulnerable to liars. With our work, we provide a first step to analyzing\nrobustness of a reputation system based on a deviation test. Using a mean-field\napproach to our stochastic process model, we show that liars have no impact\nunless their number exceeds a certain threshold (phase transition). We give\nprecise formulae for the critical values and thus provide guidelines for an\noptimal choice of parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0608102v1"
    },
    {
        "title": "On Degree-Based Decentralized Search in Complex Networks",
        "authors": [
            "Shi Xiao",
            "Gaoxi Xiao"
        ],
        "category": "cs.PF",
        "published_year": "2006",
        "summary": "  Decentralized search aims to find the target node in a large network by using\nonly local information. The applications of it include peer-to-peer file\nsharing, web search and anything else that requires locating a specific target\nin a complex system. In this paper, we examine the degree-based decentralized\nsearch method. Specifically, we evaluate the efficiency of the method in\ndifferent cases with different amounts of available local information. In\naddition, we propose a simple refinement algorithm for significantly shortening\nthe length of the route that has been found. Some insights useful for the\nfuture developments of efficient decentralized search schemes have been\nachieved.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0610173v1"
    },
    {
        "title": "A Combined LIFO-Priority Scheme for Overload Control of E-commerce Web\n  Servers",
        "authors": [
            "Naresh Singhmar",
            "Vipul Mathur",
            "Varsha Apte",
            "D. Manjunath"
        ],
        "category": "cs.PF",
        "published_year": "2006",
        "summary": "  E-commerce Web-servers often face overload conditions during which\nrevenue-generating requests may be dropped or abandoned due to an increase in\nthe browsing requests. In this paper we present a simple, yet effective,\nmechanism for overload control of E-commerce Web-servers. We develop an\nE-commerce workload model that separates the browsing requests from\nrevenue-generating transaction requests. During overload, we apply LIFO\ndiscipline in the browsing queues and use a dynamic priority model to service\nthem. The transaction queues are given absolute priority over the browsing\nqueues. This is called the LIFO-Pri scheduling discipline. Experimental results\nshow that LIFO-Pri dramatically improves the overall Web-server throughput\nwhile also increasing the completion rate of revenue-generating requests. The\nWeb-server was able to operate at nearly 60% of its maximum capacity even when\noffered load was 1.5 times its capacity. Further, when compared to a single\nqueue FIFO system, there was a seven-fold increase in the number of completed\nrevenue-generating requests during overload.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0611087v1"
    },
    {
        "title": "Exact Failure Frequency Calculations for Extended Systems",
        "authors": [
            "Annie Druault-Vicard",
            "Christian Tanguy"
        ],
        "category": "cs.PF",
        "published_year": "2006",
        "summary": "  This paper shows how the steady-state availability and failure frequency can\nbe calculated in a single pass for very large systems, when the availability is\nexpressed as a product of matrices. We apply the general procedure to\n$k$-out-of-$n$:G and linear consecutive $k$-out-of-$n$:F systems, and to a\nsimple ladder network in which each edge and node may fail. We also give the\nassociated generating functions when the components have identical\navailabilities and failure rates. For large systems, the failure rate of the\nwhole system is asymptotically proportional to its size. This paves the way to\nready-to-use formulae for various architectures, as well as proof that the\ndifferential operator approach to failure frequency calculations is very useful\nand straightforward.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0612141v1"
    },
    {
        "title": "Exact solutions for the two- and all-terminal reliabilities of a simple\n  ladder network",
        "authors": [
            "Christian Tanguy"
        ],
        "category": "cs.PF",
        "published_year": "2006",
        "summary": "  The exact calculation of network reliability in a probabilistic context has\nbeen a long-standing issue of practical importance, but a difficult one, even\nfor planar graphs, with perfect nodes and with edges of identical reliability\np. Many approaches (determination of bounds, sums of disjoint products\nalgorithms, Monte Carlo evaluations, studies of the reliability polynomials,\netc.) can only provide approximations when the network's size increases. We\nconsider here a ladder graph of arbitrary size corresponding to real-life\nnetwork configurations, and give the exact, analytical solutions for the all-\nand two-terminal reliabilities. These solutions use transfer matrices, in which\nindividual reliabilities of edges and nodes are taken into account. The special\ncase of identical edge and node reliabilities -- p and rho, respectively -- is\nsolved. We show that the zeros of the two-terminal reliability polynomial\nexhibit structures which differ substantially for seemingly similar networks,\nand we compare the sensitivity of various edges. We discuss how the present\nwork may be further extended to lead to a catalog of exactly solvable networks\nin terms of reliability, which could be useful as elementary bricks for a new\nand improved set of bounds or benchmarks in the general case.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0612143v1"
    },
    {
        "title": "Exact solutions for the two- and all-terminal reliabilities of the\n  Brecht-Colbourn ladder and the generalized fan",
        "authors": [
            "Christian Tanguy"
        ],
        "category": "cs.PF",
        "published_year": "2006",
        "summary": "  The two- and all-terminal reliabilities of the Brecht-Colbourn ladder and the\ngeneralized fan have been calculated exactly for arbitrary size as well as\narbitrary individual edge and node reliabilities, using transfer matrices of\ndimension four at most. While the all-terminal reliabilities of these graphs\nare identical, the special case of identical edge ($p$) and node ($\\rho$)\nreliabilities shows that their two-terminal reliabilities are quite distinct,\nas demonstrated by their generating functions and the locations of the zeros of\nthe reliability polynomials, which undergo structural transitions at $\\rho =\n\\displaystyle {1/2}$.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0701005v1"
    },
    {
        "title": "High Performance Direct Gravitational N-body Simulations on Graphics\n  Processing Units",
        "authors": [
            "Simon Portegies Zwart",
            "Robert Belleman",
            "Peter Geldof"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  We present the results of gravitational direct $N$-body simulations using the\ncommercial graphics processing units (GPU) NVIDIA Quadro FX1400 and GeForce\n8800GTX, and compare the results with GRAPE-6Af special purpose hardware. The\nforce evaluation of the $N$-body problem was implemented in Cg using the GPU\ndirectly to speed-up the calculations. The integration of the equations of\nmotions were, running on the host computer, implemented in C using the 4th\norder predictor-corrector Hermite integrator with block time steps. We find\nthat for a large number of particles ($N \\apgt 10^4$) modern graphics\nprocessing units offer an attractive low cost alternative to GRAPE special\npurpose hardware. A modern GPU continues to give a relatively flat scaling with\nthe number of particles, comparable to that of the GRAPE. Using the same time\nstep criterion the total energy of the $N$-body system was conserved better\nthan to one in $10^6$ on the GPU, which is only about an order of magnitude\nworse than obtained with GRAPE. For $N\\apgt 10^6$ the GeForce 8800GTX was about\n20 times faster than the host computer. Though still about an order of\nmagnitude slower than GRAPE, modern GPU's outperform GRAPE in their low cost,\nlong mean time between failure and the much larger onboard memory; the\nGRAPE-6Af holds at most 256k particles whereas the GeForce 8800GTF can hold 9\nmillion particles in memory.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0702135v1"
    },
    {
        "title": "A Technical Report On Grid Benchmarking using SEE V.O",
        "authors": [
            "John Kouvakis",
            "Fotis Georgatos"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  Grids include heterogeneous resources, which are based on different hardware\nand software architectures or components. In correspondence with this diversity\nof the infrastructure, the execution time of any single job, as well as the\ntotal grid performance can both be affected substantially, which can be\ndemonstrated by measurements. Running a simple benchmarking suite can show this\nheterogeneity and give us results about the differences over the grid sites.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0703086v2"
    },
    {
        "title": "Availability assessment of SunOS/Solaris Unix Systems based on Syslogd\n  and wtmpx logfiles : a case study",
        "authors": [
            "Cristina Simache",
            "Mohamed Kaaniche"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  This paper presents a measurement-based availability assessment study using\nfield data collected during a 4-year period from 373 SunOS/Solaris Unix\nworkstations and servers interconnected through a local area network. We focus\non the estimation of machine uptimes, downtimes and availability based on the\nidentification of failures that caused total service loss. Data corresponds to\nsyslogd event logs that contain a large amount of information about the normal\nactivity of the studied systems as well as their behavior in the presence of\nfailures. It is widely recognized that the information contained in such event\nlogs might be incomplete or imperfect. The solution investigated in this paper\nto address this problem is based on the use of auxiliary sources of data\nobtained from wtmpx files maintained by the SunOS/Solaris Unix operating\nsystem. The results obtained suggest that the combined use of wtmpx and syslogd\nlog files provides more complete information on the state of the target systems\nthat is useful to provide availability estimations that better reflect reality.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0860v1"
    },
    {
        "title": "A Hierarchical Approach for Dependability Analysis of a Commercial\n  Cache-Based RAID Storage Architecture",
        "authors": [
            "Mohamed Kaaniche",
            "Luigi Romano",
            "Zbigniew Kalbarczyk",
            "Ravishankar Iyer",
            "Rick Karcich"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  We present a hierarchical simulation approach for the dependability analysis\nand evaluation of a highly available commercial cache-based RAID storage\nsystem. The archi-tecture is complex and includes several layers of\noverlap-ping error detection and recovery mechanisms. Three ab-straction levels\nhave been developed to model the cache architecture, cache operations, and\nerror detection and recovery mechanism. The impact of faults and errors\noc-curring in the cache and in the disks is analyzed at each level of the\nhierarchy. A simulation submodel is associated with each abstraction level. The\nmodels have been devel-oped using DEPEND, a simulation-based environment for\nsystem-level dependability analysis, which provides facili-ties to inject\nfaults into a functional behavior model, to simulate error detection and\nrecovery mechanisms, and to evaluate quantitative measures. Several fault\nmodels are defined for each submodel to simulate cache component failures, disk\nfailures, transmission errors, and data errors in the cache memory and in the\ndisks. Some of the parame-ters characterizing fault injection in a given\nsubmodel cor-respond to probabilities evaluated from the simulation of the\nlower-level submodel. Based on the proposed method-ology, we evaluate and\nanalyze 1) the system behavior un-der a real workload and high error rate\n(focusing on error bursts), 2) the coverage of the error detection mechanisms\nimplemented in the system and the error latency distribu-tions, and 3) the\naccumulation of errors in the cache and in the disks.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0879v1"
    },
    {
        "title": "A Technical Report On Grid Benchmarking using ATLAS V.O",
        "authors": [
            "John Kouvakis",
            "Fotis Georgatos"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  Grids include heterogeneous resources, which are based on different hardware\nand software architectures or components. In correspondence with this diversity\nof the infrastructure, the execution time of any single job, as well as the\ntotal grid performance can both be affected substantially, which can be\ndemonstrated by measurements. Running a simple benchmarking suite can show this\nheterogeneity and give us results about the differences over the grid sites.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.1915v3"
    },
    {
        "title": "Nano-Sim: A Step Wise Equivalent Conductance based Statistical Simulator\n  for Nanotechnology Circuit Design",
        "authors": [
            "Bharat Sukhwani",
            "Uday Padmanabhan",
            "Janet M. Wang"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  New nanotechnology based devices are replacing CMOS devices to overcome CMOS\ntechnology's scaling limitations. However, many such devices exhibit\nnon-monotonic I-V characteristics and uncertain properties which lead to the\nnegative differential resistance (NDR) problem and the chaotic performance.\nThis paper proposes a new circuit simulation approach that can effectively\nsimulate nanotechnology devices with uncertain input sources and negative\ndifferential resistance (NDR) problem. The experimental results show a 20-30\ntimes speedup comparing with existing simulators.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4633v1"
    },
    {
        "title": "A Prediction Packetizing Scheme for Reducing Channel Traffic in\n  Transaction-Level Hardware/Software Co-Emulation",
        "authors": [
            "Jae-Gon Lee",
            "Moo-Kyoung Chung",
            "Ki-Yong Ahn",
            "Sang-Heon Lee",
            "Chong-Min Kyung"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  This paper presents a scheme for efficient channel usage between simulator\nand accelerator where the accelerator models some RTL sub-blocks in the\naccelerator-based hardware/software co-simulation while the simulator runs\ntransaction-level model of the remaining part of the whole chip being verified.\nWith conventional simulation accelerator, evaluations of simulator and\naccelerator alternate at every valid simulation time, which results in poor\nsimulation performance due to startup overhead of simulator-accelerator channel\naccess. The startup overhead can be reduced by merging multiple transactions on\nthe channel into a single burst traffic. We propose a predictive packetizing\nscheme for reducing channel traffic by merging as many transactions into a\nburst traffic as possible based on 'prediction and rollback.' Under ideal\ncondition with 100% prediction accuracy, the proposed method shows a\nperformance gain of 1500% compared to the conventional one.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4701v1"
    },
    {
        "title": "Simulation Methodology for Analysis of Substrate Noise Impact on Analog\n  / RF Circuits Including Interconnect Resistance",
        "authors": [
            "C. Soens",
            "G. Van Der Plas",
            "P. Wambacq",
            "S. Donnay"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  This paper reports a novel simulation methodology for analysis and prediction\nof substrate noise impact on analog / RF circuits taking into account the role\nof the parasitic resistance of the on-chip interconnect in the impact\nmechanism. This methodology allows investigation of the role of the separate\ndevices (also parasitic devices) in the analog / RF circuit in the overall\nimpact. This way is revealed which devices have to be taken care of (shielding,\ntopology change) to protect the circuit against substrate noise. The developed\nmethodology is used to analyze impact of substrate noise on a 3 GHz LC-tank\nVoltage Controlled Oscillator (VCO) designed in a high-ohmic 0.18 $\\mu$m 1PM6\nCMOS technology. For this VCO (in the investigated frequency range from DC to\n15 MHz) impact is mainly caused by resistive coupling of noise from the\nsubstrate to the non-ideal on-chip ground interconnect, resulting in analog\nground bounce and frequency modulation. Hence, the presented test-case reveals\nthe important role of the on-chip interconnect in the phenomenon of substrate\nnoise impact.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.4723v1"
    },
    {
        "title": "SPARK00: A Benchmark Package for the Compiler Evaluation of\n  Irregular/Sparse Codes",
        "authors": [
            "H. L. A. van der Spek",
            "E. M. Bakker",
            "H. A. G. Wijshoff"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  We propose a set of benchmarks that specifically targets a major cause of\nperformance degradation in high performance computing platforms: irregular\naccess patterns. These benchmarks are meant to be used to asses the performance\nof optimizing compilers on codes with a varying degree of irregular access. The\nirregularity caused by the use of pointers and indirection arrays are a major\nchallenge for optimizing compilers. Codes containing such patterns are\nnotoriously hard to optimize but they have a huge impact on the performance of\nmodern architectures, which are under-utilized when encountering irregular\nmemory accesses. In this paper, a set of benchmarks is described that\nexplicitly measures the performance of kernels containing a variety of\ndifferent access patterns found in real world applications. By offering a\nvarying degree of complexity, we provide a platform for measuring the\neffectiveness of transformations. The difference in complexity stems from a\ndifference in traversal patterns, the use of multiple indirections and control\nflow statements. The kernels used cover a variety of different access patterns,\nnamely pointer traversals, indirection arrays, dynamic loop bounds and run-time\ndependent if-conditions. The kernels are small enough to be fully understood\nwhich makes this benchmark set very suitable for the evaluation of\nrestructuring transformations.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.3897v1"
    },
    {
        "title": "Asymptotic Mean Time To Failure and Higher Moments for Large, Recursive\n  Networks",
        "authors": [
            "Christian Tanguy"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  This paper deals with asymptotic expressions of the Mean Time To Failure\n(MTTF) and higher moments for large, recursive, and non-repairable systems in\nthe context of two-terminal reliability. Our aim is to extend the well-known\nresults of the series and parallel cases. We first consider several exactly\nsolvable configurations of identical components with exponential failure-time\ndistribution functions to illustrate different (logarithmic or power-law)\nbehaviors as the size of the system, indexed by an integer n, increases. The\ngeneral case is then addressed: it provides a simple interpretation of the\norigin of the power-law exponent and an efficient asymptotic expression for the\ntotal reliability of large, recursive systems. Finally, we assess the influence\nof the non-exponential character of the component reliability on the\nn-dependence of the MTTF.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.0626v1"
    },
    {
        "title": "Exact two-terminal reliability of some directed networks",
        "authors": [
            "Christian Tanguy"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  The calculation of network reliability in a probabilistic context has long\nbeen an issue of practical and academic importance. Conventional approaches\n(determination of bounds, sums of disjoint products algorithms, Monte Carlo\nevaluations, studies of the reliability polynomials, etc.) only provide\napproximations when the network's size increases, even when nodes do not fail\nand all edges have the same reliability p. We consider here a directed, generic\ngraph of arbitrary size mimicking real-life long-haul communication networks,\nand give the exact, analytical solution for the two-terminal reliability. This\nsolution involves a product of transfer matrices, in which individual\nreliabilities of edges and nodes are taken into account. The special case of\nidentical edge and node reliabilities (p and rho, respectively) is addressed.\nWe consider a case study based on a commonly-used configuration, and assess the\ninfluence of the edges being directed (or not) on various measures of network\nperformance. While the two-terminal reliability, the failure frequency and the\nfailure rate of the connection are quite similar, the locations of complex\nzeros of the two-terminal reliability polynomials exhibit strong differences,\nand various structure transitions at specific values of rho. The present work\ncould be extended to provide a catalog of exactly solvable networks in terms of\nreliability, which could be useful as building blocks for new and improved\nbounds, as well as benchmarks, in the general case.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.0629v1"
    },
    {
        "title": "WCET analysis of multi-level set-associative instruction caches",
        "authors": [
            "Damien Hardy",
            "Isabelle Puaut"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  With the advent of increasingly complex hardware in real-time embedded\nsystems (processors with performance enhancing features such as pipelines,\ncache hierarchy, multiple cores), many processors now have a set-associative L2\ncache. Thus, there is a need for considering cache hierarchies when validating\nthe temporal behavior of real-time systems, in particular when estimating\ntasks' worst-case execution times (WCETs). To the best of our knowledge, there\nis only one approach for WCET estimation for systems with cache hierarchies\n[Mueller, 1997], which turns out to be unsafe for set-associative caches. In\nthis paper, we highlight the conditions under which the approach described in\n[Mueller, 1997] is unsafe. A safe static instruction cache analysis method is\nthen presented. Contrary to [Mueller, 1997] our method supports set-associative\nand fully associative caches. The proposed method is experimented on\nmedium-size and large programs. We show that the method is most of the time\ntight. We further show that in all cases WCET estimations are much tighter when\nconsidering the cache hierarchy than when considering only the L1 cache. An\nevaluation of the analysis time is conducted, demonstrating that analysing the\ncache hierarchy has a reasonable computation time.\n",
        "pdf_link": "http://arxiv.org/pdf/0807.0993v1"
    },
    {
        "title": "Optimizing Compiler for Engineering Problems",
        "authors": [
            "Petr R. Ivankov"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  New information technologies provide a lot of prospects for performance\nimprovement. One of them is \"Dynamic Source Code Generation and Compilation\".\nThis article shows how this way provides high performance for engineering\nproblems.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.3100v1"
    },
    {
        "title": "A Model for Probabilistic Reasoning on Assume/Guarantee Contracts",
        "authors": [
            "Benoît Delahaye",
            "Benoît Caillaud"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  In this paper, we present a probabilistic adaptation of an Assume/Guarantee\ncontract formalism. For the sake of generality, we assume that the extended\nstate machines used in the contracts and implementations define sets of runs on\na given set of variables, that compose by intersection over the common\nvariables. In order to enable probabilistic reasoning, we consider that the\ncontracts dictate how certain input variables will behave, being either\nnon-deterministic, or probabilistic; the introduction of probabilistic\nvariables leading us to tune the notions of implementation, refinement and\ncomposition. As shown in the report, this probabilistic adaptation of the\nAssume/Guarantee contract theory preserves compositionality and therefore\nallows modular reliability analysis, either with a top-down or a bottom-up\napproach.\n",
        "pdf_link": "http://arxiv.org/pdf/0811.1151v1"
    },
    {
        "title": "An Approximation of the Outage Probability for Multi-hop AF Fixed Gain\n  Relay",
        "authors": [
            "Jun Kyoung Lee",
            "Janghoon Yang",
            "Dong Ku Kim"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  In this letter, we present a closed-form approximation of the outage\nprobability for the multi-hop amplify-and-forward (AF) relaying systems with\nfixed gain in Rayleigh fading channel. The approximation is derived from the\noutage event for each hop. The simulation results show the tightness of the\nproposed approximation in low and high signal-to-noise ratio (SNR) region.\n",
        "pdf_link": "http://arxiv.org/pdf/0812.0904v1"
    },
    {
        "title": "Using constraint programming to resolve the multi-source/multi-site data\n  movement paradigm on the Grid",
        "authors": [
            "Michal Zerola",
            "Jerome Lauret",
            "Roman Bartak",
            "Michal Sumbera"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  In order to achieve both fast and coordinated data transfer to collaborative\nsites as well as to create a distribution of data over multiple sites,\nefficient data movement is one of the most essential aspects in distributed\nenvironment. With such capabilities at hand, truly distributed task scheduling\nwith minimal latencies would be reachable by internationally distributed\ncollaborations (such as ones in HENP) seeking for scavenging or maximizing on\ngeographically spread computational resources. But it is often not all clear\n(a) how to move data when available from multiple sources or (b) how to move\ndata to multiple compute resources to achieve an optimal usage of available\nresources. We present a method of creating a Constraint Programming (CP) model\nconsisting of sites, links and their attributes such as bandwidth for grid\nnetwork data transfer also considering user tasks as part of the objective\nfunction for an optimal solution. We will explore and explain trade-off between\nschedule generation time and divergence from the optimal solution and show how\nto improve and render viable the solution's finding time by using search tree\ntime limit, approximations, restrictions such as symmetry breaking or grouping\nsimilar tasks together, or generating sequence of optimal schedules by\nsplitting the input problem. Results of data transfer simulation for each case\nwill also include a well known Peer-2-Peer model, and time taken to generate a\nschedule as well as time needed for a schedule execution will be compared to a\nCP optimal solution. We will additionally present a possible implementation\naimed to bring a distributed datasets (multiple sources) to a given site in a\nminimal time.\n",
        "pdf_link": "http://arxiv.org/pdf/0901.0148v1"
    },
    {
        "title": "Towards a Statistical Methodology to Evaluate Program Speedups and their\n  Optimisation Techniques",
        "authors": [
            "Sid Touati"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  The community of program optimisation and analysis, code performance\nevaluation, parallelisation and optimising compilation has published since many\ndecades hundreds of research and engineering articles in major conferences and\njournals. These articles study efficient algorithms, strategies and techniques\nto accelerate programs execution times, or optimise other performance metrics\n(MIPS, code size, energy/power, MFLOPS, etc.). Many speedups are published, but\nnobody is able to reproduce them exactly. The non-reproducibility of our\nresearch results is a dark point of the art, and we cannot be qualified as {\\it\ncomputer scientists} if we do not provide rigorous experimental methodology.\nThis article provides a first effort towards a correct statistical protocol for\nanalysing and measuring speedups. As we will see, some common mistakes are done\nby the community inside published articles, explaining part of the\nnon-reproducibility of the results. Our current article is not sufficient by\nits own to deliver a complete experimental methodology, further efforts must be\ndone by the community to decide about a common protocol for our future\nexperiences. Anyway, our community should take care about the aspect of\nreproducibility of the results in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.1035v8"
    },
    {
        "title": "The Multi-Branched Method of Moments for Queueing Networks",
        "authors": [
            "Giuliano Casale"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  We propose a new exact solution algorithm for closed multiclass product-form\nqueueing networks that is several orders of magnitude faster and less memory\nconsuming than established methods for multiclass models, such as the Mean\nValue Analysis (MVA) algorithm. The technique is an important generalization of\nthe recently proposed Method of Moments (MoM) which, differently from MVA,\nrecursively computes higher-order moments of queue-lengths instead of mean\nvalues.\n  The main contribution of this paper is to prove that the information used in\nthe MoM recursion can be increased by considering multiple recursive branches\nthat evaluate models with different number of queues. This reformulation allows\nto formulate a simpler matrix difference equation which leads to large\ncomputational savings with respect to the original MoM recursion. Computational\nanalysis shows several cases where the proposed algorithm is between 1,000 and\n10,000 times faster and less memory consuming than the original MoM, thus\nextending the range of multiclass models where exact solutions are feasible.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.3065v1"
    },
    {
        "title": "Methodology for assessing system performance loss within a proactive\n  maintenance framework",
        "authors": [
            "Pierre Cocheteux",
            "Alexandre Voisin",
            "Eric Levrat",
            "Benoît Iung"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  Maintenance plays now a critical role in manufacturing for achieving\nimportant cost savings and competitive advantage while preserving product\nconditions. It suggests moving from conventional maintenance practices to\npredictive strategy. Indeed the maintenance action has to be done at the right\ntime based on the system performance and component Remaining Useful Life (RUL)\nassessed by a prognostic process. In that way, this paper proposes a\nmethodology in order to evaluate the performance loss of the system according\nto the degradation of component and the deviations of system input flows. This\nmethodology is supported by the neuro-fuzzy tool ANFIS (Adaptive Neuro-Fuzzy\nInference Systems) that allows to integrate knowledge from two different\nsources: expertise and real data. The feasibility and added value of such\nmethodology is then highlighted through an application case extracted from the\nTELMA platform used for education and research.\n",
        "pdf_link": "http://arxiv.org/pdf/0906.1680v1"
    },
    {
        "title": "Performance of Network and Service Monitoring Frameworks",
        "authors": [
            "Abdelkader Lahmadi",
            "Laurent Andrey",
            "Olivier Festor"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  The efficiency and the performance of anagement systems is becoming a hot\nresearch topic within the networks and services management community. This\nconcern is due to the new challenges of large scale managed systems, where the\nmanagement plane is integrated within the functional plane and where management\nactivities have to carry accurate and up-to-date information. We defined a set\nof primary and secondary metrics to measure the performance of a management\napproach. Secondary metrics are derived from the primary ones and quantifies\nmainly the efficiency, the scalability and the impact of management activities.\nTo validate our proposals, we have designed and developed a benchmarking\nplatform dedicated to the measurement of the performance of a JMX manager-agent\nbased management system. The second part of our work deals with the collection\nof measurement data sets from our JMX benchmarking platform. We mainly studied\nthe effect of both load and the number of agents on the scalability, the impact\nof management activities on the user perceived performance of a managed server\nand the delays of JMX operations when carrying variables values. Our findings\nshow that most of these delays follow a Weibull statistical distribution. We\nused this statistical model to study the behavior of a monitoring algorithm\nproposed in the literature, under heavy tail delays distribution. In this case,\nthe view of the managed system on the manager side becomes noisy and out of\ndate.\n",
        "pdf_link": "http://arxiv.org/pdf/0907.3047v1"
    },
    {
        "title": "Transmission Performance Analysis of Digital Wire and Wireless Optical\n  Links in Local and Wide Areas Optical Networks",
        "authors": [
            "Abd El Naser A. Mohamed",
            "Mohamed M. E. El Halawany",
            "Ahmed Nabih Zaki Rashed",
            "Amina E. M. El Nabawy"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  In the present paper, the transmission performance analysis of digital wire\nand wireless optical links in local and wide areas optical networks have been\nmodeled and parametrically investigated over wide range of the affecting\nparameters. Moreover, we have analyzed the basic equations of the comparative\nstudy of the performance of digital fiber optic links with wire and wireless\noptical links. The development of optical wireless communication systems is\naccelerating as a high cost effective to wire fiber optic links. The optical\nwireless technology is used mostly in wide bandwidth data transmission\napplications. Finally, we have investigated the maximum transmission distance\nand data transmission bit rates that can be achieved within digital wire and\nwireless optical links for local and wide areas optical network applications.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.1057v1"
    },
    {
        "title": "Experimental Performances Analysis of Load Balancing Algorithms in IEEE\n  802.11",
        "authors": [
            "Hamdi Salah",
            "Soudani Adel",
            "Tourki Rached"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  In IEEE 802.11, load balancing algorithms (LBA) consider only the associated\nstations to balance the load of the available access points (APs). However,\nalthough the APs are balanced, it causes a bad situation if the AP has a lower\nsignal length (SNR) less than the neighbor APs. So, balance the load and\nassociate one mobile station to an access point without care about the signal\nto noise ratio (SNR) of the AP cause possibly an unforeseen QoS, such as the\nbit rate, the end to end delay, the packet loss. In this way, we study an\nimprovement load balancing algorithm with SNR integration at the selection\npolicy.\n",
        "pdf_link": "http://arxiv.org/pdf/0908.4256v1"
    },
    {
        "title": "uFLIP: Understanding Flash IO Patterns",
        "authors": [
            "Luc Bouganim",
            "Björn Jónsson",
            "Philippe Bonnet"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  Does the advent of flash devices constitute a radical change for secondary\nstorage? How should database systems adapt to this new form of secondary\nstorage? Before we can answer these questions, we need to fully understand the\nperformance characteristics of flash devices. More specifically, we want to\nestablish what kind of IOs should be favored (or avoided) when designing\nalgorithms and architectures for flash-based systems. In this paper, we focus\non flash IO patterns, that capture relevant distribution of IOs in time and\nspace, and our goal is to quantify their performance. We define uFLIP, a\nbenchmark for measuring the response time of flash IO patterns. We also present\na benchmarking methodology which takes into account the particular\ncharacteristics of flash devices. Finally, we present the results obtained by\nmeasuring eleven flash devices, and derive a set of design hints that should\ndrive the development of flash-based systems on current devices.\n",
        "pdf_link": "http://arxiv.org/pdf/0909.1780v1"
    },
    {
        "title": "Performance Evaluation of Wimax Physical Layer under Adaptive Modulation\n  Techniques and Communication Channels",
        "authors": [
            "Md. Ashraful Islam",
            "Riaz Uddin Mondal",
            "Md. Zahid Hasan"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  Wimax (Worldwide Interoperability for Microwave Access) is a promising\ntechnology which can offer high speed voice, video and data service up to the\ncustomer end. The aim of this paper is the performance evaluation of an Wimax\nsystem under different combinations of digital modulation (BPSK, QPSK, 4 QAM\nand 16 QAM) and different communication channels AWGN and fading channels\n(Rayleigh and Rician). And the Wimax system incorporates Reed Solomon (RS)\nencoder with Convolutional encoder with half and two third rated codes in FEC\nchannel coding. The simulation results of estimated Bit Error Rate (BER)\ndisplays that the implementation of interleaved RS code (255, 239, 8) with two\nthird rated Convolutional code under BPSK modulation technique is highly\neffective to combat in the Wimax communication system. To complete this\nperformance analysis in Wimax based systems, a segment of audio signal is used\nfor analysis. The transmitted audio message is found to have retrieved\neffectively under noisy situation.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.0819v1"
    },
    {
        "title": "Performance limitations for sparse matrix-vector multiplications on\n  current multicore environments",
        "authors": [
            "Gerald Schubert",
            "Georg Hager",
            "Holger Fehske"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  The increasing importance of multicore processors calls for a reevaluation of\nestablished numerical algorithms in view of their ability to profit from this\nnew hardware concept. In order to optimize the existent algorithms, a detailed\nknowledge of the different performance-limiting factors is mandatory. In this\ncontribution we investigate sparse matrix-vector multiplication, which is the\ndominant operation in many sparse eigenvalue solvers. Two conceptually\ndifferent storage schemes and computational kernels have been conceived in the\npast to target cache-based and vector architectures, respectively. Starting\nfrom a series of microbenchmarks we apply the gained insight on optimized\nsparse MVM implementations, whose serial and OpenMP-parallel performance we\nreview on state-of-the-art multicore systems.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.4836v1"
    },
    {
        "title": "OMI4papps: Optimisation, Modelling and Implementation for Highly\n  Parallel Applications",
        "authors": [
            "Volker Weinberg",
            "Matthias Brehm",
            "Iris Christadler"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  This article reports on first results of the KONWIHR-II project OMI4papps at\nthe Leibniz Supercomputing Centre (LRZ). The first part describes Apex-MAP, a\ntunable synthetic benchmark designed to simulate the performance of typical\nscientific applications. Apex-MAP mimics common memory access patterns and\ndifferent computational intensity of scientific codes. An approach for\nmodelling LRZ's application mix is given whichh makes use of performance\ncounter measurements of real applications running on \"HLRB II\", an SGI Altix\nsystem based on 9728 Intel Montecito dual-cores.\n  The second part will show how the Apex-MAP benchmark could be used to\nsimulate the performance of two mathematical kernels frequently used in\nscientific applications: a dense matrix-matrix multiplication and a sparse\nmatrix-vector multiplication. The performance of both kernels has been\nintensively studied on x86 cores and hardware accelerators. We will compare the\npredicted performance with measured data to validate our Apex-MAP approach.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1860v2"
    },
    {
        "title": "Fault Tolerant Real Time Systems",
        "authors": [
            "A. Christy Persya",
            "T. R. Gopalakrishnan Nair"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Real time systems are systems in which there is a commitment for timely\nresponse by the computer to external stimuli. Real time applications have to\nfunction correctly even in presence of faults. Fault tolerance can be achieved\nby either hardware or software or time redundancy. Safety-critical applications\nhave strict time and cost constraints, which means that not only faults have to\nbe tolerated but also the constraints should be satisfied. Deadline scheduling\nmeans that the taskwith the earliest required response time is processed. The\nmost common scheduling algorithms are :Rate Monotonic(RM) and Earliest deadline\nfirst(EDF).This paper deals with the interaction between the fault tolerant\nstrategy and the EDF real time scheduling strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.3756v1"
    },
    {
        "title": "A Performance Study of GA and LSH in Multiprocessor Job Scheduling",
        "authors": [
            "S. R. Vijayalakshmi",
            "G. Padmavathi"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Multiprocessor task scheduling is an important and computationally difficult\nproblem. This paper proposes a comparison study of genetic algorithm and list\nscheduling algorithm. Both algorithms are naturally parallelizable but have\nheavy data dependencies. Based on experimental results, this paper presents a\ndetailed analysis of the scalability, advantages and disadvantages of each\nalgorithm. Multiprocessors have emerged as a powerful computing means for\nrunning real-time applications, especially where a uni-processor system would\nnot be sufficient enough to execute all the tasks. The high performance and\nreliability of multiprocessors have made them a powerful computing resource.\nSuch computing environment requires an efficient algorithm to determine when\nand on which processor a given task should execute. In multiprocessor systems,\nan efficient scheduling of a parallel program onto the processors that\nminimizes the entire execution time is vital for achieving a high performance.\nThis scheduling problem is known to be NP- Hard. In multiprocessor scheduling\nproblem, a given program is to be scheduled in a given multiprocessor system\nsuch that the program's execution time is minimized. The last job must be\ncompleted as early as possible. Genetic algorithm (GA) is one of the widely\nused techniques for constrained optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1149v1"
    },
    {
        "title": "Performance Analysis of Software to Hardware Task Migration in Codesign",
        "authors": [
            "Dorsaf Sebai",
            "Abderrazak Jemai",
            "Imed Bennour"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  The complexity of multimedia applications in terms of intensity of\ncomputation and heterogeneity of treated data led the designers to embark them\non multiprocessor systems on chip. The complexity of these systems on one hand\nand the expectations of the consumers on the other hand complicate the\ndesigners job to conceive and supply strong and successful systems in the\nshortest deadlines. They have to explore the different solutions of the design\nspace and estimate their performances in order to deduce the solution that\nrespects their design constraints. In this context, we propose the modeling of\none of the design space possible solutions: the software to hardware task\nmigration. This modeling exploits the synchronous dataflow graphs to take into\naccount the different migration impacts and estimate their performances in\nterms of throughput.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.1154v1"
    },
    {
        "title": "A Rank Based Replacement Policy for Multimedia Server Cache Using\n  Zipf-Like Law",
        "authors": [
            "T R Gopalakrishnan Nair",
            "P Jayarekha"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  The cache replacement algorithm plays an important role in the overall\nperformance of Proxy-Server system. In this paper we have proposed VoD cache\nmemory replacement algorithm for a multimedia server system. We propose a Rank\nbased cache replacement policy to manage the cache space in individual proxy\nserver cache. Proposed replacement strategy incorporates in a simple way the\nmost important characteristics of the video and its accesses such as its size,\naccess frequency, recentness of the last access and the cost incurred while\ntransferring the requested video from the server to the proxy. We compare our\nalgorithm with some popular cache replacement algorithm using simulation. The\nvideo objects are ranked based on the access trend by considering the factors\nsuch as size, frequency and cost. Many studies have demonstrated that\nZipf's-like law can govern many features of the VoD and is used to describe the\npopularity of the video. In this paper, we have designed a model, which ranks\nthe video on the basis of its popularity using the Zipf-like law. The video\nwith higher ranking is named \"hot\", while the video with lower ranking is named\n\"cold\". The result show that the proposed rank based algorithm improves cache\nhit ratio, cache byte ratio and average request latencies compared to other\nalgorithms. Our experimental results indicate that Rank based cache replacement\nalgorithm outperforms LRU, LFU and Greedy Dual.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4062v1"
    },
    {
        "title": "Measuring Bandwidth for Super Computer Workloads",
        "authors": [
            "A. Neela Madheswari",
            "R. S. D. Wahida Banu"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Parallel computing plays a major role in almost all the fields from research\nto major concern problem solving purposes. Many researches are till now\nfocusing towards the area of parallel processing. Nowadays it extends its usage\ntowards the end user application such as GPU as well as multi-core processor\ndevelopment. The bandwidth measurement is essential for resource management and\nfor studying the various performance factors of the existing super computer\nsystems which will be helpful for better system utilization since super\ncomputers are very few and their resources should be properly utilized. In this\npaper the real workload trace of one of the super computers LANL is taken and\nshown how the bandwidth is estimated with the given parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1003.4064v1"
    },
    {
        "title": "Performance Evaluation of Components Using a Granularity-based Interface\n  Between Real-Time Calculus and Timed Automata",
        "authors": [
            "Karine Altisen",
            "Yanhong Liu",
            "Matthieu Moy"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  To analyze complex and heterogeneous real-time embedded systems, recent works\nhave proposed interface techniques between real-time calculus (RTC) and timed\nautomata (TA), in order to take advantage of the strengths of each technique\nfor analyzing various components. But the time to analyze a state-based\ncomponent modeled by TA may be prohibitively high, due to the state space\nexplosion problem. In this paper, we propose a framework of granularity-based\ninterfacing to speed up the analysis of a TA modeled component. First, we\nabstract fine models to work with event streams at coarse granularity. We\nperform analysis of the component at multiple coarse granularities and then\nbased on RTC theory, we derive lower and upper bounds on arrival patterns of\nthe fine output streams using the causality closure algorithm. Our framework\ncan help to achieve tradeoffs between precision and analysis time.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.2637v1"
    },
    {
        "title": "Applying Stochastic Network Calculus to 802.11 Backlog and Delay\n  Analysis",
        "authors": [
            "Yue Wang"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Stochastic network calculus provides an elegant way to characterize traffic\nand service processes. However, little effort has been made on applying it to\nmulti-access communication systems such as 802.11. In this paper, we take the\nfirst step to apply it to the backlog and delay analysis of an 802.11 wireless\nlocal network. In particular, we address the following questions: In applying\nstochastic network calculus, under what situations can we derive stable backlog\nand delay bounds? How to derive the backlog and delay bounds of an 802.11\nwireless node? And how tight are these bounds when compared with simulations?\nTo answer these questions, we first derive the general stability condition of a\nwireless node (not restricted to 802.11). From this, we give the specific\nstability condition of an 802.11 wireless node. Then we derive the backlog and\ndelay bounds of an 802.11 node based on an existing model of 802.11. We observe\nthat the derived bounds are loose when compared with ns-2 simulations,\nindicating that improvements are needed in the current version of stochastic\nnetwork calculus.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3109v1"
    },
    {
        "title": "Automatic Mapping Tasks to Cores - Evaluating AMTHA Algorithm in\n  Multicore Architectures",
        "authors": [
            "Laura De Giusti",
            "Franco Chichizola",
            "Marcelo Naiouf",
            "Armando De Giusti",
            "Emilio Luque"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  The AMTHA (Automatic Mapping Task on Heterogeneous Architectures) algorithm\nfor task-to-processors assignment and the MPAHA (Model of Parallel Algorithms\non Heterogeneous Architectures) model are presented. The use of AMTHA is\nanalyzed for multicore processor-based architectures, considering the\ncommunication model among processes in use. The results obtained in the tests\ncarried out are presented, comparing the real execution times on multicores of\na set of synthetic applications with the predictions obtained with AMTHA.\nFinally current lines of research are presented, focusing on clusters of\nmulticores and hybrid programming paradigms.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3254v1"
    },
    {
        "title": "Comparison of the Performance of Two Service Disciplines for a Shared\n  Bus Multiprocessor with Private Caches",
        "authors": [
            "Angel Vassilev Nikolov",
            "Lerato Lerato"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  In this paper, we compare two analytical models for evaluation of cache\ncoherence overhead of a shared bus multiprocessor with private caches. The\nmodels are based on a closed queuing network with different service\ndisciplines. We find that the priority discipline can be used as a lower-level\nbound. Some numerical results are shown graphically.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.3560v1"
    },
    {
        "title": "Space-efficient scheduling of stochastically generated tasks",
        "authors": [
            "Tomáš Brázdil",
            "Javier Esparza",
            "Stefan Kiefer",
            "Michael Luttenberger"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  We study the problem of scheduling tasks for execution by a processor when\nthe tasks can stochastically generate new tasks. Tasks can be of different\ntypes, and each type has a fixed, known probability of generating other tasks.\nWe present results on the random variable S^sigma modeling the maximal space\nneeded by the processor to store the currently active tasks when acting under\nthe scheduler sigma. We obtain tail bounds for the distribution of S^sigma for\nboth offline and online schedulers, and investigate the expected value of\nS^sigma.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.4286v2"
    },
    {
        "title": "A New Benchmark For Evaluation Of Graph-Theoretic Algorithms",
        "authors": [
            "Andy B. Yoo",
            "Yang Liu",
            "Sheila Vaidya",
            "Stephen Poole"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  We propose a new graph-theoretic benchmark in this paper. The benchmark is\ndeveloped to address shortcomings of an existing widely-used graph benchmark.\nWe thoroughly studied a large number of traditional and contemporary graph\nalgorithms reported in the literature to have clear understanding of their\nalgorithmic and run-time characteristics. Based on this study, we designed a\nsuite of kernels, each of which represents a specific class of graph\nalgorithms. The kernels are designed to capture the typical run-time behavior\nof target algorithms accurately, while limiting computational and spatial\noverhead to ensure its computation finishes in reasonable time. We expect that\nthe developed benchmark will serve as a much needed tool for evaluating\ndifferent architectures and programming models to run graph algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.0806v1"
    },
    {
        "title": "Analyzing the Performance of Active Queue Management Algorithms",
        "authors": [
            "G. F. Ali Ahammed",
            "Reshma Banu"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Congestion is an important issue which researchers focus on in the\nTransmission Control Protocol (TCP) network environment. To keep the stability\nof the whole network, congestion control algorithms have been extensively\nstudied. Queue management method employed by the routers is one of the\nimportant issues in the congestion control study. Active queue management (AQM)\nhas been proposed as a router-based mechanism for early detection of congestion\ninside the network. In this paper we analyzed several active queue management\nalgorithms with respect to their abilities of maintaining high resource\nutilization, identifying and restricting disproportionate bandwidth usage, and\ntheir deployment complexity. We compare the performance of FRED, BLUE, SFB, and\nCHOKe based on simulation results, using RED and Drop Tail as the evaluation\nbaseline. The characteristics of different algorithms are also discussed and\ncompared. Simulation is done by using Network Simulator(NS2) and the graphs are\ndrawn using X- graph.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.1992v1"
    },
    {
        "title": "Seeing Through Black Boxes : Tracking Transactions through Queues under\n  Monitoring Resource Constraints",
        "authors": [
            "Animashree Anandkumar",
            "Ting He",
            "Chatschik Bisdikian",
            "Dakshi Agrawal"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  The problem of optimal allocation of monitoring resources for tracking\ntransactions progressing through a distributed system, modeled as a queueing\nnetwork, is considered. Two forms of monitoring information are considered,\nviz., locally unique transaction identifiers, and arrival and departure\ntimestamps of transactions at each processing queue. The timestamps are assumed\navailable at all the queues but in the absence of identifiers, only enable\nimprecise tracking since parallel processing can result in out-of-order\ndepartures. On the other hand, identifiers enable precise tracking but are not\navailable without proper instrumentation. Given an instrumentation budget, only\na subset of queues can be selected for production of identifiers, while the\nremaining queues have to resort to imprecise tracking using timestamps. The\ngoal is then to optimally allocate the instrumentation budget to maximize the\noverall tracking accuracy. The challenge is that the optimal allocation\nstrategy depends on accuracies of timestamp-based tracking at different queues,\nwhich has complex dependencies on the arrival and service processes, and the\nqueueing discipline. We propose two simple heuristics for allocation by\npredicting the order of timestamp-based tracking accuracies of different\nqueues. We derive sufficient conditions for these heuristics to achieve\noptimality through the notion of stochastic comparison of queues. Simulations\nshow that our heuristics are close to optimality, even when the parameters\ndeviate from these conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.1674v2"
    },
    {
        "title": "Decentralized Fair Scheduling in Two-Hop Relay-Assisted Cognitive OFDMA\n  Systems",
        "authors": [
            "Rui Wang",
            "Vincent K. N. Lau",
            "Ying Cui"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  In this paper, we consider a two-hop relay-assisted cognitive downlink OFDMA\nsystem (named as secondary system) dynamically accessing a spectrum licensed to\na primary network, thereby improving the efficiency of spectrum usage. A\ncluster-based relay-assisted architecture is proposed for the secondary system,\nwhere relay stations are employed for minimizing the interference to the users\nin the primary network and achieving fairness for cell-edge users. Based on\nthis architecture, an asymptotically optimal solution is derived for jointly\ncontrolling data rates, transmission power, and subchannel allocation to\noptimize the average weighted sum goodput where the proportional fair\nscheduling (PFS) is included as a special case. This solution supports\ndecentralized implementation, requires small communication overhead, and is\nrobust against imperfect channel state information at the transmitter (CSIT)\nand sensing measurement. The proposed solution achieves significant throughput\ngains and better user-fairness compared with the existing designs. Finally, we\nderived a simple and asymptotically optimal scheduling solution as well as the\nassociated closed-form performance under the proportional fair scheduling for a\nlarge number of users. The system throughput is shown to be\n$\\mathcal{O}\\left(N(1-q_p)(1-q_p^N)\\ln\\ln K_c\\right)$, where $K_c$ is the\nnumber of users in one cluster, $N$ is the number of subchannels and $q_p$ is\nthe active probability of primary users.\n",
        "pdf_link": "http://arxiv.org/pdf/1006.4824v1"
    },
    {
        "title": "On the Performance Evaluation and Analysis of the Hybridised Bittorrent\n  Protocol with Partial Mobility Characteristics",
        "authors": [
            "George C. Violaris",
            "Constandinos X. Mavromoustakis"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Engaging mobility with file sharing is considered very promising in today's\nrun Anywhere, Anytime, Anything (3As) environments. The Bittorrent file sharing\nprotocol can be rarely combined with the mobility scenario framework since\nresources are not available due to the dynamically changing topology network.\nAs a result, mobility in P2P-oriented file sharing platforms, degrades the\nend-to-end efficiency and the system's performance. This work proposes a new\nhybridized model, which takes into account the mobility characteristics of the\ncombined Bittorrent protocol in a centralized manner enabling partial mobility\ncharacteristics, where the clients of the network use a distinct technique to\ndifferentiate between mobile and static nodes. Many parameters were taken into\nconsideration like the round trip delays, the diffusion process, and the\nseeding techniques, targeting the maximization of the average throughput in the\nclustered swarms containing mobile peers. Partial mobility characteristics are\nset in a peer-tracker and peer-peer communication enhancement schema with\npartial mobility, allowing an optimistic approach to attain high availability\nand throughput response as simulation results show.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.1708v1"
    },
    {
        "title": "Forever Young: Aging Control For Smartphones In Hybrid Networks",
        "authors": [
            "Eitan Altman",
            "Rachid El-Azouzi",
            "Daniel Sadoc Menasche",
            "Yuedong Xu"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  The demand for Internet services that require frequent updates through small\nmessages, such as microblogging, has tremendously grown in the past few years.\nAlthough the use of such applications by domestic users is usually free, their\naccess from mobile devices is subject to fees and consumes energy from limited\nbatteries. If a user activates his mobile device and is in range of a service\nprovider, a content update is received at the expense of monetary and energy\ncosts. Thus, users face a tradeoff between such costs and their messages aging.\nThe goal of this paper is to show how to cope with such a tradeoff, by devising\n\\emph{aging control policies}. An aging control policy consists of deciding,\nbased on the current utility of the last message received, whether to activate\nthe mobile device, and if so, which technology to use (WiFi or 3G). We present\na model that yields the optimal aging control policy. Our model is based on a\nMarkov Decision Process in which states correspond to message ages. Using our\nmodel, we show the existence of an optimal strategy in the class of threshold\nstrategies, wherein users activate their mobile devices if the age of their\nmessages surpasses a given threshold and remain inactive otherwise. We then\nconsider strategic content providers (publishers) that offer \\emph{bonus\npackages} to users, so as to incent them to download updates of advertisement\ncampaigns. We provide simple algorithms for publishers to determine optimal\nbonus levels, leveraging the fact that users adopt their optimal aging control\nstrategies. The accuracy of our model is validated against traces from the\nUMass DieselNet bus network.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.4733v3"
    },
    {
        "title": "Performance analysis of Xen virtual machines in real-world scenarios",
        "authors": [
            "Adrian Heissler"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  This paper presents results of the performance benchmarks of the Open Source\nhypervisor Xen. The study focuses on the network related performance as well as\non the application related performance of multiple virtual machines that were\nrunning on the same Xen hypervisor. The comparison was carried out using a\nself-developed benchmark suite that consists of easily available Open Source\ntools. The goal is to measure the performance of the hypervisor in typical\nreal-world application scenarios when used for \"mass virtual hosting\", such as\nhosting solutions of so called virtual private servers for small-to-medium\nsized businesses environments. The results of the benchmarks show, that the\ntested Xen setup offers good performance with respect to network traffic stress\ntests, but only 75% of the performance of the non-virtualized reference\nenvironment. This application performance score decreases as more virtual\nmachines are running simultaneously.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.5878v1"
    },
    {
        "title": "Low Power Reversible Parallel Binary Adder/Subtractor",
        "authors": [
            "H G Rangaraju",
            "U. Venugopal",
            "K N Muralidhara",
            "K B Raja"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  In recent years, Reversible Logic is becoming more and more prominent\ntechnology having its applications in Low Power CMOS, Quantum Computing,\nNanotechnology, and Optical Computing. Reversibility plays an important role\nwhen energy efficient computations are considered. In this paper, Reversible\neight-bit Parallel Binary Adder/Subtractor with Design I, Design II and Design\nIII are proposed. In all the three design approaches, the full Adder and\nSubtractors are realized in a single unit as compared to only full Subtractor\nin the existing design. The performance analysis is verified using number\nreversible gates, Garbage input/outputs and Quantum Cost. It is observed that\nReversible eight-bit Parallel Binary Adder/Subtractor with Design III is\nefficient compared to Design I, Design II and existing design.\n",
        "pdf_link": "http://arxiv.org/pdf/1009.6218v1"
    },
    {
        "title": "A framework to experiment optimizations for real-time and embedded\n  software",
        "authors": [
            "Hugues Cassé",
            "Karine Heydemann",
            "Haluk Ozaktas",
            "Jonathan Ponroy",
            "Christine Rochange",
            "Olivier Zendra"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Typical constraints on embedded systems include code size limits, upper\nbounds on energy consumption and hard or soft deadlines. To meet these\nrequirements, it may be necessary to improve the software by applying various\nkinds of transformations like compiler optimizations, specific mapping of code\nand data in the available memories, code compression, etc. However, a\ntransformation that aims at improving the software with respect to a given\ncriterion might engender side effects on other criteria and these effects must\nbe carefully analyzed. For this purpose, we have developed a common framework\nthat makes it possible to experiment various code transfor-mations and to\nevaluate their impact of various criteria. This work has been carried out\nwithin the French ANR MORE project.\n",
        "pdf_link": "http://arxiv.org/pdf/1011.6031v1"
    },
    {
        "title": "Parallel sparse matrix-vector multiplication as a test case for hybrid\n  MPI+OpenMP programming",
        "authors": [
            "Gerald Schubert",
            "Georg Hager",
            "Holger Fehske",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  We evaluate optimized parallel sparse matrix-vector operations for two\nrepresentative application areas on widespread multicore-based cluster\nconfigurations. First the single-socket baseline performance is analyzed and\nmodeled with respect to basic architectural properties of standard multicore\nchips. Going beyond the single node, parallel sparse matrix-vector operations\noften suffer from an unfavorable communication to computation ratio. Starting\nfrom the observation that nonblocking MPI is not able to hide communication\ncost using standard MPI implementations, we demonstrate that explicit overlap\nof communication and computation can be achieved by using a dedicated\ncommunication thread, which may run on a virtual core. We compare our approach\nto pure MPI and the widely used \"vector-like\" hybrid programming strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.0091v1"
    },
    {
        "title": "Accurate Performance Analysis of Opportunistic Decode-and-Forward\n  Relaying",
        "authors": [
            "Kamel Tourki",
            "Hong-Chuan Yang",
            "Mohamed-Slim Alouni"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  In this paper, we investigate an opportunistic relaying scheme where the\nselected relay assists the source-destination (direct) communication. In our\nstudy, we consider a regenerative opportunistic relaying scheme in which the\ndirect path can be considered unusable, and takes into account the effect of\nthe possible erroneously detected and transmitted data at the best relay. We\nfirst derive statistics based on exact probability density function (PDF) of\neach hop. Then, the PDFs are used to determine accurate closed form expressions\nfor end-to-end bit-error rate (BER) of binary phase-shift keying (BPSK)\nmodulation. Furthermore, we evaluate the asymptotical performance analysis and\nthe diversity order is deduced. Finally, we validate our analysis by showing\nthat performance simulation results coincide with our analytical results over\ndifferent network architectures.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.5335v1"
    },
    {
        "title": "Scheduling in a random environment: stability and asymptotic optimality",
        "authors": [
            "U. Ayesta",
            "M. Erausquin",
            "M. Jonckheere",
            "I. M. Verloop"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  We investigate the scheduling of a common resource between several concurrent\nusers when the feasible transmission rate of each user varies randomly over\ntime. Time is slotted and users arrive and depart upon service completion. This\nmay model for example the flow-level behavior of end-users in a narrowband HDR\nwireless channel (CDMA 1xEV-DO). As performance criteria we consider the\nstability of the system and the mean delay experienced by the users. Given the\ncomplexity of the problem we investigate the fluid-scaled system, which allows\nto obtain important results and insights for the original system: (1) We\ncharacterize for a large class of scheduling policies the stability conditions\nand identify a set of maximum stable policies, giving in each time slot\npreference to users being in their best possible channel condition. We find in\nparticular that many opportunistic scheduling policies like Score-Based,\nProportionally Best or Potential Improvement are stable under the maximum\nstability conditions, whereas the opportunistic scheduler Relative-Best or the\ncmu-rule are not. (2) We show that choosing the right tie-breaking rule is\ncrucial for the performance (e.g. average delay) as perceived by a user. We\nprove that a policy is asymptotically optimal if it is maximum stable and the\ntie-breaking rule gives priority to the user with the highest departure\nprobability. We will refer to such tie-breaking rule as myopic. (3) We derive\nthe growth rates of the number of users in the system in overload settings\nunder various policies, which give additional insights on the performance. (4)\nWe conclude that simple priority-index policies with the myopic tie-breaking\nrule, are stable and asymptotically optimal. All our findings are validated\nwith extensive numerical experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1101.5794v1"
    },
    {
        "title": "Measuring NUMA effects with the STREAM benchmark",
        "authors": [
            "Lars Bergstrom"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Modern high-end machines feature multiple processor packages, each of which\ncontains multiple independent cores and integrated memory controllers connected\ndirectly to dedicated physical RAM. These packages are connected via a shared\nbus, creating a system with a heterogeneous memory hierarchy. Since this shared\nbus has less bandwidth than the sum of the links to memory, aggregate memory\nbandwidth is higher when parallel threads all access memory local to their\nprocessor package than when they access memory attached to a remote package.\n  But, the impact of this heterogeneous memory architecture is not easily\nunderstood from vendor benchmarks. Even where these measurements are available,\nthey provide only best-case memory throughput. This work presents a series of\nmodifications to the well-known STREAM benchmark to measure the effects of NUMA\non both a 48-core AMD Opteron machine and a 32-core Intel Xeon machine.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.3225v1"
    },
    {
        "title": "Performance Analysis of Sequential Method for HandOver in Cognitive\n  Radio Networks",
        "authors": [
            "Hossein Shokri",
            "Mohammad Mozaffari",
            "Adnan Gavili",
            "Masoumeh Nasiri-Kenari"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  This paper has been withdrawn by the author due to a crucial problem in Lemma\n3. This equation must be changed.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.0880v2"
    },
    {
        "title": "Mathematical Model for the Optimal Utilization Percentile in M/M/1\n  Systems: A Contribution about Knees in Performance Curves",
        "authors": [
            "Francisco A. Gonzalez-Horta",
            "Rogerio A. Enriquez-Caldera",
            "Juan M. Ramirez-Cortes",
            "Jorge Martinez-Carballido",
            "Eldamira Buenfil-Alpuche"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Performance curves of queuing systems can be analyzed by separating them into\nthree regions: the flat region, the knee region, and the exponential region.\nPractical considerations, usually locate the knee region between 70-90% of the\ntheoretical maximum utilization. However, there is not a clear agreement about\nwhere the boundaries between regions are, and where exactly the utilization\nknee is located. An open debate about knees in performance curves was\nundertaken at least 20 years ago. This historical debate is mainly divided\nbetween those who claim that a knee in the curve is not a well-defined term in\nmathematics, or it is a subjective and not really meaningful concept, and those\nwho define knees mathematically and consider their relevance and application.\nIn this paper, we present a mathematical model and analysis for identifying the\nthree mentioned regions on performance curves for M/M/1 systems; specifically,\nwe found the knees, or optimal utilization percentiles, at the vertices of the\nhyperbolas that relate response time as a function of utilization. Using these\nresults, we argue that an adaptive and optimal queuing system could be deployed\nby keeping load and throughput within the knee region.\n",
        "pdf_link": "http://arxiv.org/pdf/1106.2380v2"
    },
    {
        "title": "A Stochastic Broadcast Pi-Calculus",
        "authors": [
            "Lei Song",
            "Flemming Nielson",
            "Bo Friis Nielsen"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  In this paper we propose a stochastic broadcast PI-calculus which can be used\nto model server-client based systems where synchronization is always governed\nby only one participant. Therefore, there is no need to determine the joint\nsynchronization rates. We also take immediate transitions into account which is\nuseful to model behaviors with no impact on the temporal properties of a\nsystem. Since immediate transitions may introduce non-determinism, we will show\nhow these non-determinism can be resolved, and as result a valid CTMC will be\nobtained finally. Also some practical examples are given to show the\napplication of this calculus.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.1202v1"
    },
    {
        "title": "On the Performance of Space Shift Keying (SSK) Modulation with Imperfect\n  Channel Knowledge",
        "authors": [
            "Marco Di Renzo",
            "Dario De Leonardis",
            "Fabio Graziosi",
            "Harald Haas"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  In this paper, we study the sensitivity and robustness of Space Shift Keying\n(SSK) modulation to imperfect channel knowledge at the receiver. Unlike the\ncommon widespread belief, we show that SSK modulation is more robust to\nimperfect channel knowledge than other state-of-the-art transmission\ntechnologies, and only few training pilots are needed to get reliable enough\nchannel estimates for data detection. More precisely, we focus our attention on\nthe so-called Time-Orthogonal-Signal-Design (TOSD-) SSK modulation scheme,\nwhich is an improved version of SSK modulation offering transmit-diversity\ngains, and provide the following contributions: i) we develop a closed-form\nanalytical framework to compute the Average Bit Error Probability (ABEP) of a\nmismatched detector for TOSD-SSK modulation, which can be used for arbitrary\ntransmit-antenna, receive-antenna, channel fading, and training pilots; ii) we\nperform a comparative study of the performance of TOSD-SSK modulation and the\nAlamouti code under the same imperfect channel knowledge, and show that\nTOSD-SSK modulation is more robust to channel estimation errors; iii) we point\nout that only few pilot pulses are required to get performance very close to\nthe perfect channel knowledge lower-bound; and iv) we verify that transmit- and\nreceive-diversity gains of TOSD-SSK modulation are preserved even for a\nmismatched receiver.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4922v1"
    },
    {
        "title": "An Improved Analytical Expression for Write Amplification in NAND Flash",
        "authors": [
            "Luojie Xiang",
            "Brian Kurkoski"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Agarwal et al. gave an closed-form expression for write amplification in NAND\nflash memory by finding the probability of a page being valid over the whole\nflash memory. This paper gives an improved analytic expression for write\namplification in NAND flash memory by finding the probability of a page being\ninvalid over the block selected for garbage collection. The improved expression\nuses Lambert W function. Through asymptotic analysis, write amplification is\nshown to depend on overprovisioning factor only, consistent with the previous\nwork. Comparison with numerical simulations shows that the improved expression\nachieves a more accurate prediction of write amplification. For example, when\nthe overprovisioning factor is 0.3, the expression proposed by this paper gives\na write amplification of 2.36 whereas that of the previous work gives 2.17,\nwhen the actual value is 2.35.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.4245v1"
    },
    {
        "title": "A Survey on Delay-Aware Resource Control for Wireless Systems --- Large\n  Deviation Theory, Stochastic Lyapunov Drift and Distributed Stochastic\n  Learning",
        "authors": [
            "Ying Cui",
            "Vincent K. N. Lau",
            "Rui Wang",
            "Huang Huang",
            "Shunqing Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  In this tutorial paper, a comprehensive survey is given on several major\nsystematic approaches in dealing with delay-aware control problems, namely the\nequivalent rate constraint approach, the Lyapunov stability drift approach and\nthe approximate Markov Decision Process (MDP) approach using stochastic\nlearning. These approaches essentially embrace most of the existing literature\nregarding delay-aware resource control in wireless systems. They have their\nrelative pros and cons in terms of performance, complexity and implementation\nissues. For each of the approaches, the problem setup, the general solution and\nthe design methodology are discussed. Applications of these approaches to\ndelay-aware resource allocation are illustrated with examples in single-hop\nwireless networks. Furthermore, recent results regarding delay-aware multi-hop\nrouting designs in general multi-hop networks are elaborated. Finally, the\ndelay performance of the various approaches are compared through simulations\nusing an example of the uplink OFDMA systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.4535v1"
    },
    {
        "title": "A Generalized Loss Network Model with Overflow for Capacity Planning of\n  a Perinatal Network",
        "authors": [
            "Md Asaduzzaman",
            "Thierry J Chaussalet"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  We develop a generalized loss network framework for capacity planning of a\nperinatal network in the UK. Decomposing the network by hospitals, each unit is\nanalyzed with a GI/G/c/0 overflow loss network model. A two-moment\napproximation is performed to obtain the steady state solution of the GI/G/c/0\nloss systems, and expressions for rejection probability and overflow\nprobability have been derived. Using the model framework, the number of\nrequired cots can be estimated based on the rejection probability at each level\nof care of the neonatal units in a network. The generalization ensures that the\nmodel can be applied to any perinatal network for renewal arrival and discharge\nprocesses.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.6544v2"
    },
    {
        "title": "Simple and Effective Dynamic Provisioning for Power-Proportional Data\n  Centers",
        "authors": [
            "Tan Lu",
            "Minghua Chen"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Energy consumption represents a significant cost in data center operation. A\nlarge fraction of the energy, however, is used to power idle servers when the\nworkload is low. Dynamic provisioning techniques aim at saving this portion of\nthe energy, by turning off unnecessary servers. In this paper, we explore how\nmuch performance gain can knowing future workload information brings to dynamic\nprovisioning. In particular, we study the dynamic provisioning problem under\nthe cost model that a running server consumes a fixed amount energy per unit\ntime, and develop online solutions with and without future workload information\navailable. We first reveal an elegant structure of the off-line dynamic\nprovisioning problem, which allows us to characterize and achieve the optimal\nsolution in a {}\"divide-and-conquer\" manner. We then exploit this insight to\ndesign three online algorithms with competitive ratios $2-\\alpha$,\n$(e-\\alpha)/(e-1)\\approx1.58-\\alpha/(e-1)$ and $e/(e-1+\\alpha)$, respectively,\nwhere $0\\leq\\alpha\\leq1$ is the fraction of a critical window in which future\nworkload information is available. A fundamental observation is that\n\\emph{future workload information beyond the critical window will not}\n\\emph{improve dynamic provisioning performance}. Our algorithms are\ndecentralized and are simple to implement. We demonstrate their effectiveness\nin simulations using real-world traces. We also compare their performance with\nstate-of-the-art solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.0442v2"
    },
    {
        "title": "Performance engineering for the Lattice Boltzmann method on GPGPUs:\n  Architectural requirements and performance results",
        "authors": [
            "Johannes Habich",
            "Christian Feichtinger",
            "Harald Köstler",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  GPUs offer several times the floating point performance and memory bandwidth\nof current standard two socket CPU servers, e.g. NVIDIA C2070 vs. Intel Xeon\nWestmere X5650. The lattice Boltzmann method has been established as a flow\nsolver in recent years and was one of the first flow solvers to be successfully\nported and that performs well on GPUs. We demonstrate advanced optimization\nstrategies for a D3Q19 lattice Boltzmann based incompressible flow solver for\nGPGPUs and CPUs based on NVIDIA CUDA and OpenCL. Since the implemented\nalgorithm is limited by memory bandwidth, we concentrate on improving memory\naccess. Basic data layout issues for optimal data access are explained and\ndiscussed. Furthermore, the algorithmic steps are rearranged to improve\nscattered access of the GPU memory. The importance of occupancy is discussed as\nwell as optimization strategies to improve overall concurrency. We arrive at a\nwell-optimized GPU kernel, which is integrated into a larger framework that can\nhandle single phase fluid flow simulations as well as particle-laden flows. Our\n3D LBM GPU implementation reaches up to 650 MLUPS in single precision and 290\nMLUPS in double precision on an NVIDIA Tesla C2070.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.0850v1"
    },
    {
        "title": "Stabilization of Branching Queueing Networks",
        "authors": [
            "Tomáš Brázdil",
            "Stefan Kiefer"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Queueing networks are gaining attraction for the performance analysis of\nparallel computer systems. A Jackson network is a set of interconnected\nservers, where the completion of a job at server i may result in the creation\nof a new job for server j. We propose to extend Jackson networks by \"branching\"\nand by \"control\" features. Both extensions are new and substantially expand the\nmodelling power of Jackson networks. On the other hand, the extensions raise\ncomputational questions, particularly concerning the stability of the networks,\ni.e, the ergodicity of the underlying Markov chain. We show for our extended\nmodel that it is decidable in polynomial time if there exists a controller that\nachieves stability. Moreover, if such a controller exists, one can efficiently\ncompute a static randomized controller which stabilizes the network in a very\nstrong sense; in particular, all moments of the queue sizes are finite.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.1041v1"
    },
    {
        "title": "A Temporal Approach to Stochastic Network Calculus",
        "authors": [
            "Jing Xie",
            "Yuming Jiang",
            "Min Xie"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Stochastic network calculus is a newly developed theory for stochastic\nservice guarantee analysis of computer networks. In the current stochastic\nnetwork calculus literature, its fundamental models are based on the cumulative\namount of traffic or cumulative amount of service. However, there are network\nscenarios where direct application of such models is difficult. This paper\npresents a temporal approach to stochastic network calculus. The key idea is to\ndevelop models and derive results from the time perspective. Particularly, we\ndefine traffic models and service models based on the cumulative packet\ninter-arrival time and the cumulative packet service time, respectively.\nRelations among these models as well as with the existing models in the\nliterature are established. In addition, we prove the basic properties of the\nproposed models, such as delay bound and backlog bound, output\ncharacterization, concatenation property and superposition property. These\nresults form a temporal stochastic network calculus and compliment the existing\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/1112.2822v1"
    },
    {
        "title": "Minimizing Slowdown in Heterogeneous Size-Aware Dispatching Systems\n  (full version)",
        "authors": [
            "Esa Hyytiä",
            "Samuli Aalto",
            "Aleksi Penttinen"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  We consider a system of parallel queues where tasks are assigned (dispatched)\nto one of the available servers upon arrival. The dispatching decision is based\non the full state information, i.e., on the sizes of the new and existing jobs.\nWe are interested in minimizing the so-called mean slowdown criterion\ncorresponding to the mean of the sojourn time divided by the processing time.\nAssuming no new jobs arrive, the shortest-processing-time-product (SPTP)\nschedule is known to minimize the slowdown of the existing jobs. The main\ncontribution of this paper is three-fold: 1) To show the optimality of SPTP\nwith respect to slowdown in a single server queue under Poisson arrivals; 2) to\nderive the so-called size-aware value functions for\nM/G/1-FIFO/LIFO/SPTP/SPT/SRPT with general holding costs of which the slowdown\ncriterion is a special case; and 3) to utilize the value functions to derive\nefficient dispatching policies so as to minimize the mean slowdown in a\nheterogeneous server system. The derived policies offer a significantly better\nperformance than e.g., the size-aware-task-assignment with equal load (SITA-E)\nand least-work-left (LWL) policies.\n",
        "pdf_link": "http://arxiv.org/pdf/1203.5040v1"
    },
    {
        "title": "Model for Predicting End User Web Page Response Time",
        "authors": [
            "Sathya Narayanan Nagarajan",
            "Srijith Ravikumar"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Perceived responsiveness of a web page is one of the most important and least\nunderstood metrics of web page design, and is critical for attracting and\nmaintaining a large audience. Web pages can be designed to meet performance\nSLAs early in the product lifecycle if there is a way to predict the apparent\nresponsiveness of a particular page layout. Response time of a web page is\nlargely influenced by page layout and various network characteristics. Since\nthe network characteristics vary widely from country to country, accurately\nmodeling and predicting the perceived responsiveness of a web page from the end\nuser's perspective has traditionally proven very difficult. We propose a model\nfor predicting end user web page response time based on web page, network,\nbrowser download and browser rendering characteristics. We start by\nunderstanding the key parameters that affect perceived response time. We then\nmodel each of these parameters individually using experimental tests and\nstatistical techniques. Finally, we demonstrate the effectiveness of this model\nby conducting an experimental study with Yahoo! web pages in two countries and\ncompare it with 3rd party measurement application.\n",
        "pdf_link": "http://arxiv.org/pdf/1204.6304v1"
    },
    {
        "title": "Uncertainty Analysis of the Adequacy Assessment Model of a Distributed\n  Generation System",
        "authors": [
            "Yanfu Li",
            "Enrico Zio"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Due to the inherent aleatory uncertainties in renewable generators, the\nreliability/adequacy assessments of distributed generation (DG) systems have\nbeen particularly focused on the probabilistic modeling of random behaviors,\ngiven sufficient informative data. However, another type of uncertainty\n(epistemic uncertainty) must be accounted for in the modeling, due to\nincomplete knowledge of the phenomena and imprecise evaluation of the related\ncharacteristic parameters. In circumstances of few informative data, this type\nof uncertainty calls for alternative methods of representation, propagation,\nanalysis and interpretation. In this study, we make a first attempt to\nidentify, model, and jointly propagate aleatory and epistemic uncertainties in\nthe context of DG systems modeling for adequacy assessment. Probability and\npossibility distributions are used to model the aleatory and epistemic\nuncertainties, respectively. Evidence theory is used to incorporate the two\nuncertainties under a single framework. Based on the plausibility and belief\nfunctions of evidence theory, the hybrid propagation approach is introduced. A\ndemonstration is given on a DG system adapted from the IEEE 34 nodes\ndistribution test feeder. Compared to the pure probabilistic approach, it is\nshown that the hybrid propagation is capable of explicitly expressing the\nimprecision in the knowledge on the DG parameters into the final adequacy\nvalues assessed. It also effectively captures the growth of uncertainties with\nhigher DG penetration levels.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.1204v1"
    },
    {
        "title": "Hierarchical Performance Modeling for Ranking Dense Linear Algebra\n  Algorithms",
        "authors": [
            "Elmar Peise"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  A large class of dense linear algebra operations, such as LU decomposition or\ninversion of a triangular matrix, are usually performed by blocked algorithms.\nFor one such operation, typically, not only one but many algorithmic variants\nexist; depending on computing architecture, libraries and problem size, each\nvariant attains a different performances. We propose methods and tools to rank\nthe algorithmic variants according to their performance for a given scenario\nwithout executing them.\n  For this purpose, we identify the routines upon which the algorithms are\nbuilt. A first tool - the Sampler - measures the performance of these routines.\nUsing the Sampler, a second tool models their performance. The generated models\nare then used to predict the performance of the considered algorithms. For a\ngiven scenario, these predictions allow us to correctly rank the algorithms\naccording to their performance without executing them. With the help of the\nsame tools, algorithmic parameters such as block-size can be optimally tuned.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.5217v3"
    },
    {
        "title": "Characterizing the Impact of the Workload on the Value of Dynamic\n  Resizing in Data Centers",
        "authors": [
            "Kai Wang",
            "Minghong Lin",
            "Florin Ciucu",
            "Adam Wierman",
            "Chuang Lin"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Energy consumption imposes a significant cost for data centers; yet much of\nthat energy is used to maintain excess service capacity during periods of\npredictably low load. Resultantly, there has recently been interest in\ndeveloping designs that allow the service capacity to be dynamically resized to\nmatch the current workload. However, there is still much debate about the value\nof such approaches in real settings. In this paper, we show that the value of\ndynamic resizing is highly dependent on statistics of the workload process. In\nparticular, both slow time-scale non-stationarities of the workload (e.g., the\npeak-to-mean ratio) and the fast time-scale stochasticity (e.g., the burstiness\nof arrivals) play key roles. To illustrate the impact of these factors, we\ncombine optimization-based modeling of the slow time-scale with stochastic\nmodeling of the fast time scale. Within this framework, we provide both\nanalytic and numerical results characterizing when dynamic resizing does (and\ndoes not) provide benefits.\n",
        "pdf_link": "http://arxiv.org/pdf/1207.6295v2"
    },
    {
        "title": "An Upper Bound on the Convergence Time for Distributed Binary Consensus",
        "authors": [
            "Shang Shang",
            "Paul W. Cuff",
            "Sanjeev R. Kulkarni",
            "Pan Hui"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  The problem addressed in this paper is the analysis of a distributed\nconsensus algorithm for arbitrary networks, proposed by B\\'en\\'ezit et al.. In\nthe initial setting, each node in the network has one of two possible states\n(\"yes\" or \"no\"). Nodes can update their states by communicating with their\nneighbors via a 2-bit message in an asynchronous clock setting. Eventually, all\nnodes reach consensus on the majority states. We use the theory of electric\nnetworks, random walks, and couplings of Markov chains to derive an O(N4 logN)\nupper bound for the expected convergence time on an arbitrary graph of size N.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.0525v2"
    },
    {
        "title": "Solid State Disk Object-Based Storage with Trim Commands",
        "authors": [
            "Tasha Frankie",
            "Gordon Hughes",
            "Ken Kreutz-Delgado"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  This paper presents a model of NAND flash SSD utilization and write\namplification when the ATA/ATAPI SSD Trim command is incorporated into\nobject-based storage under a variety of user workloads, including a uniform\nrandom workload with objects of fixed size and a uniform random workload with\nobjects of varying sizes. We first summarize the existing models for write\namplification in SSDs for workloads with and without the Trim command, then\npropose an alteration of the models that utilizes a framework of object-based\nstorage. The utilization of objects and pages in the SSD is derived, with the\nanalytic results compared to simulation. Finally, the effect of objects on\nwrite amplification and its computation is discussed along with a potential\napplication to optimization of SSD usage through object storage metadata\nservers that allocate object classes of distinct object size.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.5975v1"
    },
    {
        "title": "Performance Evaluation: Ball-Tree and KD-Tree in the Context of MST",
        "authors": [
            "Hazarath Munaga",
            "Venkata Jarugumalli"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Now a days many algorithms are invented or being inventing to find the\nsolution for Euclidean Minimum Spanning Tree, EMST, problem, as its\napplicability is increasing in much wide range of fields containing spatial or\nspatio temporal data viz. astronomy which consists of millions of spatial data.\nTo solve this problem, we are presenting a technique by adopting the dual tree\nalgorithm for finding efficient EMST and experimented on a variety of real time\nand synthetic datasets. This paper presents the observed experimental\nobservations and the efficiency of the dual tree framework, in the context of\nkdtree and ball tree on spatial datasets of different dimensions.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.6122v1"
    },
    {
        "title": "Critical Utility Infrastructural Resilience",
        "authors": [
            "Giovanna Dondossola",
            "Geert Deconinck",
            "Felicita Di Giandomenico",
            "Susanna Donatelli",
            "Mohamed Kaaniche",
            "Paulo Verissimo"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  The paper refers to CRUTIAL, CRitical UTility InfrastructurAL Resilience, a\nEuropean project within the research area of Critical Information\nInfrastructure Protection, with a specific focus on the infrastructures\noperated by power utilities, widely recognized as fundamental to national and\ninternational economy, security and quality of life. Such infrastructures faced\nwith the recent market deregulations and the multiple interdependencies with\nother infrastructures are becoming more and more vulnerable to various threats,\nincluding accidental failures and deliberate sabotage and malicious attacks.\nThe subject of CRUTIAL research are small scale networked ICT systems used to\ncontrol and manage the electric power grid, in which artifacts controlling the\nphysical process of electricity transportation need to be connected with\ncorporate and societal applications performing management and maintenance\nfunctionality. The peculiarity of such ICT-supported systems is that they are\nrelated to the power system dynamics and its emergency conditions. Specific\neffort need to be devoted by the Electric Power community and by the\nInformation Technology community to influence the technological progress in\norder to allow commercial intelligent electronic devices to be effectively\ndeployed for the protection of citizens against cyber threats to electric power\nmanagement and control systems. A well-founded know-how needs to be built\ninside the industrial power sector to allow all the involved stakeholders to\nachieve their service objectives without compromising the resilience properties\nof the logical and physical assets that support the electric power provision.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5736v1"
    },
    {
        "title": "Modeling the resilience of large and evolving systems",
        "authors": [
            "Mohamed Kaaniche",
            "Paolo Lollini",
            "Andrea Bondavalli",
            "Karama Kanoun"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  This paper summarizes the state of knowledge and ongoing research on methods\nand techniques for resilience evaluation, taking into account the\nresilience-scaling challenges and properties related to the ubiquitous\ncomputerized systems. We mainly focus on quantitative evaluation approaches\nand, in particular, on model-based evaluation techniques that are commonly used\nto evaluate and compare, from the dependability point of view, different\narchitecture alternatives at the design stage. We outline some of the main\nmodeling techniques aiming at mastering the largeness of analytical\ndependability models at the construction level. Actually, addressing the model\nlargeness problem is important with respect to the investigation of the\nscalability of current techniques to meet the complexity challenges of\nubiquitous systems. Finally we present two case studies in which some of the\npresented techniques are applied for modeling web services and General Packet\nRadio Service (GPRS) mobile telephone networks, as prominent examples of large\nand evolving systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.5738v1"
    },
    {
        "title": "Power Consumption Analysis of a Modern Smartphone",
        "authors": [
            "Muhammad Yasir Malik"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  This paper presents observations about power consumption of a latest\nsmartphone. Modern smartphones are powerful devices with different choices of\ndata connections and other functional modes. This paper provides analysis of\npower utilization for these different operation modes. Also, we present power\nconsumption by vital operating system (OS) components.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.1896v2"
    },
    {
        "title": "Blind Estimation of Primary User Traffic Parameters Under Sensing Errors",
        "authors": [
            "Wesam Gabran",
            "Przemysław Pawełczak",
            "Chun-Hao Liu",
            "Danijela Cabric"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  In this work we investigate the bounds on the estimation accuracy of Primary\nUser (PU) traffic parameters with exponentially distributed busy and idle\ntimes. We derive closed-form expressions for the Cramer-Rao bounds on the mean\nsquared estimation error for the blind joint estimation of the PU traffic\nparameters, specifically, the duty cycle, and the mean arrival and departure\nrates. Moreover, we present the corresponding maximum-likelihood estimators for\nthe traffic parameters. In addition, we derive a modified likelihood function\nfor the joint estimation of traffic parameters when spectrum sensing errors are\nconsidered, and we present the impact of spectrum sensing errors on the\nestimation error via simulations. Finally, we consider a duty cycle estimator,\ncommon in traffic estimation literature, that is based on averaging the traffic\nsamples. We derive, in closed-form, the mean squared estimation error of the\nconsidered estimator under spectrum sensing errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.5514v1"
    },
    {
        "title": "Sharp Bounds in Stochastic Network Calculus",
        "authors": [
            "Florin Ciucu",
            "Felix Poloczek",
            "Jens Schmitt"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  The practicality of the stochastic network calculus (SNC) is often questioned\non grounds of potential looseness of its performance bounds. In this paper it\nis uncovered that for bursty arrival processes (specifically Markov-Modulated\nOn-Off (MMOO)), whose amenability to \\textit{per-flow} analysis is typically\nproclaimed as a highlight of SNC, the bounds can unfortunately indeed be very\nloose (e.g., by several orders of magnitude off). In response to this uncovered\nweakness of SNC, the (Standard) per-flow bounds are herein improved by deriving\na general sample-path bound, using martingale based techniques, which\naccommodates FIFO, SP, EDF, and GPS scheduling. The obtained (Martingale)\nbounds gain an exponential decay factor of ${\\mathcal{O}}(e^{-\\alpha n})$ in\nthe number of flows $n$. Moreover, numerical comparisons against simulations\nshow that the Martingale bounds are remarkably accurate for FIFO, SP, and EDF\nscheduling; for GPS scheduling, although the Martingale bounds substantially\nimprove the Standard bounds, they are numerically loose, demanding for\nimprovements in the core SNC analysis of GPS.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.4114v2"
    },
    {
        "title": "Stochastic Modeling of Large-Scale Solid-State Storage Systems:\n  Analysis, Design Tradeoffs and Optimization",
        "authors": [
            "Yongkun Li",
            "Patrick P. C. Lee",
            "John C. S. Lui"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Solid state drives (SSDs) have seen wide deployment in mobiles, desktops, and\ndata centers due to their high I/O performance and low energy consumption. As\nSSDs write data out-of-place, garbage collection (GC) is required to erase and\nreclaim space with invalid data. However, GC poses additional writes that\nhinder the I/O performance, while SSD blocks can only endure a finite number of\nerasures. Thus, there is a performance-durability tradeoff on the design space\nof GC. To characterize the optimal tradeoff, this paper formulates an\nanalytical model that explores the full optimal design space of any GC\nalgorithm. We first present a stochastic Markov chain model that captures the\nI/O dynamics of large-scale SSDs, and adapt the mean-field approach to derive\nthe asymptotic steady-state performance. We further prove the model convergence\nand generalize the model for all types of workload. Inspired by this model, we\npropose a randomized greedy algorithm (RGA) that can operate along the optimal\ntradeoff curve with a tunable parameter. Using trace-driven simulation on\nDiskSim with SSD add-ons, we demonstrate how RGA can be parameterized to\nrealize the performance-durability tradeoff.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.4816v2"
    },
    {
        "title": "Identifying Compiler Options to Minimise Energy Consumption for Embedded\n  Platforms",
        "authors": [
            "James Pallister",
            "Simon Hollis",
            "Jeremy Bennett"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  This paper presents an analysis of the energy consumption of an extensive\nnumber of the optimisations a modern compiler can perform. Using GCC as a test\ncase, we evaluate a set of ten carefully selected benchmarks for five different\nembedded platforms.\n  A fractional factorial design is used to systematically explore the large\noptimisation space (2^82 possible combinations), whilst still accurately\ndetermining the effects of optimisations and optimisation combinations.\nHardware power measurements on each platform are taken to ensure all\narchitectural effects on the energy consumption are captured.\n  We show that fractional factorial design can find more optimal combinations\nthan relying on built in compiler settings. We explore the relationship between\nrun-time and energy consumption, and identify scenarios where they are and are\nnot correlated.\n  A further conclusion of this study is the structure of the benchmark has a\nlarger effect than the hardware architecture on whether the optimisation will\nbe effective, and that no single optimisation is universally beneficial for\nexecution time or energy consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.6485v2"
    },
    {
        "title": "A Taxonomy of Performance Assurance Methodologies and its Application in\n  High Performance Computer Architectures",
        "authors": [
            "Hemant Rotithor"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  This paper presents a systematic approach to the complex problem of high\nconfidence performance assurance of high performance architectures based on\nmethods used over several generations of industrial microprocessors. A taxonomy\nis presented for performance assurance through three key stages of a product\nlife cycle-high level performance, RTL performance, and silicon performance.\nThe proposed taxonomy includes two components-independent performance assurance\nspace for each stage and a correlation performance assurance space between\nstages. It provides a detailed insight into the performance assurance space in\nterms of coverage provided taking into account capabilities and limitations of\ntools and methodologies used at each stage. An application of the taxonomy to\ncases described in the literature and to high performance Intel architectures\nis shown. The proposed work should be of interest to manufacturers of high\nperformance microprocessor/chipset architectures and has not been discussed in\nthe literature.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3767v1"
    },
    {
        "title": "Performance Bounds for Multiclass FIFO in Communication Networks: A\n  Deterministic Case",
        "authors": [
            "Yuming Jiang"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Multiclass FIFO is used in communication networks such as in input-queueing\nrouters/switches and in wireless networks. For the concern of providing service\nguarantees in such networks, it is crucial to have analytical results, e.g.\nbounds, on the performance of multi-class FIFO. Surprisingly, there are few\nsuch results in the literature. This paper is devoted to filling the gap.\nSpecifically, a single hop deterministic case is studied, for which, delay and\nbacklog bounds are derived, in addition to guaranteed rate and service curve\ncharacterizations that may be exploited to extend the analysis to network\ncases.\n",
        "pdf_link": "http://arxiv.org/pdf/1306.4773v1"
    },
    {
        "title": "BEEBS: Open Benchmarks for Energy Measurements on Embedded Platforms",
        "authors": [
            "James Pallister",
            "Simon Hollis",
            "Jeremy Bennett"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  This paper presents and justifies an open benchmark suite named BEEBS,\ntargeted at evaluating the energy consumption of embedded processors.\n  We explore the possible sources of energy consumption, then select individual\nbenchmarks from contemporary suites to cover these areas. Version one of BEEBS\nis presented here and contains 10 benchmarks that cover a wide range of typical\nembedded applications. The benchmark suite is portable across diverse\narchitectures and is freely available.\n  The benchmark suite is extensively evaluated, and the properties of its\nconstituent programs are analysed. Using real hardware platforms we show case\nexamples which illustrate the difference in power dissipation between three\nprocessor architectures and their related ISAs. We observe significant\ndifferences in the average instruction dissipation between the architectures of\n4.4x, specifically 170uW/MHz (ARM Cortex-M0), 65uW/MHz (Adapteva Epiphany) and\n88uW/MHz (XMOS XS1-L1).\n",
        "pdf_link": "http://arxiv.org/pdf/1308.5174v2"
    },
    {
        "title": "Using Chip Multithreading to Speed Up Scenario-Based Design Space\n  Exploration",
        "authors": [
            "P. van Stralen"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  To cope with the complex embedded system design, early design space\nexploration (DSE) is used to make design decisions early in the design phase.\nFor early DSE it is crucial that the running time of the exploration is as\nsmall as possible. In this paper, we describe both the porting of our\nscenario-based DSE to the SPARC T3-4 server and the analysis of its performance\nbehavior.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.6469v1"
    },
    {
        "title": "Mixed Polling with Rerouting and Applications",
        "authors": [
            "Veeraruna Kavitha",
            "Richard Combes"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Queueing systems with a single server in which customers wait to be served at\na finite number of distinct locations (buffers/queues) are called discrete\npolling systems. Polling systems in which arrivals of users occur anywhere in a\ncontinuum are called continuous polling systems. Often one encounters a\ncombination of the two systems: the users can either arrive in a continuum or\nwait in a finite set (i.e. wait at a finite number of queues). We call these\nsystems mixed polling systems. Also, in some applications, customers are\nrerouted to a new location (for another service) after their service is\ncompleted. In this work, we study mixed polling systems with rerouting. We\nobtain their steady state performance by discretization using the known pseudo\nconservation laws of discrete polling systems. Their stationary expected\nworkload is obtained as a limit of the stationary expected workload of a\ndiscrete system. The main tools for our analysis are: a) the fixed point\nanalysis of infinite dimensional operators and; b) the convergence of Riemann\nsums to an integral.\n  We analyze two applications using our results on mixed polling systems and\ndiscuss the optimal system design. We consider a local area network, in which a\nmoving ferry facilitates communication (data transfer) using a wireless link.\nWe also consider a distributed waste collection system and derive the optimal\ncollection point. In both examples, the service requests can arrive anywhere in\na subset of the two dimensional plane. Namely, some users arrive in a\ncontinuous set while others wait for their service in a finite set. The only\npolling systems that can model these applications are mixed systems with\nrerouting as introduced in this manuscript.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1365v1"
    },
    {
        "title": "An Aggregation Technique For Large-Scale PEPA Models With Non-Uniform\n  Populations",
        "authors": [
            "Alireza Pourranjbar",
            "Jane Hillston"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Performance analysis based on modelling consists of two major steps: model\nconstruction and model analysis. Formal modelling techniques significantly aid\nmodel construction but can exacerbate model analysis. In particular, here we\nconsider the analysis of large-scale systems which consist of one or more\nentities replicated many times to form large populations. The replication of\nentities in such models can cause their state spaces to grow exponentially to\nthe extent that their exact stochastic analysis becomes computationally\nexpensive or even infeasible.\n  In this paper, we propose a new approximate aggregation algorithm for a class\nof large-scale PEPA models. For a given model, the method quickly checks if it\nsatisfies a syntactic condition, indicating that the model may be solved\napproximately with high accuracy. If so, an aggregated CTMC is generated\ndirectly from the model description. This CTMC can be used for efficient\nderivation of an approximate marginal probability distribution over some of the\nmodel's populations. In the context of a large-scale client-server system, we\ndemonstrate the usefulness of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1613v1"
    },
    {
        "title": "Software Autotuning for Sustainable Performance Portability",
        "authors": [
            "Azamat Mametjanov",
            "Boyana Norris"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Scientific software applications are increasingly developed by large\ninterdiscplinary teams operating on functional modules organized around a\ncommon software framework, which is capable of integrating new functional\ncapabilities without modifying the core of the framework. In such environment,\nsoftware correctness and modularity take precedence at the expense of code\nperformance, which is an important concern during execution on supercomputing\nfacilities, where the allocation of core-hours is a valuable resource. To\nalleviate the performance problems, we propose automated performance tuning\n(autotuning) of software to extract the maximum performance on a given hardware\nplatform and to enable performance portability across heterogeneous hardware\nplatforms. The resulting code remains generic without committing to a\nparticular software stack and yet is compile-time specializable for maximal\nsustained performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.1894v1"
    },
    {
        "title": "A Survey of Embedded Software Profiling Methodologies",
        "authors": [
            "Rajendra Patel",
            "Arvind Rajwat"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Embedded Systems combine one or more processor cores with dedicated logic\nrunning on an ASIC or FPGA to meet design goals at reasonable cost. It is\nachieved by profiling the application with variety of aspects like performance,\nmemory usage, cache hit versus cache miss, energy consumption, etc. Out of\nthese, performance estimation is more important than others. With ever\nincreasing system complexities, it becomes quite necessary to carry out\nperformance estimation of embedded software implemented in a particular\nprocessor for fast design space exploration. Such profiled data also guides the\ndesigner how to partition the system for Hardware (HW) and Software (SW)\nenvironments. In this paper, we propose a classification for currently\navailable Embedded Software Profiling Tools, and we present different academic\nand industrial approaches in this context. Based on these observations, it will\nbe easy to identify such common principles and needs which are required for a\ntrue Software Profiling Tool for a particular application.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2949v1"
    },
    {
        "title": "Theoretical Evaluation of Offloading through Wireless LANs",
        "authors": [
            "Hiroshi Saito",
            "Ryoichi Kawahara"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Offloading of cellular traffic through a wireless local area network (WLAN)\nis theoretically evaluated. First, empirical data sets of the locations of WLAN\ninternet access points are analyzed and an inhomogeneous Poisson process\nconsisting of high, normal, and low density regions is proposed as a spatial\npoint process model for these configurations. Second, performance metrics, such\nas mean available bandwidth for a user and the number of vertical handovers,\nare evaluated for the proposed model through geometric analysis. Explicit\nformulas are derived for the metrics, although they depend on many parameters\nsuch as the number of WLAN access points, the shape of each WLAN coverage\nregion, the location of each WLAN access point, the available bandwidth (bps)\nof the WLAN, and the shape and available bandwidth (bps) of each subregion\nidentified by the channel quality indicator in a cell of the cellular network.\nExplicit formulas strongly suggest that the bandwidth a user experiences does\nnot depend on the user mobility. This is because the bandwidth available by a\nuser who does not move and that available by a user who moves are the same or\napproximately the same as a probabilistic distribution. Numerical examples show\nthat parameters, such as the size of regions where placement of WLAN access\npoints is not allowed and the mean density of WLANs in high density regions,\nhave a large impact on performance metrics. In particular, a homogeneous\nPoisson process model as the WLAN access point location model largely\noverestimates the mean available bandwidth for a user and the number of\nvertical handovers. The overestimated mean available bandwidth is, for example,\nabout 50% in a certain condition.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.2486v1"
    },
    {
        "title": "Parallelism Via Concurrency at Multiple Levels",
        "authors": [
            "Kamran Latif"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  In this paper we examine the key elements determining the best performance of\ncomputing by increasing the frequency of a single chip and to get the minimum\nlatency during execution of the programs to achieve best possible output. It is\nnot enough to provide concurrent improvements in the hardware as Software also\nhave to introduce concurrency in order to exploit the parallelism. The software\nparallelism is defined by the control and data dependency of programs whereas\nHardware refers to the type of parallelism defined by the machine architecture\nand hardware multiplicity.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0184v1"
    },
    {
        "title": "Block-Structured Supermarket Models",
        "authors": [
            "Quan-Lin Li",
            "John C. S. Lui"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Supermarket models are a class of parallel queueing networks with an adaptive\ncontrol scheme that play a key role in the study of resource management of,\nsuch as, computer networks, manufacturing systems and transportation networks.\nWhen the arrival processes are non-Poisson and the service times are\nnon-exponential, analysis of such a supermarket model is always limited,\ninteresting, and challenging.\n  This paper describes a supermarket model with non-Poisson inputs: Markovian\nArrival Processes (MAPs) and with non-exponential service times: Phase-type\n(PH) distributions, and provides a generalized matrix-analytic method which is\nfirst combined with the operator semigroup and the mean-field limit. When\ndiscussing such a more general supermarket model, this paper makes some new\nresults and advances as follows: (1) Providing a detailed probability analysis\nfor setting up an infinite-dimensional system of differential vector equations\nsatisfied by the expected fraction vector, where \"the invariance of environment\nfactors\" is given as an important result. (2) Introducing the phase-type\nstructure to the operator semigroup and to the mean-field limit, and a\nLipschitz condition can be obtained by means of a unified matrix-differential\nalgorithm. (3) The matrix-analytic method is used to compute the fixed point\nwhich leads to performance computation of this system. Finally, we use some\nnumerical examples to illustrate how the performance measures of this\nsupermarket model depend on the non-Poisson inputs and on the non-exponential\nservice times. Thus the results of this paper give new highlight on\nunderstanding influence of non-Poisson inputs and of non-exponential service\ntimes on performance measures of more general supermarket models.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.0285v1"
    },
    {
        "title": "Patch-based Hybrid Modelling of Spatially Distributed Systems by Using\n  Stochastic HYPE - ZebraNet as an Example",
        "authors": [
            "Cheng Feng"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Individual-based hybrid modelling of spatially distributed systems is usually\nexpensive. Here, we consider a hybrid system in which mobile agents spread over\nthe space and interact with each other when in close proximity. An\nindividual-based model for this system needs to capture the spatial attributes\nof every agent and monitor the interaction between each pair of them. As a\nresult, the cost of simulating this model grows exponentially as the number of\nagents increases. For this reason, a patch-based model with more abstraction\nbut better scalability is advantageous. In a patch-based model, instead of\nrepresenting each agent separately, we model the agents in a patch as an\naggregation. This property significantly enhances the scalability of the model.\nIn this paper, we convert an individual-based model for a spatially distributed\nnetwork system for wild-life monitoring, ZebraNet, to a patch-based stochastic\nHYPE model with accurate performance evaluation. We show the ease and\nexpressiveness of stochastic HYPE for patch-based modelling of hybrid systems.\nMoreover, a mean-field analytical model is proposed as the fluid flow\napproximation of the stochastic HYPE model, which can be used to investigate\nthe average behaviour of the modelled system over an infinite number of\nsimulation runs of the stochastic HYPE model.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2069v1"
    },
    {
        "title": "Exact Solutions for M/M/c/Setup Queues",
        "authors": [
            "Tuan Phung-Duc"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Recently multiserver queues with setup times have been extensively studied\nbecause they have applications in power-saving data centers. The most\nchallenging model is the M/M/$c$/Setup queue where a server is turned off when\nit is idle and is turned on if there are some waiting jobs. Recently, Gandhi et\nal.~(SIGMETRICS 2013, QUESTA 2014) present the recursive renewal reward\napproach as a new mathematical tool to analyze the model. In this paper, we\nderive exact solutions for the same model using two alternative methodologies:\ngenerating function approach and matrix analytic method. The former yields\nseveral theoretical insights into the systems while the latter provides an\nexact recursive algorithm to calculate the joint stationary distribution and\nthen some performance measures so as to give new application insights.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.3084v4"
    },
    {
        "title": "Dealing with Zero Density Using Piecewise Phase-type Approximation",
        "authors": [
            "Ľuboš Korenčiak",
            "Jan Krčál",
            "Vojtěch Řehák"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Every probability distribution can be approximated up to a given precision by\na phase-type distribution, i.e. a distribution encoded by a continuous time\nMarkov chain (CTMC). However, an excessive number of states in the\ncorresponding CTMC is needed for some standard distributions, in particular\nmost distributions with regions of zero density such as uniform or shifted\ndistributions. Addressing this class of distributions, we suggest an\nalternative representation by CTMC extended with discrete-time transitions.\nUsing discrete-time transitions we split the density function into multiple\nintervals. Within each interval, we then approximate the density with standard\nphase-type fitting. We provide an experimental evidence that our method\nrequires only a moderate number of states to approximate such distributions\nwith regions of zero density. Furthermore, the usage of CTMC with discrete-time\ntransitions is supported by a number of techniques for their analysis. Thus,\nour results promise an efficient approach to the transient analysis of a class\nof non-Markovian models.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.7527v1"
    },
    {
        "title": "Accelerating unstructured finite volume computations on\n  field-programmable gate arrays",
        "authors": [
            "Zoltan Nagy",
            "Csaba Nemes",
            "Antal Hiba",
            "Arpad Csik",
            "Andras Kiss",
            "Miklos Ruszinko",
            "Peter Szolgay"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Accurate simulations of various physical processes on digital computers\nrequires huge computing performance, therefore accelerating these scientific\nand engineering applications has a great importance. Density of programmable\nlogic devices doubles in every 18 months according to Moore's Law. On the\nrecent devices around one hundred double precision floating-point adders and\nmultipliers can be implemented. In the paper an FPGA based framework is\ndescribed to efficiently utilize this huge computing power to accelerate\nsimulation of complex physical spatiotemporal phenomena. Simulating complicated\ngeometries requires unstructured spatial discretization which results in\nirregular memory access patterns severely limiting computing performance. Data\nlocality is improved by mesh node renumbering technique which results in\npredictable memory access pattern. Additionally storing a small window of node\ndata in the on-chip memory of the FPGA can increase data reuse and decrease\nmemory bandwidth requirements. Generation of the floating-point data path and\ncontrol structure of the arithmetic unit containing dozens of operators is a\nvery challenging task when the goal is high operating frequency. Long and high\nfanout control lines and improper placement can severely affect computing\nperformance. In the paper an automatic data path generation and partitioning\nalgorithm is presented to eliminate long delays and aid placement of the\ncircuit. Efficiency and use of the framework is described by a case study\nsolving the Euler equations on an unstructured mesh using finite volume\ntechnique. On the currently available largest FPGA the generated architecture\ncontains three processing elements working in parallel providing 90 times\nspeedup compared to a high performance microprocessor core.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.5715v1"
    },
    {
        "title": "Buffer occupancy asymptotics in rate proportional sharing networks with\n  heterogeneous long-tailed inputs",
        "authors": [
            "Oczan Ozturk",
            "Ravi R. Mazumdar",
            "Nikolay B. Likhanov"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  In this paper, we consider a network of rate proportional processor sharing\nservers in which sessions with long-tailed duration arrive as Poisson\nprocesses. In particular, we assume that a session of type $n$ transmits at a\nrate $r_n$ bits per unit time and lasts for a random time $\\tau_n$ with a\ngeneralized Pareto distribution given by $P \\{\\tau_n > x\\} \\sim \\alpha_n\nx^{-(1+\\beta_n)}$ for large $x$, where $\\alpha_n, \\beta_n > 0$. The weights are\ntaken to be the rates of the flows. The network is assumed to be loop-free with\nrespect to source-destination routes. We characterize the order $O-$asymptotics\nof the complementary buffer occupancy distribution at each node in terms of the\ninput characteristics of the sessions. In particular, we show that the\ndistributions obey a power law whose exponent can be calculated via solving a\nfixed point and deterministic knapsack problem. The paper concludes with some\ncanonical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.2867v1"
    },
    {
        "title": "Modeling LRU caches with Shot Noise request processes",
        "authors": [
            "Emilio Leonardi",
            "Giovanni Luca Torrisi"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  In this paper we analyze Least Recently Used (LRU) caches operating under the\nShot Noise requests Model (SNM). The SNM was recently proposed to better\ncapture the main characteristics of today Video on Demand (VoD) traffic. We\ninvestigate the validity of Che's approximation through an asymptotic analysis\nof the cache eviction time. In particular, we provide a large deviation\nprinciple, a law of large numbers and a central limit theorem for the cache\neviction time, as the cache size grows large. Finally, we derive upper and\nlower bounds for the \"hit\" probability in tandem networks of caches under Che's\napproximation.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.4759v4"
    },
    {
        "title": "Efficient analysis of caching strategies under dynamic content\n  popularity",
        "authors": [
            "Michele Garetto",
            "Emilio Leonardi",
            "Stefano Traverso"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  In this paper we develop a novel technique to analyze both isolated and\ninterconnected caches operating under different caching strategies and\nrealistic traffic conditions. The main strength of our approach is the ability\nto consider dynamic contents which are constantly added into the system\ncatalogue, and whose popularity evolves over time according to desired\nprofiles. We do so while preserving the simplicity and computational efficiency\nof models developed under stationary popularity conditions, which are needed to\nanalyze several caching strategies. Our main achievement is to show that the\nimpact of content popularity dynamics on cache performance can be effectively\ncaptured into an analytical model based on a fixed content catalogue (i.e., a\ncatalogue whose size and objects' popularity do not change over time).\n",
        "pdf_link": "http://arxiv.org/pdf/1411.7224v1"
    },
    {
        "title": "Proceedings of the 5th International Workshop on Adaptive Self-tuning\n  Computing Systems 2015 (ADAPT'15)",
        "authors": [
            "Christophe Dubach",
            "Grigori Fursin"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  This is the proceedings of the 5th International Workshop on Adaptive\nSelf-tuning Computing Systems 2015 (ADAPT'15).\n",
        "pdf_link": "http://arxiv.org/pdf/1412.2347v1"
    },
    {
        "title": "Accelerating Correlation Power Analysis Using Graphics Processing Units",
        "authors": [
            "Hasindu Gamaarachchi",
            "Roshan Ragel",
            "Darshana Jayasinghe"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Correlation Power Analysis (CPA) is a type of power analysis based side\nchannel attack that can be used to derive the secret key of encryption\nalgorithms including DES (Data Encryption Standard) and AES (Advanced\nEncryption Standard). A typical CPA attack on unprotected AES is performed by\nanalysing a few thousand power traces that requires about an hour of\ncomputational time on a general purpose CPU. Due to the severity of this\nsituation, a large number of researchers work on countermeasures to such\nattacks. Verifying that a proposed countermeasure works well requires\nperforming the CPA attack on about 1.5 million power traces. Such processing,\neven for a single attempt of verification on commodity hardware would run for\nseveral days making the verification process infeasible. Modern Graphics\nProcessing Units (GPUs) have support for thousands of light weight threads,\nmaking them ideal for parallelizable algorithms like CPA. While the cost of a\nGPU being lesser than a high performance multicore server, still the GPU\nperformance for this algorithm is many folds better than that of a multicore\nserver. We present an algorithm and its implementation on GPU for CPA on\n128-bit AES that is capable of executing 1300x faster than that on a single\nthreaded CPU and more than 60x faster than that on a 32 threaded multicore\nserver. We show that an attack that would take hours on the multicore server\nwould take even less than a minute on a much cost effective GPU.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.7682v1"
    },
    {
        "title": "Effective Handling of Urgent Jobs - Speed Up Scheduling for Computing\n  Applications",
        "authors": [
            "Yash Gupta",
            "Kamalakar Karlapalem"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  A queue is required when a service provider is not able to handle jobs\narriving over the time. In a highly flexible and dynamic environment, some jobs\nmight demand for faster execution at run-time especially when the resources are\nlimited and the jobs are competing for acquiring resources. A user might demand\nfor speed up (reduced wait time) for some of the jobs present in the queue at\nrun time. In such cases, it is required to accelerate (directly sending the job\nto the server) urgent jobs (requesting for speed up) ahead of other jobs\npresent in the queue for an earlier completion of urgent jobs. Under the\nassumption of no additional resources, such acceleration of jobs would result\nin slowing down of other jobs present in the queue. In this paper, we formulate\nthe problem of Speed Up Scheduling without acquiring any additional resources\nfor the scheduling of on-line speed up requests posed by a user at run-time and\npresent algorithms for the same. We apply the idea of Speed Up Scheduling to\ntwo different domains -Web Scheduling and CPU Scheduling. We demonstrate our\nresults with a simulation based model using trace driven workload and synthetic\ndatasets to show the usefulness of Speed Up scheduling. Speed Up provides a new\nway of addressing urgent jobs, provides a different evaluation criteria for\ncomparing scheduling algorithms and has practical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.06301v1"
    },
    {
        "title": "On the tradeoff of average delay, average service cost, and average\n  utility for single server queues with monotone policies",
        "authors": [
            "Vineeth Bala Sukumaran"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  In this thesis, we study the optimal tradeoff of average delay, average\nservice cost, and average utility for single server queueing models, with and\nwithout admission control. The continuous time and discrete time queueing\nmodels that we consider are motivated by cross-layer models for noisy\npoint-to-point links, with random packet arrivals. We study the above tradeoff\nproblem for a class of admissible policies, which are monotone and stationary\nand obtain an asymptotic characterization of the minimum average delay as a\nfunction of the average service cost and average utility constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.06382v1"
    },
    {
        "title": "Communication Patterns in Mean Field Models for Wireless Sensor Networks",
        "authors": [
            "Mahmoud Talebi",
            "Jan Friso Groote",
            "Jean-Paul Linnartz"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Wireless sensor networks are usually composed of a large number of nodes, and\nwith the increasing processing power and power consumption efficiency they are\nexpected to run more complex protocols in the future. These pose problems in\nthe field of verification and performance evaluation of wireless networks. In\nthis paper, we tailor the mean-field theory as a modeling technique to analyze\ntheir behavior. We apply this method to the slotted ALOHA protocol, and\nestablish results on the long term trends of the protocol within a very large\nnetwork, specially regarding the stability of ALOHA-type protocols.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.07693v2"
    },
    {
        "title": "Are Markov Models Effective for Storage Reliability Modelling?",
        "authors": [
            "Prasenjit Karmakar",
            "K. Gopinath"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Continuous Time Markov Chains (CTMC) have been used extensively to model\nreliability of storage systems. While the exponentially distributed sojourn\ntime of Markov models is widely known to be unrealistic (and it is necessary to\nconsider Weibull-type models for components such as disks), recent work has\nalso highlighted some additional infirmities with the CTMC model, such as the\nability to handle repair times. Due to the memoryless property of these models,\nany failure or repair of one component resets the \"clock\" to zero with any\npartial repair or aging in some other subsystem forgotten. It has therefore\nbeen argued that simulation is the only accurate technique available for\nmodelling the reliability of a storage system with multiple components.\n  We show how both the above problematic aspects can be handled when we\nconsider a careful set of approximations in a detailed model of the system. A\ndetailed model has many states, and the transitions between them and the\ncurrent state captures the \"memory\" of the various components. We model a\nnon-exponential distribution using a sum of exponential distributions, along\nwith the use of a CTMC solver in a probabilistic model checking tool that has\nsupport for reducing large state spaces. Furthermore, it is possible to get\nresults close to what is obtained through simulation and at much lower cost.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.07931v1"
    },
    {
        "title": "Run Time Approximation of Non-blocking Service Rates for Streaming\n  Systems",
        "authors": [
            "Jonathan C. Beard",
            "Roger D. Chamberlain"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Stream processing is a compute paradigm that promises safe and efficient\nparallelism. Modern big-data problems are often well suited for stream\nprocessing's throughput-oriented nature. Realization of efficient stream\nprocessing requires monitoring and optimization of multiple communications\nlinks. Most techniques to optimize these links use queueing network models or\nnetwork flow models, which require some idea of the actual execution rate of\neach independent compute kernel within the system. What we want to know is how\nfast can each kernel process data independent of other communicating kernels.\nThis is known as the \"service rate\" of the kernel within the queueing\nliterature. Current approaches to divining service rates are static. Modern\nworkloads, however, are often dynamic. Shared cloud systems also present\napplications with highly dynamic execution environments (multiple users,\nhardware migration, etc.). It is therefore desirable to continuously re-tune an\napplication during run time (online) in response to changing conditions. Our\napproach enables online service rate monitoring under most conditions,\nobviating the need for reliance on steady state predictions for what are\nprobably non-steady state phenomena. First, some of the difficulties associated\nwith online service rate determination are examined. Second, the algorithm to\napproximate the online non-blocking service rate is described. Lastly, the\nalgorithm is implemented within the open source RaftLib framework for\nvalidation using a simple microbenchmark as well as two full streaming\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00591v2"
    },
    {
        "title": "ALEA: Fine-grain Energy Profiling with Basic Block Sampling",
        "authors": [
            "Lev Mukhanov",
            "Dimitrios S. Nikolopoulos",
            "Bronis R. de Supinski"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Energy efficiency is an essential requirement for all contemporary computing\nsystems. We thus need tools to measure the energy consumption of computing\nsystems and to understand how workloads affect it. Significant recent research\neffort has targeted direct power measurements on production computing systems\nusing on-board sensors or external instruments. These direct methods have in\nturn guided studies of software techniques to reduce energy consumption via\nworkload allocation and scaling. Unfortunately, direct energy measurements are\nhampered by the low power sampling frequency of power sensors. The coarse\ngranularity of power sensing limits our understanding of how power is allocated\nin systems and our ability to optimize energy efficiency via workload\nallocation.\n  We present ALEA, a tool to measure power and energy consumption at the\ngranularity of basic blocks, using a probabilistic approach. ALEA provides\nfine-grained energy profiling via statistical sampling, which overcomes the\nlimitations of power sensing instruments. Compared to state-of-the-art energy\nmeasurement tools, ALEA provides finer granularity without sacrificing\naccuracy. ALEA achieves low overhead energy measurements with mean error rates\nbetween 1.4% and 3.5% in 14 sequential and parallel benchmarks tested on both\nIntel and ARM platforms. The sampling method caps execution time overhead at\napproximately 1%. ALEA is thus suitable for online energy monitoring and\noptimization. Finally, ALEA is a user-space tool with a portable,\nmachine-independent sampling method. We demonstrate two use cases of ALEA,\nwhere we reduce the energy consumption of a k-means computational kernel by 37%\nand an ocean modelling code by 33%, compared to high-performance execution\nbaselines, by varying the power optimization strategy between basic blocks.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.00825v2"
    },
    {
        "title": "Power Aware Wireless File Downloading: A Lyapunov Indexing Approach to A\n  Constrained Restless Bandit Problem",
        "authors": [
            "Xiaohan Wei",
            "Michael J. Neely"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  This paper treats power-aware throughput maxi-mization in a multi-user file\ndownloading system. Each user can receive a new file only after its previous\nfile is finished. The file state processes for each user act as coupled Markov\nchains that form a generalized restless bandit system. First, an optimal\nalgorithm is derived for the case of one user. The algorithm maximizes\nthroughput subject to an average power constraint. Next, the one-user algorithm\nis extended to a low complexity heuristic for the multi-user problem. The\nheuristic uses a simple online index policy. In a special case with no\npower-constraint, the multi-user heuristic is shown to be throughput optimal.\nSimulations are used to demonstrate effectiveness of the heuristic in the\ngeneral case. For simple cases where the optimal solution can be computed\noffline, the heuristic is shown to be near-optimal for a wide range of\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.04074v1"
    },
    {
        "title": "Inhomogeneous CTMC Model of a Call Center with Balking and Abandonment",
        "authors": [
            "Maciej Rafal Burak"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  This paper considers a nonstationary multiserver queuing model with\nabandonment and balking for inbound call centers. We present a continuous time\nMarkov chain (CTMC) model which captures the important characteristics of an\ninbound call center and obtain a numerical solution for its transient state\nprobabilities using uniformization method with steady-state detection.\nKeywords: call center, transient, Markov processes, numerical methods,\nuniformization, abandonment, balking\n",
        "pdf_link": "http://arxiv.org/pdf/1504.07908v1"
    },
    {
        "title": "Data dependent energy modelling for worst case energy consumption\n  analysis",
        "authors": [
            "James Pallister",
            "Steve Kerrison",
            "Jeremy Morse",
            "Kerstin Eder"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Safely meeting Worst Case Energy Consumption (WCEC) criteria requires\naccurate energy modeling of software. We investigate the impact of instruction\noperand values upon energy consumption in cacheless embedded processors.\nExisting instruction-level energy models typically use measurements from random\ninput data, providing estimates unsuitable for safe WCEC analysis.\n  We examine probabilistic energy distributions of instructions and propose a\nmodel for composing instruction sequences using distributions, enabling WCEC\nanalysis on program basic blocks. The worst case is predicted with statistical\nanalysis. Further, we verify that the energy of embedded benchmarks can be\ncharacterised as a distribution, and compare our proposed technique with other\nmethods of estimating energy consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.03374v3"
    },
    {
        "title": "Network Simulator - Visão Geral da Ferramenta de Simulação de\n  Redes",
        "authors": [
            "Marcos Portnoi",
            "Rafael Gonçalves Bezerra de Araújo"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  This paper describes NS - Network Simulator, the computer networks simulation\ntool. We offer an overview NS, and also analyze its characteristics and\nfunctions. Finally, we present in detail all steps for preparing a simulation\nof a simple model in NS.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.05135v1"
    },
    {
        "title": "Scalable Reliability Modelling of RAID Storage Subsystems",
        "authors": [
            "Prasenjit Karmakar",
            "K. Gopinath"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Reliability modelling of RAID storage systems with its various components\nsuch as RAID controllers, enclosures, expanders, interconnects and disks is\nimportant from a storage system designer's point of view. A model that can\nexpress all the failure characteristics of the whole RAID storage system can be\nused to evaluate design choices, perform cost reliability trade-offs and\nconduct sensitivity analyses. However, including such details makes the\ncomputational models of reliability quickly infeasible.\n  We present a CTMC reliability model for RAID storage systems that scales to\nmuch larger systems than heretofore reported and we try to model all the\ncomponents as accurately as possible. We use several state-space reduction\ntechniques at the user level, such as aggregating all in-series components and\nhierarchical decomposition, to reduce the size of our model. To automate\ncomputation of reliability, we use the PRISM model checker as a CTMC solver\nwhere appropriate. Our modelling techniques using PRISM are more practical (in\nboth time and effort) compared to previously reported Monte-Carlo simulation\ntechniques.\n  Our model for RAID storage systems (that includes, for example, disks,\nexpanders, enclosures) uses Weibull distributions for disks and, where\nappropriate, correlated failure modes for disks, while we use exponential\ndistributions with independent failure modes for all other components. To use\nthe CTMC solver, we approximate the Weibull distribution for a disk using sum\nof exponentials and we confirm that this model gives results that are in\nreasonably good agreement with those from the sequential Monte Carlo simulation\nmethods for RAID disk subsystems reported in literature earlier. Using a\ncombination of scalable techniques, we are able to model and compute\nreliability for fairly large configurations with upto 600 disks using this\nmodel.\n",
        "pdf_link": "http://arxiv.org/pdf/1508.02055v1"
    },
    {
        "title": "Automatic Loop Kernel Analysis and Performance Modeling With Kerncraft",
        "authors": [
            "Julian Hammer",
            "Georg Hager",
            "Jan Eitzinger",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Analytic performance models are essential for understanding the performance\ncharacteristics of loop kernels, which consume a major part of CPU cycles in\ncomputational science. Starting from a validated performance model one can\ninfer the relevant hardware bottlenecks and promising optimization\nopportunities. Unfortunately, analytic performance modeling is often tedious\neven for experienced developers since it requires in-depth knowledge about the\nhardware and how it interacts with the software. We present the \"Kerncraft\"\ntool, which eases the construction of analytic performance models for streaming\nkernels and stencil loop nests. Starting from the loop source code, the problem\nsize, and a description of the underlying hardware, Kerncraft can ideally\npredict the single-core performance and scaling behavior of loops on multicore\nprocessors using the Roofline or the Execution-Cache-Memory (ECM) model. We\ndescribe the operating principles of Kerncraft with its capabilities and\nlimitations, and we show how it may be used to quickly gain insights by\naccelerated analytic modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.03778v2"
    },
    {
        "title": "Latency Analysis of an Aerial Video Tracking System Using Fiacre and\n  Tina",
        "authors": [
            "Silvano Dal Zilio",
            "Bernard Berthomieu",
            "Didier Le Botlan"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  We describe our experience with modeling a video tracking system used to\ndetect and follow moving targets from an airplane. We provide a formal model\nthat takes into account the real-time properties of the system and use it to\ncompute the worst and best-case end to end latency. We also compute a lower\nbound on the delay between the loss of two frames. Our approach is based on the\nmodel-checking tool Tina, that provides state-space generation and\nmodel-checking algorithms for an extension of Time Petri Nets with data and\npriorities. We propose several models divided in two main categories: first\nTime Petri Net models, which are used to study the behavior of the system in\nthe most basic way; then models based on the Fiacre specification language,\nwhere we take benefit of richer data structures to directly model the buffering\nof video information and the use of an unbounded number of frame identifiers.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.06506v1"
    },
    {
        "title": "Contrasting Effects of Replication in Parallel Systems: From Overload to\n  Underload and Back",
        "authors": [
            "Felix Poloczek",
            "Florin Ciucu"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Task replication has recently been advocated as a practical solution to\nreduce latencies in parallel systems. In addition to several convincing\nempirical studies, some others provide analytical results, yet under some\nstrong assumptions such as Poisson arrivals, exponential service times, or\nindependent service times of the replicas themselves, which may lend themselves\nto some contrasting and perhaps contriving behavior. For instance, under the\nsecond assumption, an overloaded system can be stabilized by a replication\nfactor, but can be sent back in overload through further replication. In turn,\nunder the third assumption, strictly larger stability regions of replication\nsystems do not necessarily imply smaller delays.\n  Motivated by the need to dispense with such common and restricting\nassumptions, which may additionally cause unexpected behavior, we develop a\nunified and general theoretical framework to compute tight bounds on the\ndistribution of response times in general replication systems. These results\nimmediately lend themselves to the optimal number of replicas minimizing\nresponse time quantiles, depending on the parameters of the system (e.g., the\ndegree of correlation amongst replicas). As a concrete application of our\nframework, we design a novel replication policy which can improve the stability\nregion of classical fork-join queueing systems by $\\mathcal{O}(\\ln K)$, in the\nnumber of servers $K$.\n",
        "pdf_link": "http://arxiv.org/pdf/1602.07978v1"
    },
    {
        "title": "Balanced Fair Resource Sharing in Computer Clusters",
        "authors": [
            "Thomas Bonald",
            "Céline Comte"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We represent a computer cluster as a multi-server queue with some arbitrary\nbipartite graph of compatibilities between jobs and servers. Each server\nprocesses its jobs sequentially in FCFS order. The service rate of a job at any\ngiven time is the sum of the service rates of all servers processing this job.\nWe show that the corresponding queue is quasi-reversible and use this property\nto design a scheduling algorithm achieving balanced fair sharing of the service\ncapacity.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.06763v3"
    },
    {
        "title": "Stochastic Modeling of Hybrid Cache Systems",
        "authors": [
            "Gaoying Ju",
            "Yongkun Li",
            "Yinlong Xu",
            "Jiqiang Chen",
            "John C. S. Lui"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  In recent years, there is an increasing demand of big memory systems so to\nperform large scale data analytics. Since DRAM memories are expensive, some\nresearchers are suggesting to use other memory systems such as non-volatile\nmemory (NVM) technology to build large-memory computing systems. However,\nwhether the NVM technology can be a viable alternative (either economically and\ntechnically) to DRAM remains an open question. To answer this question, it is\nimportant to consider how to design a memory system from a \"system\nperspective\", that is, incorporating different performance characteristics and\nprice ratios from hybrid memory devices.\n  This paper presents an analytical model of a \"hybrid page cache system\" so to\nunderstand the diverse design space and performance impact of a hybrid cache\nsystem. We consider (1) various architectural choices, (2) design strategies,\nand (3) configuration of different memory devices. Using this model, we provide\nguidelines on how to design hybrid page cache to reach a good trade-off between\nhigh system throughput (in I/O per sec or IOPS) and fast cache reactivity which\nis defined by the time to fill the cache. We also show how one can configure\nthe DRAM capacity and NVM capacity under a fixed budget. We pick PCM as an\nexample for NVM and conduct numerical analysis. Our analysis indicates that\nincorporating PCM in a page cache system significantly improves the system\nperformance, and it also shows larger benefit to allocate more PCM in page\ncache in some cases. Besides, for the common setting of performance-price ratio\nof PCM, \"flat architecture\" offers as a better choice, but \"layered\narchitecture\" outperforms if PCM write performance can be significantly\nimproved in the future.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.00714v2"
    },
    {
        "title": "Optimal Placement of Cores, Caches and Memory Controllers in Network\n  On-Chip",
        "authors": [
            "Diman Zad Tootaghaj",
            "Farshid Farhat"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Parallel programming is emerging fast and intensive applications need more\nresources, so there is a huge demand for on-chip multiprocessors. Accessing L1\ncaches beside the cores are the fastest after registers but the size of private\ncaches cannot increase because of design, cost and technology limits. Then\nsplit I-cache and D-cache are used with shared LLC (last level cache). For a\nunified shared LLC, bus interface is not scalable, and it seems that\ndistributed shared LLC (DSLLC) is a better choice. Most of papers assume a\ndistributed shared LLC beside each core in on-chip network. Many works assume\nthat DSLLCs are placed in all cores; however, we will show that this design\nignores the effect of traffic congestion in on-chip network. In fact, our work\nfocuses on optimal placement of cores, DSLLCs and even memory controllers to\nminimize the expected latency based on traffic load in a mesh on-chip network\nwith fixed number of cores and total cache capacity. We try to do some\nanalytical modeling deriving intended cost function and then optimize the mean\ndelay of the on-chip network communication. This work is supposed to be\nverified using some traffic patterns that are run on CSIM simulator.\n",
        "pdf_link": "http://arxiv.org/pdf/1607.04298v4"
    },
    {
        "title": "Coz: Finding Code that Counts with Causal Profiling",
        "authors": [
            "Charlie Curtsinger",
            "Emery D. Berger"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Improving performance is a central concern for software developers. To locate\noptimization opportunities, developers rely on software profilers. However,\nthese profilers only report where programs spent their time: optimizing that\ncode may have no impact on performance. Past profilers thus both waste\ndeveloper time and make it difficult for them to uncover significant\noptimization opportunities.\n  This paper introduces causal profiling. Unlike past profiling approaches,\ncausal profiling indicates exactly where programmers should focus their\noptimization efforts, and quantifies their potential impact. Causal profiling\nworks by running performance experiments during program execution. Each\nexperiment calculates the impact of any potential optimization by virtually\nspeeding up code: inserting pauses that slow down all other code running\nconcurrently. The key insight is that this slowdown has the same relative\neffect as running that line faster, thus \"virtually\" speeding it up.\n  We present Coz, a causal profiler, which we evaluate on a range of\nhighly-tuned applications: Memcached, SQLite, and the PARSEC benchmark suite.\nCoz identifies previously unknown optimization opportunities that are both\nsignificant and targeted. Guided by Coz, we improve the performance of\nMemcached by 9%, SQLite by 25%, and accelerate six PARSEC applications by as\nmuch as 68%; in most cases, these optimizations involve modifying under 10\nlines of code.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03676v1"
    },
    {
        "title": "Performance prediction of finite-difference solvers for different\n  computer architectures",
        "authors": [
            "Mathias Louboutin",
            "Michael Lange",
            "Felix Herrmann",
            "Navjot Kukreja",
            "Gerard Gorman"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  The life-cycle of a partial differential equation (PDE) solver is often\ncharacterized by three development phases: the development of a stable\nnumerical discretization, development of a correct (verified) implementation,\nand the optimization of the implementation for different computer\narchitectures. Often it is only after significant time and effort has been\ninvested that the performance bottlenecks of a PDE solver are fully understood,\nand the precise details varies between different computer architectures. One\nway to mitigate this issue is to establish a reliable performance model that\nallows a numerical analyst to make reliable predictions of how well a numerical\nmethod would perform on a given computer architecture, before embarking upon\npotentially long and expensive implementation and optimization phases. The\navailability of a reliable performance model also saves developer effort as it\nboth informs the developer on what kind of optimisations are beneficial, and\nwhen the maximum expected performance has been reached and optimisation work\nshould stop. We show how discretization of a wave equation can be theoretically\nstudied to understand the performance limitations of the method on modern\ncomputer architectures. We focus on the roofline model, now broadly used in the\nhigh-performance computing community, which considers the achievable\nperformance in terms of the peak memory bandwidth and peak floating point\nperformance of a computer with respect to algorithmic choices. A first\nprinciples analysis of operational intensity for key time-stepping\nfinite-difference algorithms is presented. With this information available at\nthe time of algorithm design, the expected performance on target computer\nsystems can be used as a driver for algorithm design.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.03984v3"
    },
    {
        "title": "Robust benchmarking in noisy environments",
        "authors": [
            "Jiahao Chen",
            "Jarrett Revels"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We propose a benchmarking strategy that is robust in the presence of timer\nerror, OS jitter and other environmental fluctuations, and is insensitive to\nthe highly nonideal statistics produced by timing measurements. We construct a\nmodel that explains how these strongly nonideal statistics can arise from\nenvironmental fluctuations, and also justifies our proposed strategy. We\nimplement this strategy in the BenchmarkTools Julia package, where it is used\nin production continuous integration (CI) pipelines for developing the Julia\nlanguage and its ecosystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.04295v1"
    },
    {
        "title": "An ECM-based energy-efficiency optimization approach for\n  bandwidth-limited streaming kernels on recent Intel Xeon processors",
        "authors": [
            "Johannes Hofmann",
            "Dietmar Fey"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We investigate an approach that uses low-level analysis and the\nexecution-cache-memory (ECM) performance model in combination with tuning of\nhardware parameters to lower energy requirements of memory-bound applications.\nThe ECM model is extended appropriately to deal with software optimizations\nsuch as non-temporal stores. Using incremental steps and the ECM model, we\nanalytically quantify the impact of various single-core optimizations and\npinpoint microarchitectural improvements that are relevant to energy\nconsumption. Using a 2D Jacobi solver as example that can serve as a blueprint\nfor other memory-bound applications, we evaluate our approach on the four most\nrecent Intel Xeon E5 processors (Sandy Bridge-EP, Ivy Bridge-EP, Haswell-EP,\nand Broadwell-EP). We find that chip energy consumption can be reduced in the\nrange of 2.0-2.4$\\times$ on the examined processors.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.03347v1"
    },
    {
        "title": "Poly-Symmetry in Processor-Sharing Systems",
        "authors": [
            "Thomas Bonald",
            "Céline Comte",
            "Virag Shah",
            "Gustavo de Veciana"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We consider a system of processor-sharing queues with state-dependent service\nrates. These are allocated according to balanced fairness within a polymatroid\ncapacity set. Balanced fairness is known to be both insensitive and\nPareto-efficient in such systems, which ensures that the performance metrics,\nwhen computable, will provide robust insights into the real performance of the\nsystem considered. We first show that these performance metrics can be\nevaluated with a complexity that is polynomial in the system size if the system\nis partitioned into a finite number of parts, so that queues are exchangeable\nwithin each part and asymmetric across different parts. This in turn allows us\nto derive stochastic bounds for a larger class of systems which satisfy less\nrestrictive symmetry assumptions. These results are applied to practical\nexamples of tree data networks, such as backhaul networks of Internet service\nproviders, and computer clusters.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.00560v3"
    },
    {
        "title": "10-millisecond Computing",
        "authors": [
            "Gang Lu",
            "Jianfeng Zhan",
            "Tianshu Hao",
            "Lei Wang"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Despite computation becomes much complex on data with an unprecedented scale,\nwe argue computers or smart devices should and will consistently provide\ninformation and knowledge to human being in the order of a few tens\nmilliseconds. We coin a new term 10-millisecond computing to call attention to\nthis class of workloads. 10-millisecond computing raises many challenges for\nboth software and hardware stacks. In this paper, using a typical\nworkload-memcached on a 40-core server (a main-stream server in near future),\nwe quantitatively measure 10-ms computing's challenges to conventional\noperating systems. For better communication, we propose a simple metric-outlier\nproportion to measure quality of service: for N completed requests or jobs, if\nM jobs or requests' latencies exceed the outlier threshold t, the outlier\nproportion is M/N . For a 1K-scale system running Linux (version 2.6.32), LXC\n(version 0.7.5) or XEN (version 4.0.0), respectively, we surprisingly find that\nso as to reduce the service outlier proportion to 10% (10% users will feel QoS\ndegradation), the outlier proportion of a single server has to be reduced by\n871X, 2372X, 2372X accordingly. Also, we discuss the possible design spaces of\n10-ms computing systems from perspectives of datacenter architectures,\nnetworking, OS and scheduling, and benchmarking.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.01267v3"
    },
    {
        "title": "Breakdown of a Benchmark Score Without Internal Analysis of Benchmarking\n  Program",
        "authors": [
            "Naoki Matagawa",
            "Kazuyuki Shudo"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  A breakdown of a benchmark score is how much each aspect of the system\nperformance affects the score. Existing methods require internal analysis on\nthe benchmarking program and then involve the following problems: (1) require a\ncertain amount of labor for code analysis, profiling, simulation, and so on and\n(2) require the benchmarking program itself. In this paper, we present a method\nfor breaking down a benchmark score without internal analysis of the\nbenchmarking program. The method utilizes regression analysis of benchmark\nscores on a number of systems. Experimental results with 3 benchmarks on 15\nAndroid smartphones showed that our method could break down those benchmark\nscores even though there is room for improvement in accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.06307v1"
    },
    {
        "title": "Groups of Repairmen and Repair-based Load Balancing in Supermarket\n  Models with Repairable Servers",
        "authors": [
            "Na Li",
            "Quan-Lin Li",
            "Zhe George Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Supermarket models are a class of interesting parallel queueing networks with\ndynamic randomized load balancing and real-time resource management. When the\nparallel servers are subject to breakdowns and repairs, analysis of such a\nsupermarket model becomes more difficult and challenging. In this paper, we\napply the mean-field theory to studying four interrelated supermarket models\nwith repairable servers, and numerically indicate impact of the different\nrepairman groups on performance of the systems. First, we set up the systems of\nmean-field equations for the supermarket models with repairable servers. Then\nwe prove the asymptotic independence of the supermarket models through the\noperator semi-group and the mean-field limit. Furthermore, we show that the\nfixed points of the supermarket models satisfy the systems of nonlinear\nequations. Finally, we use the fixed points to give numerical computation for\nperformer analysis, and provide valuable observations on model improvement.\nTherefore, this paper provides a new and effective method in the study of\ncomplex supermarket models.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.08823v1"
    },
    {
        "title": "Stationary Distribution of a Generalized LRU-MRU Content Cache",
        "authors": [
            "George Kesidis"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Many different caching mechanisms have been previously proposed, exploring\ndifferent insertion and eviction policies and their performance individually\nand as part of caching networks. We obtain a novel closed-form stationary\ninvariant distribution for a generalization of LRU and MRU caching nodes under\na reference Markov model. Numerical comparisons are made with an \"Incremental\nRank Progress\" (IRP a.k.a. CLIMB) and random eviction (a.k.a. random\nreplacement) methods under a steady-state Zipf popularity distribution. The\nrange of cache hit probabilities is smaller under MRU and larger under IRP\ncompared to LRU. We conclude with the invariant distribution for a special case\nof a random-eviction caching tree-network and associated discussion.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.04849v8"
    },
    {
        "title": "Correcting for Non-Markovian Asymptotic Effects using Markovian\n  Representation",
        "authors": [
            "Vitali Volovoi"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Asymptotic properties of Markov Processes, such as steady state probabilities\nor hazard rate for absorbing states can be efficiently calculated by means of\nlinear algebra even for large-scale problems. This paper discusses the methods\nfor adjusting parameters of the Markov models to account for non-constant\ntransition rates. In particular, transitions with fixed delays are considered\nalong with the transitions that follow Weibull and lognormal distributions.\nProcedures for both steady-state solutions in the absence of an absorbing\nstate, and for hazard rates to an absorbing state are provided and demonstrated\non several examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.01070v2"
    },
    {
        "title": "Scheduling Distributed Resources in Heterogeneous Private Clouds",
        "authors": [
            "George Kesidis",
            "Yuquan Shan",
            "Yujia Wang",
            "Bhuvan Urgaonkar",
            "Jalal Khamse-Ashari",
            "Ioanns Lambadaris"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We first consider the static problem of allocating resources to ( i.e. ,\nscheduling) multiple distributed application framework s, possibly with\ndifferent priorities and server preferences , in a private cloud with\nheterogeneous servers. Several fai r scheduling mechanisms have been proposed\nfor this purpose. We extend pr ior results on max-min and proportional fair\nscheduling to t his constrained multiresource and multiserver case for generi c\nfair scheduling criteria. The task efficiencies (a metric r elated to\nproportional fairness) of max-min fair allocations found b y progressive\nfilling are compared by illustrative examples . They show that \"server\nspecific\" fairness criteria and those that are b ased on residual (unreserved)\nresources are more efficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.06102v5"
    },
    {
        "title": "Mira: A Framework for Static Performance Analysis",
        "authors": [
            "Kewen Meng",
            "Boyana Norris"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  The performance model of an application can pro- vide understanding about its\nruntime behavior on particular hardware. Such information can be analyzed by\ndevelopers for performance tuning. However, model building and analyzing is\nfrequently ignored during software development until perfor- mance problems\narise because they require significant expertise and can involve many\ntime-consuming application runs. In this paper, we propose a fast, accurate,\nflexible and user-friendly tool, Mira, for generating performance models by\napplying static program analysis, targeting scientific applications running on\nsupercomputers. We parse both the source code and binary to estimate\nperformance attributes with better accuracy than considering just source or\njust binary code. Because our analysis is static, the target program does not\nneed to be executed on the target architecture, which enables users to perform\nanalysis on available machines instead of conducting expensive exper- iments on\npotentially expensive resources. Moreover, statically generated models enable\nperformance prediction on non-existent or unavailable architectures. In\naddition to flexibility, because model generation time is significantly reduced\ncompared to dynamic analysis approaches, our method is suitable for rapid\napplication performance analysis and improvement. We present several scientific\napplication validation results to demonstrate the current capabilities of our\napproach on small benchmarks and a mini application.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07575v1"
    },
    {
        "title": "Application of the Computer Capacity to the Analysis of Processors\n  Evolution",
        "authors": [
            "Boris Ryabko",
            "Anton Rakitskiy"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  The notion of computer capacity was proposed in 2012, and this quantity has\nbeen estimated for computers of different kinds.\n  In this paper we show that, when designing new processors, the manufacturers\nchange the parameters that affect the computer capacity. This allows us to\npredict the values of parameters of future processors. As the main example we\nuse Intel processors, due to the accessibility of detailed description of all\ntheir technical characteristics.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07730v1"
    },
    {
        "title": "Cost-Performance Tradeoffs in Fusing Unreliable Computational Units",
        "authors": [
            "Mehmet A. Donmez",
            "Maxim Raginsky",
            "Andrew C. Singer",
            "Lav R. Varshney"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We investigate fusing several unreliable computational units that perform the\nsame task. We model an unreliable computational outcome as an additive\nperturbation to its error-free result in terms of its fidelity and cost. We\nanalyze performance of repetition-based strategies that distribute cost across\nseveral unreliable units and fuse their outcomes. When the cost is a convex\nfunction of fidelity, the optimal repetition-based strategy in terms of\nincurred cost while achieving a target mean-square error (MSE) performance may\nfuse several computational units. For concave and linear costs, a single more\nreliable unit incurs lower cost compared to fusion of several lower cost and\nless reliable units while achieving the same MSE performance. We show how our\nresults give insight into problems from theoretical neuroscience, circuits, and\ncrowdsourcing.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.07779v1"
    },
    {
        "title": "A refined and asymptotic analysis of optimal stopping problems of Bruss\n  and Weber",
        "authors": [
            "Guy Louchard"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  The classical secretary problem has been generalized over the years into\nseveral directions. In this paper we confine our interest to those\ngeneralizations which have to do with the more general problem of stopping on a\nlast observation of a specific kind. We follow Dendievel, (where a bibliography\ncan be found) who studies several types of such problems, mainly initiated by\nBruss and Weber. Whether in discrete time or continuous time, whether all\nparameters are known or must be sequentially estimated, we shall call such\nproblems simply \"Bruss-Weber problems\". Our contribution in the present paper\nis a refined analysis of several problems in this class and a study of the\nasymptotic behaviour of solutions.\n  The problems we consider center around the following model. Let\n$X_1,X_2,\\ldots,X_n$ be a sequence of independent random variables which can\ntake three values: $\\{+1,-1,0\\}.$ Let $p:=\\P(X_i=1), p':=\\P(X_i=-1),\n\\qt:=\\P(X_i=0), p\\geq p'$, where $p+p'+\\qt=1$. The goal is to maximize the\nprobability of stopping on a value $+1$ or $-1$ appearing for the last time in\nthe sequence. Following a suggestion by Bruss, we have also analyzed an\nx-strategy with incomplete information: the cases $p$ known, $n$ unknown, then\n$n$ known, $p$ unknown and finally $n,p$ unknown are considered. We also\npresent simulations of the corresponding complete selection algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1705.09482v1"
    },
    {
        "title": "Tracking System Behaviour from Resource Usage Data",
        "authors": [
            "Niyazi Sorkunlu",
            "Varun Chandola",
            "Abani Patra"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Resource usage data, collected using tools such as TACC Stats, capture the\nresource utilization by nodes within a high performance computing system. We\npresent methods to analyze the resource usage data to understand the system\nperformance and identify performance anomalies. The core idea is to model the\ndata as a three-way tensor corresponding to the compute nodes, usage metrics,\nand time. Using the reconstruction error between the original tensor and the\ntensor reconstructed from a low rank tensor decomposition, as a scalar\nperformance metric, enables us to monitor the performance of the system in an\nonline fashion. This error statistic is then used for anomaly detection that\nrelies on the assumption that the normal/routine behavior of the system can be\ncaptured using a low rank approx- imation of the original tensor. We evaluate\nthe performance of the algorithm using information gathered from system logs\nand show that the performance anomalies identified by the proposed method\ncorrelates with critical errors reported in the system logs. Results are shown\nfor data collected for 2013 from the Lonestar4 system at the Texas Advanced\nComputing Center (TACC)\n",
        "pdf_link": "http://arxiv.org/pdf/1705.10756v1"
    },
    {
        "title": "On Resource Pooling and Separation for LRU Caching",
        "authors": [
            "Jian Tan",
            "Guocong Quan",
            "Kaiyi Ji",
            "Ness Shroff"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Caching systems using the Least Recently Used (LRU) principle have now become\nubiquitous. A fundamental question for these systems is whether the cache space\nshould be pooled together or divided to serve multiple flows of data item\nrequests in order to minimize the miss probabilities. In this paper, we show\nthat there is no straight yes or no answer to this question, depending on\ncomplex combinations of critical factors, including, e.g., request rates,\noverlapped data items across different request flows, data item popularities\nand their sizes. Specifically, we characterize the asymptotic miss\nprobabilities for multiple competing request flows under resource pooling and\nseparation for LRU caching when the cache size is large.\n  Analytically, we show that it is asymptotically optimal to jointly serve\nmultiple flows if their data item sizes and popularity distributions are\nsimilar and their arrival rates do not differ significantly; the\nself-organizing property of LRU caching automatically optimizes the resource\nallocation among them asymptotically. Otherwise, separating these flows could\nbe better, e.g., when data sizes vary significantly. We also quantify critical\npoints beyond which resource pooling is better than separation for each of the\nflows when the overlapped data items exceed certain levels. Technically, we\ngeneralize existing results on the asymptotic miss probability of LRU caching\nfor a broad class of heavy-tailed distributions and extend them to multiple\ncompeting flows with varying data item sizes, which also validates the Che\napproximation under certain conditions. These results provide new insights on\nimproving the performance of caching systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.01673v1"
    },
    {
        "title": "GARDENIA: A Domain-specific Benchmark Suite for Next-generation\n  Accelerators",
        "authors": [
            "Zhen Xu",
            "Xuhao Chen",
            "Jie Shen",
            "Yang Zhang",
            "Cheng Chen",
            "Canqun Yang"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  This paper presents the Graph Analytics Repository for Designing\nNext-generation Accelerators (GARDENIA), a benchmark suite for studying\nirregular algorithms on massively parallel accelerators. Existing generic\nbenchmarks for accelerators have mainly focused on high performance computing\n(HPC) applications with limited control and data irregularity, while available\ngraph analytics benchmarks do not apply state-of-the-art algorithms and/or\noptimization techniques. GARDENIA includes emerging irregular applications in\nbig-data and machine learning domains which mimic massively multithreaded\ncommercial programs running on modern large-scale datacenters. Our\ncharacterization shows that GARDENIA exhibits irregular microarchitectural\nbehavior which is quite different from structured workloads and\nstraightforward-implemented graph benchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.04567v4"
    },
    {
        "title": "Optimal Threshold Policies for Robust Data Center Control",
        "authors": [
            "Paul Weng",
            "Zeqi Qiu",
            "John Costanzo",
            "Xiaoqi Yin",
            "Bruno Sinopoli"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  With the simultaneous rise of energy costs and demand for cloud computing,\nefficient control of data centers becomes crucial. In the data center control\nproblem, one needs to plan at every time step how many servers to switch on or\noff in order to meet stochastic job arrivals while trying to minimize\nelectricity consumption. This problem becomes particularly challenging when\nservers can be of various types and jobs from different classes can only be\nserved by certain types of server, as it is often the case in real data\ncenters. We model this problem as a robust Markov Decision Process (i.e., the\ntransition function is not assumed to be known precisely). We give sufficient\nconditions (which seem to be reasonable and satisfied in practice) guaranteeing\nthat an optimal threshold policy exists. This property can then be exploited in\nthe design of an efficient solving method, which we provide. Finally, we\npresent some experimental results demonstrating the practicability of our\napproach and compare with a previous related approach based on model predictive\ncontrol.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.07036v4"
    },
    {
        "title": "Insensitivity of the mean-field Limit of Loss Systems Under Power-of-d\n  Routing",
        "authors": [
            "Thirupathaiah Vasantam",
            "Arpan Mukhopadhyay",
            "Ravi R Mazumdar"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  In this paper, we study large multi-server loss models under power-of-$d$\nrouting scheme when service time distributions are general with finite mean.\nPrevious works have addressed the exponential service time case when the number\nof servers goes to infinity giving rise to a mean field model. The fixed point\nof limiting mean field equations (MFE) was shown to be insensitive to the\nservice time distribution through simulation. Showing insensitivity to general\nservice time distributions has remained an open problem. Obtaining the MFE in\nthis case poses a challenge due to the resulting Markov description of the\nsystem being in positive orthant as opposed to a finite chain in the\nexponential case. In this paper, we first obtain the MFE and then show that the\nMFE has a unique fixed point that coincides with the fixed point in the\nexponential case thus establishing insensitivity. The approach is via a\nmeasure-valued Markov process representation and the martingale problem to\nestablish the mean-field limit. The techniques can be applied to other queueing\nmodels.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.09328v1"
    },
    {
        "title": "Queueing systems with renovation vs. queues with RED. Supplementary\n  Material",
        "authors": [
            "Mikhail Konovalov",
            "Rostislav Razumchik"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  In this note we consider M/D/1/N queue with renovation and derive analytic\nexpressions for the following performance characteristics: stationary loss\nrate, moments of the number in the system. Moments of consecutive losses,\nwaiting/sojourn time are out of scope. The motivation for studying these\ncharacteristics is in the comparison of renovation with known active queue\nmechanisms like RED.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.01477v1"
    },
    {
        "title": "GB-PANDAS: Throughput and heavy-traffic optimality analysis for affinity\n  scheduling",
        "authors": [
            "Ali Yekkehkhany",
            "Avesta Hojjati",
            "Mohammad H Hajiesmaili"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Dynamic affinity scheduling has been an open problem for nearly three\ndecades. The problem is to dynamically schedule multi-type tasks to\nmulti-skilled servers such that the resulting queueing system is both stable in\nthe capacity region (throughput optimality) and the mean delay of tasks is\nminimized at high loads near the boundary of the capacity region (heavy-traffic\noptimality). As for applications, data-intensive analytics like MapReduce,\nHadoop, and Dryad fit into this setting, where the set of servers is\nheterogeneous for different task types, so the pair of task type and server\ndetermines the processing rate of the task. The load balancing algorithm used\nin such frameworks is an example of affinity scheduling which is desired to be\nboth robust and delay optimal at high loads when hot-spots occur. Fluid model\nplanning, the MaxWeight algorithm, and the generalized $c\\mu$-rule are among\nthe first algorithms proposed for affinity scheduling that have theoretical\nguarantees on being optimal in different senses, which will be discussed in the\nrelated work section. All these algorithms are not practical for use in data\ncenter applications because of their non-realistic assumptions. The\njoin-the-shortest-queue-MaxWeight (JSQ-MaxWeight), JSQ-Priority, and\nweighted-workload algorithms are examples of load balancing policies for\nsystems with two and three levels of data locality with a rack structure. In\nthis work, we propose the Generalized-Balanced-Pandas algorithm (GB-PANDAS) for\na system with multiple levels of data locality and prove its throughput\noptimality. We prove this result under an arbitrary distribution for service\ntimes, whereas most previous theoretical work assumes geometric distribution\nfor service times. The extensive simulation results show that the GB-PANDAS\nalgorithm alleviates the mean delay and has a better performance than the\nJSQ-MaxWeight algorithm by twofold\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08115v1"
    },
    {
        "title": "Report from GI-Dagstuhl Seminar 16394: Software Performance Engineering\n  in the DevOps World",
        "authors": [
            "Andre van Hoorn",
            "Pooyan Jamshidi",
            "Philipp Leitner",
            "Ingo Weber"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  This report documents the program and the outcomes of GI-Dagstuhl Seminar\n16394 \"Software Performance Engineering in the DevOps World\".\n  The seminar addressed the problem of performance-aware DevOps. Both, DevOps\nand performance engineering have been growing trends over the past one to two\nyears, in no small part due to the rise in importance of identifying\nperformance anomalies in the operations (Ops) of cloud and big data systems and\nfeeding these back to the development (Dev). However, so far, the research\ncommunity has treated software engineering, performance engineering, and cloud\ncomputing mostly as individual research areas. We aimed to identify\ncross-community collaboration, and to set the path for long-lasting\ncollaborations towards performance-aware DevOps.\n  The main goal of the seminar was to bring together young researchers (PhD\nstudents in a later stage of their PhD, as well as PostDocs or Junior\nProfessors) in the areas of (i) software engineering, (ii) performance\nengineering, and (iii) cloud computing and big data to present their current\nresearch projects, to exchange experience and expertise, to discuss research\nchallenges, and to develop ideas for future collaborations.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.08951v1"
    },
    {
        "title": "Comparison of Parallelisation Approaches, Languages, and Compilers for\n  Unstructured Mesh Algorithms on GPUs",
        "authors": [
            "G. D. Balogh",
            "I. Z. Reguly",
            "G. R. Mudalige"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Efficiently exploiting GPUs is increasingly essential in scientific\ncomputing, as many current and upcoming supercomputers are built using them. To\nfacilitate this, there are a number of programming approaches, such as CUDA,\nOpenACC and OpenMP 4, supporting different programming languages (mainly C/C++\nand Fortran). There are also several compiler suites (clang, nvcc, PGI, XL)\neach supporting different combinations of languages. In this study, we take a\ndetailed look at some of the currently available options, and carry out a\ncomprehensive analysis and comparison using computational loops and\napplications from the domain of unstructured mesh computations. Beyond runtimes\nand performance metrics (GB/s), we explore factors that influence performance\nsuch as register counts, occupancy, usage of different memory types,\ninstruction counts, and algorithmic differences. Results of this work show how\nclang's CUDA compiler frequently outperform NVIDIA's nvcc, performance issues\nwith directive-based approaches on complex kernels, and OpenMP 4 support\nmaturing in clang and XL; currently around 10% slower than CUDA.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.01845v1"
    },
    {
        "title": "Practical Bounds on Optimal Caching with Variable Object Sizes",
        "authors": [
            "Daniel S. Berger",
            "Nathan Beckmann",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Many recent caching systems aim to improve miss ratios, but there is no good\nsense among practitioners of how much further miss ratios can be improved. In\nother words, should the systems community continue working on this problem?\nCurrently, there is no principled answer to this question. In practice, object\nsizes often vary by several orders of magnitude, where computing the optimal\nmiss ratio (OPT) is known to be NP-hard. The few known results on caching with\nvariable object sizes provide very weak bounds and are impractical to compute\non traces of realistic length.\n  We propose a new method to compute upper and lower bounds on OPT. Our key\ninsight is to represent caching as a min-cost flow problem, hence we call our\nmethod the flow-based offline optimal (FOO). We prove that, under simple\nindependence assumptions, FOO's bounds become tight as the number of objects\ngoes to infinity. Indeed, FOO's error over 10M requests of production CDN and\nstorage traces is negligible: at most 0.3%. FOO thus reveals, for the first\ntime, the limits of caching with variable object sizes. While FOO is very\naccurate, it is computationally impractical on traces with hundreds of millions\nof requests. We therefore extend FOO to obtain more efficient bounds on OPT,\nwhich we call practical flow-based offline optimal (PFOO). We evaluate PFOO on\nseveral full production traces and use it to compare OPT to prior online\npolicies. This analysis shows that current caching systems are in fact still\nfar from optimal, suffering 11-43% more cache misses than OPT, whereas the best\nprior offline bounds suggest that there is essentially no room for improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.03709v4"
    },
    {
        "title": "Lattice Boltzmann Benchmark Kernels as a Testbed for Performance\n  Analysis",
        "authors": [
            "Markus Wittmann",
            "Viktor Haag",
            "Thomas Zeiser",
            "Harald Köstler",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Lattice Boltzmann methods (LBM) are an important part of current\ncomputational fluid dynamics (CFD). They allow easy implementations and\nboundary handling. However, competitive time to solution not only depends on\nthe choice of a reasonable method, but also on an efficient implementation on\nmodern hardware. Hence, performance optimization has a long history in the\nlattice Boltzmann community. A variety of options exists regarding the\nimplementation with direct impact on the solver performance. Experimenting and\nevaluating each option often is hard as the kernel itself is typically embedded\nin a larger code base. With our suite of lattice Boltzmann kernels we provide\nthe infrastructure for such endeavors. Already included are several kernels\nranging from simple to fully optimized implementations. Although these kernels\nare not fully functional CFD solvers, they are equipped with a solid\nverification method. The kernels may act as an reference for performance\ncomparisons and as a blue print for optimization strategies. In this paper we\ngive an overview of already available kernels, establish a performance model\nfor each kernel, and show a comparison of implementations and recent\narchitectures.\n",
        "pdf_link": "http://arxiv.org/pdf/1711.11468v1"
    },
    {
        "title": "SOAP: One Clean Analysis of All Age-Based Scheduling Policies",
        "authors": [
            "Ziv Scully",
            "Mor Harchol-Balter",
            "Alan Scheller-Wolf"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We consider an extremely broad class of M/G/1 scheduling policies called\nSOAP: Schedule Ordered by Age-based Priority. The SOAP policies include almost\nall scheduling policies in the literature as well as an infinite number of\nvariants which have never been analyzed, or maybe not even conceived. SOAP\npolicies range from classic policies, like first-come, first-serve (FCFS),\nforeground-background (FB), class-based priority, and shortest remaining\nprocessing time (SRPT); to much more complicated scheduling rules, such as the\nfamously complex Gittins index policy and other policies in which a job's\npriority changes arbitrarily with its age. While the response time of policies\nin the former category is well understood, policies in the latter category have\nresisted response time analysis. We present a universal analysis of all SOAP\npolicies, deriving the mean and Laplace-Stieltjes transform of response time.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.00790v2"
    },
    {
        "title": "Task Scheduling for Heterogeneous Multicore Systems",
        "authors": [
            "Zhuo Chen",
            "Diana Marculescu"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  In recent years, as the demand for low energy and high performance computing\nhas steadily increased, heterogeneous computing has emerged as an important and\npromising solution. Because most workloads can typically run most efficiently\non certain types of cores, mapping tasks on the best available resources can\nnot only save energy but also deliver high performance. However, optimal task\nscheduling for performance and/or energy is yet to be solved for heterogeneous\nplatforms. The work presented herein mathematically formulates the optimal\nheterogeneous system task scheduling as an optimization problem using queueing\ntheory. We analytically solve for the common case of two processor types, e.g.,\nCPU+GPU, and give an optimal policy (CAB). We design the GrIn heuristic to\nefficiently solve for near-optimal policy for any number of processor types\n(within 1.6% of the optimal). Both policies work for any task size distribution\nand processing order, and are therefore, general and practical. We extensively\nsimulate and validate the theory, and implement the proposed policy in a\nCPU-GPU real platform to show the optimal throughput and energy improvement.\nComparing to classic policies like load-balancing, our results range from\n1.08x~2.24x better performance or 1.08x~2.26x better energy efficiency in\nsimulations, and 2.37x~9.07x better performance in experiments.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03209v1"
    },
    {
        "title": "Priority-Aware Near-Optimal Scheduling for Heterogeneous Multi-Core\n  Systems with Specialized Accelerators",
        "authors": [
            "Zhuo Chen",
            "Diana Marculescu"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  To deliver high performance in power limited systems, architects have turned\nto using heterogeneous systems, either CPU+GPU or mixed CPU-hardware systems.\nHowever, in systems with different processor types and task affinities,\nscheduling tasks becomes more challenging than in homogeneous multi-core\nsystems or systems without task affinities. The problem is even more complex\nwhen specialized accelerators and task priorities are included. In this paper,\nwe provide a formal proof for the optimal scheduling policy for heterogeneous\nsystems with arbitrary number of resource types, including specialized\naccelerators, independent of the task arrival rate, task size distribution, and\nresource processing order. We transform the optimal scheduling policy to a\nnonlinear integer optimization problem and propose a fast, near-optimal\nalgorithm. An additional heuristic is proposed for the case of priority-aware\nscheduling. Our experimental results demonstrate that the proposed algorithm is\nonly 0.3% from the optimal and superior to conventional scheduling policies.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.03246v1"
    },
    {
        "title": "Collaborative Uploading in Heterogeneous Networks: Optimal and Adaptive\n  Strategies",
        "authors": [
            "Wasiur R. KhudaBukhsh",
            "Bastian Alt",
            "Sounak Kar",
            "Amr Rizk",
            "Heinz Koeppl"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Collaborative uploading describes a type of crowdsourcing scenario in\nnetworked environments where a device utilizes multiple paths over neighboring\ndevices to upload content to a centralized processing entity such as a cloud\nservice. Intermediate devices may aggregate and preprocess this data stream.\nSuch scenarios arise in the composition and aggregation of information, e.g.,\nfrom smartphones or sensors. We use a queuing theoretic description of the\ncollaborative uploading scenario, capturing the ability to split data into\nchunks that are then transmitted over multiple paths, and finally merged at the\ndestination. We analyze replication and allocation strategies that control the\nmapping of data to paths and provide closed-form expressions that pinpoint the\noptimal strategy given a description of the paths' service distributions.\nFinally, we provide an online path-aware adaptation of the allocation strategy\nthat uses statistical inference to sequentially minimize the expected waiting\ntime for the uploaded data. Numerical results show the effectiveness of the\nadaptive approach compared to the proportional allocation and a variant of the\njoin-the-shortest-queue allocation, especially for bursty path conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.04175v2"
    },
    {
        "title": "The L-CSC cluster: greenest supercomputer in the world in Green500 list\n  of November 2014",
        "authors": [
            "D. Rohr",
            "G. Neskovic",
            "M. Radtke",
            "V. Lindenstruth"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  The L-CSC (Lattice Computer for Scientific Computing) is a general purpose\ncompute cluster built of commodity hardware installed at GSI. Its main\noperational purpose is Lattice QCD (LQCD) calculations for physics simulations.\nQuantum Chromo Dynamics (QCD) is the physical theory describing the strong\nforce, one of the four known fundamental interactions in the universe. L-CSC\nleverages a multi-GPU design accommodating the huge demand of LQCD for memory\nbandwidth. In recent years, heterogeneous clusters with accelerators such as\nGPUs have become more and more powerful while supercomputers in general have\nshown enormous increases in power consumption making electricity costs and\ncooling a significant factor in the total cost of ownership. Using mainly GPUs\nfor processing, L-CSC is very power efficient, and its architecture was\noptimized to provide the greatest possible power efficiency. This paper\npresents the cluster design as well as optimizations to improve the power\nefficiency. It examines the power measurements performed for the Green500 list\nof the most power efficient supercomputers in the world which led to the number\n1 position as the greenest supercomputer in November 2014.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.09431v1"
    },
    {
        "title": "A Loop-Based Methodology for Reducing Computational Redundancy in\n  Workload Sets",
        "authors": [
            "Elie M. Shaccour",
            "Mohammad M. Mansour"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  The design of general purpose processors relies heavily on a workload\ngathering step in which representative programs are collected from various\napplication domains. Processor performance, when running the workload set, is\nprofiled using simulators that model the targeted processor architecture.\nHowever, simulating the entire workload set is prohibitively time-consuming,\nwhich precludes considering a large number of programs. To reduce simulation\ntime, several techniques in the literature have exploited the internal program\nrepetitiveness to extract and execute only representative code segments.\nExisting so- lutions are based on reducing cross-program computational\nredundancy or on eliminating internal-program redundancy to decrease execution\ntime. In this work, we propose an orthogonal and complementary loop- centric\nmethodology that targets loop-dominant programs by exploiting internal-program\ncharacteristics to reduce cross-program computational redundancy. The approach\nemploys a newly developed framework that extracts and analyzes core loops\nwithin workloads. The collected characteristics model memory behavior,\ncomputational complexity, and data structures of a program, and are used to\nconstruct a signature vector for each program. From these vectors,\ncross-workload similarity metrics are extracted, which are processed by a novel\nheuristic to exclude similar programs and reduce redundancy within the set.\nFinally, a reverse engineering approach that synthesizes executable\nmicro-benchmarks having the same instruction mix as the loops in the original\nworkload is introduced. A tool that automates the flow steps of the proposed\nmethodology is developed. Simulation results demonstrate that applying the\nproposed methodology to a set of workloads reduces the set size by half, while\npreserving the main characterizations of the initial workloads.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.00094v1"
    },
    {
        "title": "Asymptotic Miss Ratio of LRU Caching with Consistent Hashing",
        "authors": [
            "Kaiyi Ji",
            "Guocong Quan",
            "Jian Tan"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  To efficiently scale data caching infrastructure to support emerging big data\napplications, many caching systems rely on consistent hashing to group a large\nnumber of servers to form a cooperative cluster. These servers are organized\ntogether according to a random hash function. They jointly provide a unified\nbut distributed hash table to serve swift and voluminous data item requests.\nDifferent from the single least-recently-used (LRU) server that has already\nbeen extensively studied, theoretically characterizing a cluster that consists\nof multiple LRU servers remains yet to be explored. These servers are not\nsimply added together; the random hashing complicates the behavior. To this\nend, we derive the asymptotic miss ratio of data item requests on a LRU cluster\nwith consistent hashing. We show that these individual cache spaces on\ndifferent servers can be effectively viewed as if they could be pooled together\nto form a single virtual LRU cache space parametrized by an appropriate cache\nsize. This equivalence can be established rigorously under the condition that\nthe cache sizes of the individual servers are large. For typical data caching\nsystems this condition is common. Our theoretical framework provides a\nconvenient abstraction that can directly apply the results from the simpler\nsingle LRU cache to the more complex LRU cluster with consistent hashing.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02436v2"
    },
    {
        "title": "Optimal Content Replication and Request Matching in Large Caching\n  Systems",
        "authors": [
            "Arpan Mukhopadhyay",
            "Nidhi Hegde",
            "Marc Lelarge"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  We consider models of content delivery networks in which the servers are\nconstrained by two main resources: memory and bandwidth. In such systems, the\nthroughput crucially depends on how contents are replicated across servers and\nhow the requests of specific contents are matched to servers storing those\ncontents. In this paper, we first formulate the problem of computing the\noptimal replication policy which if combined with the optimal matching policy\nmaximizes the throughput of the caching system in the stationary regime. It is\nshown that computing the optimal replication policy for a given system is an\nNP-hard problem. A greedy replication scheme is proposed and it is shown that\nthe scheme provides a constant factor approximation guarantee. We then propose\na simple randomized matching scheme which avoids the problem of interruption in\nservice of the ongoing requests due to re-assignment or repacking of the\nexisting requests in the optimal matching policy. The dynamics of the caching\nsystem is analyzed under the combination of proposed replication and matching\nschemes. We study a limiting regime, where the number of servers and the\narrival rates of the contents are scaled proportionally, and show that the\nproposed policies achieve asymptotic optimality. Extensive simulation results\nare presented to evaluate the performance of different policies and study the\nbehavior of the caching system under different service time distributions of\nthe requests.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.02889v1"
    },
    {
        "title": "Effect of Meltdown and Spectre Patches on the Performance of HPC\n  Applications",
        "authors": [
            "Nikolay A. Simakov",
            "Martins D. Innus",
            "Matthew D. Jones",
            "Joseph P. White",
            "Steven M. Gallo",
            "Robert L. DeLeon",
            "Thomas R. Furlani"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In this work we examine how the updates addressing Meltdown and Spectre\nvulnerabilities impact the performance of HPC applications. To study this we\nuse the application kernel module of XDMoD to test the performance before and\nafter the application of the vulnerability patches. We tested the performance\ndifference for multiple application and benchmarks including: NWChem, NAMD,\nHPCC, IOR, MDTest and IMB. The results show that although some specific\nfunctions can have performance decreased by as much as 74%, the majority of\nindividual metrics indicates little to no decrease in performance. The\nreal-world applications show a 2-3% decrease in performance for single node\njobs and a 5-11% decrease for parallel multi node jobs.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.04329v2"
    },
    {
        "title": "BOPS, Not FLOPS! A New Metric and Roofline Performance Model For\n  Datacenter Computing",
        "authors": [
            "Lei Wang",
            "Jianfeng Zhan",
            "Wanling Gao",
            "KaiYong Yang",
            "ZiHan Jiang",
            "Rui Ren",
            "Xiwen He",
            "Chunjie Luo"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  For emerging datacenter (in short, DC) workloads, such as online Internet\nservices or offline data analytics, how to evaluate the upper bound performance\nand provide apple-to-apple comparisons are fundamental problems. To this end, a\nunified computation-centric metric is an essential requirement. As the most\nimportant computation-centric performance metric, FLOPS has guided computing\nsystems evolutions for many years. However, our observations demonstrate that\nthe average FLOPS efficiency of the DC workloads is only 0.1%, which implies\nthat FLOPS is inappropriate for DC computing. To address the above issue, we\npropose BOPS (Basic Operations Per Second), which is the average number of BOPs\n(Basic OPerations) completed per second. We conduct the analysis on the\ncharacteristics of seventeen typical DC workloads and extract the minimum\nrepresentative computation operations set, which is composed of integer and\nfloating point computation operations of arithmetic, comparing and array\naddressing. Then, we propose the formalized BOPS definition and the BOPS based\nupper bound performance model. Finally, the BOPS measuring tool is also\nimplemented. We perform experiments with seventeen DC workloads on three\ntypical Intel processors platforms. First, BOPS can reflect the performance gap\nof different computing systems, the bias between the peak BOPS performance gap\nand the average DC workloads' wall clock time gap is no more than 10%. Second,\nthe Sort workload can achieve 32% BOPS efficiency on the experimental platform.\nAt last, we present two use cases of BOPS. One is the BOPS based system\nevaluation, we illustrate that BOPS can compare performance of workloads from\nmultiple domains. The other is BOPS based optimizations. We show that under the\nguiding of the BOPS based upper bound model, the Sort workload and the Redis\nworkload achieve 4.4X and 1.2X performance improvements respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.09212v4"
    },
    {
        "title": "A Measurement Theory of Locality",
        "authors": [
            "Liang Yuan",
            "Chen Ding",
            "Peter Denning",
            "Yunquan Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Locality is a fundamental principle used extensively in program and system\noptimization. It can be measured in many ways. This paper formalizes the\nmetrics of locality into a measurement theory. The new theory includes the\nprecise definition of locality metrics based on access frequency, reuse time,\nreuse distance, working set, footprint, and the cache miss ratio. It gives the\nformal relation between these definitions and the proofs of equivalence or\nnon-equivalence. It provides the theoretical justification for four successful\nlocality models in operating systems, programming languages, and computer\narchitectures which were developed empirically.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.01254v3"
    },
    {
        "title": "Tuning Streamed Applications on Intel Xeon Phi: A Machine Learning Based\n  Approach",
        "authors": [
            "Peng Zhang",
            "Jianbin Fang",
            "Tao Tang",
            "Canqun Yang",
            "Zheng Wang"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Many-core accelerators, as represented by the XeonPhi coprocessors and\nGPGPUs, allow software to exploit spatial and temporal sharing of computing\nresources to improve the overall system performance. To unlock this performance\npotential requires software to effectively partition the hardware resource to\nmaximize the overlap between hostdevice communication and accelerator\ncomputation, and to match the granularity of task parallelism to the resource\npartition. However, determining the right resource partition and task\nparallelism on a per program, per dataset basis is challenging. This is because\nthe number of possible solutions is huge, and the benefit of choosing the right\nsolution may be large, but mistakes can seriously hurt the performance. In this\npaper, we present an automatic approach to determine the hardware resource\npartition and the task granularity for any given application, targeting the\nIntel XeonPhi architecture. Instead of hand-crafting the heuristic for which\nthe process will have to repeat for each hardware generation, we employ machine\nlearning techniques to automatically learn it. We achieve this by first\nlearning a predictive model offline using training programs; we then use the\nlearned model to predict the resource partition and task granularity for any\nunseen programs at runtime. We apply our approach to 23 representative parallel\napplications and evaluate it on a CPU-XeonPhi mixed heterogenous many-core\nplatform. Our approach achieves, on average, a 1.6x (upto 5.6x) speedup, which\ntranslates to 94.5% of the performance delivered by a theoretically perfect\npredictor.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.02760v1"
    },
    {
        "title": "On the Power-of-d-choices with Least Loaded Server Selection",
        "authors": [
            "Tim Hellemans",
            "Benny Van Houdt"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Motivated by distributed schedulers that combine the power-of-d-choices with\nlate binding and systems that use replication with cancellation-on-start, we\nstudy the performance of the LL(d) policy which assigns a job to a server that\ncurrently has the least workload among d randomly selected servers in\nlarge-scale homogeneous clusters. We consider general service time\ndistributions and propose a partial integro-differential equation to describe\nthe evolution of the system. This equation relies on the earlier proven ansatz\nfor LL(d) which asserts that the workload distribution of any finite set of\nqueues becomes independent of one another as the number of servers tends to\ninfinity. Based on this equation we propose a fixed point iteration for the\nlimiting workload distribution and study its convergence. For exponential job\nsizes we present a simple closed form expression for the limiting workload\ndistribution that is valid for any work-conserving service discipline as well\nas for the limiting response time distribution in case of\nfirst-come-first-served scheduling. We further show that for phase-type\ndistributed job sizes the limiting workload and response time distribution can\nbe expressed via the unique solution of a simple set of ordinary differential\nequations. Numerical and analytical results that compare response time of the\nclassic power-of-d-choices algorithm and the LL(d) policy are also presented\nand the accuracy of the limiting response time distribution for finite systems\nis illustrated using simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.05420v1"
    },
    {
        "title": "Less is More: Exploiting the Standard Compiler Optimization Levels for\n  Better Performance and Energy Consumption",
        "authors": [
            "Kyriakos Georgiou",
            "Craig Blackmore",
            "Samuel Xavier-de-Souza",
            "Kerstin Eder"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  This paper presents the interesting observation that by performing fewer of\nthe optimizations available in a standard compiler optimization level such as\n-O2, while preserving their original ordering, significant savings can be\nachieved in both execution time and energy consumption. This observation has\nbeen validated on two embedded processors, namely the ARM Cortex-M0 and the ARM\nCortex-M3, using two different versions of the LLVM compilation framework; v3.8\nand v5.0. Experimental evaluation with 71 embedded benchmarks demonstrated\nperformance gains for at least half of the benchmarks for both processors. An\naverage execution time reduction of 2.4% and 5.3% was achieved across all the\nbenchmarks for the Cortex-M0 and Cortex-M3 processors, respectively, with\nexecution time improvements ranging from 1% up to 90% over the -O2. The savings\nthat can be achieved are in the same range as what can be achieved by the\nstate-of-the-art compilation approaches that use iterative compilation or\nmachine learning to select flags or to determine phase orderings that result in\nmore efficient code. In contrast to these time consuming and expensive to apply\ntechniques, our approach only needs to test a limited number of optimization\nconfigurations, less than 64, to obtain similar or even better savings.\nFurthermore, our approach can support multi-criteria optimization as it targets\nexecution time, energy consumption and code size at the same time.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.09845v1"
    },
    {
        "title": "Dynamic Load Balancing with Tokens",
        "authors": [
            "Céline Comte"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Efficiently exploiting the resources of data centers is a complex task that\nrequires efficient and reliable load balancing and resource allocation\nalgorithms. The former are in charge of assigning jobs to servers upon their\narrival in the system, while the latter are responsible for sharing server\nresources between their assigned jobs. These algorithms should take account of\nvarious constraints, such as data locality, that restrict the feasible job\nassignments. In this paper, we propose a token-based mechanism that efficiently\nbalances load between servers without requiring any knowledge on job arrival\nrates and server capacities. Assuming a balanced fair sharing of the server\nresources, we show that the resulting dynamic load balancing is insensitive to\nthe job size distribution. Its performance is compared to that obtained under\nthe best static load balancing and in an ideal system that would constantly\noptimize the resource utilization.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01783v2"
    },
    {
        "title": "A Survey of Miss-Ratio Curve Construction Techniques",
        "authors": [
            "Daniel Byrne"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Miss-ratio curve (MRC), or equivalently hit-ratio curve (HRC), construction\ntechniques have recently gathered the attention of many researchers. Recent\nadvancements have allowed for approximating these curves in constant time,\nallowing for online working-set-size (WSS) measurement. Techniques span the\nalgorithmic design paradigm from classic dynamic programming to artificial\nintelligence inspired techniques. Our survey produces broad classification of\nthe current techniques primarily based on \\emph{what} locality metric is being\nrecorded and \\emph{how} that metric is stored for processing.\n  Applications of theses curves span from dynamic cache partitioning in the\nprocessor, to improving block allocation at the operating system level. Our\nsurvey will give an overview of the historical, exact MRC construction methods,\nand compare them with the state-of-the-art methods present in today's\nliterature. In addition, we will show where there are still open areas of\nresearch and remain excited to see what this domain can produce with a strong\ntheoretical background.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.01972v1"
    },
    {
        "title": "Some parametrized dynamic priority policies for 2-class M/G/1 queues:\n  completeness and applications",
        "authors": [
            "Manu K. Gupta",
            "N. Hemachandra",
            "J. Venkateswaran"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Completeness of a dynamic priority scheduling scheme is of fundamental\nimportance for the optimal control of queues in areas as diverse as computer\ncommunications, communication networks, supply chains and manufacturing\nsystems. Our first main contribution is to identify the mean waiting time\ncompleteness as a unifying aspect for four different dynamic priority\nscheduling schemes by proving their completeness and equivalence in 2-class\nM/G/1 queue. These dynamic priority schemes are earliest due date based, head\nof line priority jump, relative priority, and probabilistic priority.\n  In our second main contribution, we characterize the optimal scheduling\npolicies for the case studies in different domains by exploiting the\ncompleteness of above dynamic priority schemes. The major theme of second main\ncontribution is resource allocation/optimal control in revenue management\nproblems for contemporary systems such as cloud computing, high-performance\ncomputing, etc., where congestion is inherent. Using completeness and\ntheoretically tractable nature of relative priority policy, we study the impact\nof approximation in a fairly generic data network utility framework. We\nintroduce the notion of min-max fairness in multi-class queues and show that a\nsimple global FCFS policy is min-max fair. Next, we re-derive the celebrated\n$c/\\rho$ rule for 2-class M/G/1 queues by an elegant argument and also simplify\na complex joint pricing and scheduling problem for a wider class of scheduling\npolicies.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03564v1"
    },
    {
        "title": "Pliant: Leveraging Approximation to Improve Datacenter Resource\n  Efficiency",
        "authors": [
            "Neeraj Kulkarni",
            "Feng Qi",
            "Christina Delimitrou"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Cloud multi-tenancy is typically constrained to a single interactive service\ncolocated with one or more batch, low-priority services, whose performance can\nbe sacrificed when deemed necessary. Approximate computing applications offer\nthe opportunity to enable tighter colocation among multiple applications whose\nperformance is important. We present Pliant, a lightweight cloud runtime that\nleverages the ability of approximate computing applications to tolerate some\nloss in their output quality to boost the utilization of shared servers. During\nperiods of high resource contention, Pliant employs incremental and\ninterference-aware approximation to reduce contention in shared resources, and\nprevent QoS violations for co-scheduled interactive, latency-critical services.\nWe evaluate Pliant across different interactive and approximate computing\napplications, and show that it preserves QoS for all co-scheduled workloads,\nwhile incurring a 2.1\\% loss in output quality, on average.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.05671v1"
    },
    {
        "title": "Intermediate Data Caching Optimization for Multi-Stage and Parallel Big\n  Data Frameworks",
        "authors": [
            "Zhengyu Yang",
            "Danlin Jia",
            "Stratis Ioannidis",
            "Ningfang Mi",
            "Bo Sheng"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In the era of big data and cloud computing, large amounts of data are\ngenerated from user applications and need to be processed in the datacenter.\nData-parallel computing frameworks, such as Apache Spark, are widely used to\nperform such data processing at scale. Specifically, Spark leverages\ndistributed memory to cache the intermediate results, represented as Resilient\nDistributed Datasets (RDDs). This gives Spark an advantage over other parallel\nframeworks for implementations of iterative machine learning and data mining\nalgorithms, by avoiding repeated computation or hard disk accesses to retrieve\nRDDs. By default, caching decisions are left at the programmer's discretion,\nand the LRU policy is used for evicting RDDs when the cache is full. However,\nwhen the objective is to minimize total work, LRU is woefully inadequate,\nleading to arbitrarily suboptimal caching decisions. In this paper, we design\nan algorithm for multi-stage big data processing platforms to adaptively\ndetermine and cache the most valuable intermediate datasets that can be reused\nin the future. Our solution automates the decision of which RDDs to cache: this\namounts to identifying nodes in a direct acyclic graph (DAG) representing\ncomputations whose outputs should persist in the memory. Our experiment results\nshow that our proposed cache optimization solution can improve the performance\nof machine learning applications on Spark decreasing the total work to\nrecompute RDDs by 12%.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10563v2"
    },
    {
        "title": "Queuing Theoretic Models for Multicast and Coded-Caching in Downlink\n  Wireless Systems",
        "authors": [
            "Mahadesh Panju",
            "Ramkumar Raghu",
            "Vinod Sharma",
            "Rajesh Ramachandran"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  We consider a server connected to $L$ users over a shared finite capacity\nlink. Each user is equipped with a cache. File requests at the users are\ngenerated as independent Poisson processes according to a popularity profile\nfrom a library of $M$ files. The server has access to all the files in the\nlibrary. Users can store parts of the files or full files from the library in\ntheir local caches. The server should send missing parts of the files requested\nby the users. The server attempts to fulfill the pending requests with minimal\ntransmissions exploiting multicasting and coding opportunities among the\npending requests. We study the performance of this system in terms of queuing\ndelays for the naive multicasting and several coded multicasting schemes\nproposed in the literature. We also provide approximate expressions for the\nmean queuing delay for these models and establish their effectiveness with\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10590v1"
    },
    {
        "title": "A Basic Result on the Superposition of Arrival Processes in\n  Deterministic Networks",
        "authors": [
            "Yuming Jiang"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Time-Sensitive Networking (TSN) and Deterministic Networking (DetNet) are\nemerging standards to enable deterministic, delay-critical communication in\nsuch networks. This naturally (re-)calls attention to the network calculus\ntheory (NC), since a rich set of results for delay guarantee analysis have\nalready been developed there. One could anticipate an immediate adoption of\nthose existing network calculus results to TSN and DetNet. However, the\nfundamental difference between the traffic specification adopted in TSN and\nDetNet and those traffic models in NC makes this difficult, let alone that\nthere is a long-standing open challenge in NC. To address them, this paper\nconsiders an arrival time function based max-plus NC traffic model. In\nparticular, for the former, the mapping between the TSN / DetNet and the NC\ntraffic model is proved. For the latter, the superposition property of the\narrival time function based NC traffic model is found and proved. Appealingly,\nthe proved superposition property shows a clear analogy with that of a\nwell-known counterpart traffic model in NC. These results help make an\nimportant step towards the development of a system theory for delay guarantee\nanalysis of TSN / DetNet networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.10973v2"
    },
    {
        "title": "AutoTiering: Automatic Data Placement Manager in Multi-Tier All-Flash\n  Datacenter",
        "authors": [
            "Zhengyu Yang",
            "Morteza Hoseinzadeh",
            "Allen Andrews",
            "Clay Mayers",
            "David Evans",
            "Rory Bolt",
            "Janki Bhimani",
            "Ningfang Mi",
            "Steven Swanson"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In the year of 2017, the capital expenditure of Flash-based Solid State\nDrivers (SSDs) keeps declining and the storage capacity of SSDs keeps\nincreasing. As a result, the \"selling point\" of traditional spinning Hard Disk\nDrives (HDDs) as a backend storage - low cost and large capacity - is no longer\nunique, and eventually they will be replaced by low-end SSDs which have large\ncapacity but perform orders of magnitude better than HDDs. Thus, it is widely\nbelieved that all-flash multi-tier storage systems will be adopted in the\nenterprise datacenters in the near future. However, existing caching or tiering\nsolutions for SSD-HDD hybrid storage systems are not suitable for all-flash\nstorage systems. This is because that all-flash storage systems do not have a\nlarge speed difference (e.g., 10x) among each tier. Instead, different\nspecialties (such as high performance, high capacity, etc.) of each tier should\nbe taken into consideration. Motivated by this, we develop an automatic data\nplacement manager called \"AutoTiering\" to handle virtual machine disk files\n(VMDK) allocation and migration in an all-flash multi-tier datacenter to best\nutilize the storage resource, optimize the performance, and reduce the\nmigration overhead. AutoTiering is based on an optimization framework, whose\ncore technique is to predict VM's performance change on different tiers with\ndifferent specialties without conducting real migration. As far as we know,\nAutoTiering is the first optimization solution designed for all-flash\nmulti-tier datacenters. We implement AutoTiering on VMware ESXi, and\nexperimental results show that it can significantly improve the I/O performance\ncompared to existing solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.01988v1"
    },
    {
        "title": "Enabling Cross-Event Optimization in Discrete-Event Simulation Through\n  Compile-Time Event Batching",
        "authors": [
            "Marc Leinweber",
            "Hannes Hartenstein",
            "Philipp Andelfinger"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  A discrete-event simulation (DES) involves the execution of a sequence of\nevent handlers dynamically scheduled at runtime. As a consequence, a priori\nknowledge of the control flow of the overall simulation program is limited. In\nparticular, powerful optimizations supported by modern compilers can only be\napplied on the scope of individual event handlers, which frequently involve\nonly a few lines of code. We propose a method that extends the scope for\ncompiler optimizations in discrete-event simulations by generating batches of\nmultiple events that are subjected to compiler optimizations as contiguous\nprocedures. A runtime mechanism executes suitable batches at negligible\noverhead. Our method does not require any compiler extensions and introduces\nonly minor additional effort during model development. The feasibility and\npotential performance gains of the approach are illustrated on the example of\nan idealized proof-ofconcept model. We believe that the applicability of the\napproach extends to general event-driven programs.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04303v1"
    },
    {
        "title": "SRPT for Multiserver Systems",
        "authors": [
            "Isaac Grosof",
            "Ziv Scully",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  The Shortest Remaining Processing Time (SRPT) scheduling policy and its\nvariants have been extensively studied in both theoretical and practical\nsettings. While beautiful results are known for single-server SRPT, much less\nis known for multiserver SRPT. In particular, stochastic analysis of the M/G/k\nunder multiserver SRPT is entirely open. Intuition suggests that multiserver\nSRPT should be optimal or near-optimal for minimizing mean response time.\nHowever, the only known analysis of multiserver SRPT is in the worst-case\nadversarial setting, where SRPT can be far from optimal. In this paper, we give\nthe first stochastic analysis bounding mean response time of the M/G/k under\nmultiserver SRPT. Using our response time bound, we show that multiserver SRPT\nhas asymptotically optimal mean response time in the heavy-traffic limit. The\nkey to our bounds is a strategic combination of stochastic and worst-case\ntechniques. Beyond SRPT, we prove similar response time bounds and optimality\nresults for several other multiserver scheduling policies.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.07686v1"
    },
    {
        "title": "DRESS: Dynamic RESource-reservation Scheme for Congested Data-intensive\n  Computing Platforms",
        "authors": [
            "Ying Mao",
            "Victoria Green",
            "Jiayin Wang",
            "Haoyi Xiong",
            "Zhishan Guo"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In the past few years, we have envisioned an increasing number of businesses\nstart driving by big data analytics, such as Amazon recommendations and Google\nAdvertisements. At the back-end side, the businesses are powered by big data\nprocessing platforms to quickly extract information and make decisions. Running\non top of a computing cluster, those platforms utilize scheduling algorithms to\nallocate resources. An efficient scheduler is crucial to the system performance\ndue to limited resources, e.g. CPU and Memory, and a large number of user\ndemands. However, besides requests from clients and current status of the\nsystem, it has limited knowledge about execution length of the running jobs,\nand incoming jobs' resource demands, which make assigning resources a\nchallenging task. If most of the resources are occupied by a long-running job,\nother jobs will have to keep waiting until it releases them. This paper\npresents a new scheduling strategy, named DRESS that particularly aims to\noptimize the allocation among jobs with various demands. Specifically, it\nclassifies the jobs into two categories based on their requests, reserves a\nportion of resources for each of category, and dynamically adjusts the reserved\nratio by monitoring the pending requests and estimating release patterns of\nrunning jobs. The results demonstrate DRESS significantly reduces the\ncompletion time for one category, up to 76.1% in our experiments, and in the\nmeanwhile, maintains a stable overall system performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.08359v2"
    },
    {
        "title": "Heterogeneous MacroTasking (HeMT) for Parallel Processing in the Public\n  Cloud",
        "authors": [
            "Yuquan Shan",
            "George Kesidis",
            "Bhuvan Urgaonkar",
            "Jorg Schad",
            "Jalal Khamse-Ashari",
            "Ioannis Lambadaris"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Using tiny, equal-sized tasks (Homogeneous microTasking, HomT) has long been\nregarded an effective way of load balancing in parallel computing systems. When\ncombined with nodes pulling in work upon becoming idle, HomT has the desirable\nproperty of automatically adapting its load distribution to the processing\ncapacities of participating nodes - more powerful nodes finish their work\nsooner and, therefore, pull in additional work faster. As a result, HomT is\ndeemed especially desirable in settings with heterogeneous (and possibly\npossessing dynamically changing) processing capacities. However, HomT does have\nadditional scheduling and I/O overheads that might make this load balancing\nscheme costly in some scenarios. In this paper, we first analyze these\nadvantages and disadvantages of HomT. We then propose an alternative load\nbalancing scheme - Heterogeneous MacroTasking (HeMT) - wherein workload is\nintentionally partitioned according to nodes' processing capacity. Our goal is\nto study when HeMT is able to overcome the performance disadvantages of HomT.\nWe implement a prototype of HeMT within the Apache Spark application framework\nwith complementary enhancements to the Apache Mesos cluster manager. Spark's\nbuilt-in scheduler, when parameterized appropriately, implements HomT. Our\nexperimental results show that HeMT out-performs HomT when accurate\nworkload-specific estimates of nodes' processing capacities are learned. As\nrepresentative results, Spark with HeMT offers about 10% better average\ncompletion times for realistic data processing workloads over the default\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.00988v1"
    },
    {
        "title": "Performance analysis and optimization of the JOREK code for many-core\n  CPUs",
        "authors": [
            "T. B. Fehér",
            "M. Hölzl",
            "G. Latu",
            "G. T. A. Huijsmans"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  This report investigates the performance of the JOREK code on the Intel\nKnights Landing and Skylake processor architectures. The OpenMP scaling of the\nmatrix construction part of the code was analyzed and improved synchronization\nmethods were implemented. A new switch was implemented to control the number of\nthreads used for the linear equation solver independently from other parts of\nthe code. The matrix construction subroutine was vectorized, and the data\nlocality was also improved. These steps led to a factor of two speedup for the\nmatrix construction.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04413v1"
    },
    {
        "title": "Load balancing with heterogeneous schedulers",
        "authors": [
            "Urtzi Ayesta",
            "Manu K Gupta",
            "Ina Maria Verloop"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Load balancing is a common approach in web server farms or inventory routing\nproblems. An important issue in such systems is to determine the server to\nwhich an incoming request should be routed to optimize a given performance\ncriteria. In this paper, we assume the server's scheduling disciplines to be\nheterogeneous. More precisely, a server implements a scheduling discipline\nwhich belongs to the class of limited processor sharing (LPS-$d$) scheduling\ndisciplines. Under LPS-$d$, up to $d$ jobs can be served simultaneously, and\nhence, includes as special cases First Come First Served ($d=1$) and Processor\nSharing ($d=\\infty$).\n  In order to obtain efficient heuristics, we model the above load-balancing\nframework as a multi-armed restless bandit problem. Using the relaxation\ntechnique, as first developed in the seminal work of Whittle, we derive\nWhittle's index policy for general cost functions and obtain a closed-form\nexpression for Whittle's index in terms of the steady-state distribution.\nThrough numerical computations, we investigate the performance of Whittle's\nindex with two different performance criteria: linear cost criterion and a cost\ncriterion that depends on the first and second moment of the throughput. Our\nresults show that \\emph{(i)} the structure of Whittle's index policy can\nstrongly depend on the scheduling discipline implemented in the server, i.e.,\non $d$, and that \\emph{(ii)} Whittle's index policy significantly outperforms\nstandard dispatching rules such as Join the Shortest Queue (JSQ), Join the\nShortest Expected Workload (JSEW), and Random Server allocation (RSA).\n",
        "pdf_link": "http://arxiv.org/pdf/1810.07782v1"
    },
    {
        "title": "Improving OpenCL Performance by Specializing Compiler Phase Selection\n  and Ordering",
        "authors": [
            "Ricardo Nobre",
            "Luís Reis",
            "João M. P. Cardoso"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Automatic compiler phase selection/ordering has traditionally been focused on\nCPUs and, to a lesser extent, FPGAs. We present experiments regarding compiler\nphase ordering specialization of OpenCL kernels targeting a GPU. We use\niterative exploration to specialize LLVM phase orders on 15 OpenCL benchmarks\nto an NVIDIA GPU. We analyze the generated NVIDIA PTX code for the various\nversions to identify the main causes of the most significant improvements and\npresent results of a set of experiments that demonstrate the importance of\nusing specific phase orders. Using specialized compiler phase orders, we were\nable to achieve geometric mean improvements of 1.54x (up to 5.48x) and 1.65x\n(up to 5.7x) over PTX generated by the NVIDIA CUDA compiler from CUDA versions\nof the same kernels, and over execution of the OpenCL kernels compiled from\nsource with the NVIDIA OpenCL driver, respectively. We also evaluate the use of\ncode-features in the OpenCL kernels. More specifically, we evaluate an approach\nthat achieves geometric mean improvements of 1.49x and 1.56x over the same\nOpenCL baseline, by using the compiler sequences of the 1 or 3 most similar\nbenchmarks, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.10496v1"
    },
    {
        "title": "Hoard: A Distributed Data Caching System to Accelerate Deep Learning\n  Training on the Cloud",
        "authors": [
            "Christian Pinto",
            "Yiannis Gkoufas",
            "Andrea Reale",
            "Seetharami Seelam",
            "Steven Eliuk"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Deep Learning system architects strive to design a balanced system where the\ncomputational accelerator -- FPGA, GPU, etc, is not starved for data. Feeding\ntraining data fast enough to effectively keep the accelerator utilization high\nis difficult when utilizing dedicated hardware like GPUs. As accelerators are\ngetting faster, the storage media \\& data buses feeding the data have not kept\npace and the ever increasing size of training data further compounds the\nproblem. We describe the design and implementation of a distributed caching\nsystem called Hoard that stripes the data across fast local disks of multiple\nGPU nodes using a distributed file system that efficiently feeds the data to\nensure minimal degradation in GPU utilization due to I/O starvation. Hoard can\ncache the data from a central storage system before the start of the job or\nduring the initial execution of the job and feeds the cached data for\nsubsequent epochs of the same job and for different invocations of the jobs\nthat share the same data requirements, e.g. hyper-parameter tuning. Hoard\nexposes a POSIX file system interface so the existing deep learning frameworks\ncan take advantage of the cache without any modifications. We show that Hoard,\nusing two NVMe disks per node and a distributed file system for caching,\nachieves a 2.1x speed-up over a 10Gb/s NFS central storage system on a 16 GPU\n(4 nodes, 4 GPUs per node) cluster for a challenging AlexNet ImageNet image\nclassification benchmark with 150GB of input dataset. As a result of the\ncaching, Hoard eliminates the I/O bottlenecks introduced by the shared storage\nand increases the utilization of the system by 2x compared to using the shared\nstorage without the cache.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.00669v1"
    },
    {
        "title": "Speed Based Optimal Power Control in Small Cell Networks",
        "authors": [
            "Veeraruna Kavitha",
            "Manu K. Gupta",
            "Veronique Capdevielle",
            "Rahul Kishor",
            "Majed Haddad"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Small cell networks promise good quality of service (QoS) even for cell edge\nusers, however pose challenges to cater to the high-speed users. The major\ndifficulty being that of frequent handovers and the corresponding handover\nlosses, which significantly depend upon the speed of the user. It was shown\npreviously that the optimal cell size increases with speed. Thus, in scenarios\nwith diverse users (speeds spanning over large ranges), it would be inefficient\nto serve all users using common cell radius and it is practically infeasible to\ndesign different cell sizes for different speeds. Alternatively, we propose to\nallocate power to a user based on its speed, e.g., higher power virtually\nincreases the cell size. We solve well known Hamiltonian Jacobi equations under\ncertain assumptions to obtain a power law, optimal for load factor and busy\nprobability, for any given average power constraint and cell size. The optimal\npower control turns out to be linear in speed. We build a system level\nsimulator for small cell network, using elaborate Monte-Carlo simulations, and\nshow that the performance of the system improves significantly with linear\npower law. The power law is tested even for the cases, for which the system\ndoes not satisfy the assumptions required by the theory. For example, the\nlinear power law has significant improvement in comparison with the 'equal\npower' system, even in presence of time varying and random interference. We\nobserve good improvement in almost all cases with improvements up to 89\\% for\ncertain configurations.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.03862v1"
    },
    {
        "title": "AdaptMemBench: Application-Specific MemorySubsystem Benchmarking",
        "authors": [
            "Mahesh Lakshminarasimhan",
            "Catherine Olschanowsky"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Optimizing scientific applications to take full advan-tage of modern memory\nsubsystems is a continual challenge forapplication and compiler developers.\nFactors beyond working setsize affect performance. A benchmark framework that\nexploresthe performance in an application-specific manner is essential\ntocharacterize memory performance and at the same time informmemory-efficient\ncoding practices. We present AdaptMemBench,a configurable benchmark framework\nthat measures achievedmemory performance by emulating application-specific\naccesspatterns with a set of kernel-independent driver templates. Thisframework\ncan explore the performance characteristics of a widerange of access patterns\nand can be used as a testbed for potentialoptimizations due to the flexibility\nof polyhedral code generation.We demonstrate the effectiveness of AdaptMemBench\nwith casestudies on commonly used computational kernels such as triadand\nmultidimensional stencil patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07778v1"
    },
    {
        "title": "LBICA: A Load Balancer for I/O Cache Architectures",
        "authors": [
            "Saba Ahmadian",
            "Reza Salkhordeh",
            "Hossein Asadi"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In recent years, enterprise Solid-State Drives (SSDs) are used in the caching\nlayer of high-performance servers to close the growing performance gap between\nprocessing units and storage subsystem. SSD-based I/O caching is typically not\neffective in workloads with burst accesses in which the caching layer itself\nbecomes the performance bottleneck because of the large number of accesses.\nExisting I/O cache architectures mainly focus on maximizing the cache hit ratio\nwhile they neglect the average queue time of accesses. Previous studies\nsuggested bypassing the cache when burst accesses are identified. These\nschemes, however, are not applicable to a general cache configuration and also\nresult in significant performance degradation on burst accesses. In this paper,\nwe propose a novel I/O cache load balancing scheme (LBICA) with adaptive write\npolicy management to prevent the I/O cache from becoming performance bottleneck\nin burst accesses. Our proposal, unlike previous schemes, which disable the I/O\ncache or bypass the requests into the disk subsystem in burst accesses,\nselectively reduces the number of waiting accesses in the SSD queue and\nbalances the load between the I/O cache and the disk subsystem while providing\nthe maximum performance. The proposed scheme characterizes the workload based\non the type of in-queue requests and assigns an effective cache write policy.\nWe aim to bypass the accesses which 1) are served faster by the disk subsystem\nor 2) cannot be merged with other accesses in the I/O cache queue. Doing so,\nthe selected requests are responded by the disk layer, preventing from\noverloading the I/O cache. Our evaluations on a physical system shows that\nLBICA reduces the load on the I/O cache by 48% and improves the performance of\nburst workloads by 30% compared to the latest state-of-the-art load balancing\nscheme.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.08720v1"
    },
    {
        "title": "Reliable Access to Massive Restricted Texts: Experience-based Evaluation",
        "authors": [
            "Zong Peng",
            "Beth Plale"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Libraries are seeing growing numbers of digitized textual corpora that\nfrequently come with restrictions on their content. Computational analysis\ncorpora that are large, while of interest to scholars, can be cumbersome\nbecause of the combination of size, granularity of access, and access\nrestrictions. Efficient management of such a collection for general access\nespecially under failures depends on the primary storage system. In this paper,\nwe identify the requirements of managing for computational analysis a massive\ntext corpus and use it as basis to evaluate candidate storage solutions. The\nstudy based on the 5.9 billion page collection of the HathiTrust digital\nlibrary. Our findings led to the choice of Cassandra 3.x for the primary back\nend store, which is currently in deployment in the HathiTrust Research Center.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.00771v1"
    },
    {
        "title": "heSRPT: Optimal Parallel Scheduling of Jobs With Known Sizes",
        "authors": [
            "Benjamin Berg",
            "Rein Vesilo",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  When parallelizing a set of jobs across many servers, one must balance a\ntrade-off between granting priority to short jobs and maintaining the overall\nefficiency of the system. When the goal is to minimize the mean flow time of a\nset of jobs, it is usually the case that one wants to complete short jobs\nbefore long jobs. However, since jobs usually cannot be parallelized with\nperfect efficiency, granting strict priority to the short jobs can result in\nvery low system efficiency which in turn hurts the mean flow time across jobs.\nIn this paper, we derive the optimal policy for allocating servers to jobs at\nevery moment in time in order to minimize mean flow time across jobs. We assume\nthat jobs follow a sublinear, concave speedup function, and hence jobs\nexperience diminishing returns from being allocated additional servers. We show\nthat the optimal policy, heSRPT, will complete jobs according to their size\norder, but maintains overall system efficiency by allocating some servers to\neach job at every moment in time. We compare heSRPT with state-of-the-art\nallocation policies from the literature and show that heSRPT outperforms its\ncompetitors by at least 30%, and often by much more.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.09346v2"
    },
    {
        "title": "Effect of payload size on goodput when message segmentations occur for\n  wireless networks: Case of packet corruptions recovered by stop-and-wait\n  protocol",
        "authors": [
            "Takashi Ikegawa"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This paper investigates the effect of payload size on goodput for wireless\nnetworks where packets created from a message through a segmentation function\nare lost due to bit errors and they are recovered by a stop-and-wait protocol.\nTo achieve this, we derive the exact analytical form of goodput using the\nanalytical form of a packet-size distribution, given a message-size\ndistribution and a payload size. In previous work, the packet sizes are assumed\nto be constant, which are payload size plus header size, although actual\nsegmented packets are not constant in size. Hence, this constant packet-size\nassumption may be not justified for goodput analysis. From numerical results,\nwe show that the constant packet-size assumption is not justified under low\nbit-error rates. Furthermore, we indicate that the curves of goodput are\nconcave in payload size under high bit-error rates. In addition, we show that\nthe larger mean bit-error burst length yields less concave curves of goodput.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.10171v1"
    },
    {
        "title": "Algorithms of evaluation of the waiting time and the modelling of the\n  terminal activity",
        "authors": [
            "Gh. Miscoi",
            "A. Costea",
            "R. I. Ţicu",
            "C. Pomazan"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This paper approaches the application of the waiting model with Poisson\ninputs and priorities in the port activity. The arrival of ships in the\nmaritime terminal is numerically modelled, and specific parameters for the\ndistribution functions of service and of inputs are determined, in order to\nestablish the waiting time of ships in the seaport and a stationary process.\nThe modelling is based on waiting times and on the traffic coefficient.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.11588v1"
    },
    {
        "title": "Performance Analysis of Priority-Aware NoCs with Deflection Routing\n  under Traffic Congestion",
        "authors": [
            "Sumit K. Mandal",
            "Anish Krishnakumar",
            "Raid Ayoub",
            "Michael Kishinevsky",
            "Umit Y. Ogras"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Priority-aware networks-on-chip (NoCs) are used in industry to achieve\npredictable latency under different workload conditions. These NoCs incorporate\ndeflection routing to minimize queuing resources within routers and achieve low\nlatency during low traffic load. However, deflected packets can exacerbate\ncongestion during high traffic load since they consume the NoC bandwidth.\nState-of-the-art analytical models for priority-aware NoCs ignore deflected\ntraffic despite its significant latency impact during congestion. This paper\nproposes a novel analytical approach to estimate end-to-end latency of\npriority-aware NoCs with deflection routing under bursty and heavy traffic\nscenarios. Experimental evaluations show that the proposed technique\noutperforms alternative approaches and estimates the average latency for real\napplications with less than 8% error compared to cycle-accurate simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.03904v3"
    },
    {
        "title": "Proceedings of the 1st OMNeT++ Community Summit, Hamburg, Germany,\n  September 2, 2014",
        "authors": [
            "Anna Förster",
            "Christoph Sommer",
            "Till Steinbach",
            "Matthias Wählisch"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  This is the Proceedings of the 1st OMNeT++ Community Summit, which was held\nin Hamburg, Germany, September 2, 2014.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.0093v2"
    },
    {
        "title": "Characterizing and Subsetting Big Data Workloads",
        "authors": [
            "Zhen Jia",
            "Jianfeng Zhan",
            "Lei Wang",
            "Rui Han",
            "Sally A. McKee",
            "Qiang Yang",
            "Chunjie Luo",
            "Jingwei Li"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Big data benchmark suites must include a diversity of data and workloads to\nbe useful in fairly evaluating big data systems and architectures. However,\nusing truly comprehensive benchmarks poses great challenges for the\narchitecture community. First, we need to thoroughly understand the behaviors\nof a variety of workloads. Second, our usual simulation-based research methods\nbecome prohibitively expensive for big data. As big data is an emerging field,\nmore and more software stacks are being proposed to facilitate the development\nof big data applications, which aggravates hese challenges. In this paper, we\nfirst use Principle Component Analysis (PCA) to identify the most important\ncharacteristics from 45 metrics to characterize big data workloads from\nBigDataBench, a comprehensive big data benchmark suite. Second, we apply a\nclustering technique to the principle components obtained from the PCA to\ninvestigate the similarity among big data workloads, and we verify the\nimportance of including different software stacks for big data benchmarking.\nThird, we select seven representative big data workloads by removing redundant\nones and release the BigDataBench simulation version, which is publicly\navailable from http://prof.ict.ac.cn/BigDataBench/simulatorversion/.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.0792v1"
    },
    {
        "title": "Performance analysis of a 240 thread tournament level MCTS Go program on\n  the Intel Xeon Phi",
        "authors": [
            "S. Ali Mirsoleimani",
            "Aske Plaat",
            "Jos Vermaseren",
            "Jaap van den Herik"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  In 2013 Intel introduced the Xeon Phi, a new parallel co-processor board. The\nXeon Phi is a cache-coherent many-core shared memory architecture claiming\nCPU-like versatility, programmability, high performance, and power efficiency.\nThe first published micro-benchmark studies indicate that many of Intel's\nclaims appear to be true. The current paper is the first study on the Phi of a\ncomplex artificial intelligence application. It contains an open source MCTS\napplication for playing tournament quality Go (an oriental board game). We\nreport the first speedup figures for up to 240 parallel threads on a real\nmachine, allowing a direct comparison to previous simulation studies. After a\nsubstantial amount of work, we observed that performance scales well up to 32\nthreads, largely confirming previous simulation results of this Go program,\nalthough the performance surprisingly deteriorates between 32 and 240 threads.\nFurthermore, we report (1) unexpected performance anomalies between the Xeon\nPhi and Xeon CPU for small problem sizes and small numbers of threads, and (2)\nthat performance is sensitive to scheduling choices. Achieving good performance\non the Xeon Phi for complex programs is not straightforward; it requires a deep\nunderstanding of (1) search patterns, (2) of scheduling, and (3) of the\narchitecture and its many cores and caches. In practice, the Xeon Phi is less\nstraightforward to program for than originally envisioned by Intel.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.4297v2"
    },
    {
        "title": "Cache-aware Performance Modeling and Prediction for Dense Linear Algebra",
        "authors": [
            "Elmar Peise",
            "Paolo Bientinesi"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Countless applications cast their computational core in terms of dense linear\nalgebra operations. These operations can usually be implemented by combining\nthe routines offered by standard linear algebra libraries such as BLAS and\nLAPACK, and typically each operation can be obtained in many alternative ways.\nInterestingly, identifying the fastest implementation -- without executing it\n-- is a challenging task even for experts. An equally challenging task is that\nof tuning each routine to performance-optimal configurations. Indeed, the\nproblem is so difficult that even the default values provided by the libraries\nare often considerably suboptimal; as a solution, normally one has to resort to\nexecuting and timing the routines, driven by some form of parameter search. In\nthis paper, we discuss a methodology to solve both problems: identifying the\nbest performing algorithm within a family of alternatives, and tuning\nalgorithmic parameters for maximum performance; in both cases, we do not\nexecute the algorithms themselves. Instead, our methodology relies on timing\nand modeling the computational kernels underlying the algorithms, and on a\ntechnique for tracking the contents of the CPU cache. In general, our\nperformance predictions allow us to tune dense linear algebra algorithms within\nfew percents from the best attainable results, thus allowing computational\nscientists and code developers alike to efficiently optimize their linear\nalgebra routines and codes.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.8602v1"
    },
    {
        "title": "Pushing the Limits of Online Auto-tuning: Machine Code Optimization in\n  Short-Running Kernels",
        "authors": [
            "Fernando Endo",
            "Damien Couroussé",
            "Henri-Pierre Charles"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We propose an online auto-tuning approach for computing kernels. Differently\nfrom existing online auto-tuners, which regenerate code with long compilation\nchains from the source to the binary code, our approach consists on deploying\nauto-tuning directly at the level of machine code generation. This allows\nauto-tuning to pay off in very short-running applications. As a proof of\nconcept, our approach is demonstrated in two benchmarks, which execute during\nhundreds of milliseconds to a few seconds only. In a CPU-bound kernel, the\naverage speedups achieved are 1.10 to 1.58 depending on the target\nmicro-architecture, up to 2.53 in the most favourable conditions (all run-time\noverheads included). In a memory-bound kernel, less favourable to our runtime\nauto-tuning optimizations, the average speedups are 1.04 to 1.10, up to 1.30 in\nthe best configuration. Despite the short execution times of our benchmarks,\nthe overhead of our runtime auto-tuning is between 0.2 and 4.2% only of the\ntotal application execution times. By simulating the CPU-bound application in\n11 different CPUs, we showed that, despite the clear hardware disadvantage of\nIn-Order (io) cores vs. Out-of-Order (ooo) equivalent cores, online auto-tuning\nin io CPUs obtained an average speedup of 1.03 and an energy efficiency\nimprovement of 39~\\% over the SIMD reference in ooo CPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.04566v1"
    },
    {
        "title": "Towards Optimality in Parallel Scheduling",
        "authors": [
            "Benjamin Berg",
            "Jan-Pieter Dorsman",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  To keep pace with Moore's law, chip designers have focused on increasing the\nnumber of cores per chip rather than single core performance. In turn, modern\njobs are often designed to run on any number of cores. However, to effectively\nleverage these multi-core chips, one must address the question of how many\ncores to assign to each job. Given that jobs receive sublinear speedups from\nadditional cores, there is an obvious tradeoff: allocating more cores to an\nindividual job reduces the job's runtime, but in turn decreases the efficiency\nof the overall system. We ask how the system should schedule jobs across cores\nso as to minimize the mean response time over a stream of incoming jobs.\n  To answer this question, we develop an analytical model of jobs running on a\nmulti-core machine. We prove that EQUI, a policy which continuously divides\ncores evenly across jobs, is optimal when all jobs follow a single speedup\ncurve and have exponentially distributed sizes. EQUI requires jobs to change\ntheir level of parallelization while they run. Since this is not possible for\nall workloads, we consider a class of \"fixed-width\" policies, which choose a\nsingle level of parallelization, k, to use for all jobs. We prove that,\nsurprisingly, it is possible to achieve EQUI's performance without requiring\njobs to change their levels of parallelization by using the optimal fixed level\nof parallelization, k*. We also show how to analytically derive the optimal k*\nas a function of the system load, the speedup curve, and the job size\ndistribution.\n  In the case where jobs may follow different speedup curves, finding a good\nscheduling policy is even more challenging. We find that policies like EQUI\nwhich performed well in the case of a single speedup function now perform\npoorly. We propose a very simple policy, GREEDY*, which performs near-optimally\nwhen compared to the numerically-derived optimal policy.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07097v2"
    },
    {
        "title": "Adaptive Performance Optimization under Power Constraint in Multi-thread\n  Applications with Diverse Scalability",
        "authors": [
            "Stefano Conoci",
            "Pierangelo Di Sanzo",
            "Bruno Ciciani",
            "Francesco Quaglia"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  In modern data centers, energy usage represents one of the major factors\naffecting operational costs. Power capping is a technique that limits the power\nconsumption of individual systems, which allows reducing the overall power\ndemand at both cluster and data center levels. However, literature power\ncapping approaches do not fit well the nature of important applications based\non first-class multi-thread technology. For these applications performance may\nnot grow linearly as a function of the thread-level parallelism because of the\nneed for thread synchronization while accessing shared resources, such as\nshared data. In this paper we consider the problem of maximizing the\napplication performance under a power cap by dynamically tuning the\nthread-level parallelism and the power state of the CPU-cores. Based on\nexperimental observations, we design an adaptive technique that selects in\nlinear time the optimal combination of thread-level parallelism and CPU-core\npower state for the specific workload profile of the multi-threaded\napplication. We evaluate our proposal by relying on different benchmarks,\nconfigured to use different thread synchronization methods, and compare its\neffectiveness to different state-of-the-art techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.09642v2"
    },
    {
        "title": "Reproducibility Report for the Paper: Modeling of Request Cloning in\n  Cloud Server Systems using Processor Sharing",
        "authors": [
            "Alessandro Pellegrini"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The authors have uploaded their artifact on Zenodo, which ensures a long-term\nretention of the artifact. The code is suitably documented, and some examples\nare given. A minimalistic overall description of the engine is provided. The\nartifact allows to setup the environment quite quickly, and the dependencies\nare well documented. The process to regenerate data for the figures in the\npaper completes, and all results are reproducible.\n  This paper can thus receive the Artifacts Available badge and the Artifacts\nEvaluated-Functional. Given the high quality of the artifact, also the\nArtifacts Evaluated-Reusable badge can be assigned.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.04416v1"
    },
    {
        "title": "A Prompt Report on the Performance of Intel Optane DC Persistent Memory\n  Module",
        "authors": [
            "Takahiro Hirofuchi",
            "Ryousei Takano"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  In this prompt report, we present the basic performance evaluation of Intel\nOptane Data Center Persistent Memory Module (Optane DCPMM), which is the first\ncommercially-available, byte-addressable non-volatile memory modules released\nin April 2019. Since at the moment of writing only a few reports on its\nperformance were published, this letter is intended to complement other\nperformance studies. Through experiments using our own measurement tools, we\nobtained that the latency of random read-only access was approximately 374 ns.\nThat of random writeback-involving access was 391 ns. The bandwidths of\nread-only and writeback-involving access for interleaved memory modules were\napproximately 38 GB/s and 3 GB/s, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.06018v1"
    },
    {
        "title": "MDInference: Balancing Inference Accuracy and Latency for Mobile\n  Applications",
        "authors": [
            "Samuel S. Ogden",
            "Tian Guo"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Deep Neural Networks are allowing mobile devices to incorporate a wide range\nof features into user applications. However, the computational complexity of\nthese models makes it difficult to run them effectively on resource-constrained\nmobile devices. Prior work approached the problem of supporting deep learning\nin mobile applications by either decreasing model complexity or utilizing\npowerful cloud servers. These approaches each only focus on a single aspect of\nmobile inference and thus they often sacrifice overall performance.\n  In this work we introduce a holistic approach to designing mobile deep\ninference frameworks. We first identify the key goals of accuracy and latency\nfor mobile deep inference and the conditions that must be met to achieve them.\nWe demonstrate our holistic approach through the design of a hypothetical\nframework called MDInference. This framework leverages two complementary\ntechniques; a model selection algorithm that chooses from a set of cloud-based\ndeep learning models to improve inference accuracy and an on-device request\nduplication mechanism to bound latency. Through empirically-driven simulations\nwe show that MDInference improves aggregate accuracy over static approaches by\nover 40% without incurring SLA violations. Additionally, we show that with a\ntarget latency of 250ms, MDInference increased the aggregate accuracy in 99.74%\ncases on faster university networks and 96.84% cases on residential networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.06603v3"
    },
    {
        "title": "Performance Analysis of Load Balancing Policies with Memory",
        "authors": [
            "Tim Hellemans",
            "Benny Van Houdt"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Joining the shortest or least loaded queue among $d$ randomly selected queues\nare two fundamental load balancing policies. Under both policies the dispatcher\ndoes not maintain any information on the queue length or load of the servers.\nIn this paper we analyze the performance of these policies when the dispatcher\nhas some memory available to store the ids of some of the idle servers. We\nconsider methods where the dispatcher discovers idle servers as well as methods\nwhere idle servers inform the dispatcher about their state.\n  We focus on large-scale systems and our analysis uses the cavity method. The\nmain insight provided is that the performance measures obtained via the cavity\nmethod for a load balancing policy {\\it with} memory reduce to the performance\nmeasures for the same policy {\\it without} memory provided that the arrival\nrate is properly scaled. Thus, we can study the performance of load balancers\nwith memory in the same manner as load balancers without memory. In particular\nthis entails closed form solutions for joining the shortest or least loaded\nqueue among $d$ randomly selected queues with memory in case of exponential job\nsizes. Moreover, we obtain a simple closed form expression for the (scaled)\nexpected waiting time as the system tends towards instability.\n  We present simulation results that support our belief that the approximation\nobtained by the cavity method becomes exact as the number of servers tends to\ninfinity.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.06906v2"
    },
    {
        "title": "Re-evaluating scaling methods for distributed parallel systems",
        "authors": [
            "János Végh"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The paper explains why Amdahl's Law shall be interpreted specifically for\ndistributed parallel systems and why it generated so many debates, discussions,\nand abuses. We set up a general model and list many of the terms affecting\nparallel processing. We scrutinize the validity of neglecting certain terms in\ndifferent approximations, with special emphasis on the famous scaling laws of\nparallel processing. We clarify that when using the right interpretation of\nterms, Amdahl's Law is the governing law of all kinds of parallel processing.\nAmdahl's Law describes among others the history of supercomputing, the inherent\nperformance limitation of the different kinds of parallel processing and it is\nthe basic Law of the 'modern computing' paradigm, that the computing systems\nworking under extreme computing conditions are desperately needed.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.08316v2"
    },
    {
        "title": "Toward a Unified Performance and Power Consumption NAND Flash Memory\n  Model of Embedded and Solid State Secondary Storage Systems",
        "authors": [
            "Pierre Olivier",
            "Jalil Boukhobza",
            "Eric Senn"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  This paper presents a set of models dedicated to describe a flash storage\nsubsystem structure, functions, performance and power consumption behaviors.\nThese models cover a large range of today's NAND flash memory applications.\nThey are designed to be implemented in simulation tools allowing to estimate\nand compare performance and power consumption of I/O requests on flash memory\nbased storage systems. Such tools can also help in designing and validating new\nflash storage systems and management mechanisms. This work is integrated in a\nglobal project aiming to build a framework simulating complex flash storage\nhierarchies for performance and power consumption analysis. This tool will be\nhighly configurable and modular with various levels of usage complexity\naccording to the required aim: from a software user point of view for\nsimulating storage systems, to a developer point of view for designing, testing\nand validating new flash storage management systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.1217v1"
    },
    {
        "title": "On the Catalyzing Effect of Randomness on the Per-Flow Throughput in\n  Wireless Networks",
        "authors": [
            "Florin Ciucu",
            "Jens Schmitt"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  This paper investigates the throughput capacity of a flow crossing a\nmulti-hop wireless network, whose geometry is characterized by general\nrandomness laws including Uniform, Poisson, Heavy-Tailed distributions for both\nthe nodes' densities and the number of hops. The key contribution is to\ndemonstrate \\textit{how} the \\textit{per-flow throughput} depends on the\ndistribution of 1) the number of nodes $N_j$ inside hops' interference sets, 2)\nthe number of hops $K$, and 3) the degree of spatial correlations. The\nrandomness in both $N_j$'s and $K$ is advantageous, i.e., it can yield larger\nscalings (as large as $\\Theta(n)$) than in non-random settings. An interesting\nconsequence is that the per-flow capacity can exhibit the opposite behavior to\nthe network capacity, which was shown to suffer from a logarithmic decrease in\nthe presence of randomness. In turn, spatial correlations along the end-to-end\npath are detrimental by a logarithmic term.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.7271v1"
    },
    {
        "title": "The Implications of Diverse Applications and Scalable Data Sets in\n  Benchmarking Big Data Systems",
        "authors": [
            "Zhen Jia",
            "Runlin Zhou",
            "Chunge Zhu",
            "Lei Wang",
            "Wanling Gao",
            "Yingjie Shi",
            "Jianfeng Zhan",
            "Lixin Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Now we live in an era of big data, and big data applications are becoming\nmore and more pervasive. How to benchmark data center computer systems running\nbig data applications (in short big data systems) is a hot topic. In this\npaper, we focus on measuring the performance impacts of diverse applications\nand scalable volumes of data sets on big data systems. For four typical data\nanalysis applications---an important class of big data applications, we find\ntwo major results through experiments: first, the data scale has a significant\nimpact on the performance of big data systems, so we must provide scalable\nvolumes of data sets in big data benchmarks. Second, for the four applications,\neven all of them use the simple algorithms, the performance trends are\ndifferent with increasing data scales, and hence we must consider not only\nvariety of data sets but also variety of applications in benchmarking big data\nsystems.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.7943v1"
    },
    {
        "title": "Characterizing Data Analysis Workloads in Data Centers",
        "authors": [
            "Zhen Jia",
            "Lei Wang",
            "Jianfeng Zhan",
            "Lixin Zhang",
            "Chunjie Luo"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  As the amount of data explodes rapidly, more and more corporations are using\ndata centers to make effective decisions and gain a competitive edge. Data\nanalysis applications play a significant role in data centers, and hence it has\nbecame increasingly important to understand their behaviors in order to further\nimprove the performance of data center computer systems. In this paper, after\ninvestigating three most important application domains in terms of page views\nand daily visitors, we choose eleven representative data analysis workloads and\ncharacterize their micro-architectural characteristics by using hardware\nperformance counters, in order to understand the impacts and implications of\ndata analysis workloads on the systems equipped with modern superscalar\nout-of-order processors. Our study on the workloads reveals that data analysis\napplications share many inherent characteristics, which place them in a\ndifferent class from desktop (SPEC CPU2006), HPC (HPCC), and service workloads,\nincluding traditional server workloads (SPECweb2005) and scale-out service\nworkloads (four among six benchmarks in CloudSuite), and accordingly we give\nseveral recommendations for architecture and system optimizations. On the basis\nof our workload characterization work, we released a benchmark suite named\nDCBench for typical datacenter workloads, including data analysis and service\nworkloads, with an open-source license on our project home page on\nhttp://prof.ict.ac.cn/DCBench. We hope that DCBench is helpful for performing\narchitecture and small-to-medium scale system researches for datacenter\ncomputing.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.8013v1"
    },
    {
        "title": "On Time-Sensitive Revenue Management and Energy Scheduling in Green Data\n  Centers",
        "authors": [
            "Huangxin Wang",
            "Jean X. Zhang",
            "Fei Li"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  In this paper, we design an analytically and experimentally better online\nenergy and job scheduling algorithm with the objective of maximizing net profit\nfor a service provider in green data centers. We first study the previously\nknown algorithms and conclude that these online algorithms have provable poor\nperformance against their worst-case scenarios. To guarantee an online\nalgorithm's performance in hindsight, we design a randomized algorithm to\nschedule energy and jobs in the data centers and prove the algorithm's expected\ncompetitive ratio in various settings. Our algorithm is theoretical-sound and\nit outperforms the previously known algorithms in many settings using both real\ntraces and simulated data. An optimal offline algorithm is also implemented as\nan empirical benchmark.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4865v2"
    },
    {
        "title": "Benchmarking Big Data Systems: State-of-the-Art and Future Directions",
        "authors": [
            "Rui Han",
            "Zhen Jia",
            "Wanling Gao",
            "Xinhui Tian",
            "Lei Wang"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  The great prosperity of big data systems such as Hadoop in recent years makes\nthe benchmarking of these systems become crucial for both research and industry\ncommunities. The complexity, diversity, and rapid evolution of big data systems\ngives rise to various new challenges about how we design generators to produce\ndata with the 4V properties (i.e. volume, velocity, variety and veracity), as\nwell as implement application-specific but still comprehensive workloads.\nHowever, most of the existing big data benchmarks can be described as attempts\nto solve specific problems in benchmarking systems. This article investigates\nthe state-of-the-art in benchmarking big data systems along with the future\nchallenges to be addressed to realize a successful and efficient benchmark.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.01494v1"
    },
    {
        "title": "Short Note on Costs of Floating Point Operations on current x86-64\n  Architectures: Denormals, Overflow, Underflow, and Division by Zero",
        "authors": [
            "Markus Wittmann",
            "Thomas Zeiser",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Simple floating point operations like addition or multiplication on\nnormalized floating point values can be computed by current AMD and Intel\nprocessors in three to five cycles. This is different for denormalized numbers,\nwhich appear when an underflow occurs and the value can no longer be\nrepresented as a normalized floating-point value. Here the costs are about two\nmagnitudes higher.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.03997v1"
    },
    {
        "title": "Tandem Queueing Systems Maximum Throughput Problem",
        "authors": [
            "Daniel Marian Merezeanu",
            "Daniela Andone"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  In this paper we consider the problem of maximum throughput for tandem\nqueueing system. We modeled this system as a Quasi-Birth-Death process. In\norder to do this we named level the number of customers waiting in the first\nbuffer (including the customer in service) and we called phase the state of the\nremining servers. Using this model we studied the problem of maximum throughput\nof the system: the maximum arrival rate that a given system could support\nbefore becoming saturated, or unstable. We considered different particular\ncases of such systems, which were obtained by modifying the capacity of the\nintermediary buffers, the arrival rate and the service rates. The results of\nthe simulations are presented in our paper and can be summed up in the\nfollowing conclusions: 1. The analytic formula for the maximum throughput of\nthe system tends to become rather complicated when the number of servers\nincrease 2. The maximum throughput of the system converges as the number of\nservers increases 3. The homogeneous case reveals an interesting\ncharacteristic: if we reverse the order of the servers, maximum thruoughput of\nthe system remains unchanged The QBD process used for the case of Poisson\narrivals can be extended to model more general arrival processes.\n",
        "pdf_link": "http://arxiv.org/pdf/1512.05882v1"
    },
    {
        "title": "GPGPU Performance Estimation with Core and Memory Frequency Scaling",
        "authors": [
            "Qiang Wang",
            "Xiaowen Chu"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Graphics Processing Units (GPUs) support dynamic voltage and frequency\nscaling (DVFS) in order to balance computational performance and energy\nconsumption. However, there still lacks simple and accurate performance\nestimation of a given GPU kernel under different frequency settings on real\nhardware, which is important to decide best frequency configuration for energy\nsaving. This paper reveals a fine-grained model to estimate the execution time\nof GPU kernels with both core and memory frequency scaling. Over a 2.5x range\nof both core and memory frequencies among 12 GPU kernels, our model achieves\naccurate results (within 3.5\\%) on real hardware. Compared with the cycle-level\nsimulators, our model only needs some simple micro-benchmark to extract a set\nof hardware parameters and performance counters of the kernels to produce this\nhigh accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.05308v2"
    },
    {
        "title": "Light traffic behavior under the power-of-two load balancing strategy:\n  The case of heterogeneous servers",
        "authors": [
            "Ane Izagirre",
            "Armand M. Makowski"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We consider a multi-server queueing system under the power-of-two policy with\nPoisson job arrivals, heterogeneous servers and a general job requirement\ndistribution; each server operates under the first-come first-serve policy and\nthere are no buffer constraints. We analyze the performance of this system in\nlight traffic by evaluating the first two light traffic derivatives of the\naverage job response time. These expressions point to several interesting\nstructural features associated with server heterogeneity in light traffic: For\nunequal capacities, the average job response time is seen to decrease for small\nvalues of the arrival rate, and the more diverse the server speeds, the greater\nthe gain in performance. These theoretical findings are assessed through\nlimited simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06004v1"
    },
    {
        "title": "Steady state availability general equations of decision and sequential\n  processes in Continuous Time Markov Chain models",
        "authors": [
            "Eduardo M. Vasconcelos"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Continuous Time Markov Chain (CMTC) is widely used to describe and analyze\nsystems in several knowledge areas. Steady state availability is one important\nanalysis that can be made through Markov chain formalism that allows\nresearchers generate equations for several purposes, such as channel capacity\nestimation in wireless networks as well as system performance estimations. The\nproblem with this kind of analysis is the complex process to generating these\nequations. In this letter, we have developed general equations for decision and\nsequential processes of CMTC Models, aiming to help researchers to develop\nsteady state availability equations. We also have developed the general\nequation here termed as Closed Decision Process.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.06415v1"
    },
    {
        "title": "Kerncraft: A Tool for Analytic Performance Modeling of Loop Kernels",
        "authors": [
            "Julian Hammer",
            "Jan Eitzinger",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Achieving optimal program performance requires deep insight into the\ninteraction between hardware and software. For software developers without an\nin-depth background in computer architecture, understanding and fully utilizing\nmodern architectures is close to impossible. Analytic loop performance modeling\nis a useful way to understand the relevant bottlenecks of code execution based\non simple machine models. The Roofline Model and the Execution-Cache-Memory\n(ECM) model are proven approaches to performance modeling of loop nests. In\ncomparison to the Roofline model, the ECM model can also describes the\nsingle-core performance and saturation behavior on a multicore chip. We give an\nintroduction to the Roofline and ECM models, and to stencil performance\nmodeling using layer conditions (LC). We then present Kerncraft, a tool that\ncan automatically construct Roofline and ECM models for loop nests by\nperforming the required code, data transfer, and LC analysis. The layer\ncondition analysis allows to predict optimal spatial blocking factors for loop\nnests. Together with the models it enables an ab-initio estimate of the\npotential benefits of loop blocking optimizations and of useful block sizes. In\ncases where LC analysis is not easily possible, Kerncraft supports a cache\nsimulator as a fallback option. Using a 25-point long-range stencil we\ndemonstrate the usefulness and predictive power of the Kerncraft tool.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.04653v1"
    },
    {
        "title": "Benchmarking the computing resources at the Instituto de Astrofísica\n  de Canarias",
        "authors": [
            "Nicola Caon",
            "Antonio Dorta",
            "Juan Carlos Trelles Arjona"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  The aim of this study is the characterization of the computing resources used\nby researchers at the \"Instituto de Astrof\\'isica de Canarias\" (IAC). Since\nthere is a huge demand of computing time and we use tools such as HTCondor to\nimplement High Throughput Computing (HTC) across all available PCs, it is\nessential for us to assess in a quantitative way, using objective parameters,\nthe performances of our computing nodes. In order to achieve that, we have run\na set of benchmark tests on a number of different desktop and laptop PC models\namong those used in our institution. In particular, we run the \"Polyhedron\nFortran Benchmarks\" suite, using three different compilers: GNU Fortran\nCompiler, Intel Fortran Compiler and the PGI Fortran Compiler; execution times\nare then normalized to the reference values published by Polyhedron. The same\ntests were run multiple times on a same PCs, and on 3 to 5 PCs of the same\nmodel (whenever possible) to check for repeatability and consistency of the\nresults. We found that in general execution times, for a given PC model, are\nconsistent within an uncertainty of about 10%, and show a gain in CPU speed of\na factor of about 3 between the oldest PCs used at the IAC (7-8 years old) and\nthe newest ones.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.04942v1"
    },
    {
        "title": "An analysis of core- and chip-level architectural features in four\n  generations of Intel server processors",
        "authors": [
            "Johannes Hofmann",
            "Georg Hager",
            "Gerhard Wellein",
            "Dietmar Fey"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  This paper presents a survey of architectural features among four generations\nof Intel server processors (Sandy Bridge, Ivy Bridge, Haswell, and Broad- well)\nwith a focus on performance with floating point workloads. Starting on the core\nlevel and going down the memory hierarchy we cover instruction throughput for\nfloating-point instructions, L1 cache, address generation capabilities, core\nclock speed and its limitations, L2 and L3 cache bandwidth and latency, the\nimpact of Cluster on Die (CoD) and cache snoop modes, and the Uncore clock\nspeed. Using microbenchmarks we study the influence of these factors on code\nperformance. This insight can then serve as input for analytic performance\nmodels. We show that the energy efficiency of the LINPACK and HPCG benchmarks\ncan be improved considerably by tuning the Uncore clock speed without\nsacrificing performance, and that the Graph500 benchmark performance may profit\nfrom a suitable choice of cache snoop mode settings.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.07554v1"
    },
    {
        "title": "Delay Asymptotics and Bounds for Multi-Task Parallel Jobs",
        "authors": [
            "Weina Wang",
            "Mor Harchol-Balter",
            "Haotian Jiang",
            "Alan Scheller-Wolf",
            "R. Srikant"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We study delay of jobs that consist of multiple parallel tasks, which is a\ncritical performance metric in a wide range of applications such as data file\nretrieval in coded storage systems and parallel computing. In this problem,\neach job is completed only when all of its tasks are completed, so the delay of\na job is the maximum of the delays of its tasks. Despite the wide attention\nthis problem has received, tight analysis is still largely unknown since\nanalyzing job delay requires characterizing the complicated correlation among\ntask delays, which is hard to do.\n  We first consider an asymptotic regime where the number of servers, $n$, goes\nto infinity, and the number of tasks in a job, $k^{(n)}$, is allowed to\nincrease with $n$. We establish the asymptotic independence of any $k^{(n)}$\nqueues under the condition $k^{(n)} = o(n^{1/4})$. This greatly generalizes the\nasymptotic-independence type of results in the literature where asymptotic\nindependence is shown only for a fixed constant number of queues. As a\nconsequence of our independence result, the job delay converges to the maximum\nof independent task delays.\n  We next consider the non-asymptotic regime. Here we prove that independence\nyields a stochastic upper bound on job delay for any $n$ and any $k^{(n)}$ with\n$k^{(n)}\\le n$. The key component of our proof is a new technique we develop,\ncalled \"Poisson oversampling\". Our approach converts the job delay problem into\na corresponding balls-and-bins problem. However, in contrast with typical\nballs-and-bins problems where there is a negative correlation among bins, we\nprove that our variant exhibits positive correlation.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.00296v3"
    },
    {
        "title": "Relocation in Car Sharing Systems with Shared Stackable Vehicles:\n  Modelling Challenges and Outlook",
        "authors": [
            "Chiara Boldrini",
            "Riccardo Incaini",
            "Raffaele Bruno"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Car sharing is expected to reduce traffic congestion and pollution in cities\nwhile at the same time improving accessibility to public transport. However,\nthe most popular form of car sharing, one-way car sharing, still suffers from\nthe vehicle unbalance problem. Innovative solutions to this issue rely on\ncustom vehicles with stackable capabilities: customers or operators can drive a\ntrain of vehicles if necessary, thus efficiently bringing several cars from an\narea with few requests to an area with many requests. However, how to model a\ncar sharing system with stackable vehicles is an open problem in the related\nliterature. In this paper, we propose a queueing theoretical model to fill this\ngap, and we use this model to derive an upper-bound on user-based relocation\ncapabilities. We also validate, for the first time in the related literature,\nlegacy queueing theoretical models against a trace of real car sharing data.\nFinally, we present preliminary results about the impact, on car availability,\nof simple user-based relocation heuristics with stackable vehicles. Our results\nindicate that user-based relocation schemes that exploit vehicle stackability\ncan significantly improve car availability at stations.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.01113v1"
    },
    {
        "title": "Ciw: An open source discrete event simulation library",
        "authors": [
            "Geraint I. Palmer",
            "Vincent A. Knight",
            "Paul R. Harper",
            "Asyl L. Hawa"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  This paper introduces Ciw, an open source library for conducting discrete\nevent simulations that has been developed in Python. The strengths of the\nlibrary are illustrated in terms of best practice and reproducibility for\ncomputational research. An analysis of Ciw's performance and comparison to\nseveral alternative discrete event simulation frameworks is presented.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.03561v1"
    },
    {
        "title": "Computation of gray-level co-occurrence matrix based on CUDA and its\n  optimization",
        "authors": [
            "Huichao Hong",
            "Lixin Zheng",
            "Shuwan Pan"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  As in various fields like scientific research and industrial application, the\ncomputation time optimization is becoming a task that is of increasing\nimportance because of its highly parallel architecture. The graphics processing\nunit is regarded as a powerful engine for application programs that demand\nfairly high computation capabilities. Based on this, an algorithm was\nintroduced in this paper to optimize the method used to compute the gray-level\nco-occurrence matrix (GLCM) of an image, and strategies (e.g., \"copying\",\n\"image partitioning\", etc.) were proposed to optimize the parallel algorithm.\nResults indicate that without losing the computational accuracy, the speed-up\nratio of the GLCM computation of images with different resolutions by GPU by\nthe use of CUDA was 50 times faster than that of the GLCM computation by CPU,\nwhich manifested significantly improved performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.06189v1"
    },
    {
        "title": "Analysis of the Leakage Queue: A Queueing Model for Energy Storage\n  Systems with Self-discharge",
        "authors": [
            "Majid Raeis",
            "Almut Burchard",
            "Jorg Liebeherr"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Energy storage is a crucial component of the smart grid, since it provides\nthe ability to buffer transient fluctuations of the energy supply from\nrenewable sources. Even without a load, energy storage systems experience a\nreduction of the stored energy through self-discharge. In some storage\ntechnologies, the rate of self-discharge can exceed 50% of the stored energy\nper day. In this paper, we investigate the self-discharge phenomenon in energy\nstorage using a queueing system model, which we refer to as leakage queue. When\nthe average net charge is positive, we discover that the leakage queue operates\nin one of two regimes: a leakage-dominated regime and a capacity-dominated\nregime. We find that in the leakage-dominated regime, the stored energy\nstabilizes at a point that is below the storage capacity. Under suitable\nindependence assumptions for energy supply and demand, the stored energy in\nthis regime closely follows a normal distribution. We present two methods for\ncomputing probabilities of underflow and overflow at a leakage queue. The\nmethods are validated in a numerical example where the energy supply resembles\na wind energy source.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09506v1"
    },
    {
        "title": "Performance optimizations for scalable CFD applications on hybrid\n  CPU+MIC heterogeneous computing system with millions of cores",
        "authors": [
            "Yong-Xian Wang",
            "Li-Lun Zhang",
            "Wei Liu",
            "Xing-Hua Cheng",
            "Yu Zhuang",
            "Anthony T. Chronopoulos"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  For computational fluid dynamics (CFD) applications with a large number of\ngrid points/cells, parallel computing is a common efficient strategy to reduce\nthe computational time. How to achieve the best performance in the modern\nsupercomputer system, especially with heterogeneous computing resources such as\nhybrid CPU+GPU, or a CPU + Intel Xeon Phi (MIC) co-processors, is still a great\nchallenge.\n  An in-house parallel CFD code capable of simulating three dimensional\nstructured grid applications is developed and tested in this study. Several\nmethods of parallelization, performance optimization and code tuning both in\nthe CPU-only homogeneous system and in the heterogeneous system are proposed\nbased on identifying potential parallelism of applications, balancing the work\nload among all kinds of computing devices, tuning the multi-thread code toward\nbetter performance in intra-machine node with hundreds of CPU/MIC cores, and\noptimizing the communication among inter-nodes, inter-cores, and between CPUs\nand MICs.\n  Some benchmark cases from model and/or industrial CFD applications are tested\non the Tianhe-1A and Tianhe-2 supercomputer to evaluate the performance. Among\nthese CFD cases, the maximum number of grid cells reached 780 billion. The\ntuned solver successfully scales up to half of the entire Tianhe-2\nsupercomputer system with over 1.376 million of heterogeneous cores. The test\nresults and performance analysis are discussed in detail.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.09995v1"
    },
    {
        "title": "Power Modelling for Heterogeneous Cloud-Edge Data Centers",
        "authors": [
            "Kai Chen",
            "Blesson Varghese",
            "Peter Kilpatrick",
            "Dimitrios S. Nikolopoulos"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Existing power modelling research focuses not on the method used for\ndeveloping models but rather on the model itself. This paper aims to develop a\nmethod for deploying power models on emerging processors that will be used, for\nexample, in cloud-edge data centers. Our research first develops a hardware\ncounter selection method that appropriately selects counters most correlated to\npower on ARM and Intel processors. Then, we propose a two stage power model\nthat works across multiple architectures. The key results are: (i) the\nautomated hardware performance counter selection method achieves comparable\nselection to the manual selection methods reported in literature, and (ii) the\ntwo stage power model can predict dynamic power more accurately on both ARM and\nIntel processors when compared to classic power models.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.10325v1"
    },
    {
        "title": "Collecting and Presenting Reproducible Intranode Stencil Performance:\n  INSPECT",
        "authors": [
            "Julian Hornich",
            "Julian Hammer",
            "Georg Hager",
            "Thomas Gruber",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Stencil algorithms have been receiving considerable interest in HPC research\nfor decades. The techniques used to approach multi-core stencil performance\nmodeling and engineering span basic runtime measurements, elaborate performance\nmodels, detailed hardware counter analysis, and thorough scaling behavior\nevaluation. Due to the plurality of approaches and stencil patterns, we set out\nto develop a generalizable methodology for reproducible measurements\naccompanied by state-of-the-art performance models. Our open-source toolchain,\nand collected results are publicly available in the \"Intranode Stencil\nPerformance Evaluation Collection\" (INSPECT). We present the underlying\nmethodologies, models and tools involved in gathering and documenting the\nperformance behavior of a collection of typical stencil patterns across\nmultiple architectures and hardware configuration options. Our aim is to endow\nperformance-aware application developers with reproducible baseline performance\ndata and validated models to initiate a well-defined process of performance\nassessment and optimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.08138v2"
    },
    {
        "title": "EasyCrash: Exploring Non-Volatility of Non-Volatile Memory for High\n  Performance Computing Under Failures",
        "authors": [
            "Jie Ren",
            "Kai Wu",
            "Dong Li"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Emerging non-volatile memory (NVM) is promising for building future HPC.\nLeveraging the non-volatility of NVM as main memory, we can restart the\napplication using data objects remaining on NVM when the application crashes.\nThis paper explores this solution to handle HPC under failures, based on the\nobservation that many HPC applications have good enough intrinsic fault\ntolerance. To improve the possibility of successful recomputation with correct\noutcomes and ignorable performance loss, we introduce EasyCrash, a framework to\ndecide how to selectively persist application data objects during application\nexecution. Our evaluation shows that EasyCrash transforms 54% of crashes that\ncannot correctly recompute into the correct computation while incurring a\nnegligible performance overhead (1.5% on average). Using EasyCrash and\napplication intrinsic fault tolerance, 82% of crashes can successfully\nrecompute. When EasyCrash is used with a traditional checkpoint scheme, it\nenables up to 24% improvement (15% on average) in system efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.10081v1"
    },
    {
        "title": "ScalAna: Automating Scaling Loss Detection with Graph Analysis",
        "authors": [
            "Yuyang Jin",
            "Haojie Wang",
            "Teng Yu",
            "Xiongchao Tang",
            "Torsten Hoefler",
            "Xu Liu",
            "Jidong Zhai"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Scaling a parallel program to modern supercomputers is challenging due to\ninter-process communication, Amdahl's law, and resource contention. Performance\nanalysis tools for finding such scaling bottlenecks either base on profiling or\ntracing. Profiling incurs low overheads but does not capture detailed\ndependencies needed for root-cause analysis. Tracing collects all information\nat prohibitive overheads. In this work, we design ScalAna that uses static\nanalysis techniques to achieve the best of both worlds - it enables the\nanalyzability of traces at a cost similar to profiling. ScalAna first leverages\nstatic compiler techniques to build a Program Structure Graph, which records\nthe main computation and communication patterns as well as the program's\ncontrol structures. At runtime, we adopt lightweight techniques to collect\nperformance data according to the graph structure and generate a Program\nPerformance Graph. With this graph, we propose a novel approach, called\nbacktracking root cause detection, which can automatically and efficiently\ndetect the root cause of scaling loss. We evaluate ScalAna with real\napplications. Results show that our approach can effectively locate the root\ncause of scaling loss for real applications and incurs 1.73% overhead on\naverage for up to 2,048 processes. We achieve up to 11.11% performance\nimprovement by fixing the root causes detected by ScalAna on 2,048 processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01692v1"
    },
    {
        "title": "Efficiency Near the Edge: Increasing the Energy Efficiency of FFTs on\n  GPUs for Real-time Edge Computing",
        "authors": [
            "Karel Adámek",
            "Jan Novotný",
            "Jeyarajan Thiyagalingam",
            "Wesley Armour"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The Square Kilometre Array (SKA) is an international initiative for\ndeveloping the world's largest radio telescope with a total collecting area of\nover a million square meters. The scale of the operation, combined with the\nremote location of the telescope, requires the use of energy-efficient\ncomputational algorithms. This, along with the extreme data rates that will be\nproduced by the SKA and the requirement for a real-time observing capability,\nnecessitates in-situ data processing in an edge style computing solution. More\ngenerally, energy efficiency in the modern computing landscape is becoming of\nparamount concern. Whether it be the power budget that can limit some of the\nworld's largest supercomputers, or the limited power available to the smallest\nInternet-of-Things devices. In this paper, we study the impact of hardware\nfrequency scaling on the energy consumption and execution time of the Fast\nFourier Transform (FFT) on NVIDIA GPUs using the cuFFT library. The FFT is used\nin many areas of science and it is one of the key algorithms used in radio\nastronomy data processing pipelines. Through the use of frequency scaling, we\nshow that we can lower the power consumption of the NVIDIA V100 GPU when\ncomputing the FFT by up to 60% compared to the boost clock frequency, with less\nthan a 10% increase in the execution time. Furthermore, using one common core\nclock frequency for all tested FFT lengths, we show on average a 50% reduction\nin power consumption compared to the boost core clock frequency with an\nincrease in the execution time still below 10%. We demonstrate how these\nresults can be used to lower the power consumption of existing data processing\npipelines. These savings, when considered over years of operation, can yield\nsignificant financial savings, but can also lead to a significant reduction of\ngreenhouse gas emissions.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.06009v2"
    },
    {
        "title": "Towards an Objective Metric for the Performance of Exact Triangle Count",
        "authors": [
            "Mark P. Blanco",
            "Scott McMillan",
            "Tze Meng Low"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The performance of graph algorithms is often measured in terms of the number\nof traversed edges per second (TEPS). However, this performance metric is\ninadequate for a graph operation such as exact triangle counting. In triangle\ncounting, execution times on graphs with a similar number of edges can be\ndistinctly different as demonstrated by results from the past Graph Challenge\nentries. We discuss the need for an objective performance metric for graph\noperations and the desired characteristics of such a metric such that it more\naccurately captures the interactions between the amount of work performed and\nthe capabilities of the hardware on which the code is executed. Using exact\ntriangle counting as an example, we derive a metric that captures how certain\ntechniques employed in many implementations improve performance. We demonstrate\nthat our proposed metric can be used to evaluate and compare multiple\napproaches for triangle counting, using a SIMD approach as a case study against\na scalar baseline.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.07935v3"
    },
    {
        "title": "On the Throughput Optimization in Large-Scale Batch-Processing Systems",
        "authors": [
            "Sounak Kar",
            "Robin Rehrmann",
            "Arpan Mukhopadhyay",
            "Bastian Alt",
            "Florin Ciucu",
            "Heinz Koeppl",
            "Carsten Binnig",
            "Amr Rizk"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We analyze a data-processing system with $n$ clients producing jobs which are\nprocessed in \\textit{batches} by $m$ parallel servers; the system throughput\ncritically depends on the batch size and a corresponding sub-additive speedup\nfunction. In practice, throughput optimization relies on numerical searches for\nthe optimal batch size, a process that can take up to multiple days in existing\ncommercial systems. In this paper, we model the system in terms of a closed\nqueueing network; a standard Markovian analysis yields the optimal throughput\nin $\\omega\\left(n^4\\right)$ time. Our main contribution is a mean-field model\nof the system for the regime where the system size is large. We show that the\nmean-field model has a unique, globally attractive stationary point which can\nbe found in closed form and which characterizes the asymptotic throughput of\nthe system as a function of the batch size. Using this expression we find the\n\\textit{asymptotically} optimal throughput in $O(1)$ time. Numerical settings\nfrom a large commercial system reveal that this asymptotic optimum is accurate\nin practical finite regimes.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.09433v1"
    },
    {
        "title": "Investigating Applications on the A64FX",
        "authors": [
            "Adrian Jackson",
            "Michèle Weiland",
            "Nick Brown",
            "Andrew Turner",
            "Mark Parsons"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The A64FX processor from Fujitsu, being designed for computational simulation\nand machine learning applications, has the potential for unprecedented\nperformance in HPC systems. In this paper, we evaluate the A64FX by\nbenchmarking against a range of production HPC platforms that cover a number of\nprocessor technologies. We investigate the performance of complex scientific\napplications across multiple nodes, as well as single node and mini-kernel\nbenchmarks. This paper finds that the performance of the A64FX processor across\nour chosen benchmarks often significantly exceeds other platforms, even without\nspecific application optimisations for the processor instruction set or\nhardware. However, this is not true for all the benchmarks we have undertaken.\nFurthermore, the specific configuration of applications can have an impact on\nthe runtime and performance experienced.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.11806v1"
    },
    {
        "title": "Pass-and-Swap Queues",
        "authors": [
            "Céline Comte",
            "Jan-Pieter Dorsman"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Order-independent (OI) queues, introduced by Berezner, Kriel, and Krzesinski\nin 1995, expanded the family of multi-class queues that are known to have a\nproduct-form stationary distribution by allowing for intricate class-dependent\nservice rates. This paper further broadens this family by introducing\npass-and-swap (P&S) queues, an extension of OI queues where, upon a service\ncompletion, the customer that completes service is not necessarily the one that\nleaves the system. More precisely, we supplement the OI queue model with an\nundirected graph on the customer classes, which we call a swapping graph, such\nthat there is an edge between two classes if customers of these classes can be\nswapped with one another. When a customer completes service, it passes over\ncustomers in the remainder of the queue until it finds a customer it can swap\npositions with, that is, a customer whose class is a neighbor in the graph. In\nits turn, the customer that is ejected from its position takes the position of\nthe next customer it can be swapped with, and so on. This is repeated until a\ncustomer can no longer find another customer to be swapped with; this customer\nis the one that leaves the queue. After proving that P&S queues have a\nproduct-form stationary distribution, we derive a necessary and sufficient\nstability condition for (open networks of) P&S queues that also applies to OI\nqueues. We then study irreducibility properties of closed networks of P&S\nqueues and derive the corresponding product-form stationary distribution.\nLastly, we demonstrate that closed networks of P&S queues can be applied to\ndescribe the dynamics of new and existing load-distribution and scheduling\nprotocols in clusters of machines in which jobs have assignment constraints.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.12299v3"
    },
    {
        "title": "Stationary analysis of the Shortest Queue First service policy",
        "authors": [
            "Fabrice Guillemin",
            "Alain Simonian"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  We analyze the so-called Shortest Queue First (SQF) queueing discipline\nwhereby a unique server addresses queues in parallel by serving at any time\nthat queue with the smallest workload. Considering a stationary system composed\nof two parallel queues and assuming Poisson arrivals and general service time\ndistributions, we first establish the functional equations satisfied by the\nLaplace transforms of the workloads in each queue. We further specialize these\nequations to the so-called \"symmetric case\", with same arrival rates and\nidentical exponential service time distributions at each queue; we then obtain\na functional equation $$ M(z) = q(z) \\cdot M \\circ h(z) + L(z) $$ for unknown\nfunction $M$, where given functions $q$, $L$ and $h$ are related to one branch\nof a cubic polynomial equation. We study the analyticity domain of function $M$\nand express it by a series expansion involving all iterates of function $h$.\nThis allows us to determine empty queue probabilities along with the tail of\nthe workload distribution in each queue. This tail appears to be identical to\nthat of the Head-of-Line preemptive priority system, which is the key feature\ndesired for the SQF discipline.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3490v1"
    },
    {
        "title": "Stationary analysis of the \"Shortest Queue First\" service policy: the\n  asymmetric case",
        "authors": [
            "Fabrice Guillemin",
            "Alain Simonian"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  As a follow-up to a recent paper considering two symmetric queues, the\n\\textit{Shortest Queue First} service discipline is presently analysed for two\ngeneral asymmetric queues. Using the results previously established and\nassuming exponentially distributed service times, the bivariate Laplace\ntransform of workloads in each queue is shown to depend on the solution\n$\\mathbf{M}$ to a two-dimensional functional equation $$ \\mathbf{M} = Q_1 \\cdot\n\\mathbf{M}\\circ h_1 + Q_2 \\cdot \\mathbf{M}\\circ h_2 + \\mathbf{L} $$ with given\nmatrices $Q_1$, $Q_2$ and vector $\\mathbf{L}$ and where functions $h_1$ and\n$h_2$ are defined each on some rational curve; solution $\\mathbf{M}$ can then\nrepresented by a series expansion involving the semi-group $< h_1, h_2 >$\ngenerated by these two functions. The empty queue probabilities along with the\ntail behaviour of the workload distribution at each queue are characterised.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3496v1"
    },
    {
        "title": "Analysis of a non-work conserving Generalized Processor Sharing queue",
        "authors": [
            "Fabrice Guillemin"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  We consider in this paper a non work-conserving Generalized Processor Sharing\n(GPS) system composed of two queues with Poisson arrivals and exponential\nservice times. Using general results due to Fayolle \\emph{et al}, we first\nestablish the stability condition for this system. We then determine the\nfunctional equation satisfied by the generating function of the numbers of jobs\nin both queues and the associated Riemann-Hilbert problem. We prove the\nexistence and the uniqueness of the solution. This allows us to completely\ncharacterize the system, in particular to compute the empty queue probability.\nWe finally derive the tail asymptotics of the number of jobs in one queue.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.3536v1"
    },
    {
        "title": "A Metric for Performance Portability",
        "authors": [
            "S. J. Pennycook",
            "J. D. Sewall",
            "V. W. Lee"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  The term \"performance portability\" has been informally used in computing to\nrefer to a variety of notions which generally include: 1) the ability to run\none application across multiple hardware platforms; and 2) achieving some\nnotional level of performance on these platforms. However, there has been a\nnoticeable lack of consensus on the precise meaning of the term, and authors'\nconclusions regarding their success (or failure) to achieve performance\nportability have thus been subjective. Comparing one approach to performance\nportability with another has generally been marked with vague claims and\nverbose, qualitative explanation of the comparison. This paper presents a\nconcise definition for performance portability, along with a simple metric that\naccurately captures the performance and portability of an application across\ndifferent platforms. The utility of this metric is then demonstrated with a\nretroactive application to previous work.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.07409v1"
    },
    {
        "title": "Geographical Load Balancing across Green Datacenters",
        "authors": [
            "Giovanni Neglia",
            "Matteo Sereno",
            "Giuseppe Bianchi"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  \"Geographic Load Balancing\" is a strategy for reducing the energy cost of\ndata centers spreading across different terrestrial locations. In this paper,\nwe focus on load balancing among micro-datacenters powered by renewable energy\nsources. We model via a Markov Chain the problem of scheduling jobs by\nprioritizing datacenters where renewable energy is currently available. Not\nfinding a convenient closed form solution for the resulting chain, we use mean\nfield techniques to derive an asymptotic approximate model which instead is\nshown to have an extremely simple and intuitive steady state solution. After\nproving, using both theoretical and discrete event simulation results, that the\nsystem performance converges to the asymptotic model for an increasing number\nof datacenters, we exploit the simple closed form model's solution to\ninvestigate relationships and trade-offs among the various system parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.03709v1"
    },
    {
        "title": "Optimizing Stochastic Scheduling in Fork-Join Queueing Models: Bounds\n  and Applications",
        "authors": [
            "Wasiur R. KhudaBukhsh",
            "Amr Rizk",
            "Alexander Frömmgen",
            "Heinz Koeppl"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Fork-Join (FJ) queueing models capture the dynamics of system parallelization\nunder synchronization constraints, for example, for applications such as\nMapReduce, multipath transmission and RAID systems. Arriving jobs are first\nsplit into tasks and mapped to servers for execution, such that a job can only\nleave the system when all of its tasks are executed.\n  In this paper, we provide computable stochastic bounds for the waiting and\nresponse time distributions for heterogeneous FJ systems under general\nparallelization benefit. Our main contribution is a generalized mathematical\nframework for probabilistic server scheduling strategies that are essentially\ncharacterized by a probability distribution over the number of utilized\nservers, and the optimization thereof. We highlight the trade-off between the\nscaling benefit due to parallelization and the FJ inherent synchronization\npenalty. Further, we provide optimal scheduling strategies for arbitrary\nscaling regimes that map to different levels of parallelization benefit. One\nnotable insight obtained from our results is that different applications with\nvarying parallelization benefits result in different optimal strategies.\nFinally, we complement our analytical results by applying them to various\napplications showing the optimality of the proposed scheduling strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.05486v3"
    },
    {
        "title": "A Generalized Performance Evaluation Framework for Parallel Systems with\n  Output Synchronization",
        "authors": [
            "Wasiur R. KhudaBukhsh",
            "Sounak Kar",
            "Amr Rizk",
            "Heinz Koeppl"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Frameworks, such as MapReduce and Hadoop are abundant nowadays. They seek to\nreap benefits of parallelization, albeit subject to a synchronization\nconstraint at the output. Fork-Join (FJ) queuing models are used to analyze\nsuch systems. Arriving jobs are split into tasks each of which is mapped to\nexactly one server. A job leaves the system when all of its tasks are executed.\n  As a metric of performance, we consider waiting times for both\nwork-conserving and non-work conserving server systems under a mathematical\nset-up general enough to take into account possible phase-type behavior of the\nservers, and as suggested by recent evidences, bursty arrivals.\n  To this end, we present a Markov-additive process framework for an FJ system\nand provide computable bounds on tail probabilities of steady-state waiting\ntimes, for both types of servers separately. We apply our results to three\nscenarios, namely, non-renewal (Markov-modulated) arrivals, servers showing\nphase-type behavior, and Markov-modulated arrivals and services. We compare our\nbounds against estimates obtained through simulations and also provide a\ntheoretical conceptualization of provisions in FJ systems. Finally, we\ncalibrate our model with real data traces, and illustrate how our bounds can be\nused to devise provisions.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.05543v1"
    },
    {
        "title": "M/g/c/c state dependent queueing model for road traffic simulation",
        "authors": [
            "Nacira Guerrouahane",
            "Djamil Aissani",
            "Louiza Bouallouche-Medjkoune",
            "Nadir Farhi"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  In this paper, we present a stochastic queuing model for the road traffic,\nwhich captures the stationary density-flow relationships in both uncongested\nand congestion conditions. The proposed model is based on the $M/g/c/c$ state\ndependent queuing model of Jain and Smith, and is inspired from the\ndeterministic Godunov scheme for the road traffic simulation. We first propose\na reformulation of the $M/g/c/c$ state dependent model that works with\ndensity-flow fundamental diagrams rather than density-speed relationships. We\nthen extend this model in order to consider upstream traffic demand as well as\ndownstream traffic supply. Finally, we calculate the speed and travel time\ndistributions for the $M/g/c/c$ state dependent queuing model and for the\nproposed model, and derive stationary performance measures (expected number of\ncars, blocking probability, expected travel time, and throughput). A comparison\nwith results predicted by the $M/g/c/c$ state dependent queuing model shows\nthat the proposed model correctly represents the dynamics of traffic and gives\ngood performances measures. The results illustrate the good accuracy of the\nproposed model.\n",
        "pdf_link": "http://arxiv.org/pdf/1612.09532v1"
    },
    {
        "title": "Modeling Impact of Human Errors on the Data Unavailability and Data Loss\n  of Storage Systems",
        "authors": [
            "Mostafa Kishani",
            "Hossein Asadi"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Data storage systems and their availability play a crucial role in\ncontemporary datacenters. Despite using mechanisms such as automatic fail-over\nin datacenters, the role of human agents and consequently their destructive\nerrors is inevitable. Due to very large number of disk drives used in exascale\ndatacenters and their high failure rates, the disk subsystem in storage systems\nhas become a major source of Data Unavailability (DU) and Data Loss (DL)\ninitiated by human errors. In this paper, we investigate the effect of\nIncorrect Disk Replacement Service (IDRS) on the availability and reliability\nof data storage systems. To this end, we analyze the consequences of IDRS in a\ndisk array, and conduct Monte Carlo simulations to evaluate DU and DL during\nmission time. The proposed modeling framework can cope with a) different\nstorage array configurations and b) Data Object Survivability (DOS),\nrepresenting the effect of system level redundancies such as remote backups and\nmirrors. In the proposed framework, the model parameters are obtained from\nindustrial and scientific reports alongside field data which have been\nextracted from a datacenter operating with 70 storage racks. The results show\nthat ignoring the impact of IDRS leads to unavailability underestimation by up\nto three orders of magnitude. Moreover, our study suggests that by considering\nthe effect of human errors, the conventional beliefs about the dependability of\ndifferent Redundant Array of Independent Disks (RAID) mechanisms should be\nrevised. The results show that RAID1 can result in lower availability compared\nto RAID5 in the presence of human errors. The results also show that employing\nautomatic fail-over policy (using hot spare disks) can reduce the drastic\nimpacts of human errors by two orders of magnitude.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01352v2"
    },
    {
        "title": "Towards an Achievable Performance for the Loop Nests",
        "authors": [
            "Aniket Shivam",
            "Neftali Watkinson",
            "Alexandru Nicolau",
            "David Padua",
            "Alexander V. Veidenbaum"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Numerous code optimization techniques, including loop nest optimizations,\nhave been developed over the last four decades. Loop optimization techniques\ntransform loop nests to improve the performance of the code on a target\narchitecture, including exposing parallelism. Finding and evaluating an\noptimal, semantic-preserving sequence of transformations is a complex problem.\nThe sequence is guided using heuristics and/or analytical models and there is\nno way of knowing how close it gets to optimal performance or if there is any\nheadroom for improvement. This paper makes two contributions. First, it uses a\ncomparative analysis of loop optimizations/transformations across multiple\ncompilers to determine how much headroom may exist for each compiler. And\nsecond, it presents an approach to characterize the loop nests based on their\nhardware performance counter values and a Machine Learning approach that\npredicts which compiler will generate the fastest code for a loop nest. The\nprediction is made for both auto-vectorized, serial compilation and for\nauto-parallelization. The results show that the headroom for state-of-the-art\ncompilers ranges from 1.10x to 1.42x for the serial code and from 1.30x to\n1.71x for the auto-parallelized code. These results are based on the Machine\nLearning predictions.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.00603v1"
    },
    {
        "title": "Communication over a time correlated channel with an energy harvesting\n  transmitter",
        "authors": [
            "Mehdi Salehi Heydar Abad",
            "Deniz Gunduz",
            "Ozgur Ercetin"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In this work, communication over a time-correlated point-to-point wireless\nchannel is studied for an energy harvesting (EH) transmitter. In this model, we\ntake into account the time and energy cost of acquiring channel state\ninformation. At the beginning of the time slot, the EH transmitter, has to\nchoose among three possible actions: i) deferring the transmission to save its\nenergy for future use, ii) transmitting without sensing, and iii) sensing the\nchannel before transmission. At each time slot, the transmitter chooses one of\nthe three possible actions to maximize the total expected discounted number of\nbits transmitted over an infinite time horizon. This problem can be formulated\nas a partially observable Markov decision process (POMDP) which is then\nconverted to an ordinary MDP by introducing a belief on the channel state, and\nthe optimal policy is shown to exhibit a threshold behavior on the belief\nstate, with battery-dependent threshold values. Optimal threshold values and\ncorresponding optimal performance are characterized through numerical\nsimulations, and it is shown that having the sensing action and intelligently\nusing it to track the channel state improves the achievable long-term\nthroughput significantly.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.04567v1"
    },
    {
        "title": "Energy harvesting wireless networks with correlated energy sources",
        "authors": [
            "Mehdi Salehi Heydar Abad",
            "Deniz Gunduz",
            "Ozgur Ercetin"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This work considers a system with two energy harvesting (EH) nodes\ntransmitting to a common destination over a random access channel. The amount\nof harvested energy is assumed to be random and independent over time, but\ncorrelated among the nodes possibly with respect to their relative position. A\nthreshold-based transmission policy is developed for the maximization of the\nexpected aggregate network throughput. Assuming that there is no a priori\nchannel state or EH information available to the nodes, the aggregate network\nthroughput is obtained. The optimal thresholds are determined for two\npractically important special cases: i) at any time only one of the sensors\nharvests energy due to, for example, physical separation of the nodes; ii) the\nnodes are spatially close, and at any time, either both nodes or none of them\nharvests energy.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.04890v1"
    },
    {
        "title": "A Valgrind Tool to Compute the Working Set of a Software Process",
        "authors": [
            "Martin Becker",
            "Samarjit Chakraborty"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This paper introduces a new open-source tool for the dynamic analyzer\nValgrind. The tool measures the amount of memory that is actively being used by\na process at any given point in time. While there exist numerous tools to\nmeasure the memory requirements of a process, the vast majority only focuses on\nmetrics like resident or proportional set sizes, which include memory that was\nonce claimed, but is momentarily disused. Consequently, such tools do not\npermit drawing conclusions about how much cache or RAM a process actually\nrequires at each point in time, and thus cannot be used for performance\ndebugging. The few tools which do measure only actively used memory, however,\nhave limitations in temporal resolution and introspection. In contrast, our\ntool offers an easy way to compute the memory that has recently been accessed\nat any point in time, reflecting how cache and RAM requirements change over\ntime. In particular, this tool computes the set of memory references made\nwithin a fixed time interval before any point in time, known as the working\nset, and captures call stacks for interesting peaks in the working set size. We\nfirst introduce the tool, then we run some examples comparing the output from\nour tool with similar memory tools, and we close with a discussion of\nlimitations\n",
        "pdf_link": "http://arxiv.org/pdf/1902.11028v1"
    },
    {
        "title": "Load Balancing Guardrails: Keeping Your Heavy Traffic on the Road to Low\n  Response Times",
        "authors": [
            "Isaac Grosof",
            "Ziv Scully",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Load balancing systems, comprising a central dispatcher and a scheduling\npolicy at each server, are widely used in practice, and their response time has\nbeen extensively studied in the theoretical literature. While much is known\nabout the scenario where the scheduling at the servers is\nFirst-Come-First-Served (FCFS), to minimize mean response time we must use\nShortest-Remaining-Processing-Time (SRPT) scheduling at the servers. Much less\nis known about dispatching polices when SRPT scheduling is used. Unfortunately,\ntraditional dispatching policies that are used in practice in systems with FCFS\nservers often have poor performance in systems with SRPT servers. In this\npaper, we devise a simple fix that can be applied to any dispatching policy.\nThis fix, called guardrails, ensures that the dispatching policy yields optimal\nmean response time under heavy traffic when used in a system with SRPT servers.\nAny dispatching policy, when augmented with guardrails, becomes heavy-traffic\noptimal. Our results yield the first analytical bounds on mean response time\nfor load balancing systems with SRPT scheduling at the servers.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03439v1"
    },
    {
        "title": "Multiplicação de matrizes: uma comparação entre as\n  abordagens sequencial (CPU) e paralela (GPU)",
        "authors": [
            "Andre G. C. Pacheco"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Designing problems using matrices is very important in Computer Science.\nFields like graph computer, graphs theory, and machine learning use matrices\nvery often to solve their own problems. The most often matrix operation is the\nmultiplication. It may be time-consuming if the matrices to be multiplied are\nlarge. For this reason, the parallel computer became a must to tackle this\nproblem. In this report, it is presented a comparison between sequential and\nparallel approaches to computing the matrix multiplication using CUDA and\nopenMP. The results show the importance of parallelizing mainly when the\nmatrices are large.\n  A modelagem de problemas utilizando matrizes \\'e de extrema import\\^ancia\npara Ci\\^encia da Computa\\c{c}\\~ao. \\'Areas como computa\\c{c}\\~ao gr\\'afica,\ngrafos e aprendizado de m\\'aquina utilizam matrizes com alta frequ\\^encia para\nsolucionar seus respectivos problemas. Dessa forma, operar matrizes de maneira\neficiente \\'e muito importante para o desempenho de algoritmos. Uma das\nopera\\c{c}\\~oes de matrizes mais utilizadas \\'e a multiplica\\c{c}\\~ao, que se\ntorna um empecilho para o desempenho computacional de algoritmos na medida que\no tamanho das matrizes a serem multiplicadas aumentam. Por conta disso, a\ncomputa\\c{c}\\~ao paralela se tornou uma solu\\c{c}\\~ao padr\\~ao para abordar tal\nproblema. Neste trabalho \\'e apresentado uma compara\\c{c}\\~ao entre as\nabordagens sequencial e paralela para multiplica\\c{c}\\~ao de matrizes\nutilizando CUDA e OpenMP. O resultado da an\\'alise realizada entre o tamanho da\nmatriz e o desempenho da multiplica\\c{c}\\~ao mostra a import\\^ancia da\nparaleliza\\c{c}\\~ao principalmente para matrizes de ordem elevada.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.03641v1"
    },
    {
        "title": "QPS-r: A Cost-Effective Crossbar Scheduling Algorithm and Its Stability\n  and Delay Analysis",
        "authors": [
            "Long Gong",
            "Jun Xu",
            "Liang Liu",
            "Siva Theja Maguluri"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In an input-queued switch, a crossbar schedule, or a matching between the\ninput ports and the output ports needs to be computed in each switching cycle,\nor time slot. Designing switching algorithms with very low computational\ncomplexity, that lead to high throughput and small delay is a challenging\nproblem. There appears to be a fundamental tradeoff between the computational\ncomplexity of the switching algorithm and the resultants throughput and delay.\nParallel maximal matching algorithms (adapted for switching) appear to have\nstricken a sweet spot in this tradeoff, and prior work has shown the following\nperformance guarantees. Using maximal matchings in every time slot results in\nat least 50% switch throughput and order-optimal (i.e., independent of the\nswitch size N) average delay bounds for various traffic arrival processes. On\nthe other hand, their computational complexity can be as low as $O(log^2N)$ per\nport/processor, which is much lower than those of the algorithms such as\nmaximum weighted matching which ensures better throughput performance.\n  In this work, we propose QPS-r, a parallel iterative switching algorithm that\nhas the lowest possible computational complexity: O(1) per port. Using Lyapunov\nstability analysis, we show that the throughput and delay performance is\nidentical to that of maximal matching algorithm. Although QPS-r builds upon an\nexisting technique called Queue-Proportional Sampling (QPS), in this paper, we\nprovide analytical guarantees on its throughput and delay under i.i.d. traffic\nas well as a Markovian traffic model which can model many realistic traffic\npatterns. We also demonstrate that QPS-3 (running 3 iterations) has comparable\nempirical throughput and delay performances as iSLIP (running $log_2 N$\niterations), a refined and optimized representative maximal matching algorithm\nadapted for switching.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.05392v3"
    },
    {
        "title": "Propagation and Decay of Injected One-Off Delays on Clusters: A Case\n  Study",
        "authors": [
            "Ayesha Afzal",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Analytic, first-principles performance modeling of distributed-memory\napplications is difficult due to a wide spectrum of random disturbances caused\nby the application and the system. These disturbances (commonly called \"noise\")\ndestroy the assumptions of regularity that one usually employs when\nconstructing simple analytic models. Despite numerous efforts to quantify,\ncategorize, and reduce such effects, a comprehensive quantitative understanding\nof their performance impact is not available, especially for long delays that\nhave global consequences for the parallel application. In this work, we\ninvestigate various traces collected from synthetic benchmarks that mimic real\napplications on simulated and real message-passing systems in order to pinpoint\nthe mechanisms behind delay propagation. We analyze the dependence of the\npropagation speed of idle waves emanating from injected delays with respect to\nthe execution and communication properties of the application, study how such\ndelays decay under increased noise levels, and how they interact with each\nother. We also show how fine-grained noise can make a system immune against the\nadverse effects of propagating idle waves. Our results contribute to a better\nunderstanding of the collective phenomena that manifest themselves in\ndistributed-memory parallel applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.10603v3"
    },
    {
        "title": "The Impact of GPU DVFS on the Energy and Performance of Deep Learning:\n  an Empirical Study",
        "authors": [
            "Zhenheng Tang",
            "Yuxin Wang",
            "Qiang Wang",
            "Xiaowen Chu"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Over the past years, great progress has been made in improving the computing\npower of general-purpose graphics processing units (GPGPUs), which facilitates\nthe prosperity of deep neural networks (DNNs) in multiple fields like computer\nvision and natural language processing. A typical DNN training process\nrepeatedly updates tens of millions of parameters, which not only requires huge\ncomputing resources but also consumes significant energy. In order to train\nDNNs in a more energy-efficient way, we empirically investigate the impact of\nGPU Dynamic Voltage and Frequency Scaling (DVFS) on the energy consumption and\nperformance of deep learning. Our experiments cover a wide range of GPU\narchitectures, DVFS settings, and DNN configurations. We observe that, compared\nto the default core frequency settings of three tested GPUs, the optimal core\nfrequency can help conserve 8.7%$\\sim$23.1% energy consumption for different\nDNN training cases. Regarding the inference, the benefits vary from\n19.6%$\\sim$26.4%. Our findings suggest that GPU DVFS has great potentials to\nhelp develop energy efficient DNN training/inference schemes.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.11012v1"
    },
    {
        "title": "Real-Time Prediction of Delay Distribution in Service Systems using\n  Mixture Density Networks",
        "authors": [
            "Majid Raeis",
            "Ali Tizghadam",
            "Alberto Leon-Garcia"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Motivated by interest in providing more efficient services in customer\nservice systems, we use statistical learning methods and delay history\ninformation to predict the conditional distribution of the customers' waiting\ntimes in queueing systems. From the predicted distributions, descriptive\nstatistics of the system such as the mean, variance and percentiles of the\nwaiting times can be obtained, which can be used for delay announcements, SLA\nconformance and better system management. We model the conditional\ndistributions by mixtures of Gaussians, parameters of which can be estimated\nusing Mixture Density Networks. The evaluations show that exploiting more delay\nhistory information can result in much more accurate predictions under\nrealistic time-varying arrival assumptions.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.08368v1"
    },
    {
        "title": "Performance benefits of Intel(R) OptaneTM DC persistent memory for the\n  parallel processing of large neuroimaging data",
        "authors": [
            "Valerie Hayot-Sasson",
            "Shawn T Brown",
            "Tristan Glatard"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Open-access neuroimaging datasets have reached petabyte scale, and continue\nto grow. The ability to leverage the entirety of these datasets is limited to a\nrestricted number of labs with both the capacity and infrastructure to process\nthe data. Whereas Big Data engines have significantly reduced application\nperformance penalties with respect to data movement, their applied strategies\n(e.g. data locality, in-memory computing and lazy evaluation) are not\nnecessarily practical within neuroimaging workflows where intermediary results\nmay need to be materialized to shared storage for post-processing analysis. In\nthis paper we evaluate the performance advantage brought by Intel(R) OptaneTM\nDC persistent memory for the processing of large neuroimaging datasets using\nthe two available configurations modes: Memory mode and App Direct mode. We\nemploy a synthetic algorithm on the 76 GiB and 603 GiB BigBrain, as well as\napply a standard neuroimaging application on the Consortium for Reliability and\nReproducibility (CoRR) dataset using 25 and 96 parallel processes in both\ncases. Our results show that the performance of applications leveraging\npersistent memory is superior to that of other storage devices,with the\nexception of DRAM. This is the case in both Memory and App Direct mode and\nirrespective of the amount of data and parallelism. Furthermore, persistent\nmemory in App Direct mode is believed to benefit from the use of DRAM as a\ncache for writing when output data is significantly smaller than available\nmemory. We believe the use of persistent memory will be beneficial to both\nneuroimaging applications running on HPC or visualization of large,\nhigh-resolution images.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.11794v1"
    },
    {
        "title": "On the Asymptotic Optimality of Work-Conserving Disciplines in\n  Completion Time Minimization",
        "authors": [
            "Wenxin Li"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In this paper, we prove that under mild stochastic assumptions,\nwork-conserving disciplines are asymptotic optimal for minimizing total\ncompletion time. As a byproduct of our analysis, we obtain tight upper bound on\nthe competitive ratios of work-conserving disciplines on minimizing the metric\nof flow time.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.12535v4"
    },
    {
        "title": "Finally, how many efficiencies supercomputers have? And, what do they\n  measure?",
        "authors": [
            "János Végh"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Using an extremely large number of processing elements in computing systems\nleads to unexpected phenomena, such as different efficiencies of the same\nsystem for different tasks, that cannot be explained in the frame of classical\ncomputing paradigm. The simple non-technical (but considering the temporal\nbehavior of the components) model, introduced here, enables us to set up a\nframe and formalism, needed to explain those unexpected experiences around\nsupercomputing. Introducing temporal behavior into computer science also\nexplains why only the extreme scale computing enabled us to reveal the\nexperienced limitations. The paper shows, that degradation of efficiency of\nparallelized sequential systems is a natural consequence of the classical\ncomputing paradigm, instead of being an engineering imperfectness. The\nworkload, that supercomputers run, is much responsible for wasting energy, as\nwell as limiting the size and type of tasks. Case studies provide insight, how\ndifferent contributions compete for dominating the resulting payload\nperformance of a computing system, and how enhancing the interconnection\ntechnology made computing+communication to dominate in defining the efficiency\nof supercomputers. Our model also enables to derive predictions about\nsupercomputer performance limitations for the near future, as well as it\nprovides hints for enhancing supercomputer components. Phenomena experienced in\nlarge-scale computing show interesting parallels with phenomena experienced in\nscience, more than a century ago, and through their studying a modern science\nwas developed.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.01266v9"
    },
    {
        "title": "A Fast Analytical Model of Fully Associative Caches",
        "authors": [
            "Tobias Gysi",
            "Tobias Grosser",
            "Laurin Brandner",
            "Torsten Hoefler"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  While the cost of computation is an easy to understand local property, the\ncost of data movement on cached architectures depends on global state, does not\ncompose, and is hard to predict. As a result, programmers often fail to\nconsider the cost of data movement. Existing cache models and simulators\nprovide the missing information but are computationally expensive. We present a\nlightweight cache model for fully associative caches with least recently used\n(LRU) replacement policy that gives fast and accurate results. We count the\ncache misses without explicit enumeration of all memory accesses by using\nsymbolic counting techniques twice: 1) to derive the stack distance for each\nmemory access and 2) to count the memory accesses with stack distance larger\nthan the cache size. While this technique seems infeasible in theory, due to\nnon-linearities after the first round of counting, we show that the counting\nproblems are sufficiently linear in practice. Our cache model often computes\nthe results within seconds and contrary to simulation the execution time is\nmostly problem size independent. Our evaluation measures modeling errors below\n0.6% on real hardware. By providing accurate data placement information we\nenable memory hierarchy aware software development.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.01653v1"
    },
    {
        "title": "Parallel Binary Code Analysis",
        "authors": [
            "Xiaozhu Meng",
            "Jonathon M. Anderson",
            "John Mellor-Crummey",
            "Mark W. Krentel",
            "Barton P. Miller",
            "Srđan Milaković"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Binary code analysis is widely used to assess a program's correctness,\nperformance, and provenance. Binary analysis applications often construct\ncontrol flow graphs, analyze data flow, and use debugging information to\nunderstand how machine code relates to source lines, inlined functions, and\ndata types. To date, binary analysis has been single-threaded, which is too\nslow for applications such as performance analysis and software forensics,\nwhere it is becoming common to analyze binaries that are gigabytes in size and\nin large batches that contain thousands of binaries.\n  This paper describes our design and implementation for accelerating the task\nof constructing control flow graphs (CFGs) from binaries with multithreading.\nExisting research focuses on addressing challenging code constructs encountered\nduring constructing CFGs, including functions sharing code, jump table\nanalysis, non-returning functions, and tail calls. However, existing analyses\ndo not consider the complex interactions between concurrent analysis of shared\ncode, making it difficult to extend existing serial algorithms to be parallel.\nA systematic methodology to guide the design of parallel algorithms is\nessential. We abstract the task of constructing CFGs as repeated applications\nof several core CFG operations regarding to creating functions, basic blocks,\nand edges. We then derive properties among CFG operations, including operation\ndependency, commutativity, monotonicity. These operation properties guide our\ndesign of a new parallel analysis for constructing CFGs. We achieved as much as\n25$\\times$ speedup for constructing CFGs on 64 hardware threads. Binary\nanalysis applications are significantly accelerated with the new parallel\nanalysis: we achieve 8$\\times$ for a performance analysis tool and 7$\\times$\nfor a software forensic tool with 16 hardware threads.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10621v3"
    },
    {
        "title": "Understanding Memory Access Patterns Using the BSC Performance Tools",
        "authors": [
            "Harald Servat",
            "Jesús Labarta",
            "Hans-Christian Hoppe",
            "Judit Giménez",
            "Antonio J. Peña"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The growing gap between processor and memory speeds results in complex memory\nhierarchies as processors evolve to mitigate such divergence by taking\nadvantage of the locality of reference. In this direction, the BSC performance\nanalysis tools have been recently extended to provide insight relative to the\napplication memory accesses depicting their temporal and spatial\ncharacteristics, correlating with the source-code and the achieved performance\nsimultaneously. These extensions rely on the Precise Event-Based Sampling\n(PEBS) mechanism available in recent Intel processors to capture information\nregarding the application memory accesses. The sampled information is later\ncombined with the Folding technique to represent a detailed temporal evolution\nof the memory accesses and in conjunction with the achieved performance and the\nsource-code counterpart. The results obtained from the combination of these\ntools help not only application developers but also processor architects to\nunderstand better how the application behaves and how the system performs. In\nthis paper, we describe a tighter integration of the sampling mechanism into\nthe monitoring package. We also demonstrate the value of the complete workflow\nby exploring already optimized state--of--the--art benchmarks, providing\ndetailed insight of their memory access behavior. We have taken advantage of\nthis insight to apply small modifications that improve the applications'\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05872v2"
    },
    {
        "title": "High Performance and Portable Convolution Operators for ARM-based\n  Multicore Processors",
        "authors": [
            "Pablo San Juan",
            "Adrián Castelló",
            "Manuel F. Dolz",
            "Pedro Alonso-Jordá",
            "Enrique S. Quintana-Ortí"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The considerable impact of Convolutional Neural Networks on many Artificial\nIntelligence tasks has led to the development of various high performance\nalgorithms for the convolution operator present in this type of networks. One\nof these approaches leverages the \\imcol transform followed by a general matrix\nmultiplication (GEMM) in order to take advantage of the highly optimized\nrealizations of the GEMM kernel in many linear algebra libraries. The main\nproblems of this approach are 1) the large memory workspace required to host\nthe intermediate matrices generated by the IM2COL transform; and 2) the time to\nperform the IM2COL transform, which is not negligible for complex neural\nnetworks. This paper presents a portable high performance convolution algorithm\nbased on the BLIS realization of the GEMM kernel that avoids the use of the\nintermediate memory by taking advantage of the BLIS structure. In addition, the\nproposed algorithm eliminates the cost of the explicit IM2COL transform, while\nmaintaining the portability and performance of the underlying realization of\nGEMM in BLIS.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.06410v1"
    },
    {
        "title": "Latency Analysis of Multiple Classes of AVB Traffic in TSN with Standard\n  Credit Behavior using Network Calculus",
        "authors": [
            "Luxi Zhao",
            "Paul Pop",
            "Zhong Zheng",
            "Hugo Daigmorte",
            "Marc Boyer"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Time-Sensitive Networking (TSN) is a set of amendments that extend Ethernet\nto support distributed safety-critical and real-time applications in the\nindustrial automation, aerospace and automotive areas. TSN integrates multiple\ntraffic types and supports interactions in several combinations. In this paper\nwe consider the configuration supporting Scheduled Traffic (ST) traffic\nscheduled based on Gate-Control-Lists (GCLs), Audio-Video-Bridging (AVB)\ntraffic according to IEEE 802.1BA that has bounded latencies, and Best-Effort\n(BE) traffic, for which no guarantees are provided. The paper extends the\ntiming analysis method to multiple AVB classes and proofs the credit bounds for\nmultiple classes of AVB traffic, respectively under frozen and non-frozen\nbehaviors of credit during guard band (GB). They are prerequisites for\nnon-overflow credits of Credit-Based Shaper (CBS) and preventing starvation of\nAVB traffic. Moreover, this paper proposes an improved timing analysis method\nreducing the pessimism for the worst-case end-to-end delays of AVB traffic by\nconsidering the limitations from the physical link rate and the output of CBS.\nFinally, we evaluate the improved analysis method on both synthetic and\nreal-world test cases, showing the significant reduction of pessimism on\nlatency bounds compared to related work, and presenting the correctness\nvalidation compared with simulation results. We also compare the AVB latency\nbounds in the case of frozen and non-frozen credit during GB. Additionally, we\nevaluate the scalability of our method with variation of the load of ST flows\nand of the bandwidth reservation for AVB traffic.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.08256v2"
    },
    {
        "title": "Optimal Resource Allocation for Elastic and Inelastic Jobs",
        "authors": [
            "Benjamin Berg",
            "Mor Harchol-Balter",
            "Benjamin Moseley",
            "Weina Wang",
            "Justin Whitehouse"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Modern data centers are tasked with processing heterogeneous workloads\nconsisting of various classes of jobs. These classes differ in their arrival\nrates, size distributions, and job parallelizability. With respect to\nparalellizability, some jobs are elastic, meaning they can parallelize linearly\nacross many servers. Other jobs are inelastic, meaning they can only run on a\nsingle server. Although job classes can differ drastically, they are typically\nforced to share a single cluster. When sharing a cluster among heterogeneous\njobs, one must decide how to allocate servers to each job at every moment in\ntime. In this paper, we design and analyze allocation policies which aim to\nminimize the mean response time across jobs, where a job's response time is the\ntime from when it arrives until it completes.\n  We model this problem in a stochastic setting where each job may be elastic\nor inelastic. Job sizes are drawn from exponential distributions, but are\nunknown to the system. We show that, in the common case where elastic jobs are\nlarger on average than inelastic jobs, the optimal allocation policy is\nInelastic-First, giving inelastic jobs preemptive priority over elastic jobs.\nWe obtain this result by introducing a novel sample path argument. We also show\nthat there exist cases where Elastic-First (giving priority to elastic jobs)\nperforms better than Inelastic-First. We then provide the first analysis of\nmean response time under both Elastic-First and Inelastic-First by leveraging\nrecent techniques for solving high-dimensional Markov chains.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.09745v1"
    },
    {
        "title": "Storage Workload Modelling by Hidden Markov Models: Application to FLASH\n  Memory",
        "authors": [
            "P. G. Harrison",
            "S. K. Harrison",
            "N. M. Patel",
            "S. Zertal"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  A workload analysis technique is presented that processes data from operation\ntype traces and creates a Hidden Markov Model (HMM) to represent the workload\nthat generated those traces. The HMM can be used to create representative\ntraces for performance models, such as simulators, avoiding the need to\nrepeatedly acquire suitable traces. It can also be used to estimate directly\nthe transition probabilities and rates of a Markov modulated arrival process,\nfor use as input to an analytical performance model of Flash memory. The HMMs\nobtained from industrial workloads are validated by comparing their\nautocorrelation functions and other statistics with those of the corresponding\nmonitored time series. Further, the performance model applications are\nillustrated by numerical examples.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.3315v1"
    },
    {
        "title": "A lightweight optimization selection method for Sparse Matrix-Vector\n  Multiplication",
        "authors": [
            "Athena Elafrou",
            "Georgios Goumas",
            "Nectarios Koziris"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  In this paper, we propose an optimization selection methodology for the\nubiquitous sparse matrix-vector multiplication (SpMV) kernel. We propose two\nmodels that attempt to identify the major performance bottleneck of the kernel\nfor every instance of the problem and then select an appropriate optimization\nto tackle it. Our first model requires online profiling of the input matrix in\norder to detect its most prevailing performance issue, while our second model\nonly uses comprehensive structural features of the sparse matrix. Our method\ndelivers high performance stability for SpMV across different platforms and\nsparse matrices, due to its application and architecture awareness. Our\nexperimental results demonstrate that a) our approach is able to distinguish\nand appropriately optimize special matrices in multicore platforms that fall\nout of the standard class of memory bandwidth bound matrices, and b) lead to a\nsignificant performance gain of 29% in a manycore platform compared to an\narchitecture-centric optimization, as a result of the successful selection of\nthe appropriate optimization for the great majority of the matrices. With a\nruntime overhead equivalent to a couple dozen SpMV operations, our approach is\npractical for use in iterative numerical solvers of real-life applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.02494v3"
    },
    {
        "title": "Model-Driven Automatic Tiling with Cache Associativity Lattices",
        "authors": [
            "David Adjiashvili",
            "Utz-Uwe Haus",
            "Adrian Tate"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Traditional compiler optimization theory distinguishes three separate classes\nof cache miss -- Cold, Conflict and Capacity. Tiling for cache is typically\nguided by capacity miss counts. Models of cache function have not been\neffectively used to guide cache tiling optimizations due to model error and\nexpense. Instead, heuristic or empirical approaches are used to select tilings.\nWe argue that conflict misses, traditionally neglected or seen as a small\nconstant effect, are the only fundamentally important cache miss category, that\nthey form a solid basis by which caches can become modellable, and that models\nleaning on cache associatvity analysis can be used to generate cache performant\ntilings. We develop a mathematical framework that expresses potential and\nactual cache misses in associative caches using Associativity Lattices. We show\nthese lattices to possess two theoretical advantages over rectangular tiles --\nvolume maximization and miss regularity. We also show that to generate such\nlattice tiles requires, unlike rectangular tiling, no explicit, expensive\nlattice point counting. We also describe an implementation of our lattice\ntiling approach, show that it can be used to give speedups of over 10x versus\nunoptimized code, and despite currently only tiling for one level of cache, can\nalready be competitive with the aggressive compiler optimizations used in\ngeneral purposes compares such as GCC and Intel's ICC. We also show that the\ntiling approach can lead to reasonable automatic parallelism when compared to\nexisting auto-threading compilers.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.05585v1"
    },
    {
        "title": "HPA: An Opportunistic Approach to Embedded Energy Efficiency",
        "authors": [
            "Baptiste Delporte",
            "Roberto Rigamonti",
            "Alberto Dassatti"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Reducing energy consumption is a challenge that is faced on a daily basis by\nteams from the High-Performance Computing as well as the Embedded domain. This\nissue is mostly attacked from an hardware perspective, by devising\narchitectures that put energy efficiency as a primary target, often at the cost\nof processing power. Lately, computing platforms have become more and more\nheterogeneous, but the exploitation of these additional capabilities is so\ncomplex from the application developer's perspective that they are left unused\nmost of the time, resulting therefore in a supplemental waste of energy rather\nthan in faster processing times.\n  In this paper we present a transparent, on-the-fly optimization scheme that\nallows a generic application to automatically exploit the available computing\nunits to partition its computational load. We have called our approach\nHeterogeneous Platform Accelerator (HPA). The idea is to use profiling to\nautomatically select a computing-intensive candidate for acceleration, and then\ndistribute the computations to the different units by off-loading blocks of\ncode to them.\n  Using an NVIDIA Jetson TK1 board, we demonstrate that not only HPA results in\nfaster processing speed, but also in a considerable reduction in the total\nenergy absorbed.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.08635v1"
    },
    {
        "title": "Online Scheduling of Spark Workloads with Mesos using Different Fair\n  Allocation Algorithms",
        "authors": [
            "Yuquan Shan",
            "Aman Jain",
            "George Kesidis",
            "Bhuvan Urgaonkar",
            "Jalal Khamse-Ashari",
            "Ioannis Lambadaris"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In the following, we present example illustrative and experimental results\ncomparing fair schedulers allocating resources from multiple servers to\ndistributed application frameworks. Resources are allocated so that at least\none resource is exhausted in every server. Schedulers considered include DRF\n(DRFH) and Best-Fit DRF (BF-DRF), TSF, and PS-DSF. We also consider server\nselection under Randomized Round Robin (RRR) and based on their residual\n(unreserved) resources. In the following, we consider cases with frameworks of\nequal priority and without server-preference constraints. We first give typical\nresults of a illustrative numerical study and then give typical results of a\nstudy involving Spark workloads on Mesos which we have modified and\nopen-sourced to prototype different schedulers.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.00922v3"
    },
    {
        "title": "On the accuracy and usefulness of analytic energy models for\n  contemporary multicore processors",
        "authors": [
            "Johannes Hofmann",
            "Georg Hager",
            "Dietmar Fey"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  This paper presents refinements to the execution-cache-memory performance\nmodel and a previously published power model for multicore processors. The\ncombination of both enables a very accurate prediction of performance and\nenergy consumption of contemporary multicore processors as a function of\nrelevant parameters such as number of active cores as well as core and Uncore\nfrequencies. Model validation is performed on the Sandy Bridge-EP and\nBroadwell-EP microarchitectures. Production-related variations in chip quality\nare demonstrated through a statistical analysis of the fit parameters obtained\non one hundred Broadwell-EP CPUs of the same model. Insights from the models\nare used to explain the performance- and energy-related behavior of the\nprocessors for scalable as well as saturating (i.e., memory-bound) codes. In\nthe process we demonstrate the models' capability to identify optimal operating\npoints with respect to highest performance, lowest energy-to-solution, and\nlowest energy-delay product and identify a set of best practices for\nenergy-efficient execution.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.01618v1"
    },
    {
        "title": "A Markov Chain Monte Carlo Approach to Cost Matrix Generation for\n  Scheduling Performance Evaluation",
        "authors": [
            "Louis-Claude Canon",
            "Mohamad El Sayah",
            "Pierre-Cyrille Héam"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In high performance computing, scheduling of tasks and allocation to machines\nis very critical especially when we are dealing with heterogeneous execution\ncosts. Simulations can be performed with a large variety of environments and\napplication models. However, this technique is sensitive to bias when it relies\non random instances with an uncontrolled distribution. We use methods from the\nliterature to provide formal guarantee on the distribution of the instance. In\nparticular, it is desirable to ensure a uniform distribution among the\ninstances with a given task and machine heterogeneity. In this article, we\npropose a method that generates instances (cost matrices) with a known\ndistribution where tasks are scheduled on machines with heterogeneous execution\ncosts.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08121v1"
    },
    {
        "title": "Unix Memory Allocations are Not Poisson",
        "authors": [
            "James Garnett",
            "Elizabeth Bradley"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In multitasking operating systems, requests for free memory are traditionally\nmodeled as a stochastic counting process with independent,\nexponentially-distributed interarrival times because of the analytic simplicity\nsuch Poisson models afford. We analyze the distribution of several million unix\npage commits to show that although this approach could be valid over relatively\nlong timespans, the behavior of the arrival process over shorter periods is\ndecidedly not Poisson. We find that this result holds regardless of the\noriginator of the request: unlike network packets, there is little difference\nbetween system- and user-level page-request distributions. We believe this to\nbe due to the bursty nature of page allocations, which tend to occur in either\nsmall or extremely large increments. Burstiness and persistent variance have\nrecently been found in self-similar processes in computer networks, but we show\nthat although page commits are both bursty and possess high variance over long\ntimescales, they are probably not self-similar. These results suggest that\naltogether different models are needed for fine-grained analysis of memory\nsystems, an important consideration not only for understanding behavior but\nalso for the design of online control systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.08981v1"
    },
    {
        "title": "Effect of payload size on mean response time when message segmentations\n  occur using $\\rm{M}^{\\rm X}/\\rm{G}/1$ queueing model",
        "authors": [
            "Takashi Ikegawa"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  This paper proposes the $\\rm{M}^{\\rm X}/\\rm{G}/1$ queueing model to represent\narrivals of segmented packets when message segmentations occur. This queueing\nmodel enables us to derive the closed form of mean response time, given payload\nsize, message size distribution and message arrival rate. From a numerical\nresult, we show that the mean response time is more convex in payload sizes if\nmessage arrival rate is larger in a scenario where Web objects are delivered\nover a physical link.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.10553v1"
    },
    {
        "title": "Fine-Grained Energy and Performance Profiling framework for Deep\n  Convolutional Neural Networks",
        "authors": [
            "Crefeda Faviola Rodrigues",
            "Graham Riley",
            "Mikel Lujan"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  There is a huge demand for on-device execution of deep learning algorithms on\nmobile and embedded platforms. These devices present constraints on the\napplication due to limited resources and power. Hence, developing\nenergy-efficient solutions to address this issue will require innovation in\nalgorithmic design, software and hardware. Such innovation requires\nbenchmarking and characterization of Deep Neural Networks based on performance\nand energy-consumption alongside accuracy. However, current benchmarks studies\nin existing deep learning frameworks (for example, Caffe, Tensorflow, Torch and\nothers) are based on performance of these applications on high-end CPUs and\nGPUs. In this work, we introduce a benchmarking framework called \"SyNERGY\" to\nmeasure the energy and time of 11 representative Deep Convolutional Neural\nNetworks on embedded platforms such as NVidia Jetson TX1. We integrate ARM's\nStreamline Performance Analyser with standard deep learning frameworks such as\nCaffe and CuDNNv5, to study the execution behaviour of current deep learning\nmodels at a fine-grained level (or specific layers) on image processing tasks.\nIn addition, we build an initial multi-variable linear regression model to\npredict energy consumption of unseen neural network models based on the number\nof SIMD instructions executed and main memory accesses of the CPU cores of the\nTX1 with an average relative test error rate of 8.04 +/- 5.96 %. Surprisingly,\nwe find that it is possible to refine the model to predict the number of SIMD\ninstructions and main memory accesses solely from the application's\nMultiply-Accumulate (MAC) counts, hence, eliminating the need for actual\nmeasurements. Our predicted results demonstrate 7.08 +/- 6.0 % average relative\nerror over actual energy measurements of all 11 networks tested, except\nMobileNet. By including MobileNet the average relative test error increases to\n17.33 +/- 12.2 %.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.11151v2"
    },
    {
        "title": "Compiler Phase Ordering as an Orthogonal Approach for Reducing Energy\n  Consumption",
        "authors": [
            "Ricardo Nobre",
            "Luís Reis",
            "João M. P. Cardoso"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Compiler writers typically focus primarily on the performance of the\ngenerated program binaries when selecting the passes and the order in which\nthey are applied in the standard optimization levels, such as GCC -O3. In some\ndomains, such as embedded systems and High-Performance Computing (HPC), it\nmight be sometimes acceptable to slowdown computations if the energy consumed\ncan be significantly decreased. Embedded systems often rely on a battery and\nbesides energy also have power dissipation limitations, while HPC centers have\na growing concern with electricity and cooling costs. Relying on power policies\nto apply frequency/voltage scaling and/or change the CPU to idle states (e.g.,\nalternate between power levels in bursts) as the main method to reduce energy\nleaves potential for improvement using other orthogonal approaches. In this\nwork we evaluate the impact of compiler pass sequences specialization (also\nknown as compiler phase ordering) as a means to reduce the energy consumed by a\nset of programs/functions when comparing with the use of the standard compiler\nphase orders provided by, e.g., -OX flags. We use our phase selection and\nordering framework to explore the design space in the context of a Clang+LLVM\ncompiler targeting a multicore ARM processor in an ODROID board and a dual x86\ndesktop representative of a node in a Supercomputing center. Our experiments\nwith a set of representative kernels show that there we can reduce energy\nconsumption by up to 24% and that some of these improvements can only be\npartially explained by improvements to execution time. The experiments show\ncases where applications that run faster consume more energy. Additionally, we\nmake an effort to characterize the compiler sequence exploration space in terms\nof their impact on performance and energy.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00638v1"
    },
    {
        "title": "A Queuing Model for CPU Functional Unit and Issue Queue Configuration",
        "authors": [
            "Shane Carroll",
            "Wei-Ming Ling"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In a superscalar processor, instructions of various types flow through an\nexecution pipeline, traversing hardware resources which are mostly shared among\nmany different instruction types. A notable exception to shared pipeline\nresources is the collection of functional units, the hardware that performs\nspecific computations. In a trade-off of cost versus performance, a pipeline\ndesigner must decide how many of each type of functional unit to place in a\nprocessor's pipeline. In this paper, we model a superscalar processor's issue\nqueue and functional units as a novel queuing network. We treat the issue queue\nas a finite-sized waiting area and the functional units as servers. In addition\nto common queuing problems, customers of the network share the queue but wait\nfor specific servers to become ready (e.g., addition instructions wait for\nadders). Furthermore, the customers in this queue are not necessary ready for\nservice, since instructions may be waiting for operands. In this paper we model\na novel queuing network that provides a solution to the expected queue length\nof each type of instruction. This network and its solution can also be\ngeneralized to other problems, notably other resource-allocation issues that\narise in superscalar pipelines.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.08586v1"
    },
    {
        "title": "The Power of d Choices in Scheduling for Data Centers with Heterogeneous\n  Servers",
        "authors": [
            "Amir Moaddeli",
            "Iman Nabati Ahmadi",
            "Negin Abhar"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  MapReduce framework is the de facto in big data and its applications where a\nbig data-set is split into small data chunks that are replicated on different\nservers among thousands of servers. The heterogeneous server structure of the\nsystem makes the scheduling much harder than scheduling for systems with\nhomogeneous servers. Throughput optimality of the system on one hand and delay\noptimality on the other hand creates a dilemma for assigning tasks to servers.\nThe JSQ-MaxWeight and Balanced-Pandas algorithms are the states of the arts\nalgorithms with theoretical guarantees on throughput and delay optimality for\nsystems with two and three levels of data locality. However, the scheduling\ncomplexity of these two algorithms are way too much. Hence, we use the power of\n$d$ choices algorithm combined with the Balanced-Pandas algorithm and the\nJSQ-MaxWeight algorithm, and compare the complexity of the simple algorithms\nand the power of $d$ choices versions of them. We will further show that the\nBalanced-Pandas algorithm combined with the power of the $d$ choices,\nBalanced-Pandas-Pod, not only performs better than simple Balanced-Pandas, but\nalso is less sensitive to the parameter $d$ than the combination of the\nJSQ-MaxWeight algorithm and the power of the $d$ choices, JSQ-MaxWeight-Pod. In\nfact in our extensive simulation results, the Balanced-Pandas-Pod algorithm is\nperforming better than the simple Balanced-Pandas algorithm in low and medium\nloads, where data centers are usually performing at, and performs almost the\nsame as the Balanced-Pandas algorithm at high loads. Note that the load\nbalancing complexity of Balanced-Pandas and JSQ-MaxWeight algorithms are\n$O(M)$, where $M$ is the number of servers in the system which is in the order\nof thousands servers, whereas the complexity of Balanced-Pandas-Pod and\nJSQ-MaxWeight-Pod are $O(1)$, that makes the central scheduler faster and saves\nenergy.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.00447v1"
    },
    {
        "title": "A Processor-Sharing model for the Performance of Virtualized Network\n  Functions",
        "authors": [
            "Fabrice Guillemin",
            "Veronica Quintuna Rodriguez",
            "Alain Simonian"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The parallel execution of requests in a Cloud Computing platform, as for\nVirtualized Network Functions, is modeled by an $M^{[X]}/M/1$ Processor-Sharing\n(PS) system, where each request is seen as a batch of unit jobs. The\nperformance of such paralleled system can then be measured by the quantiles of\nthe batch sojourn time distribution. In this paper, we address the evaluation\nof this distribution for the $M^{[X]}/M/1$-PS queue with batch arrivals and\ngeometrically distributed batch size. General results on the residual busy\nperiod (after a tagged batch arrival time) and the number of unit jobs served\nduring this residual busy period are first derived. This enables us to provide\nan approximation for the distribution tail of the batch sojourn time whose\naccuracy is confirmed by simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05615v1"
    },
    {
        "title": "On the sojourn of an arbitrary customer in an $M/M/1$ Processor Sharing\n  Queue",
        "authors": [
            "Fabrice Guillemin",
            "Veronica Quintuna Rodriguez"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In this paper, we consider the number of both arrivals and departures seen by\na tagged customer while in service in a classical $M/M/1$ processor sharing\nqueue. By exploiting the underlying orthogonal structure of this queuing system\nrevealed in an earlier study, we compute the distributions of these two\nquantities and prove that they are equal in distribution. We moreover derive\nthe asymptotic behavior of this common distribution. The knowledge of the\nnumber of departures seen by a tagged customer allows us to test the validity\nof an approximation, which consists of assuming that the tagged customer is\nrandomly served among those customers in the residual busy period of the queue\nfollowing the arrival of the tagged customer. A numerical evidence shows that\nthis approximation is reasonable for moderate values of the number of\ndepartures, given that the asymptotic behaviors of the distributions are very\ndifferent even if the exponential decay rates are equal.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05654v1"
    },
    {
        "title": "Performance Models for Data Transfers: A Case Study with Molecular\n  Chemistry Kernels",
        "authors": [
            "Suraj Kumar",
            "Lionel Eyraud-Dubois",
            "Sriram Krishnamoorthy"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  With increasing complexity of hardwares, systems with different memory nodes\nare ubiquitous in High Performance Computing (HPC). It is paramount to develop\nstrategies to overlap the data transfers between memory nodes with computations\nin order to exploit the full potential of these systems. In this article, we\nconsider the problem of deciding the order of data transfers between two memory\nnodes for a set of independent tasks with the objective to minimize the\nmakespan. We prove that with limited memory capacity, obtaining the optimal\norder of data transfers is a NP-complete problem. We propose several heuristics\nfor this problem and provide details about their favorable situations. We\npresent an analysis of our heuristics on traces, obtained by running 2\nmolecular chemistry kernels, namely, Hartree-Fock (HF) and Coupled Cluster\nSingle Double (CCSD) on 10 nodes of an HPC system. Our results show that some\nof our heuristics achieve significant overlap for moderate memory capacities\nand are very close to the lower bound of makespan.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.06825v1"
    },
    {
        "title": "Performance of Devito on HPC-Optimised ARM Processors",
        "authors": [
            "Hermes Senger",
            "Jaime F. de Souza",
            "Edson S. Gomi",
            "Fabio Luporini",
            "Gerard J. Gorman"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We evaluate the performance of Devito, a domain specific language (DSL) for\nfinite differences on Arm ThunderX2 processors. Experiments with two common\nseismic computational kernels demonstrate that Arm processors can deliver\ncompetitive performance compared to other Intel Xeon processors.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.03653v2"
    },
    {
        "title": "MLP Aware Scheduling Techniques in Multithreaded Processors",
        "authors": [
            "Murthy Durbhakula"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Major chip manufacturers have all introduced Multithreaded processors. These\nprocessors are used for running a variety of workloads. Efficient resource\nutilization is an important design aspect in such processors. Particularly, it\nis important to take advantage of available memory-level parallelism(MLP). In\nthis paper I propose a MLP aware operating system (OS) scheduling algorithm for\nMultithreaded Multi-core processors. By observing the MLP available in each\nthread and by balancing it with available MLP resources in the system the OS\nwill come up with a new schedule of threads for the next quantum that could\npotentially improve overall performance. We do a qualitative comparison of our\nsolution with other hardware and software techniques. This work can be extended\nby doing a quantitative evaluation and by further refining the scheduling\noptimization.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04236v1"
    },
    {
        "title": "uPredict: A User-Level Profiler-Based Predictive Framework for Single VM\n  Applications in Multi-Tenant Clouds",
        "authors": [
            "Hamidreza Moradi",
            "Wei Wang",
            "Amanda Fernandez",
            "Dakai Zhu"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Most existing studies on performance prediction for virtual machines (VMs) in\nmulti-tenant clouds are at system level and generally require access to\nperformance counters in Hypervisors. In this work, we propose uPredict, a\nuser-level profiler-based performance predictive framework for single-VM\napplications in multi-tenant clouds. Here, three micro-benchmarks are specially\ndevised to assess the contention of CPUs, memory and disks in a VM,\nrespectively. Based on measured performance of an application and\nmicro-benchmarks, the application and VM-specific predictive models can be\nderived by exploiting various regression and neural network based techniques.\nThese models can then be used to predict the application's performance using\nthe in-situ profiled resource contention with the micro-benchmarks. We\nevaluated uPredict extensively with representative benchmarks from PARSEC, NAS\nParallel Benchmarks and CloudSuite, on both a private cloud and two public\nclouds. The results show that the average prediction errors are between 9.8% to\n17% for various predictive models on the private cloud with high resource\ncontention, while the errors are within 4% on public clouds. A smart\nload-balancing scheme powered by uPredict is presented and can effectively\nreduce the execution and turnaround times of the considered application by 19%\nand 10%, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.04491v1"
    },
    {
        "title": "Computing System Congestion Management Using Exponential Smoothing\n  Forecasting",
        "authors": [
            "James F Brady"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  An overloaded computer must finish what it starts and not start what will\nfail or hang. A congestion management algorithm the author developed, and\nSiemens Corporation patented for telecom products, effectively manages traffic\noverload with its unique formulation of Exponential Smoothing forecasting.\nSiemens filed for exclusive rights to this technique in 2003 and obtained US\npatent US7301903B2 in 2007 with this author, an employee at the time of the\nfiling, the sole inventor. A computer program, written in C language, which\nexercises the methodology is listed at the end of this document and available\non GitHub.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.08123v4"
    },
    {
        "title": "Survey and Benchmarking of Machine Learning Accelerators",
        "authors": [
            "Albert Reuther",
            "Peter Michaleas",
            "Michael Jones",
            "Vijay Gadepally",
            "Siddharth Samsi",
            "Jeremy Kepner"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Advances in multicore processors and accelerators have opened the flood gates\nto greater exploration and application of machine learning techniques to a\nvariety of applications. These advances, along with breakdowns of several\ntrends including Moore's Law, have prompted an explosion of processors and\naccelerators that promise even greater computational and machine learning\ncapabilities. These processors and accelerators are coming in many forms, from\nCPUs and GPUs to ASICs, FPGAs, and dataflow accelerators. This paper surveys\nthe current state of these processors and accelerators that have been publicly\nannounced with performance and power consumption numbers. The performance and\npower values are plotted on a scatter graph and a number of dimensions and\nobservations from the trends on this plot are discussed and analyzed. For\ninstance, there are interesting trends in the plot regarding power consumption,\nnumerical precision, and inference versus training. We then select and\nbenchmark two commercially-available low size, weight, and power (SWaP)\naccelerators as these processors are the most interesting for embedded and\nmobile machine learning inference applications that are most applicable to the\nDoD and other SWaP constrained users. We determine how they actually perform\nwith real-world images and neural network models, compare those results to the\nreported performance and power consumption values and evaluate them against an\nIntel CPU that is used in some embedded applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.11348v1"
    },
    {
        "title": "Sentinel: Runtime Data Management on Heterogeneous Main MemorySystems\n  for Deep Learning",
        "authors": [
            "Jie Ren",
            "Jiaolin Luo",
            "Kai Wu",
            "Minjia Zhang",
            "Dong Li"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Software-managed heterogeneous memory (HM) provides a promising solution to\nincrease memory capacity and cost efficiency. However, to release the\nperformance potential of HM, we face a problem of data management. Given an\napplication with various execution phases and each with possibly distinct\nworking sets, we must move data between memory components of HM to optimize\nperformance. The deep neural network (DNN), as a common workload on data\ncenters, imposes great challenges on data management on HM. This workload often\nemploys a task dataflow execution model, and is featured with a large amount of\nsmall data objects and fine-grained operations (tasks). This execution model\nimposes challenges on memory profiling and efficient data migration.\n  We present Sentinel, a runtime system that automatically optimizes data\nmigration (i.e., data management) on HM to achieve performance similar to that\non the fast memory-only system with a much smaller capacity of fast memory. To\nachieve this,Sentinel exploits domain knowledge about deep learning to adopt a\ncustom approach for data management. Sentinel leverages workload repeatability\nto break the dilemma between profiling accuracy and overhead; It enables\nprofiling and data migration at the granularity of data objects (not pages), by\ncontrolling memory allocation. This method bridges the semantic gap between\noperating system and applications. By associating data objects with the DNN\ntopology, Sentinel avoids unnecessary data movement and proactively triggers\ndata movement. Using only 20% of peak memory consumption of DNN models as fast\nmemory size, Sentinel achieves the same or comparable performance (at most 8%\nperformance difference) to that of the fast memory-only system on common DNN\nmodels; Sentinel also consistently outperforms a state-of-the-art solution by\n18%.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.05182v1"
    },
    {
        "title": "Branch prediction related Optimizations for Multithreaded Processors",
        "authors": [
            "Murthy Durbhakula"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Major chip manufacturers have all introduced Multithreaded processors. These\nprocessors are used for running a variety of workloads. Efficient resource\nutilization is an important design aspect in such processors. Depending on the\nworkload, mis-speculated execution can severely impact resource utilization and\npower utilization. In general, compared to a uniprocessor, a multithreaded\nprocessor may have better tolerance towards mis-speculation. However there can\nstill be phases where even a multi-threaded processor performance may get\nimpacted by branch induced mis-speculation. In this paper I propose monitoring\nthe branch predictor behavior of various hardware threads running on the\nmulti-threaded processor and use that information as a feedback to the thread\narbiter/picker which schedules the next thread to fetch instructions from. If I\nfind that a particular thread is going through a phase where it is consistently\nmis-predicting its branches and its average branch misprediction stall is above\na specific threshold then I temporarily reduce the priority for picking that\nthread. I do a qualitative comparison of various solutions to the problem of\nresource inefficiency caused due to mis-speculated branches in multithreaded\nprocessors. This work can be extended by doing a quantitative evaluation.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.08999v1"
    },
    {
        "title": "A Data-Assisted Reliability Model for Carrier-Assisted Cold Data Storage\n  Systems",
        "authors": [
            "Suayb S. Arslan",
            "James Peng",
            "Turguy Goker"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Cold data storage systems are used to allow long term digital preservation\nfor institutions' archives. The common functionality among cold and warm/hot\ndata storage is that the data is stored on some physical medium for read-back\nat a later time. However in cold storage, write and read operations are not\nnecessarily done in the same exact geographical location. Hence, a third party\nassistance is typically utilized to bring together the medium and the drive. On\nthe other hand, the reliability modeling of such a decomposed system poses few\nchallenges that do not necessarily exist in other warm/hot storage alternatives\nsuch as fault detection and absence of the carrier, all totaling up to the data\nunavailability issues. In this paper, we propose a generalized non-homogenous\nMarkov model that encompasses the aging of the carriers in order to address the\nrequirements of today's cold data storage systems in which the data is encoded\nand spread across multiple nodes for the long-term data retention. We have\nderived useful lower/upper bounds on the overall system availability.\nFurthermore, the collected field data is used to estimate parameters of a\nWeibull distribution to accurately predict the lifetime of the carriers in an\nexample scale-out setting. In this study, we numerically demonstrate the\nsignificance of carriers' presence and the key role that their timely\nmaintenance plays on the long-term reliability and availability of the stored\ncontent.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00329v1"
    },
    {
        "title": "Graph-based Approach for Buffer-aware Timing Analysis of Heterogeneous\n  Wormhole NoCs under Bursty Traffic",
        "authors": [
            "Frederic Giroudot",
            "Ahlem Mifdaoui"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This paper addresses the problem of worst-case timing analysis of\nheterogeneous wormhole NoCs, i.e., routers with different buffer sizes and\ntransmission speeds, when consecutive packet queuing (CPQ) occurs. The latter\nmeans that there are several consecutive packets of one flow queuing in the\nnetwork. This scenario happens in the case of bursty traffic but also for\nnon-schedulable traffic. Conducting such an analysis is known to be a\nchallenging issue due to the sophisticated congestion patterns when enabling\nbackpressure mechanisms. We tackle this problem through extending the\napplicability domain of our previous work for computing maximum delay bounds\nusing Network Calculus, called Buffer-aware worst-case Timing Analysis (BATA).\nWe propose a new Graph-based approach to improve the analysis of indirect\nblocking due to backpressure, while capturing the CPQ effect and keeping the\ninformation about dependencies between flows. Furthermore, the introduced\napproach improves the computation of indirect-blocking delay bounds in terms of\ncomplexity and ensures the safety of these bounds even for non-schedulable\ntraffic. We provide further insights into the tightness and complexity issues\nof worst-case delay bounds yielded by the extended BATA with the Graph-based\napproach, denoted G-BATA. Our assessments show that the complexity has\ndecreased by up to 100 times while offering an average tightness ratio of 71%,\nwith reference to the basic BATA. Finally, we evaluate the yielded improvements\nwith G-BATA for a realistic use case against a recent state-of-the-art\napproach. This evaluation shows the applicability of G-BATA under more general\nassumptions and the impact of such a feature on the tightness and computation\ntime.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.02430v1"
    },
    {
        "title": "nanoBench: A Low-Overhead Tool for Running Microbenchmarks on x86\n  Systems",
        "authors": [
            "Andreas Abel",
            "Jan Reineke"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We present nanoBench, a tool for evaluating small microbenchmarks using\nhardware performance counters on Intel and AMD x86 systems. Most existing tools\nand libraries are intended to either benchmark entire programs, or program\nsegments in the context of their execution within a larger program. In\ncontrast, nanoBench is specifically designed to evaluate small, isolated pieces\nof code. Such code is common in microbenchmark-based hardware analysis\ntechniques.\n  Unlike previous tools, nanoBench can execute microbenchmarks directly in\nkernel space. This allows to benchmark privileged instructions, and it enables\nmore accurate measurements. The reading of the performance counters is\nimplemented with minimal overhead avoiding functions calls and branches. As a\nconsequence, nanoBench is precise enough to measure individual memory accesses.\n  We illustrate the utility of nanoBench at the hand of two case studies.\nFirst, we briefly discuss how nanoBench has been used to determine the latency,\nthroughput, and port usage of more than 13,000 instruction variants on recent\nx86 processors. Second, we show how to generate microbenchmarks to precisely\ncharacterize the cache architectures of eleven Intel Core microarchitectures.\nThis includes the most comprehensive analysis of the employed cache replacement\npolicies to date.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.03282v2"
    },
    {
        "title": "Stochastic Automata Network for Performance Evaluation of Heterogeneous\n  SoC Communication",
        "authors": [
            "Ulhas Deshmukh",
            "Vineet Sahula"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  To meet ever increasing demand for performance of emerging System-on-Chip\n(SoC) applications, designer employ techniques for concurrent communication\nbetween components. Hence communication architecture becomes complex and major\nperformance bottleneck. An early performance evaluation of communication\narchitecture is the key to reduce design time, time-to-market and consequently\ncost of the system. Moreover, it helps to optimize system performance by\nselecting appropriate communication architecture. However, performance model of\nconcurrent communication is complex to describe and hard to solve. In this\npaper, we propose methodology for performance evaluation of bus based\ncommunication architectures, modeling for which is based on modular Stochastic\nAutomata Network (SAN). We employ Generalized Semi Markov Process (GSMP) model\nfor each module of the SAN that emulates dynamic behavior of a Processing\nElement (PE) of an SoC architecture. The proposed modeling approach provides an\nearly estimation of performance parameters viz. memory bandwidth, average queue\nlength at memory and average waiting time seen by a processing element; while\nwe provide parameters viz. number of processing elements, the mean computation\ntime of processing elements and the first and second moments of connection time\nbetween processing elements and memories, as input to the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.05503v1"
    },
    {
        "title": "Performance Analysis of Modified SRPT in Multiple-Processor Multitask\n  Scheduling",
        "authors": [
            "Wenxin Li",
            "Ness Shroff"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  In this paper we study the multiple-processor multitask scheduling problem in\nboth deterministic and stochastic models, where each job have several tasks and\nis complete only when all its tasks are finished. We consider and analyze\nModified Shortest Remaining Processing Time (M-SRPT) scheduling algorithm, a\nsimple modification of SRPT, which always schedules jobs according to SRPT\nwhenever possible, while processes tasks in an arbitrary order. The M-SRPT\nalgorithm is proved to achieve a competitive ratio of $\\Theta(\\log \\alpha\n+\\beta)$ for minimizing response time, where $\\alpha$ denotes the ratio between\nmaximum job workload and minimum job workload, $\\beta$ represents the ratio\nbetween maximum non-preemptive task workload and minimum job workload. In\naddition, the competitive ratio achieved is shown to be optimal (up to a\nconstant factor), when there are constant number of machines. We further\nconsider the problem under Poisson arrival and general workload distribution\n(\\ie, M/GI/$N$ system), and show that M-SRPT achieves asymptotic optimal mean\nresponse time when the traffic intensity $\\rho$ approaches $1$, if job size\ndistribution has finite support. Beyond finite job workload, the asymptotic\noptimality of M-SRPT also holds for infinite job size distributions with\ncertain probabilistic assumptions, for example, M/M/$N$ system with finite task\nworkload. As a special case, we show that M-SRPT is asymptotic optimal in\nM/M/$1$ model, in which the task size distribution is allowed to have infinite\nsupport.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.06632v7"
    },
    {
        "title": "Scalable Load Balancing in the Presence of Heterogeneous Servers",
        "authors": [
            "Kristen Gardner",
            "Jazeem Abdul Jaleel",
            "Alexander Wickeham",
            "Sherwin Doroudi"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Heterogeneity is becoming increasingly ubiquitous in modern large-scale\ncomputer systems. Developing good load balancing policies for systems whose\nresources have varying speeds is crucial in achieving low response times.\nIndeed, how best to dispatch jobs to servers is a classical and well-studied\nproblem in the queueing literature. Yet the bulk of existing work on\nlarge-scale systems assumes homogeneous servers; unfortunately, policies that\nperform well in the homogeneous setting can cause unacceptably poor\nperformance---or even instability---in heterogeneous systems.\n  We adapt the \"power-of-d\" versions of both the Join-the-Idle-Queue and\nJoin-the-Shortest-Queue policies to design two corresponding families of\nheterogeneity-aware dispatching policies, each of which is parameterized by a\npair of routing probabilities. Unlike their heterogeneity-unaware counterparts,\nour policies use server speed information both when choosing which servers to\nquery and when probabilistically deciding where (among the queried servers) to\ndispatch jobs. Both of our policy families are analytically tractable: our mean\nresponse time and queue length distribution analyses are exact as the number of\nservers approaches infinity, under standard assumptions. Furthermore, our\npolicy families achieve maximal stability and outperform well-known dispatching\nrules---including heterogeneity-aware policies such as\nShortest-Expected-Delay---with respect to mean response time.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.13987v1"
    },
    {
        "title": "Performance Measurements within Asynchronous Task-based Runtime Systems:\n  A Double White Dwarf Merger as an Application",
        "authors": [
            "Patrick Diehl",
            "Dominic Marcello",
            "Parsa Amini",
            "Hartmut Kaiser",
            "Sagiv Shiber",
            "Geoffrey C. Clayton",
            "Juhan Frank",
            "Gregor Daiß",
            "Dirk Pflüger",
            "David Eder",
            "Alice Koniges",
            "Kevin Huck"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Analyzing performance within asynchronous many-task-based runtime systems is\nchallenging because millions of tasks are launched concurrently. Especially for\nlong-term runs the amount of data collected becomes overwhelming. We study HPX\nand its performance-counter framework and APEX to collect performance data and\nenergy consumption. We added HPX application-specific performance counters to\nthe Octo-Tiger full 3D AMR astrophysics application. This enables the combined\nvisualization of physical and performance data to highlight bottlenecks with\nrespect to different solvers. We examine the overhead introduced by these\nmeasurements, which is around 1%, with respect to the overall application\nruntime. We perform a convergence study for four different levels of refinement\nand analyze the application's performance with respect to adaptive grid\nrefinement. The measurements' overheads are small, enabling the combined use of\nperformance data and physical properties with the goal of improving the code's\nperformance. All of these measurements were obtained on NERSC's Cori, Louisiana\nOptical Network Infrastructure's QueenBee2, and Indiana University's Big Red 3.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.00223v4"
    },
    {
        "title": "DV-DVFS: Merging Data Variety and DVFS Technique to Manage the Energy\n  Consumption of Big Data Processing",
        "authors": [
            "Hossein Ahmadvand",
            "Fouzhan Foroutan",
            "Mahmood Fathy"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Data variety is one of the most important features of Big Data. Data variety\nis the result of aggregating data from multiple sources and uneven distribution\nof data. This feature of Big Data causes high variation in the consumption of\nprocessing resources such as CPU consumption. This issue has been overlooked in\nprevious works. To overcome the mentioned problem, in the present work, we used\nDynamic Voltage and Frequency Scaling (DVFS) to reduce the energy consumption\nof computation. To this goal, we consider two types of deadlines as our\nconstraint. Before applying the DVFS technique to computer nodes, we estimate\nthe processing time and the frequency needed to meet the deadline. In the\nevaluation phase, we have used a set of data sets and applications. The\nexperimental results show that our proposed approach surpasses the other\nscenarios in processing real datasets. Based on the experimental results in\nthis paper, DV-DVFS can achieve up to 15% improvement in energy consumption.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.03751v1"
    },
    {
        "title": "NumaPerf: Predictive and Full NUMA Profiling",
        "authors": [
            "Xin Zhao",
            "Jin Zhou",
            "Hui Guan",
            "Wei Wang",
            "Xu Liu",
            "Tongping Liu"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Parallel applications are extremely challenging to achieve the optimal\nperformance on the NUMA architecture, which necessitates the assistance of\nprofiling tools. However, existing NUMA-profiling tools share some similar\nshortcomings, such as portability, effectiveness, and helpfulness issues. This\npaper proposes a novel profiling tool - NumaPerf - that overcomes these issues.\nNumaPerf aims to identify potential performance issues for any NUMA\narchitecture, instead of only on the current hardware. To achieve this,\nNumaPerf focuses on memory sharing patterns between threads, instead of real\nremote accesses. NumaPerf further detects potential thread migrations and load\nimbalance issues that could significantly affect the performance but are\nomitted by existing profilers. NumaPerf also separates cache coherence issues\nthat may require different fix strategies. Based on our extensive evaluation,\nNumaPerf is able to identify more performance issues than any existing tool,\nwhile fixing these bugs leads to up to 5.94x performance speedup.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.05204v1"
    },
    {
        "title": "Performance Optimizations of Recursive Electronic Structure Solvers\n  targeting Multi-Core Architectures (LA-UR-20-26665)",
        "authors": [
            "Adetokunbo A. Adedoyin",
            "Christian F. A. Negre",
            "Jamaludin Mohd-Yusof",
            "Nicolas Bock",
            "Daniel Osei-Kuffuor",
            "Jean-Luc Fattebert",
            "Michael E. Wall",
            "Anders M. N. Niklasson",
            "Susan M. Mniszewski"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  As we rapidly approach the frontiers of ultra large computing resources,\nsoftware optimization is becoming of paramount interest to scientific\napplication developers interested in efficiently leveraging all available\non-Node computing capabilities and thereby improving a requisite science per\nwatt metric. The scientific application of interest here is the Basic Math\nLibrary (BML) that provides a singular interface for linear algebra operation\nfrequently used in the Quantum Molecular Dynamics (QMD) community. The\nprovisioning of a singular interface indicates the presence of an abstraction\nlayer which in-turn suggests commonalities in the code-base and therefore any\noptimization or tuning introduced in the core of code-base has the ability to\npositively affect the performance of the aforementioned library as a whole.\nWith that in mind, we proceed with this investigation by performing a survey of\nthe entirety of the BML code-base, and extract, in form of micro-kernels,\ncommon snippets of code. We introduce several optimization strategies into\nthese micro-kernels including 1.) Strength Reduction 2.) Memory Alignment for\nlarge arrays 3.) Non Uniform Memory Access (NUMA) aware allocations to enforce\ndata locality and 4.) appropriate thread affinity and bindings to enhance the\noverall multi-threaded performance. After introducing these optimizations, we\nbenchmark the micro-kernels and compare the run-time before and after\noptimization for several target architectures. Finally we use the results as a\nguide to propagating the optimization strategies into the BML code-base. As a\ndemonstration, herein, we test the efficacy of these optimization strategies by\ncomparing the benchmark and optimized versions of the code.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.08505v1"
    },
    {
        "title": "HPC AI500: Representative, Repeatable and Simple HPC AI Benchmarking",
        "authors": [
            "Zihan Jiang",
            "Wanling Gao",
            "Fei Tang",
            "Xingwang Xiong",
            "Lei Wang",
            "Chuanxin Lan",
            "Chunjie Luo",
            "Hongxiao Li",
            "Jianfeng Zhan"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Recent years witness a trend of applying large-scale distributed deep\nlearning algorithms (HPC AI) in both business and scientific computing areas,\nwhose goal is to speed up the training time to achieve a state-of-the-art\nquality. The HPC AI benchmarks accelerate the process. Unfortunately,\nbenchmarking HPC AI systems at scale raises serious challenges. This paper\npresents a representative, repeatable and simple HPC AI benchmarking\nmethodology. Among the seventeen AI workloads of AIBench Training -- by far the\nmost comprehensive AI Training benchmarks suite -- we choose two representative\nand repeatable AI workloads. The selected HPC AI benchmarks include both\nbusiness and scientific computing: Image Classification and Extreme Weather\nAnalytics. To rank HPC AI systems, we present a new metric named Valid FLOPS,\nemphasizing both throughput performance and a target quality. The\nspecification, source code, datasets, and HPC AI500 ranking numbers are\npublicly available from \\url{https://www.benchcouncil.org/HPCAI500/}.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12848v1"
    },
    {
        "title": "ESPBench: The Enterprise Stream Processing Benchmark",
        "authors": [
            "Guenter Hesse",
            "Christoph Matthies",
            "Michael Perscheid",
            "Matthias Uflacker",
            "Hasso Plattner"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Growing data volumes and velocities in fields such as Industry 4.0 or the\nInternet of Things have led to the increased popularity of data stream\nprocessing systems. Enterprises can leverage these developments by enriching\ntheir core business data and analyses with up-to-date streaming data. Comparing\nstreaming architectures for these complex use cases is challenging, as existing\nbenchmarks do not cover them. ESPBench is a new enterprise stream processing\nbenchmark that fills this gap. We present its architecture, the benchmarking\nprocess, and the query workload. We employ ESPBench on three state-of-the-art\nstream processing systems, Apache Spark, Apache Flink, and Hazelcast Jet, using\nprovided query implementations developed with Apache Beam. Our results\nhighlight the need for the provided ESPBench toolkit that supports benchmark\nexecution, as it enables query result validation and objective latency\nmeasures.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.06775v1"
    },
    {
        "title": "Accelerating Sparse Approximate Matrix Multiplication on GPUs",
        "authors": [
            "Xiaoyan Liu",
            "Yi Liu",
            "Ming Dun",
            "Bohong Yin",
            "Hailong Yang",
            "Zhongzhi Luan",
            "Depei Qian"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Although the matrix multiplication plays a vital role in computational linear\nalgebra, there are few efficient solutions for matrix multiplication of the\nnear-sparse matrices. The Sparse Approximate Matrix Multiply (SpAMM) is one of\nthe algorithms to fill the performance gap neglected by traditional\noptimizations for dense/sparse matrix multiplication. However, existing SpAMM\nalgorithms fail to exploit the performance potential of GPUs for acceleration.\nIn this paper, we present cuSpAMM, the first parallel SpAMM algorithm optimized\nfor multiple GPUs. Several performance optimizations have been proposed,\nincluding algorithm re-design to adapt to the thread parallelism, blocking\nstrategies for memory access optimization, and the acceleration with the tensor\ncore. In addition, we scale cuSpAMM to run on multiple GPUs with an effective\nload balance scheme. We evaluate cuSpAMM on both synthesized and real-world\ndatasets on multiple GPUs. The experiment results show that cuSpAMM achieves\nsignificant performance speedup compared to vendor optimized cuBLAS and\ncuSPARSE libraries.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.13042v1"
    },
    {
        "title": "Early Performance Prediction of Web Services",
        "authors": [
            "Ch Ram Mohan Reddy",
            "D. Evangelin Geetha",
            "K. G. Srinivasa",
            "T. V. Suresh Kumar",
            "K. Rajani Kanth"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Web Service is an interface which implements business logic. Performance is\nan important quality aspect of Web services because of their distributed\nnature. Predicting the performance of web services during early stages of\nsoftware development is significant. In this paper we model web service using\nUnified Modeling Language, Use Case Diagram, Sequence Diagram, Deployment\nDiagram. We obtain the Performance metrics by simulating the web services model\nusing a simulation tool Simulation of Multi-Tier Queuing Architecture. We have\nidentified the bottle neck resources.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.2034v1"
    },
    {
        "title": "Optimizing the Performance of Streaming Numerical Kernels on the IBM\n  Blue Gene/P PowerPC 450 Processor",
        "authors": [
            "Tareq M. Malas",
            "Aron J. Ahmadia",
            "Jed Brown",
            "John A. Gunnels",
            "David E. Keyes"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Several emerging petascale architectures use energy-efficient processors with\nvectorized computational units and in-order thread processing. On these\narchitectures the sustained performance of streaming numerical kernels,\nubiquitous in the solution of partial differential equations, represents a\nchallenge despite the regularity of memory access. Sophisticated optimization\ntechniques are required to fully utilize the Central Processing Unit (CPU).\n  We propose a new method for constructing streaming numerical kernels using a\nhigh-level assembly synthesis and optimization framework. We describe an\nimplementation of this method in Python targeting the IBM Blue Gene/P\nsupercomputer's PowerPC 450 core. This paper details the high-level design,\nconstruction, simulation, verification, and analysis of these kernels utilizing\na subset of the CPU's instruction set.\n  We demonstrate the effectiveness of our approach by implementing several\nthree-dimensional stencil kernels over a variety of cached memory scenarios and\nanalyzing the mechanically scheduled variants, including a 27-point stencil\nachieving a 1.7x speedup over the best previously published results.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.3496v1"
    },
    {
        "title": "Power Aware Wireless File Downloading: A Constrained Restless Bandit\n  Approach",
        "authors": [
            "Xiaohan Wei",
            "Michael J. Neely"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  This paper treats power-aware throughput maximization in a multi-user file\ndownloading system. Each user can receive a new file only after its previous\nfile is finished. The file state processes for each user act as coupled Markov\nchains that form a generalized restless bandit system. First, an optimal\nalgorithm is derived for the case of one user. The algorithm maximizes\nthroughput subject to an average power constraint. Next, the one-user algorithm\nis extended to a low complexity heuristic for the multi-user problem. The\nheuristic uses a simple online index policy and its effectiveness is shown via\nsimulation. For simple 3-user cases where the optimal solution can be computed\noffline, the heuristic is shown to be near-optimal for a wide range of\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.3824v1"
    },
    {
        "title": "Asymptotic and Numerical Analysis of Multiserver Retrial Queue with\n  Guard Channel for Cellular Networks",
        "authors": [
            "Kazuki Kajiwara",
            "Tuan Phung-Duc"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  This paper considers a retrial queueing model for a base station in cellular\nnetworks where fresh calls and handover calls are available. Fresh calls are\ninitiated from the cell of the base station. On the other hand, a handover call\nhas been connecting to a base station and moves to another one. In order to\nkeep the continuation of the communication, it is desired that an available\nchannel in the new base station is immediately assigned to the handover call.\nTo this end, a channel is reserved as the guard channel for handover calls in\nbase stations. Blocked fresh and handover calls join a virtual orbit and repeat\ntheir attempts in a later time. We assume that a base station can recognize\nretrial calls and give them the same priority as that of handover calls. We\nmodel a base station by a multiserver retrial queue with priority customers for\nwhich a level-dependent QBD process is formulated. We obtain Taylor series\nexpansion for the nonzero elements of the rate matrices of the level-dependent\nQBD. Using the expansion results, we obtain an asymptotic upper bound for the\njoint stationary distribution of the number of busy channels and that of\ncustomers in the orbit. Furthermore, we derive an efficient numerical algorithm\nto calculate the joint stationary distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1405.1916v1"
    },
    {
        "title": "Optimizing Performance of Continuous-Time Stochastic Systems using\n  Timeout Synthesis",
        "authors": [
            "Tomáš Brázdil",
            "Ľuboš Korenčiak",
            "Jan Krčál",
            "Petr Novotný",
            "Vojtěch Řehák"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  We consider parametric version of fixed-delay continuous-time Markov chains\n(or equivalently deterministic and stochastic Petri nets, DSPN) where\nfixed-delay transitions are specified by parameters, rather than concrete\nvalues. Our goal is to synthesize values of these parameters that, for a given\ncost function, minimise expected total cost incurred before reaching a given\nset of target states. We show that under mild assumptions, optimal values of\nparameters can be effectively approximated using translation to a Markov\ndecision process (MDP) whose actions correspond to discretized values of these\nparameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.4777v4"
    },
    {
        "title": "Synthetic Generation of Solar States for Smart Grid: A Multiple Segment\n  Markov Chain Apptoach",
        "authors": [
            "Wayes Tushar",
            "Shisheng Huang",
            "Chau Yuen",
            " Jian",
            " Zhang",
            "David B. Smith"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  The use of photovoltaic (PV) sources is becoming very popular in smart grid\nfor their ecological benefits, with higher scalability and utilization for\nlocal generation and delivery. PV can also potentially avoid the energy losses\nthat are normally associated with long-range grid distribution. The increased\npenetration of solar panels, however, has introduced a need for solar energy\nmodels that are capable of producing realistic synthetic data with small error\nmargins. Such models, for instance, can be used to design the appropriate size\nof energy storage devices or to determine the maximum charging rate of a\nPV-powered electric vehicle (EV) charging station. In this regard, this paper\nproposes a stochastic model for solar generation using a Markov chain approach.\nBased on real data, it is first shown that the solar states are\ninter-dependent, and thus suitable for modeling using a Markov model. Then, the\nprobabilities of transition between states are shown to be heterogeneous over\ndifferent time segments. A model is proposed that captures the inter temporal\ndependency of solar irradiance through segmentation of the Markov chain across\ndifferent times of the day. In the studied model, different state transition\nmatrices are constructed for different time segments, which the proposed\nalgorithm then uses to generate the solar states for different times of the\nday. Numerical examples are provided to show the effectiveness of the proposed\nsynthetic generator.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.4879v1"
    },
    {
        "title": "Towards energy efficiency and maximum computational intensity for\n  stencil algorithms using wavefront diamond temporal blocking",
        "authors": [
            "Tareq Malas",
            "Georg Hager",
            "Hatem Ltaief",
            "David Keyes"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  We study the impact of tunable parameters on computational intensity (i.e.,\ninverse code balance) and energy consumption of multicore-optimized wavefront\ndiamond temporal blocking (MWD) applied to different stencil-based update\nschemes. MWD combines the concepts of diamond tiling and multicore-aware\nwavefront blocking in order to achieve lower cache size requirements than\nstandard single-core wavefront temporal blocking. We analyze the impact of the\ncache block size on the theoretical and observed code balance, introduce loop\ntiling in the leading dimension to widen the range of applicable diamond sizes,\nand show performance results on a contemporary Intel CPU. The impact of code\nbalance on power dissipation on the CPU and in the DRAM is investigated and\nshows that DRAM power is a decisive factor for energy consumption, which is\nstrongly influenced by the code balance. Furthermore we show that highest\nperformance does not necessarily lead to lowest energy even if the clock speed\nis fixed.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.5561v1"
    },
    {
        "title": "A Roofline Visualization Framework",
        "authors": [
            "Wyatt Spear",
            "Boyana Norris"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  The Roofline Model and its derivatives provide an intuitive representation of\nthe best achievable performance on a given architecture. The Roofline Toolkit\nproject is a collaboration among researchers at Argonne National Laboratory,\nLawrence Berkeley National Laboratory, and the University of Oregon and\nconsists of three main parts: hardware characterization, software\ncharacterization, and data manipulation and visualization interface. These\ncomponents address the different aspects of performance data acquisition and\nmanipulation required for performance analysis, modeling and optimization of\ncodes on existing and emerging architectures. In this paper we introduce an\ninitial implementation of the third component, a system for visualizing\nroofline charts and managing roofline performance analysis data. We discuss the\nimplementation and rationale for the integration of the roofline visualization\nsystem into the Eclipse IDE. An overview of our continuing efforts and goals in\nthe development of this project is provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.06223v1"
    },
    {
        "title": "FastCap: An Efficient and Fair Algorithm for Power Capping in Many-Core\n  Systems",
        "authors": [
            "Yanpei Liu",
            "Guilherme Cox",
            "Qingyuan Deng",
            "Stark C. Draper",
            "Ricardo Bianchini"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Future servers will incorporate many active lowpower modes for different\nsystem components, such as cores and memory. Though these modes provide\nflexibility for power management via Dynamic Voltage and Frequency Scaling\n(DVFS), they must be operated in a coordinated manner. Such coordinated control\ncreates a combinatorial space of possible power mode configurations. Given the\nrapid growth of the number of cores, it is becoming increasingly challenging to\nquickly select the configuration that maximizes the performance under a given\npower budget. Prior power capping techniques do not scale well to large numbers\nof cores, and none of those works has considered memory DVFS. In this paper, we\npresent FastCap, our optimization approach for system-wide power capping, using\nboth CPU and memory DVFS. Based on a queuing model, FastCap formulates power\ncapping as a non-linear optimization problem where we seek to maximize the\nsystem performance under a power budget, while promoting fairness across\napplications. Our FastCap algorithm solves the optimization online and\nefficiently (low complexity on the number of cores), using a small set of\nperformance counters as input. To evaluate FastCap, we simulate it for a\nmany-core server running different types of workloads. Our results show that\nFastCap caps power draw accurately, while producing better application\nperformance and fairness than many existing CPU power capping methods (even\nafter they are extended to use of memory DVFS as well).\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01313v1"
    },
    {
        "title": "Performance Assessment of WhatsApp and IMO on Android Operating System\n  (Lollipop and KitKat) during VoIP calls using 3G or WiFi",
        "authors": [
            "R. C. de Oliveira",
            "H. M. de Oliveira",
            "R. A. Ramalho",
            "L. P. S. Viana"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  This paper assesses the performance of mobile messaging and VoIP connections.\nWe investigate the CPU usage of WhatsApp and IMO under different scenarios.\nThis analysis also enabled a comparison of the performance of these\napplications on two Android operating system (OS) versions: KitKat or Lollipop.\nTwo models of smartphones were considered, viz. Galaxy Note 4 and Galaxy S4.\nThe applications behavior was statistically investigated for both sending and\nreceiving VoIP calls. Connections have been examined over 3G and WiFi. The\nhandset model plays a decisive role in CPU usage of the application. t-tests\nshowed that IMO has a better performance that WhatsApp whatever be the Android\nat a significance level 1%, on Galaxy Note 4. In contrast, WhatsApp requires\nless CPU than IMO on Galaxy S4 whatever be the OS and access (3G/WiFi). Galaxy\nNote 4 using WiFi always outperformed S4 in terms of processing efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01316v2"
    },
    {
        "title": "Age-of-Information in the Presence of Error",
        "authors": [
            "Kun Chen",
            "Longbo Huang"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We consider the peak age-of-information (PAoI) in an M/M/1 queueing system\nwith packet delivery error, i.e., update packets can get lost during\ntransmissions to their destination. We focus on two types of policies, one is\nto adopt Last-Come-First-Served (LCFS) scheduling, and the other is to utilize\nretransmissions, i.e., keep transmitting the most recent packet. Both policies\ncan effectively avoid the queueing delay of a busy channel and ensure a small\nPAoI. Exact PAoI expressions under both policies with different error\nprobabilities are derived, including First-Come-First-Served (FCFS), LCFS with\npreemptive priority, LCFS with non-preemptive priority, Retransmission with\npreemptive priority, and Retransmission with non-preemptive priority. Numerical\nresults obtained from analysis and simulation are presented to validate our\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.00559v1"
    },
    {
        "title": "Asymptotics of Insensitive Load Balancing and Blocking Phases",
        "authors": [
            "Matthieu Jonckheere",
            "Balakrishna Prabhu"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We address the problem of giving robust performance bounds based on the study\nof the asymptotic behavior of the insensitive load balancing schemes when the\nnumber of servers and the load scales jointly. These schemes have the desirable\nproperty that the stationary distribution of the resulting stochastic network\ndepends on the distribution of job sizes only through its mean. It was shown\nthat they give good estimates of performance indicators for systems with finite\nbuffers, generalizing henceforth Erlang's formula whereas optimal policies are\nalready theoretically and computationally out of reach for networks of moderate\nsize. We study a single class of traffic acting on a symmetric set of processor\nsharing queues with finite buffers and we consider the case where the load\nscales with the number of servers. We characterize central limit theorems and\nlarge deviations, the response of symmetric systems under those schemes at\ndifferent scales and show that three amplitudes of deviations can be\nidentified. A central limit scaling takes place for a sub-critical load; for\n$\\rho=1$, the number of free servers scales like $n^{ {\\theta \\over \\theta+1}}$\n($\\theta$ being the buffer depth and $n$ being the number of servers) and is of\norder 1 for super-critical loads. This further implies the existence of\ndifferent phases for the blocking probability, Before a (refined) critical load\n$\\rho_c(n)=1-a n^{- {\\theta \\over \\theta+1}}$, the blocking is exponentially\nsmall and becomes of order $ n^{- {\\theta \\over \\theta+1}}$ at $\\rho_c(n)$.\nThis generalizes the well-known Quality and Efficiency Driven (QED) regime or\nHalfin-Whitt regime for a one-dimensional queue, and leads to a generalized\nstaffing rule for a given target blocking probability.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.02996v1"
    },
    {
        "title": "On Stability and Sojourn Time of Peer-to-Peer Queuing Systems",
        "authors": [
            "Taoyu Li",
            "Minghua Chen",
            "Tony Lee",
            "Xing Li"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Recent development of peer-to-peer (P2P) services (e.g. streaming, file\nsharing, and storage) systems introduces a new type of queue systems that\nreceive little attention before, where both job and server arrive and depart\nrandomly. Current study on these models focuses on the stability condition,\nunder exponential workload assumption. This paper extends existing result in\ntwo aspects. In the first part of the paper we relax the exponential workload\nassumption, and study the stability of systems with general workload\ndistribution. The second part of the paper focuses on the job sojourn time. An\nupper bound and a lower bound for job sojourn time are investigated. We\nevaluate tightness of the bounds by numerical analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.03068v1"
    },
    {
        "title": "Delay Bounds for Multiclass FIFO",
        "authors": [
            "Yuming Jiang",
            "Vishal Misra"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  FIFO is perhaps the simplest scheduling discipline. For single-class FIFO,\nits delay guarantee performance has been extensively studied: The well-known\nresults include a stochastic delay bound for $GI/GI/1$ by Kingman and a\ndeterministic delay bound for $D/D/1$ by Cruz. However, for multiclass FIFO,\nfew such results are available. To fill the gap, we prove delay bounds for\nmulticlass FIFO in this work, considering both deterministic and stochastic\ncases. Specifically, delay bounds are presented for multiclass D/D/1, GI/GI/1\nand G/G/1. In addition, examples are provided for several basic settings to\ndemonstrate the obtained bounds in more explicit forms, which are also compared\nwith simulation results.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.05753v2"
    },
    {
        "title": "On Pollaczek-Khinchine Formula for Peer-to-Peer Networks",
        "authors": [
            "Jian Zhang",
            "Tony T. Lee",
            "Tong Ye",
            "Weisheng Hu"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  The performance analysis of peer-to-peer (P2P) networks calls for a new kind\nof queueing model, in which jobs and service stations arrive randomly. Except\nin some simple special cases, in general, the queueing model with varying\nservice rate is mathematically intractable. Motivated by the P-K formula for\nM/G/1 queue, we developed a limiting analysis approach based on the connection\nbetween the fluctuation of service rate and the mean queue length. Considering\nthe two extreme service rates, we proved the conjecture on the lower bound and\nupper bound of mean queue length previously postulated. Furthermore, an\napproximate P-K formula to estimate the mean queue length is derived from the\nconvex combination of these two bounds and the conditional mean queue length\nunder the overload condition. We confirmed the accuracy of our approximation by\nextensive simulation studies with different system parameters. We also verified\nthat all limiting cases of the system behavior are consistent with the\npredictions of our formula.\n",
        "pdf_link": "http://arxiv.org/pdf/1605.08146v1"
    },
    {
        "title": "Performance Modeling and Prediction for Dense Linear Algebra",
        "authors": [
            "Elmar Peise"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  This dissertation introduces measurement-based performance modeling and\nprediction techniques for dense linear algebra algorithms. As a core principle,\nthese techniques avoid executions of such algorithms entirely, and instead\npredict their performance through runtime estimates for the underlying compute\nkernels. For a variety of operations, these predictions allow to quickly select\nthe fastest algorithm configurations from available alternatives. We consider\ntwo scenarios that cover a wide range of computations:\n  To predict the performance of blocked algorithms, we design\nalgorithm-independent performance models for kernel operations that are\ngenerated automatically once per platform. For various matrix operations,\ninstantaneous predictions based on such models both accurately identify the\nfastest algorithm, and select a near-optimal block size.\n  For performance predictions of BLAS-based tensor contractions, we propose\ncache-aware micro-benchmarks that take advantage of the highly regular\nstructure inherent to contraction algorithms. At merely a fraction of a\ncontraction's runtime, predictions based on such micro-benchmarks identify the\nfastest combination of tensor traversal and compute kernel.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.01341v1"
    },
    {
        "title": "Delay Comparison of Delivery and Coding Policies in Data Clusters",
        "authors": [
            "Virag Shah",
            "Anne Bouillard",
            "Francois Baccelli"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  A key function of cloud infrastructure is to store and deliver diverse files,\ne.g., scientific datasets, social network information, videos, etc. In such\nsystems, for the purpose of fast and reliable delivery, files are divided into\nchunks, replicated or erasure-coded, and disseminated across servers. It is\nneither known in general how delays scale with the size of a request nor how\ndelays compare under different policies for coding, data dissemination, and\ndelivery.\n  Motivated by these questions, we develop and explore a set of evolution\nequations as a unified model which captures the above features. These equations\nallow for both efficient simulation and mathematical analysis of several\ndelivery policies under general statistical assumptions. In particular, we\nquantify in what sense a workload aware delivery policy performs better than a\nworkload agnostic policy. Under a dynamic or stochastic setting, the sample\npath comparison of these policies does not hold in general. The comparison is\nshown to hold under the weaker increasing convex stochastic ordering, still\nstronger than the comparison of averages.\n  This result further allows us to obtain insightful computable performance\nbounds. For example, we show that in a system where files are divided into\nchunks of equal size, replicated or erasure-coded, and disseminated across\nservers at random, the job delays increase sub-logarithmically in the request\nsize for small and medium-sized files but linearly for large files.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.02384v2"
    },
    {
        "title": "Theoretical Performance Analysis of Vehicular Broadcast Communications\n  at Intersection and their Optimization",
        "authors": [
            "Tatsuaki Kimura",
            "Hiroshi Saito"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  In this paper, we propose an optimization method for the broadcast rate in\nvehicle-to-vehicle (V2V) broadcast communications at an intersection on the\nbasis of theoretical analysis. We consider a model in which locations of\nvehicles are modeled separately as queuing and running segments and derive key\nperformance metrics of V2V broadcast communications via a stochastic geometry\napproach. Since these theoretical expressions are mathematically intractable,\nwe developed closed-form approximate formulae for them. Using them, we optimize\nthe broadcast rate such that the mean number of successful receivers per unit\ntime is maximized. Because of the closed form approximation, the optimal rate\ncan be used as a guideline for a real-time control-method, which is not\nachieved through time-consuming simulations. We evaluated our method through\nnumerical examples and demonstrated the effectiveness of our method.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.09606v3"
    },
    {
        "title": "Relative Age of Information: Maintaining Freshness while Considering the\n  Most Recently Generated Information",
        "authors": [
            "George Kesidis",
            "Takis Konstantopoulos",
            "Michael Zazanis"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  A queueing system handling a sequence of message arrivals is considered where\neach message obsoletes all previous messages. The objective is to assess the\nfreshness of the latest message/information that has been successfully\ntransmitted, i.e., \"age of information\" (AoI). We study a variation of\ntraditional AoI, the \"Relative AoI\", here defined so as to account for the\npresence of newly arrived messages/information to the queue to be transmitted.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.00443v3"
    },
    {
        "title": "Sprintz: Time Series Compression for the Internet of Things",
        "authors": [
            "Davis Blalock",
            "Samuel Madden",
            "John Guttag"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Thanks to the rapid proliferation of connected devices, sensor-generated time\nseries constitute a large and growing portion of the world's data. Often, this\ndata is collected from distributed, resource-constrained devices and\ncentralized at one or more servers. A key challenge in this setup is reducing\nthe size of the transmitted data without sacrificing its quality. Lower quality\nreduces the data's utility, but smaller size enables both reduced network and\nstorage costs at the servers and reduced power consumption in sensing devices.\nA natural solution is to compress the data at the sensing devices.\nUnfortunately, existing compression algorithms either violate the memory and\nlatency constraints common for these devices or, as we show experimentally,\nperform poorly on sensor-generated time series.\n  We introduce a time series compression algorithm that achieves\nstate-of-the-art compression ratios while requiring less than 1KB of memory and\nadding virtually no latency. This method is suitable not only for low-power\ndevices collecting data, but also for servers storing and querying data; in the\nlatter context, it can decompress at over 3GB/s in a single thread, even faster\nthan many algorithms with much lower compression ratios. A key component of our\nmethod is a high-speed forecasting algorithm that can be trained online and\nsignificantly outperforms alternatives such as delta coding.\n  Extensive experiments on datasets from many domains show that these results\nhold not only for sensor data but also across a wide array of other time\nseries.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02515v1"
    },
    {
        "title": "MARS: Memory Aware Reordered Source",
        "authors": [
            "Ishwar Bhati",
            "Udit Dhawan",
            "Jayesh Gaur",
            "Sreenivas Subramoney",
            "Hong Wang"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Memory bandwidth is critical in today's high performance computing systems.\nThe bandwidth is particularly paramount for GPU workloads such as 3D Gaming,\nImaging and Perceptual Computing, GPGPU due to their data-intensive nature. As\nthe number of threads and data streams in the GPUs increases with each\ngeneration, along with a high available memory bandwidth, memory efficiency is\nalso crucial in order to achieve desired performance. In presence of multiple\nconcurrent data streams, the inherent locality in a single data stream is often\nlost as these streams are interleaved while moving through multiple levels of\nmemory system. In DRAM based main memory, the poor request locality reduces\nrow-buffer reuse resulting in underutilized and inefficient memory bandwidth.\n  In this paper we propose Memory-Aware Reordered Source (\\textit{MARS})\narchitecture to address memory inefficiency arising from highly interleaved\ndata streams. The key idea of \\textit{MARS} is that with a sufficiently large\nlookahead before the main memory, data streams can be reordered based on their\nrow-buffer address to regain the lost locality and improve memory efficiency.\nWe show that \\textit{MARS} improves achieved memory bandwidth by 11\\% for a set\nof synthetic microbenchmarks. Moreover, MARS does so without any specific\nknowledge of the memory configuration.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.03518v1"
    },
    {
        "title": "Heavy-traffic Delay Optimality in Pull-based Load Balancing Systems:\n  Necessary and Sufficient Conditions",
        "authors": [
            "Xingyu Zhou",
            "Jian Tan",
            "Ness Shroff"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In this paper, we consider a load balancing system under a general pull-based\npolicy. In particular, each arrival is randomly dispatched to one of the\nservers whose queue lengths are below a threshold, if there are any; otherwise,\nthis arrival is randomly dispatched to one of the entire set of servers. We are\ninterested in the fundamental relationship between the threshold and the delay\nperformance of the system in heavy traffic. To this end, we first establish the\nfollowing necessary condition to guarantee heavy-traffic delay optimality: the\nthreshold will grow to infinity as the exogenous arrival rate approaches the\nboundary of the capacity region (i.e., the load intensity approaches one) but\nthe growth rate should be slower than a polynomial function of the mean number\nof tasks in the system. As a special case of this result, we directly show that\nthe delay performance of the popular pull-based policy Join-Idle-Queue (JIQ)\nlies strictly between that of any heavy-traffic delay optimal policy and that\nof random routing. We further show that a sufficient condition for\nheavy-traffic delay optimality is that the threshold grows logarithmically with\nthe mean number of tasks in the system. This result directly resolves a\ngeneralized version of the conjecture by Kelly and Laws.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.06911v1"
    },
    {
        "title": "I/O Workload Management for All-Flash Datacenter Storage Systems Based\n  on Total Cost of Ownership",
        "authors": [
            "Zhengyu Yang",
            "Manu Awasthi",
            "Mrinmoy Ghosh",
            "Janki Bhimani",
            "Ningfang Mi"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Recently, the capital expenditure of flash-based Solid State Driver (SSDs)\nkeeps declining and the storage capacity of SSDs keeps increasing. As a result,\nall-flash storage systems have started to become more economically viable for\nlarge shared storage installations in datacenters, where metrics like Total\nCost of Ownership (TCO) are of paramount importance. On the other hand, flash\ndevices suffer from write amplification, which, if unaccounted, can\nsubstantially increase the TCO of a storage system. In this paper, we first\ndevelop a TCO model for datacenter all-flash storage systems, and then plug a\nWrite Amplification model (WAF) of NVMe SSDs we build based on empirical data\ninto this TCO model. Our new WAF model accounts for workload characteristics\nlike write rate and percentage of sequential writes. Furthermore, using both\nthe TCO and WAF models as the optimization criterion, we design new flash\nresource management schemes (MINTCO) to guide datacenter managers to make\nworkload allocation decisions under the consideration of TCO for SSDs. Based on\nthat, we also develop MINTCO-RAID to support RAID SSDs and MINTCO-OFFLINE to\noptimize the offline workload-disk deployment problem during the initialization\nphase. Experimental results show that MINTCO can reduce the TCO and keep\nrelatively high throughput and space utilization of the entire datacenter\nstorage resources.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.05928v3"
    },
    {
        "title": "FFT Convolutions are Faster than Winograd on Modern CPUs, Here is Why",
        "authors": [
            "Aleksandar Zlateski",
            "Zhen Jia",
            "Kai Li",
            "Fredo Durand"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Winograd-based convolution has quickly gained traction as a preferred\napproach to implement convolutional neural networks (ConvNet) on various\nhardware platforms because it requires fewer floating point operations than\nFFT-based or direct convolutions.\n  This paper compares three highly optimized implementations (regular FFT--,\nGauss--FFT--, and Winograd--based convolutions) on modern multi-- and\nmany--core CPUs. Although all three implementations employed the same\noptimizations for modern CPUs, our experimental results with two popular\nConvNets (VGG and AlexNet) show that the FFT--based implementations generally\noutperform the Winograd--based approach, contrary to the popular belief.\n  To understand the results, we use a Roofline performance model to analyze the\nthree implementations in detail, by looking at each of their computation phases\nand by considering not only the number of floating point operations, but also\nthe memory bandwidth and the cache sizes. The performance analysis explains\nwhy, and under what conditions, the FFT--based implementations outperform the\nWinograd--based one, on modern CPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.07851v1"
    },
    {
        "title": "SCOPE: C3SR Systems Characterization and Benchmarking Framework",
        "authors": [
            "Carl Pearson",
            "Abdul Dakkak",
            "Cheng Li",
            "Sarah Hashash",
            "Jinjun Xiong",
            "Wen-mei Hwu"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  This report presents the design of the Scope infrastructure for extensible\nand portable benchmarking. Improvements in high- performance computing systems\nrely on coordination across different levels of system abstraction. Developing\nand defining accurate performance measurements is necessary at all levels of\nthe system hierarchy, and should be as accessible as possible to developers\nwith different backgrounds. The Scope project aims to lower the barrier to\nentry for developing performance benchmarks by providing a software\narchitecture that allows benchmarks to be developed independently, by providing\nuseful C/C++ abstractions and utilities, and by providing a Python package for\ngenerating publication-quality plots of resulting measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.08311v1"
    },
    {
        "title": "Is Your Load Generator Launching Web Requests in Bunches?",
        "authors": [
            "James F Brady"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  One problem with load test quality, almost always overlooked, is the\npotential for the load generator's user thread pool to sync up and dispatch\nqueries in bunches rather than independently from each other like real users\ninitiate their requests. A spiky launch pattern misrepresents workload flow as\nwell as yields erroneous application response time statistics. This paper\ndescribes what a real user request timing pattern looks like, illustrates how\nto identify it in the load generation environment, and exercises a free\ndownloadable tool which measures how well the load generator is mimicking the\ntiming pattern of real web user requests.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10663v4"
    },
    {
        "title": "Measuring Software Performance on Linux",
        "authors": [
            "Martin Becker",
            "Samarjit Chakraborty"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Measuring and analyzing the performance of software has reached a high\ncomplexity, caused by more advanced processor designs and the intricate\ninteraction between user programs, the operating system, and the processor's\nmicroarchitecture. In this report, we summarize our experience about how\nperformance characteristics of software should be measured when running on a\nLinux operating system and a modern processor. In particular, (1) We provide a\ngeneral overview about hardware and operating system features that may have a\nsignificant impact on timing and how they interact, (2) we identify sources of\nerrors that need to be controlled in order to obtain unbiased measurement\nresults, and (3) we propose a measurement setup for Linux to minimize errors.\nAlthough not the focus of this report, we describe the measurement process\nusing hardware performance counters, which can faithfully reflect the real\nbottlenecks on a given processor. Our experiments confirm that our measurement\nsetup has a large impact on the results. More surprisingly, however, they also\nsuggest that the setup can be negligible for certain analysis methods.\nFurthermore, we found that our setup maintains significantly better performance\nunder background load conditions, which means it can be used to improve\nsoftware in high-performance applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.01412v2"
    },
    {
        "title": "Stabilizing the virtual response time in single-server processor sharing\n  queues with slowly time-varying arrival rates",
        "authors": [
            "Yongkyu Cho",
            "Young Myoung Ko"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Motivated by the work of Whitt, who studied stabilization of the mean virtual\nwaiting time (excluding service time) in a $GI_t/GI_t/1/FCFS$ queue, this paper\ninvestigates the stabilization of the mean virtual response time in a\nsingle-server processor sharing (PS) queueing system with a time-varying\narrival rate and a service rate control (a $GI_t/GI_t/1/PS$ queue). We propose\nand compare a modified square-root (SR) control and a difference-matching (DM)\ncontrol to stabilize the mean virtual response time of a $GI_t/GI_t/1/PS$\nqueue. Extensive simulation studies with various settings of arrival processes\nand service times show that the DM control outperforms the SR control for\nheavy-traffic conditions, and that the SR control performs better for\nlight-traffic conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.01611v1"
    },
    {
        "title": "Defining Big Data Analytics Benchmarks for Next Generation\n  Supercomputers",
        "authors": [
            "Drew Schmidt",
            "Junqi Yin",
            "Michael Matheson",
            "Bronson Messer",
            "Mallikarjun Shankar"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  The design and construction of high performance computing (HPC) systems\nrelies on exhaustive performance analysis and benchmarking. Traditionally this\nactivity has been geared exclusively towards simulation scientists, who,\nunsurprisingly, have been the primary customers of HPC for decades. However,\nthere is a large and growing volume of data science work that requires these\nlarge scale resources, and as such the calls for inclusion and investments in\ndata for HPC have been increasing. So when designing a next generation HPC\nplatform, it is necessary to have HPC-amenable big data analytics benchmarks.\nIn this paper, we propose a set of big data analytics benchmarks and sample\ncodes designed for testing the capabilities of current and next generation\nsupercomputers.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.02287v1"
    },
    {
        "title": "Spatter: A Tool for Evaluating Gather / Scatter Performance",
        "authors": [
            "Patrick Lavin",
            "Jeffrey Young",
            "Jason Riedy",
            "Richard Vuduc",
            "Aaron Vose",
            "Dan Ernst"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  This paper describes a new benchmark tool, Spatter, for assessing memory\nsystem architectures in the context of a specific category of indexed accesses\nknown as gather and scatter. These types of operations are increasingly used to\nexpress sparse and irregular data access patterns, and they have widespread\nutility in many modern HPC applications including scientific simulations, data\nmining and analysis computations, and graph processing. However, many\ntraditional benchmarking tools like STREAM, STRIDE, and GUPS focus on\ncharacterizing only uniform stride or fully random accesses despite evidence\nthat modern applications use varied sets of more complex access patterns.\n  Spatter is an open-source benchmark that provides a tunable and configurable\nframework to benchmark a variety of indexed access patterns, including\nvariations of gather/scatter that are seen in HPC mini-apps evaluated in this\nwork. The design of Spatter includes tunable backends for OpenMP and CUDA, and\nexperiments show how it can be used to evaluate 1) uniform access patterns for\nCPU and GPU, 2) prefetching regimes for gather/scatter, 3) compiler\nimplementations of vectorization for gather/scatter, and 4) trace-driven \"proxy\npatterns\" that reflect the patterns found in multiple applications. The results\nfrom Spatter experiments show that GPUs typically outperform CPUs for these\noperations, and that Spatter can better represent the performance of some\ncache-dependent mini-apps than traditional STREAM bandwidth measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.03743v5"
    },
    {
        "title": "HPS: A C++11 High Performance Serialization Library",
        "authors": [
            "Junhao Li"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Data serialization is a common and crucial component in high performance\ncomputing. In this paper, I present a C++11 based serialization library for\nperformance critical systems. It provides an interface similar to Boost but up\nto 150% faster and beats several popular serialization libraries.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.04556v2"
    },
    {
        "title": "Global attraction of ODE-based mean field models with hyperexponential\n  job sizes",
        "authors": [
            "Benny Van Houdt"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Mean field modeling is a popular approach to assess the performance of large\nscale computer systems. The evolution of many mean field models is\ncharacterized by a set of ordinary differential equations that have a unique\nfixed point. In order to prove that this unique fixed point corresponds to the\nlimit of the stationary measures of the finite systems, the unique fixed point\nmust be a global attractor. While global attraction was established for various\nsystems in case of exponential job sizes, it is often unclear whether these\nproof techniques can be generalized to non-exponential job sizes. In this paper\nwe show how simple monotonicity arguments can be used to prove global\nattraction for a broad class of ordinary differential equations that capture\nthe evolution of mean field models with hyperexponential job sizes. This class\nincludes both existing as well as previously unstudied load balancing schemes\nand can be used for systems with either finite or infinite buffers. The main\nnovelty of the approach exists in using a Coxian representation for the\nhyperexponential job sizes and a partial order that is stronger than the\ncomponentwise partial order used in the exponential case.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.05239v2"
    },
    {
        "title": "Accelerating Reduction and Scan Using Tensor Core Units",
        "authors": [
            "Abdul Dakkak",
            "Cheng Li",
            "Isaac Gelado",
            "Jinjun Xiong",
            "Wen-mei Hwu"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Driven by deep learning, there has been a surge of specialized processors for\nmatrix multiplication, referred to as TensorCore Units (TCUs). These TCUs are\ncapable of performing matrix multiplications on small matrices (usually 4x4 or\n16x16) to accelerate the convolutional and recurrent neural networks in deep\nlearning workloads. In this paper we leverage NVIDIA's TCU to express both\nreduction and scan with matrix multiplication and show the benefits -- in terms\nof program simplicity, efficiency, and performance. Our algorithm exercises the\nNVIDIA TCUs which would otherwise be idle, achieves 89%-98% of peak memory copy\nbandwidth, and is orders of magnitude faster (up to 100x for reduction and 3x\nfor scan) than state-of-the-art methods for small segment sizes -- common in\nmachine learning and scientific applications. Our algorithm achieves this while\ndecreasing the power consumption by up to 22% for reduction and16%for scan.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.09736v2"
    },
    {
        "title": "Evaluation of Intel Memory Drive Technology Performance for Scientific\n  Applications",
        "authors": [
            "Vladimir Mironov",
            "Andrey Kudryavtsev",
            "Yuri Alexeev",
            "Alexander Moskovsky",
            "Igor Kulikov",
            "Igor Chernykh"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In this paper, we present benchmark data for Intel Memory Drive Technology\n(IMDT), which is a new generation of Software-defined Memory (SDM) based on\nIntel ScaleMP collaboration and using 3D XPointTM based Intel Solid-State\nDrives (SSDs) called Optane. We studied IMDT performance for synthetic\nbenchmarks, scientific kernels, and applications. We chose these benchmarks to\nrepresent different patterns for computation and accessing data on disks and\nmemory. To put performance of IMDT in comparison, we used two memory\nconfigurations: hybrid IMDT DDR4/Optane and DDR4 only systems. The performance\nwas measured as a percentage of used memory and analyzed in detail. We found\nthat for some applications DDR4/Optane hybrid configuration outperforms DDR4\nsetup by up to 20%.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.10220v1"
    },
    {
        "title": "The L-CSC cluster: Optimizing power efficiency to become the greenest\n  supercomputer in the world in the Green500 list of November 2014",
        "authors": [
            "David Rohr",
            "Gvozden Neskovic",
            "Volker Lindenstruth"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  The L-CSC (Lattice Computer for Scientific Computing) is a general purpose\ncompute cluster built with commodity hardware installed at GSI. Its main\noperational purpose is Lattice QCD (LQCD) calculations for physics simulations.\nQuantum Chromo Dynamics (QCD) is the physical theory describing the strong\nforce, one of the four known fundamental interactions in the universe. L-CSC\nleverages a multi-GPU design accommodating the huge demand of LQCD for memory\nbandwidth. In recent years, heterogeneous clusters with accelerators such as\nGPUs have become more and more powerful while supercomputers in general have\nshown enormous increases in power consumption making electricity costs and\ncooling a significant factor in the total cost of ownership. Using mainly GPUs\nfor processing, L-CSC is very power-efficient, and its architecture was\noptimized to provide the greatest possible power efficiency. This paper\npresents the cluster design as well as optimizations to improve the power\nefficiency. It examines the power measurements performed for the Green500 list\nof the most power-efficient supercomputers in the world which led to the number\n1 position as the greenest supercomputer in November 2014.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.11475v1"
    },
    {
        "title": "Resource Allocation in One-dimensional Distributed Service Networks",
        "authors": [
            "Nitish K. Panigrahy",
            "Prithwish Basu",
            "Philippe Nain",
            "Don Towsley",
            "Ananthram Swami",
            "Kevin S. Chan",
            "Kin K. Leung"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We consider assignment policies that allocate resources to users, where both\nresources and users are located on a one-dimensional line. First, we consider\nunidirectional assignment policies that allocate resources only to users\nlocated to their left. We propose the Move to Right (MTR) policy, which scans\nfrom left to right assigning nearest rightmost available resource to a user,\nand contrast it to the Unidirectional Gale-Shapley (UGS) matching policy. While\nboth these policies are optimal among all unidirectional policies, we show that\nthey are equivalent with respect to the expected distance traveled by a request\n(request distance), although MTR is fairer. Moreover, we show that when user\nand resource locations are modeled by statistical point processes, and\nresources are allowed to satisfy more than one user, the spatial system under\nunidirectional policies can be mapped into bulk service queuing systems, thus\nallowing the application of a plethora of queuing theory results that yield\nclosed form expressions. As we consider a case where different resources can\nsatisfy different numbers of users, we also generate new results for bulk\nservice queues. We also consider bidirectional policies where there are no\ndirectional restrictions on resource allocation and develop an algorithm for\ncomputing the optimal assignment which is more efficient than known algorithms\nin the literature when there are more resources than users. Finally, numerical\nevaluation of performance of unidirectional and bidirectional allocation\nschemes yields design guidelines beneficial for resource placement.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.02414v4"
    },
    {
        "title": "Three Other Models of Computer System Performance",
        "authors": [
            "Mark D. Hill"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This note argues for more use of simple models beyond Amdahl's Law:\nBottleneck Analysis, Little's Law, and a M/M/1 Queue.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.02926v1"
    },
    {
        "title": "Blind GB-PANDAS: A Blind Throughput-Optimal Load Balancing Algorithm for\n  Affinity Scheduling",
        "authors": [
            "Ali Yekkehkhany",
            "Rakesh Nagi"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Dynamic affinity load balancing of multi-type tasks on multi-skilled servers,\nwhen the service rate of each task type on each of the servers is known and can\npossibly be different from each other, is an open problem for over three\ndecades. The goal is to do task assignment on servers in a real time manner so\nthat the system becomes stable, which means that the queue lengths do not\ndiverge to infinity in steady state (throughput optimality), and the mean task\ncompletion time is minimized (delay optimality). The fluid model planning,\nMax-Weight, and c-$\\mu$-rule algorithms have theoretical guarantees on\noptimality in some aspects for the affinity problem, but they consider a\ncomplicated queueing structure and either require the task arrival rates, the\nservice rates of tasks on servers, or both. In many cases that are discussed in\nthe introduction section, both task arrival rates and service rates of\ndifferent task types on different servers are unknown. In this work, the Blind\nGB-PANDAS algorithm is proposed which is completely blind to task arrival rates\nand service rates. Blind GB-PANDAS uses an exploration-exploitation approach\nfor load balancing. We prove that Blind GB-PANDAS is throughput optimal under\narbitrary and unknown distributions for service times of different task types\non different servers and unknown task arrival rates. Blind GB-PANDAS desires to\nroute an incoming task to the server with the minimum weighted-workload, but\nsince the service rates are unknown, such routing of incoming tasks is not\nguaranteed which makes the throughput optimality analysis more complicated than\nthe case where service rates are known. Our extensive experimental results\nreveal that Blind GB-PANDAS significantly outperforms existing methods in terms\nof mean task completion time at high loads.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.04047v2"
    },
    {
        "title": "High order concentrated non-negative matrix-exponential functions",
        "authors": [
            "Gabor Horvath",
            "Illes Horvath",
            "Miklos Telek"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Highly concentrated functions play an important role in many research fields\nincluding control system analysis and physics, and they turned out to be the\nkey idea behind inverse Laplace transform methods as well.\n  This paper uses the matrix-exponential family of functions to create highly\nconcentrated functions, whose squared coefficient of variation (SCV) is very\nlow. In the field of stochastic modeling, matrix-exponential functions have\nbeen used for decades. They have many advantages: they are easy to manipulate,\nalways non-negative, and integrals involving matrix-exponential functions often\nhave closed-form solutions. For the time being there is no symbolic\nconstruction available to obtain the most concentrated matrix-exponential\nfunctions, and the numerical optimization-based approach has many pitfalls,\ntoo.\n  In this paper, we present a numerical optimization-based procedure to\nconstruct highly concentrated matrix-exponential functions. To make the\nobjective function explicit and easy to evaluate we introduce and use a new\nrepresentation called hyper-trigonometric representation. This representation\nmakes it possible to achieve very low SCV.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.08627v1"
    },
    {
        "title": "Overbooking Microservices in the Cloud",
        "authors": [
            "George Kesidis"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We consider the problem of scheduling serverless-computing instances such as\nAmazon Lambda functions, or scheduling microservices within (privately held)\nvirtual machines (VMs). Instead of a quota per tenant/customer, we assume\ndemand for Lambda functions is modulated by token-bucket mechanisms per tenant.\nSuch quotas are due to, e.g., limited resources (as in a fog/edge-cloud\ncontext) or to prevent excessive unauthorized invocation of numerous instances\nby malware. Based on an upper bound on the stationary number of active \"Lambda\nservers\" considering the execution-time distribution of Lambda functions, we\ndescribe an approach that the cloud could use to overbook Lambda functions for\nimproved utilization of IT resources. An earlier bound for a single service\ntier is extended to multiple service tiers. For the context of scheduling\nmicroservices in a private setting, the framework could be used to determine\nthe required VM resources for a token-bucket constrained workload stream.\nFinally, we note that the looser Markov inequality may be useful in settings\nwhere the job service times are dependent.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.09842v2"
    },
    {
        "title": "Approximate Solution Approach and Performability Evaluation of Large\n  Scale Beowulf Clusters",
        "authors": [
            "Yonal Kirsal",
            "Yoney Kirsal Ever"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Beowulf clusters are very popular and deployed worldwide in support of\nscientific computing, because of the high computational power and performance.\nHowever, they also pose several challenges, and yet they need to provide high\navailability. The practical large-scale Beowulf clusters result in\nunpredictable, fault-tolerant, often detrimental outcomes. Successful\ndevelopment of high performance in storing and processing huge amounts of data\nin large-scale clusters necessitates accurate quality of service (QoS)\nevaluation. This leads to develop as well as design, analytical models to\nunderstand and predict of complex system behaviour in order to ensure\navailability of large-scale systems. Exact modelling of such clusters is not\nfeasible due to the nature of the large scale nodes and the diversity of user\nrequests. An analytical model for QoS of large-scale server farms and solution\napproaches are necessary. In this paper, analytical modelling of large-scale\nBeowulf clusters is considered together with availability issues. A generic and\nflexible approximate solution approach is developed to handle large number of\nnodes for performability evaluation. The proposed analytical model and the\napproximate solution approach provide flexibility to evaluate the QoS\nmeasurements for such systems. In order to show the efficacy and the accuracy\nof the proposed approach, the results obtained from the analytical model are\nvalidated with the results obtained from the discrete event simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.08044v1"
    },
    {
        "title": "Simple Near-Optimal Scheduling for the M/G/1",
        "authors": [
            "Ziv Scully",
            "Mor Harchol-Balter",
            "Alan Scheller-Wolf"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We consider the problem of preemptively scheduling jobs to minimize mean\nresponse time of an M/G/1 queue. When we know each job's size, the shortest\nremaining processing time (SRPT) policy is optimal. Unfortunately, in many\nsettings we do not have access to each job's size. Instead, we know only the\njob size distribution. In this setting the Gittins policy is known to minimize\nmean response time, but its complex priority structure can be computationally\nintractable. A much simpler alternative to Gittins is the shortest expected\nremaining processing time (SERPT) policy. While SERPT is a natural extension of\nSRPT to unknown job sizes, it is unknown whether or not SERPT is close to\noptimal for mean response time.\n  We present a new variant of SERPT called monotonic SERPT (M-SERPT) which is\nas simple as SERPT but has provably near-optimal mean response time at all\nloads for any job size distribution. Specifically, we prove the mean response\ntime ratio between M-SERPT and Gittins is at most 3 for load $\\rho \\leq 8/9$\nand at most 5 for any load. This makes M-SERPT the only non-Gittins scheduling\npolicy known to have a constant-factor approximation ratio for mean response\ntime.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.10792v3"
    },
    {
        "title": "Automatic Throughput and Critical Path Analysis of x86 and ARM Assembly\n  Kernels",
        "authors": [
            "Jan Laukemann",
            "Julian Hammer",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Useful models of loop kernel runtimes on out-of-order architectures require\nan analysis of the in-core performance behavior of instructions and their\ndependencies. While an instruction throughput prediction sets a lower bound to\nthe kernel runtime, the critical path defines an upper bound. Such predictions\nare an essential part of analytic (i.e., white-box) performance models like the\nRoofline and Execution-Cache-Memory (ECM) models. They enable a better\nunderstanding of the performance-relevant interactions between hardware\narchitecture and loop code. The Open Source Architecture Code Analyzer (OSACA)\nis a static analysis tool for predicting the execution time of sequential\nloops. It previously supported only x86 (Intel and AMD) architectures and\nsimple, optimistic full-throughput execution. We have heavily extended OSACA to\nsupport ARM instructions and critical path prediction including the detection\nof loop-carried dependencies, which turns it into a versatile\ncross-architecture modeling tool. We show runtime predictions for code on Intel\nCascade Lake, AMD Zen, and Marvell ThunderX2 micro-architectures based on\nmachine models from available documentation and semi-automatic benchmarking.\nThe predictions are compared with actual measurements.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00214v2"
    },
    {
        "title": "Memory Centric Characterization and Analysis of SPEC CPU2017 Suite",
        "authors": [
            "Sarabjeet Singh",
            "Manu Awasthi"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In this paper we provide a comprehensive, memory-centric characterization of\nthe SPEC CPU2017 benchmark suite, using a number of mechanisms including\ndynamic binary instrumentation, measurements on native hardware using hardware\nperformance counters and OS based tools.\n  We present a number of results including working set sizes, memory capacity\nconsumption and, memory bandwidth utilization of various workloads. Our\nexperiments reveal that the SPEC CPU2017 workloads are surprisingly memory\nintensive, with approximately 50% of all dynamic instructions being memory\nintensive ones. We also show that there is a large variation in the memory\nfootprint and bandwidth utilization profiles of the entire suite, with some\nbenchmarks using as much as 16 GB of main memory and up to 2.3 GB/s of memory\nbandwidth.\n  We also perform instruction execution and distribution analysis of the suite\nand find that the average instruction count for SPEC CPU2017 workloads is an\norder of magnitude higher than SPEC CPU2006 ones. In addition, we also find\nthat FP benchmarks of the SPEC 2017 suite have higher compute requirements: on\naverage, FP workloads execute three times the number of compute operations as\ncompared to INT workloads.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.00651v1"
    },
    {
        "title": "Throughput and Delay Analysis of Slotted Aloha with Batch Service",
        "authors": [
            "Huanhuan Huang",
            "Tong Ye",
            "Tony T. Lee"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In this paper, we study the throughput and delay performances of the slotted\nAloha with batch service, which has wide applications in random access\nnetworks. Different from the classical slotted Aloha, each node in the slotted\nAloha with batch service can transmit up to M packets once it succeeds in\nchannel competition. The throughput is substantially improved because up to M\npackets jointly undertake the overhead due to contention. In an innovative\nvacation model developed in this paper, we consider each batch of data\ntransmission as a busy period of each node, and the process between two\nsuccessive busy periods as a vacation period. We then formulate the number of\narrivals during a vacation period in a renewal-type equation, which\ncharacterizes the dependency between busy periods and vacation periods. Based\non this formulation, we derive the mean waiting time of a packet and the\nbounded delay region for the slotted Aloha with batch service. Our results\nindicate the throughput and delay performances are substantially improved with\nthe increase of batch sizeM, and the bounded delay region is enlarged\naccordingly. As M goes to infinity, we find the saturated throughput can\napproach 100% of channel capacity, and the system remains stable irrespective\nof the population size and transmission probability.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.07312v1"
    },
    {
        "title": "Please come back later: Benefiting from deferrals in service systems",
        "authors": [
            "Anmol Kagrecha",
            "Jayakrishnan Nair"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The performance evaluation of loss service systems, where customers who\ncannot be served upon arrival get dropped, has a long history going back to the\nclassical Erlang B model. In this paper, we consider the performance benefits\narising from the possibility of deferring customers who cannot be served upon\narrival. Specifically, we consider an Erlang B type loss system where the\nsystem operator can, subject to certain constraints, ask a customer arriving\nwhen all servers are busy, to come back at a specified time in the future. If\nthe system is still fully loaded when the deferred customer returns, she gets\ndropped for good. For such a system, we ask: How should the system operator\ndetermine the rearrival times of the deferred customers based on the state of\nthe system (which includes those customers already deferred and yet to arrive)?\nHow does one quantify the performance benefit of such a deferral policy? Our\ncontributions are as follows. We propose a simple state-dependent policy for\ndetermining the rearrival times of deferred customers. For this policy, we\ncharacterize the long run fraction of customers dropped. We also analyse a\nrelaxation where the deferral times are bounded in expectation. Via extensive\nnumerical evaluations, we demonstrate the superiority of the proposed\nstate-dependent policies over naive state-independent deferral policies.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.12894v1"
    },
    {
        "title": "High Performance Code Generation in MLIR: An Early Case Study with GEMM",
        "authors": [
            "Uday Bondhugula"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  This article is primarily meant to present an early case study on using MLIR,\na new compiler intermediate representation infrastructure, for high-performance\ncode generation. Aspects of MLIR covered in particular include memrefs, the\naffine dialect, and polyhedral utilities and pass infrastructure surrounding\nthose. This article is also aimed at showing the role compiler infrastructure\ncould play in generating code that is competitive with highly tuned manually\ndeveloped libraries, albeit in a more modular, reusable, and automatable way.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.00532v1"
    },
    {
        "title": "Achieving Zero Asymptotic Queueing Delay for Parallel Jobs",
        "authors": [
            "Wentao Weng",
            "Weina Wang"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Zero queueing delay is highly desirable in large-scale computing systems.\nExisting work has shown that it can be asymptotically achieved by using the\ncelebrated Power-of-$d$-choices (pod) policy with a probe overhead $d =\n\\omega\\left(\\frac{\\log N}{1-\\lambda}\\right)$, and it is impossible when $d =\nO\\left(\\frac{1}{1-\\lambda}\\right)$, where $N$ is the number of servers and\n$\\lambda$ is the load of the system. However, these results are based on the\nmodel where each job is an indivisible unit, which does not capture the\nparallel structure of jobs in today's predominant parallel computing paradigm.\n  This paper thus considers a model where each job consists of a batch of\nparallel tasks. Under this model, we propose a new notion of zero (asymptotic)\nqueueing delay that requires the job delay under a policy to approach the job\ndelay given by the max of its tasks' service times, i.e., the job delay\nassuming its tasks entered service right upon arrival. This notion quantifies\nthe effect of queueing on a job level for jobs consisting of multiple tasks,\nand thus deviates from the conventional zero queueing delay for single-task\njobs in the literature.\n  We show that zero queueing delay for parallel jobs can be achieved using the\nbatch-filling policy (a variant of the celebrated pod policy) with a probe\noverhead $d = \\omega\\left(\\frac{1}{(1-\\lambda)\\log k}\\right)$ in the\nsub-Halfin-Whitt heavy-traffic regime, where $k$ is the number of tasks in each\njob { and $k$ properly scales with $N$ (the number of servers)}. This result\ndemonstrates that for parallel jobs, zero queueing delay can be achieved with a\nsmaller probe overhead. We also establish an impossibility result: we show that\nzero queueing delay cannot be achieved if $d = \\exp\\left({o\\left(\\frac{\\log\nN}{\\log k}\\right)}\\right)$.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.02081v3"
    },
    {
        "title": "Offsite Autotuning Approach -- Performance Model Driven Autotuning\n  Applied to Parallel Explicit ODE Methods",
        "authors": [
            "Johannes Seiferth",
            "Matthias Korch",
            "Thomas Rauber"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Autotuning techniques are a promising approach to minimize the otherwise\ntedious manual effort of optimizing scientific applications for a specific\ntarget platform. Ideally, an autotuning approach is capable of reliably\nidentifying the most efficient implementation variant(s) for a new target\nsystem or new characteristics of the input by applying suitable program\ntransformations and analytic models. In this work, we introduce Offsite, an\noffline autotuning approach which automates this selection process at\ninstallation time by rating implementation variants based on an analytic\nperformance model without requiring time-consuming runtime experiments. From\nabstract multilevel YAML description languages, Offsite automatically derives\noptimized, platform-specific and problem-specific code of possible\nimplementation variants and applies the performance model to these\nimplementation variants.\n  We apply Offsite to parallel numerical methods for ordinary differential\nequations (ODEs). In particular, we investigate tuning a specific class of\nexplicit ODE solvers (PIRK methods) for various initial value problems (IVPs)\non shared-memory systems. Our experiments demonstrate that Offsite is able to\nreliably identify a set of the most efficient implementation variants for given\ntest configurations (ODE solver, IVP, platform) and is capable of effectively\nhandling important autotuning scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03695v1"
    },
    {
        "title": "Refined Mean Field Analysis of the Gossip Shuffle Protocol -- extended\n  version --",
        "authors": [
            "Nicolas Gast",
            "Diego Latella",
            "Mieke Massink"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Gossip protocols form the basis of many smart collective adaptive systems.\nThey are a class of fully decentralised, simple but robust protocols for the\ndistribution of information throughout large scale networks with hundreds or\nthousands of nodes. Mean field analysis methods have made it possible to\napproximate and analyse performance aspects of such large scale protocols in an\nefficient way. Taking the gossip shuffle protocol as a benchmark, we evaluate a\nrecently developed refined mean field approach. We illustrate the gain in\naccuracy this can provide for the analysis of medium size models analysing two\nkey performance measures. We also show that refined mean field analysis\nrequires special attention to correctly capture the coordination aspects of the\ngossip shuffle protocol.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.07519v1"
    },
    {
        "title": "HPC AI500: The Methodology, Tools, Roofline Performance Models, and\n  Metrics for Benchmarking HPC AI Systems",
        "authors": [
            "Zihan Jiang",
            "Lei Wang",
            "Xingwang Xiong",
            "Wanling Gao",
            "Chunjie Luo",
            "Fei Tang",
            "Chuanxin Lan",
            "Hongxiao Li",
            "Jianfeng Zhan"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The recent years witness a trend of applying large-scale distributed deep\nlearning in both business and scientific computing areas, whose goal is to\nspeed up the training time to achieve a state-of-the-art quality. The HPC\ncommunity feels a great interest in building the HPC AI systems that are\ndedicated to running those workloads. The HPC AI benchmarks accelerate the\nprocess. Unfortunately, benchmarking HPC AI systems at scale raises serious\nchallenges. None of previous HPC AI benchmarks achieve the goal of being\nequivalent, relevant, representative, affordable, and repeatable. This paper\npresents a comprehensive methodology, tools, Roofline performance models, and\ninnovative metrics for benchmarking, optimizing, and ranking HPC AI systems,\nwhich we call HPC AI500 V2.0. We abstract the HPC AI system into nine\nindependent layers, and present explicit benchmarking rules and procedures to\nassure equivalence of each layer, repeatability, and replicability. On the\nbasis of AIBench -- by far the most comprehensive AI benchmarks suite, we\npresent and build two HPC AI benchmarks from both business and scientific\ncomputing: Image Classification, and Extreme Weather Analytics, achieving both\nrepresentativeness and affordability. To rank the performance and\nenergy-efficiency of HPC AI systems, we propose Valid FLOPS, and Valid FLOPS\nper watt, which impose a penalty on failing to achieve the target quality. We\npropose using convolution and GEMM -- the two most intensively-used kernel\nfunctions to measure the upper bound performance of the HPC AI systems, and\npresent HPC AI roofline models for guiding performance optimizations. The\nevaluations show our methodology, benchmarks, performance models, and metrics\ncan measure, optimize, and rank the HPC AI systems in a scalable, simple, and\naffordable way. HPC AI500 V2.0 are publicly available from\nhttp://www.benchcouncil.org/benchhub/hpc-ai500-benchmark.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.00279v1"
    },
    {
        "title": "COCOA: Cold Start Aware Capacity Planning for Function-as-a-Service\n  Platforms",
        "authors": [
            "Alim Ul Gias",
            "Giuliano Casale"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Function-as-a-Service (FaaS) is increasingly popular in the software industry\ndue to the implied cost-savings in event-driven workloads and its synergy with\nDevOps. To size an on-premise FaaS platform, it is important to estimate the\nrequired CPU and memory capacity to serve the expected loads. Given the\nservice-level agreements, it is however challenging to take the cold start\nissue into account during the sizing process. We have investigated the\nsimilarity of this problem with the hit rate improvement problem in TTL caches\nand concluded that solutions for TTL cache, although potentially applicable,\nlead to over-provisioning in FaaS. Thus, we propose a novel approach, COCOA, to\nsolve this issue. COCOA uses a queueing-based approach to assess the effect of\ncold starts on FaaS response times. It also considers different memory\nconsumption values depending on whether the function is idle or in execution.\nUsing an event-driven FaaS simulator, FaasSim, we have developed, we show that\nCOCOA can reduce over-provisioning by over 70% in some workloads, while\nsatisfying the service-level agreements.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.01222v1"
    },
    {
        "title": "Discrete-time Queueing Model of Age of Information with Multiple\n  Information Sources",
        "authors": [
            "Nail Akar",
            "Ozancan Dogan"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Information freshness in IoT-based status update systems has recently been\nstudied through the Age of Information (AoI) and Peak AoI (PAoI) performance\nmetrics. In this paper, we study a discrete-time server arising in multi-source\nIoT systems which accepts incoming information packets from multiple\ninformation sources so as to be forwarded to a remote monitor for status update\npurposes. Under the assumption of Bernoulli information packet arrivals and a\ncommon geometric service time distribution across all the sources, we\nnumerically obtain the exact per-source distributions of AoI and PAoI in\nmatrix-geometric form for three different queueing disciplines: i)\nNon-Preemptive Bufferless (NPB) ii) Preemptive Bufferless (PB) iii)\nNon-Preemptive Single Buffer with Replacement (NPSBR). The proposed numerical\nalgorithm employs the theory of Discrete-Time Markov Chains (DTMC) of\nQuasi-Birth-Death (QBD) type and is matrix analytical, i.e, the algorithm is\nbased on numerically stable and efficient vector-matrix operations.Numerical\nexamples are provided to validate the accuracy and effectiveness of the\nproposed queueing model. We also present a numerical example on the optimum\nchoice of the Bernoulli parameters in a practical IoT system with two sources\nwith diverse AoI requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11650v1"
    },
    {
        "title": "The Multi-Source Preemptive M/PH/1/1 Queue with Packet Errors: Exact\n  Distribution of the Age of Information and Its Peak",
        "authors": [
            "Ozancan Dogan",
            "Nail Akar"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Age of Information (AoI) and Peak AoI (PAoI) and their analytical models have\nrecently drawn substantial amount of attention in information theory and\nwireless communications disciplines, in the context of qualitative assessment\nof information freshness in status update systems. We take a queueing-theoretic\napproach and study a probabilistically preemptive bufferless $M/PH/1/1$\nqueueing system with arrivals stemming from $N$ separate information sources,\nwith the aim of modeling a generic status update system. In this model, a new\ninformation packet arrival from source $m$ is allowed to preempt a packet from\nsource $n$ in service, with a probability depending on $n$ and $m$. To make the\nmodel even more general than the existing ones, for each of the information\nsources, we assume a distinct PH-type service time distribution and a distinct\npacket error probability. Subsequently, we obtain the exact distributions of\nthe AoI and PAoI for each of the information sources using matrix-analytical\nalgorithms and in particular the theory of Markov fluid queues and sample path\narguments. This is in contrast with existing methods that rely on Stochastic\nHybrid Systems (SHS) which obtain only the average values and in less general\nsettings. Numerical examples are provided to validate the proposed approach as\nwell as to give engineering insight on the impact of preemption probabilities\non certain AoI and PAoI performance figures.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.11656v1"
    },
    {
        "title": "Reinforcement Learning Assisted Load Test Generation for E-Commerce\n  Applications",
        "authors": [
            "Golrokh Hamidi"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Background: End-user satisfaction is not only dependent on the correct\nfunctioning of the software systems but is also heavily dependent on how well\nthose functions are performed. Therefore, performance testing plays a critical\nrole in making sure that the system responsively performs the indented\nfunctionality. Load test generation is a crucial activity in performance\ntesting. Existing approaches for load test generation require expertise in\nperformance modeling, or they are dependent on the system model or the source\ncode.\n  Aim: This thesis aims to propose and evaluate a model-free learning-based\napproach for load test generation, which doesn't require access to the system\nmodels or source code.\n  Method: In this thesis, we treated the problem of optimal load test\ngeneration as a reinforcement learning (RL) problem. We proposed two RL-based\napproaches using q-learning and deep q-network for load test generation. In\naddition, we demonstrated the applicability of our tester agents on a\nreal-world software system. Finally, we conducted an experiment to compare the\nefficiency of our proposed approaches to a random load test generation approach\nand a baseline approach.\n  Results: Results from the experiment show that the RL-based approaches\nlearned to generate effective workloads with smaller sizes and in fewer steps.\nThe proposed approaches led to higher efficiency than the random and baseline\napproaches.\n  Conclusion: Based on our findings, we conclude that RL-based agents can be\nused for load test generation, and they act more efficiently than the random\nand baseline approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.12094v1"
    },
    {
        "title": "Analytical Performance Modeling of NoCs under Priority Arbitration and\n  Bursty Traffic",
        "authors": [
            "Sumit K. Mandal",
            "Raid Ayoub",
            "Michael Kishinevsky",
            "Mohammad M. Islam",
            "Umit Y. Ogras"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Networks-on-Chip (NoCs) used in commercial many-core processors typically\nincorporate priority arbitration. Moreover, they experience bursty traffic due\nto application workloads. However, most state-of-the-art NoC analytical\nperformance analysis techniques assume fair arbitration and simple traffic\nmodels. To address these limitations, we propose an analytical modeling\ntechnique for priority-aware NoCs under bursty traffic. Experimental\nevaluations with synthetic and bursty traffic show that the proposed approach\nhas less than 10% modeling error with respect to cycle-accurate NoC simulator.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.13951v1"
    },
    {
        "title": "Stability for Two-class Multiserver-job Systems",
        "authors": [
            "Isaac Grosof",
            "Mor Harchol-Balter",
            "Alan Scheller-Wolf"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Multiserver-job systems, where jobs require concurrent service at many\nservers, occur widely in practice. Much is known in the dropping setting, where\njobs are immediately discarded if they require more servers than are currently\navailable. However, very little is known in the more practical setting where\njobs queue instead.\n  In this paper, we derive a closed-form analytical expression for the\nstability region of a two-class (non-dropping) multiserver-job system where\neach class of jobs requires a distinct number of servers and requires a\ndistinct exponential distribution of service time, and jobs are served in\nfirst-come-first-served (FCFS) order. This is the first result of any kind for\nan FCFS multiserver-job system where the classes have distinct service\ndistributions. Our work is based on a technique that leverages the idea of a\n\"saturated\" system, in which an unlimited number of jobs are always available.\n  Our analytical formula provides insight into the behavior of FCFS\nmultiserver-job systems, highlighting the huge wastage (idle servers while jobs\nare in the queue) that can occur, as well as the nonmonotonic effects of the\nservice rates on wastage.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.00631v1"
    },
    {
        "title": "Trade-off between accuracy and tractability of network calculus in FIFO\n  networks",
        "authors": [
            "Anne Bouillard"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Computing accurate deterministic performance bounds is a strong need for\ncommunication technologies having strong requirements on latency and\nreliability. Beyond new scheduling protocols such as TSN, the FIFO policy\nremains at work within each class of communication.\n  In this paper, we focus on computing deterministic performance bounds in FIFO\nnetworks in the network calculus framework. We propose a new algorithm based on\nlinear programming that presents a trade-off between accuracy and tractability.\nThis algorithm is first presented for tree networks. In a second time, we\ngeneralize our approach and present a linear program for computing performance\nbounds for arbitrary topologies, including cyclic dependencies. Finally, we\nprovide numerical results, both of toy examples and real topologies, to assess\nthe interest of our approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.09263v1"
    },
    {
        "title": "Proximity Based Load Balancing Policies on Graphs: A Simulation Study",
        "authors": [
            "Nitish K. Panigrahy",
            "Thirupathaiah Vasantam",
            "Prithwish Basu",
            "Don Towsley"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Distributed load balancing is the act of allocating jobs among a set of\nservers as evenly as possible. There are mainly two versions of the load\nbalancing problem that have been studied in the literature: static and dynamic.\nThe static interpretation leads to formulating the load balancing problem as a\ncase with jobs (balls) never leaving the system and accumulating at the servers\n(bins) whereas the dynamic setting deals with the case when jobs arrive and\nleave the system after service completion. This paper designs and evaluates\nserver proximity aware job allocation policies for treating load balancing\nproblems with a goal to reduce the communication cost associated with the jobs.\nWe consider a class of proximity aware Power of Two (POT) choice based\nassignment policies for allocating jobs to servers, where servers are\ninterconnected as an n-vertex graph G(V, E). For the static version, we assume\neach job arrives at one of the servers, u. For the dynamic setting, we assume G\nto be a circular graph and job arrival process at each server is described by a\nPoisson point process with the job service time exponentially distributed. For\nboth settings, we then assign each job to the server with minimum load among\nservers u and v where v is chosen according to one of the following two\npolicies: (i) Unif-POT(k): Sample a server v uniformly at random from k-hop\nneighborhood of u (ii) InvSq-POT(k): Sample a server v from k-hop neighborhood\nof u with probability proportional to the inverse square of the distance\nbetween u and v. Our simulation results show that both the policies\nconsistently produce a load distribution which is much similar to that of a\nclassical proximity oblivious POT policy.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.01485v1"
    },
    {
        "title": "An approach to define Very High Capacity Networks with improved quality\n  at an affordable cost",
        "authors": [
            "Giovanni Santella",
            "Francesco Vatalaro"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  This paper aims to propose one possible approach in the setting of VHCNs\n(Very High Capacity Networks) performance targets that should be capable of\npromoting efficient investments for operators and, at the same time, improving\nthe benefits for end-users. To this aim, we suggest relying on some specific\nKPIs (Key Performance Indicators), especially throughput - i.e., the bandwidth\nas perceived by the customer - valid at the application layer, instead of the\nphysical layer data-rate. In this regard, the paper underlines that the\nbandwidth perceived is strictly linked to the latency. The most important\nimplication is that some of the most demanding services envisaged for the\nfuture (e.g., mobile virtual and augmented reality, tactile internet) cannot be\nmet by merely increasing the low-level protocol data-rate. Therefore, for the\nVHCNs reducing latency through Edge Cloud Computing (ECC) is a mandatory\npre-requisite.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.03685v1"
    },
    {
        "title": "Tools for modelling and simulating the Smart Grid",
        "authors": [
            "Ricardo M. Czekster"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The Smart Grid (SG) is a Cyber-Physical System (CPS) considered a critical\ninfrastructure divided into cyber (software) and physical (hardware)\ncounterparts that complement each other. It is responsible for timely power\nprovision wrapped by Information and Communication Technologies (ICT) for\nhandling bi-directional energy flows in electric power grids. Enacting control\nand performance over the massive infrastructure of the SG requires convenient\nanalysis methods. Modelling and simulation (M&S) is a performance evaluation\ntechnique used to study virtually any system by testing designs and\nartificially creating 'what-if' scenarios for system reasoning and advanced\nanalysis. M&S avoids stressing the actual physical infrastructure and systems\nin production by addressing the problem in a purely computational perspective.\nPresent work compiles a non-exhaustive list of tools for M&S of interest when\ntackling SG capabilities. Our contribution is to delineate available options\nfor modellers when considering power systems in combination with ICT. We also\nshow the auxiliary tools and details of most relevant solutions pointing out\nmajor features and combinations over the years.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.07968v3"
    },
    {
        "title": "Optimal Transaction Queue Waiting in Blockchain Mining",
        "authors": [
            "Gholamreza Ramezan",
            "Cyril Leung",
            "Chunyan Miao"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Blockchain systems are being used in a wide range of application domains.\nThey can support trusted transactions in time critical applications. In this\npaper, we study how miners should pick up transactions from a transaction pool\nso as to minimize the average waiting time per transaction. We derive an\nexpression for the average transaction waiting time of the proposed mining\nscheme and determine the optimum decision rule. Numerical results show that the\naverage waiting time per transaction can be reduced by about 10% compared to\nthe traditional no-wait scheme in which miners immediately start the next\nmining round using all transactions waiting in the pool.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.10886v1"
    },
    {
        "title": "Verifiable Failure Localization in Smart Grid under Cyber-Physical\n  Attacks",
        "authors": [
            "Yudi Huang",
            "Ting He",
            "Nilanjan Ray Chaudhuri",
            "Thomas La Porta"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Cyber-physical attacks impose a significant threat to the smart grid, as the\ncyber attack makes it difficult to identify the actual damage caused by the\nphysical attack. To defend against such attacks, various inference-based\nsolutions have been proposed to estimate the states of grid elements (e.g.,\ntransmission lines) from measurements outside the attacked area, out of which a\nfew have provided theoretical conditions for guaranteed accuracy. However,\nthese conditions are usually based on the ground truth states and thus not\nverifiable in practice. To solve this problem, we develop (i) verifiable\nconditions that can be tested based on only observable information, and (ii)\nefficient algorithms for verifying the states of links (i.e., transmission\nlines) within the attacked area based on these conditions. Our numerical\nevaluations based on the Polish power grid and IEEE 300-bus system demonstrate\nthat the proposed algorithms are highly successful in verifying the states of\ntruly failed links, and can thus greatly help in prioritizing repairs during\nthe recovery process.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.07129v1"
    },
    {
        "title": "Comparing Broadband ISP Performance using Big Data from M-Lab",
        "authors": [
            "Xiaohong Deng",
            "Yun Feng",
            "Thanchanok Sutjarittham",
            "Hassan Habibi Gharakheili",
            "Blanca Gallego",
            "Vijay Sivaraman"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Comparing ISPs on broadband speed is challenging, since measurements can vary\ndue to subscriber attributes such as operation system and test conditions such\nas access capacity, server distance, TCP window size, time-of-day, and network\nsegment size. In this paper, we draw inspiration from observational studies in\nmedicine, which face a similar challenge in comparing the effect of treatments\non patients with diverse characteristics, and have successfully tackled this\nusing \"causal inference\" techniques for {\\em post facto} analysis of medical\nrecords. Our first contribution is to develop a tool to pre-process and\nvisualize the millions of data points in M-Lab at various time- and\nspace-granularities to get preliminary insights on factors affecting broadband\nperformance. Next, we analyze 24 months of data pertaining to twelve ISPs\nacross three countries, and demonstrate that there is observational bias in the\ndata due to disparities amongst ISPs in their attribute distributions. For our\nthird contribution, we apply a multi-variate matching method to identify\nsuitable cohorts that can be compared without bias, which reveals that ISPs are\ncloser in performance than thought before. Our final contribution is to refine\nour model by developing a method for estimating speed-tier and re-apply\nmatching for comparison of ISP performance. Our results challenge conventional\nrankings of ISPs, and pave the way towards data-driven approaches for unbiased\ncomparisons of ISPs world-wide.\n",
        "pdf_link": "http://arxiv.org/pdf/2101.09795v1"
    },
    {
        "title": "Unleashing the Power of Paying Multiplexing Only Once in Stochastic\n  Network Calculus",
        "authors": [
            "Anne Bouillard",
            "Paul Nikolaus",
            "Jens Schmitt"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The stochastic network calculus (SNC) holds promise as a versatile and\nuniform framework to calculate probabilistic performance bounds in networks of\nqueues. A great challenge to accurate bounds and efficient calculations are\nstochastic dependencies between flows due to resource sharing inside the\nnetwork. However, by carefully utilizing the basic SNC concepts in the network\nanalysis the necessity of taking these dependencies into account can be\nminimized. To that end, we unleash the power of the pay multiplexing only once\nprinciple (PMOO, known from the deterministic network calculus) in the SNC\nanalysis. We choose an analytic combinatorics presentation of the results in\norder to ease complex calculations. In tree-reducible networks, a subclass of\ngeneral feedforward networks, we obtain an effective analysis in terms of\navoiding the need to take internal flow dependencies into account. In a\ncomprehensive numerical evaluation, we demonstrate how this unleashed PMOO\nanalysis can reduce the known gap between simulations and SNC calculations\nsignificantly, and how it favourably compares to state-of-the art SNC\ncalculations in terms of accuracy and computational effort. Motivated by these\npromising results, we also consider general feedforward networks, when some\nflow dependencies have to be taken into account. To that end, the unleashed\nPMOO analysis is extended to the partially dependent case and a case study of a\ncanonical example topology, known as the diamond network, is provided, again\ndisplaying favourable results over the state of the art.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.14215v3"
    },
    {
        "title": "Download time analysis for distributed storage systems with node\n  failures",
        "authors": [
            "Tim Hellemans",
            "Arti Yardi",
            "Tejas Bodas"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  We consider a distributed storage system which stores several hot (popular)\nand cold (less popular) data files across multiple nodes or servers. Hot files\nare stored using repetition codes while cold files are stored using erasure\ncodes. The nodes are prone to failure and hence at any given time, we assume\nthat only a fraction of the nodes are available. Using a cavity process based\nmean field framework, we analyze the download time for users accessing hot or\ncold data in the presence of failed nodes. Our work also illustrates the impact\nof the choice of the storage code on the download time performance of users in\nthe system.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.02926v1"
    },
    {
        "title": "Towards Performance Clarity of Edge Video Analytics",
        "authors": [
            "Zhujun Xiao",
            "Zhengxu Xia",
            "Haitao Zheng",
            "Ben Y. Zhao",
            "Junchen Jiang"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Edge video analytics is becoming the solution to many safety and management\ntasks. Its wide deployment, however, must first address the tension between\ninference accuracy and resource (compute/network) cost. This has led to the\ndevelopment of video analytics pipelines (VAPs), which reduce resource cost by\ncombining DNN compression/speedup techniques with video processing heuristics.\nOur measurement study on existing VAPs, however, shows that today's methods for\nevaluating VAPs are incomplete, often producing premature conclusions or\nambiguous results. This is because each VAP's performance varies substantially\nacross videos and time (even under the same scenario) and is sensitive to\ndifferent subsets of video content characteristics.\n  We argue that accurate VAP evaluation must first characterize the complex\ninteraction between VAPs and video characteristics, which we refer to as VAP\nperformance clarity. We design and implement Yoda, the first VAP benchmark to\nachieve performance clarity. Using primitive-based profiling and a carefully\ncurated benchmark video set, Yoda builds a performance clarity profile for each\nVAP to precisely define its accuracy/cost tradeoff and its relationship with\nvideo characteristics. We show that Yoda substantially improves VAP evaluations\nby (1) providing a comprehensive, transparent assessment of VAP performance and\nits dependencies on video characteristics; (2) explicitly identifying\nfine-grained VAP behaviors that were previously hidden by large performance\nvariance; and (3) revealing strengths/weaknesses among different VAPs and new\ndesign opportunities.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.08694v1"
    },
    {
        "title": "Conditional Waiting Time Analysis in Tandem Polling Queues",
        "authors": [
            "Ravi Suman",
            "Ananth Krishnamurthy"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  We analyze a tandem network of polling queues with two product types and two\nstations. We assume that external arrivals to the network follow a Poisson\nprocess, and service times at each station are exponentially distributed. For\nthis system, we determine the mean conditional waiting time for an arriving\ncustomer using a sample path analysis approach. The approach classifies system\nstate upon arrival into scenarios and exploits an inherent structure in the\nsequence of events that occur till the customer departs to obtain conditional\nwaiting time estimates. We conduct numerical studies to show both the accuracy\nof our conditional waiting time estimates and their practical importance.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.11104v1"
    },
    {
        "title": "Deficit Round-Robin: A Second Network Calculus Analysis",
        "authors": [
            "Seyed Mohammadhossein Tabatabaee",
            "Jean-Yves Le Boudec"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Deficit Round-Robin (DRR) is a widespread scheduling algorithm that provides\nfair queueing with variable-length packets. Bounds on worst-case delays for DRR\nwere found by Boyer et al., who used a rigorous network calculus approach and\ncharacterized the service obtained by one flow of interest by means of a convex\nstrict service curve. These bounds do not make any assumptions on the\ninterfering traffic hence are pessimistic when the interfering traffic is\nconstrained by some arrival curves. For such cases, two improvements were\nproposed. The former, by Soni et al., uses a correction term derived from a\nsemi-rigorous heuristic; unfortunately, these bounds are incorrect, as we show\nby exhibiting a counter-example. The latter, by Bouillard, rigorously derive\nconvex strict service curves for DRR that account for the arrival curve\nconstraints of the interfering traffic. In this paper, we improve on these\nresults in two ways. First, we derive a non-convex strict service curve for DRR\nthat improves on Boyer et al. when there is no arrival constraint on the\ninterfering traffic. Second, we provide an iterative method to improve any\nstrict service curve (including Bouillard's) when there are arrival constraints\nfor the interfering traffic. As of today, our results provide the best-known\nworst-case delay bounds for DRR. They are obtained by using the method of the\npseudo-inverse.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.01034v1"
    },
    {
        "title": "Towards Cost-Optimal Policies for DAGs to Utilize IaaS Clouds with\n  Online Learning",
        "authors": [
            "Xiaohu Wu",
            "Han Yu",
            "Giuliano Casale",
            "Guanyu Gao"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Premier cloud service providers (CSPs) offer two types of purchase options,\nnamely on-demand and spot instances, with time-varying features in availability\nand price. Users like startups have to operate on a limited budget and\nsimilarly others hope to reduce their costs. While interacting with a CSP,\ncentral to their concerns is the process of cost-effectively utilizing\ndifferent purchase options possibly in addition to self-owned instances. A job\nin data-intensive applications is typically represented by a directed acyclic\ngraph which can further be transformed into a chain of tasks. The key to\nachieving cost efficiency is determining the allocation of a specific deadline\nto each task, as well as the allocation of different types of instances to the\ntask. In this paper, we propose a framework that determines the optimal\nallocation of deadlines to tasks. The framework also features an optimal policy\nto determine the allocation of spot and on-demand instances in a predefined\ntime window, and a near-optimal policy for allocating self-owned instances. The\npolicies are designed to be parametric to support the usage of online learning\nto infer the optimal values against the dynamics of cloud markets. Finally,\nseveral intuitive heuristics are used as baselines to validate the cost\nimprovement brought by the proposed solutions. We show that the cost\nimprovement over the state-of-the-art is up to 24.87% when spot and on-demand\ninstances are considered and up to 59.05% when self-owned instances are\nconsidered.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.01847v1"
    },
    {
        "title": "LLAMA: The Low-Level Abstraction For Memory Access",
        "authors": [
            "Bernhard Manfred Gruber",
            "Guilherme Amadio",
            "Jakob Blomer",
            "Alexander Matthes",
            "René Widera",
            "Michael Bussmann"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The performance gap between CPU and memory widens continuously. Choosing the\nbest memory layout for each hardware architecture is increasingly important as\nmore and more programs become memory bound. For portable codes that run across\nheterogeneous hardware architectures, the choice of the memory layout for data\nstructures is ideally decoupled from the rest of a program. This can be\naccomplished via a zero-runtime-overhead abstraction layer, underneath which\nmemory layouts can be freely exchanged.\n  We present the Low-Level Abstraction of Memory Access (LLAMA), a C++ library\nthat provides such a data structure abstraction layer with example\nimplementations for multidimensional arrays of nested, structured data. LLAMA\nprovides fully C++ compliant methods for defining and switching custom memory\nlayouts for user-defined data types. The library is extensible with third-party\nallocators.\n  Providing two close-to-life examples, we show that the LLAMA-generated AoS\n(Array of Structs) and SoA (Struct of Arrays) layouts produce identical code\nwith the same performance characteristics as manually written data structures.\nIntegrations into the SPEC CPU\\textsuperscript{\\textregistered} lbm benchmark\nand the particle-in-cell simulation PIConGPU demonstrate LLAMA's abilities in\nreal-world applications. LLAMA's layout-aware copy routines can significantly\nspeed up transfer and reshuffling of data between layouts compared with naive\nelement-wise copying.\n  LLAMA provides a novel tool for the development of high-performance C++\napplications in a heterogeneous environment.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.04284v3"
    },
    {
        "title": "A New Upper Bound on Cache Hit Probability for Non-anticipative Caching\n  Policies",
        "authors": [
            "Nitish K. Panigrahy",
            "Philippe Nain",
            "Giovanni Neglia",
            "Don Towsley"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Caching systems have long been crucial for improving the performance of a\nwide variety of network and web based online applications. In such systems,\nend-to-end application performance heavily depends on the fraction of objects\ntransferred from the cache, also known as the cache hit probability. Many\ncaching policies have been proposed and implemented to improve the hit\nprobability. In this work, we propose a new method to compute an upper bound on\nhit probability for all non-anticipative caching policies, i.e., for policies\nthat have no knowledge of future requests. Our key insight is to order the\nobjects according to the ratio of their Hazard Rate (HR) function values to\ntheir sizes and place in the cache the objects with the largest ratios till the\ncache capacity is exhausted. Under some statistical assumptions, we prove that\nour proposed HR to size ratio based ordering model computes the maximum\nachievable hit probability and serves as an upper bound for all\nnon-anticipative caching policies. We derive closed form expressions for the\nupper bound under some specific object request arrival processes. We also\nprovide simulation results to validate its correctness and to compare it to the\nstate-of-the-art upper bounds. We find it to be tighter than state-of-the-art\nupper bounds for a variety of object request arrival processes.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.06457v1"
    },
    {
        "title": "QWin: Enforcing Tail Latency SLO at Shared Storage Backend",
        "authors": [
            "Liuying Ma",
            "Zhenqing Liu",
            "Jin Xiong",
            "Dejun Jiang"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Consolidating latency-critical (LC) and best-effort (BE) tenants at storage\nbackend helps to increase resources utilization. Even if tenants use dedicated\nqueues and threads to achieve performance isolation, threads are still contend\nfor CPU cores. Therefore, we argue that it is necessary to partition cores\nbetween LC and BE tenants, and meanwhile each core is dedicated to run a\nthread. Expect for frequently changing bursty load, fluctuated service time at\nstorage backend also drastically changes the need of cores. In order to\nguarantee tail latency service level objectives (SLOs), the abrupt changing\nneed of cores must be satisfied immediately. Otherwise, tail latency SLO\nviolation happens. Unfortunately, partitioning-based approaches lack the\nability to react the changing need of cores, resulting in extreme spikes in\nlatency and SLO violation happens. In this paper, we present QWin, a tail\nlatency SLO aware core allocation to enforce tail latency SLO at shared storage\nbackend. QWin consists of an SLO-to-core calculation model that accurately\ncalculates the number of cores combining with definitive runtime load\ndetermined by a flexible request-based window, and an autonomous core\nallocation that adjusts cores at adaptive frequency by dynamically changing\ncore policies. When consolidating multiple LC and BE tenants, QWin outperforms\nthe-state-of-the-art approaches in guaranteeing tail latency SLO for LC tenants\nand meanwhile increasing bandwidth of BE tenants by up to 31x.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.09206v1"
    },
    {
        "title": "Comparing the behavior of OpenMP Implementations with various\n  Applications on two different Fujitsu A64FX platforms",
        "authors": [
            "Benjamin Michalowicz",
            "Eric Raut",
            "Yan Kang",
            "Tony Curtis",
            "Barbara Chapman",
            "Dossay Oryspayev"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The development of the A64FX processor by Fujitsu has been a massive\ninnovation in vectorized processors and led to Fugaku: the current world's\nfastest supercomputer. We use a variety of tools to analyze the behavior and\nperformance of several OpenMP applications with different compilers, and how\nthese applications scale on the different A64FX processors on clusters at Stony\nBrook University and RIKEN.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.09787v1"
    },
    {
        "title": "Opening the Black Box: Performance Estimation during Code Generation for\n  GPUs",
        "authors": [
            "Dominik Ernst",
            "Georg Hager",
            "Markus Holzer",
            "Matthias Knorr",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Automatic code generation is frequently used to create implementations of\nalgorithms specifically tuned to particular hardware and application\nparameters. The code generation process involves the selection of adequate code\ntransformations, tuning parameters, and parallelization strategies. To cover\nthe huge search space, code generation frameworks may apply time-intensive\nautotuning, exploit scenario-specific performance models, or treat performance\nas an intangible black box that must be described via machine learning.\n  This paper addresses the selection problem by identifying the relevant\nperformance-defining mechanisms through a performance model coupled with an\nanalytic hardware metric estimator. This enables a quick exploration of large\nconfiguration spaces to identify highly efficient candidates with high\naccuracy.\n  Our current approach targets memory-intensive GPGPU applications and focuses\non the correct modeling of data transfer volumes to all levels of the memory\nhierarchy. We show how our method can be coupled to the pystencils stencil code\ngenerator, which is used to generate kernels for a range four 3D25pt stencil\nand a complex two phase fluid solver based on the Lattice Boltzmann Method. For\nboth, it delivers a ranking that can be used to select the best performing\ncandidate.\n  The method is not limited to stencil kernels, but can be integrated into any\ncode generator that can generate the required address expressions.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.01143v1"
    },
    {
        "title": "Performance landscape of resource-constrained platforms targeting DNNs",
        "authors": [
            "Panagiotis Miliadis",
            "Christos-Savvas Bouganis",
            "Dionisios Pnevmatikatos"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Over the recent years, a significant number of complex, deep neural networks\nhave been developed for a variety of applications including speech and face\nrecognition, computer vision in the areas of health-care, automatic\ntranslation, image classification, etc. Moreover, there is an increasing demand\nin deploying these networks in resource-constrained edge devices. As the\ncomputational demands of these models keep increasing, pushing to their limits\nthe targeted devices, the constant development of new hardware systems tailored\nto those workloads has been observed. Since programmability of these diverse\nand complex platforms -- compounded by the rapid development of new DNN models\n-- is a major challenge, platform vendors have developed Machine Learning\ntailored SDKs to maximize the platform's performance.\n  This work investigates the performance achieved on a number of modern\ncommodity embedded platforms coupled with the vendors' provided software\nsupport when state-of-the-art DNN models from image classification, object\ndetection and image segmentation are targeted. The work quantifies the relative\nlatency gains of the particular embedded platforms and provides insights on the\nrelationship between the required minimum batch size for achieving maximum\nthroughput, concluding that modern embedded systems reach their maximum\nperformance even for modest batch sizes when a modern state of the art DNN\nmodel is targeted. Overall, the presented results provide a guide for the\nexpected performance for a number of state-of-the-art DNNs on popular embedded\nplatforms across the image classification, detection and segmentation domains.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.10047v2"
    },
    {
        "title": "uiCA: Accurate Throughput Prediction of Basic Blocks on Recent Intel\n  Microarchitectures",
        "authors": [
            "Andreas Abel",
            "Jan Reineke"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Performance models that statically predict the steady-state throughput of\nbasic blocks on particular microarchitectures, such as IACA, Ithemal, llvm-mca,\nOSACA, or CQA, can guide optimizing compilers and aid manual software\noptimization. However, their utility heavily depends on the accuracy of their\npredictions. The average error of existing models compared to measurements on\nthe actual hardware has been shown to lie between 9% and 36%. But how good is\nthis? To answer this question, we propose an extremely simple analytical\nthroughput model that may serve as a baseline. Surprisingly, this model is\nalready competitive with the state of the art, indicating that there is\nsignificant potential for improvement.\n  To explore this potential, we develop a simulation-based throughput\npredictor. To this end, we propose a detailed parametric pipeline model that\nsupports all Intel Core microarchitecture generations released between 2011 and\n2021. We evaluate our predictor on an improved version of the BHive benchmark\nsuite and show that its predictions are usually within 1% of measurement\nresults, improving upon prior models by roughly an order of magnitude. The\nexperimental evaluation also demonstrates that several microarchitectural\ndetails considered to be rather insignificant in previous work, are in fact\nessential for accurate prediction.\n  Our throughput predictor is available as open source at\nhttps://github.com/andreas-abel/uiCA.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.14210v3"
    },
    {
        "title": "FIRESTARTER 2: Dynamic Code Generation for Processor Stress Tests",
        "authors": [
            "Robert Schöne",
            "Markus Schmidl",
            "Mario Bielert",
            "Daniel Hackenberg"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Processor stress tests target to maximize processor power consumption by\nexecuting highly demanding workloads. They are typically used to test the\ncooling and electrical infrastructure of compute nodes or larger systems in\nlabs or data centers. While multiple of these tools already exists, they have\nto be re-evaluated and updated regularly to match the developments in computer\narchitecture. This paper presents the first major update of FIRESTARTER, an\nOpen Source tool specifically designed to create near-peak power consumption.\nThe main new features concern the online generation of workloads and automatic\nself-tuning for specific hardware configurations. We further apply these new\nfeatures on an AMD Rome system and demonstrate the optimization process. Our\nanalysis shows how accesses to the different levels of the memory hierarchy\ncontribute to the overall power consumption. Finally, we demonstrate how the\nauto-tuning algorithm can cope with different processor configurations and how\nthese influence the effectiveness of the created workload.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.01470v2"
    },
    {
        "title": "On the equivalence of holding cost and response time for evaluating\n  performance of queues",
        "authors": [
            "Dylan Solms"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  This self-contained discussion relates the long-run average holding cost per\nunit time to the long-run average response time per customer in a $G/G/1$ queue\nwith no assumption made on the order of service. The only restriction\nestablished is that the system be ergodic. This is achieved using standard\nqueuing theory. The practical relevance of such a result is discussed in the\ncontext of simulation output analysis as well as through an application to\nformulating a Markov Decision Process that minimises long-run average response\ntime per customer.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.07656v1"
    },
    {
        "title": "Automatic Timing-Coherent Transactor Generation for Mixed-level\n  Simulations",
        "authors": [
            "Li-Chun Chen",
            "Hsin-I Wu",
            "Ren-Song Tsay"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  In this paper we extend the concept of the traditional transactor, which\nfocuses on correct content transfer, to a new timing-coherent transactor that\nalso accurately aligns the timing of each transaction boundary so that\ndesigners can perform precise concurrent system behavior analysis in\nmixed-abstraction-level system simulations which are essential to increasingly\ncomplex system designs. To streamline the process, we also developed an\nautomatic approach for timing-coherent transactor generation. Our approach is\nactually applied in mixed-level simulations and the results show that it\nachieves 100% timing accuracy while the conventional approach produces results\nof 25% to 44% error rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04148v1"
    },
    {
        "title": "An Effective Early Multi-core System Shared Cache Design Method Based on\n  Reuse-distance Analysis",
        "authors": [
            "Hsin-Yu Ho",
            "Ren-Song Tsay"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  In this paper, we proposed an effective and efficient multi-core shared-cache\ndesign optimization approach based on reuse-distance analysis of the data\ntraces of target applications. Since data traces are independent of system\nhardware architectures, a designer can easily compute the best cache design at\nthe early system design phase using our approach. We devise a very efficient\nand yet accurate method to derive the aggregated reuse-distance histograms of\nconcurrent applications for accurate cache performance analysis and\noptimization. Essentially, the actual shared-cache contention results of\nconcurrent applications are embedded in the aggregated reuse-distance\nhistograms and therefore the approach is very effective. The experimental\nresults show that the average error rate of shared-cache miss-count estimations\nof our approach is less than 2.4%. Using a simple scanning search method, one\ncan easily determine the true optimal cache configurations at the early system\ndesign phase.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04621v1"
    },
    {
        "title": "A fluid reservoir model for the Age of Information through\n  energy-harvesting transmitters",
        "authors": [
            "Ioannis Z. Koukoutsidis"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  We apply a fluid-reservoir model to study the Age-of-Information (AoI) of\nupdate packets through energy-harvesting transmitters. The model is closer to\nhow energy is stored and depleted in reality, and can reveal the system\nbehavior for different settings of packet arrival rates, service rates, and\nenergy charging and depletion rates. We present detailed results for both\nfinite and infinite transmitter buffers and an infinite energy reservoir, and\nsome indicative results for a finite reservoir. The results are derived for the\nmean AoI in the case of an infinite transmitter buffer and an infinite\nreservoir, and for the mean peak AoI for the remaining cases. The results show\nthat, similar to a system without energy constraints, the transmitter buffer\nshould be kept to a minimum in order to avoid queueing delays and maintain\nfreshness of updates. Furthermore, a high update packet rate is only helpful in\nenergy-rich regimes, whereas in energy-poor regimes more frequent updates\ndeplete the energy reservoir and result in higher AoI values.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.12317v1"
    },
    {
        "title": "An Analysis into the Performance and Memory Usage of MATLAB Strings",
        "authors": [
            "Travis Near"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  MATLAB is a mathematical computing environment used by many engineers,\nmathematicians, and students to process and understand their data. Important to\nall data science is the managing of textual data. MATLAB supports two textual\ndata containers: (1) cell arrays of characters and (2) string arrays. This\nresearch showcases the strengths of string arrays over cell arrays by\nquantifying their performance, memory contiguity, syntax readability, interface\nfluidity, and autocomplete capabilities. These results demonstrate that string\narrays often run 2x to 40x faster than cell arrays for common string\nbenchmarks, are optimized for data locality by reducing metadata overhead, and\noffer a more expressive syntax due to their automatic data type conversions and\nvectorized methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.12567v1"
    },
    {
        "title": "WCFS: A new framework for analyzing multiserver systems",
        "authors": [
            "Isaac Grosof",
            "Mor Harchol-Balter",
            "Alan Scheller-Wolf"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Multiserver queueing systems are found at the core of a wide variety of\npractical systems. Many important multiserver models have a\npreviously-unexplained similarity: identical mean response time behavior is\nempirically observed in the heavy traffic limit. We explain this similarity for\nthe first time.\n  We do so by introducing the work-conserving finite-skip (WCFS) framework,\nwhich encompasses a broad class of important models. This class includes the\nheterogeneous M/G/k, the limited processor sharing policy for the M/G/1, the\nthreshold parallelism model, and the multiserver-job model under a novel\nscheduling algorithm.\n  We prove that for all WCFS models, scaled mean response time $E[T](1-\\rho)$\nconverges to the same value, $E[S^2]/(2E[S])$, in the heavy-traffic limit,\nwhich is also the heavy traffic limit for the M/G/1/FCFS. Moreover, we prove\nadditively tight bounds on mean response time for the WCFS class, which hold\nfor all load $\\rho$. For each of the four models mentioned above, our bounds\nare the first known bounds on mean response time.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.12663v3"
    },
    {
        "title": "Online Application Guidance for Heterogeneous Memory Systems",
        "authors": [
            "M. Ben Olson",
            "Brandon Kammerdiener",
            "Kshitij A. Doshi",
            "Terry Jones",
            "Michael R. Jantz"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Many high end and next generation computing systems to incorporated\nalternative memory technologies to meet performance goals. Since these\ntechnologies present distinct advantages and tradeoffs compared to conventional\nDDR* SDRAM, such as higher bandwidth with lower capacity or vice versa, they\nare typically packaged alongside conventional SDRAM in a heterogeneous memory\narchitecture. To utilize the different types of memory efficiently, new data\nmanagement strategies are needed to match application usage to the best\navailable memory technology. However, current proposals for managing\nheterogeneous memories are limited because they either: 1) do not consider\nhigh-level application behavior when assigning data to different types of\nmemory, or 2) require separate program execution (with a representative input)\nto collect information about how the application uses memory resources.\n  This work presents a toolset for addressing the limitations of existing\napproaches for managing complex memories. It extends the application runtime\nlayer with automated monitoring and management routines that assign application\ndata to the best tier of memory based on previous usage, without any need for\nsource code modification or a separate profiling run. It evaluates this\napproach on a state-of-the-art server platform with both conventional DDR4\nSDRAM and non-volatile Intel Optane DC memory, using both memory-intensive high\nperformance computing (HPC) applications as well as standard benchmarks.\nOverall, the results show that this approach improves program performance\nsignificantly compared to a standard unguided approach across a variety of\nworkloads and system configurations. Additionally, we show that this approach\nachieves similar performance as a comparable offline profiling-based approach\nafter a short startup period, without requiring separate program execution or\noffline analysis steps.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.02150v1"
    },
    {
        "title": "Infinite Servers Queue Systems Busy Period Time Length Distribution and\n  Parameters Study through Computational Simulation",
        "authors": [
            "Manuel Alberto M. Ferreira"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  A FORTRAN program to simulate the operation of infinite servers queues is\npresented in this work. Poisson arrivals processes are considered but not only.\nFor many parameters of interest in queuing systems study or application, either\nthere are not theoretical results or, existing, they are mathematically\nintractable what makes their utility doubtful. In this case a possible issue is\nto use simulation methods in order to get more useful results. Indeed, using\nsimulation, some experiences may be performed and the respective results used\nto conjecture about certain queue systems interesting quantities. In this paper\nthis procedure is followed to learn something more about quantities of interest\nfor those infinite servers queue systems, in particular about busy period\nparameters and probability distributions.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.09526v1"
    },
    {
        "title": "How to Schedule Near-Optimally under Real-World Constraints",
        "authors": [
            "Ziv Scully",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Scheduling is a critical part of practical computer systems, and scheduling\nhas also been extensively studied from a theoretical perspective.\nUnfortunately, there is a gap between theory and practice, as the optimal\nscheduling policies presented by theory can be difficult or impossible to\nperfectly implement in practice. In this work, we use recent breakthroughs in\nqueueing theory to begin to bridge this gap. We show how to translate\ntheoretically optimal policies -- which provably minimize mean response time\n(a.k.a. latency) -- into near-optimal policies that are easily implemented in\npractical settings. Specifically, we handle the following real-world\nconstraints:\n  - We show how to schedule in systems where job sizes (a.k.a. running time)\nare unknown, or only partially known. We do so using simple policies that\nachieve performance very close to the much more complicated theoretically\noptimal policies.\n  - We show how to schedule in systems that have only a limited number of\npriority levels available. We show how to adapt theoretically optimal policies\nto this constrained setting and determine how many levels we need for\nnear-optimal performance.\n  - We show how to schedule in systems where job preemption can only happen at\nspecific checkpoints. Adding checkpoints allows for smarter scheduling, but\neach checkpoint incurs time overhead. We give a rule of thumb that\nnear-optimally balances this tradeoff.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.11579v1"
    },
    {
        "title": "Experience with PCIe streaming on FPGA for high throughput ML\n  inferencing",
        "authors": [
            "Piyush Manavar",
            "Manoj Nambiar",
            "Nupur Sumeet",
            "Rekha Singhal",
            "Sharod Choudhary",
            "Amey Pandit"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Achieving maximum possible rate of inferencing with minimum hardware\nresources plays a major role in reducing enterprise operational costs. In this\npaper we explore use of PCIe streaming on FPGA based platforms to achieve high\nthroughput. PCIe streaming is a unique capability available on FPGA that\neliminates the need for memory copy overheads. We have presented our results\nfor inferences on a gradient boosted trees model, for online retail\nrecommendations. We compare the results achieved with the popular library\nimplementations on GPU and the CPU platforms and observe that the PCIe\nstreaming enabled FPGA implementation achieves the best overall measured\nperformance. We also measure power consumption across all platforms and find\nthat the PCIe streaming on FPGA platform achieves the 25x and 12x better energy\nefficiency than an implementation on CPU and GPU platforms, respectively. We\ndiscuss the conditions that need to be met, in order to achieve this kind of\nacceleration on the FPGA. Further, we analyze the run time statistics on GPU\nand FPGA and identify opportunities to enhance performance on both the\nplatforms.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.11719v1"
    },
    {
        "title": "REMR: A Reliability Evaluation Method for Dynamic Edge Computing Network\n  under Time Constraints",
        "authors": [
            "Liang Chen",
            "Jianpeng Qi",
            "Xiao Su",
            "Rui Wang"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  While the concept of Artificial Intelligent Internet of Things\\ (AIoT) is\nbooming, computation and/or communication-intensive tasks accompanied by\nseveral sub-tasks are slowly moving from centralized deployment to edge-side\ndeployment. The idea of edge computing also makes intelligent services sink\nlocally. But in actual scenarios like dynamic edge computing networks (DECN),\ndue to fluctuations in available computing resources of intermediate servers\nand changes in bandwidth during data transmission, service reliability becomes\ndifficult to guarantee. Coupled with changes in the amount of data in a\nservice, the above three problems all make the existing reliability evaluation\nmethods no longer accurate. To study the effect of distributed service\ndeployment strategies under such a background, this paper proposes a\nreliability evaluation method (REMR) based on lower boundary rule under time\nconstraint to study the degree of the rationality of a service deployment plan\ncombined with DECN. In this scenario, time delay is the main concern which\nwould be affected by three quantitative factors: data packet storing and\nsending time, data transmission time and the calculation time of executing\nsub-tasks on the node devices, specially while the last two are in dynamic\nscenarios. In actual calculation, based on the idea of the minimal paths, the\nsolution set would to be found that can meet requirements in the current\ndeployment. Then the reliability of the service supported by the solution sets\nwould be found out based on the principle of inclusion-exclusion combined with\nthe distribution of available data transmission bandwidth and the distribution\nof node available computing resources. Besides a illustrative example was\nprovided, to verify the calculated reliability of the designed service\ndeployment plan, the NS3 is utilized along with Google cluster data set for\nsimulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01913v1"
    },
    {
        "title": "Meterstick: Benchmarking Performance Variability in Cloud and\n  Self-hosted Minecraft-like Games Extended Technical Report",
        "authors": [
            "Jerrit Eickhoff",
            "Jesse Donkervliet",
            "Alexandru Iosup"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Due to increasing popularity and strict performance requirements, online\ngames have become a workload of interest for the performance engineering\ncommunity. One of the most popular types of online games is the Minecraft-like\nGame (MLG), in which players can terraform the environment. The most popular\nMLG, Minecraft, provides not only entertainment, but also educational support\nand social interaction, to over 130 million people world-wide. MLGs currently\nsupport their many players by replicating isolated instances that support each\nonly up to a few hundred players under favorable conditions. In practice, as we\nshow here, the real upper limit of supported players can be much lower. In this\nwork, we posit that performance variability is a key cause for the lack of\nscalability in MLGs. We propose a novel operational model for MLGs and use it\nto design the first benchmark that focuses on MLG performance variability,\ndefining specialized workloads, metrics, and processes. We conduct real-world\nbenchmarking of MLGs and find environment-based workloads and cloud deployment\nto be significant sources of performance variability: peak-latency degrades\nsharply to 20.7 times the arithmetic mean, and exceeds by a factor of 7.4 the\nperformance requirements. We derive actionable insights for game-developers,\ngame-operators, and other stakeholders to tame performance variability.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06963v2"
    },
    {
        "title": "Adding semantics to measurements: Ontology-guided, systematic\n  performance analysis",
        "authors": [
            "Attila Klenik",
            "András Pataricza"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The design and operation of modern software systems exhibit a shift towards\nvirtualization, containerization and service-based orchestration. Performance\ncapacity engineering and resource utilization tuning become priority\nrequirements in such environments.\n  Measurement-based performance evaluation is the cornerstone of capacity\nengineering and designing for performance. Moreover, the increasing complexity\nof systems necessitates rigorous performance analysis approaches. However,\nempirical performance analysis lacks sophisticated model-based support similar\nto the functional design of the system.\n  The paper proposes an ontology-based approach for facilitating and guiding\nthe empirical evaluation throughout its various steps. Hyperledger Fabric\n(HLF), an open-source blockchain platform by the Linux Foundation, is modelled\nand evaluated as a pilot example of the approach, using the standard TPC-C\nperformance benchmark workload.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.11270v1"
    },
    {
        "title": "Porting a benchmark with a classic workload to blockchain: TPC-C on\n  Hyperledger Fabric",
        "authors": [
            "Attila Klenik",
            "Imre Kocsis"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Many cross-organization cooperation applications of blockchain-based\ndistributed ledger technologies (DLT) do not aim at innovation at the\ncooperation pattern level: essentially the same ''business'' is conducted by\nthe parties, but this time without a central party to be trusted with\nbookkeeping. The migration to DLT is expected to have a negative performance\nimpact, but some DLTs, such as Hyperledger Fabric, are accepted to be much\nbetter suited performance-wise to such use cases than others. However, with the\nsomewhat surprising, but ongoing absence of application-level performance\nbenchmarks for DLTs, cross-DLT comparison for \"classic\" workloads and the\nevaluation of the performance impact of \"blockchainification\" is still\nill-supported. We present the design and Hyperledger Caliper-based open\nimplementation of a full port of the classic TPC-C benchmark to Hyperledger\nFabric, complete with a structured approach for transforming the original\ndatabase schema to a smart contract data model. Initial measurements about the\nworkload characteristics that will affect the design of large-scale performance\nevaluations are also included.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.11277v1"
    },
    {
        "title": "Supporting RISC-V Performance Counters through Performance analysis\n  tools for Linux (Perf)",
        "authors": [
            "Joao Mario Domingos",
            "Pedro Tomas",
            "Leonel Sousa"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Increased attention to RISC-V in Cloud, Data Center, Automotive and\nNetworking applications, has been fueling the move of RISC-V to the\nhigh-performance computing scenario. However, lack of powerful performance\nmonitoring tools will result in poorly optimized applications and,\nconsequently, a limited computing performance. While the RISC-V ISA already\ndefines a hardware performance monitor (HPM), current software gives limited\nsupport for monitoring performance. In this paper we introduce extensions and\nmodifications to the Performance analysis tools for Linux(perf/perf_events),\nLinux kernel, and OpenSBI, aiming to achieve full support for the RISC-V\nperformance monitoring specification. Preliminary testing and evaluation was\ncarried out in Linux 5.7 running on a FPGA booted CVA6 CPU, formerly named\nAriane, showing a monitoring overhead of 0.283%.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.11767v1"
    },
    {
        "title": "Performance of Load Balancers with Bounded Maximum Queue Length in case\n  of Non-Exponential Job Sizes",
        "authors": [
            "Tim Hellemans",
            "Grzegorz Kielanski",
            "Benny Van Houdt"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  In large-scale distributed systems, balancing the load in an efficient way is\ncrucial in order to achieve low latency. Recently, some load balancing policies\nhave been suggested which are able to achieve a bounded maximum queue length in\nthe large-scale limit. However, these policies have thus far only been studied\nin case of exponential job sizes. As job sizes are more variable in real\nsystems, we investigate how the performance of these policies (and in\nparticular the value of these bounds) is impacted by the job size distribution.\n  We present a unified analysis which can be used to compute the bound on the\nqueue length in case of phase-type distributed job sizes for four load\nbalancing policies. We find that in most cases, the bound on the maximum queue\nlength can be expressed in closed form. In addition, we obtain job size\n(in)dependent bounds on the expected response time.\n  Our methodology relies on the use of the cavity process. That is, we\nconjecture that the cavity process captures the behaviour of the real system as\nthe system size grows large. For each policy, we illustrate the accuracy of the\ncavity process by means of simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.03905v1"
    },
    {
        "title": "Competitive Online Optimization with Multiple Inventories: A\n  Divide-and-Conquer Approach",
        "authors": [
            "Qiulin Lin",
            "Yanfang Mo",
            "Junyan Su",
            "Minghua Chen"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  We study a competitive online optimization problem with multiple inventories.\nIn the problem, an online decision maker seeks to optimize the allocation of\nmultiple capacity-limited inventories over a slotted horizon, while the\nallocation constraints and revenue function come online at each slot. The\nproblem is challenging as we need to allocate limited inventories under\nadversarial revenue functions and allocation constraints, while our decisions\nare coupled among multiple inventories and different slots. We propose a\ndivide-and-conquer approach that allows us to decompose the problem into\nseveral single inventory problems and solve it in a two-step manner with almost\nno optimality loss in terms of competitive ratio (CR). Our approach provides\nnew angles, insights and results to the problem, which differs from the\nwidely-adopted primal-and-dual framework. Specifically, when the gradients of\nthe revenue functions are bounded in a positive range, we show that our\napproach can achieve a tight CR that is optimal when the number of inventories\nis small, which is better than all existing ones. For an arbitrary number of\ninventories, the CR we achieve is within an additive constant of one to a lower\nbound of the best possible CR among all online algorithms for the problem. We\nfurther characterize a general condition for generalizing our approach to\ndifferent applications. For example, for a generalized one-way trading problem\nwith price elasticity, where no previous results are available, our approach\nobtains an online algorithm that achieves the optimal CR up to a constant\nfactor.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.02975v1"
    },
    {
        "title": "Optimal Decision Making in Active Queue Management",
        "authors": [
            "Sounak Kar",
            "Bastian Alt",
            "Heinz Koeppl",
            "Amr Rizk"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Active Queue Management (AQM) aims to prevent bufferbloat and serial drops in\nrouter and switch FIFO packet buffers that usually employ drop-tail queueing.\nAQM describes methods to send proactive feedback to TCP flow sources to\nregulate their rate using selective packet drops or markings. Traditionally,\nAQM policies relied on heuristics to approximately provide Quality of Service\n(QoS) such as a target delay for a given flow. These heuristics are usually\nbased on simple network and TCP control models together with the monitored\nbuffer filling. A primary drawback of these heuristics is that their way of\naccounting flow characteristics into the feedback mechanism and the\ncorresponding effect on the state of congestion are not well understood. In\nthis work, we show that taking a probabilistic model for the flow rates and the\ndequeueing pattern, a Semi-Markov Decision Process (SMDP) can be formulated to\nobtain an optimal packet-dropping policy. This policy-based AQM, named PAQMAN,\ntakes into account a steady-state model of TCP and a target delay for the\nflows. Additionally, we present an inference algorithm that builds on TCP\ncongestion control in order to calibrate the model parameters governing\nunderlying network conditions. Using simulation, we show that the prescribed\nAQM yields comparable throughput to state-of-the-art AQM algorithms while\nreducing delays significantly.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.10352v2"
    },
    {
        "title": "Markovian Analysis of Coordination Strategies in Tandem Polling Queues\n  with Setups",
        "authors": [
            "Ravi Suman",
            "Ananth Krishnamurthy"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  We analyze a network of tandem polling queues with two stations operating\nunder synchronized polling (SP) and out-of-sync polling (OP) strategies, and\nwith nonzero setups. We conduct an exact analysis using a decomposition\napproach to compare the performance in terms of throughput and mean waiting\ntimes to investigate when one strategy might be preferred over the other. We\nalso numerically investigate the condition for network stability operating\nunder the two strategies and show that polling network is unstable when there\nis bottleneck at downstream stations. We find that the SP strategy outperforms\nthe OP strategy in case of product and station symmetric networks while under\ncertain settings of product and station asymmetry, OP strategy outperforms the\nSP strategy.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.00410v1"
    },
    {
        "title": "CachePerf: A Unified Cache Miss Classifier via Hybrid Hardware Sampling",
        "authors": [
            "Jin Zhou",
            " Steven",
            " Tang",
            "Hanmei Yang",
            "Tongping Liu"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  The cache plays a key role in determining the performance of applications, no\nmatter for sequential or concurrent programs on homogeneous and heterogeneous\narchitecture. Fixing cache misses requires to understand the origin and the\ntype of cache misses. However, this remains to be an unresolved issue even\nafter decades of research. This paper proposes a unified profiling\ntool--CachePerf--that could correctly identify different types of cache misses,\ndifferentiate allocator-induced issues from those of applications, and exclude\nminor issues without much performance impact. The core idea behind CachePerf is\na hybrid sampling scheme: it employs the PMU-based coarse-grained sampling to\nselect very few susceptible instructions (with frequent cache misses) and then\nemploys the breakpoint-based fine-grained sampling to collect the memory access\npattern of these instructions. Based on our evaluation, CachePerf only imposes\n14% performance overhead and 19% memory overhead (for applications with large\nfootprints), while identifying the types of cache misses correctly. CachePerf\ndetected 9 previous-unknown bugs. Fixing the reported bugs achieves from 3% to\n3788% performance speedup. CachePerf will be an indispensable complementary to\nexisting profilers due to its effectiveness and low overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.08943v2"
    },
    {
        "title": "Preventing Outages under Coordinated Cyber-Physical Attack with Secured\n  PMUs",
        "authors": [
            "Yudi Huang",
            "Ting He",
            "Nilanjan Ray Chaudhuri",
            "Thomas La Porta"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Due to the potentially severe consequences of coordinated cyber-physical\nattacks (CCPA), the design of defenses has gained significant attention. A\npopular approach is to eliminate the existence of attacks by either securing\nexisting sensors or deploying secured PMUs. In this work, we improve this\napproach by lowering the defense target from eliminating attacks to preventing\noutages and reducing the required number of PMUs. To this end, we formulate the\nproblem of PMU Placement for Outage Prevention (PPOP) under DC power flow model\nas a tri-level non-linear optimization problem and transform it into a bi-level\nmixed-integer linear programming (MILP) problem. Then, we propose an\nalternating optimization framework to solve PPOP by iteratively adding\nconstraints, for which we develop two constraint generation algorithms. In\naddition, for large-scale grids, we propose a polynomial-time heuristic\nalgorithm to obtain suboptimal solutions. Next, we extend our solution to\nachieve the defense goal under AC power flow model. Finally, we evaluate our\nalgorithm on IEEE 30-bus, 57-bus, 118-bus, and 300-bus systems, which\ndemonstrates the potential of the proposed approach in greatly reducing the\nrequired number of PMUs.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.13969v1"
    },
    {
        "title": "Towards Comparing Performance of Algorithms in Hardware and Software",
        "authors": [
            "Maja H. Kirkeby",
            "Martin Schoeberl"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  In this paper, we report on a preliminary investigation of the potential\nperformance gain of programs implemented in field-programmable gate arrays\n(FPGAs) using a high-level language Chisel compared to ordinary high-level\nsoftware implementations executed on general-purpose computers and small and\ncheap computers. FPGAs inherently support parallel evaluations, while\nsequential computers do not. For this preliminary investigation, we have chosen\na highly parallelizable program as a case study to show an upper bound of\nperformance gain. The purpose is to demonstrate whether or not programming\nFPGAs has the potential for performance optimizations of ordinary programs. We\nhave developed and evaluated Conway's Game of Life for an FPGA, a small and\ncheap computer Raspberry Pi 4, and a MacBook Pro Laptop. We have compared the\nperformance of programs over different input sizes to decide the relative\nincrease in runtime.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.03394v2"
    },
    {
        "title": "Energy-Efficient Data Transfer Optimization via Decision-Tree Based\n  Uncertainty Reduction",
        "authors": [
            "Hasibul Jamil",
            "Lavone Rodolph",
            "Jacob Goldverg",
            "Tevfik Kosar"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  The increase and rapid growth of data produced by scientific instruments, the\nInternet of Things (IoT), and social media is causing data transfer performance\nand resource consumption to garner much attention in the research community.\nThe network infrastructure and end systems that enable this extensive data\nmovement use a substantial amount of electricity, measured in terawatt-hours\nper year. Managing energy consumption within the core networking infrastructure\nis an active research area, but there is a limited amount of work on reducing\npower consumption at the end systems during active data transfers. This paper\npresents a novel two-phase dynamic throughput and energy optimization model\nthat utilizes an offline decision-search-tree based clustering technique to\nencapsulate and categorize historical data transfer log information and an\nonline search optimization algorithm to find the best application and kernel\nlayer parameter combination to maximize the achieved data transfer throughput\nwhile minimizing the energy consumption. Our model also incorporates an\nensemble method to reduce aleatoric uncertainty in finding optimal application\nand kernel layer parameters during the offline analysis phase. The experimental\nevaluation results show that our decision-tree based model outperforms the\nstate-of-the-art solutions in this area by achieving 117% higher throughput on\naverage and also consuming 19% less energy at the end systems during active\ndata transfers.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.07601v2"
    },
    {
        "title": "LwHBench: A low-level hardware component benchmark and dataset for\n  Single Board Computers",
        "authors": [
            "Pedro Miguel Sánchez Sánchez",
            "José María Jorquera Valero",
            "Alberto Huertas Celdrán",
            "Gérôme Bovet",
            "Manuel Gil Pérez",
            "Gregorio Martínez Pérez"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  In today's computing environment, where Artificial Intelligence (AI) and data\nprocessing are moving toward the Internet of Things (IoT) and Edge computing\nparadigms, benchmarking resource-constrained devices is a critical task to\nevaluate their suitability and performance. Between the employed devices,\nSingle-Board Computers arise as multi-purpose and affordable systems. The\nliterature has explored Single-Board Computers performance when running\nhigh-level benchmarks specialized in particular application scenarios, such as\nAI or medical applications. However, lower-level benchmarking applications and\ndatasets are needed to enable new Edge-based AI solutions for network, system\nand service management based on device and component performance, such as\nindividual device identification. Thus, this paper presents LwHBench, a\nlow-level hardware benchmarking application for Single-Board Computers that\nmeasures the performance of CPU, GPU, Memory and Storage taking into account\nthe component constraints in these types of devices. LwHBench has been\nimplemented for Raspberry Pi devices and run for 100 days on a set of 45\ndevices to generate an extensive dataset that allows the usage of AI techniques\nin scenarios where performance data can help in the device management process.\nBesides, to demonstrate the inter-scenario capability of the dataset, a series\nof AI-enabled use cases about device identification and context impact on\nperformance are presented as exploration of the published data. Finally, the\nbenchmark application has been adapted and applied to an agriculture-focused\nscenario where three RockPro64 devices are present.\n",
        "pdf_link": "http://arxiv.org/pdf/2204.08516v2"
    },
    {
        "title": "Multimedia Services Placement Algorithm for Cloud-Fog Hierarchical\n  Environments",
        "authors": [
            "Fillipe Santos",
            "Roger Immich",
            "Edmundo R. M. Madeira"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  With the rapid development of mobile communication, multimedia services have\nexperienced explosive growth in the last few years. The high quantity of mobile\nusers, both consuming and producing these services to and from the Cloud\nComputing (CC), can outpace the available bandwidth capacity. Fog Computing\n(FG) presents itself as a solution to improve on this and other issues. With a\nreduction in network latency, real-time applications benefit from improved\nresponse time and greater overall user experience. Taking this into account,\nthe main goal of this work is threefold. Firstly, it is proposed a method to\nbuild an environment based on Cloud-Fog Computing (CFC). Secondly, it is\ndesigned two models based on Autoregressive Integrated Moving Average (ARIMA)\nand Long Short-Term Memory (LSTM). The goal is to predict demand and reserve\nthe nodes' storage capacity to improve the positioning of multimedia services.\nLater, an algorithm for the multimedia service placement problem which is aware\nof data traffic prediction is proposed. The goal is to select the minimum\nnumber of nodes, considering their hardware capacities for providing multimedia\nservices in such a way that the latency for servicing all the demands is\nminimized. An evaluation with actual data showed that the proposed algorithm\nselects the nodes closer to the user to meet their demands. This improves the\nservices delivered to end-users and enhances the deployed network to mitigate\nprovider costs. Moreover, reduce the demand to Cloud allowing turning off\nservers in the data center not to waste energy\n",
        "pdf_link": "http://arxiv.org/pdf/2204.11321v1"
    },
    {
        "title": "Efficient LSM-Tree Key-Value Data Management on Hybrid SSD/HDD Zoned\n  Storage",
        "authors": [
            "Jinhong Li",
            "Qiuping Wang",
            "Patrick P. C. Lee"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Zoned storage devices, such as zoned namespace (ZNS) solid-state drives\n(SSDs) and host-managed shingled magnetic recording (HM-SMR) hard-disk drives\n(HDDs), expose interfaces for host-level applications to support fine-grained,\nhigh-performance storage management. Combining ZNS SSDs and HM-SMR HDDs into a\nunified hybrid storage system is a natural direction to scale zoned storage at\nlow cost, yet how to effectively incorporate zoned storage awareness into\nhybrid storage is a non-trivial issue. We make a case for key-value (KV) stores\nbased on log-structured merge trees (LSM-trees) as host-level applications, and\npresent HHZS, a middleware system that bridges an LSM-tree KV store with hybrid\nzoned storage devices based on hints. HHZS leverages hints issued by the\nflushing, compaction, and caching operations of the LSM-tree KV store to manage\nKV objects in placement, migration, and caching in hybrid ZNS SSD and HM-SMR\nHDD zoned storage. Experiments show that our HHZS prototype, when running on\nreal ZNS SSD and HM-SMR HDD devices, achieves the highest throughput compared\nwith all baselines under various settings.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.11753v1"
    },
    {
        "title": "Scheduling to Optimize Sojourn Time of Successful Jobs",
        "authors": [
            "Yuan Yao",
            "Marco Paolieri",
            "Leana Golubchik"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Deep neural networks training jobs and other iterative computations\nfrequently include checkpoints where jobs can be canceled based on the current\nvalue of monitored metrics. While most of existing results focus on the\nperformance of all jobs (both successfully completed and canceled), in this\nwork we explore scheduling policies that improve the sojourn time of successful\njobs, which are typically more valuable to the user. Our model assumes that\neach job has a known discrete size distribution (e.g., estimated from previous\nexecution logs) where the largest size value indicates a successful completion,\nwhile other size values correspond to termination checkpoints. In the\nsingle-server case where all jobs are available for scheduling simultaneously,\nwe prove that optimal schedules do not preempt jobs, even when preemption\noverhead is negligible. Based on this, we develop a scheduling policy that\nminimizes the sojourn time of successful jobs asymptotically, i.e., when the\nnumber of jobs grows to infinity. Through an extensive numerical study, we show\nthat this policy performs better than existing alternatives even when the\nnumber of jobs is finite. For more realistic scenarios with multiple servers\nand dynamic jobs arrivals, we propose an online approach based on our\nsingle-server scheduling policy. Through an extensive simulation study, using\nreal-world traces, we demonstrate that this online approach results in better\naverage sojourn time for successful jobs as compared to existing techniques.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.12891v2"
    },
    {
        "title": "A Framework for Simulating Real-world Stream Data of the Internet of\n  Things",
        "authors": [
            "Weirong Xiu",
            "Baozhu Li",
            "Xusheng Du",
            "Zheng Chu"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  With the rapid growth in the number of devices of the Internet of Things\n(IoT), the volume and types of stream data are rapidly increasing in the real\nworld. Unfortunately, the stream data has the characteristics of infinite and\nperiodic volatility in the real world, which cause problems with the\ninefficient stream processing tasks. In this study, we report our recent\nefforts on this issue, with a focus on simulating stream data. Firstly, we\nexplore the characteristics of the real-world stream data of the IoT, which\nhelps us to understand the stream data in the real world. Secondly, the\npipeline of simulating stream data is proposed, which can accurately and\nefficiently simulate the characteristics of the stream data to improve\nefficiency for specific tasks. Finally, we design and implement a novel\nframework that can simulate various stream data for related stream processing\ntasks. To verify the validity of the proposed framework, we apply this\nframework to stream processing task running in the stream processing system.\nThe experimental results reveal that the related stream processing task is\naccelerated by at least 24 times using our proposed simulation framework with\nthe premise of ensuring volatility and trends of stream data.\n",
        "pdf_link": "http://arxiv.org/pdf/2205.14244v2"
    },
    {
        "title": "INTERPLAY: An Intelligent Model for Predicting Performance Degradation\n  due to Multi-cache Way-disabling",
        "authors": [
            "Panagiota Nikolaou",
            "Yiannakis Sazeides",
            "Maria K. Michael"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Modern and future processors need to remain functionally correct in the\npresence of permanent faults to sustain scaling benefits and limit field\nreturns. This paper presents a combined analytical and microarchitectural\nsimulation-based framework called INTERPLAY, that can rapidly predict, at\ndesign-time, the performance degradation expected from a processor employing\nway-disabling to handle permanent faults in caches while in-the-field. The\nproposed model can predict a program's performance with an accuracy of up to\n98.40% for a processor with a two-level cache hierarchy, when multiple caches\nsuffer from faults and need to disable one or more of their ways. INTERPLAY is\n9.2x faster than an exhaustive simulation approach since it only needs the\ntraining simulation runs for the single-cache way-disabling configurations to\npredict the performance for any multi-cache way-disabling configuration.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.11566v1"
    },
    {
        "title": "Rate Lifting for Stochastic Process Algebra: Exploiting Structural\n  Properties",
        "authors": [
            "Markus Siegle",
            "Amin Soltanieh"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  This report presents an algorithm for determining the unknown rates in the\nsequential processes of a Stochastic Process Algebra model, provided that the\nrates in the combined flat model are given. Such a rate lifting is useful for\nmodel reengineering and model repair. Technically, the algorithm works by\nsolving systems of nonlinear equations and, if necessary, adjusting the model`s\nsynchronisation structure without changing its transition system. This report\ncontains the complete pseudo-code of the algorithm. The approach taken by the\nalgorithm exploits some structural properties of Stochastic Process Algebra\nsystems, which are formulated here for the first time and could be very\nbeneficial also in other contexts.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.14505v1"
    },
    {
        "title": "Cluster Resource Management for Dynamic Workloads by Online Optimization",
        "authors": [
            "Nader Alfares",
            "George Kesidis",
            "Ata Fatahi Baarzi",
            "Aman Jain"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Over the past ten years, many different approaches have been proposed for\ndifferent aspects of the problem of resources management for long running,\ndynamic and diverse workloads such as processing query streams or distributed\ndeep learning. Particularly for applications consisting of containerized\nmicroservices, researchers have attempted to address problems of dynamic\nselection of, for example: types and quantities of virtualized services (e.g.,\nIaaS/VMs), vertical and horizontal scaling of different microservices,\nassigning microservices to VMs, task scheduling, or some combination thereof.\nIn this context, we argue that frameworks like simulated annealing are highly\nsuitable for online navigation of trade-offs between performance (SLO) and\ncost, particularly when the complex workloads and cloud-service offerings vary\nover time. Based on a macroscopic objective that combines both performance and\ncost terms, annealing facilitates light-weight and coherent policies of\nexploration and exploitation. In this paper, we first give some background on\nsimulated annealing and then experimentally demonstrate its usefulness for\ndifferent case studies, including service selection for both a single type of\nworkload (e.g., distributed deep learning) and a mixture of workload types\n(exploring a partially categorical set of options), and container sizing for\nmicroservice benchmarks. We conclude with a discussion of how the basic\nannealing platform can be applied to other resource-management problems,\nhybridized with other methods, and accommodate user-specified rules of thumb.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.04594v2"
    },
    {
        "title": "Perun: Performance Version System",
        "authors": [
            "Tomáš Fiedor",
            "Jiří Pavela",
            "Adam Rogalewicz",
            "Tomáš Vojnar"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  In this paper, we present Perun: an open-source tool suite for\nprofiling-based performance analysis. At its core, Perun maintains links\nbetween project versions and the corresponding stored performance profiles,\nwhich are then leveraged for automated detection of performance changes in new\nproject versions. The Perun tool suite further includes multiple profilers (and\nis designed such that further profilers can be easily added), a performance\nfuzz-tester for workload generation, methods for deriving performance models,\nand numerous visualization methods. We demonstrate how Perun can help\ndevelopers to analyze their program performance on two examples: detection and\nlocalization of a performance degradation and generation of inputs forcing\nperformance issues to show up.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.12900v2"
    },
    {
        "title": "A Probabilistic Bound for Peak Age of Information Guarantee",
        "authors": [
            "Ailing Zhong",
            "Zhidu Li",
            "Tong Tang",
            "Dapeng Wu",
            "Ruyan Wang",
            "Yuming Jiang"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  This paper considers the distribution of a general peak age of information\n(AoI) model and develops a general analysis approach for probabilistic\nperformance guarantee from the time-domain perspective. Firstly, a general\nrelationship between the peak AoI and the inter-arrival and service times of\npackets is revealed. With the help of martingale theory, a probabilistic bound\non the peak AoI is then derived for the general case of endogenous\nindependently and identically distributed increments in information generation\nand transmission processes. Thereafter, the application of the obtained bound\nis illustrated with the M/M/1 and D/M/1 queuing models. The validity of the\nproposed bound is finally examined with numerical results.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.03022v1"
    },
    {
        "title": "A Software Package for Queueing Networks and Markov Chains analysis",
        "authors": [
            "Moreno Marzolla"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Queueing networks and Markov chains are widely used for conducting\nperformance and reliability studies. In this paper we describe the queueing\npackage, a free software package for queueing networks and Markov chain\nanalysis for GNU Octave. The queueing package provides implementations of\nnumerical algorithms for computing transient and steady-state performance\nmeasures of discrete and continuous Markov chains, and for steady-state\nanalysis of single-station queueing systems and queueing networks. We\nillustrate the design principles of the queueing package, describe its most\nsalient features and provide some usage examples.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.04220v1"
    },
    {
        "title": "Exploring the Effects of Multicast Communication on DDS Performance",
        "authors": [
            "Kaleem Peeroo",
            "Peter Popov",
            "Vladimir Stankovic"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  The Data Distribution Service (DDS) is an Object Management Group (OMG)\nstandard for high-performance and real-time systems. DDS is a data-centric\nmiddleware based on the publish-subscribe communication pattern and is used in\nmany mission-critical, or even safety-critical, systems such as air traffic\ncontrol and robot operating system (ROS2).\n  This research aims at identifying how the usage of multicast affects the\nperformance of DDS communication for varying numbers of participants\n(publishers and subscribers). The results show that DDS configured for\nmulticast communication can exhibit worse performance under a high load (a\ngreater number of participants) than DDS configured for unicast communication.\nThis counter-intuitive result reinforces the need for researchers and\npractitioners to be clear about the details of how multicast communication\noperates on the network.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.09001v1"
    },
    {
        "title": "Matching Queues with Abandonments in Quantum Switches: Stability and\n  Throughput Analysis",
        "authors": [
            "Martin Zubeldia",
            "Prakirt R. Jhunjhunwala",
            "Siva Theja Maguluri"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Inspired by quantum switches, we consider a discrete-time multi-way matching\nsystem with two classes of arrivals: requests for entangled pair of qubits\nbetween two nodes, and qubits from each node that can be used to serve the\nrequests. An important feature of this model is that qubits decohere and so\nabandon over time. In contrast to classical server-based queueing models, the\ncombination of queueing, server-less multi-way matching, and (potentially\ncorrelated) abandonments make the analysis a challenging problem. The primary\nfocus of this paper is to study a simple system consisting of two types of\nrequests and three types of qubits operating under a Max-Weight policy.\n  In this setting, we characterize the stability region under the Max-Weight\npolicy by adopting a two-time scale fluid limit to get a handle on the\nabandonments. In particular, we show that Max-Weight is throughput optimal and\nthat it can achieve throughputs larger than the ones that can be achieved by\nnon-idling policies when the requests are infinitely backlogged. Moreover,\ndespite the use of the Max-Weight policy, we show that there can be a\ncounter-intuitive behavior in the system: the longest requests queue can have a\npositive drift for some time even if the overall system is stable.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.12324v4"
    },
    {
        "title": "Universal Policy Tracking: Scheduling for Wireless Networks with Delayed\n  State Observation",
        "authors": [
            "Bai Liu",
            "Eytan Modiano"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Numerous scheduling algorithms have been proposed to optimize various\nperformance metrics like throughput, delay and utility in wireless networks.\nHowever, these algorithms often require instantaneous access to network state\ninformation, which is not always available. While network stability can\nsometimes be achieved with delayed state information, other performance metrics\nsuch as latency may degrade. Thus, instead of simply stabilizing the system,\nour goal is to design a framework that can mimic arbitrary scheduling\nalgorithms with performance guarantees. A naive approach is to make decisions\ndirectly with delayed information, but we show that such methods may lead to\npoor performance. Instead, we propose the Universal Tracking (UT) algorithm\nthat can mimic the actions of arbitrary scheduling algorithms under observation\ndelay. We rigorously show that the performance gap between UT and the\nscheduling algorithm being tracked is bounded by constants. Our numerical\nexperiments show that UT significantly outperforms the naive approach in\nvarious applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.13812v2"
    },
    {
        "title": "PMT: Power Measurement Toolkit",
        "authors": [
            "Stefano Corda",
            "Bram Veenboer",
            "Emma Tolley"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Efficient use of energy is essential for today's supercomputing systems, as\nenergy cost is generally a major component of their operational cost. Research\ninto \"green computing\" is needed to reduce the environmental impact of running\nthese systems. As such, several scientific communities are evaluating the\ntrade-off between time-to-solution and energy-to-solution. While the runtime of\nan application is typically easy to measure, power consumption is not.\nTherefore, we present the Power Measurement Toolkit (PMT), a high-level\nsoftware library capable of collecting power consumption measurements on\nvarious hardware. The library provides a standard interface to easily measure\nthe energy use of devices such as CPUs and GPUs in critical application\nsections.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.03724v2"
    },
    {
        "title": "OpenStack and Google Cloud performance comparison in Infrastructure as a\n  Service model",
        "authors": [
            "Michał Łątkowski",
            "Robert Nowak"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Cloud computing is becoming common, and the choice of proper infrastructure\nis essential. One of main issues is choosing between private and public clound,\nbetween commercial and non-commercial solutions. This paper aims to compare the\nparameters of OpenStack and Google Cloud systems. Both systems deliver a\ncomputing cloud service, enabling the user to use the infrastructure as a\nservice (IaaS) model. We developed the pipeline using the Python programming\nlanguage and its libraries, which enable communication with the aforementioned\nclouds. We measured various parameters of instances and task execution:\ninstance launch and deletion times, and their dependence on the number of\nlaunched instances. Moreover, we used benchmark algorithms to check the\ninstance performance. We analysed the results and the factors that contributed\nto them and provided conclusions, recommendations, and suggestions for further\nresearch based on the gathered data.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.09691v1"
    },
    {
        "title": "Workload Similarity Analysis using Machine Learning Techniques",
        "authors": [
            "Ashish Ledalla",
            "Vineet Singh",
            "Deepak Mishra"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Finding the similarity between two workload behaviors is helpful in 1.\ncreating proxy workloads 2. characterizing an unknown workload's behavior by\nmatching its behavior against known workloads. In this article, we propose a\nmethod to measure the similarity between two workloads using machine\nlearning-based analysis of the performance telemetry data collected for the\nexecution runs of the two workloads. We also demonstrate the accuracy of the\ntechnique by measuring the similarity between a variety of know benchmark\nworkloads.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.13757v2"
    },
    {
        "title": "Violation Probabilities of AoI and PAoI and Optimal Arrival Rate\n  Allocation for the IoT-based Multi-Source Status Update System",
        "authors": [
            "Tianci Zhang",
            "Shutong Chen",
            "Zhengchuan Chen",
            "Zhong Tian",
            "Yunjian Jia",
            "Min Wang",
            "Dapeng Oliver Wu"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Lots of real-time applications over Internet of things (IoT)-based status\nupdate systems have imperative demands on information freshness, which is\nusually evaluated by age of information (AoI). Compared to the average AoI and\npeak AoI (PAoI), violation probabilities and distributions of AoI and PAoI\ncharacterize the timeliness in more details. This paper studies the timeliness\nof the IoT-based multi-source status update system. By modeling the system as a\nmulti-source M/G/1/1 bufferless preemptive queue, general formulas of violation\nprobabilities and probability density functions (p.d.f.s) of AoI and PAoI are\nderived with a time-domain approach. For the case with negativeexponentially\ndistributed service time, the violation probabilities and p.d.f.s are obtained\nin closed form. Moreover, the maximal violation probabilities of AoI and PAoI\nare proposed to characterize the overall timeliness. To improve the overall\ntimeliness under the resource constraint of IoT-device, the arrival rate\nallocation scheme is used to minimize the maximal violation probabilities. It\nis proved that the optimal arrival rates can be found by convex optimization\nalgorithms. In addition, it is obtained that the minimum of maximal violation\nprobability of AoI (or PAoI) is achieved only if all violation probabilities of\nAoI (or PAoI) are equal. Finally, numerical results verify the theoretical\nanalysis and show the effectiveness of the arrival rate allocation scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.16172v1"
    },
    {
        "title": "Towards Maximizing Nonlinear Delay Sensitive Rewards in Queuing Systems",
        "authors": [
            "Sushmitha Shree S",
            "Avijit Mandal",
            "Avhishek Chatterjee",
            "Krishna Jagannathan"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  We consider maximizing the long-term average reward in a single server queue,\nwhere the reward obtained for a job is a non-increasing function of its sojourn\ntime. The motivation behind this work comes from multiple applications,\nincluding quantum information processing and multimedia streaming. We introduce\na new service discipline, shortest predicted sojourn time (SPST), which, in\nsimulations, performs better than well-known disciplines. We also present some\nlimited analytical guarantees for this highly intricate problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.00367v1"
    },
    {
        "title": "Optimal Scheduling in the Multiserver-job Model under Heavy Traffic",
        "authors": [
            "Isaac Grosof",
            "Ziv Scully",
            "Mor Harchol-Balter",
            "Alan Scheller-Wolf"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Multiserver-job systems, where jobs require concurrent service at many\nservers, occur widely in practice. Essentially all of the theoretical work on\nmultiserver-job systems focuses on maximizing utilization, with almost nothing\nknown about mean response time. In simpler settings, such as various known-size\nsingle-server-job settings, minimizing mean response time is merely a matter of\nprioritizing small jobs. However, for the multiserver-job system, prioritizing\nsmall jobs is not enough, because we must also ensure servers are not\nunnecessarily left idle. Thus, minimizing mean response time requires\nprioritizing small jobs while simultaneously maximizing throughput. Our\nquestion is how to achieve these joint objectives.\n  We devise the ServerFilling-SRPT scheduling policy, which is the first policy\nto minimize mean response time in the multiserver-job model in the heavy\ntraffic limit. In addition to proving this heavy-traffic result, we present\nempirical evidence that ServerFilling-SRPT outperforms all existing scheduling\npolicies for all loads, with improvements by orders of magnitude at higher\nloads.\n  Because ServerFilling-SRPT requires knowing job sizes, we also define the\nServerFilling-Gittins policy, which is optimal when sizes are unknown or\npartially known.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.02762v1"
    },
    {
        "title": "MMBench: Benchmarking End-to-End Multi-modal DNNs and Understanding\n  Their Hardware-Software Implications",
        "authors": [
            "Cheng Xu",
            "Xiaofeng Hou",
            "Jiacheng Liu",
            "Chao Li",
            "Tianhao Huang",
            "Xiaozhi Zhu",
            "Mo Niu",
            "Lingyu Sun",
            "Peng Tang",
            "Tongqiao Xu",
            "Kwang-Ting Cheng",
            "Minyi Guo"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  The explosive growth of various types of big data and advances in AI\ntechnologies have catalyzed a new type of workloads called multi-modal DNNs.\nMulti-modal DNNs are capable of interpreting and reasoning about information\nfrom multiple modalities, making them more applicable to real-world AI\nscenarios. In recent research, multi-modal DNNs have outperformed the best\nuni-modal DNN in a wide range of distributed computing applications from\ntraditional multimedia systems to emerging autonomous edge systems. However,\ndespite their importance and superiority, very limited research attention has\nbeen devoted to understand the characteristics of multi-modal DNNs and their\nimplications on current computing software/hardware platforms. Existing\nbenchmarks either target uni-modal DNNs or only focus on the algorithm\ncharacteristics of multi-modal DNNs. There lacks representative benchmark\nsuites that provide comprehensive system and architecture level analysis of\nmulti-modal networks.\n  To advance the understanding of these multi-modal DNN workloads and\nfacilitate related research, we present MMBench, an open-source, end-to-end\nbenchmark suite consisting of a set of real-world multi-modal DNN workloads\nwith relevant performance metrics for evaluation. We then use MMBench to\nconduct an in-depth analysis on the characteristics of multi-modal DNNs. We\ndemonstrate their unique characteristics of clear multi-stage execution,\nfrequent synchronization and high heterogeneity, which distinguish them from\nconventional uni-modal DNNs. Finally, we conduct a case study and extend our\nbenchmark to edge devices. We hope that our work can provide insights for\nfuture software/hardware design and optimization to underpin multi-modal DNNs\non both cloud and edge computing platforms.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.01241v4"
    },
    {
        "title": "Queues on interacting networks",
        "authors": [
            "Maria Vlasiou"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Interacting networks are different in nature to single networks. The study of\nqueuing processes on interacting networks is underdeveloped. It presents new\nmathematical challenges and is of importance to applications. This area of\noperations research deserves careful study: queuing theory needs to incorporate\nhigh-order network interactions in the performance analysis of a queuing\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.07945v1"
    },
    {
        "title": "Comparison of Three Job Mapping Algorithms for Supercomputer Resource\n  Managers",
        "authors": [
            "A. V. Baranov",
            "E. A. Kiselev",
            "B. M. Shabanov",
            "A. A. Sorokin",
            "P. N. Telegin"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Performance of supercomputer depends on the quality of resource manager, one\nof its functions is assignment of jobs to the nodes of clusters or MPP\ncomputers. Parts of parallel programs interact with each other with different\nintensity, and mapping of program to supercomputer nodes influence efficiency\nof the run. At each program run graph representing application program is to be\nmapped onto graph of nodes representing a subset of computer system. The both\ngraphs are not known beforehand, hence the mapping must be done in reasonable\ntime while scheduling resources. Three mapping algorithms were explored:\nparallel versions of simulated annealing, genetic and composite algorithms. A\nset of experimental runs with different algorithms parameters was performed,\ncomparison of mapping quality and runtime was made, and suggestions on\napplicability of algorithms for resource managers were provided.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.12443v1"
    },
    {
        "title": "Updates on the Low-Level Abstraction of Memory Access",
        "authors": [
            "Bernhard Manfred Gruber"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Choosing the best memory layout for each hardware architecture is\nincreasingly important as more and more programs become memory bound. For\nportable codes that run across heterogeneous hardware architectures, the choice\nof the memory layout for data structures is ideally decoupled from the rest of\na program. The low-level abstraction of memory access (LLAMA) is a C++ library\nthat provides a zero-runtime-overhead abstraction layer, underneath which\nmemory mappings can be freely exchanged to customize data layouts, memory\naccess and access instrumentation, focusing on multidimensional arrays of\nnested, structured data.\n  After its scientific debut, several improvements and extensions have been\nadded to LLAMA. This includes compile-time array extents for\nzero-memory-overhead views, support for computations during memory access, new\nmappings for bit-packing, switching types, byte-splitting, memory access\ninstrumentation, and explicit SIMD support. This contribution provides an\noverview of recent developments in the LLAMA library.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.08251v2"
    },
    {
        "title": "WPC: Whole-picture Workload Characterization",
        "authors": [
            "Lei Wang",
            "Kaiyong Yang",
            "Chenxi Wang",
            "Wanling Gao",
            "Chunjie Luo",
            "Fan Zhang",
            "Zhongxin Ge",
            "Li Zhang",
            "Guoxin Kang",
            "Jianfeng Zhan"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  This article raises an important and challenging workload characterization\nissue: can we uncover each critical component across the stacks contributing\nwhat percentages to any specific bottleneck? The typical critical components\ninclude languages, programming frameworks, runtime environments, instruction\nset architectures (ISA), operating systems (OS), and microarchitecture.\nTackling this issue could help propose a systematic methodology to guide the\nsoftware and hardware co-design and critical component optimizations. We\npropose a whole-picture workload characterization (WPC) methodology to answer\nthe above issue. In essence, WPC is an iterative ORFE loop consisting of four\nsteps: Observation, Reference, Fusion, and Exploration. WPC observes different\nlevel data (observation), fuses and normalizes the performance data (fusion)\nwith respect to the well-designed standard reference workloads suite\n(reference), and explores the software and hardware co-design space\n(exploration) to investigate the impacts of critical components across the\nstacks. We build and open-source the WPC tool. Our evaluations confirm WPC can\nquantitatively reveal the contributions of the language, framework, runtime\nenvironment, ISA, OS, and microarchitecture to the primary pipeline efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.12954v1"
    },
    {
        "title": "Runtime-Adaptable Selective Performance Instrumentation",
        "authors": [
            "Sebastian Kreutzer",
            "Christian Iwainsky",
            "Marta Garcia-Gasulla",
            "Victor Lopez",
            "Christian Bischof"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Automated code instrumentation, i.e. the insertion of measurement hooks into\na target application by the compiler, is an established technique for\ncollecting reliable, fine-grained performance data. The set of functions to\ninstrument has to be selected with care, as instrumenting every available\nfunction typically yields too large a runtime overhead, thus skewing the\nmeasurement. No \"one-suits-all\" selection mechanism exists, since the\ninstrumentation decision is dependent on the measurement objective, the limit\nfor tolerable runtime overhead and peculiarities of the target application. The\nCompiler-assisted Performance Instrumentation (CaPI) tool assists in creating\nsuch instrumentation configurations, by enabling the user to combine different\nselection mechanisms as part of a configurable selection pipeline, operating on\na statically constructed whole-program call-graph. Previously, CaPI relied on a\nstatic instrumentation workflow which made the process of refining the initial\nselection quite cumbersome for large-scale codes, as the application had to be\nrecompiled after each adjustment. In this work, we present new\nruntime-adaptable instrumentation capabilities for CaPI which do not require\nrecompilation when instrumentation changes are made. To this end, the XRay\ninstrumentation feature of the LLVM compiler was extended to support the\ninstrumentation of shared dynamic objects. An XRay-compatible runtime system\nwas added to CaPI that instruments selected functions at program start, thereby\nsignificantly reducing the required time for selection refinements.\nFurthermore, an interface to the TALP tool for recording parallel efficiency\nmetrics was implemented, alongside a specialized selection module for creating\nsuitable coarse-grained region instrumentations.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.11110v1"
    },
    {
        "title": "Characterizing the Performance of Emerging Deep Learning, Graph, and\n  High Performance Computing Workloads Under Interference",
        "authors": [
            "Hao Xu",
            "Shuang Song",
            "Ze Mao"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Throughput-oriented computing via co-running multiple applications in the\nsame machine has been widely adopted to achieve high hardware utilization and\nenergy saving on modern supercomputers and data centers. However, efficiently\nco-running applications raises new design challenges, mainly because\napplications with diverse requirements can stress out shared hardware resources\n(IO, Network and Cache) at various levels. The disparities in resource usage\ncan result in interference, which in turn can lead to unpredictable co-running\nbehaviors. To better understand application interference, prior work provided\ndetailed execution characterization. However, these characterization approaches\neither emphasize on traditional benchmarks or fall into a single application\ndomain. To address this issue, we study 25 up-to-date applications and\nbenchmarks from various application domains and form 625 consolidation pairs to\nthoroughly analyze the execution interference caused by application co-running.\nMoreover, we leverage mini-benchmarks and real applications to pinpoint the\nprovenance of co-running interference in both hardware and software aspects.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.15763v1"
    },
    {
        "title": "Load Balancing with Job-Size Testing: Performance Improvement or\n  Degradation?",
        "authors": [
            "Jonatha Anselmi",
            "Josu Doncel"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  In the context of decision making under explorable uncertainty, scheduling\nwith testing is a powerful technique used in the management of computer systems\nto improve performance via better job-dispatching decisions. Upon job arrival,\na scheduler may run some \\emph{testing algorithm} against the job to extract\nsome information about its structure, e.g., its size, and properly classify it.\nThe acquisition of such knowledge comes with a cost because the testing\nalgorithm delays the dispatching decisions, though this is under control. In\nthis paper, we analyze the impact of such extra cost in a load balancing\nsetting by investigating the following questions: does it really pay off to\ntest jobs? If so, under which conditions? Under mild assumptions connecting the\ninformation extracted by the testing algorithm in relationship with its running\ntime, we show that whether scheduling with testing brings a performance\ndegradation or improvement strongly depends on the traffic conditions, system\nsize and the coefficient of variation of job sizes. Thus, the general answer to\nthe above questions is non-trivial and some care should be considered when\ndeploying a testing policy. Our results are achieved by proposing a load\nbalancing model for scheduling with testing that we analyze in two limiting\nregimes. When the number of servers grows to infinity in proportion to the\nnetwork demand, we show that job-size testing actually degrades performance\nunless short jobs can be predicted reliably almost instantaneously and the\nnetwork load is sufficiently high. When the coefficient of variation of job\nsizes grows to infinity, we construct testing policies inducing an arbitrarily\nlarge performance gain with respect to running jobs untested.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.00899v2"
    },
    {
        "title": "WASEF: Web Acceleration Solutions Evaluation Framework",
        "authors": [
            "Moumena Chaqfeh",
            "Rashid Tahir",
            "Ayaz Rehman",
            "Jesutofunmi Kupoluyi",
            "Saad Ullah",
            "Russell Coke",
            "Muhammad Junaid",
            "Muhammad Arham",
            "Marc Wiggerman",
            "Abijith Radhakrishnan",
            "Ivano Malavolta",
            "Fareed Zaffar",
            "Yasir Zaki"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  The World Wide Web has become increasingly complex in recent years. This\ncomplexity severely affects users in the developing regions due to slow\ncellular data connectivity and usage of low-end smartphone devices. Existing\nsolutions to simplify the Web are generally evaluated using several different\nmetrics and settings, which hinders the comparison of these solutions against\neach other. Hence, it is difficult to select the appropriate solution for a\nspecific context and use case. This paper presents Wasef, a framework that uses\na comprehensive set of timing, saving, and quality metrics to evaluate and\ncompare different web complexity solutions in a reproducible manner and under\nrealistic settings. The framework integrates a set of existing state-of-the-art\nsolutions and facilitates the addition of newer solutions down the line. Wasef\nfirst creates a cache of web pages by crawling both landing and internal ones.\nEach page in the cache is then passed through a web complexity solution to\ngenerate an optimized version of the page. Finally, each optimized version is\nevaluated in a consistent manner using a uniform environment and metrics. We\ndemonstrate how the framework can be used to compare and contrast the\nperformance characteristics of different web complexity solutions under\nrealistic conditions. We also show that the accessibility to pages in\ndeveloping regions can be significantly improved, by evaluating the top 100\nglobal pages in the developed world against the top 100 pages in the lowest 50\ndeveloping countries. Results show a significant difference in terms of\ncomplexity and a potential benefit for our framework in improving web\naccessibility in these countries.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.09568v1"
    },
    {
        "title": "Analysis and Mitigation of Shared Resource Contention on Heterogeneous\n  Multicore: An Industrial Case Study",
        "authors": [
            "Michael Bechtel",
            "Heechul Yun"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  In this paper, we present a solution to the industrial challenge put forth by\nARM in 2022. We systematically analyze the effect of shared resource contention\nto an augmented reality head-up display (AR-HUD) case-study application of the\nindustrial challenge on a heterogeneous multicore platform, NVIDIA Jetson Nano.\nWe configure the AR-HUD application such that it can process incoming image\nframes in real-time at 20Hz on the platform. We use Microarchitectural\nDenial-of-Service (DoS) attacks as aggressor workloads of the challenge and\nshow that they can dramatically impact the latency and accuracy of the AR-HUD\napplication. This results in significant deviations of the estimated\ntrajectories from known ground truths, despite our best effort to mitigate\ntheir influence by using cache partitioning and real-time scheduling of the\nAR-HUD application. To address the challenge, we propose RT-Gang++, a\npartitioned real-time gang scheduling framework with last-level cache (LLC) and\nintegrated GPU bandwidth throttling capabilities. By applying RT-Gang++, we are\nable to achieve desired level of performance of the AR-HUD application even in\nthe presence of fully loaded aggressor tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13110v3"
    },
    {
        "title": "Understanding the Benefits of Hardware-Accelerated Communication in\n  Model-Serving Applications",
        "authors": [
            "Walid A. Hanafy",
            "Limin Wang",
            "Hyunseok Chang",
            "Sarit Mukherjee",
            "T. V. Lakshman",
            "Prashant Shenoy"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  It is commonly assumed that the end-to-end networking performance of edge\noffloading is purely dictated by that of the network connectivity between end\ndevices and edge computing facilities, where ongoing innovation in 5G/6G\nnetworking can help. However, with the growing complexity of edge-offloaded\ncomputation and dynamic load balancing requirements, an offloaded task often\ngoes through a multi-stage pipeline that spans across multiple compute nodes\nand proxies interconnected via a dedicated network fabric within a given edge\ncomputing facility. As the latest hardware-accelerated transport technologies\nsuch as RDMA and GPUDirect RDMA are adopted to build such network fabric, there\nis a need for good understanding of the full potential of these technologies in\nthe context of computation offload and the effect of different factors such as\nGPU scheduling and characteristics of computation on the net performance gain\nachievable by these technologies. This paper unveils detailed insights into the\nlatency overhead in typical machine learning (ML)-based computation pipelines\nand analyzes the potential benefits of adopting hardware-accelerated\ncommunication. To this end, we build a model-serving framework that supports\nvarious communication mechanisms. Using the framework, we identify performance\nbottlenecks in state-of-the-art model-serving pipelines and show how\nhardware-accelerated communication can alleviate them. For example, we show\nthat GPUDirect RDMA can save 15--50\\% of model-serving latency, which amounts\nto 70--160 ms.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.03165v2"
    },
    {
        "title": "Model-Based Performance Analysis of the HyTeG Finite Element Framework",
        "authors": [
            "Dominik Thönnes",
            "Ulrich Rüde"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  In this work, we present how code generation techniques significantly improve\nthe performance of the computational kernels in the HyTeG software framework.\nThis HPC framework combines the performance and memory advantages of\nmatrix-free multigrid solvers with the flexibility of unstructured meshes. The\npystencils code generation toolbox is used to replace the original abstract C++\nkernels with highly optimized loop nests. The performance of one of those\nkernels (the matrix-vector multiplication) is thoroughly analyzed using the\nExecution-Cache-Memory (ECM) performance model. We validate these predictions\nby measurements on the SuperMUC-NG supercomputer. The experiments show that the\nperformance mostly matches the predictions. In cases where the prediction does\nnot match, we discuss the discrepancies. Additionally, we conduct a node-level\nscaling study which shows the expected behavior for a memory-bound compute\nkernel.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.15116v1"
    },
    {
        "title": "Reconfigurable Intelligent Surface Assisted Free Space Optical\n  Information and Power Transfer",
        "authors": [
            "Wen Fang",
            "Wen Chen",
            "Qingqing Wu",
            "Kunlun Wang",
            "Shunqing Zhang",
            "Qingwen Liu",
            "Jun Li"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Free space optical (FSO) transmission has emerged as a key candidate\ntechnology for 6G to expand new spectrum and improve network capacity due to\nits advantages of large bandwidth, low electromagnetic interference, and high\nenergy efficiency. Resonant beam operating in the infrared band utilizes\nspatially separated laser cavities to enable safe and mobile high-power energy\nand high-rate information transmission but is limited by line-of-sight (LOS)\nchannel. In this paper, we propose a reconfigurable intelligent surface (RIS)\nassisted resonant beam simultaneous wireless information and power transfer\n(SWIPT) system and establish an optical field propagation model to analyze the\nchannel state information (CSI), in which LOS obstruction can be detected\nsensitively and non-line-of-sight (NLOS) transmission can be realized by\nchanging the phased of resonant beam in RIS. Numerical results demonstrate\nthat, apart from the transmission distance, the NLOS performance depends on\nboth the horizontal and vertical positions of RIS. The maximum NLOS energy\nefficiency can achieve 55% within a transfer distance of 10m, a translation\ndistance of $\\pm$4mm, and rotation angle of $\\pm$50{\\deg}.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.07138v1"
    },
    {
        "title": "Empirical Investigation of Factors influencing Function as a Service\n  Performance in Different Cloud/Edge System Setups",
        "authors": [
            "Anastasia-Dimitra Lipitakis",
            "George Kousiouris",
            "Mara Nikolaidou",
            "Cleopatra Bardaki",
            "Dimosthenis Anagnostopoulos"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Experimental data can aid in gaining insights about a system operation, as\nwell as determining critical aspects of a modelling or simulation process. In\nthis paper, we analyze the data acquired from an extensive experimentation\nprocess in a serverless Function as a Service system (based on the open source\nApache Openwhisk) that has been deployed across 3 available cloud/edge\nlocations with different system setups. Thus, they can be used to model\ndistribution of functions through multi-location aware scheduling mechanisms.\nThe experiments include different traffic arrival rates, different setups for\nthe FaaS system, as well as different configurations for the hardware and\nplatform used. We analyse the acquired data for the three FaaS system setups\nand discuss their differences presenting interesting conclusions with relation\nto transient effects of the system, such as the effect on wait and execution\ntime. We also demonstrate interesting trade-offs with relation to system setup\nand indicate a number of factors that can affect system performance and should\nbe taken under consideration in modelling attempts of such systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14312v1"
    },
    {
        "title": "Quantum Computer Simulations at Warp Speed: Assessing the Impact of GPU\n  Acceleration",
        "authors": [
            "Jennifer Faj",
            "Ivy Peng",
            "Jacob Wahlgren",
            "Stefano Markidis"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Quantum computer simulators are crucial for the development of quantum\ncomputing. In this work, we investigate the suitability and performance impact\nof GPU and multi-GPU systems on a widely used simulation tool - the state\nvector simulator Qiskit Aer. In particular, we evaluate the performance of both\nQiskit's default Nvidia Thrust backend and the recent Nvidia cuQuantum backend\non Nvidia A100 GPUs. We provide a benchmark suite of representative quantum\napplications for characterization. For simulations with a large number of\nqubits, the two GPU backends can provide up to 14x speedup over the CPU\nbackend, with Nvidia cuQuantum providing further 1.5-3x speedup over the\ndefault Thrust backend. Our evaluation on a single GPU identifies the most\nimportant functions in Nvidia Thrust and cuQuantum for different quantum\napplications and their compute and memory bottlenecks. We also evaluate the\ngate fusion and cache-blocking optimizations on different quantum applications.\nFinally, we evaluate large-number qubit quantum applications on multi-GPU and\nidentify data movement between host and GPU as the limiting factor for the\nperformance.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14860v1"
    },
    {
        "title": "Optimization of Resources to Minimize Power Dissipation in 5G Wireless\n  Networks",
        "authors": [
            "Jyotsna Rani",
            "Ganesh Prasad"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  In today's modern communications, with evolution of various applications, the\ndemand of data rate is increasing exponentially at the cost of huge consumption\nof available resources. It has been recorded that the communication networks\ndissipate nearly 1\\% of the world-wide total power consumption, results in\nmillions of tons of CO2 emission due to their production and thereby causes\nvarious environmental health hazards. The optimal utilization of available\nresources that can balance the present coexisting problem without any\ncompromise on the high throughput demand, paves the way for the next generation\ngreen 5G wireless networks. In this chapter, we study the minimization of total\npower consumption while satisfying the desired coverage of the user equipments\n(UEs) to provide the minimum throughput over the network. In this regard, the\ndeployment of base stations (BSs), their number, and transmit power are\noptimized in two scenarios (i) when the UEs are large in 5G wireless network\nand (ii) when moderate UEs are distributed over the field.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.09385v1"
    },
    {
        "title": "A Linear Combination-based Method to Construct Proxy Benchmarks for Big\n  Data Workloads",
        "authors": [
            "Yikang Yang",
            "Lei Wang",
            "Jianfeng Zhan"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  During early stages of CPU design, benchmarks can only run on simulators to\nevaluate CPU performance. However, most big data benchmarks are too huge at\ncode size scale, which causes them to be unable to finish running on simulators\nat an acceptable time cost. Moreover, big data benchmarks usually need complex\nsoftware stacks to support their running, which is hard to be ported on\nsimulators. Proxy benchmarks, without long running times and complex software\nstacks, have the same micro-architectural metrics as real benchmarks, which\nmeans they can represent real benchmarks' micro-architectural characteristics.\nTherefore, proxy benchmarks can replace real benchmarks to run on simulators to\nevaluate the CPU performance. The biggest challenge is how to guarantee that\nthe proxy benchmarks have exactly the same micro-architectural metrics as real\nbenchmarks when the number of micro-architectural metrics is very large. To\ndeal with this challenge, we propose a linear combination-based proxy benchmark\ngeneration methodology that transforms this problem into solving a linear\nequation system. We also design the corresponding algorithms to ensure the\nlinear equation is astringency, which means that although sometimes the linear\nequation system doesn't have a unique solution, the algorithm can find the best\nsolution by the non-negative least square method. We generate fifteen proxy\nbenchmarks and evaluate their running time and accuracy in comparison to\ncorresponding real benchmarks for Mysql and RockDB. On the typical Intel Xeon\nplatform, the average running time is 1.62s, and the average accuracy of every\nmicro-architectural metric is over 92%, while the longest running time of real\nbenchmarks is nearly 4 hours. We also conduct two case studies that demonstrate\nthat our proxy benchmarks are consistent with real benchmarks both before and\nafter prefetch or Hyper-Threading is turned on.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.10281v1"
    },
    {
        "title": "The Importance of Worst-Case Memory Contention Analysis for\n  Heterogeneous SoCs",
        "authors": [
            "Lorenzo Carletti",
            "Gianluca Brilli",
            "Alessandro Capotondi",
            "Paolo Valente",
            "Andrea Marongiu"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Memory interference may heavily inflate task execution times in Heterogeneous\nSystems-on-Chips (HeSoCs). Knowing worst-case interference is consequently\nfundamental for supporting the correct execution of time-sensitive\napplications. In most of the literature, worst-case interference is assumed to\nbe generated by, and therefore is estimated through read-intensive synthetic\nworkloads with no caching. Yet these workloads do not always generate\nworst-case interference. This is the consequence of the general results\nreported in this work. By testing on multiple architectures, we determined that\nthe highest interference generation traffic pattern is actually hardware\ndependant, and that making assumptions could lead to a severe underestimation\nof the worst-case (in our case, of more than 9x).\n",
        "pdf_link": "http://arxiv.org/pdf/2309.12864v1"
    },
    {
        "title": "Unbalanced Job Approximation using Taylor Series Expansion and Review of\n  Performance Bounds",
        "authors": [
            "Alexander Thomasian"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Unbalanced Job Approximation - UJA is a family of low-cost formulas to obtain\nthe throughput of Queueing Networks - QNs with fixed rate servers using Taylor\nseries expansion of job loadings with respect to the mean loading. UJA with one\nterm yields the same throughput as optimistic Balanced Job Bound - BJB, which\nat some point exceeds the maximum asymptotic throughput. The accuracy of the\nestimated throughput increases with more terms in the Taylor series. UJA can be\nused in parametric studies by reducing the cost of solving large QNs by\naggregating stations into a single Flow-Equivalent Service Center - FESCs\ndefined by its throughput characteristic. While UJA has been extended to two\nclasses it may be applied to more classes by job class aggregation. BJB has\nbeen extended to QNs with delay servers and multiple jobs classes by Eager and\nSevcik, throughput bounds by Eager and Sevcik, Kriz, Proportional Bound - PB\nand Prop. Approximation Bound - PAM by Hsieh and Lam and Geometric Bound - GB\nby Casale et al. are reviewed.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.15172v1"
    },
    {
        "title": "The RESET and MARC Techniques, with Application to Multiserver-Job\n  Analysis",
        "authors": [
            "Isaac Grosof",
            "Yige Hong",
            "Mor Harchol-Balter",
            "Alan Scheller-Wolf"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Multiserver-job (MSJ) systems, where jobs need to run concurrently across\nmany servers, are increasingly common in practice. The default service ordering\nin many settings is First-Come First-Served (FCFS) service. Virtually all\ntheoretical work on MSJ FCFS models focuses on characterizing the stability\nregion, with almost nothing known about mean response time.\n  We derive the first explicit characterization of mean response time in the\nMSJ FCFS system. Our formula characterizes mean response time up to an additive\nconstant, which becomes negligible as arrival rate approaches throughput, and\nallows for general phase-type job durations.\n  We derive our result by utilizing two key techniques: REduction to Saturated\nfor Expected Time (RESET) and MArkovian Relative Completions (MARC).\n  Using our novel RESET technique, we reduce the problem of characterizing mean\nresponse time in the MSJ FCFS system to an M/M/1 with Markovian service rate\n(MMSR). The Markov chain controlling the service rate is based on the saturated\nsystem, a simpler closed system which is far more analytically tractable.\n  Unfortunately, the MMSR has no explicit characterization of mean response\ntime. We therefore use our novel MARC technique to give the first explicit\ncharacterization of mean response time in the MMSR, again up to constant\nadditive error. We specifically introduce the concept of \"relative\ncompletions,\" which is the cornerstone of our MARC technique.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.01621v1"
    },
    {
        "title": "Spectrum Sharing Towards Delay Deterministic Wireless Network: Delay\n  Performance Analysis",
        "authors": [
            "Zhiqing Wei",
            "Ling Zhang",
            "Gaofeng Nie",
            "Huici Wu",
            "Ning Zhang",
            "Zeyang Meng",
            "Zhiyong Feng"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  To accommodate Machine-type Communication (MTC) service, the wireless network\nneeds to support low-delay and low-jitter data transmission, realizing delay\ndeterministic wireless network. This paper analyzes the delay and jitter of the\nwireless network with and without spectrum sharing. When sharing the spectrum\nof the licensed network, the spectrum band of wireless network can be expanded,\nsuch that the delay and jitter of data transmission are reduced. The challenge\nof this research is to model the relation between the delay/jitter and the\nparameters such as node distribution, transmit power, and bandwidth, etc. To\nthis end, this paper applies stochastic geometry and queueing theory to analyze\nthe outage probability of the licensed network and the delay performance of the\nwireless network with and without spectrum sharing. By establishing the M/G/1\nqueueing model for the queueing of the Base Station (BS) in the wireless\nnetwork, the downlink delay and jitter are derived. Monte Carlo simulation\nresults show that the spectrum sharing reduces the delay and jitter without\ncausing serious interference to the licensed network, which can lay a\nfoundation for the application of spectrum sharing in delay deterministic\nwireless network supporting MTC service.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.07122v1"
    },
    {
        "title": "Collaborative Precoding Design for Adjacent Integrated Sensing and\n  Communication Base Stations",
        "authors": [
            "Wangjun Jiang",
            "Zhiqing Wei",
            "Fan Liu",
            "Zhiyong Feng",
            "Ping Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Integrated sensing and communication (ISAC) base stations can provide\ncommunication and wide range sensing information for vehicles via downlink (DL)\ntransmission, thus enhancing vehicle driving safety. One major challenge for\nrealizing high performance communication and sensing is how to deal with the DL\nmutual interference among adjacent ISAC base stations, which includes not only\ncommunication related interference, but also radar sensing related\ninterference. In this paper, we establish a DL mutual interference model of\nadjacent ISAC base stations, and analyze the relationship for mutual\ninterference channels between communications and radar sensing. To improve the\nsensing and communication performance, we propose a collaborative precoding\ndesign for coordinated adjacent base stations to mitigate the mutual\ninterference under the transmit power constraint and constant modulus\nconstraint, which is formulated as a non-convex optimization problem. We first\nrelax the problem into a convex programming by omitting the rank constraint,\nand propose a joint optimization algorithm to solve the problem. We furthermore\npropose a sequential optimization algorithm, which divides the collaborative\nprecoding design problem into four subproblems and finds the optimum via a\ngradient descent algorithm. Finally, we evaluate the collaborative precoding\ndesign algorithms by considering sensing and communication performance via\nnumerical results.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.08246v1"
    },
    {
        "title": "Facile: Fast, Accurate, and Interpretable Basic-Block Throughput\n  Prediction",
        "authors": [
            "Andreas Abel",
            "Shrey Sharma",
            "Jan Reineke"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Basic-block throughput models such as uiCA, IACA, GRANITE, Ithemal, llvm-mca,\nOSACA, or CQA guide optimizing compilers and help performance engineers\nidentify and eliminate bottlenecks. For this purpose, basic-block throughput\nmodels should ideally be fast, accurate, and interpretable.\n  Recent advances have significantly improved accuracy: uiCA, the\nstate-of-the-art model, achieves an error of about 1% relative to measurements\nacross a wide range of microarchitectures. The computational efficiency of\nthroughput models, which is equally important for widespread adoption,\nespecially in compilers, has so far received little attention.\n  In this paper, we introduce Facile, an analytical throughput model that is\nfast, accurate, and interpretable. Facile analyzes different potential\nbottlenecks independently and analytically. Due to its compositional nature,\nFacile's predictions directly pinpoint the bottlenecks. We evaluate Facile on a\nwide range of microarchitectures and show that it is almost two orders of\nmagnitude faster than existing models while achieving state-of-the-art\naccuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.13212v1"
    },
    {
        "title": "Exploring the Potential of Flexible 8-bit Format: Design and Algorithm",
        "authors": [
            "Zhuoyi Zhang",
            "Yunchen Zhang",
            "Gonglei Shi",
            "Yu Shen",
            "Ruihao Gong",
            "Xiaoxu Xia",
            "Qi Zhang",
            "Lewei Lu",
            "Xianglong Liu"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Neural network quantization is widely used to reduce model inference\ncomplexity in real-world deployments. However, traditional integer quantization\nsuffers from accuracy degradation when adapting to various dynamic ranges.\nRecent research has focused on a new 8-bit format, FP8, with hardware support\nfor both training and inference of neural networks but lacks guidance for\nhardware design. In this paper, we analyze the benefits of using FP8\nquantization and provide a comprehensive comparison of FP8 with INT\nquantization. Then we propose a flexible mixed-precision quantization framework\nthat supports various number systems, enabling optimal selection of the most\nappropriate quantization format for different neural network architectures.\nExperimental results demonstrate that our proposed framework achieves\ncompetitive performance compared to full precision on various tasks, including\nimage classification, object detection, segmentation, and natural language\nunderstanding. Our work furnishes critical insights into the tangible benefits\nand feasibility of employing FP8 quantization, paving the way for heightened\nneural network efficiency in tangible scenarios. Our code is available in the\nsupplementary material.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.13513v2"
    },
    {
        "title": "Modeling and Design of the Communication Sensing and Control Coupled\n  Closed-Loop Industrial System",
        "authors": [
            "Zeyang Meng",
            "Dingyou Ma",
            "Shengfeng Wang",
            "Zhiqing Wei",
            "Zhiyong Feng"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  With the advent of 5G era, factories are transitioning towards wireless\nnetworks to break free from the limitations of wired networks. In 5G-enabled\nfactories, unmanned automatic devices such as automated guided vehicles and\nrobotic arms complete production tasks cooperatively through the periodic\ncontrol loops. In such loops, the sensing data is generated by sensors, and\ntransmitted to the control center through uplink wireless communications. The\ncorresponding control commands are generated and sent back to the devices\nthrough downlink wireless communications. Since wireless communications,\nsensing and control are tightly coupled, there are big challenges on the\nmodeling and design of such closed-loop systems. In particular, existing\ntheoretical tools of these functionalities have different modelings and\nunderlying assumptions, which make it difficult for them to collaborate with\neach other. Therefore, in this paper, an analytical closed-loop model is\nproposed, where the performances and resources of communication, sensing and\ncontrol are deeply related. To achieve the optimal control performance, a\nco-design of communication resource allocation and control method is proposed,\ninspired by the model predictive control algorithm. Numerical results are\nprovided to demonstrate the relationships between the resources and control\nperformances.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.15560v1"
    },
    {
        "title": "OpenMP behavior in low resource and high stress mobile environment",
        "authors": [
            "Kaijun Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  This paper investigates the use of OpenMP for parallel post processing in\nobejct detection on personal Android devices, where resources like\ncomputational power, memory, and battery are limited. Specifically, it explores\nvarious configurations of thread count, CPU affinity, and chunk size on a Redmi\nNote 10 Pro with an ARM Cortex A76 CPU. The study finds that using four threads\noffers a maximum post processing speedup of 2.3x but increases overall\ninference time by 2.7x. A balanced configuration of two threads achieves a 1.8x\nspeedup in post processing and a 2% improvement in overall program performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14171v1"
    },
    {
        "title": "GVEL: Fast Graph Loading in Edgelist and Compressed Sparse Row (CSR)\n  formats",
        "authors": [
            "Subhajit Sahu"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Efficient IO techniques are crucial in high-performance graph processing\nframeworks like Gunrock and Hornet, as fast graph loading can help minimize\nprocessing time and reduce system/cloud usage charges. This research study\npresents approaches for efficiently reading an Edgelist from a text file and\nconverting it to a Compressed Sparse Row (CSR) representation. On a server with\ndual 16-core Intel Xeon Gold 6226R processors and Seagate Exos 10e2400 HDDs,\nour approach, which we term as GVEL, outperforms Hornet, Gunrock, and PIGO by\nsignificant margins in CSR reading, exhibiting an average speedup of 78x, 112x,\nand 1.8x, respectively. For Edgelist reading, GVEL is 2.6x faster than PIGO on\naverage, and achieves a Edgelist read rate of 1.9 billion edges/s. For every\ndoubling of threads, GVEL improves performance at an average rate of 1.9x and\n1.7x for reading Edgelist and reading CSR respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.14650v5"
    },
    {
        "title": "GPU Graph Processing on CXL-Based Microsecond-Latency External Memory",
        "authors": [
            "Shintaro Sano",
            "Yosuke Bando",
            "Kazuhiro Hiwada",
            "Hirotsugu Kajihara",
            "Tomoya Suzuki",
            "Yu Nakanishi",
            "Daisuke Taki",
            "Akiyuki Kaneko",
            "Tatsuo Shiozawa"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  In GPU graph analytics, the use of external memory such as the host DRAM and\nsolid-state drives is a cost-effective approach to processing large graphs\nbeyond the capacity of the GPU onboard memory. This paper studies the use of\nCompute Express Link (CXL) memory as alternative external memory for GPU graph\nprocessing in order to see if this emerging memory expansion technology enables\ngraph processing that is as fast as using the host DRAM. Through analysis and\nevaluation using FPGA prototypes, we show that representative GPU graph\ntraversal algorithms involving fine-grained random access can tolerate an\nexternal memory latency of up to a few microseconds introduced by the CXL\ninterface as well as by the underlying memory devices. This insight indicates\nthat microsecond-latency flash memory may be used as CXL memory devices to\nrealize even more cost-effective GPU graph processing while still achieving\nperformance close to using the host DRAM.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.03113v1"
    },
    {
        "title": "Heavy-Traffic Optimal Size- and State-Aware Dispatching",
        "authors": [
            "Runhan Xie",
            "Isaac Grosof",
            "Ziv Scully"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Dispatching systems, where arriving jobs are immediately assigned to one of\nmultiple queues, are ubiquitous in computer systems and service systems. A\nnatural and practically relevant model is one in which each queue serves jobs\nin FCFS (First-Come First-Served) order. We consider the case where the\ndispatcher is size-aware, meaning it learns the size (i.e. service time) of\neach job as it arrives; and state-aware, meaning it always knows the amount of\nwork (i.e. total remaining service time) at each queue. While size- and\nstate-aware dispatching to FCFS queues has been extensively studied, little is\nknown about optimal dispatching for the objective of minimizing mean delay. A\nmajor obstacle is that no nontrivial lower bound on mean delay is known, even\nin heavy traffic (i.e. the limit as load approaches capacity). This makes it\ndifficult to prove that any given policy is optimal, or even heavy-traffic\noptimal.\n  In this work, we propose the first size- and state-aware dispatching policy\nthat provably minimizes mean delay in heavy traffic. Our policy, called CARD\n(Controlled Asymmetry Reduces Delay), keeps all but one of the queues short,\nthen routes as few jobs as possible to the one long queue. We prove an upper\nbound on CARD's mean delay, and we prove the first nontrivial lower bound on\nthe mean delay of any size- and state-aware dispatching policy. Both results\napply to any number of servers. Our bounds match in heavy traffic, implying\nCARD's heavy-traffic optimality. In particular, CARD's heavy-traffic\nperformance improves upon that of LWL (Least Work Left), SITA (Size Interval\nTask Assignment), and other policies from the literature whose heavy-traffic\nperformance is known.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.16377v2"
    },
    {
        "title": "Approximations to Study the Impact of the Service Discipline in Systems\n  with Redundancy",
        "authors": [
            "Nicolas Gast",
            "Benny van Houdt"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  As job redundancy has been recognized as an effective means to improve\nperformance of large-scale computer systems, queueing systems with redundancy\nhave been studied by various authors. Existing results include methods to\ncompute the queue length distribution and response time but only when the\nservice discipline is First-Come-First-Served (FCFS). For other service\ndisciplines, such as Processor Sharing (PS), or Last-Come-First-Served (LCFS),\nonly the stability conditions are known. In this paper we develop the first\nmethods to approximate the queue length distribution in a queueing system with\nredundancy under various service disciplines. We focus on a system with\nexponential job sizes, i.i.d. copies, and a large number of servers. We first\nderive a mean field approximation that is independent of the scheduling policy.\nIn order to study the impact of service discipline, we then derive refinements\nof this approximation to specific scheduling policies. In the case of Processor\nSharing, we provide a pair and a triplet approximation. The pair approximation\ncan be regarded as a refinement of the classic mean field approximation and\ntakes the service discipline into account, while the triplet approximation\nfurther refines the pair approximation. We also develop a pair approximation\nfor three other service disciplines: First-Come-First-Served, Limited Processor\nSharing and Last-Come-First-Served. We present numerical evidence that shows\nthat all the approximations presented in the paper are highly accurate, but\nthat none of them are asymptotically exact (as the number of servers goes to\ninfinity). This makes these approximations suitable to study the impact of the\nservice discipline on the queue length distribution. Our results show that FCFS\nyields the shortest queue length, and that the differences are more substantial\nat higher loads.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.07713v1"
    },
    {
        "title": "Hierarchical Analyses Applied to Computer System Performance: Review and\n  Call for Further Studies",
        "authors": [
            "Alexander Thomasian"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  We review studies based on analytic and simulation methods for hierarchical\nperformance analysis of Queueing Network - QN models, which result in an order\nof magnitude reduction in performance evaluation cost with respect to\nsimulation. The computational cost at the lower level is reduced when the\ncomputer system is modeled as a product-form QN. A Continuous Time Markov Chain\n- CTMC or discrete-event simulation can then be used at the higher level. We\nfirst consider a multiprogrammed transaction - txn processing system with\nPoisson arrivals and predeclared locks requests. Txn throughputs obtained by\nthe analysis of multiprogrammed computer systems serve as the transition rates\nin a higher level CTMC to determine txn response times. We next analyze a task\nsystem where task precedence relationships are specified by a directed acyclic\ngraph to determine its makespan. Task service demands are specified on the\ndevices of a computer system. The composition of tasks in execution determines\ntxn throughputs, which serve as transition rates among the states of the higher\nlevel CTMC model. As a third example we consider the hierarchical simulation of\na timesharing system with two user classes. Txn throughputs in processing\nvarious combinations of requests are obtained by analyzing a closed\nproduct-form QN model. A discrete event simulator is provided. More detailed QN\nmodeling parameters, such as the distribution of the number of cycles in\ncentral server model - CSM affects the performance of a fork/join queueing\nsystem. This detail can be taken into account in Schwetman's hybrid simulation\nmethod, which counts remaining cycles in CSM. We propose an extension to hybrid\nsimulation to adjust job service demands according to elapsed time, rather than\ncounting cycles. An example where Equilibrium Point Analysis to reduce\ncomputaional cost is privided.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.09292v1"
    },
    {
        "title": "Accurate and Scalable Many-Node Simulation",
        "authors": [
            "Stijn Eyerman",
            "Wim Heirman",
            "Kristof Du Bois",
            "Ibrahim Hur"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Accurate performance estimation of future many-node machines is challenging\nbecause it requires detailed simulation models of both node and network.\nHowever, simulating the full system in detail is unfeasible in terms of compute\nand memory resources. State-of-the-art techniques use a two-phase approach that\ncombines detailed simulation of a single node with network-only simulation of\nthe full system. We show that these techniques, where the detailed node\nsimulation is done in isolation, are inaccurate because they ignore two\nimportant node-level effects: compute time variability, and inter-node\ncommunication.\n  We propose a novel three-stage simulation method to allow scalable and\naccurate many-node simulation, combining native profiling, detailed node\nsimulation and high-level network simulation. By including timing variability\nand the impact of external nodes, our method leads to more accurate estimates.\nWe validate our technique against measurements on a multi-node cluster, and\nreport an average 6.7% error on 64 nodes (maximum error of 12%), compared to on\naverage 27% error and up to 54% when timing variability and the scaling\noverhead are ignored. At higher node counts, the prediction error of ignoring\nvariable timings and scaling overhead continues to increase compared to our\ntechnique, and may lead to selecting the wrong optimal cluster configuration.\n  Using our technique, we are able to accurately project performance to\nthousands of nodes within a day of simulation time, using only a single or a\nfew simulation hosts. Our method can be used to quickly explore large many-node\ndesign spaces, including node micro-architecture, node count and network\nconfiguration.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.09877v1"
    },
    {
        "title": "Systematic Performance Evaluation Framework for LEO Mega-Constellation\n  Satellite Networks",
        "authors": [
            "Yu Wang",
            "Chuili Kong",
            "Xian Meng",
            "Hejia Luo",
            "Ke-Xin Li",
            "Jun Wang"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Low Earth orbit (LEO) mega-constellation satellite networks have shown great\npotential to extend the coverage capability of conventional terrestrial\nnetworks. How to systematically define, quantify, and assess the technical\nperformance of LEO mega-constellation satellite networks remains an open issue.\nIn this paper, we propose a comprehensive key performance indicator (KPI)\nframework for mega-constellation based LEO satellite networks. An efficient LEO\nconstellation oriented performance evaluation methodology is then carefully\ndesigned by resorting to the concept of interfering area and spherical\ngeographic cell. We have carried out rigorous system-level simulations and\nprovided numerical results to assess the KPI framework. It can be observed that\nthe achieved area traffic capacity of the reference LEO constellation is around\n4 Kbps/km2, with service availability ranging from 0.36 to 0.39. Besides, the\naverage access success probability and handover failure rate is approximate to\n96% and 10%, respectively, in the nearest satellite association scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.11934v1"
    },
    {
        "title": "Dawn of the Dead(line Misses): Impact of Job Dismiss on the Deadline\n  Miss Rate",
        "authors": [
            "Jian-Jia Chen",
            "Mario Günzel",
            "Peter Bella",
            "Georg von der Brüggen",
            "Kuan-Hsun Chen"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Occasional deadline misses are acceptable for soft real-time systems.\nQuantifying probabilistic and deterministic characteristics of deadline misses\nis therefore essential to ensure that deadline misses indeed happen only\noccasionally. This is supported by recent research activities on probabilistic\nworst-case execution time, worst-case deadline failure probability, the maximum\nnumber of deadline misses, upper bounds on the deadline miss probability, and\nthe deadline miss rate. This paper focuses on the deadline miss rate of a\nperiodic soft real-time task in the long run. Our model assumes that this soft\nreal-time task has an arbitrary relative deadline and that a job can still be\nexecuted after a deadline-miss until a dismiss point. This model generalizes\nthe existing models that either dismiss a job immediately after its deadline\nmiss or never dismiss a job. We provide mathematical notation on the\nconvergence of the deadline miss rate in the long run and essential properties\nto calculate the deadline miss rate. Specifically, we use a Markov chain to\nmodel the execution behavior of a periodic soft real-time task. We present the\nrequired ergodicity property to ensure that the deadline miss rate in the long\nrun is described by a stationary distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.15503v1"
    },
    {
        "title": "Acceleration and energy consumption optimization in cascading\n  classifiers for face detection on low-cost ARM big.LITTLE asymmetric\n  architectures",
        "authors": [
            "Alberto Corpas",
            "Luis Costero",
            "Guillermo Botella",
            "Francisco D. Igual",
            "Carlos García",
            "Manuel Rodríguez"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  This paper proposes a mechanism to accelerate and optimize the energy\nconsumption of a face detection software based on Haar-like cascading\nclassifiers, taking advantage of the features of low-cost Asymmetric Multicore\nProcessors (AMPs) with limited power budget. A modelling and task\nscheduling/allocation is proposed in order to efficiently make use of the\nexisting features on big.LITTLE ARM processors, including: (I) source-code\nadaptation for parallel computing, which enables code acceleration by applying\nthe OmpSs programming model, a task-based programming model that handles\ndata-dependencies between tasks in a transparent fashion; (II) different OmpSs\ntask allocation policies which take into account the processor asymmetry and\ncan dynamically set processing resources in a more efficient way based on their\nparticular features. The proposed mechanism can be efficiently applied to take\nadvantage of the processing elements existing on low-cost and low-energy\nmulti-core embedded devices executing object detection algorithms based on\ncascading classifiers. Although these classifiers yield the best results for\ndetection algorithms in the field of computer vision, their high computational\nrequirements prevent them from being used on these devices under real-time\nrequirements. Finally, we compare the energy efficiency of a heterogeneous\narchitecture based on asymmetric multicore processors with a suitable task\nscheduling, with that of a homogeneous symmetric architecture.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.04090v1"
    },
    {
        "title": "Reconsidering the performance of DEVS modeling and simulation\n  environments using the DEVStone benchmark",
        "authors": [
            "José L. Risco-Martín",
            "Saurabh Mittal",
            "Juan Carlos Fabero",
            "Marina Zapater",
            "Román Hermida"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The Discrete Event System Specification formalism (DEVS), which supports\nhierarchical and modular model composition, has been widely used to understand,\nanalyze and develop a variety of systems. DEVS has been implemented in various\nlanguages and platforms over the years. The DEVStone benchmark was conceived to\ngenerate a set of models with varied structure and behavior, and to automate\nthe evaluation of the performance of DEVS-based simulators. However, DEVStone\nis still in a preliminar phase and more model analysis is required. In this\npaper, we revisit DEVStone introducing new equations to compute the number of\nevents triggered. We also introduce a new benchmark, called HOmem, designed as\nan alternative version of HOmod, with similar CPU and memory requirements, but\nwith an easier implementation and analytically more manageable. Finally, we\ncompare both the performance and memory footprint of five different DEVS\nsimulators in two different hardware platforms.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.05483v1"
    },
    {
        "title": "Integrating ytopt and libEnsemble to Autotune OpenMC",
        "authors": [
            "Xingfu Wu",
            "John R. Tramm",
            "Jeffrey Larson",
            "John-Luke Navarro",
            "Prasanna Balaprakash",
            "Brice Videau",
            "Michael Kruse",
            "Paul Hovland",
            "Valerie Taylor",
            "Mary Hall"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  ytopt is a Python machine-learning-based autotuning software package\ndeveloped within the ECP PROTEAS-TUNE project. The ytopt software adopts an\nasynchronous search framework that consists of sampling a small number of input\nparameter configurations and progressively fitting a surrogate model over the\ninput-output space until exhausting the user-defined maximum number of\nevaluations or the wall-clock time. libEnsemble is a Python toolkit for\ncoordinating workflows of asynchronous and dynamic ensembles of calculations\nacross massively parallel resources developed within the ECP PETSc/TAO project.\nlibEnsemble helps users take advantage of massively parallel resources to solve\ndesign, decision, and inference problems and expands the class of problems that\ncan benefit from increased parallelism. In this paper we present our\nmethodology and framework to integrate ytopt and libEnsemble to take advantage\nof massively parallel resources to accelerate the autotuning process.\nSpecifically, we focus on using the proposed framework to autotune the ECP\nExaSMR application OpenMC, an open source Monte Carlo particle transport code.\nOpenMC has seven tunable parameters some of which have large ranges such as the\nnumber of particles in-flight, which is in the range of 100,000 to 8 million,\nwith its default setting of 1 million. Setting the proper combination of these\nparameter values to achieve the best performance is extremely time-consuming.\nTherefore, we apply the proposed framework to autotune the MPI/OpenMP offload\nversion of OpenMC based on a user-defined metric such as the figure of merit\n(FoM) (particles/s) or energy efficiency energy-delay product (EDP) on Crusher\nat Oak Ridge Leadership Computing Facility. The experimental results show that\nwe achieve improvement up to 29.49\\% in FoM and up to 30.44\\% in EDP.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.09222v2"
    },
    {
        "title": "CesASMe and Staticdeps: static detection of memory-carried dependencies\n  for code analyzers",
        "authors": [
            "Théophile Bastian",
            "Hugo Pompougnac",
            "Alban Dutilleul",
            "Fabrice Rastello"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  A variety of code analyzers, such as IACA, uiCA, llvm-mca or Ithemal, strive\nto statically predict the throughput of a computation kernel. Each analyzer is\nbased on its own simplified CPU model reasoning at the scale of a basic block.\nFacing this diversity, evaluating their strengths and weaknesses is important\nto guide both their usage and their enhancement.\n  We present CesASMe, a fully-tooled solution to evaluate code analyzers on\nC-level benchmarks composed of a benchmark derivation procedure that feeds an\nevaluation harness. We conclude that memory-carried data dependencies are a\nmajor source of imprecision for these tools. We tackle this issue with\nstaticdeps, a static analyzer extracting memory-carried data dependencies,\nincluding across loop iterations, from an assembly basic block. We integrate\nits output to uiCA, a state-of-the-art code analyzer, to evaluate staticdeps'\nimpact on a code analyzer's precision through CesASMe.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.14567v1"
    },
    {
        "title": "Performance bottlenecks detection through microarchitectural sensitivity",
        "authors": [
            "Hugo Pompougnac",
            "Alban Dutilleul",
            "Christophe Guillon",
            "Nicolas Derumigny",
            "Fabrice Rastello"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Modern Out-of-Order (OoO) CPUs are complex systems with many components\ninterleaved in non-trivial ways. Pinpointing performance bottlenecks and\nunderstanding the underlying causes of program performance issues are critical\ntasks to make the most of hardware resources.\n  We provide an in-depth overview of performance bottlenecks in recent OoO\nmicroarchitectures and describe the difficulties of detecting them. Techniques\nthat measure resources utilization can offer a good understanding of a\nprogram's execution, but, due to the constraints inherent to Performance\nMonitoring Units (PMU) of CPUs, do not provide the relevant metrics for each\nuse case.\n  Another approach is to rely on a performance model to simulate the CPU\nbehavior. Such a model makes it possible to implement any new\nmicroarchitecture-related metric. Within this framework, we advocate for\nimplementing modeled resources as parameters that can be varied at will to\nreveal performance bottlenecks. This allows a generalization of bottleneck\nanalysis that we call sensitivity analysis.\n  We present Gus, a novel performance analysis tool that combines the\nadvantages of sensitivity analysis and dynamic binary instrumentation within a\nresource-centric CPU model. We evaluate the impact of sensitivity on bottleneck\nanalysis over a set of high-performance computing kernels.\n",
        "pdf_link": "http://arxiv.org/pdf/2402.15773v1"
    },
    {
        "title": "A Continuous Benchmarking Infrastructure for High-Performance Computing\n  Applications",
        "authors": [
            "Christoph Alt",
            "Martin Lanser",
            "Jonas Plewinski",
            "Atin Janki",
            "Axel Klawonn",
            "Harald Köstler",
            "Michael Selzer",
            "Ulrich Rüde"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  For scientific software, especially those used for large-scale simulations,\nachieving good performance and efficiently using the available hardware\nresources is essential. It is important to regularly perform benchmarks to\nensure the efficient use of hardware and software when systems are changing and\nthe software evolves. However, this can become quickly very tedious when many\noptions for parameters, solvers, and hardware architectures are available. We\npresent a continuous benchmarking strategy that automates benchmarking new code\nchanges on high-performance computing clusters. This makes it possible to track\nhow each code change affects the performance and how it evolves.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.01579v1"
    },
    {
        "title": "Explainable Port Mapping Inference with Sparse Performance Counters for\n  AMD's Zen Architectures",
        "authors": [
            "Fabian Ritter",
            "Sebastian Hack"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Performance models are instrumental for optimizing performance-sensitive\ncode. When modeling the use of functional units of out-of-order x86-64 CPUs,\ndata availability varies by the manufacturer: Instruction-to-port mappings for\nIntel's processors are available, whereas information for AMD's designs are\nlacking. The reason for this disparity is that standard techniques to infer\nexact port mappings require hardware performance counters that AMD does not\nprovide.\n  In this work, we modify the port mapping inference algorithm of the widely\nused uops.info project to not rely on Intel's performance counters. The\nmodifications are based on a formal port mapping model with a\ncounter-example-guided algorithm powered by an SMT solver. We investigate in\nhow far AMD's processors comply with this model and where unexpected\nperformance characteristics prevent an accurate port mapping. Our results\nprovide valuable insights for creators of CPU performance models as well as for\nsoftware developers who want to achieve peak performance on recent AMD CPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.16063v1"
    },
    {
        "title": "gpu_tracker: Python package for tracking and profiling GPU utilization\n  in both desktop and high-performance computing environments",
        "authors": [
            "Erik D. Huckvale",
            "Hunter N. B. Moseley"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Determining the maximum usage of random-access memory (RAM) on both the\nmotherboard and on a graphical processing unit (GPU) over the lifetime of a\ncomputing task can be extremely useful for troubleshooting points of failure as\nwell as optimizing memory utilization, especially within a high-performance\ncomputing (HPC) setting. While there are tools for tracking compute time and\nRAM, including by job management tools themselves, tracking of GPU usage, to\nour knowledge, does not currently have sufficient solutions. We present\ngpu_tracker, a Python package that tracks the computational resource usage of a\ntask while running in the background, including the real compute time that the\ntask takes to complete, its maximum RAM usage, and the maximum GPU RAM usage,\nspecifically for Nvidia GPUs. We demonstrate that gpu_tracker can seamlessly\ntrack computational resource usage with minimal overhead, both within desktop\nand HPC execution environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.01473v2"
    },
    {
        "title": "Towards self-optimization of publish/subscribe IoT systems using\n  continuous performance monitoring",
        "authors": [
            "Mohammed Djahafi",
            "Nabila Salmi"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Today, more and more embedded devices are being connected through a network,\ngenerally Internet, offering users different services. This concept refers to\nInternet of Things (IoT), bringing information and control capabilities in many\nfields like medicine, smart homes, home security, etc. Main drawbacks of IoT\nenvironment are its dependency on Internet connectivity and need continuous\ndevices power. These dependencies may affect system performances, namely\nrequest processing response times. In this context, we propose in this paper a\ncontinuous performance monitoring methodology, applied on IoT systems based on\nPublish/subscribe communication model. Our approach assesses performances using\nStochastic Petri net modeling, and self-optimizes whenever poor performances\nare detected. Our approach relies on a Stochastic Petri nets modelling and\nanalysis to assess performances. We target improving performances, in\nparticular response times, by online modification of influencing factors.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.14926v2"
    },
    {
        "title": "Performance Evaluation of CMOS Annealing with Support Vector Machine",
        "authors": [
            "Ryoga Fukuhara",
            "Makoto Morishita",
            "Takahiro Katagiri",
            "Masatoshi Kawai",
            "Toru Nagai",
            "Tetsuya Hoshino"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  In this paper, support vector machine (SVM) performance was assessed\nutilizing a quantum-inspired complementary metal-oxide semiconductor (CMOS)\nannealer. The primary focus during performance evaluation was the accuracy rate\nin binary classification problems. A comparative analysis was conducted between\nSVM running on a CPU (classical computation) and executed on a quantum-inspired\nannealer. The performance outcome was evaluated using a CMOS annealing machine,\nthereby obtaining an accuracy rate of 93.7% for linearly separable problems,\n92.7% for non-linearly separable problem 1, and 97.6% for non-linearly\nseparable problem 2. These results reveal that a CMOS annealing machine can\nachieve an accuracy rate that closely rivals that of classical computation.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.15752v2"
    },
    {
        "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
        "authors": [
            "Ziyue Qiu",
            "Juncheng Yang",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.16219v4"
    },
    {
        "title": "AmBC-NOMA-Aided Short-Packet Communication for High Mobility V2X\n  Transmissions",
        "authors": [
            "Xinyue Pei",
            "Xingwei Wang",
            "Yingyang Chen",
            "Tingrui Pei",
            "Miaowen Wen"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  In this paper, we investigate the performance of ambient backscatter\ncommunication non-orthogonal multiple access (AmBC-NOMA)-assisted short packet\ncommunication for high-mobility vehicle-to-everything transmissions. In the\nproposed system, a roadside unit (RSU) transmits a superimposed signal to a\ntypical NOMA user pair. Simultaneously, the backscatter device (BD) transmits\nits own signal towards the user pair by reflecting and modulating the RSU's\nsuperimposed signals. Due to vehicles' mobility, we consider realistic\nassumptions of time-selective fading and channel estimation errors. Theoretical\nexpressions for the average block error rates (BLERs) of both users are\nderived. Furthermore, analysis and insights on transmit signal-to-noise ratio,\nvehicles' mobility, imperfect channel estimation, the reflection efficiency at\nthe BD, and blocklength are provided. Numerical results validate the\ntheoretical findings and reveal that the AmBC-NOMA system outperforms its\northogonal multiple access counterpart in terms of BLER performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.16502v1"
    },
    {
        "title": "An Analysis of Performance Bottlenecks in MRI Pre-Processing",
        "authors": [
            "Mathieu Dugré",
            "Yohan Chatelain",
            "Tristan Glatard"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Magnetic Resonance Image (MRI) pre-processing is a critical step for\nneuroimaging analysis. However, the computational cost of MRI pre-processing\npipelines is a major bottleneck for large cohort studies and some clinical\napplications. While High-Performance Computing (HPC) and, more recently, Deep\nLearning have been adopted to accelerate the computations, these techniques\nrequire costly hardware and are not accessible to all researchers. Therefore,\nit is important to understand the performance bottlenecks of MRI pre-processing\npipelines to improve their performance. Using Intel VTune profiler, we\ncharacterized the bottlenecks of several commonly used MRI-preprocessing\npipelines from the ANTs, FSL, and FreeSurfer toolboxes. We found that few\nfunctions contributed to most of the CPU time, and that linear interpolation\nwas the largest contributor. Data access was also a substantial bottleneck. We\nidentified a bug in the ITK library that impacts the performance of ANTs\npipeline in single-precision and a potential issue with the OpenMP scaling in\nFreeSurfer recon-all. Our results provide a reference for future efforts to\noptimize MRI pre-processing pipelines.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.17650v1"
    },
    {
        "title": "Impact of Generative AI (Large Language Models) on the PRA model\n  construction and maintenance, observations",
        "authors": [
            "Valentin Rychkov",
            "Claudia Picoco",
            "Emilie Caleca"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The rapid development of Large Language Models (LLMs) and Generative\nPre-Trained Transformers(GPTs) in the field of Generative Artificial\nIntelligence (AI) can significantly impact task automation in themodern\neconomy. We anticipate that the PRA field will inevitably be affected by this\ntechnology. Thus, themain goal of this paper is to engage the risk assessment\ncommunity into a discussion of benefits anddrawbacks of this technology for\nPRA. We make a preliminary analysis of possible application of LLM\ninProbabilistic Risk Assessment (PRA) modeling context referring to the ongoing\nexperience in softwareengineering field. We explore potential application\nscenarios and the necessary conditions for controlledLLM usage in PRA modeling\n(whether static or dynamic). Additionally, we consider the potential impact\nofthis technology on PRA modeling tools.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.01133v4"
    },
    {
        "title": "Modeling Common Cause Failure in Dynamic PRA",
        "authors": [
            "Claudia Picoco",
            "Valentin Rychkov"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  In this paper we propose a dynamic model of Common Cause Failures (CCF) that\nallows to generate common cause events in time. The proposed model is a\ngeneralization of Binomial Failure Rate Model (Atwood model) that can generate\nstaggered failures of multiple components due to a common cause. We implement\nthe model using statechart formalism, a similar implementation can be adopted\nin other modeling languages like Petri Nets or Hybrid Stochastic Automata. The\npresented model was integrated in a Dynamic PRA study.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.08879v1"
    },
    {
        "title": "DSO: A GPU Energy Efficiency Optimizer by Fusing Dynamic and Static\n  Information",
        "authors": [
            "Qiang Wang",
            "Laiyi Li",
            "Weile Luo",
            "Yijia Zhang",
            "Bingqiang Wang"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Increased reliance on graphics processing units (GPUs) for high-intensity\ncomputing tasks raises challenges regarding energy consumption. To address this\nissue, dynamic voltage and frequency scaling (DVFS) has emerged as a promising\ntechnique for conserving energy while maintaining the quality of service (QoS)\nof GPU applications. However, existing solutions using DVFS are hindered by\ninefficiency or inaccuracy as they depend either on dynamic or static\ninformation respectively, which prevents them from being adopted to practical\npower management schemes. To this end, we propose a novel energy efficiency\noptimizer, called DSO, to explore a light weight solution that leverages both\ndynamic and static information to model and optimize the GPU energy efficiency.\nDSO firstly proposes a novel theoretical energy efficiency model which reflects\nthe DVFS roofline phenomenon and considers the tradeoff between performance and\nenergy. Then it applies machine learning techniques to predict the parameters\nof the above model with both GPU kernel runtime metrics and static code\nfeatures. Experiments on modern DVFS-enabled GPUs indicate that DSO can enhance\nenergy efficiency by 19% whilst maintaining performance within a 5% loss\nmargin.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.13096v1"
    },
    {
        "title": "Billion-files File Systems (BfFS): A Comparison",
        "authors": [
            "Sohail Shaikh"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  As the volume of data being produced is increasing at an exponential rate\nthat needs to be processed quickly, it is reasonable that the data needs to be\navailable very close to the compute devices to reduce transfer latency. Due to\nthis need, local filesystems are getting close attention to understand their\ninner workings, performance, and more importantly their limitations. This study\nanalyzes few popular Linux filesystems: EXT4, XFS, BtrFS, ZFS, and F2FS by\ncreating, storing, and then reading back one billion files from the local\nfilesystem. The study also captured and analyzed read/write throughput, storage\nblocks usage, disk space utilization and overheads, and other metrics useful\nfor system designers and integrators. Furthermore, the study explored other\nside effects such as filesystem performance degradation during and after these\nlarge numbers of files and folders are created.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.01805v1"
    },
    {
        "title": "Automated PMC-based Power Modeling Methodology for Modern Mobile GPUs",
        "authors": [
            "Pranab Dash",
            "Y. Charlie Hu",
            "Abhilash Jindal"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The rise of machine learning workload on smartphones has propelled GPUs into\none of the most power-hungry components of modern smartphones and elevates the\nneed for optimizing the GPU power draw by mobile apps. Optimizing the power\nconsumption of mobile GPUs in turn requires accurate estimation of their power\ndraw during app execution. In this paper, we observe that the prior-art,\nutilization-frequency based GPU models cannot capture the diverse\nmicro-architectural usage of modern mobile GPUs.We show that these models\nsuffer poor modeling accuracy under diverse GPU workload, and study whether\nperformance monitoring counter (PMC)-based models recently proposed for\ndesktop/server GPUs can be applied to accurately model mobile GPU power. Our\nstudy shows that the PMCs that come with dominating mobile GPUs used in modern\nsmartphones are sufficient to model mobile GPU power, but exhibit\nmulticollinearity if used altogether. We present APGPM, the mobile GPU power\nmodeling methodology that automatically selects an optimal set of PMCs that\nmaximizes the GPU power model accuracy. Evaluation on two representative mobile\nGPUs shows that APGPM-generated GPU power models reduce the MAPE modeling error\nof prior-art by 1.95x to 2.66x (i.e., by 11.3% to 15.4%) while using only 4.66%\nto 20.41% of the total number of available PMCs.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.04886v1"
    },
    {
        "title": "TINA: Acceleration of Non-NN Signal Processing Algorithms Using NN\n  Accelerators",
        "authors": [
            "Christiaan Boerkamp",
            "Steven van der Vlugt",
            "Zaid Al-Ars"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  This paper introduces TINA, a novel framework for implementing non Neural\nNetwork (NN) signal processing algorithms on NN accelerators such as GPUs, TPUs\nor FPGAs. The key to this approach is the concept of mapping mathematical and\nlogic functions as a series of convolutional and fully connected layers. By\nmapping functions into such a small substack of NN layers, it becomes possible\nto execute non-NN algorithms on NN hardware (HW) accelerators efficiently, as\nwell as to ensure the portability of TINA implementations to any platform that\nsupports such NN accelerators. Results show that TINA is highly competitive\ncompared to alternative frameworks, specifically for complex functions with\niterations. For a Polyphase Filter Bank use case TINA shows GPU speedups of up\nto 80x vs a CPU baseline with NumPy compared to 8x speedup achieved by\nalternative frameworks. The framework is open source and publicly available at\nhttps://github.com/ChristiaanBoe/TINA.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.16551v1"
    },
    {
        "title": "ppOpen-AT: A Directive-base Auto-tuning Language",
        "authors": [
            "Takahiro Katagiri"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  ppOpen-AT is a domain-specific language designed to ease the workload for\ndevelopers creating libraries with auto-tuning (AT) capabilities. It consists\nof a set of directives that allow for the automatic generation of code\nnecessary for AT by placing annotations in the source program. This approach\nsignificantly reduces the effort required by numerical library developers. This\ntechnical report details the implementation of the AT software and its extended\nfunctions, and provides an explanation of the internal specifications of\nppOpen-AT.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.16607v1"
    },
    {
        "title": "Scaler: Efficient and Effective Cross Flow Analysis",
        "authors": [
            " Steven",
            " Tang",
            "Mingcan Xiang",
            "Yang Wang",
            "Bo Wu",
            "Jianjun Chen",
            "Tongping Liu"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Performance analysis is challenging as different components (e.g.,different\nlibraries, and applications) of a complex system can interact with each other.\nHowever, few existing tools focus on understanding such interactions. To bridge\nthis gap, we propose a novel analysis method \"Cross Flow Analysis (XFA)\" that\nmonitors the interactions/flows across these components. We also built the\nScaler profiler that provides a holistic view of the time spent on each\ncomponent (e.g., library or application) and every API inside each component.\nThis paper proposes multiple new techniques, such as Universal Shadow Table,\nand Relation-Aware Data Folding. These techniques enable Scaler to achieve low\nruntime overhead, low memory overhead, and high profiling accuracy. Based on\nour extensive experimental results, Scaler detects multiple unknown performance\nissues inside widely-used applications, and therefore will be a useful\ncomplement to existing work.\n  The reproduction package including the source code, benchmarks, and\nevaluation scripts, can be found at https://doi.org/10.5281/zenodo.13336658.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.00854v3"
    },
    {
        "title": "Computational Algorithms for the Product Form Solution of Closed Queuing\n  Networks with Finite Buffers and Skip-Over Policy",
        "authors": [
            "Gianfranco Balbo",
            "Andrea Marin",
            "Diletta Olliaro",
            "Matteo Sereno"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Closed queuing networks with finite capacity buffers and skip-over policies\nare fundamental models in the performance evaluation of computer and\ncommunication systems. This technical report presents the details of\ncomputational algorithms to derive the key performance metrics for such\nnetworks. The primary focus is on the efficient computation of the\nnormalization constant, which is critical for determining the steady-state\nprobabilities of the network states under investigation. A convolution\nalgorithm is proposed, which paves the way for the computation of key\nperformance indices, such as queue length distribution and throughput,\naccommodating the intricacies introduced by finite capacity constraints and\nskip-over mechanisms. Finally, an extension of the traditional Mean Value\nAnalysis algorithm addressing numerical stability is provided. The approaches\ndiscussed here allow make the investigation of large-scale networks feasible\nand enable the development of robust implementations of these techniques for\npractical use.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08075v1"
    },
    {
        "title": "RAVE: RISC-V Analyzer of Vector Executions, a QEMU tracing plugin",
        "authors": [
            "Pablo Vizcaino",
            "Filippo Mantovani",
            "Jesus Labarta",
            "Roger Ferrer"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Simulators are crucial during the development of a chip, like the RISC-V\naccelerator designed in the European Processor Initiative project. In this\npaper, we showcase the limitations of the current simulation solutions in the\nproject and propose using QEMU with RAVE, a plugin we implement and describe in\nthis document. This methodology can rapidly simulate and analyze applications\nrunning on the v1.0 and v0.7.1 RISC-V V-extension. Our plugin reports the\nvector and scalar instructions alongside useful information such as the\nvector-length being used, the single-element-width, and the register usage,\namong other vectorization metrics. We provide an API used from the simulated\nApplication to control the RAVE plugin and the capability to generate\nvectorization traces that can be analyzed using Paraver. Finally, we\ndemonstrate the efficiency of our solution between different evaluated machines\nand against other simulation methods used in the European Processor Accelerator\n(EPAC) project.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.13639v1"
    },
    {
        "title": "Balanced Splitting: A Framework for Achieving Zero-wait in the\n  Multiserver-job Model",
        "authors": [
            "Jonatha Anselmi",
            "Josu Doncel"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  We present a new framework for designing nonpreemptive and job-size oblivious\nscheduling policies in the multiserver-job queueing model. The main requirement\nis to identify a static and balanced sub-partition of the server set and ensure\nthat the servers in each set of that sub-partition can only handle jobs of a\ngiven class and in a first-come first-served order. A job class is determined\nby the number of servers to which it has exclusive access during its entire\nexecution and the probability distribution of its service time. This approach\naims to reduce delays by preventing small jobs from being blocked by larger\nones that arrived first, and it is particularly beneficial when the job size\nvariability intra resp. inter classes is small resp. large. In this setting, we\npropose a new scheduling policy, Balanced-Splitting. We provide a sufficient\ncondition for the stability of Balanced-Splitting and show that the resulting\nqueueing probability, i.e., the probability that an arriving job needs to wait\nfor processing upon arrival, vanishes in both the subcritical (the load is kept\nfixed to a constant less than one) and critical (the load approaches one from\nbelow) many-server limiting regimes. Crucial to our analysis is a connection\nwith the M/GI/s/s queue and Erlang's loss formula, which allows our analysis to\nrely on fundamental results from queueing theory. Numerical simulations show\nthat the proposed policy performs better than several preemptive/nonpreemptive\nsize-aware/oblivious policies in various practical scenarios. This is also\nconfirmed by simulations running on real traces from High Performance Computing\n(HPC) workloads. The delays induced by Balanced-Splitting are also competitive\nwith those induced by state-of-the-art policies such as First-Fit-SRPT and\nServerFilling-SRPT, though our approach has the advantage of not requiring\npreemption, nor the knowledge of job sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.18557v1"
    },
    {
        "title": "ZERNIPAX: A Fast and Accurate Zernike Polynomial Calculator in Python",
        "authors": [
            "Yigit Gunsur Elmacioglu",
            "Rory Conlin",
            "Daniel W. Dudt",
            "Dario Panici",
            "Egemen Kolemen"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Zernike Polynomials serve as an orthogonal basis on the unit disc, and have\nbeen proven to be effective in optics simulations, astrophysics, and more\nrecently in plasma simulations. Unlike Bessel functions, they maintain finite\nvalues at the disc center, ensuring inherent analyticity along the axis. We\ndeveloped ZERNIPAX, an open-source Python package capable of utilizing\nCPU/GPUs, leveraging Google's JAX package and available on\nhttps://github.com/PlasmaControl/FastZernike.git as well as PyPI. Our\nimplementation of the recursion relation between Jacobi polynomials\nsignificantly improves computation time compared to alternative methods by use\nof parallel computing while still preserving accuracy for mode numbers n>100.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.19156v1"
    },
    {
        "title": "Streaming Data in HPC Workflows Using ADIOS",
        "authors": [
            "Greg Eisenhauer",
            "Norbert Podhorszki",
            "Ana Gainaru",
            "Scott Klasky",
            "Philip E. Davis",
            "Manish Parashar",
            "Matthew Wolf",
            "Eric Suchtya",
            "Erick Fredj",
            "Vicente Bolea",
            "Franz Pöschel",
            "Klaus Steiniger",
            "Michael Bussmann",
            "Richard Pausch",
            "Sunita Chandrasekaran"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The \"IO Wall\" problem, in which the gap between computation rate and data\naccess rate grows continuously, poses significant problems to scientific\nworkflows which have traditionally relied upon using the filesystem for\nintermediate storage between workflow stages. One way to avoid this problem in\nscientific workflows is to stream data directly from producers to consumers and\navoiding storage entirely. However, the manner in which this is accomplished is\nkey to both performance and usability. This paper presents the Sustainable\nStaging Transport, an approach which allows direct streaming between\ntraditional file writers and readers with few application changes. SST is an\nADIOS \"engine\", accessible via standard ADIOS APIs, and because ADIOS allows\nengines to be chosen at run-time, many existing file-oriented ADIOS workflows\ncan utilize SST for direct application-to-application communication without any\nsource code changes. This paper describes the design of SST and presents\nperformance results from various applications that use SST, for feeding model\ntraining with simulation data with substantially higher bandwidth than the\ntheoretical limits of Frontier's file system, for strong coupling of separately\ndeveloped applications for multiphysics multiscale simulation, or for in situ\nanalysis and visualization of data to complete all data processing shortly\nafter the simulation finishes.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00178v1"
    },
    {
        "title": "Tuning Fast Memory Size based on Modeling of Page Migration for Tiered\n  Memory",
        "authors": [
            "Shangye Chen",
            "Jin Huang",
            "Shuangyan Yang",
            "Jie Liu",
            "Huaicheng Li",
            "Dimitrios Nikolopoulos",
            "Junhee Ryu",
            "Jinho Baek",
            "Kwangsik Shin",
            "Dong Li"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Tiered memory, built upon a combination of fast memory and slow memory,\nprovides a cost-effective solution to meet ever-increasing requirements from\nemerging applications for large memory capacity. Reducing the size of fast\nmemory is valuable to improve memory utilization in production and reduce\nproduction costs because fast memory tends to be expensive. However, deciding\nthe fast memory size is challenging because there is a complex interplay\nbetween application characterization and the overhead of page migration used to\nmitigate the impact of limited fast memory capacity. In this paper, we\nintroduce a system, Tuna, to decide fast memory size based on modeling of page\nmigration. Tuna uses micro-benchmarking to model the impact of page migration\non application performance using three metrics. Tuna decides the fast memory\nsize based on offline modeling results and limited information on workload\ntelemetry. Evaluating with common big-memory applications and using 5% as the\nperformance loss target, we show that Tuna in combination with a page\nmanagement system (TPP) saves fast memory by 8.5% on average (up to 16%). This\nis in contrast to the 5% saving in fast memory reported by Microsoft Pond for\nthe same workloads (BFS and SSSP) and the same performance loss target.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.00328v1"
    },
    {
        "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
        "authors": [
            "Chongzhuo Yang",
            "Chang Guo",
            "Ming Zhao",
            "Zhichao Cao"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.11260v1"
    },
    {
        "title": "DRIM-ANN: An Approximate Nearest Neighbor Search Engine based on\n  Commercial DRAM-PIMs",
        "authors": [
            "Mingkai Chen",
            "Tianhua Han",
            "Cheng Liu",
            "Shengwen Liang",
            "Kuai Yu",
            "Lei Dai",
            "Ziming Yuan",
            "Ying Wang",
            "Lei Zhang",
            "Huawei Li",
            "Xiaowei Li"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Approximate Nearest Neighbor Search (ANNS), which enables efficient semantic\nsimilarity search in large datasets, has become a fundamental component of\ncritical applications such as information retrieval and retrieval-augmented\ngeneration (RAG). However, ANNS is a well-known I/O-intensive algorithm with a\nlow compute-to-I/O ratio, often requiring massive storage due to the large\nvolume of high-dimensional data. This leads to I/O bottlenecks on CPUs and\nmemory limitations on GPUs. DRAM-based Processing-in-Memory (DRAM-PIM)\narchitecture, which offers high bandwidth, large-capacity memory, and the\nability to perform efficient computation in or near the data, presents a\npromising solution for ANNS. In this work, we investigate the use of commercial\nDRAM-PIM for ANNS for the first time and propose DRIM-ANN, an optimized ANNS\nengine based on DRAM-PIMs from UPMEM. Notably, given that the target DRAM-PIM\nexhibits an even lower compute-to-I/O ratio than basic ANNS, we leverage lookup\ntables (LUTs) to replace more multiplications with I/O operations. We then\nsystematically tune ANNS to search optimized configurations with lower\ncomputational load, aligning the compute-to-I/O ratio of ANNS with that of\nDRAM-PIMs while maintaining accuracy constraints. Building on this tuned ANNS\nalgorithm, we further explore implementation optimizations to fully utilize the\ntwo thousand parallel processing units with private local memory in DRAM-PIMs.\nTo address the load imbalance caused by ANNS requests distributed across\ndifferent clusters of large datasets, we propose a load-balancing strategy that\ncombines static data layout optimization with dynamic runtime request\nscheduling. Experimental results on representative datasets show that DRIM-ANN\nachieves an average performance speedup of 2.92x compared to a 32-thread CPU\ncounterpart.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.15621v1"
    },
    {
        "title": "Industry 4.0 Connectors -- A Performance Experiment with Modbus/TCP",
        "authors": [
            "Christian Nikolajew",
            "Holger Eichelberger"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  For Industry 4.0 applications, communication protocols and data formats even\nfor legacy devices are fundamental. In this paper, we focus on the Modbus/TCP\nprotocol, which is, e.g., used in energy metering. Allowing Industry 4.0\napplications to include data from such protocols without need for programming\nwould increase flexibility and, in turn, improve development efficiency. As one\nparticular approach, we discuss the automated generation of Modbus/TCP\nconnectors for our Open Source oktoflow platform and compare the performance of\nhandcrafted as well as generated connectors in different settings, including\nindustrial energy metering devices.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.15813v1"
    },
    {
        "title": "ADS Performance Revisited",
        "authors": [
            "Alexander Weber",
            "Holger Eichelberger",
            "Jobst Hildebrand"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Real-time measurements are important for in-depth control of manufacturing\nprocesses, which, for modern AI methods, need integration with high-level\nlanguages. In our last SSP paper we investigated the performance of a Python\nand a Java-JNA based approach to integrate the Beckhoff ADS protocol for\nreal-time edge communication into an Industry 4.0 platform. There, we have\nshown that while Java outperforms Python, both solutions do not meet the\ndesired goal of 1-20kHz depending on the task. However, we are are still\nlacking an explanation for this result as well as an analysis of alternatives.\nFor the first topic, we show in this paper that 1) exchanging Java-JNA with\nJava-JNI in this setting does not further improve the performance 2) a C++\nprogram realizing the same behavior in a more direct integration does not\nperform better and 3) profiling shows that the majority of the execution is\nspend in ADS. For the second topic, we show that alternative uses of the ADS\nlibrary allow for better performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.15853v1"
    },
    {
        "title": "MambaCPU: Enhanced Correlation Mining with State Space Models for CPU\n  Performance Prediction",
        "authors": [
            "Xiaoman Liu"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Forecasting CPU performance, which involves estimating performance scores\nbased on hardware characteristics during operation, is crucial for\ncomputational system design and resource management. This research field\ncurrently faces two primary challenges. First, the diversity of CPU products\nand the specialized nature of hardware characteristics make real-world data\ncollection difficult. Second, existing approaches, whether reliant on hardware\nsimulation models or machine learning, suffer from significant drawbacks, such\nas lengthy simulation cycles, low prediction accuracy, and neglect of\ncharacteristic correlations. To address these issues, we first gathered,\npreprocessed, and standardized historical data from the 4th Generation Intel\nXeon Scalable Processors across various benchmark suites to create a new\ndataset named PerfCastDB. Subsequently, we developed a novel network, MambaCPU\n(MaC), as the baseline model for the PerfCastDB dataset. This model employs the\nmamba structure to explore global dependencies and correlations among multiple\ncharacteristics. The use of intra- and inter-group attention mechanisms further\nrefines correlations within and between characteristic groups. These techniques\nenhance MaC's capability to analyze and mine complex multivariate correlations.\nComparative experiments on the PerfCastDB dataset demonstrate that MaC\nsurpasses existing methods, confirming its effectiveness. Additionally, we have\nopen-sourced part of the dataset and the MaC code at\n\\url{https://github.com/xiaoman-liu/MaC} to facilitate further research.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.19297v2"
    },
    {
        "title": "LLload: An Easy-to-Use HPC Utilization Tool",
        "authors": [
            "Chansup Byun",
            "Albert Reuther",
            "Julie Mullen",
            "LaToya Anderson",
            "William Arcand",
            "Bill Bergeron",
            "David Bestor",
            "Alexander Bonn",
            "Daniel Burrill",
            "Vijay Gadepally",
            "Michael Houle",
            "Matthew Hubbell",
            "Hayden Jananthan",
            "Michael Jones",
            "Piotr Luszczek",
            "Peter Michaleas",
            "Lauren Milechin",
            "Guillermo Morales",
            "Andrew Prout",
            "Antonio Rosa",
            "Charles Yee",
            "Jeremy Kepner"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The increasing use and cost of high performance computing (HPC) requires new\neasy-to-use tools to enable HPC users and HPC systems engineers to\ntransparently understand the utilization of resources. The MIT Lincoln\nLaboratory Supercomputing Center (LLSC) has developed a simple command, LLload,\nto monitor and characterize HPC workloads. LLload plays an important role in\nidentifying opportunities for better utilization of compute resources. LLload\ncan be used to monitor jobs both programmatically and interactively. LLload can\ncharacterize users' jobs using various LLload options to achieve better\nefficiency. This information can be used to inform the user to optimize HPC\nworkloads and improve both CPU and GPU utilization. This includes improvements\nusing judicious oversubscription of the computing resources. Preliminary\nresults suggest significant improvement in GPU utilization and overall\nthroughput performance with GPU overloading in some cases. By enabling users to\nobserve and fix incorrect job submission and/or inappropriate execution setups,\nLLload can increase the resource usage and improve the overall throughput\nperformance. LLload is a light-weight, easy-to-use tool for both HPC users and\nHPC systems engineers to monitor HPC workloads to improve system utilization\nand efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.21036v1"
    },
    {
        "title": "Fine-Grained Clustering-Based Power Identification for Multicores",
        "authors": [
            "Mohamed R. Elshamy",
            "Mehdi Elahi",
            "Ahmad Patooghy",
            "Abdel-Hameed A. Badawy"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Fine-grained power estimation in multicore Systems on Chips (SoCs) is crucial\nfor efficient thermal management. BPI (Blind Power Identification) is a recent\napproach that determines the power consumption of different cores and the\nthermal model of the chip using only thermal sensor measurements and total\npower consumption. BPI relies on steady-state thermal data along with a naive\ninitialization in its Non-negative Matrix Factorization (NMF) process, which\nnegatively impacts the power estimation accuracy of BPI. This paper proposes a\ntwo-fold approach to reduce these impacts on BPI. First, this paper introduces\nan innovative approach for NMF initializing, i.e., density-oriented spatial\nclustering to identify centroid data points of active cores as initial values.\nThis enhances BPI accuracy by focusing on dense regions in the dataset and\nexcluding outlier data points. Second, it proposes the utilization of\nsteady-state temperature data points to enhance the power estimation accuracy\nby leveraging the physical relationship between temperature and power\nconsumption. Our extensive simulations of real-world cases demonstrate that our\napproach enhances BPI accuracy in estimating the power per core with no\nperformance cost. For instance, in a four-core processor, the proposed approach\nreduces the error rate by 76% compared to BPI and by 24% compared to the state\nof the art in the literature, namely, Blind Power Identification Steady State\n(BPISS). The results underline the potential of integrating advanced clustering\ntechniques in thermal model identification, paving the way for more accurate\nand reliable thermal management in multicores and SoCs.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.21261v1"
    },
    {
        "title": "Two Criteria for Performance Analysis of Optimization Algorithms",
        "authors": [
            "Yunpeng Jing",
            "HaiLin Liu",
            "Qunfeng Liu"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Performance analysis is crucial in optimization research, especially when\naddressing black-box problems through nature-inspired algorithms. Current\npractices often rely heavily on statistical methods, which can lead to various\nlogical paradoxes. To address this challenge, this paper introduces two\ncriteria to ensure that performance analysis is unaffected by irrelevant\nfactors. The first is the isomorphism criterion, which asserts that performance\nevaluation should remain unaffected by the modeling approach. The second is the\nIIA criterion,stating that comparisons between two algorithms should not be\ninfluenced by irrelevant third-party algorithms. Additionally, we conduct a\ncomprehensive examination of the underlying causes of these paradoxes, identify\nconditions for checking the criteria, and propose ideas to tackle these issues.\nThe criteria presented offer a framework for researchers to critically assess\nthe performance metrics or ranking methods, ultimately aiming to enhance the\nrigor of evaluation metrics and ranking methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.21677v1"
    },
    {
        "title": "Diversity in Network-Friendly Recommendations",
        "authors": [
            "Evangelia Tzimpimpaki",
            "Thrasyvoulos Spyropoulos"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  In recent years, the Internet has been dominated by content-rich platforms,\nemploying recommendation systems to provide users with more appealing content\n(e.g., videos in YouTube, movies in Netflix). While traditional content\nrecommendations are oblivious to network conditions, the paradigm of\nNetwork-Friendly Recommendations (NFR) has recently emerged, favoring content\nthat improves network performance (e.g. cached near the user), while still\nbeing appealing to the user. However, NFR algorithms sometimes achieve their\ngoal by shrinking the pool of content recommended to users. The undesirable\nside-effect is reduced content diversity, a phenomenon known as\n``content/filter bubble''. This reduced diversity is problematic for both\nusers, who are prevented from exploring a broader range of content, and content\ncreators (e.g. YouTubers) whose content may be recommended less frequently,\nleading to perceived unfairness. In this paper, we first investigate - using\nreal data and state-of-the-art NFR schemes - the extent of this phenomenon. We\nthen formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly\nrecommendations with - sufficient - content diversity), and through a series of\ntransformation steps, we manage to reduce it to a linear program that can be\nsolved fast and optimally. Our findings show that Diverse-NFR can achieve high\nnetwork gains (comparable to non-diverse NFR) while maintaining diversity\nconstraints. To our best knowledge, this is the first work that incorporates\ndiversity issues into network-friendly recommendation algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.00601v2"
    },
    {
        "title": "Overhead Measurement Noise in Different Runtime Environments",
        "authors": [
            "David Georg Reichelt",
            "Reiner Jung",
            "André van Hoorn"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  In order to detect performance changes, measurements are performed with the\nsame execution environment. In cloud environments, the noise from different\nprocesses running on the same cluster nodes might change measurement results\nand thereby make performance changes hard to measure.\n  The benchmark MooBench determines the overhead of different observability\ntools and is executed continuously. In this study, we compare the suitability\nof different execution environments to benchmark the observability overhead\nusing MooBench. To do so, we compare the execution times and standard deviation\nof MooBench in a cloud execution environment to three bare-metal execution\nenvironments. We find that bare metal servers have lower runtime and standard\ndeviation for multi-threaded MooBench execution. Nevertheless, we see that\nperformance changes up to 4.41% are detectable by GitHub actions, as long as\nonly sequential workloads are examined.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.05491v1"
    },
    {
        "title": "OSCAR-P and aMLLibrary: Profiling and Predicting the Performance of\n  FaaS-based Applications in Computing Continua",
        "authors": [
            "Roberto Sala",
            "Bruno Guindani",
            "Enrico Galimberti",
            "Federica Filippini",
            "Hamta Sedghani",
            "Danilo Ardagna",
            "Sebastián Risco",
            "Germán Moltó",
            "Miguel Caballer"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  This paper proposes an automated framework for efficient application\nprofiling and training of Machine Learning (ML) performance models, composed of\ntwo parts: OSCAR-P and aMLLibrary. OSCAR-P is an auto-profiling tool designed\nto automatically test serverless application workflows running on multiple\nhardware and node combinations in cloud and edge environments. OSCAR-P obtains\nrelevant profiling information on the execution time of the individual\napplication components. These data are later used by aMLLibrary to train\nML-based performance models. This makes it possible to predict the performance\nof applications on unseen configurations. We test our framework on clusters\nwith different architectures (x86 and arm64) and workloads, considering\nmulti-component use-case applications. This extensive experimental campaign\nproves the efficiency of OSCAR-P and aMLLibrary, significantly reducing the\ntime needed for the application profiling, data collection, and data\nprocessing. The preliminary results obtained on the ML performance models\naccuracy show a Mean Absolute Percentage Error lower than 30% in all the\nconsidered scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07687v1"
    },
    {
        "title": "Achieving Consistent and Comparable CPU Evaluation Outcomes",
        "authors": [
            "Chenxi Wang",
            "Lei Wang",
            "Wanling Gao",
            "Yikang Yang",
            "Yutong Zhou",
            "Jianfeng Zhan"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The SPEC CPU2017 benchmark suite is an industry standard for accessing CPU\nperformance. It adheres strictly to some workload and system configurations -\narbitrary specificity - while leaving other system configurations undefined -\narbitrary ambiguity. This article reveals: (1) Arbitrary specificity proves not\nmeaningful, obscuring many scenarios, as evidenced by significant performance\nvariations, a 74.49x performance difference observed on the same CPU. (2)\nArbitrary ambiguity is unfair as it fails to establish the same configurations\nfor comparing different CPUs.\n  We propose an innovative CPU evaluation methodology. It considers all\nworkload and system configurations valid and mandates each configuration to be\nwell-defined to avoid arbitrary specificity and ambiguity. To reduce the\nevaluation cost, a sampling approach is proposed to select a subset of the\nconfigurations. To expose CPU performance under different scenarios, it treats\nall outcomes under each configuration as equally important. Finally, it\nutilizes confidence level and confidence interval to report the outcomes to\navoid bias.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.08494v1"
    },
    {
        "title": "SparseInfer: Training-free Prediction of Activation Sparsity for Fast\n  LLM Inference",
        "authors": [
            "Jiho Shin",
            "Hoeseok Yang",
            "Youngmin Yi"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Leveraging sparsity is crucial for optimizing large language model inference.\nhowever, modern LLMs employing SiLU as their activation function exhibit\nminimal activation sparsity. Recent research has proposed replacing SiLU with\nReLU to induce significant activation sparsity and showed no downstream task\naccuracy degradation through fine tuning. However, taking full advantage of it\nrequired training a predictor to estimate this sparsity. In this paper, we\nintroduce SparseInfer, a simple, light weight, and training free predictor for\nactivation sparsity of ReLU field LLMs, in which activation sparsity is\npredicted by comparing only the sign bits of inputs and weights. To compensate\nfor possible prediction inaccuracy, an adaptive tuning of the predictor's\nconservativeness is enabled, which can also serve as a control knob for\noptimizing LLM inference. The proposed method achieves approximately faster\ninference speed over the state of the art, with negligible accuracy loss of\nwithin 1%p.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.12692v1"
    },
    {
        "title": "Static Reuse Profile Estimation for Array Applications",
        "authors": [
            "Abdur Razzak",
            "Atanu Barai",
            "Nandakishore Santhi",
            "Abdel-Hameed A. Badawy"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.13854v1"
    },
    {
        "title": "Optimizing Winograd Convolution on ARMv8 processors",
        "authors": [
            "Haoyuan Gui",
            "Xiaoyu Zhang",
            "Chong Zhang",
            "Zitong Su",
            "Huiyuan Li"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  As Convolutional Neural Networks (CNNs) gain prominence in deep learning,\nalgorithms like Winograd Convolution have been introduced to enhance\ncomputational efficiency. However, existing implementations often face\nchallenges such as high transformation overhead, suboptimal computation\nefficiency, and reduced parallel performance in some layers. We propose a fused\nWinograd Convolution algorithm optimized for ARMv8 CPUs, integrating input\ntransformation, filter transformation, computation, and output transformation\ninto a single pipeline. By maintaining consecutive memory access and using a\ncustom z-shaped data layout, our approach fully utilizes an optimized GEMM\nmicro-kernel with a ping-pong technique. Additionally, we introduce a\nmulti-dimensional parallel strategy that adapts to convolutional layer scales.\nTo maximize performance, we manually optimize each kernel in AArch64 assembly\nand carefully tune blocking parameters. Experimental results show speedups of\nup to 4.74x, 4.10x, 4.72x, and 10.57x over NCNN, NNPACK, FastConv, and ACL on\nthe Kunpeng 920 platform using multiple threads, with respective gains of\n3.85x, 2.81x, 4.20x, and 7.80x on the AWS Graviton2, and 3.32x, 3.68x, 8.00x,\nand 9.28x on the Phytium 2000+.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.16152v2"
    },
    {
        "title": "Accurate Performance Modeling And Uncertainty Analysis of Lossy\n  Compression in Scientific Applications",
        "authors": [
            "Youyuan Liu",
            "Taolue Yang",
            "Sian Jin"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Scientific applications typically generate large volumes of floating-point\ndata, making lossy compression one of the most effective methods for data\nreduction, thereby lowering storage requirements and improving performance in\nlarge-scale applications. However, variations in compression time can\nsignificantly impact overall performance improvement, due to inaccurate\nscheduling, workload imbalances, etc. Existing approaches rely on empirical\nmethods to predict the compression performance, which often lack\ninterpretability and suffer from limitations in accuracy and generalizability.\nIn this paper, we propose surrogate models for predicting the compression time\nof prediction-based lossy compression and provide a detailed analysis of the\nfactors influencing time variability with uncertainty analysis. Our evaluation\nshows that our solution can accuratly predict the compression time with 5%\naverage error across six scientific datasets. It also provides accurate 95%\nconfidence interval, which is essential for time-sensitive scheduling and\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.07015v1"
    },
    {
        "title": "Analyzing Practical Policies for Multiresource Job Scheduling",
        "authors": [
            "Zhongrui Chen",
            "Isaac Grosof",
            "Benjamin Berg"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Modern cloud computing workloads are composed of multiresource jobs that\nrequire a variety of computational resources in order to run, such as CPU\ncores, memory, disk space, or hardware accelerators. A single cloud server can\ntypically run many multiresource jobs in parallel, but only if the server has\nsufficient resources to satisfy the demands of every job. A scheduling policy\nmust therefore select sets of multiresource jobs to run in parallel in order to\nminimize the mean response time across jobs -- the average time from when a job\narrives to the system until it is completed. Unfortunately, achieving low\nresponse times by selecting sets of jobs that fully utilize the available\nserver resources has proven to be a difficult problem.\n  In this paper, we develop and analyze a new class of policies for scheduling\nmultiresource jobs, called Markovian Service Rate (MSR) policies. While prior\nscheduling policies for multiresource jobs are either highly complex to analyze\nor hard to implement, our MSR policies are simple to implement and are amenable\nto response time analysis. We show that the class of MSR policies is\nthroughput-optimal in that we can use an MSR policy to stabilize the system\nwhenever it is possible to do so. We also derive bounds on the mean response\ntime under an MSR algorithm that are tight up to an additive constant. These\nbounds can be applied to systems with different preemption behaviors, such as\nfully preemptive systems, non-preemptive systems, and systems that allow\npreemption with setup times. We show how our theoretical results can be used to\nselect a good MSR policy as a function of the system arrival rates, job service\nrequirements, the server's resource capacities, and the resource demands of the\njobs.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.08915v1"
    },
    {
        "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
        "authors": [
            "Miguel O. Blom",
            "Kristian F. D. Rietveld",
            "Rob V. van Nieuwpoort"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.16001v1"
    },
    {
        "title": "Resource Allocation Influence on Application Performance in Sliced\n  Testbeds",
        "authors": [
            "Rodrigo Moreira",
            "Larissa F. Rodrigues Moreira",
            "Tereza C. Carvalho",
            "Flávio de Oliveira Silva"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Modern network architectures have shaped market segments, governments, and\ncommunities with intelligent and pervasive applications. Ongoing digital\ntransformation through technologies such as softwarization, network slicing,\nand AI drives this evolution, along with research into Beyond 5G (B5G) and 6G\narchitectures. Network slices require seamless management, observability, and\nintelligent-native resource allocation, considering user satisfaction, cost\nefficiency, security, and energy. Slicing orchestration architectures have been\nextensively studied to accommodate these requirements, particularly in resource\nallocation for network slices. This study explored the observability of\nresource allocation regarding network slice performance in two nationwide\ntestbeds. We examined their allocation effects on slicing connectivity latency\nusing a partial factorial experimental method with Central Processing Unit\n(CPU) and memory combinations. The results reveal different resource impacts\nacross the testbeds, indicating a non-uniform influence on the CPU and memory\nwithin the same network slice.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.16716v1"
    },
    {
        "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
        "authors": [
            "Harsh Kumar",
            "R. Govindarajan"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.19051v1"
    },
    {
        "title": "A Priori Loop Nest Normalization: Automatic Loop Scheduling in Complex\n  Applications",
        "authors": [
            "Lukas Trümper",
            "Philipp Schaad",
            "Berke Ates",
            "Alexandru Calotoiu",
            "Marcin Copik",
            "Torsten Hoefler"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The same computations are often expressed differently across software\nprojects and programming languages. In particular, how computations involving\nloops are expressed varies due to the many possibilities to permute and compose\nloops. Since each variant may have unique performance properties, automatic\napproaches to loop scheduling must support many different optimization recipes.\nIn this paper, we propose a priori loop nest normalization to align loop nests\nand reduce the variation before the optimization. Specifically, we define and\napply normalization criteria, mapping loop nests with different memory access\npatterns to the same canonical form. Since the memory access pattern is\nsusceptible to loop variations and critical for performance, this normalization\nallows many loop nests to be optimized by the same optimization recipe. To\nevaluate our approach, we apply the normalization with optimizations designed\nfor only the canonical form, improving the performance of many different loop\nnest variants. Across multiple implementations of 15 benchmarks using different\nlanguages, we outperform a baseline compiler in C on average by a factor of\n$21.13$, state-of-the-art auto-schedulers such as Polly and the Tiramisu\nauto-scheduler by $2.31$ and $2.89$, as well as performance-oriented\nPython-based frameworks such as NumPy, Numba, and DaCe by $9.04$, $3.92$, and\n$1.47$. Furthermore, we apply the concept to the CLOUDSC cloud microphysics\nscheme, an actively used component of the Integrated Forecasting System,\nachieving a 10% speedup over the highly-tuned Fortran code.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.20179v1"
    },
    {
        "title": "Latency and Privacy-Aware Resource Allocation in Vehicular Edge\n  Computing",
        "authors": [
            "Hossein Ahmadvand",
            "Fouzhan Foroutan"
        ],
        "category": "cs.PF",
        "published_year": "2025",
        "summary": "  The rapid increase in the number of connected vehicles has led to the\ngeneration of vast amounts of data. As a significant portion of this data\npertains to vehicle-to-vehicle and vehicle-to-infrastructure communications, it\nis predominantly generated at the edge. Considering the enormous volume of\ndata, real-time applications, and privacy concerns, it is crucial to process\nthe data at the edge. Neglecting the management of processing resources in\nvehicular edge computing (VEC) could lead to numerous challenges as a\nsubstantial number of vehicles with diverse safety, economic, and entertainment\napplications, along with their data processing, emerge in the near future [1].\nPrevious research in VEC resource allocation has primarily focused on issues\nsuch as response time and privacy preservation techniques. However, an approach\nthat takes into account privacy-aware resource allocation based on vehicular\nnetwork architecture and application requirements has not yet been proposed. In\nthis paper, we present a privacy and latency-aware approach for allocating\nprocessing resources at the edge of the vehicular network, considering the\nspecific requirements of different applications. Our approach involves\ncategorizing vehicular network applications based on their processing accuracy,\nreal-time processing needs, and privacy preservation requirements. We further\ndivide the vehicular network edge into two parts: the user layer (OBUs) is\nconsidered for processing applications with privacy requirements, while the\nallocation of resources in the RSUs and cloud layer is based on the specific\nneeds of different applications. In this study, we evaluate the quality of\nservice based on parameters such as privacy preservation, processing cost,\nmeeting deadlines, and result quality. Comparative analyses demonstrate that\nour approach enhances service quality by 55% compared to existing\nstate-of-the-art methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.02804v2"
    },
    {
        "title": "The X-Files: Investigating Alien Performance in a Thin-client World",
        "authors": [
            "Neil J. Gunther"
        ],
        "category": "cs.PF",
        "published_year": "2000",
        "summary": "  Many scientific applications use the X11 window environment; an open source\nwindows GUI standard employing a client/server architecture. X11 promotes:\ndistributed computing, thin-client functionality, cheap desktop displays,\ncompatibility with heterogeneous servers, remote services and administration,\nand greater maturity than newer web technologies. This paper details the\nauthor's investigations into close encounters with alien performance in\nX11-based seismic applications running on a 200-node cluster, backed by 2 TB of\nmass storage. End-users cited two significant UFOs (Unidentified Faulty\nOperations) i) long application launch times and ii) poor interactive response\ntimes. The paper is divided into three major sections describing Close\nEncounters of the 1st Kind: citings of UFO experiences, the 2nd Kind: recording\nevidence of a UFO, and the 3rd Kind: contact and analysis. UFOs do exist and\nthis investigation presents a real case study for evaluating workload analysis\nand other diagnostic tools.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0006016v1"
    },
    {
        "title": "Efficient cache use for stencil operations on structured discretization\n  grids",
        "authors": [
            "Michael A. Frumkin",
            "Rob F. Van der Wijngaart"
        ],
        "category": "cs.PF",
        "published_year": "2000",
        "summary": "  We derive tight bounds on cache misses for evaluation of explicit stencil\noperators on structured grids. Our lower bound is based on the isoperimetrical\nproperty of the discrete octahedron. Our upper bound is based on good surface\nto volume ratio of a parallelepiped spanned by a reduced basis of the inter-\nference lattice of a grid. Measurements show that our algorithm typically\nreduces the number of cache misses by factor of three relative to a compiler\noptimized code. We show that stencil calculations on grids whose interference\nlattice have a short vector feature abnormally high numbers of cache misses. We\ncall such grids unfavorable and suggest to avoid these in computations by\nappropriate padding. By direct measurements on MIPS R10000 we show a good\ncorrelation of abnormally high cache misses and unfavorable three-dimensional\ngrids.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0007027v1"
    },
    {
        "title": "The Role of Commutativity in Constraint Propagation Algorithms",
        "authors": [
            "Krzysztof R. Apt"
        ],
        "category": "cs.PF",
        "published_year": "2000",
        "summary": "  Constraint propagation algorithms form an important part of most of the\nconstraint programming systems. We provide here a simple, yet very general\nframework that allows us to explain several constraint propagation algorithms\nin a systematic way. In this framework we proceed in two steps. First, we\nintroduce a generic iteration algorithm on partial orderings and prove its\ncorrectness in an abstract setting. Then we instantiate this algorithm with\nspecific partial orderings and functions to obtain specific constraint\npropagation algorithms.\n  In particular, using the notions commutativity and semi-commutativity, we\nshow that the {\\tt AC-3}, {\\tt PC-2}, {\\tt DAC} and {\\tt DPC} algorithms for\nachieving (directional) arc consistency and (directional) path consistency are\ninstances of a single generic algorithm. The work reported here extends and\nsimplifies that of Apt \\citeyear{Apt99b}.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0012010v1"
    },
    {
        "title": "Efficient Instrumentation for Performance Profiling",
        "authors": [
            "Edu Metz",
            "Raimondas Lencevicius"
        ],
        "category": "cs.PF",
        "published_year": "2003",
        "summary": "  Performance profiling consists of tracing a software system during execution\nand then analyzing the obtained traces. However, traces themselves affect the\nperformance of the system distorting its execution. Therefore, there is a need\nto minimize the effect of the tracing on the underlying system's performance.\nTo achieve this, the trace set needs to be optimized according to the\nperformance profiling problem being solved. Our position is that such\nminimization can be achieved only by adding the software trace design and\nimplementation to the overall software development process. In such a process,\nthe performance analyst supplies the knowledge of performance measurement\nrequirements, while the software developer supplies the knowledge of the\nsoftware. Both of these are needed for an optimal trace placement.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0307058v1"
    },
    {
        "title": "Benchmarking Blunders and Things That Go Bump in the Night",
        "authors": [
            "Neil J. Gunther"
        ],
        "category": "cs.PF",
        "published_year": "2004",
        "summary": "  Benchmarking; by which I mean any computer system that is driven by a\ncontrolled workload, is the ultimate in performance testing and simulation.\nAside from being a form of institutionalized cheating, it also offer countless\nopportunities for systematic mistakes in the way the workloads are applied and\nthe resulting measurements interpreted. Right test, wrong conclusion is a\nubiquitous mistake that happens because test engineers tend to treat data as\ndivine. Such reverence is not only misplaced, it's also a sure ticket to\nproduction hell when the application finally goes live. I demonstrate how such\nmistakes can be avoided by means of two war stories that are real WOPRs. (a)\nHow to resolve benchmark flaws over the psychic hotline and (b) How benchmarks\ncan go flat with too much Java juice. In each case I present simple performance\nmodels and show how they can be applied to correctly assess benchmark data.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0404043v1"
    },
    {
        "title": "DiPerF: an automated DIstributed PERformance testing Framework",
        "authors": [
            "Catalin Dumitrescu",
            "Ioan Raicu",
            "Matei Ripeanu",
            "Ian Foster"
        ],
        "category": "cs.PF",
        "published_year": "2004",
        "summary": "  We present DiPerF, a distributed performance testing framework, aimed at\nsimplifying and automating service performance evaluation. DiPerF coordinates a\npool of machines that test a target service, collects and aggregates\nperformance metrics, and generates performance statistics. The aggregate data\ncollected provide information on service throughput, on service \"fairness\" when\nserving multiple clients concurrently, and on the impact of network latency on\nservice performance. Furthermore, using this data, it is possible to build\npredictive models that estimate a service performance given the service load.\nWe have tested DiPerF on 100+ machines on two testbeds, Grid3 and PlanetLab,\nand explored the performance of job submission services (pre WS GRAM and WS\nGRAM) included with Globus Toolkit 3.2.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0410012v1"
    },
    {
        "title": "Correlated dynamics in human printing behavior",
        "authors": [
            "Uli Harder",
            "Maya Paczuski"
        ],
        "category": "cs.PF",
        "published_year": "2004",
        "summary": "  Arrival times of requests to print in a student laboratory were analyzed.\nInter-arrival times between subsequent requests follow a universal scaling law\nrelating time intervals and the size of the request, indicating a scale\ninvariant dynamics with respect to the size. The cumulative distribution of\nfile sizes is well-described by a modified power law often seen in\nnon-equilibrium critical systems. For each user, waiting times between their\nindividual requests show long range dependence and are broadly distributed from\nseconds to weeks. All results are incompatible with Poisson models, and may\nprovide evidence of critical dynamics associated with voluntary thought\nprocesses in the brain.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0412027v1"
    },
    {
        "title": "Compression Scheme for Faster and Secure Data Transmission Over Internet",
        "authors": [
            "B. S. Shajeemohan",
            "Dr. V. K. Govindan"
        ],
        "category": "cs.PF",
        "published_year": "2006",
        "summary": "  Compression algorithms reduce the redundancy in data representation to\ndecrease the storage required for that data. Data compression offers an\nattractive approach to reducing communication costs by using available\nbandwidth effectively. Over the last decade there has been an unprecedented\nexplosion in the amount of digital data transmitted via the Internet,\nrepresenting text, images, video, sound, computer programs, etc. With this\ntrend expected to continue, it makes sense to pursue research on developing\nalgorithms that can most effectively use available network bandwidth by\nmaximally compressing data. It is also important to consider the security\naspects of the data being transmitted while compressing it, as most of the text\ndata transmitted over the Internet is very much vulnerable to a multitude of\nattacks. This paper is focused on addressing this problem of lossless\ncompression of text files with an added security.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0601097v2"
    },
    {
        "title": "Benchmark Problems for Constraint Solving",
        "authors": [
            "Alin Suciu",
            "Rodica Potolea",
            "Tudor Muresan"
        ],
        "category": "cs.PF",
        "published_year": "2006",
        "summary": "  Constraint Programming is roughly a new software technology introduced by\nJaffar and Lassez in 1987 for description and effective solving of large,\nparticularly combinatorial, problems especially in areas of planning and\nscheduling. In the following we define three problems for constraint solving\nfrom the domain of electrical networks; based on them we define 43 related\nproblems. For the defined set of problems we benchmarked five systems: ILOG\nOPL, AMPL, GAMS, Mathematica and UniCalc. As expected some of the systems\nperformed very well for some problems while others performed very well on\nothers.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0603099v1"
    },
    {
        "title": "Empirical analysis and statistical modeling of attack processes based on\n  honeypots",
        "authors": [
            "Mohamed Kaaniche",
            "Y. Deswarte",
            "Eric Alata",
            "Marc Dacier",
            "Vincent Nicomette"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  Honeypots are more and more used to collect data on malicious activities on\nthe Internet and to better understand the strategies and techniques used by\nattackers to compromise target systems. Analysis and modeling methodologies are\nneeded to support the characterization of attack processes based on the data\ncollected from the honeypots. This paper presents some empirical analyses based\non the data collected from the Leurr{\\'e}.com honeypot platforms deployed on\nthe Internet and presents some preliminary modeling studies aimed at fulfilling\nsuch objectives.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0861v1"
    },
    {
        "title": "An architecture-based dependability modeling framework using AADL",
        "authors": [
            "Ana-Elena Rugina",
            "Karama Kanoun",
            "Mohamed Kaaniche"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  For efficiency reasons, the software system designers' will is to use an\nintegrated set of methods and tools to describe specifications and designs, and\nalso to perform analyses such as dependability, schedulability and performance.\nAADL (Architecture Analysis and Design Language) has proved to be efficient for\nsoftware architecture modeling. In addition, AADL was designed to accommodate\nseveral types of analyses. This paper presents an iterative dependency-driven\napproach for dependability modeling using AADL. It is illustrated on a small\nexample. This approach is part of a complete framework that allows the\ngeneration of dependability analysis and evaluation models from AADL models to\nsupport the analysis of software and system architectures, in critical\napplication domains.\n",
        "pdf_link": "http://arxiv.org/pdf/0704.0865v1"
    },
    {
        "title": "An Extensible Timing Infrastructure for Adaptive Large-scale\n  Applications",
        "authors": [
            "Dylan Stark",
            "Gabrielle Allen",
            "Tom Goodale",
            "Thomas Radke",
            "Erik Schnetter"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  Real-time access to accurate and reliable timing information is necessary to\nprofile scientific applications, and crucial as simulations become increasingly\ncomplex, adaptive, and large-scale. The Cactus Framework provides flexible and\nextensible capabilities for timing information through a well designed\ninfrastructure and timing API. Applications built with Cactus automatically\ngain access to built-in timers, such as gettimeofday and getrusage,\nsystem-specific hardware clocks, and high-level interfaces such as PAPI. We\ndescribe the Cactus timer interface, its motivation, and its implementation. We\nthen demonstrate how this timing information can be used by an example\nscientific application to profile itself, and to dynamically adapt itself to a\nchanging environment at run time.\n",
        "pdf_link": "http://arxiv.org/pdf/0705.3015v1"
    },
    {
        "title": "A General Theory of Computational Scalability Based on Rational\n  Functions",
        "authors": [
            "Neil J. Gunther"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  The universal scalability law of computational capacity is a rational\nfunction C_p = P(p)/Q(p) with P(p) a linear polynomial and Q(p) a second-degree\npolynomial in the number of physical processors p, that has been long used for\nstatistical modeling and prediction of computer system performance. We prove\nthat C_p is equivalent to the synchronous throughput bound for a\nmachine-repairman with state-dependent service rate. Simpler rational\nfunctions, such as Amdahl's law and Gustafson speedup, are corollaries of this\nqueue-theoretic bound. C_p is further shown to be both necessary and sufficient\nfor modeling all practical characteristics of computational scalability.\n",
        "pdf_link": "http://arxiv.org/pdf/0808.1431v2"
    },
    {
        "title": "Multidimensional Visualization of Oracle Performance Using Barry007",
        "authors": [
            "Tanel Poder",
            "Neil J. Gunther"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  Most generic performance tools display only system-level performance data\nusing 2-dimensional plots or diagrams and this limits the informational detail\nthat can be displayed. Moreover, a modern relational database system, like\nOracle, can concurrently serve thousands of client processes with different\nworkload characteristics, so that generic performance-data displays inevitably\nhide important information. Drawing on our previous work, this paper\ndemonstrates the application of Barry007 multidimensional visualization to the\nanalysis of Oracle end-user, session-level, performance data, showing both\ncollective trends and individual performance anomalies.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.2532v1"
    },
    {
        "title": "Getting in the Zone for Successful Scalability",
        "authors": [
            "Jim Holtman",
            "Neil J. Gunther"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  The universal scalability law (USL) is an analytic model used to quantify\napplication scaling. It is universal because it subsumes Amdahl's law and\nGustafson linearized scaling as special cases. Using simulation, we show: (i)\nthat the USL is equivalent to synchronous queueing in a load-dependent machine\nrepairman model and (ii) how USL, Amdahl's law, and Gustafson scaling can be\nregarded as boundaries defining three scalability zones. Typical throughput\nmeasurements lie across all three zones. Simulation scenarios provide deeper\ninsight into queueing effects and thus provide a clearer indication of which\napplication features should be tuned to get into the optimal performance zone.\n",
        "pdf_link": "http://arxiv.org/pdf/0809.2541v1"
    },
    {
        "title": "A Proof of Concept for Optimizing Task Parallelism by Locality Queues",
        "authors": [
            "Markus Wittmann",
            "Georg Hager"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  Task parallelism as employed by the OpenMP task construct, although ideal for\ntackling irregular problems or typical producer/consumer schemes, bears some\npotential for performance bottlenecks if locality of data access is important,\nwhich is typically the case for memory-bound code on ccNUMA systems. We present\na programming technique which ameliorates adverse effects of dynamic task\ndistribution by sorting tasks into locality queues, each of which is preferably\nprocessed by threads that belong to the same locality domain. Dynamic\nscheduling is fully preserved inside each domain, and is preferred over\npossible load imbalance even if non-local access is required. The effectiveness\nof the approach is demonstrated using a blocked six-point stencil solver as a\ntoy model.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.1884v1"
    },
    {
        "title": "Introducing a Performance Model for Bandwidth-Limited Loop Kernels",
        "authors": [
            "Jan Treibig",
            "Georg Hager"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  We present a performance model for bandwidth limited loop kernels which is\nfounded on the analysis of modern cache based microarchitectures. This model\nallows an accurate performance prediction and evaluation for existing\ninstruction codes. It provides an in-depth understanding of how performance for\ndifferent memory hierarchy levels is made up. The performance of raw memory\nload, store and copy operations and a stream vector triad are analyzed and\nbenchmarked on three modern x86-type quad-core architectures in order to\ndemonstrate the capabilities of the model.\n",
        "pdf_link": "http://arxiv.org/pdf/0905.0792v1"
    },
    {
        "title": "Multi-core architectures: Complexities of performance prediction and the\n  impact of cache topology",
        "authors": [
            "Jan Treibig",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  The balance metric is a simple approach to estimate the performance of\nbandwidth-limited loop kernels. However, applying the method to in-cache\nsituations and modern multi-core architectures yields unsatisfactory results.\nThis paper analyzes the in uence of cache hierarchy design on performance\npredictions for bandwidth-limited loop kernels on current mainstream\nprocessors. We present a diagnostic model with improved predictive power,\ncorrecting the limitations of the simple balance metric. The importance of code\nexecution overhead even in bandwidth-bound situations is emphasized. Finally we\nanalyze the impact of synchronization overhead on multi-threaded performance\nwith a special emphasis on the in uence of cache topology.\n",
        "pdf_link": "http://arxiv.org/pdf/0910.4865v1"
    },
    {
        "title": "Multicore-aware parallel temporal blocking of stencil codes for shared\n  and distributed memory",
        "authors": [
            "Markus Wittmann",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  New algorithms and optimization techniques are needed to balance the\naccelerating trend towards bandwidth-starved multicore chips. It is well known\nthat the performance of stencil codes can be improved by temporal blocking,\nlessening the pressure on the memory interface. We introduce a new pipelined\napproach that makes explicit use of shared caches in multicore environments and\nminimizes synchronization and boundary overhead. For clusters of shared-memory\nnodes we demonstrate how temporal blocking can be employed successfully in a\nhybrid shared/distributed-memory environment.\n",
        "pdf_link": "http://arxiv.org/pdf/0912.4506v1"
    },
    {
        "title": "RapidMind: Portability across Architectures and its Limitations",
        "authors": [
            "Iris Christadler",
            "Volker Weinberg"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Recently, hybrid architectures using accelerators like GPGPUs or the Cell\nprocessor have gained much interest in the HPC community. The RapidMind\nMulti-Core Development Platform is a programming environment that allows\ngenerating code which is able to seamlessly run on hardware accelerators like\nGPUs or the Cell processor and multicore CPUs both from AMD and Intel. This\npaper describes the ports of three mathematical kernels to RapidMind which are\nchosen as synthetic benchmarks and representatives of scientific codes.\nPerformance of these kernels has been measured on various RapidMind backends\n(cuda, cell and x86) and compared to other hardware-specific implementations\n(using CUDA, Cell SDK and Intel MKL). The results give an insight in the degree\nof portability of RapidMind code and code performance across different\narchitectures.\n",
        "pdf_link": "http://arxiv.org/pdf/1001.1902v2"
    },
    {
        "title": "Modeling the Probability of Failure on LDAP Binding Operations in\n  Iplanet Web Proxy 3.6 Server",
        "authors": [
            "Alejandro Chinea Manrique de Lara"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  This paper is devoted to the theoretical analysis of a problem derived from\ninteraction between two Iplanet products: Web Proxy Server and the Directory\nServer. In particular, a probabilistic and stochastic-approximation model is\nproposed to minimize the occurrence of LDAP connection failures in Iplanet Web\nProxy 3.6 Server. The proposed model serves not only to provide a\nparameterization of the aforementioned phenomena, but also to provide\nmeaningful insights illustrating and supporting these theoretical results. In\naddition, we shall also address practical considerations when estimating the\nparameters of the proposed model from experimental data. Finally, we shall\nprovide some interesting results from real-world data collected from our\ncustomers.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.2450v1"
    },
    {
        "title": "Efficient multicore-aware parallelization strategies for iterative\n  stencil computations",
        "authors": [
            "Jan Treibig",
            "Gerhard Wellein",
            "Georg Hager"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Stencil computations consume a major part of runtime in many scientific\nsimulation codes. As prototypes for this class of algorithms we consider the\niterative Jacobi and Gauss-Seidel smoothers and aim at highly efficient\nparallel implementations for cache-based multicore architectures. Temporal\ncache blocking is a known advanced optimization technique, which can reduce the\npressure on the memory bus significantly. We apply and refine this optimization\nfor a recently presented temporal blocking strategy designed to explicitly\nutilize multicore characteristics. Especially for the case of Gauss-Seidel\nsmoothers we show that simultaneous multi-threading (SMT) can yield substantial\nperformance improvements for our optimized algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1741v1"
    },
    {
        "title": "Profit-Aware Server Allocation for Green Internet Services",
        "authors": [
            "Michele Mazzucco",
            "Dmytro Dyachuk",
            "Marios Dikaiakos"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  A server farm is examined, where a number of servers are used to offer a\nservice to impatient customers. Every completed request generates a certain\namount of profit, running servers consume electricity for power and cooling,\nwhile waiting customers might leave the system before receiving service if they\nexperience excessive delays. A dynamic allocation policy aiming at satisfying\nthe conflicting goals of maximizing the quality of users' experience while\nminimizing the cost for the provider is introduced and evaluated. The results\nof several experiments are described, showing that the proposed scheme performs\nwell under different traffic conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3059v1"
    },
    {
        "title": "Allocation and Admission Policies for Service Streams",
        "authors": [
            "Michele Mazzucco",
            "Isi Mitrani",
            "Mike Fisher",
            "Paul McKee"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  A service provisioning system is examined, where a number of servers are used\nto offer different types of services to paying customers. A customer is charged\nfor the execution of a stream of jobs; the number of jobs in the stream and the\nrate of their submission is specified. On the other hand, the provider promises\na certain quality of service (QoS), measured by the average waiting time of the\njobs in the stream. A penalty is paid if the agreed QoS requirement is not met.\nThe objective is to maximize the total average revenue per unit time. Dynamic\npolicies for making server allocation and stream admission decisions are\nintroduced and evaluated. The results of several simulations are described.\n",
        "pdf_link": "http://arxiv.org/pdf/1102.3703v1"
    },
    {
        "title": "Expression Templates Revisited: A Performance Analysis of the Current ET\n  Methodology",
        "authors": [
            "Klaus Iglberger",
            "Georg Hager",
            "Jan Treibig",
            "Ulrich Ruede"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  In the last decade, Expression Templates (ET) have gained a reputation as an\nefficient performance optimization tool for C++ codes. This reputation builds\non several ET-based linear algebra frameworks focused on combining both elegant\nand high-performance C++ code. However, on closer examination the assumption\nthat ETs are a performance optimization technique cannot be maintained. In this\npaper we demonstrate and explain the inability of current ET-based frameworks\nto deliver high performance for dense and sparse linear algebra operations, and\nintroduce a new \"smart\" ET implementation that truly allows the combination of\nhigh performance code with the elegance and maintainability of a\ndomain-specific language.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.1729v1"
    },
    {
        "title": "Pushing the limits for medical image reconstruction on recent standard\n  multicore processors",
        "authors": [
            "Jan Treibig",
            "Georg Hager",
            "Hannes G. Hofmann",
            "Joachim Hornegger",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Volume reconstruction by backprojection is the computational bottleneck in\nmany interventional clinical computed tomography (CT) applications. Today\nvendors in this field replace special purpose hardware accelerators by standard\nhardware like multicore chips and GPGPUs. Medical imaging algorithms are on the\nverge of employing High Performance Computing (HPC) technology, and are\ntherefore an interesting new candidate for optimization. This paper presents\nlow-level optimizations for the backprojection algorithm, guided by a thorough\nperformance analysis on four generations of Intel multicore processors\n(Harpertown, Westmere, Westmere EX, and Sandy Bridge).\n  We choose the RabbitCT benchmark, a standardized testcase well supported in\nindustry, to ensure transparent and comparable results. Our aim is to provide\nnot only the fastest possible implementation but also compare to performance\nmodels and hardware counter data in order to fully understand the results. We\nseparate the influence of algorithmic optimizations, parallelization, SIMD\nvectorization, and microarchitectural issues and pinpoint problems with current\nSIMD instruction set extensions on standard CPUs (SSE, AVX). The use of\nassembly language is mandatory for best performance. Finally we compare our\nresults to the best GPGPU implementations available for this open competition\nbenchmark.\n",
        "pdf_link": "http://arxiv.org/pdf/1104.5243v2"
    },
    {
        "title": "AWRP: Adaptive Weight Ranking Policy for Improving Cache Performance",
        "authors": [
            "Debabala Swain",
            "Bijay Paikaray",
            "Debabrata Swain"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Due to the huge difference in performance between the computer memory and\nprocessor, the virtual memory management plays a vital role in system\nperformance. A Cache memory is the fast memory which is used to compensate the\nspeed difference between the memory and processor. This paper gives an adaptive\nreplacement policy over the traditional policy which has low overhead, better\nperformance and is easy to implement. Simulations show that our algorithm\nperforms better than Least-Recently-Used (LRU), First-In-First-Out (FIFO) and\nClock with Adaptive Replacement (CAR).\n",
        "pdf_link": "http://arxiv.org/pdf/1107.4851v1"
    },
    {
        "title": "A Stochastic Calculus for Network Systems with Renewable Energy Sources",
        "authors": [
            "Kui Wu",
            "Yuming Jiang",
            "Dimitri Marinakis"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  We consider the performance modeling and evaluation of network systems\npowered with renewable energy sources such as solar and wind energy. Such\nenergy sources largely depend on environmental conditions, which are hard to\npredict accurately. As such, it may only make sense to require the network\nsystems to support a soft quality of service (QoS) guarantee, i.e., to\nguarantee a service requirement with a certain high probability. In this paper,\nwe intend to build a solid mathematical foundation to help better understand\nthe stochastic energy constraint and the inherent correlation between QoS and\nthe uncertain energy supply. We utilize a calculus approach to model the\ncumulative amount of charged energy and the cumulative amount of consumed\nenergy. We derive upper and lower bounds on the remaining energy level based on\na stochastic energy charging rate and a stochastic energy discharging rate. By\nbuilding the bridge between energy consumption and task execution (i.e.,\nservice), we study the QoS guarantee under the constraint of uncertain energy\nsources. We further show how performance bounds can be improved if some strong\nassumptions can be made.\n",
        "pdf_link": "http://arxiv.org/pdf/1108.1554v1"
    },
    {
        "title": "Stability of a Peer-to-Peer Communication System",
        "authors": [
            "Ji Zhu",
            "Bruce Hajek"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  This paper focuses on the stationary portion of file download in an\nunstructured peer-to-peer network, which typically follows for many hours after\na flash crowd initiation. The model includes the case that peers can have some\npieces at the time of arrival. The contribution of the paper is to identify how\nmuch help is needed from the seeds, either fixed seeds or peer seeds (which are\npeers remaining in the system after obtaining a complete collection) to\nstabilize the system. The dominant cause for instability is the missing piece\nsyndrome, whereby one piece becomes very rare in the network. It is shown that\nstability can be achieved with only a small amount of help from peer\nseeds--even with very little help from a fixed seed, peers need dwell as peer\nseeds on average only long enough to upload one additional piece. The region of\nstability is insensitive to the piece selection policy. Network coding can\nsubstantially increase the region of stability in case a portion of the new\npeers arrive with randomly coded pieces.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.2753v2"
    },
    {
        "title": "Optimization strategies for parallel CPU and GPU implementations of a\n  meshfree particle method",
        "authors": [
            "Jose M. Domínguez",
            "Alejandro J. C. Crespo",
            "Moncho Gómez-Gesteira"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Much of the current focus in high performance computing (HPC) for\ncomputational fluid dynamics (CFD) deals with grid based methods. However,\nparallel implementations for new meshfree particle methods such as Smoothed\nParticle Hydrodynamics (SPH) are less studied. In this work, we present\noptimizations for both central processing unit (CPU) and graphics processing\nunit (GPU) of a SPH method. These optimization strategies can be further\napplied to many other meshfree methods. The obtained performance for each\narchitecture and a comparison between the most efficient implementations for\nCPU and GPU are shown.\n",
        "pdf_link": "http://arxiv.org/pdf/1110.3711v3"
    },
    {
        "title": "Parametric Estimation of the Ultimate Size of Hypercomputers",
        "authors": [
            "Dmitry Zinoviev"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  The performance of the emerging petaflops-scale supercomputers of the nearest\nfuture (hypercomputers) will be governed not only by the clock frequency of the\nprocessing nodes or by the width of the system bus, but also by such factors as\nthe overall power consumption and the geometric size. In this paper, we study\nthe influence of such parameters on one of the most important characteristics\nof a general purpose computer - on the degree of multithreading that must be\npresent in an application to make the use of the hypercomputer justifiable. Our\nmajor finding is that for the class of applications with purely random memory\naccess patterns \"super-fast computing\" and \"high-performance computing\" are\nessentially synonyms for \"massively-parallel computing.\"\n",
        "pdf_link": "http://arxiv.org/pdf/1111.4287v1"
    },
    {
        "title": "On the Reliability of RAID Systems: An Argument for More Check Drives",
        "authors": [
            "Sarah Edge Mann",
            "Michael Anderson",
            "Marek Rychlik"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  In this paper we address issues of reliability of RAID systems. We focus on\n\"big data\" systems with a large number of drives and advanced error correction\nschemes beyond \\RAID{6}. Our RAID paradigm is based on Reed-Solomon codes, and\nthus we assume that the RAID consists of $N$ data drives and $M$ check drives.\nThe RAID fails only if the combined number of failed drives and sector errors\nexceeds $M$, a property of Reed-Solomon codes.\n  We review a number of models considered in the literature and build upon them\nto construct models usable for a large number of data and check drives. We\nattempt to account for a significant number of factors that affect RAID\nreliability, such as drive replacement or lack thereof, mistakes during service\nsuch as replacing the wrong drive, delayed repair, and the finite duration of\nRAID reconstruction. We evaluate the impact of sector failures that do not\nresult in drive replacement.\n  The reader who needs to consider large $M$ and $N$ will find applicable\nmathematical techniques concisely summarized here, and should be able to apply\nthem to similar problems. Most methods are based on the theory of continuous\ntime Markov chains, but we move beyond this framework when we consider the\nfixed time to rebuild broken hard drives, which we model using systems of delay\nand partial differential equations.\n  One universal statement is applicable across various models: increasing the\nnumber of check drives in all cases increases the reliability of the system,\nand is vastly superior to other approaches of ensuring reliability such as\nmirroring.\n",
        "pdf_link": "http://arxiv.org/pdf/1202.4423v1"
    },
    {
        "title": "Heavy Traffic Optimal Resource Allocation Algorithms for Cloud Computing\n  Clusters",
        "authors": [
            "Siva Theja Maguluri",
            "R Srikant",
            "Lei Ying"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Cloud computing is emerging as an important platform for business, personal\nand mobile computing applications. In this paper, we study a stochastic model\nof cloud computing, where jobs arrive according to a stochastic process and\nrequest resources like CPU, memory and storage space. We consider a model where\nthe resource allocation problem can be separated into a routing or load\nbalancing problem and a scheduling problem. We study the\njoin-the-shortest-queue routing and power-of-two-choices routing algorithms\nwith MaxWeight scheduling algorithm. It was known that these algorithms are\nthroughput optimal. In this paper, we show that these algorithms are queue\nlength optimal in the heavy traffic limit.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.1264v1"
    },
    {
        "title": "Best practices for HPM-assisted performance engineering on modern\n  multicore processors",
        "authors": [
            "Jan Treibig",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Many tools and libraries employ hardware performance monitoring (HPM) on\nmodern processors, and using this data for performance assessment and as a\nstarting point for code optimizations is very popular. However, such data is\nonly useful if it is interpreted with care, and if the right metrics are chosen\nfor the right purpose. We demonstrate the sensible use of hardware performance\ncounters in the context of a structured performance engineering approach for\napplications in computational science. Typical performance patterns and their\nrespective metric signatures are defined, and some of them are illustrated\nusing case studies. Although these generic concepts do not depend on specific\ntools or environments, we restrict ourselves to modern x86-based multicore\nprocessors and use the likwid-perfctr tool under the Linux OS.\n",
        "pdf_link": "http://arxiv.org/pdf/1206.3738v1"
    },
    {
        "title": "Exploring performance and power properties of modern multicore chips via\n  simple machine models",
        "authors": [
            "Georg Hager",
            "Jan Treibig",
            "Johannes Habich",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Modern multicore chips show complex behavior with respect to performance and\npower. Starting with the Intel Sandy Bridge processor, it has become possible\nto directly measure the power dissipation of a CPU chip and correlate this data\nwith the performance properties of the running code. Going beyond a simple\nbottleneck analysis, we employ the recently published Execution-Cache-Memory\n(ECM) model to describe the single- and multi-core performance of streaming\nkernels. The model refines the well-known roofline model, since it can predict\nthe scaling and the saturation behavior of bandwidth-limited loop kernels on a\nmulticore chip. The saturation point is especially relevant for considerations\nof energy consumption. From power dissipation measurements of benchmark\nprograms with vastly different requirements to the hardware, we derive a\nsimple, phenomenological power model for the Sandy Bridge processor. Together\nwith the ECM model, we are able to explain many peculiarities in the\nperformance and power behavior of multicore processors, and derive guidelines\nfor energy-efficient execution of parallel programs. Finally, we show that the\nECM and power models can be successfully used to describe the scaling and power\nbehavior of a lattice-Boltzmann flow solver code.\n",
        "pdf_link": "http://arxiv.org/pdf/1208.2908v4"
    },
    {
        "title": "Distribution of the Number of Retransmissions of Bounded Documents",
        "authors": [
            "Predrag R. Jelenković",
            "Evangelia D. Skiani"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Retransmission-based failure recovery represents a primary approach in\nexisting communication networks that guarantees data delivery in the presence\nof channel failures. Recent work has shown that, when data sizes have infinite\nsupport, retransmissions can cause long (-tailed) delays even if all traffic\nand network characteristics are light-tailed. In this paper we investigate the\npractically important case of bounded data units 0 <= L_b <= b under the\ncondition that the hazard functions of the distributions of data sizes and\nchannel statistics are proportional. To this end, we provide an explicit and\nuniform characterization of the entire body of the retransmission distribution\nPr[N_b > n] in both n and b. Our main discovery is that this distribution can\nbe represented as the product of a power law and Gamma distribution. This\nrigorous approximation clearly demonstrates the coupling of a power law\ndistribution, dominating the main body, and the Gamma distribution, determining\nthe exponential tail. Our results are validated via simulation experiments and\ncan be useful for designing retransmission-based systems with the required\nperformance characteristics. From a broader perspective, this study applies to\nany other system, e.g., computing, where restart mechanisms are employed after\na job processing failure.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.8421v2"
    },
    {
        "title": "Multiple Antenna Cyclostationary Spectrum Sensing Based on the Cyclic\n  Correlation Significance Test",
        "authors": [
            "Paulo Urriza",
            "Eric Rebeiz",
            "Danijela Cabric"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  In this paper, we propose and analyze a spectrum sensing method based on\ncyclostationarity specifically targeted for receivers with multiple antennas.\nThis detection method is used for determining the presence or absence of\nprimary users in cognitive radio networks based on the eigenvalues of the\ncyclic covariance matrix of received signals. In particular, the cyclic\ncorrelation significance test is used to detect a specific signal-of-interest\nby exploiting knowledge of its cyclic frequencies. Analytical expressions for\nthe probability of detection and probability of false-alarm under both\nspatially uncorrelated or spatially correlated noise are derived and verified\nby simulation. The detection performance in a Rayleigh flat-fading environment\nis found and verified through simulations. One of the advantages of the\nproposed method is that the detection threshold is shown to be independent of\nboth the number of samples and the noise covariance, effectively eliminating\nthe dependence on accurate noise estimation. The proposed method is also shown\nto provide higher detection probability and better robustness to noise\nuncertainty than existing multiple-antenna cyclostationary-based spectrum\nsensing algorithms under both AWGN as well as a quasi-static Rayleigh fading\nchannel.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.0313v1"
    },
    {
        "title": "Stochastic Superoptimization",
        "authors": [
            "Eric Schkufza",
            "Rahul Sharma",
            "Alex Aiken"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  We formulate the loop-free, binary superoptimization task as a stochastic\nsearch problem. The competing constraints of transformation correctness and\nperformance improvement are encoded as terms in a cost function, and a Markov\nChain Monte Carlo sampler is used to rapidly explore the space of all possible\nprograms to find one that is an optimization of a given target program.\nAlthough our method sacrifices com- pleteness, the scope of programs we are\nable to reason about, and the quality of the programs we produce, far exceed\nthose of existing superoptimizers. Beginning from binaries com- piled by llvm\n-O0 for 64-bit X86, our prototype implemen- tation, STOKE, is able to produce\nprograms which either match or outperform the code sequences produced by gcc\nwith full optimizations enabled, and, in some cases, expert handwritten\nassembly.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.0557v1"
    },
    {
        "title": "Data-parallel programming with Intel Array Building Blocks (ArBB)",
        "authors": [
            "Volker Weinberg"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Intel Array Building Blocks is a high-level data-parallel programming\nenvironment designed to produce scalable and portable results on existing and\nupcoming multi- and many-core platforms.\n  We have chosen several mathematical kernels - a dense matrix-matrix\nmultiplication, a sparse matrix-vector multiplication, a 1-D complex FFT and a\nconjugate gradients solver - as synthetic benchmarks and representatives of\nscientific codes and ported them to ArBB. This whitepaper describes the ArBB\nports and presents performance and scaling measurements on the Westmere-EX\nbased system SuperMIG at LRZ in comparison with OpenMP and MKL.\n",
        "pdf_link": "http://arxiv.org/pdf/1211.1581v1"
    },
    {
        "title": "Operational semantics for product-form solution",
        "authors": [
            "Maria Grazia Vigliotti"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  In this paper we present product-form solutions from the point of view of\nstochastic process algebra. In previous work we have shown how to derive\nproduct-form solutions for a formalism called Labelled Markov Automata (LMA).\nLMA are very useful as their relation with the Continuous Time Markov Chains is\nvery direct. The disadvantage of using LMA is that the proofs of properties are\ncumbersome. In fact, in LMA it is not possible to use the inductive structure\nof the language in a proof. In this paper we consider a simple stochastic\nprocess algebra that has the great advantage of simplifying the proofs. This\nsimple language has been inspired by PEPA, however, detailed analysis of the\nsemantics of cooperation will show the differences between the two formalisms.\nIt will also be shown that the semantics of the cooperation in process algebra\ninfluences the correctness of the derivation of the product-form solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1212.4846v1"
    },
    {
        "title": "Performance Evaluation of Sparse Matrix Multiplication Kernels on Intel\n  Xeon Phi",
        "authors": [
            "Erik Saule",
            "Kamer Kaya",
            "Umit V. Catalyurek"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Intel Xeon Phi is a recently released high-performance coprocessor which\nfeatures 61 cores each supporting 4 hardware threads with 512-bit wide SIMD\nregisters achieving a peak theoretical performance of 1Tflop/s in double\nprecision. Many scientific applications involve operations on large sparse\nmatrices such as linear solvers, eigensolver, and graph mining algorithms. The\ncore of most of these applications involves the multiplication of a large,\nsparse matrix with a dense vector (SpMV). In this paper, we investigate the\nperformance of the Xeon Phi coprocessor for SpMV. We first provide a\ncomprehensive introduction to this new architecture and analyze its peak\nperformance with a number of micro benchmarks. Although the design of a Xeon\nPhi core is not much different than those of the cores in modern processors,\nits large number of cores and hyperthreading capability allow many application\nto saturate the available memory bandwidth, which is not the case for many\ncutting-edge processors. Yet, our performance studies show that it is the\nmemory latency not the bandwidth which creates a bottleneck for SpMV on this\narchitecture. Finally, our experiments show that Xeon Phi's sparse kernel\nperformance is very promising and even better than that of cutting-edge general\npurpose processors and GPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.1078v1"
    },
    {
        "title": "Software model refactoring based on performance analysis: better working\n  on software or performance side?",
        "authors": [
            "Davide Arcelli",
            "Vittorio Cortellessa"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Several approaches have been introduced in the last few years to tackle the\nproblem of interpreting model-based performance analysis results and\ntranslating them into architectural feedback. Typically the interpretation can\ntake place by browsing either the software model or the performance model. In\nthis paper, we compare two approaches that we have recently introduced for this\ngoal: one based on the detection and solution of performance antipatterns, and\nanother one based on bidirectional model transformations between software and\nperformance models. We apply both approaches to the same example in order to\nillustrate the differences in the obtained performance results. Thereafter, we\nraise the level of abstraction and we discuss the pros and cons of working on\nthe software side and on the performance side.\n",
        "pdf_link": "http://arxiv.org/pdf/1302.5171v1"
    },
    {
        "title": "Queuing Theoretic Analysis of Power-performance Tradeoff in\n  Power-efficient Computing",
        "authors": [
            "Yanpei Liu",
            "Stark C. Draper",
            "Nam Sung Kim"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  In this paper we study the power-performance relationship of power-efficient\ncomputing from a queuing theoretic perspective. We investigate the interplay of\nseveral system operations including processing speed, system on/off decisions,\nand server farm size. We identify that there are oftentimes \"sweet spots\" in\npower-efficient operations: there exist optimal combinations of processing\nspeed and system settings that maximize power efficiency. For the single server\ncase, a widely deployed threshold mechanism is studied. We show that there\nexist optimal processing speed and threshold value pairs that minimize the\npower consumption. This holds for the threshold mechanism with job batching.\nFor the multi-server case, it is shown that there exist best processing speed\nand server farm size combinations.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.1561v1"
    },
    {
        "title": "Optimization of FASTEST-3D for Modern Multicore Systems",
        "authors": [
            "Christoph Scheit",
            "Georg Hager",
            "Jan Treibig",
            "Stefan Becker",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  FASTEST-3D is an MPI-parallel finite-volume flow solver based on\nblock-structured meshes that has been developed at the University of\nErlangen-Nuremberg since the early 1990s. It can be used to solve the laminar\nor turbulent incompressible Navier-Stokes equations. Up to now its scalability\nwas strongly limited by a rather rigid communication infrastructure, which led\nto a dominance of MPI time already at small process counts.\n  This paper describes several optimizations to increase the performance,\nscalability, and flexibility of FASTEST-3D. First, a node-level performance\nanalysis is carried out in order to pinpoint the main bottlenecks and identify\nsweet spots for energy-efficient execution. In addition, a single-precision\nversion of the solver for the linear equation system arising from the\ndiscretization of the governing equations is devised, which significantly\nincreases the single-core performance. Then the communication mechanisms in\nFASTEST-3D are analyzed and a new communication strategy based on non-blocking\ncalls is implemented. Performance results with the revised version show\nsignificantly increased single-node performance and considerably improved\ncommunication patterns along with much better parallel scalability. In this\ncontext we discuss the concept of \"acceptable parallel efficiency\" and how it\ninfluences the real gain of the optimizations. Scaling measurements are carried\nout on a modern petascale system. The obtained improvements are of major\nimportance for the use of FASTEST-3D on current high-performance computer\nclusters and will help to perform simulations with much higher spatial and\ntemporal resolution to tackle turbulent flow in technical applications.\n",
        "pdf_link": "http://arxiv.org/pdf/1303.4538v1"
    },
    {
        "title": "Stochastic Analysis on RAID Reliability for Solid-State Drives",
        "authors": [
            "Yongkun Li",
            "Patrick P. C. Lee",
            "John C. S. Lui"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Solid-state drives (SSDs) have been widely deployed in desktops and data\ncenters. However, SSDs suffer from bit errors, and the bit error rate is time\ndependent since it increases as an SSD wears down. Traditional storage systems\nmainly use parity-based RAID to provide reliability guarantees by striping\nredundancy across multiple devices, but the effectiveness of RAID in SSDs\nremains debatable as parity updates aggravate the wearing and bit error rates\nof SSDs. In particular, an open problem is that how different parity\ndistributions over multiple devices, such as the even distribution suggested by\nconventional wisdom, or uneven distributions proposed in recent RAID schemes\nfor SSDs, may influence the reliability of an SSD RAID array. To address this\nfundamental problem, we propose the first analytical model to quantify the\nreliability dynamics of an SSD RAID array. Specifically, we develop a\n\"non-homogeneous\" continuous time Markov chain model, and derive the transient\nreliability solution. We validate our model via trace-driven simulations and\nconduct numerical analysis to provide insights into the reliability dynamics of\nSSD RAID arrays under different parity distributions and subject to different\nbit error rates and array configurations. Designers can use our model to decide\nthe appropriate parity distribution based on their reliability requirements.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.1863v1"
    },
    {
        "title": "Throughput Optimal Scheduling Policies in Networks of Interacting Queues",
        "authors": [
            "Emilio Leonardi"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  This report considers a fairly general model of constrained queuing networks\nthat allows us to represent both MMBP (Markov Modulated Bernoulli Processes)\narrivals and time-varying service constraints. We derive a set of sufficient\nconditions for throughput optimality of scheduling policies that encompass and\ngeneralize all the previously obtained results in the field. This leads to the\ndefinition of new classes of (non diagonal) throughput optimal scheduling\npolicies. We prove the stability of queues by extending the traditional\nLyapunov drift criteria methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.2554v3"
    },
    {
        "title": "Multithreaded Input-Sensitive Profiling",
        "authors": [
            "Emilio Coppa",
            "Camil Demetrescu",
            "Irene Finocchi",
            "Romolo Marotta"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Input-sensitive profiling is a recent performance analysis technique that\nmakes it possible to estimate the empirical cost function of individual\nroutines of a program, helping developers understand how performance scales to\nlarger inputs and pinpoint asymptotic bottlenecks in the code. A current\nlimitation of input-sensitive profilers is that they specifically target\nsequential computations, ignoring any communication between threads. In this\npaper we show how to overcome this limitation, extending the range of\napplicability of the original approach to multithreaded applications and to\napplications that operate on I/O streams. We develop new metrics for\nautomatically estimating the size of the input given to each routine\nactivation, addressing input produced by non-deterministic memory stores\nperformed by other threads as well as by the OS kernel (e.g., in response to\nI/O or network operations). We provide real case studies, showing that our\nextension allows it to characterize the behavior of complex applications more\nprecisely than previous approaches. An extensive experimental investigation on\na variety of benchmark suites (including the SPEC OMP2012 and the PARSEC\nbenchmarks) shows that our Valgrind-based input-sensitive profiler incurs an\noverhead comparable to other prominent heavyweight analysis tools, while\ncollecting significantly more performance points from each profiling session\nand correctly characterizing both thread-induced and external input.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.3804v1"
    },
    {
        "title": "Chip-level and multi-node analysis of energy-optimized lattice-Boltzmann\n  CFD simulations",
        "authors": [
            "Markus Wittmann",
            "Georg Hager",
            "Thomas Zeiser",
            "Jan Treibig",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Memory-bound algorithms show complex performance and energy consumption\nbehavior on multicore processors. We choose the lattice-Boltzmann method (LBM)\non an Intel Sandy Bridge cluster as a prototype scenario to investigate if and\nhow single-chip performance and power characteristics can be generalized to the\nhighly parallel case. First we perform an analysis of a sparse-lattice LBM\nimplementation for complex geometries. Using a single-core performance model,\nwe predict the intra-chip saturation characteristics and the optimal operating\npoint in terms of energy to solution as a function of implementation details,\nclock frequency, vectorization, and number of active cores per chip. We show\nthat high single-core performance and a correct choice of the number of active\ncores per chip are the essential optimizations for lowest energy to solution at\nminimal performance degradation. Then we extrapolate to the MPI-parallel level\nand quantify the energy-saving potential of various optimizations and execution\nmodes, where we find these guidelines to be even more important, especially\nwhen communication overhead is non-negligible. In our setup we could achieve\nenergy savings of 35% in this case, compared to a naive approach. We also\ndemonstrate that a simple non-reflective reduction of the clock speed leaves\nmost of the energy saving potential unused.\n",
        "pdf_link": "http://arxiv.org/pdf/1304.7664v3"
    },
    {
        "title": "First experiences with the Intel MIC architecture at LRZ",
        "authors": [
            "Volker Weinberg",
            "Momme Allalen"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  With the rapidly growing demand for computing power new accelerator based\narchitectures have entered the world of high performance computing since around\n5 years. In particular GPGPUs have recently become very popular, however\nprogramming GPGPUs using programming languages like CUDA or OpenCL is\ncumbersome and error-prone. Trying to overcome these difficulties, Intel\ndeveloped their own Many Integrated Core (MIC) architecture which can be\nprogrammed using standard parallel programming techniques like OpenMP and MPI.\nIn the beginning of 2013, the first production-level cards named Intel Xeon Phi\ncame on the market. LRZ has been considered by Intel as a leading research\ncentre for evaluating coprocessors based on the MIC architecture since 2010\nunder strict NDA. Since the Intel Xeon Phi is now generally available, we can\nshare our experience on programming Intel's new MIC architecture.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.3123v1"
    },
    {
        "title": "Dominant block guided optimal cache size estimation to maximize IPC of\n  embedded software",
        "authors": [
            "Rajendra Patel",
            "Arvind Rajawat"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  Embedded system software is highly constrained from performance, memory\nfootprint, energy consumption and implementing cost view point. It is always\ndesirable to obtain better Instructions per Cycle. Instruction cache has major\ncontribution in improving IPC. Cache memories are realized on the same chip\nwhere the processor is running. This considerably increases the system cost as\nwell. Hence, it is required to maintain a trade off between cache sizes and\nperformance improvement offered. Determining the number of cache lines and size\nof cache line are important parameters for cache designing. The design space\nfor cache is quite large. It is time taking to execute the given application\nwith different cache sizes on an instruction set simulator to figure out the\noptimal cache size. In this paper, a technique is proposed to identify a number\nof cache lines and cache line size for the L1 instruction cache that will offer\nbest or nearly best IPC. Cache size is derived, at a higher abstraction level,\nfrom basic block analysis in the Low Level Virtual Machine environment. The\ncache size estimated is cross validated by simulating the set of benchmark\napplications with different cache sizes in simple scalar simulator. The\nproposed method seems to be superior in terms of estimation accuracy and\nestimation time as compared to the existing methods for estimation of optimal\ncache size parameters like cache line size, number of cache lines.\n",
        "pdf_link": "http://arxiv.org/pdf/1312.2306v1"
    },
    {
        "title": "Characterizing Workload of Web Applications on Virtualized Servers",
        "authors": [
            "Xiajun Wang",
            "Song Huang",
            "Song Fu",
            "Krishna Kavi"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  With the ever increasing demands of cloud computing services, planning and\nmanagement of cloud resources has become a more and more important issue which\ndirected affects the resource utilization and SLA and customer satisfaction.\nBut before any management strategy is made, a good understanding of\napplications' workload in virtualized environment is the basic fact and\nprinciple to the resource management methods. Unfortunately, little work has\nbeen focused on this area. Lack of raw data could be one reason; another reason\nis that people still use the traditional models or methods shared under\nnon-virtualized environment. The study of applications' workload in virtualized\nenvironment should take on some of its peculiar features comparing to the\nnon-virtualized environment. In this paper, we are open to analyze the workload\ndemands that reflect applications' behavior and the impact of virtualization.\nThe results are obtained from an experimental cloud testbed running web\napplications, specifically the RUBiS benchmark application. We profile the\nworkload dynamics on both virtualized and non-virtualized environments and\ncompare the findings. The experimental results are valuable for us to estimate\nthe performance of applications on computer architectures, to predict SLA\ncompliance or violation based on the projected application workload and to\nguide the decision making to support applications with the right hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.3549v1"
    },
    {
        "title": "On Big Data Benchmarking",
        "authors": [
            "Rui Han",
            "Xiaoyi Lu"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Big data systems address the challenges of capturing, storing, managing,\nanalyzing, and visualizing big data. Within this context, developing benchmarks\nto evaluate and compare big data systems has become an active topic for both\nresearch and industry communities. To date, most of the state-of-the-art big\ndata benchmarks are designed for specific types of systems. Based on our\nexperience, however, we argue that considering the complexity, diversity, and\nrapid evolution of big data systems, for the sake of fairness, big data\nbenchmarks must include diversity of data and workloads. Given this motivation,\nin this paper, we first propose the key requirements and challenges in\ndeveloping big data benchmarks from the perspectives of generating data with 4V\nproperties (i.e. volume, velocity, variety and veracity) of big data, as well\nas generating tests with comprehensive workloads for big data systems. We then\npresent the methodology on big data benchmarking designed to address these\nchallenges. Next, the state-of-the-art are summarized and compared, following\nby our vision for future research directions.\n",
        "pdf_link": "http://arxiv.org/pdf/1402.5194v1"
    },
    {
        "title": "Performance Benefits of DataMPI: A Case Study with BigDataBench",
        "authors": [
            "Fan Liang",
            "Chen Feng",
            "Xiaoyi Lu",
            "Zhiwei Xu"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Apache Hadoop and Spark are gaining prominence in Big Data processing and\nanalytics. Both of them are widely deployed on Internet companies. On the other\nhand, high-performance data analysis requirements are causing academical and\nindustrial communities to adopt state-of-the-art technologies in HPC to solve\nBig Data problems. Recently, we have proposed a key-value pair based\ncommunication library, DataMPI, which is extending MPI to support\nHadoop/Spark-like Big Data Computing jobs. In this paper, we use BigDataBench,\na Big Data benchmark suite, to do comprehensive studies on performance and\nresource utilization characterizations of Hadoop, Spark and DataMPI. From our\nexperiments, we observe that the job execution time of DataMPI has up to 55%\nand 39% speedups compared with those of Hadoop and Spark, respectively. Most of\nthe benefits come from the high-efficiency communication mechanisms in DataMPI.\nWe also notice that the resource (CPU, memory, disk and network I/O)\nutilizations of DataMPI are also more efficient than those of the other two\nframeworks.\n",
        "pdf_link": "http://arxiv.org/pdf/1403.3480v1"
    },
    {
        "title": "High Level Programming for Heterogeneous Architectures",
        "authors": [
            "Oren Segal",
            "Martin Margala",
            "Sai Rahul Chalamalasetti",
            "Mitch Wright"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  This work presents an effort to bridge the gap between abstract high level\nprogramming and OpenCL by extending an existing high level Java programming\nframework (APARAPI), based on OpenCL, so that it can be used to program FPGAs\nat a high level of abstraction and increased ease of programmability. We run\nseveral real world algorithms to assess the performance of the framework on\nboth a low end and a high end system. On the low end and high end systems\nrespectively we observed up to 78-80 percent power reduction and 4.8X-5.3X\nspeed increase running NBody simulation, as well as up to 65-80 percent power\nreduction and 6.2X-7X speed increase for a KMeans, MapReduce algorithm running\non top of the Hadoop framework and APARAPI.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.4964v1"
    },
    {
        "title": "Performance Analysis of Linear-Equality-Constrained Least-Squares\n  Estimation",
        "authors": [
            "Reza Arablouei",
            "Kutluyıl Doğançay"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  We analyze the performance of a linear-equality-constrained least-squares\n(CLS) algorithm and its relaxed version, called rCLS, that is obtained via the\nmethod of weighting. The rCLS algorithm solves an unconstrained least-squares\nproblem that is augmented by incorporating a weighted form of the linear\nconstraints. As a result, unlike the CLS algorithm, the rCLS algorithm is\namenable to our approach to performance analysis presented here, which is akin\nto the energy-conservation-based methodology. Therefore, we initially inspect\nthe convergence properties and evaluate the precision of estimation as well as\nsatisfaction of the constraints for the rCLS algorithm in both mean and\nmean-square senses. Afterwards, we examine the performance of the CLS algorithm\nby evaluating the limiting performance of the rCLS algorithm as the relaxation\nparameter (weight) approaches infinity. Numerical examples verify the accuracy\nof the theoretical findings.\n",
        "pdf_link": "http://arxiv.org/pdf/1408.6721v2"
    },
    {
        "title": "Investigation of the relationship between code change set n-grams and\n  change in energy consumption",
        "authors": [
            "Stephen Romansky"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  The amount of software running on mobile devices is constantly growing as\nconsumers and industry purchase more battery powered devices. On the other\nhand, tools that provide developers with feed- back on how their software\nchanges affect battery life are not widely available. This work employs Green\nMining, the study of the rela- tionship between energy consumption and software\nchangesets, and n-gram language models to evaluate if source code changeset\nperplex- ity correlates with change in energy consumption. A correlation be-\ntween perplexity and change in energy consumption would permit the development\nof a tool that predicts the impact a code changeset may have on a software\napplications energy consumption. The case study results show that there is weak\nto no correlation between cross en- tropy and change in energy consumption.\nTherefore, future areas of investigation are proposed.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.2047v1"
    },
    {
        "title": "A Flexible Framework for Accurate Simulation of Cloud In-Memory Data\n  Stores",
        "authors": [
            "Pierangelo Di Sanzo",
            "Francesco Quaglia",
            "Bruno Ciciani",
            "Alessandro Pellegrini",
            "Diego Didona",
            "Paolo Romano",
            "Roberto Palmieri",
            "Sebastiano Peluso"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  In-memory (transactional) data stores are recognized as a first-class data\nmanagement technology for cloud platforms, thanks to their ability to match the\nelasticity requirements imposed by the pay-as-you-go cost model. On the other\nhand, defining the well-suited amount of cache servers to be deployed, and the\ndegree of in-memory replication of slices of data, in order to optimize\nreliability/availability and performance tradeoffs, is far from being a trivial\ntask. Yet, it is an essential aspect of the provisioning process of cloud\nplatforms, given that it has an impact on how well cloud resources are actually\nexploited. To cope with the issue of determining optimized configurations of\ncloud in-memory data stores, in this article we present a flexible simulation\nframework offering skeleton simulation models that can be easily specialized in\norder to capture the dynamics of diverse data grid systems, such as those\nrelated to the specific protocol used to provide data consistency and/or\ntransactional guarantees. Besides its flexibility, another peculiar aspect of\nthe framework lies in that it integrates simulation and machine-learning\n(black-box) techniques, the latter being essentially used to capture the\ndynamics of the data-exchange layer (e.g. the message passing layer) across the\ncache servers. This is a relevant aspect when considering that the actual\ndata-transport/networking infrastructure on top of which the data grid is\ndeployed might be unknown, hence being not feasible to be modeled via white-box\n(namely purely simulative) approaches. We also provide an extended experimental\nstudy aimed at validating instances of simulation models supported by our\nframework against execution dynamics of real data grid systems deployed on top\nof either private or public cloud infrastructures.\n",
        "pdf_link": "http://arxiv.org/pdf/1411.7910v1"
    },
    {
        "title": "An asymptotically optimal policy and state-space collapse for the\n  multi-class shared queue",
        "authors": [
            "Mark Shifrin"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  We consider a multi-class G/G/1 queue with a finite shared buffer. There is\ntask admission and server scheduling control which aims to minimize the cost\nwhich consists of holding and rejection components. We construct a policy that\nis asymptotically optimal in the heavy traffic limit. The policy stems from\nsolution to Harrison-Taksar (HT) free boundary problem and is expressed by a\nsingle free boundary point. We show that the HT problem solution translated\ninto the queuelength processes follows a specific {\\it triangular} form. This\nform implies the queuelength control policy which is different from the known\n$c\\mu$ priority rule and has a novel structure.\n  We exemplify that the probabilistic methods we exploit can be successfully\napplied to solving scheduling and admission problems in cloud computing.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.02603v2"
    },
    {
        "title": "Reward Processes and Performance Simulation in Supermarket Models with\n  Different Servers",
        "authors": [
            "Quan-Lin Li",
            "Feifei Yang",
            "Na Li"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Supermarket models with different servers become a key in modeling resource\nmanagement of stochastic networks, such as, computer networks, manufacturing\nsystems and transportation networks. While these different servers always make\nanalysis of such a supermarket model more interesting, difficult and\nchallenging. This paper provides a new novel method for analyzing the\nsupermarket model with different servers through a multi-dimensional\ncontinuous-time Markov reward processes. Firstly, the utility functions are\nconstructed for expressing a routine selection mechanism that depends on queue\nlengths, on service rates, and on some probabilities of individual preference.\nThen applying the continuous-time Markov reward processes, some segmented\nstochastic integrals of the random reward function are established by means of\nan event-driven technique. Based on this, the mean of the random reward\nfunction in a finite time period is effectively computed by means of the state\njump points of the Markov reward process, and also the mean of the discounted\nrandom reward function in an infinite time period can be calculated through the\nsame event-driven technique. Finally, some simulation experiments are given to\nindicate how the expected queue length of each server depends on the main\nparameters of this supermarket model.\n",
        "pdf_link": "http://arxiv.org/pdf/1504.08150v2"
    },
    {
        "title": "Performance analysis of the Kahan-enhanced scalar product on current\n  multicore processors",
        "authors": [
            "Johannes Hofmann",
            "Dietmar Fey",
            "Jan Eitzinger",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  We investigate the performance characteristics of a numerically enhanced\nscalar product (dot) kernel loop that uses the Kahan algorithm to compensate\nfor numerical errors, and describe efficient SIMD-vectorized implementations on\nrecent Intel processors. Using low-level instruction analysis and the\nexecution-cache-memory (ECM) performance model we pinpoint the relevant\nperformance bottlenecks for single-core and thread-parallel execution, and\npredict performance and saturation behavior. We show that the Kahan-enhanced\nscalar product comes at almost no additional cost compared to the naive\n(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD\nvectorization and unrolling, are applied. We also investigate the impact of\narchitectural changes across four generations of Intel Xeon processors.\n",
        "pdf_link": "http://arxiv.org/pdf/1505.02586v1"
    },
    {
        "title": "Analysis of the Energy-Performance Tradeoff for Delayed Mobile\n  Offloading",
        "authors": [
            "Huaming Wu",
            "Katinka Wolter"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  This paper has been withdrawn by the author\n",
        "pdf_link": "http://arxiv.org/pdf/1510.09185v2"
    },
    {
        "title": "Performance analysis of the Kahan-enhanced scalar product on current\n  multi- and manycore processors",
        "authors": [
            "Johannes Hofmann",
            "Dietmar Fey",
            "Michael Riedmann",
            "Jan Eitzinger",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We investigate the performance characteristics of a numerically enhanced\nscalar product (dot) kernel loop that uses the Kahan algorithm to compensate\nfor numerical errors, and describe efficient SIMD-vectorized implementations on\nrecent multi- and manycore processors. Using low-level instruction analysis and\nthe execution-cache-memory (ECM) performance model we pinpoint the relevant\nperformance bottlenecks for single-core and thread-parallel execution, and\npredict performance and saturation behavior. We show that the Kahan-enhanced\nscalar product comes at almost no additional cost compared to the naive\n(non-Kahan) scalar product if appropriate low-level optimizations, notably SIMD\nvectorization and unrolling, are applied. The ECM model is extended\nappropriately to accommodate not only modern Intel multicore chips but also the\nIntel Xeon Phi \"Knights Corner\" coprocessor and an IBM POWER8 CPU. This allows\nus to discuss the impact of processor features on the performance across four\nmodern architectures that are relevant for high performance computing.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.01890v1"
    },
    {
        "title": "On the Duration and Intensity of Competitions in Nonlinear Pólya Urn\n  Processes with Fitness",
        "authors": [
            "Bo Jiang",
            "Daniel R. Figueiredo",
            "Bruno Ribeiro",
            "Don Towsley"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Cumulative advantage (CA) refers to the notion that accumulated resources\nfoster the accumulation of further resources in competitions, a phenomenon that\nhas been empirically observed in various contexts. The oldest and arguably\nsimplest mathematical model that embodies this general principle is the P\\'olya\nurn process, which finds applications in a myriad of problems. The original\nmodel captures the dynamics of competitions between two equally fit agents\nunder linear CA effects, which can be readily generalized to incorporate\ndifferent fitnesses and nonlinear CA effects. We study two statistics of\ncompetitions under the generalized model, namely duration (i.e., time of the\nlast tie) and intensity (i.e., number of ties). We give rigorous mathematical\ncharacterizations of the tail distributions of both duration and intensity\nunder the various regimes for fitness and nonlinearity, which reveal very\ninteresting behaviors. For example, fitness superiority induces much shorter\ncompetitions in the sublinear regime while much longer competitions in the\nsuperlinear regime. Our findings can shed light on the application of P\\'olya\nurn processes in more general contexts where fitness and nonlinearity may be\npresent.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.02097v3"
    },
    {
        "title": "A Hybrid Performance Analysis Technique for Distributed Real-Time\n  Embedded Systems",
        "authors": [
            "Junchul Choi",
            "Hyunok Oh",
            "Soonhoi Ha"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  It remains a challenging problem to tightly estimate the worst case response\ntime of an application in a distributed embedded system, especially when there\nare dependencies between tasks. We discovered that the state-of-the art\ntechniques considering task dependencies either fail to obtain a conservative\nbound or produce a loose upper bound. We propose a novel conservative\nperformance analysis, called hybrid performance analysis, combining the\nresponse time analysis technique and the scheduling time bound analysis\ntechnique to compute a tighter bound fast. Through extensive experiments with\nrandomly generated graphs, superior performance of our proposed approach\ncompared with previous methods is confirmed.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04951v1"
    },
    {
        "title": "A Unified, Hardware-Fitted, Cross-GPU Performance Model",
        "authors": [
            "James Stevens",
            "Andreas Klöckner"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We present a mechanism to symbolically gather performance-relevant operation\ncounts from numerically-oriented subprograms (`kernels') expressed in the Loopy\nprogramming system, and apply these counts in a simple, linear model of kernel\nrun time. We use a series of `performance-instructive' kernels to fit the\nparameters of a unified model to the performance characteristics of GPU\nhardware from multiple hardware generations and vendors. We evaluate the\npredictive power of the model on a broad array of computational kernels\nrelevant to scientific computing. In terms of the geometric mean, our simple,\nvendor- and GPU-type-independent model achieves relative accuracy comparable to\nthat of previously published work using hardware specific models.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04997v1"
    },
    {
        "title": "An Analytical Solution for Probabilistic Guarantees of Reservation Based\n  Soft Real-Time Systems",
        "authors": [
            "Luigi Palopoli",
            "Daniele Fontanelli",
            "Luca Abeni",
            "Bernardo Villalba Frías"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We show a methodology for the computation of the probability of deadline miss\nfor a periodic real-time task scheduled by a resource reservation algorithm. We\npropose a modelling technique for the system that reduces the computation of\nsuch a probability to that of the steady state probability of an infinite state\nDiscrete Time Markov Chain with a periodic structure. This structure is\nexploited to develop an efficient numeric solution where different\naccuracy/computation time trade-offs can be obtained by operating on the\ngranularity of the model. More importantly we offer a closed form conservative\nbound for the probability of a deadline miss. Our experiments reveal that the\nbound remains reasonably close to the experimental probability in one real-time\napplication of practical interest. When this bound is used for the optimisation\nof the overall Quality of Service for a set of tasks sharing the CPU, it\nproduces a good sub-optimal solution in a small amount of time.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.08004v1"
    },
    {
        "title": "DINAMITE: A modern approach to memory performance profiling",
        "authors": [
            "Svetozar Miucin",
            "Conor Brady",
            "Alexandra Fedorova"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Diagnosing and fixing performance problems on multicore machines with deep\nmemory hierarchies is extremely challenging. Certain problems are best\naddressed when we can analyze the entire trace of program execution, e.g.,\nevery memory access. Unfortunately such detailed execution logs are very large\nand cannot be analyzed by direct inspection. We present DINAMITE: a toolkit for\nDynamic INstrumentation and Analysis for MassIve Trace Exploration. DINAMITE is\na collection of tools for end-to-end performance analysis: from the LLVM\ncompiler pass that instruments the program to plug-and-play tools that use a\nmodern data analytics engine Spark Streaming for trace introspection. Using\nDINAMITE we found opportunities to improve data layout in several applications\nthat resulted in 15-20% performance improvements and found a shared-variable\nbottleneck in a popular key-value store, whose elimination improved performance\nby 20x.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.00396v1"
    },
    {
        "title": "A figure of merit for describing the performance of scaling of\n  parallelization",
        "authors": [
            "János Végh",
            "Péter Molnár",
            "József Vásárhelyi"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  With the spread of multi- and many-core processors more and more typical task\nis to re-implement some source code written originally for a single processor\nto run on more than one cores. Since it is a serious investment, it is\nimportant to decide how much efforts pays off, and whether the resulting\nimplementation has as good performability as it could be. The Amdahl's law\nprovides some theoretical upper limits for the performance gain reachable\nthrough parallelizing the code, but it needs the detailed architectural\nknowledge of the program code, does not consider the housekeeping activity\nneeded for parallelization and cannot tell how the actual stage of\nparallelization implementation performs. The present paper suggests a\nquantitative measure for that goal. This figure of merit is derived\nexperimentally, from measured running time, and number of threads/cores. It can\nbe used to quantify the used parallelization technology, the connection between\nthe computing units, the acceleration technology under the given conditions,\ncommunication method within SoC, or the performance of the software\nteam/compiler.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.02686v3"
    },
    {
        "title": "On Continuous-space Embedding of Discrete-parameter Queueing Systems",
        "authors": [
            "Neha Karanjkar",
            "Madhav P. Desai",
            "Shalabh Bhatnagar"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Motivated by the problem of discrete-parameter simulation optimization (DPSO)\nof queueing systems, we consider the problem of embedding the discrete\nparameter space into a continuous one so that descent-based continuous-space\nmethods could be directly applied for efficient optimization. We show that a\nrandomization of the simulation model itself can be used to achieve such an\nembedding when the objective function is a long-run average measure. Unlike\nspatial interpolation, the computational cost of this embedding is independent\nof the number of parameters in the system, making the approach ideally suited\nto high-dimensional problems. We describe in detail the application of this\ntechnique to discrete-time queues for embedding queue capacities, number of\nservers and server-delay parameters into continuous space and empirically show\nthat the technique can produce smooth interpolations of the objective function.\nThrough an optimization case-study of a queueing network with $10^7$ design\npoints, we demonstrate that existing continuous optimizers can be effectively\napplied over such an embedding to find good solutions.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.02900v3"
    },
    {
        "title": "LITMUS: An Open Extensible Framework for Benchmarking RDF Data\n  Management Solutions",
        "authors": [
            "Harsh Thakkar",
            "Mohnish Dubey",
            "Gezim Sejdiu",
            "Axel-Cyrille Ngonga Ngomo",
            "Jeremy Debattista",
            "Christoph Lange",
            "Jens Lehmann",
            "Sören Auer",
            "Maria-Esther Vidal"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Developments in the context of Open, Big, and Linked Data have led to an\nenormous growth of structured data on the Web. To keep up with the pace of\nefficient consumption and management of the data at this rate, many data\nManagement solutions have been developed for specific tasks and applications.\nWe present LITMUS, a framework for benchmarking data management solutions.\nLITMUS goes beyond classical storage benchmarking frameworks by allowing for\nanalysing the performance of frameworks across query languages. In this\nposition paper we present the conceptual architecture of LITMUS as well as the\nconsiderations that led to this architecture.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.02800v1"
    },
    {
        "title": "Automating Large-Scale Simulation and Data Analysis with OMNeT++:\n  Lession Learned and Future Perspectives",
        "authors": [
            "Antonio Virdis",
            "Carlo Vallati",
            "Giovanni Nardini"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Simulation is widely adopted in the study of modern computer networks. In\nthis context, OMNeT++ provides a set of very effective tools that span from the\ndefinition of the network, to the automation of simulation execution and quick\nresult representation. However, as network models become more and more complex\nto cope with the evolution of network systems, the amount of simulation\nfactors, the number of simulated nodes and the size of results grow\nconsequently, leading to simulations with larger scale. In this work, we\nperform a critical analysis of the tools provided by OMNeT++ in case of such\nlarge-scale simulations. We then propose a unified and flexible software\narchitecture to support simulation automation.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.04603v1"
    },
    {
        "title": "Infinite Server Queueing Networks with Deadline Based Routing",
        "authors": [
            "Neal Master",
            "Nicholas Bambos"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Motivated by timeouts in Internet services, we consider networks of infinite\nserver queues in which routing decisions are based on deadlines. Specifically,\nat each node in the network, the total service time equals the minimum of\nseveral independent service times (e.g. the minimum of the amount of time\nrequired to complete a transaction and a deadline). Furthermore, routing\ndecisions depend on which of the independent service times achieves the minimum\n(e.g. exceeding a deadline will require the customer to be routed so they can\nre-attempt the transaction). Because current routing decisions are dependent on\npast service times, much of the existing theory on product-form queueing\nnetworks does not apply. In spite of this, we are able to show that such\nnetworks have product-form equilibrium distributions. We verify our analytic\ncharacterization with a simulation of a simple network. We also discuss\nextensions of this work to more general settings.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.05750v1"
    },
    {
        "title": "DynIMS: A Dynamic Memory Controller for In-memory Storage on HPC Systems",
        "authors": [
            "Pengfei Xuan",
            "Feng Luo",
            "Rong Ge",
            "Pradip K Srimani"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  In order to boost the performance of data-intensive computing on HPC systems,\nin-memory computing frameworks, such as Apache Spark and Flink, use local DRAM\nfor data storage. Optimizing the memory allocation to data storage is critical\nto delivering performance to traditional HPC compute jobs and throughput to\ndata-intensive applications sharing the HPC resources. Current practices that\nstatically configure in-memory storage may leave inadequate space for compute\njobs or lose the opportunity to utilize more available space for data-intensive\napplications. In this paper, we explore techniques to dynamically adjust\nin-memory storage and make the right amount of space for compute jobs. We have\ndeveloped a dynamic memory controller, DynIMS, which infers memory demands of\ncompute tasks online and employs a feedback-based control model to adapt the\ncapacity of in-memory storage. We test DynIMS using mixed HPCC and Spark\nworkloads on a HPC cluster. Experimental results show that DynIMS can achieve\nup to 5X performance improvement compared to systems with static memory\nallocations.\n",
        "pdf_link": "http://arxiv.org/pdf/1609.09294v1"
    },
    {
        "title": "Non-Asymptotic Delay Bounds for Multi-Server Systems with\n  Synchronization Constraints",
        "authors": [
            "Markus Fidler",
            "Brenton Walker",
            "Yuming Jiang"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Multi-server systems have received increasing attention with important\nimplementations such as Google MapReduce, Hadoop, and Spark. Common to these\nsystems are a fork operation, where jobs are first divided into tasks that are\nprocessed in parallel, and a later join operation, where completed tasks wait\nuntil the results of all tasks of a job can be combined and the job leaves the\nsystem. The synchronization constraint of the join operation makes the analysis\nof fork-join systems challenging and few explicit results are known. In this\nwork, we model fork-join systems using a max-plus server model that enables us\nto derive statistical bounds on waiting and sojourn times for general arrival\nand service time processes. We contribute end-to-end delay bounds for\nmulti-stage fork-join networks that grow in $\\mathcal{O}(h \\ln k)$ for $h$\nfork-join stages, each with $k$ parallel servers. We perform a detailed\ncomparison of different multi-server configurations and highlight their pros\nand cons. We also include an analysis of single-queue fork-join systems that\nare non-idling and achieve a fundamental performance gain, and compare these\nresults to both simulation and a live Spark system.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.06309v1"
    },
    {
        "title": "Evaluating load balancing policies for performance and energy-efficiency",
        "authors": [
            "Freek van den Berg",
            "Björn F. Postema",
            "Boudewijn R. Haverkort"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Nowadays, more and more increasingly hard computations are performed in\nchallenging fields like weather forecasting, oil and gas exploration, and\ncryptanalysis. Many of such computations can be implemented using a computer\ncluster with a large number of servers. Incoming computation requests are then,\nvia a so-called load balancing policy, distributed over the servers to ensure\noptimal performance. Additionally, being able to switch-off some servers during\nlow period of workload, gives potential to reduced energy consumption.\nTherefore, load balancing forms, albeit indirectly, a trade-off between\nperformance and energy consumption. In this paper, we introduce a syntax for\nload-balancing policies to dynamically select a server for each request based\non relevant criteria, including the number of jobs queued in servers, power\nstates of servers, and transition delays between power states of servers. To\nevaluate many policies, we implement two load balancers in: (i) iDSL, a\nlanguage and tool-chain for evaluating service-oriented systems, and (ii) a\nsimulation framework in AnyLogic. Both implementations are successfully\nvalidated by comparison of the results.\n",
        "pdf_link": "http://arxiv.org/pdf/1610.08172v1"
    },
    {
        "title": "On the equivalence between multiclass processor sharing and random order\n  scheduling policies",
        "authors": [
            "Konstantin Avrachenkov",
            "Tejas Bodas"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Consider a single server system serving a multiclass population. Some popular\nscheduling policies for such system are the discriminatory processor sharing\n(DPS), discriminatory random order service (DROS), generalized processor\nsharing (GPS) and weighted fair queueing (WFQ). In this paper, we propose two\nclasses of policies, namely MPS (multiclass processor sharing) and MROS\n(multiclass random order service), that generalize the four policies mentioned\nabove. For the special case when the multiclass population arrive according to\nPoisson processes and have independent and exponential service requirement with\nparameter $\\mu$, we show that the tail of the sojourn time distribution for a\nclass $i$ customer in a system with the MPS policy is a constant multiple of\nthe tail of the waiting time distribution of a class $i$ customer in a system\nwith the MROS policy. This result implies that for a class $i$ customer, the\ntail of the sojourn time distribution in a system with the DPS (GPS) scheduling\npolicy is a constant multiple of the tail of the waiting time distribution in a\nsystem with the DROS (respectively WFQ) policy.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.01722v3"
    },
    {
        "title": "A Comparison of Parallel Graph Processing Implementations",
        "authors": [
            "Samuel Pollard",
            "Boyana Norris"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  The rapidly growing number of large network analysis problems has led to the\nemergence of many parallel and distributed graph processing systems---one\nsurvey in 2014 identified over 80. Since then, the landscape has evolved; some\npackages have become inactive while more are being developed. Determining the\nbest approach for a given problem is infeasible for most developers. To enable\neasy, rigorous, and repeatable comparison of the capabilities of such systems,\nwe present an approach and associated software for analyzing the performance\nand scalability of parallel, open-source graph libraries. We demonstrate our\napproach on five graph processing packages: GraphMat, the Graph500, the Graph\nAlgorithm Platform Benchmark Suite, GraphBIG, and PowerGraph using synthetic\nand real-world datasets. We examine previously overlooked aspects of parallel\ngraph processing performance such as phases of execution and energy usage for\nthree algorithms: breadth first search, single source shortest paths, and\nPageRank and compare our results to Graphalytics.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.02003v2"
    },
    {
        "title": "Testing Docker Performance for HPC Applications",
        "authors": [
            "Alexey Ermakov",
            "Alexey Vasyukov"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  The main goal for this article is to compare performance penalties when using\nKVM virtualization and Docker containers for creating isolated environments for\nHPC applications. The article provides both data obtained using commonly\naccepted synthetic tests (High Performance Linpack) and real life applications\n(OpenFOAM). The article highlights the influence on resulting application\nperformance of major infrastructure configuration options: CPU type presented\nto VM, networking connection type used.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05592v1"
    },
    {
        "title": "A note on integrating products of linear forms over the unit simplex",
        "authors": [
            "Giuliano Casale"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Integrating a product of linear forms over the unit simplex can be done in\npolynomial time if the number of variables n is fixed (V. Baldoni et al.,\n2011). In this note, we highlight that this problem is equivalent to obtaining\nthe normalizing constant of state probabilities for a popular class of Markov\nprocesses used in queueing network theory. In light of this equivalence, we\nsurvey existing computational algorithms developed in queueing theory that can\nbe used for exact integration. For example, under some regularity conditions,\nqueueing theory algorithms can exactly integrate a product of linear forms of\ntotal degree N by solving N systems of linear equations.\n",
        "pdf_link": "http://arxiv.org/pdf/1704.05867v4"
    },
    {
        "title": "Scalable Load Balancing in Networked Systems: Universality Properties\n  and Stochastic Coupling Methods",
        "authors": [
            "Mark van der Boor",
            "Sem C. Borst",
            "Johan S. H. van Leeuwaarden",
            "Debankur Mukherjee"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We present an overview of scalable load balancing algorithms which provide\nfavorable delay performance in large-scale systems, and yet only require\nminimal implementation overhead. Aimed at a broad audience, the paper starts\nwith an introduction to the basic load balancing scenario, consisting of a\nsingle dispatcher where tasks arrive that must immediately be forwarded to one\nof $N$ single-server queues.\n  A popular class of load balancing algorithms are so-called power-of-$d$ or\nJSQ($d$) policies, where an incoming task is assigned to a server with the\nshortest queue among $d$ servers selected uniformly at random. This class\nincludes the Join-the-Shortest-Queue (JSQ) policy as a special case ($d = N$),\nwhich has strong stochastic optimality properties and yields a mean waiting\ntime that vanishes as $N$ grows large for any fixed subcritical load. However,\na nominal implementation of the JSQ policy involves a prohibitive communication\nburden in large-scale deployments. In contrast, a random assignment policy ($d\n= 1$) does not entail any communication overhead, but the mean waiting time\nremains constant as $N$ grows large for any fixed positive load.\n  In order to examine the fundamental trade-off between performance and\nimplementation overhead, we consider an asymptotic regime where $d(N)$ depends\non $N$. We investigate what growth rate of $d(N)$ is required to match the\nperformance of the JSQ policy on fluid and diffusion scale. The results\ndemonstrate that the asymptotics for the JSQ($d(N)$) policy are insensitive to\nthe exact growth rate of $d(N)$, as long as the latter is sufficiently fast,\nimplying that the optimality of the JSQ policy can asymptotically be preserved\nwhile dramatically reducing the communication overhead. We additionally show\nhow the communication overhead can be reduced yet further by the so-called\nJoin-the-Idle-Queue scheme, leveraging memory at the dispatcher.\n",
        "pdf_link": "http://arxiv.org/pdf/1712.08555v1"
    },
    {
        "title": "LCD: Low Latency Command Dissemination for A Platoon of Vehicles",
        "authors": [
            "Kai Li",
            "Wei Ni",
            "Eduardo Tovar",
            "Mohsen Guizani"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In a vehicular platoon, a lead vehicle that is responsible for managing the\nplatoon's moving directions and velocity periodically disseminates control\ncommands to following vehicles based on vehicle-to-vehicle communications.\nHowever, reducing command dissemination latency with multiple vehicles while\nensuring successful message delivery to the tail vehicle is challenging. We\npropose a new linear dynamic programming algorithm using backward induction and\ninterchange arguments to minimize the dissemination latency of the vehicles.\nFurthermore, a closed form of dissemination latency in vehicular platoon is\nobtained by utilizing Markov chain with M/M/1 queuing model. Simulation results\nconfirm that the proposed dynamic programming algorithm improves the\ndissemination rate by at least 50.9%, compared to similar algorithms in the\nliterature. Moreover, it also approximates the best performance with the\nmaximum gap of up to 0.2 second in terms of latency.\n",
        "pdf_link": "http://arxiv.org/pdf/1801.06153v1"
    },
    {
        "title": "Analytical Cost Metrics : Days of Future Past",
        "authors": [
            "Nirmal Prajapati",
            "Sanjay Rajopadhye",
            "Hristo Djidjev"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  As we move towards the exascale era, the new architectures must be capable of\nrunning the massive computational problems efficiently. Scientists and\nresearchers are continuously investing in tuning the performance of\nextreme-scale computational problems. These problems arise in almost all areas\nof computing, ranging from big data analytics, artificial intelligence, search,\nmachine learning, virtual/augmented reality, computer vision, image/signal\nprocessing to computational science and bioinformatics. With Moore's law\ndriving the evolution of hardware platforms towards exascale, the dominant\nperformance metric (time efficiency) has now expanded to also incorporate\npower/energy efficiency. Therefore, the major challenge that we face in\ncomputing systems research is: \"how to solve massive-scale computational\nproblems in the most time/power/energy efficient manner?\"\n  The architectures are constantly evolving making the current performance\noptimizing strategies less applicable and new strategies to be invented. The\nsolution is for the new architectures, new programming models, and applications\nto go forward together. Doing this is, however, extremely hard. There are too\nmany design choices in too many dimensions. We propose the following strategy\nto solve the problem: (i) Models - Develop accurate analytical models (e.g.\nexecution time, energy, silicon area) to predict the cost of executing a given\nprogram, and (ii) Complete System Design - Simultaneously optimize all the cost\nmodels for the programs (computational problems) to obtain the most\ntime/area/power/energy efficient solution. Such an optimization problem evokes\nthe notion of codesign.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.01957v1"
    },
    {
        "title": "On Learning the $cμ$ Rule in Single and Parallel Server Networks",
        "authors": [
            "Subhashini Krishnasamy",
            "Ari Arapostathis",
            "Ramesh Johari",
            "Sanjay Shakkottai"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  We consider learning-based variants of the $c \\mu$ rule for scheduling in\nsingle and parallel server settings of multi-class queueing systems.\n  In the single server setting, the $c \\mu$ rule is known to minimize the\nexpected holding-cost (weighted queue-lengths summed over classes and a fixed\ntime horizon). We focus on the problem where the service rates $\\mu$ are\nunknown with the holding-cost regret (regret against the $c \\mu$ rule with\nknown $\\mu$) as our objective. We show that the greedy algorithm that uses\nempirically learned service rates results in a constant holding-cost regret\n(the regret is independent of the time horizon). This free exploration can be\nexplained in the single server setting by the fact that any work-conserving\npolicy obtains the same number of samples in a busy cycle.\n  In the parallel server setting, we show that the $c \\mu$ rule may result in\nunstable queues, even for arrival rates within the capacity region. We then\npresent sufficient conditions for geometric ergodicity under the $c \\mu$ rule.\nUsing these results, we propose an almost greedy algorithm that explores only\nwhen the number of samples falls below a threshold. We show that this algorithm\ndelivers constant holding-cost regret because a free exploration condition is\neventually satisfied.\n",
        "pdf_link": "http://arxiv.org/pdf/1802.06723v2"
    },
    {
        "title": "A Comparative Evaluation of Log-Based Process Performance Analysis\n  Techniques",
        "authors": [
            "Fredrik Milani",
            "Fabrizio M. Maggi"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Process mining has gained traction over the past decade and an impressive\nbody of research has resulted in the introduction of a variety of process\nmining approaches measuring process performance. Having this set of techniques\navailable, organizations might find it difficult to identify which approach is\nbest suited considering context, performance indicator, and data availability.\nIn light of this challenge, this paper aims at introducing a framework for\ncategorizing and selecting performance analysis approaches based on existing\nresearch. We start from a systematic literature review for identifying the\nexisting works discussing how to measure process performance based on\ninformation retrieved from event logs. Then, the proposed framework is built\nstarting from the information retrieved from these studies taking into\nconsideration different aspects of performance analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.03965v1"
    },
    {
        "title": "ReCA: an Efficient Reconfigurable Cache Architecture for Storage Systems\n  with Online Workload Characterization",
        "authors": [
            "Reza Salkhordeh",
            "Shahriar Ebrahimi",
            "Hossein Asadi"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In recent years, SSDs have gained tremendous attention in computing and\nstorage systems due to significant performance improvement over HDDs. The cost\nper capacity of SSDs, however, prevents them from entirely replacing HDDs in\nsuch systems. One approach to effectively take advantage of SSDs is to use them\nas a caching layer to store performance critical data blocks to reduce the\nnumber of accesses to disk subsystem. Due to characteristics of Flash-based\nSSDs such as limited write endurance and long latency on write operations,\nemploying caching algorithms at the Operating System (OS) level necessitates to\ntake such characteristics into consideration. Previous caching techniques are\noptimized towards only one type of application, which affects both generality\nand applicability. In addition, they are not adaptive when the workload pattern\nchanges over time. This paper presents an efficient Reconfigurable Cache\nArchitecture (ReCA) for storage systems using a comprehensive workload\ncharacterization to find an optimal cache configuration for I/O intensive\napplications. For this purpose, we first investigate various types of I/O\nworkloads and classify them into five major classes. Based on this\ncharacterization, an optimal cache configuration is presented for each class of\nworkloads. Then, using the main features of each class, we continuously monitor\nthe characteristics of an application during system runtime and the cache\norganization is reconfigured if the application changes from one class to\nanother class of workloads. The cache reconfiguration is done online and\nworkload classes can be extended to emerging I/O workloads in order to maintain\nits efficiency with the characteristics of I/O requests. Experimental results\nobtained by implementing ReCA in a server running Linux show that the proposed\narchitecture improves performance and lifetime up to 24\\% and 33\\%,\nrespectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06747v1"
    },
    {
        "title": "Optimal Scheduling and Exact Response Time Analysis for Multistage Jobs",
        "authors": [
            "Ziv Scully",
            "Mor Harchol-Balter",
            "Alan Scheller-Wolf"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Scheduling to minimize mean response time in an M/G/1 queue is a classic\nproblem. The problem is usually addressed in one of two scenarios. In the\nperfect-information scenario, the scheduler knows each job's exact size, or\nservice requirement. In the zero-information scenario, the scheduler knows only\neach job's size distribution. The well-known shortest remaining processing time\n(SRPT) policy is optimal in the perfect-information scenario, and the more\ncomplex Gittins policy is optimal in the zero-information scenario.\n  In real systems the scheduler often has partial but incomplete information\nabout each job's size. We introduce a new job model, that of multistage jobs,\nto capture this partial-information scenario. A multistage job consists of a\nsequence of stages, where both the sequence of stages and stage sizes are\nunknown, but the scheduler always knows which stage of a job is in progress. We\ngive an optimal algorithm for scheduling multistage jobs in an M/G/1 queue and\nan exact response time analysis of our algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.06865v2"
    },
    {
        "title": "Infinite-server queueing model with MAPkGk Markov arrival streams,\n  random volume of customers in random environment subject to catastrophe",
        "authors": [
            "Khanik Kerobyan",
            "Ruben Kerobyan",
            "Koffi Enakoutsa"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In this paper the infinite server queue model in semi-Markov random\nenvironment with k Markov arrival streams, random resources of customers, and\ncatastrophes is considered. After catastrophes occur, all customers in the\nmodel are flashed out and the system jumps into recovery station. After the\nrecovery time the model works from the empty state. The transient and\nstationary joint distributions of numbers of different types of customers in\nthe model at moment t, numbers of different types of served in some interval\ncustomers, volume of accumulated resources in the model at moment t, and total\nvolume of served resources in an interval for the model without catastrophes\nare found. The transient and stationary joint distributions of numbers of\ndifferent types of customers in the model at moment t, and volume of\naccumulated resources in the model at moment t and their moments for the model\nwith catastrophes are obtained. All results are obtained using Danzig\ncollective marks method and renewal theory methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09641v1"
    },
    {
        "title": "An infinite-server queueing model MMAPkGk in semi-Markov random\n  environment with marked MAP arrival and subject to catastrophes",
        "authors": [
            "K. Kerobyan",
            "R. Covington",
            "R. Kerobyan",
            "K. Enakoutsa"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In the present paper the infinite-server MMAPkGk queueing model with random\nresource vector of customers, marked MAP arrival and semi-Markov (SM) arrival\nof catastrophes is considered. The joint generating functions (PGF) of\ntransient and stationary distributions of number of busy servers and numbers of\ndifferent types served customers, as well as Laplace transformations (LT) of\njoint distributions of total accumulated resources in the model at moment and\ntotal accumulated resources of served customers during time interval are found.\nThe basic differential and renewal equations for transient and stationary PGF\nof queue sizes of customers are found.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.09642v1"
    },
    {
        "title": "LIRS: Enabling efficient machine learning on NVM-based storage via a\n  lightweight implementation of random shuffling",
        "authors": [
            "Zhi-Lin Ke",
            "Hsiang-Yun Cheng",
            "Chia-Lin Yang"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Machine learning algorithms, such as Support Vector Machine (SVM) and Deep\nNeural Network (DNN), have gained a lot of interests recently. When training a\nmachine learning algorithm, randomly shuffle all the training data can improve\nthe testing accuracy and boost the convergence rate. Nevertheless, realizing\ntraining data random shuffling in a real system is not a straightforward\nprocess due to the slow random accesses in hard disk drive (HDD). To avoid\nfrequent random disk access, the effect of random shuffling is often limited in\nexisting approaches. With the emerging non-volatile memory-based storage\ndevice, such as Intel Optane SSD, which provides fast random accesses, we\npropose a lightweight implementation of random shuffling (LIRS) to randomly\nshuffle the indexes of the entire training dataset, and the selected training\ninstances are directly accessed from the storage and packed into batches.\nExperimental results show that LIRS can reduce the total training time of SVM\nand DNN by 49.9% and 43.5% on average, and improve the final testing accuracy\non DNN by 1.01%.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04509v1"
    },
    {
        "title": "On Coding for Reliable VNF Chaining in DCNs",
        "authors": [
            "Anna Engelmann",
            "Admela Jukan",
            "Rastin Pries"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  We study how erasure coding can improve service reliability in Data Center\nNetworks (DCN). To this end, we find that coding can be best deployed in\nsystems, where i) traffic is split into multiple parallel sub-flows, ii) each\nsub-flow is encoded; iii) SFC along with their corresponding Virtual Network\nFunctions (VNF) concatenated are replicated into at least as many VNF instances\nas there are sub-flows, resulting in parallel sub- SFCs; and iv) all coded\nsub-flows are distributed over parallel paths and processed in parallel. We\nstudy service reliability as function of the level of parallelization within\nDCN and the resulting amount of redundancy. Based on the probability theory and\nby considering failures of path segments, VNF and server failures, we\nanalytically derive the probability that parallel subflows are successfully\nprocessed by the parallelized SFC and that the original serial traffic can be\nsuccessfully recovered without service interruptions.We compare the proposed\nfailure protection with coding and the standard backup protection and evaluate\nthe related overhead of both methods, including decoding, traffic redirection\nand VNF migration. The results not only show the benefit of our scheme for\nreliability, but also a reduced overhead required in comparison to backup\nprotection.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.12092v1"
    },
    {
        "title": "A Comparative Measurement Study of Deep Learning as a Service Framework",
        "authors": [
            "Yanzhao Wu",
            "Ling Liu",
            "Calton Pu",
            "Wenqi Cao",
            "Semih Sahin",
            "Wenqi Wei",
            "Qi Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Big data powered Deep Learning (DL) and its applications have blossomed in\nrecent years, fueled by three technological trends: a large amount of digitized\ndata openly accessible, a growing number of DL software frameworks in open\nsource and commercial markets, and a selection of affordable parallel computing\nhardware devices. However, no single DL framework, to date, dominates in terms\nof performance and accuracy even for baseline classification tasks on standard\ndatasets, making the selection of a DL framework an overwhelming task. This\npaper takes a holistic approach to conduct empirical comparison and analysis of\nfour representative DL frameworks with three unique contributions. First, given\na selection of CPU-GPU configurations, we show that for a specific DL\nframework, different configurations of its hyper-parameters may have a\nsignificant impact on both performance and accuracy of DL applications. Second,\nto the best of our knowledge, this study is the first to identify the\nopportunities for improving the training time performance and the accuracy of\nDL frameworks by configuring parallel computing libraries and tuning individual\nand multiple hyper-parameters. Third, we also conduct a comparative measurement\nstudy on the resource consumption patterns of four DL frameworks and their\nperformance and accuracy implications, including CPU and memory usage, and\ntheir correlations to varying settings of hyper-parameters under different\nconfiguration combinations of hardware, parallel computing libraries. We argue\nthat this measurement study provides in-depth empirical comparison and analysis\nof four representative DL frameworks, and offers practical guidance for service\nproviders to deploying and delivering DL as a Service (DLaaS) and for\napplication developers and DLaaS consumers to select the right DL frameworks\nfor the right DL workloads.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.12210v2"
    },
    {
        "title": "An Efficient Hybrid I/O Caching Architecture Using Heterogeneous SSDs",
        "authors": [
            "Reza Salkhordeh",
            "Mostafa Hadizadeh",
            "Hossein Asadi"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  SSDs are emerging storage devices which unlike HDDs, do not have mechanical\nparts and therefore, have superior performance compared to HDDs. Due to the\nhigh cost of SSDs, entirely replacing HDDs with SSDs is not economically\njustified. Additionally, SSDs can endure a limited number of writes before\nfailing. To mitigate the shortcomings of SSDs while taking advantage of their\nhigh performance, SSD caching is practiced in both academia and industry.\nPreviously proposed caching architectures have only focused on either\nperformance or endurance and neglected to address both parameters in suggested\narchitectures. Moreover, the cost, reliability, and power consumption of such\narchitectures is not evaluated. This paper proposes a hybrid I/O caching\narchitecture that while offers higher performance than previous studies, it\nalso improves power consumption with a similar budget. The proposed\narchitecture uses DRAM, Read-Optimized SSD, and Write-Optimized SSD in a\nthree-level cache hierarchy and tries to efficiently redirect read requests to\neither DRAM or RO-SSD while sending writes to WO-SSD. To provide high\nreliability, dirty pages are written to at least two devices which removes any\nsingle point of failure. The power consumption is also managed by reducing the\nnumber of accesses issued to SSDs. The proposed architecture reconfigures\nitself between performance- and endurance-optimized policies based on the\nworkload characteristics to maintain an effective tradeoff between performance\nand endurance. We have implemented the proposed architecture on a server\nequipped with industrial SSDs and HDDs. The experimental results show that as\ncompared to state-of-the-art studies, the proposed architecture improves\nperformance and power consumption by an average of 8% and 28%, respectively,\nand reduces the cost by 5% while increasing the endurance cost by 4.7% and\nnegligible reliability penalty.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.04122v1"
    },
    {
        "title": "A Preliminary Study of Neural Network-based Approximation for HPC\n  Applications",
        "authors": [
            "Wenqian Dong",
            "Anzheng Guolu",
            "Dong Li"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Machine learning, as a tool to learn and model complicated (non)linear\nrelationships between input and output data sets, has shown preliminary success\nin some HPC problems. Using machine learning, scientists are able to augment\nexisting simulations by improving accuracy and significantly reducing\nlatencies. Our ongoing research work is to create a general framework to apply\nneural network-based models to HPC applications. In particular, we want to use\nthe neural network to approximate and replace code regions within the HPC\napplication to improve performance (i.e., reducing the execution time) of the\nHPC application. In this paper, we present our preliminary study and results.\nUsing two applications (the Newton-Raphson method and the Lennard-Jones (LJ)\npotential in LAMMP) for our case study, we achieve up to 2.7x and 2.46x\nspeedup, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/1812.07561v1"
    },
    {
        "title": "A Learned Performance Model for Tensor Processing Units",
        "authors": [
            "Samuel J. Kaufman",
            "Phitchaya Mangpo Phothilimthana",
            "Yanqi Zhou",
            "Charith Mendis",
            "Sudip Roy",
            "Amit Sabne",
            "Mike Burrows"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Accurate hardware performance models are critical to efficient code\ngeneration. They can be used by compilers to make heuristic decisions, by\nsuperoptimizers as a minimization objective, or by autotuners to find an\noptimal configuration for a specific program. However, they are difficult to\ndevelop because contemporary processors are complex, and the recent\nproliferation of deep learning accelerators has increased the development\nburden. We demonstrate a method of learning performance models from a corpus of\ntensor computation graph programs for Tensor Processing Unit (TPU) instances.\nWe show that our learned model outperforms a heavily-optimized analytical\nperformance model on two tasks -- tile-size selection and operator fusion --\nand that it helps an autotuner discover faster programs in a setting where\naccess to TPUs is limited or expensive.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.01040v2"
    },
    {
        "title": "Optimal Load Balancing in Bipartite Graphs",
        "authors": [
            "Wentao Weng",
            "Xingyu Zhou",
            "R. Srikant"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Applications in cloud platforms motivate the study of efficient load\nbalancing under job-server constraints and server heterogeneity. In this paper,\nwe study load balancing on a bipartite graph where left nodes correspond to job\ntypes and right nodes correspond to servers, with each edge indicating that a\njob type can be served by a server. Thus edges represent locality constraints,\ni.e., each job can only be served at servers which contained certain data\nand/or machine learning (ML) models. Servers in this system can have\nheterogeneous service rates. In this setting, we investigate the performance of\ntwo policies named Join-the-Fastest-of-the-Shortest-Queue (JFSQ) and\nJoin-the-Fastest-of-the-Idle-Queue (JFIQ), which are simple variants of\nJoin-the-Shortest-Queue and Join-the-Idle-Queue, where ties are broken in favor\nof the fastest servers. Under a \"well-connected\" graph condition, we show that\nJFSQ and JFIQ are asymptotically optimal in the mean response time when the\nnumber of servers goes to infinity. In addition to asymptotic optimality, we\nalso obtain upper bounds on the mean response time for finite-size systems. We\nfurther show that the well-connectedness condition can be satisfied by a random\nbipartite graph construction with relatively sparse connectivity.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.08830v1"
    },
    {
        "title": "Network calculus for parallel processing",
        "authors": [
            "G. Kesidis",
            "B. Urgaonkar",
            "Y. Shan",
            "S. Kamarava",
            "J. Liebeherr"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  In this note, we present preliminary results on the use of \"network calculus\"\nfor parallel processing systems, specifically MapReduce.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.0820v2"
    },
    {
        "title": "Instability of Sharing Systems in the Presence of Retransmissions",
        "authors": [
            "Predrag R. Jelenković",
            "Evangelia D. Skiani"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Retransmissions represent a primary failure recovery mechanism on all layers\nof communication network architecture. Similarly, fair sharing, e.g. processor\nsharing (PS), is a widely accepted approach to resource allocation among\nmultiple users. Recent work has shown that retransmissions in failure-prone,\ne.g. wireless ad hoc, networks can cause heavy tails and long delays. In this\npaper, we discover a new phenomenon showing that PS-based scheduling induces\ncomplete instability with zero throughput in the presence of retransmissions,\nregardless of how low the traffic load may be. This phenomenon occurs even when\nthe job sizes are bounded/fragmented, e.g. deterministic. Our analytical\nresults are further validated via simulation experiments. Moreover, our work\ndemonstrates that scheduling one job at a time, such as first-come-first-serve,\nachieves stability and should be preferred in these systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.5622v1"
    },
    {
        "title": "A Queueing Network Approach to the Analysis and Control of\n  Mobility-On-Demand Systems",
        "authors": [
            "Rick Zhang",
            "Marco Pavone"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  This paper presents a queueing network approach to the analysis and control\nof mobility-on-demand (MoD) systems for urban personal transportation. A MoD\nsystem consists of a fleet of vehicles providing one-way car sharing service\nand a team of drivers to rebalance such vehicles. The drivers then rebalance\nthemselves by driving select customers similar to a taxi service. We model the\nMoD system as two coupled closed Jackson networks with passenger loss. We show\nthat the system can be approximately balanced by solving two decoupled linear\nprograms and exactly balanced through nonlinear optimization. The rebalancing\ntechniques are applied to a system sizing example using taxi data in three\nneighborhoods of Manhattan, which suggests that the optimal vehicle-to-driver\nratio in a MoD system is between 3 and 5. Lastly, we formulate a real-time\nclosed-loop rebalancing policy for drivers and demonstrate its stability (in\nterms of customer wait times) for typical system loads.\n",
        "pdf_link": "http://arxiv.org/pdf/1409.6775v2"
    },
    {
        "title": "Exploiting Parallelism in Optical Network Systems: A Case Study of\n  Random Linear Network Coding (RLNC) in Ethernet-over-Optical Networks",
        "authors": [
            "Anna Engelmann",
            "Wolfgang Bziuk",
            "Admela Jukan",
            "Muriel Medard"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  As parallelism becomes critically important in the semiconductor technology,\nhigh-performance computing, and cloud applications, parallel network systems\nwill increasingly follow suit. Today, parallelism is an essential architectural\nfeature of 40/100/400 Gigabit Ethernet standards, whereby high speed Ethernet\nsystems are equipped with multiple parallel network interfaces. This creates\nnew network topology abstractions and new technology requirements: instead of a\nsingle high capacity network link, multiple Ethernet end-points and interfaces\nneed to be considered together with multiple links in form of discrete parallel\npaths. This new paradigm is enabling implementations of various new features to\nimprove overall system performance. In this paper, we analyze the performance\nof parallel network systems with network coding. In particular, by using random\nLNC (RLNC), - a code without the need for decoding, we can make use of the fact\nthat we have codes that are both distributed (removing the need for\ncoordination or optimization of resources) and composable (without the need to\nexchange code information), leading to a fully stateless operation. We propose\na novel theoretical modeling framework, including derivation of the upper and\nlower bounds as well as an expected value of the differential delay of parallel\npaths, and the resulting queue size at the receiver. The results show a great\npromise of network system parallelism in combination with RLNC: with a proper\nset of design parameters, the differential delay and the buffer size at the\nEthernet receiver can be reduced significantly, while the cross-layer design\nand routing can be greatly simplified.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.02789v1"
    },
    {
        "title": "Asymptotic Performance Evaluation of Battery Swapping and Charging\n  Station for Electric Vehicles",
        "authors": [
            "Xiaoqi Tan",
            "Bo Sun",
            "Yuan Wu",
            "Danny H. K. Tsang"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  A battery swapping and charging station (BSCS) is an energy refueling\nstation, where i) electric vehicles (EVs) with depleted batteries (DBs) can\nswap their DBs for fully-charged ones, and ii) the swapped DBs are then charged\nuntil they are fully-charged. Successful deployment of a BSCS system\nnecessitates a careful planning of swapping- and charging-related\ninfrastructures, and thus a comprehensive performance evaluation of the BSCS is\nbecoming crucial. This paper studies such a performance evaluation problem with\na novel mixed queueing network (MQN) model and validates this model with\nextensive numerical simulation. We adopt the EVs' blocking probability as our\nquality-of-service measure and focus on studying the impact of the key\nparameters of the BSCS (e.g., the numbers of parking spaces, swapping islands,\nchargers, and batteries) on the blocking probability. We prove a necessary and\nsufficient condition for showing the ergodicity of the MQN when the number of\nbatteries approaches infinity, and further prove that the blocking probability\nhas two different types of asymptotic behaviors. Meanwhile, for each type of\nasymptotic behavior, we analytically derive the asymptotic lower bound of the\nblocking probability.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.07175v2"
    },
    {
        "title": "Age of Information in a Decentralized Network of Parallel Queues with\n  Routing and Packets Losses",
        "authors": [
            "Josu Doncel",
            "Mohamad Assaad"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The paper deals with Age of Information (AoI) in a network of multiple\nsources and parallel queues with buffering capabilities, preemption in service\nand losses in served packets. The queues do not communicate between each other\nand the packets are dispatched through the queues according to a predefined\nprobabilistic routing. By making use of the Stochastic Hybrid System (SHS)\nmethod, we provide a derivation of the average AoI of a system of two parallel\nqueues (with and without buffer capabilities) and compare the results with\nthose of a single queue. We show that known results of packets delay in Queuing\nTheory do not hold for the AoI. Unfortunately, the complexity of computing the\naverage AoI using the SHS method increases highly with the number of queues. We\ntherefore provide an upper bound of the average AoI in a system of an arbitrary\nnumber of M/M/1/(N+1) queues and show its tightness in various regimes. This\nupper bound allows providing a tight approximation of the average AoI with a\nvery low complexity. We then provide a game framework that allows each source\nto determine its best probabilistic routing decision. By using Mean Field\nGames, we provide an analysis of the routing game framework, propose an\nefficient iterative method to find the routing decision of each source and\nprove its convergence to the desired equilibrium.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.01696v2"
    },
    {
        "title": "Understanding HPC Benchmark Performance on Intel Broadwell and Cascade\n  Lake Processors",
        "authors": [
            "Christie L. Alappat",
            "Johannes Hofmann",
            "Georg Hager",
            "Holger Fehske",
            "Alan R. Bishop",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Hardware platforms in high performance computing are constantly getting more\ncomplex to handle even when considering multicore CPUs alone. Numerous features\nand configuration options in the hardware and the software environment that are\nrelevant for performance are not even known to most application users or\ndevelopers. Microbenchmarks, i.e., simple codes that fathom a particular aspect\nof the hardware, can help to shed light on such issues, but only if they are\nwell understood and if the results can be reconciled with known facts or\nperformance models. The insight gained from microbenchmarks may then be applied\nto real applications for performance analysis or optimization. In this paper we\ninvestigate two modern Intel x86 server CPU architectures in depth: Broadwell\nEP and Cascade Lake SP. We highlight relevant hardware configuration settings\nthat can have a decisive impact on code performance and show how to properly\nmeasure on-chip and off-chip data transfer bandwidths. The new victim L3 cache\nof Cascade Lake and its advanced replacement policy receive due attention.\nFinally we use DGEMM, sparse matrix-vector multiplication, and the HPCG\nbenchmark to make a connection to relevant application scenarios.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.03344v2"
    },
    {
        "title": "An optimal scheduling architecture for accelerating batch algorithms on\n  Neural Network processor architectures",
        "authors": [
            "Phani Kumar Nyshadham",
            "Mohit Sinha",
            "Biswajit Mishra",
            "H S Vijay"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  In neural network topologies, algorithms are running on batches of data\ntensors. The batches of data are typically scheduled onto the computing cores\nwhich execute in parallel. For the algorithms running on batches of data, an\noptimal batch scheduling architecture is very much needed by suitably utilizing\nhardware resources - thereby resulting in significant reduction training and\ninference time. In this paper, we propose to accelerate the batch algorithms\nfor neural networks through a scheduling architecture enabling optimal compute\npower utilization. The proposed optimal scheduling architecture can be built\ninto HW or can be implemented in SW alone which can be leveraged for\naccelerating batch algorithms. The results demonstrate that the proposed\narchitecture speeds up the batch algorithms compared to the previous solutions.\nThe proposed idea applies to any HPC architecture meant for neural networks.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07062v1"
    },
    {
        "title": "AIBench: An Agile Domain-specific Benchmarking Methodology and an AI\n  Benchmark Suite",
        "authors": [
            "Wanling Gao",
            "Fei Tang",
            "Jianfeng Zhan",
            "Chuanxin Lan",
            "Chunjie Luo",
            "Lei Wang",
            "Jiahui Dai",
            "Zheng Cao",
            "Xiongwang Xiong",
            "Zihan Jiang",
            "Tianshu Hao",
            "Fanda Fan",
            "Xu Wen",
            "Fan Zhang",
            "Yunyou Huang",
            "Jianan Chen",
            "Mengjia Du",
            "Rui Ren",
            "Chen Zheng",
            "Daoyi Zheng",
            "Haoning Tang",
            "Kunlin Zhan",
            "Biao Wang",
            "Defei Kong",
            "Minghe Yu",
            "Chongkang Tan",
            "Huan Li",
            "Xinhui Tian",
            "Yatao Li",
            "Gang Lu",
            "Junchao Shao",
            "Zhenyu Wang",
            "Xiaoyu Wang",
            "Hainan Ye"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Domain-specific software and hardware co-design is encouraging as it is much\neasier to achieve efficiency for fewer tasks. Agile domain-specific\nbenchmarking speeds up the process as it provides not only relevant design\ninputs but also relevant metrics, and tools. Unfortunately, modern workloads\nlike Big data, AI, and Internet services dwarf the traditional one in terms of\ncode size, deployment scale, and execution path, and hence raise serious\nbenchmarking challenges.\n  This paper proposes an agile domain-specific benchmarking methodology.\nTogether with seventeen industry partners, we identify ten important end-to-end\napplication scenarios, among which sixteen representative AI tasks are\ndistilled as the AI component benchmarks. We propose the permutations of\nessential AI and non-AI component benchmarks as end-to-end benchmarks. An\nend-to-end benchmark is a distillation of the essential attributes of an\nindustry-scale application. We design and implement a highly extensible,\nconfigurable, and flexible benchmark framework, on the basis of which, we\npropose the guideline for building end-to-end benchmarks, and present the first\nend-to-end Internet service AI benchmark.\n  The preliminary evaluation shows the value of our benchmark suite---AIBench\nagainst MLPerf and TailBench for hardware and software designers,\nmicro-architectural researchers, and code developers. The specifications,\nsource code, testbed, and results are publicly available from the web site\n\\url{http://www.benchcouncil.org/AIBench/index.html}.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.07162v1"
    },
    {
        "title": "Optimizing Memory-Access Patterns for Deep Learning Accelerators",
        "authors": [
            "Hongbin Zheng",
            "Sejong Oh",
            "Huiqing Wang",
            "Preston Briggs",
            "Jiading Gai",
            "Animesh Jain",
            "Yizhi Liu",
            "Rich Heaton",
            "Randy Huang",
            "Yida Wang"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Deep learning (DL) workloads are moving towards accelerators for faster\nprocessing and lower cost. Modern DL accelerators are good at handling the\nlarge-scale multiply-accumulate operations that dominate DL workloads; however,\nit is challenging to make full use of the compute power of an accelerator since\nthe data must be properly staged in a software-managed scratchpad memory.\nFailing to do so can result in significant performance loss. This paper\nproposes a systematic approach which leverages the polyhedral model to analyze\nall operators of a DL model together to minimize the number of memory accesses.\nExperiments show that our approach can substantially reduce the impact of\nmemory accesses required by common neural-network models on a homegrown AWS\nmachine-learning inference chip named Inferentia, which is available through\nAmazon EC2 Inf1 instances.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.12798v1"
    },
    {
        "title": "\"The tail wags the dog\": A study of anomaly detection in commercial\n  application performance",
        "authors": [
            "Richard Gow",
            "Srikumar Venugopal",
            "Pradeep Ray"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  The IT industry needs systems management models that leverage available\napplication information to detect quality of service, scalability and health of\nservice. Ideally this technique would be common for varying application types\nwith different n-tier architectures under normal production conditions of\nvarying load, user session traffic, transaction type, transaction mix, and\nhosting environment.\n  This paper shows that a whole of service measurement paradigm utilizing a\nblack box M/M/1 queuing model and auto regression curve fitting of the\nassociated CDF are an accurate model to characterize system performance\nsignatures. This modeling method is also used to detect application slow down\nevents. The technique was shown to work for a diverse range of workloads\nranging from 76 Tx/ 5min to 19,025 Tx/ 5min. The method did not rely on\ncustomizations specific to the n-tier architecture of the systems being\nanalyzed and so the performance anomaly detection technique was shown to be\nplatform and configuration agnostic.\n",
        "pdf_link": "http://arxiv.org/pdf/1307.1743v1"
    },
    {
        "title": "Analysis of Petri Net Models through Stochastic Differential Equations",
        "authors": [
            "Marco Beccuti",
            "Enrico Bibbona",
            "Andras Horvath",
            "Roberta Sirovich",
            "Alessio Angius",
            "Gianfranco Balbo"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  It is well known, mainly because of the work of Kurtz, that density dependent\nMarkov chains can be approximated by sets of ordinary differential equations\n(ODEs) when their indexing parameter grows very large. This approximation\ncannot capture the stochastic nature of the process and, consequently, it can\nprovide an erroneous view of the behavior of the Markov chain if the indexing\nparameter is not sufficiently high. Important phenomena that cannot be revealed\ninclude non-negligible variance and bi-modal population distributions. A\nless-known approximation proposed by Kurtz applies stochastic differential\nequations (SDEs) and provides information about the stochastic nature of the\nprocess. In this paper we apply and extend this diffusion approximation to\nstudy stochastic Petri nets. We identify a class of nets whose underlying\nstochastic process is a density dependent Markov chain whose indexing parameter\nis a multiplicative constant which identifies the population level expressed by\nthe initial marking and we provide means to automatically construct the\nassociated set of SDEs. Since the diffusion approximation of Kurtz considers\nthe process only up to the time when it first exits an open interval, we extend\nthe approximation by a machinery that mimics the behavior of the Markov chain\nat the boundary and allows thus to apply the approach to a wider set of\nproblems. The resulting process is of the jump-diffusion type. We illustrate by\nexamples that the jump-diffusion approximation which extends to bounded domains\ncan be much more informative than that based on ODEs as it can provide accurate\nquantity distributions even when they are multi-modal and even for relatively\nsmall population levels. Moreover, we show that the method is faster than\nsimulating the original Markov chain.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.0975v1"
    },
    {
        "title": "Batch Arrival Multiserver Queue with Setup Time",
        "authors": [
            "Tuan Phung-Duc"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Queues with setup time are extensively studied because they have application\nin performance evaluation of power-saving data centers. In a data center, there\nare a huge number of servers which consume a large amount of energy. In the\ncurrent technology, an idle server still consumes about 60\\% of its peak\nprocessing a job. Thus, the only way to save energy is to turn off servers\nwhich are not processing a job. However, when there are some waiting jobs, we\nhave to turn on the OFF servers. A server needs some setup time to be active\nduring which it consumes energy but cannot process a job. Therefore, there\nexists a trade-off between power consumption and delay performance. Gandhi et\nal. \\cite{Gandhi10a,Gandhi10} analyze this trade-off using an M/M/$c$ queue\nwith staggered setup (one server in setup at a time). In this paper, using an\nalternative approach, we obtain generating functions for the joint stationary\ndistribution of the number of active servers and that of jobs in the system for\na more general model with batch arrivals and state-dependent setup time. We\nfurther obtain moments for the queue size. Numerical results reveal that\nkeeping the same traffic intensity, the mean power consumption decreases with\nthe mean batch size for the case of fixed batch size. One of the main\ntheoretical contribution is a new conditional decomposition formula showing\nthat the number of waiting customers under the condition that all servers are\nbusy can be decomposed to the sum of two independent random variables where the\nfirst is the same quantity in the corresponding model without setup time while\nthe second is the number of waiting customers before an arbitrary customer.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.2228v3"
    },
    {
        "title": "SleepScale: Runtime Joint Speed Scaling and Sleep States Management for\n  Power Efficient Data Centers",
        "authors": [
            "Yanpei Liu",
            "Stark C. Draper",
            "Nam Sung Kim"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Power consumption in data centers has been growing significantly in recent\nyears. To reduce power, servers are being equipped with increasingly\nsophisticated power management mechanisms. Different mechanisms offer\ndramatically different trade-offs between power savings and performance\npenalties. Considering the complexity, variety, and temporally varying nature\nof the applications hosted in a typical data center, intelligently determining\nwhich power management policy to use and when is a complicated task.\n  In this paper we analyze a system model featuring both performance scaling\nand low-power states. We reveal the interplay between performance scaling and\nlow-power states via intensive simulation and analytic verification. Based on\nthe observations, we present SleepScale, a runtime power management tool\ndesigned to efficiently exploit existing power control mechanisms. At run time,\nSleepScale characterizes power consumption and quality-of-service (QoS) for\neach low-power state and frequency setting, and selects the best policy for a\ngiven QoS constraint. We evaluate SleepScale using workload traces from data\ncenters and achieve significant power savings relative to conventional power\nmanagement strategies.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.5121v1"
    },
    {
        "title": "A Markovian Analysis of IEEE 802.11 Broadcast Transmission Networks with\n  Buffering",
        "authors": [
            "Guy Fayolle",
            "Paul Muhlethaler"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  The purpose of this paper is to analyze the so-called back-off technique of\nthe IEEE 802.11 protocol in broadcast mode with waiting queues. In contrast to\nexisting models, packets arriving when a station (or node) is in back-off state\nare not discarded, but are stored in a buffer of infinite capacity. As in\nprevious studies, the key point of our analysis hinges on the assumption that\nthe time on the channel is viewed as a random succession of transmission slots\n(whose duration corresponds to the length of a packet) and mini-slots during\nwhich the back-o? of the station is decremented. These events occur\nindependently, with given probabilities. The state of a node is represented by\na two-dimensional Markov chain in discrete-time, formed by the back-off counter\nand the number of packets at the station. Two models are proposed both of which\nare shown to cope reasonably well with the physical principles of the protocol.\nThe stabillity (ergodicity) conditions are obtained and interpreted in terms of\nmaximum throughput. Several approximations related to these models are also\ndiscussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1506.06011v1"
    },
    {
        "title": "An Infinite Dimensional Model for a Many Server Priority Queue",
        "authors": [
            "Neal Master",
            "Zhengyuan Zhou",
            "Nicholas Bambos"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We consider a Markovian many server queueing system in which customers are\npreemptively scheduled according to exogenously assigned priority levels. The\npriority levels are randomly assigned from a continuous probability measure\nrather than a discrete one and hence, the queue is modeled by an infinite\ndimensional stochastic process. We analyze the equilibrium behavior of the\nsystem and provide several results. We derive the Radon-Nikodym derivative\n(with respect to Lebesgue measure) of the measure that describes the average\ndistribution of customer priority levels in the system; we provide a formula\nfor the expected sojourn time of a customer as a function of his priority\nlevel; and we provide a formula for the expected waiting time of a customer as\na function of his priority level. We verify our theoretical analysis with\ndiscrete-event simulations. We discuss how each of our results generalizes\nprevious work on infinite dimensional models for single server priority queues.\n",
        "pdf_link": "http://arxiv.org/pdf/1701.01328v1"
    },
    {
        "title": "Comparative benchmarking of cloud computing vendors with High\n  Performance Linpack",
        "authors": [
            "Mohammad Mohammadi",
            "Timur Bazhirov"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We present a comparative analysis of the maximum performance achieved by the\nLinpack benchmark on compute intensive hardware publicly available from\nmultiple cloud providers. We study both performance within a single compute\nnode, and speedup for distributed memory calculations with up to 32 nodes or at\nleast 512 computing cores. We distinguish between hyper-threaded and\nnon-hyper-threaded scenarios and estimate the performance per single computing\ncore. We also compare results with a traditional supercomputing system for\nreference. Our findings provide a way to rank the cloud providers and\ndemonstrate the viability of the cloud for high performance computing\napplications.\n",
        "pdf_link": "http://arxiv.org/pdf/1702.02968v1"
    },
    {
        "title": "SERENADE: A Parallel Randomized Algorithm Suite for Crossbar Scheduling\n  in Input-Queued Switches",
        "authors": [
            "Long Gong",
            "Liang Liu",
            "Sen Yang",
            "Jun Xu",
            "Yi Xie",
            "Xinbing Wang"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Most of today's high-speed switches and routers adopt an input-queued\ncrossbar switch architecture. Such a switch needs to compute a matching\n(crossbar schedule) between the input ports and output ports during each\nswitching cycle (time slot). A key research challenge in designing large (in\nnumber of input/output ports $N$) input-queued crossbar switches is to develop\ncrossbar scheduling algorithms that can compute \"high quality\" matchings --\ni.e., those that result in high switch throughput (ideally $100\\%$) and low\nqueueing delays for packets -- at line rates. SERENA is one such algorithm: it\noutputs excellent matching decisions that result in $100\\%$ switch throughput\nand reasonably good queueing delays. However, since SERENA is a centralized\nalgorithm with $O(N)$ computational complexity, it cannot support switches that\nboth are large and have a very high line rate per port. In this work, we\npropose SERENADE (SERENA, the Distributed Edition), a parallel iterative\nalgorithm that emulates SERENA in only $O(\\log N)$ iterations between input\nports and output ports, and hence has a time complexity of only $O(\\log N)$ per\nport. We prove that SERENADE can exactly emulate SERENA. We also propose an\nearly-stop version of SERENADE, called O-SERENADE, to only approximately\nemulate SERENA. Through extensive simulations, we show that O-SERENADE can\nachieve 100\\% throughput and that it has similar as or slightly better delay\nperformance than SERENA under various load conditions and traffic patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.07234v4"
    },
    {
        "title": "BENCHIP: Benchmarking Intelligence Processors",
        "authors": [
            "Jinhua Tao",
            "Zidong Du",
            "Qi Guo",
            "Huiying Lan",
            "Lei Zhang",
            "Shengyuan Zhou",
            "Lingjie Xu",
            "Cong Liu",
            "Haifeng Liu",
            "Shan Tang",
            "Allen Rush",
            "Willian Chen",
            "Shaoli Liu",
            "Yunji Chen",
            "Tianshi Chen"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  The increasing attention on deep learning has tremendously spurred the design\nof intelligence processing hardware. The variety of emerging intelligence\nprocessors requires standard benchmarks for fair comparison and system\noptimization (in both software and hardware). However, existing benchmarks are\nunsuitable for benchmarking intelligence processors due to their non-diversity\nand nonrepresentativeness. Also, the lack of a standard benchmarking\nmethodology further exacerbates this problem. In this paper, we propose\nBENCHIP, a benchmark suite and benchmarking methodology for intelligence\nprocessors. The benchmark suite in BENCHIP consists of two sets of benchmarks:\nmicrobenchmarks and macrobenchmarks. The microbenchmarks consist of\nsingle-layer networks. They are mainly designed for bottleneck analysis and\nsystem optimization. The macrobenchmarks contain state-of-the-art industrial\nnetworks, so as to offer a realistic comparison of different platforms. We also\npropose a standard benchmarking methodology built upon an industrial software\nstack and evaluation metrics that comprehensively reflect the various\ncharacteristics of the evaluated intelligence processors. BENCHIP is utilized\nfor evaluating various hardware platforms, including CPUs, GPUs, and\naccelerators. BENCHIP will be open-sourced soon.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.08315v2"
    },
    {
        "title": "High-Performance Code Generation though Fusion and Vectorization",
        "authors": [
            "Jason Sewall",
            "Simon J. Pennycook"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We present a technique for automatically transforming kernel-based\ncomputations in disparate, nested loops into a fused, vectorized form that can\nreduce intermediate storage needs and lead to improved performance on\ncontemporary hardware.\n  We introduce representations for the abstract relationships and data\ndependencies of kernels in loop nests and algorithms for manipulating them into\nmore efficient form; we similarly introduce techniques for determining data\naccess patterns for stencil-like array accesses and show how this can be used\nto elide storage and improve vectorization.\n  We discuss our prototype implementation of these ideas---named HFAV---and its\nuse of a declarative, inference-based front-end to drive transformations, and\nwe present results for some prominent codes in HPC.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.08774v1"
    },
    {
        "title": "Distributed Server Allocation for Content Delivery Networks",
        "authors": [
            "Sarath Pattathil",
            "Vivek S. Borkar",
            "Gaurav S. Kasbekar"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We propose a dynamic formulation of file-sharing networks in terms of an\naverage cost Markov decision process with constraints. By analyzing a\nWhittle-like relaxation thereof, we propose an index policy in the spirit of\nWhittle and compare it by simulations with other natural heuristics.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.11471v4"
    },
    {
        "title": "ROOT I/O compression algorithms and their performance impact within Run\n  3",
        "authors": [
            "Oksana Shadura",
            "Brian Paul Bockelman"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The LHCs Run3 will push the envelope on data-intensive workflows and, since\nat the lowest level this data is managed using the ROOT software framework,\npreparations for managing this data are starting already. At the beginning of\nLHC Run 1, all ROOT data was compressed with the ZLIB algorithm; since then,\nROOT has added support for additional algorithms such as LZMA and LZ4, each\nwith unique strengths. This work must continue as industry introduces new\ntechniques - ROOT can benefit saving disk space or reducing the I/O and\nbandwidth for online and offline needs of experiments by introducing better\ncompression algorithms. In addition to alternate algorithms, we have been\nexploring alternate techniques to improve parallelism and apply\npre-conditioners to the serialized data.\n  We have performed a survey of the performance of the new compression\ntechniques. Our survey includes various use cases of data compression of ROOT\nfiles provided by different LHC experiments. We also provide insight into\nsolutions applied to resolve bottlenecks in compression algorithms, resulting\nin improved ROOT performance.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.04624v2"
    },
    {
        "title": "MultiCloud Resource Management using Apache Mesos for Planned\n  Integration with Apache Airavata",
        "authors": [
            "Pankaj Saha",
            "Madhusudhan Govindaraju",
            "Suresh Marru",
            "Marlon Pierce"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We discuss initial results and our planned approach for incorporating Apache\nMesos based resource management that will enable design and development of\nscheduling strategies for Apache Airavata jobs so that they can be launched on\nmultiple clouds, wherein several VMs do not have Public IP addresses. We\npresent initial work and next steps on the design of a meta-scheduler using\nApache Mesos. Apache Mesos presents a unified view of resources available\nacross several clouds and clusters. Our meta-scheduler can potentially examine\nand identify the cases where multiple small jobs have been submitted by the\nsame scientists and then redirect job from the same community account or user\nto different clusters. Our approach uses a NAT firewall to make nodes/VMs,\nwithout a Public IP, visible to Mesos for the unified view.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.07312v2"
    },
    {
        "title": "Retrial Queueing Models: A Survey on Theory and Applications",
        "authors": [
            "Tuan Phung-Duc"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Retrial phenomenon naturally arises in various systems such as call centers,\ncellular networks and random access protocols in local area networks. This\npaper gives a comprehensive survey on theory and applications of retrial queues\nin these systems. We investigate the state of the art of the theoretical\nresearches including exact solutions, stability, asymptotic analyses and\nmultidimensional models. We present an overview on retrial models arising from\nreal world applications. Some open problems and promising research directions\nare also discussed.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.09560v1"
    },
    {
        "title": "Stress-SGX: Load and Stress your Enclaves for Fun and Profit",
        "authors": [
            "Sébastien Vaucher",
            "Valerio Schiavoni",
            "Pascal Felber"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The latest generation of Intel processors supports Software Guard Extensions\n(SGX), a set of instructions that implements a Trusted Execution Environment\n(TEE) right inside the CPU, by means of so-called enclaves. This paper presents\nStress-SGX, an easy-to-use stress-test tool to evaluate the performance of\nSGX-enabled nodes. We build on top of the popular Stress-NG tool, while only\nkeeping the workload injectors (stressors) that are meaningful in the SGX\ncontext. We report on several insights and lessons learned about porting legacy\ncode to run inside an SGX enclave, as well as the limitations introduced by\nthis process. Finally, we use Stress-SGX to conduct a study comparing the\nperformance of different SGX-enabled machines.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.11204v1"
    },
    {
        "title": "Analysis of an M/G/1 system for the optimization of the RTG performances\n  in the delivery of containers in Abidjan Terminal",
        "authors": [
            "Bakary Kone",
            "Salimata Gueye Diagne",
            "Dethie Dione",
            "Coumba Diallo"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  In front of the major challenges to increase its productivity while\nsatisfying its customer, it is today important to establish in advance the\noperational performances of the RTG Abidjan Terminal. In this article, by using\nan M/G/1 retrial queue system, we obtained the average number of parked\ndelivery trucks and as well as their waiting time. Finally, we used Matlab to\nrepresent them graphically then analyze the RTG performances according to the\ntraffic rate.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.01659v1"
    },
    {
        "title": "GPA: A GPU Performance Advisor Based on Instruction Sampling",
        "authors": [
            "Keren Zhou",
            "Xiaozhu Meng",
            "Ryuichi Sai",
            "John Mellor-Crummey"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Developing efficient GPU kernels can be difficult because of the complexity\nof GPU architectures and programming models. Existing performance tools only\nprovide coarse-grained suggestions at the kernel level, if any. In this paper,\nwe describe GPA, a performance advisor for NVIDIA GPUs that suggests potential\ncode optimization opportunities at a hierarchy of levels, including individual\nlines, loops, and functions. To relieve users of the burden of interpreting\nperformance counters and analyzing bottlenecks, GPA uses data flow analysis to\napproximately attribute measured instruction stalls to their root causes and\nuses information about a program's structure and the GPU to match inefficiency\npatterns with suggestions for optimization. To quantify each suggestion's\npotential benefits, we developed PC sampling-based performance models to\nestimate its speedup. Our experiments with benchmarks and applications show\nthat GPA provides an insightful report to guide performance optimization. Using\nGPA, we obtained speedups on a Volta V100 GPU ranging from 1.01$\\times$ to\n3.53$\\times$, with a geometric mean of 1.22$\\times$.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.04061v2"
    },
    {
        "title": "Correlation Coefficient Analysis of the Age of Information in\n  Multi-Source Systems",
        "authors": [
            "Yukang Jiang",
            "Kiichi Tokuyama",
            "Yuichiro Wada",
            "Moeko Yajima"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  This paper studies the age of information (AoI) on an information updating\nsystem such that multiple sources share one server to process packets of\nupdated information. In such systems, packets from different sources compete\nfor the server, and thus they may suffer from being interrupted, being\nbacklogged, and becoming stale. Therefore, in order to grasp structures of such\nsystems, it is crucially important to study a metric indicating a correlation\nof different sources. In this paper, we aim to analyze the correlation of AoIs\non a single-server queueing system with multiple sources. As our contribution,\nwe provide the closed-form expression of the correlation coefficient of the\nAoIs. To this end, we first derive the Laplace-Stieltjes transform of the\nstationary distribution of each AoI for the multiple sources. Some nontrivial\nproperties on the systems are revealed from our analysis results.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.11067v1"
    },
    {
        "title": "A class of equivalent idle-time-order-based routing policies for\n  heterogeneous multi-server systems",
        "authors": [
            "Sherwin Doroudi",
            "Ragavendran Gopalakrishnan"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  We consider an M/M/N/K/FCFS system (N>0, K>=N), where the servers operate at\n(possibly) heterogeneous service rates. In this situation, the steady state\nbehavior depends on the routing policy that is used to select which idle server\nserves the next job in queue. We define a class of idle-time-order-based\npolicies (including, for example, Longest Idle Server First (LISF)) and show\nthat all policies in this class result in the same steady state behavior. In\nparticular, they are all equivalent to the naive Random routing policy.\n",
        "pdf_link": "http://arxiv.org/pdf/1305.6249v2"
    },
    {
        "title": "Cultivating Software Performance in Cloud Computing",
        "authors": [
            "Li Chen",
            "Colin Cunningham",
            "Pooja Jain",
            "Chenggang Qin",
            "Kingsum Chow"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  There exist multitudes of cloud performance metrics, including workload\nperformance, application placement, software/hardware optimization,\nscalability, capacity, reliability, agility and so on. In this paper, we\nconsider jointly optimizing the performance of the software applications in the\ncloud. The challenges lie in bringing a diversity of raw data into tidy data\nformat, unifying performance data from multiple systems based on timestamps,\nand assessing the quality of the processed performance data. Even after\nverifying the quality of cloud performance data, additional challenges block\noptimizing cloud computing. In this paper, we identify the challenges of cloud\ncomputing from the perspectives of computing environment, data collection,\nperformance analytics and production environment.\n",
        "pdf_link": "http://arxiv.org/pdf/1611.07556v1"
    },
    {
        "title": "Evaluating Impact of Human Errors on the Availability of Data Storage\n  Systems",
        "authors": [
            "Mostafa Kishani",
            "Reza Eftekhari",
            "Hossein Asadi"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In this paper, we investigate the effect of incorrect disk replacement\nservice on the availability of data storage systems. To this end, we first\nconduct Monte Carlo simulations to evaluate the availability of disk subsystem\nby considering disk failures and incorrect disk replacement service. We also\npropose a Markov model that corroborates the Monte Carlo simulation results. We\nfurther extend the proposed model to consider the effect of automatic disk\nfail-over policy. The results obtained by the proposed model show that\noverlooking the impact of incorrect disk replacement can result up to three\norders of magnitude unavailability underestimation. Moreover, this study\nsuggests that by considering the effect of human errors, the conventional\nbelieves about the dependability of different RAID mechanisms should be\nrevised. The results show that in the presence of human errors, RAID1 can\nresult in lower availability compared to RAID5.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.01360v1"
    },
    {
        "title": "Forest Packing: Fast, Parallel Decision Forests",
        "authors": [
            "James Browne",
            "Tyler M. Tomita",
            "Disa Mhembere",
            "Randal Burns",
            "Joshua T. Vogelstein"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Machine learning has an emerging critical role in high-performance computing\nto modulate simulations, extract knowledge from massive data, and replace\nnumerical models with efficient approximations. Decision forests are a critical\ntool because they provide insight into model operation that is critical to\ninterpreting learned results. While decision forests are trivially\nparallelizable, the traversals of tree data structures incur many random memory\naccesses and are very slow. We present memory packing techniques that\nreorganize learned forests to minimize cache misses during classification. The\nresulting layout is hierarchical. At low levels, we pack the nodes of multiple\ntrees into contiguous memory blocks so that each memory access fetches data for\nmultiple trees. At higher levels, we use leaf cardinality to identify the most\npopular paths through a tree and collocate those paths in cache lines. We\nextend this layout with out-of-order execution and cache-line prefetching to\nincrease memory throughput. Together, these optimizations increase the\nperformance of classification in ensembles by a factor of four over an\noptimized C++ implementation and a actor of 50 over a popular R language\nimplementation.\n",
        "pdf_link": "http://arxiv.org/pdf/1806.07300v1"
    },
    {
        "title": "Scylla: A Mesos Framework for Container Based MPI Jobs",
        "authors": [
            "Pankaj Saha",
            "Angel Beltre",
            "Madhusudhan Govindaraju"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Open source cloud technologies provide a wide range of support for creating\ncustomized compute node clusters to schedule tasks and managing resources. In\ncloud infrastructures such as Jetstream and Chameleon, which are used for\nscientific research, users receive complete control of the Virtual Machines\n(VM) that are allocated to them. Importantly, users get root access to the VMs.\nThis provides an opportunity for HPC users to experiment with new resource\nmanagement technologies such as Apache Mesos that have proven scalability,\nflexibility, and fault tolerance. To ease the development and deployment of HPC\ntools on the cloud, the containerization technology has matured and is gaining\ninterest in the scientific community. In particular, several well known\nscientific code bases now have publicly available Docker containers. While\nMesos provides support for Docker containers to execute individually, it does\nnot provide support for container inter-communication or orchestration of the\ncontainers for a parallel or distributed application. In this paper, we present\nthe design, implementation, and performance analysis of a Mesos framework,\nScylla, which integrates Mesos with Docker Swarm to enable orchestration of MPI\njobs on a cluster of VMs acquired from the Chameleon cloud [1]. Scylla uses\nDocker Swarm for communication between containerized tasks (MPI processes) and\nApache Mesos for resource pooling and allocation. Scylla allows a policy-driven\napproach to determine how the containers should be distributed across the nodes\ndepending on the CPU, memory, and network throughput requirement for each\napplication.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08386v1"
    },
    {
        "title": "Tromino: Demand and DRF Aware Multi-Tenant Queue Manager for Apache\n  Mesos Cluster",
        "authors": [
            "Pankaj Saha",
            "Angel Beltre",
            "Madhusudhan Govindaraju"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Apache Mesos, a two-level resource scheduler, provides resource sharing\nacross multiple users in a multi-tenant cluster environment. Computational\nresources (i.e., CPU, memory, disk, etc. ) are distributed according to the\nDominant Resource Fairness (DRF) policy. Mesos frameworks (users) receive\nresources based on their current usage and are responsible for scheduling their\ntasks within the allocation. We have observed that multiple frameworks can\ncause fairness imbalance in a multiuser environment. For example, a greedy\nframework consuming more than its fair share of resources can deny resource\nfairness to others. The user with the least Dominant Share is considered first\nby the DRF module to get its resource allocation. However, the default DRF\nimplementation, in Apache Mesos' Master allocation module, does not consider\nthe overall resource demands of the tasks in the queue for each user/framework.\nThis lack of awareness can result in users without any pending task receiving\nmore resource offers while users with a queue of pending tasks starve due to\ntheir high dominant shares. We have developed a policy-driven queue manager,\nTromino, for an Apache Mesos cluster where tasks for individual frameworks can\nbe scheduled based on each framework's overall resource demands and current\nresource consumption. Dominant Share and demand awareness of Tromino and\nscheduling based on these attributes can reduce (1) the impact of unfairness\ndue to a framework specific configuration, and (2) unfair waiting time due to\nhigher resource demand in a pending task queue. In the best case, Tromino can\nsignificantly reduce the average waiting time of a framework by using the\nproposed Demand-DRF aware policy.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08387v1"
    },
    {
        "title": "Exploring the Fairness and Resource Distribution in an Apache Mesos\n  Environment",
        "authors": [
            "Pankaj Saha",
            "Angel Beltre",
            "Madhusudhan Govindaraju"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Apache Mesos, a cluster-wide resource manager, is widely deployed in massive\nscale at several Clouds and Data Centers. Mesos aims to provide high cluster\nutilization via fine grained resource co-scheduling and resource fairness among\nmultiple users through Dominant Resource Fairness (DRF) based allocation. DRF\ntakes into account different resource types (CPU, Memory, Disk I/O) requested\nby each application and determines the share of each cluster resource that\ncould be allocated to the applications. Mesos has adopted a two-level\nscheduling policy: (1) DRF to allocate resources to competing frameworks and\n(2) task level scheduling by each framework for the resources allocated during\nthe previous step. We have conducted experiments in a local Mesos cluster when\nused with frameworks such as Apache Aurora, Marathon, and our own framework\nScylla, to study resource fairness and cluster utilization. Experimental\nresults show how informed decision regarding second level scheduling policy of\nframeworks and attributes like offer holding period, offer refusal cycle and\ntask arrival rate can reduce unfair resource distribution. Bin-Packing\nscheduling policy on Scylla with Marathon can reduce unfair allocation from\n38\\% to 3\\%. By reducing unused free resources in offers we bring down the\nunfairness from to 90\\% to 28\\%. We also show the effect of task arrival rate\nto reduce the unfairness from 23\\% to 7\\%.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08388v1"
    },
    {
        "title": "Evaluation of Docker Containers for Scientific Workloads in the Cloud",
        "authors": [
            "Pankaj Saha",
            "Angel Beltre",
            "Piotr Uminski",
            "Madhusudhan Govindaraju"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The HPC community is actively researching and evaluating tools to support\nexecution of scientific applications in cloud-based environments. Among the\nvarious technologies, containers have recently gained importance as they have\nsignificantly better performance compared to full-scale virtualization, support\nfor microservices and DevOps, and work seamlessly with workflow and\norchestration tools. Docker is currently the leader in containerization\ntechnology because it offers low overhead, flexibility, portability of\napplications, and reproducibility. Singularity is another container solution\nthat is of interest as it is designed specifically for scientific applications.\nIt is important to conduct performance and feature analysis of the container\ntechnologies to understand their applicability for each application and target\nexecution environment. This paper presents a (1) performance evaluation of\nDocker and Singularity on bare metal nodes in the Chameleon cloud (2) mechanism\nby which Docker containers can be mapped with InfiniBand hardware with RDMA\ncommunication and (3) analysis of mapping elements of parallel workloads to the\ncontainers for optimal resource management with container-ready orchestration\ntools. Our experiments are targeted toward application developers so that they\ncan make informed decisions on choosing the container technologies and\napproaches that are suitable for their HPC workloads on cloud infrastructure.\nOur performance analysis shows that scientific workloads for both Docker and\nSingularity based containers can achieve near-native performance. Singularity\nis designed specifically for HPC workloads. However, Docker still has\nadvantages over Singularity for use in clouds as it provides overlay networking\nand an intuitive way to run MPI applications with one container per rank for\nfine-grained resources allocation.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08415v1"
    },
    {
        "title": "Function-as-a-Service Benchmarking Framework",
        "authors": [
            "Roland Pellegrini",
            "Igor Ivkic",
            "Markus Tauber"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Cloud Service Providers deliver their products in form of 'as-a-Service',\nwhich are typically categorized by the level of abstraction. This approach\nhides the implementation details and shows only functionality to the user.\nHowever, the problem is that it is hard to measure the performance of Cloud\nservices, because they behave like black boxes. Especially with\nFunction-as-a-Service it is even more difficult because it completely hides\nserver and infrastructure management from users by design. Cloud Service\nProdivers usually restrict the maximum size of code, memory and runtime of\nCloud Functions. Nevertheless, users need clarification if more ressources are\nneeded to deliver services in high quality. In this regard, we present the\narchitectural design of a new Function-as-a-Service benchmarking tool, which\nallows users to evaluate the performance of Cloud Functions. Furthermore, the\ncapabilities of the framework are tested on an isolated platform with a\nspecific workload. The results show that users are able to get insights into\nFunction-as-a-Service environments. This, in turn, allows users to identify\nfactors which may slow down or speed up the performance of Cloud Functions.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.11707v1"
    },
    {
        "title": "The Supermarket Model with Known and Predicted Service Times",
        "authors": [
            "Michael Mitzenmacher",
            "Matteo Dell'Amico"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The supermarket model refers to a system with a large number of queues, where\nnew customers choose d queues at random and join the one with the fewest\ncustomers. This model demonstrates the power of even small amounts of choice,\nas compared to simply joining a queue chosen uniformly at random, for load\nbalancing systems. In this work we perform simulation-based studies to consider\nvariations where service times for a customer are predicted, as might be done\nin modern settings using machine learning techniques or related mechanisms. Our\nprimary takeaway is that using even seemingly weak predictions of service times\ncan yield significant benefits over blind First In First Out queueing in this\ncontext. However, some care must be taken when using predicted service time\ninformation to both choose a queue and order elements for service within a\nqueue; while in many cases using the information for both choosing and ordering\nis beneficial, in many of our simulation settings we find that simply using the\nnumber of jobs to choose a queue is better when using predicted service times\nto order jobs in a queue. In our simulations, we evaluate both synthetic and\nreal-world workloads--in the latter, service times are predicted by machine\nlearning. Our results provide practical guidance for the design of real-world\nsystems; moreover, we leave many natural theoretical open questions for future\nwork, validating their relevance to real-world situations.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.12155v4"
    },
    {
        "title": "Hardware Versus Software Fault Injection of Modern Undervolted SRAMs",
        "authors": [
            "Muhammet Abdullah Soyturk",
            "Konstantinos Parasyris",
            "Behzad Salami",
            "Osman Unsal",
            "Gulay Yalcin",
            "Leonardo Bautista Gomez"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  To improve power efficiency, researchers are experimenting with dynamically\nadjusting the supply voltage of systems below the nominal operating points.\nHowever, production systems are typically not allowed to function on voltage\nsettings that is below the reliable limit. Consequently, existing software\nfault tolerance studies are based on fault models, which inject faults on\nrandom fault locations using fault injection techniques. In this work we study\nwhether random fault injection is accurate to simulate the behavior of\nundervolted SRAMs.\n  Our study extends the Gem5 simulator to support fault injection on the caches\nof the simulated system. The fault injection framework uses fault maps, which\ndescribe the faulty bits of SRAMs, as inputs. To compare random fault injection\nand hardware guided fault injection, we use two types of fault maps. The first\ntype of maps are created through undervolting real SRAMs and observing the\nlocation of the erroneous bits, whereas the second type of maps are created by\ncorrupting random bits of the SRAMs. During our study we corrupt the L1-Dcache\nof the simulated system and we monitor the behavior of the two types of fault\nmaps on the resiliency of six benchmarks. The difference among the resiliency\nof a benchmark when tested with the different fault maps can be up to 24%.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.00154v1"
    },
    {
        "title": "BenchCouncil's View on Benchmarking AI and Other Emerging Workloads",
        "authors": [
            "Jianfeng Zhan",
            "Lei Wang",
            "Wanling Gao",
            "Rui Ren"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This paper outlines BenchCouncil's view on the challenges, rules, and vision\nof benchmarking modern workloads like Big Data, AI or machine learning, and\nInternet Services. We conclude the challenges of benchmarking modern workloads\nas FIDSS (Fragmented, Isolated, Dynamic, Service-based, and Stochastic), and\npropose the PRDAERS benchmarking rules that the benchmarks should be specified\nin a paper-and-pencil manner, relevant, diverse, containing different levels of\nabstractions, specifying the evaluation metrics and methodology, repeatable,\nand scaleable. We believe proposing simple but elegant abstractions that help\nachieve both efficiency and general-purpose is the final target of benchmarking\nin future, which may be not pressing. In the light of this vision, we shortly\ndiscuss BenchCouncil's related projects.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.00572v2"
    },
    {
        "title": "General Matrix-Matrix Multiplication Using SIMD features of the PIII",
        "authors": [
            "Douglas Aberdeen",
            "Jonathan Baxter"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Generalised matrix-matrix multiplication forms the kernel of many\nmathematical algorithms. A faster matrix-matrix multiply immediately benefits\nthese algorithms. In this paper we implement efficient matrix multiplication\nfor large matrices using the floating point Intel Pentium SIMD (Single\nInstruction Multiple Data) architecture. A description of the issues and our\nsolution is presented, paying attention to all levels of the memory hierarchy.\nOur results demonstrate an average performance of 2.09 times faster than the\nleading public domain matrix-matrix multiply routines.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.04379v1"
    },
    {
        "title": "Queueing Analysis of GPU-Based Inference Servers with Dynamic Batching:\n  A Closed-Form Characterization",
        "authors": [
            "Yoshiaki Inoue"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  GPU-accelerated computing is a key technology to realize high-speed inference\nservers using deep neural networks (DNNs). An important characteristic of\nGPU-based inference is that the computational efficiency, in terms of the\nprocessing speed and energy consumption, drastically increases by processing\nmultiple jobs together in a batch. In this paper, we formulate GPU-based\ninference servers as a batch service queueing model with batch-size dependent\nprocessing times. We first show that the energy efficiency of the server\nmonotonically increases with the arrival rate of inference jobs, which suggests\nthat it is energy-efficient to operate the inference server under a utilization\nlevel as high as possible within a latency requirement of inference jobs. We\nthen derive a closed-form upper bound for the mean latency, which provides a\nsimple characterization of the latency performance. Through simulation and\nnumerical experiments, we show that the exact value of the mean latency is well\napproximated by this upper bound. We further compare this upper bound with the\nlatency curve measured in real implementation of GPU-based inference servers\nand we show that the real performance curve is well explained by the derived\nsimple formula.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.06322v3"
    },
    {
        "title": "Is Big Data Performance Reproducible in Modern Cloud Networks?",
        "authors": [
            "Alexandru Uta",
            "Alexandru Custura",
            "Dmitry Duplyakin",
            "Ivo Jimenez",
            "Jan Rellermeyer",
            "Carlos Maltzahn",
            "Robert Ricci",
            "Alexandru Iosup"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Performance variability has been acknowledged as a problem for over a decade\nby cloud practitioners and performance engineers. Yet, our survey of top\nsystems conferences reveals that the research community regularly disregards\nvariability when running experiments in the cloud. Focusing on networks, we\nassess the impact of variability on cloud-based big-data workloads by gathering\ntraces from mainstream commercial clouds and private research clouds. Our data\ncollection consists of millions of datapoints gathered while transferring over\n9 petabytes of data. We characterize the network variability present in our\ndata and show that, even though commercial cloud providers implement mechanisms\nfor quality-of-service enforcement, variability still occurs, and is even\nexacerbated by such mechanisms and service provider policies. We show how\nbig-data workloads suffer from significant slowdowns and lack predictability\nand replicability, even when state-of-the-art experimentation techniques are\nused. We provide guidelines for practitioners to reduce the volatility of big\ndata performance, making experiments more repeatable.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09256v1"
    },
    {
        "title": "QoS-aware energy-efficient workload routing and server speed control\n  policy in data centers: a robust queueing theoretic approach",
        "authors": [
            "Seung Min Baik",
            "Young Myoung Ko"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Operating cloud service infrastructures requires high energy efficiency while\nensuring a satisfactory service level. Motivated by data centers, we consider a\nworkload routing and server speed control policy applicable to the system\noperating under fluctuating demands. Dynamic control algorithms are generally\nmore energy-efficient than static ones. However, they often require frequent\ninformation exchanges between routers and servers, making the data centers'\nmanagement hesitate to deploy these algorithms. This study presents a static\nrouting and server speed control policy that could achieve energy efficiency\nsimilar to a dynamic algorithm and eliminate the necessity of frequent\ncommunication among resources. We take a robust queueing theoretic approach to\nresponse time constraints for the quality of service (QoS) conditions. Each\nserver is modeled as a G/G/1 processor sharing queue, and the concept of\nuncertainty sets defines the domain of stochastic primitives. We derive an\napproximative upper bound of sojourn times from uncertainty sets and develop an\napproximative sojourn time quantile estimation method for QoS. Numerical\nexperiments confirm the proposed static policy offers competitive solutions\ncompared with the dynamic algorithm.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09870v2"
    },
    {
        "title": "The Two-Pass Softmax Algorithm",
        "authors": [
            "Marat Dukhan",
            "Artsiom Ablavatski"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The softmax (also called softargmax) function is widely used in machine\nlearning models to normalize real-valued scores into a probability\ndistribution. To avoid floating-point overflow, the softmax function is\nconventionally implemented in three passes: the first pass to compute the\nnormalization constant, and two other passes to compute outputs from normalized\ninputs. We analyze two variants of the Three-Pass algorithm and demonstrate\nthat in a well-optimized implementation on HPC-class processors performance of\nall three passes is limited by memory bandwidth. We then present a novel\nalgorithm for softmax computation in just two passes. The proposed Two-Pass\nalgorithm avoids both numerical overflow and the extra normalization pass by\nemploying an exotic representation for intermediate values, where each value is\nrepresented as a pair of floating-point numbers: one representing the\n\"mantissa\" and another representing the \"exponent\". Performance evaluation\ndemonstrates that on out-of-cache inputs on an Intel Skylake-X processor the\nnew Two-Pass algorithm outperforms the traditional Three-Pass algorithm by up\nto 28% in AVX512 implementation, and by up to 18% in AVX2 implementation. The\nproposed Two-Pass algorithm also outperforms the traditional Three-Pass\nalgorithm on Intel Broadwell and AMD Zen 2 processors. To foster\nreproducibility, we released an open-source implementation of the new Two-Pass\nSoftmax algorithm and other experiments in this paper as a part of XNNPACK\nlibrary at GitHub.com/google/XNNPACK.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.04438v1"
    },
    {
        "title": "Duet Benchmarking: Improving Measurement Accuracy in the Cloud",
        "authors": [
            "Lubomír Bulej",
            "Vojtěch Horký",
            "Petr Tůma",
            "François Farquet",
            "Aleksandar Prokopec"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We investigate the duet measurement procedure, which helps improve the\naccuracy of performance comparison experiments conducted on shared machines by\nexecuting the measured artifacts in parallel and evaluating their relative\nperformance together, rather than individually. Specifically, we analyze the\nbehavior of the procedure in multiple cloud environments and use experimental\nevidence to answer multiple research questions concerning the assumption\nunderlying the procedure. We demonstrate improvements in accuracy ranging from\n2.3x to 12.5x (5.03x on average) for the tested ScalaBench (and DaCapo)\nworkloads, and from 23.8x to 82.4x (37.4x on average) for the SPEC CPU 2017\nworkloads.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.05811v2"
    },
    {
        "title": "AIBench Scenario: Scenario-distilling AI Benchmarking",
        "authors": [
            "Wanling Gao",
            "Fei Tang",
            "Jianfeng Zhan",
            "Xu Wen",
            "Lei Wang",
            "Zheng Cao",
            "Chuanxin Lan",
            "Chunjie Luo",
            "Xiaoli Liu",
            "Zihan Jiang"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Modern real-world application scenarios like Internet services consist of a\ndiversity of AI and non-AI modules with huge code sizes and long and\ncomplicated execution paths, which raises serious benchmarking or evaluating\nchallenges. Using AI components or micro benchmarks alone can lead to\nerror-prone conclusions. This paper presents a methodology to attack the above\nchallenge. We formalize a real-world application scenario as a Directed Acyclic\nGraph-based model and propose the rules to distill it into a permutation of\nessential AI and non-AI tasks, which we call a scenario benchmark. Together\nwith seventeen industry partners, we extract nine typical scenario benchmarks.\nWe design and implement an extensible, configurable, and flexible benchmark\nframework. We implement two Internet service AI scenario benchmarks based on\nthe framework as proxies to two real-world application scenarios. We consider\nscenario, component, and micro benchmarks as three indispensable parts for\nevaluating. Our evaluation shows the advantage of our methodology against using\ncomponent or micro AI benchmarks alone. The specifications, source code,\ntestbed, and results are publicly available from\n\\url{https://www.benchcouncil.org/aibench/scenario/}.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.03459v4"
    },
    {
        "title": "Demonstrating 100 Gbps in and out of the public Clouds",
        "authors": [
            "Igor Sfiligoi"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  There is increased awareness and recognition that public Cloud providers do\nprovide capabilities not found elsewhere, with elasticity being a major driver.\nThe value of elastic scaling is however tightly coupled to the capabilities of\nthe networks that connect all involved resources, both in the public Clouds and\nat the various research institutions. This paper presents results of\nmeasurements involving file transfers inside public Cloud providers, fetching\ndata from on-prem resources into public Cloud instances and fetching data from\npublic Cloud storage into on-prem nodes. The networking of the three major\nCloud providers, namely Amazon Web Services, Microsoft Azure and the Google\nCloud Platform, has been benchmarked. The on-prem nodes were managed by either\nthe Pacific Research Platform or located at the University of Wisconsin -\nMadison. The observed sustained throughput was of the order of 100 Gbps in all\nthe tests moving data in and out of the public Clouds and throughput reaching\ninto the Tbps range for data movements inside the public Cloud providers\nthemselves. All the tests used HTTP as the transfer protocol.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05836v1"
    },
    {
        "title": "Threshold-based rerouting and replication for resolving job-server\n  affinity relations",
        "authors": [
            "Youri Raaijmakers",
            "Sem Borst",
            "Onno Boxma"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We consider a system with several job types and two parallel server pools.\nWithin the pools the servers are homogeneous, but across pools possibly not in\nthe sense that the service speed of a job may depend on its type as well as the\nserver pool. Immediately upon arrival, jobs are assigned to a server pool. This\ncould be based on (partial) knowledge of their type, but such knowledge might\nnot be available. Information about the job type can however be obtained while\nthe job is in service; as the service progresses, the likelihood that the\nservice speed of this job type is low increases, creating an incentive to\nexecute the job on different, possibly faster, server(s). Two policies are\nconsidered: reroute the job to the other server pool, or replicate it there.\n  We determine the effective load per server under both the rerouting and\nreplication policy for completely unknown as well as partly known job types. We\nalso examine the impact of these policies on the stability bound, and find that\nthe uncertainty in job types may significantly degrade the performance. For\n(highly) unbalanced service speeds full replication achieves the largest\nstability bound while for (nearly) balanced service speeds no replication\nmaximizes the stability bound. Finally, we discuss how the use of\nthreshold-based policies can help improve the expected latency for completely\nor partly unknown job types.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.13353v1"
    },
    {
        "title": "Toward Transparent Heterogeneous Systems",
        "authors": [
            "Baptiste Delporte",
            "Roberto Rigamonti",
            "Alberto Dassatti"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Heterogeneous parallel systems are widely spread nowadays. Despite their\navailability, their usage and adoption are still limited, and even more rarely\nthey are used to full power. Indeed, compelling new technologies are constantly\ndeveloped and keep changing the technological landscape, but each of them\ntargets a limited sub-set of supported devices, and nearly all of them require\nnew programming paradigms and specific toolsets. Software, however, can hardly\nkeep the pace with the growing number of computational capabilities, and\ndevelopers are less and less motivated in learning skills that could quickly\nbecome obsolete. In this paper we present our effort in the direction of a\ntransparent system optimization based on automatic code profiling and\nJust-In-Time compilation, that resulted in a fully-working embedded prototype\ncapable of dynamically detect computing-intensive code blocks and automatically\ndispatch them to different computation units. Experimental results show that\nour system allows gains up to 32x in performance --- after an initial warm-up\nphase --- without requiring any human intervention.\n",
        "pdf_link": "http://arxiv.org/pdf/1511.05771v2"
    },
    {
        "title": "Measuring and comparing the scaling behaviour of a high-performance CFD\n  code on different supercomputing infrastructures",
        "authors": [
            "Jérôme Frisch",
            "Ralf-Peter Mundani"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Parallel code design is a challenging task especially when addressing\npetascale systems for massive parallel processing (MPP), i.e. parallel\ncomputations on several hundreds of thousands of cores. An in-house\ncomputational fluid dynamics code, developed by our group, was designed for\nsuch high-fidelity runs in order to exhibit excellent scalability values. Basis\nfor this code is an adaptive hierarchical data structure together with an\nefficient communication and (numerical) computation scheme that supports MPP.\nFor a detailled scalability analysis, we performed several experiments on two\nof Germany's national supercomputers up to 140,000 processes. In this paper, we\nwill show the results of those experiments and discuss any bottlenecks that\ncould be observed while solving engineering-based problems such as porous media\nflows or thermal comfort assessments for problem sizes up to several hundred\nbillion degrees of freedom.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.00146v1"
    },
    {
        "title": "Design and optimisation of an efficient HDF5 I/O kernel for massive\n  parallel fluid flow simulations",
        "authors": [
            "Christoph Ertl",
            "Jérôme Frisch",
            "Ralf-Peter Mundani"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  More and more massive parallel codes running on several hundreds of thousands\nof cores enter the computational science and engineering domain, allowing\nhigh-fidelity computations on up to trillions of unknowns for very detailed\nanalyses of the underlying problems. During such runs, typically gigabytes of\ndata are being produced, hindering both efficient storage and (interactive)\ndata exploration. Here, advanced approaches based on inherently distributed\ndata formats such as HDF5 become necessary in order to avoid long latencies\nwhen storing the data and to support fast (random) access when retrieving the\ndata for visual processing. Avoiding file locking and using collective\nbuffering, write bandwidths to a single file close to the theoretical peak on a\nmodern supercomputing cluster were achieved. The structure of the output file\nsupports a very fast interactive visualisation and introduces additional\nsteering functionality.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.06534v1"
    },
    {
        "title": "The distribution of age-of-information performance measures for message\n  processing systems",
        "authors": [
            "George Kesidis",
            "Takis Konstantopoulos",
            "Michael Zazanis"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The idea behind the recently introduced \"age of information\" performance\nmeasure of a networked message processing system is that it indicates our\nknowledge regarding the \"freshness\" of the most recent piece of information\nthat can be used as a criterion for real-time control. In this foundational\npaper, we examine two such measures, one that has been extensively studied in\nthe recent literature and a new one that could be more relevant from the point\nof view of the processor. Considering these measures as stochastic processes in\na stationary environment (defined by the arrival processes, message processing\ntimes and admission controls in bufferless systems), we characterize their\ndistributions using the Palm inversion formula. Under renewal assumptions we\nderive explicit solutions for their Laplace transforms and show some\ninteresting decomposition properties. Previous work has mostly focused on\ncomputation of expectations in very particular cases. We argue that using\nbufferless or very small buffer systems is best and support this by simulation.\nWe also pose some open problems including assessment of enqueueing policies\nthat may be better in cases where one wishes to minimize more general\nfunctionals of the age of information measures.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.05924v2"
    },
    {
        "title": "Dynamic scheduling in a partially fluid, partially lossy queueing system",
        "authors": [
            "Kiran Chaudhary",
            "Veeraruna Kavitha",
            "Jayakrishnan Nair"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We consider a single server queueing system with two classes of jobs: eager\njobs with small sizes that require service to begin almost immediately upon\narrival, and tolerant jobs with larger sizes that can wait for service. While\nblocking probability is the relevant performance metric for the eager class,\nthe tolerant class seeks to minimize its mean sojourn time. In this paper, we\ndiscuss the performance of each class under dynamic scheduling policies, where\nthe scheduling of both classes depends on the instantaneous state of the\nsystem. This analysis is carried out under a certain fluid limit, where the\narrival rate and service rate of the eager class are scaled to infinity,\nholding the offered load constant. Our performance characterizations reveal a\n(dynamic) pseudo-conservation law that ties the performance of both the classes\nto the standalone blocking probabilities of the eager class. Further, the\nperformance is robust to other specifics of the scheduling policies. We also\ncharacterize the Pareto frontier of the achievable region of performance\nvectors under the same fluid limit, and identify a (two-parameter) class of\nPareto-complete scheduling policies.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.06480v3"
    },
    {
        "title": "A mechanism for balancing accuracy and scope in cross-machine black-box\n  GPU performance modeling",
        "authors": [
            "James D. Stevens",
            "Andreas Klöckner"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The ability to model, analyze, and predict execution time of computations is\nan important building block supporting numerous efforts, such as load\nbalancing, performance optimization, and automated performance tuning for high\nperformance, parallel applications. In today's increasingly heterogeneous\ncomputing environment, this task must be accomplished efficiently across\nmultiple architectures, including massively parallel coprocessors like GPUs. To\naddress this challenge, we present an approach for constructing customizable,\ncross-machine performance models for GPU kernels, including a mechanism to\nautomatically and symbolically gather performance-relevant kernel operation\ncounts, a tool for formulating mathematical models using these counts, and a\ncustomizable parameterized collection of benchmark kernels used to calibrate\nmodels to GPUs in a black-box fashion. Our approach empowers a user to manage\ntrade-offs between model accuracy, evaluation speed, and generalizability. A\nuser can define a model and customize the calibration process, making it as\nsimple or complex as desired, and as application-targeted or general as\ndesired. To evaluate our approach, we demonstrate both linear and nonlinear\nmodels; each example models execution times for multiple variants of a\nparticular computation: two matrix multiplication variants, four Discontinuous\nGalerkin (DG) differentiation operation variants, and two 2-D five-point finite\ndifference stencil variants. For each variant, we present accuracy results on\nGPUs from multiple vendors and hardware generations. We view this customizable\napproach as a response to a central question in GPU performance modeling: how\ncan we model GPU performance in a cost-explanatory fashion while maintaining\naccuracy, evaluation speed, portability, and ease of use, an attribute we\nbelieve precludes manual collection of kernel or hardware statistics.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.09538v3"
    },
    {
        "title": "Edge AIBench: Towards Comprehensive End-to-end Edge Computing\n  Benchmarking",
        "authors": [
            "Tianshu Hao",
            "Yunyou Huang",
            "Xu Wen",
            "Wanling Gao",
            "Fan Zhang",
            "Chen Zheng",
            "Lei Wang",
            "Hainan Ye",
            "Kai Hwang",
            "Zujie Ren",
            "Jianfeng Zhan"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In edge computing scenarios, the distribution of data and collaboration of\nworkloads on different layers are serious concerns for performance, privacy,\nand security issues. So for edge computing benchmarking, we must take an\nend-to-end view, considering all three layers: client-side devices, edge\ncomputing layer, and cloud servers. Unfortunately, the previous work ignores\nthis most important point. This paper presents the BenchCouncil's coordinated e\nort on edge AI benchmarks, named Edge AIBench. In total, Edge AIBench models\nfour typical application scenarios: ICU Patient Monitor, Surveillance Camera,\nSmart Home, and Autonomous Vehicle with the focus on data distribution and\nworkload collaboration on three layers. Edge AIBench is a part of the\nopen-source AIBench project, publicly available from\nhttp://www.benchcouncil.org/AIBench/index.html. We also build an edge computing\ntestbed with a federated learning framework to resolve performance, privacy,\nand security issues.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.01924v1"
    },
    {
        "title": "Performance Comparison for Neuroscience Application Benchmarks",
        "authors": [
            "Andreas Herten",
            "Thorsten Hater",
            "Wouter Klijn",
            "Dirk Pleiter"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Researchers within the Human Brain Project and related projects have in the\nlast couple of years expanded their needs for high-performance computing\ninfrastructures. The needs arise from a diverse set of science challenges that\nrange from large-scale simulations of brain models to processing of\nextreme-scale experimental data sets. The ICEI project, which is in the process\nof creating a distributed infrastructure optimised for brain research, started\nto build-up a set of benchmarks that reflect the diversity of applications in\nthis field. In this paper we analyse the performance of some selected\nbenchmarks on an IBM POWER8 and Intel Skylake based systems with and without\nGPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02702v1"
    },
    {
        "title": "TapirXLA: Embedding Fork-Join Parallelism into the XLA Compiler in\n  TensorFlow Using Tapir",
        "authors": [
            "Tao B. Schardl",
            "Siddharth Samsi"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This work introduces TapirXLA, a replacement for TensorFlow's XLA compiler\nthat embeds recursive fork-join parallelism into XLA's low-level representation\nof code. Machine-learning applications rely on efficient parallel processing to\nachieve performance, and they employ a variety of technologies to improve\nperformance, including compiler technology. But compilers in machine-learning\nframeworks lack a deep understanding of parallelism, causing them to lose\nperformance by missing optimizations on parallel computation. This work studies\nhow Tapir, a compiler intermediate representation (IR) that embeds parallelism\ninto a mainstream compiler IR, can be incorporated into a compiler for machine\nlearning to remedy this problem. TapirXLA modifies the XLA compiler in\nTensorFlow to employ the Tapir/LLVM compiler to optimize low-level parallel\ncomputation. TapirXLA encodes the parallelism within high-level TensorFlow\noperations using Tapir's representation of fork-join parallelism. TapirXLA also\nexposes to the compiler implementations of linear-algebra library routines\nwhose parallel operations are encoded using Tapir's representation. We compared\nthe performance of TensorFlow using TapirXLA against TensorFlow using an\nunmodified XLA compiler. On four neural-network benchmarks, TapirXLA speeds up\nthe parallel running time of the network by a geometric-mean multiplicative\nfactor of 30% to 100%, across four CPU architectures.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.11338v1"
    },
    {
        "title": "ModiPick: SLA-aware Accuracy Optimization For Mobile Deep Inference",
        "authors": [
            "Samuel S. Ogden",
            "Tian Guo"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Mobile applications are increasingly leveraging complex deep learning models\nto deliver features, e.g., image recognition, that require high prediction\naccuracy. Such models can be both computation and memory-intensive, even for\nnewer mobile devices, and are therefore commonly hosted in powerful remote\nservers. However, current cloud-based inference services employ static model\nselection approach that can be suboptimal for satisfying application SLAs\n(service level agreements), as they fail to account for inherent dynamic mobile\nenvironment.\n  We introduce a cloud-based technique called ModiPick that dynamically selects\nthe most appropriate model for each inference request, and adapts its selection\nto match different SLAs and execution time budgets that are caused by variable\nmobile environments. The key idea of ModiPick is to make inference speed and\naccuracy trade-offs at runtime with a pool of managed deep learning models. As\nsuch, ModiPick masks unpredictable inference time budgets and therefore meets\nSLA targets, while improving accuracy within mobile network constraints. We\nevaluate ModiPick through experiments based on prototype systems and through\nsimulations. We show that ModiPick achieves comparable inference accuracy to a\ngreedy approach while improving SLA adherence by up to 88.5%.\n",
        "pdf_link": "http://arxiv.org/pdf/1909.02053v1"
    },
    {
        "title": "ALERT: Accurate Learning for Energy and Timeliness",
        "authors": [
            "Chengcheng Wan",
            "Muhammad Santriaji",
            "Eri Rogers",
            "Henry Hoffmann",
            "Michael Maire",
            "Shan Lu"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  An increasing number of software applications incorporate runtime Deep Neural\nNetworks (DNNs) to process sensor data and return inference results to humans.\nEffective deployment of DNNs in these interactive scenarios requires meeting\nlatency and accuracy constraints while minimizing energy, a problem exacerbated\nby common system dynamics. Prior approaches handle dynamics through either (1)\nsystem-oblivious DNN adaptation, which adjusts DNN latency/accuracy tradeoffs,\nor (2) application-oblivious system adaptation, which adjusts resources to\nchange latency/energy tradeoffs. In contrast, this paper improves on the\nstate-of-the-art by coordinating application- and system-level adaptation.\nALERT, our runtime scheduler, uses a probabilistic model to detect\nenvironmental volatility and then simultaneously select both a DNN and a system\nresource configuration to meet latency, accuracy, and energy constraints. We\nevaluate ALERT on CPU and GPU platforms for image and speech tasks in dynamic\nenvironments. ALERT's holistic approach achieves more than 13% energy\nreduction, and 27% error reduction over prior approaches that adapt solely at\nthe application or system level. Furthermore, ALERT incurs only 3% more energy\nconsumption and 2% higher DNN-inference error than an oracle scheme with\nperfect application and system knowledge.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.00119v2"
    },
    {
        "title": "The Pitfall of Evaluating Performance on Emerging AI Accelerators",
        "authors": [
            "Zihan Jiang",
            "Jiansong Li",
            "Jiangfeng Zhan"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In recent years, domain-specific hardware has brought significant performance\nimprovements in deep learning (DL). Both industry and academia only focus on\nthroughput when evaluating these AI accelerators, which usually are custom\nASICs deployed in datacenter to speed up the inference phase of DL workloads.\nPursuing higher hardware throughput such as OPS (Operation Per Second) using\nvarious optimizations seems to be their main design target. However, they\nignore the importance of accuracy in the DL nature. Motivated by this, this\npaper argue that a single throughput metric can not comprehensively reflect the\nreal-world performance of AI accelerators. To reveal this pitfall, we evaluates\nseveral frequently-used optimizations on a typical AI accelerator and\nquantifies their impact on accuracy and throughout under representative DL\ninference workloads. Based on our experimental results, we find that some\noptimizations cause significant loss on accuracy in some workloads, although it\ncan improves the throughout. Furthermore, our results show the importance of\nend-to-end evaluation in DL.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.02987v1"
    },
    {
        "title": "Understanding Open Source Serverless Platforms: Design Considerations\n  and Performance",
        "authors": [
            "Junfeng Li",
            "Sameer G. Kulkarni",
            "K. K. Ramakrishnan",
            "Dan Li"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Serverless computing is increasingly popular because of the promise of lower\ncost and the convenience it provides to users who do not need to focus on\nserver management. This has resulted in the availability of a number of\nproprietary and open-source serverless solutions. We seek to understand how the\nperformance of serverless computing depends on a number of design issues using\nseveral popular open-source serverless platforms. We identify the\nidiosyncrasies affecting performance (throughput and latency) for different\nopen-source serverless platforms. Further, we observe that just having either\nresource-based (CPU and memory) or workload-based (request per second (RPS) or\nconcurrent requests) auto-scaling is inadequate to address the needs of the\nserverless platforms.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.07449v4"
    },
    {
        "title": "System Performance with varying L1 Instruction and Data Cache Sizes: An\n  Empirical Analysis",
        "authors": [
            "Ramya Akula",
            "Kartik Jain",
            "Deep Jigar Kotecha"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In this project, we investigate the fluctuations in performance caused by\nchanging the Instruction (I-cache) size and the Data (D-cache) size in the L1\ncache. We employ the Gem5 framework to simulate a system with varying\nspecifications on a single host machine. We utilize the FreqMine benchmark\navailable under the PARSEC suite as the workload program to benchmark our\nsimulated system. The Out-order CPU (O3) with Ruby memory model was simulated\nin a Full-System X86 environment with Linux OS. The chosen metrics deal with\nHit Rate, Misses, Memory Latency, Instruction Rate, and Bus Traffic within the\nsystem. Performance observed by varying L1 size within a certain range of\nvalues was used to compute Confidence Interval based statistics for relevant\nmetrics. Our expectations, corresponding experimental observations, and\ndiscrepancies are also discussed in this report.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.11642v1"
    },
    {
        "title": "GraphZero: Breaking Symmetry for Efficient Graph Mining",
        "authors": [
            "Daniel Mawhirter",
            "Sam Reinehr",
            "Connor Holmes",
            "Tongping Liu",
            "Bo Wu"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Graph mining for structural patterns is a fundamental task in many\napplications. Compilation-based graph mining systems, represented by AutoMine,\ngenerate specialized algorithms for the provided patterns and substantially\noutperform other systems. However, the generated code causes substantial\ncomputation redundancy and the compilation process incurs too much overhead to\nbe used online, both due to the inherent symmetry in the structural patterns.\n  In this paper, we propose an optimizing compiler, GraphZero, to completely\naddress these limitations through symmetry breaking based on group theory.\nGraphZero implements three novel techniques. First, its schedule explorer\nefficiently prunes the schedule space without missing any high-performance\nschedule. Second, it automatically generates and enforces a set of restrictions\nto eliminate computation redundancy. Third, it generalizes orientation, a\nsurprisingly effective optimization that was mainly used for clique patterns,\nto apply to arbitrary patterns. Evaluated on multiple graph mining applications\nand complex patterns with 7 real-world graph datasets, GraphZero demonstrates\nup to 40X performance improvement and up to 197X reduction on schedule\ngeneration overhead over AutoMine.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.12877v1"
    },
    {
        "title": "Efficient method for parallel computation of geodesic transformation on\n  CPU",
        "authors": [
            "Danijel Žlaus",
            "Domen Mongus"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This paper introduces a fast Central Processing Unit (CPU) implementation of\ngeodesic morphological operations using stream processing. In contrast to the\ncurrent state-of-the-art, that focuses on achieving insensitivity to the filter\nsizes with efficient data structures, the proposed approach achieves efficient\ncomputation of long chains of elementary $3 \\times 3$ filters using multicore\nand Single Instruction Multiple Data (SIMD) processing. In comparison to the\nrelated methods, up to $100$ times faster computation of common geodesic\noperators is achieved in this way, allowing for real-time processing (with over\n$30$ FPS) of up to $1500$ filters long chains, applied on $1024\\times 1024$\nimages. In addition, the proposed approach outperformed GPGPU, and proved to be\nmore efficient than the comparable streaming method for the computation of\nmorphological erosions and dilations with window sizes up to $183\\times 183$ in\nthe case of using char and $27\\times27$ when using double data types.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.13074v1"
    },
    {
        "title": "Staffing for many-server systems facing non-standard arrival processes",
        "authors": [
            "M. Heemskerk",
            "M. Mandjes",
            "B. Mathijsen"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Arrival processes to service systems often display (i) larger than\nanticipated fluctuations, (ii) a time-varying rate, and (iii) temporal\ncorrelation. Motivated by this, we introduce a specific non-homogeneous Poisson\nprocess that incorporates these three features. The resulting arrival process\nis fed into an infinite-server system, which is then used as a proxy for its\nmany-server counterpart. This leads to a staffing rule based on the square-root\nstaffing principle that acknowledges the three features. After a slight\nrearrangement of servers over the time slots, we succeed to stabilize system\nperformance even under highly varying and strongly correlated conditions. We\nfit the arrival stream model to real data from an emergency department and\ndemonstrate (by simulation) the performance of the novel staffing rule.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.00515v1"
    },
    {
        "title": "Joint performance analysis of ages of information in a multi-source\n  pushout server",
        "authors": [
            "Yukang Jiang",
            "Naoto Miyoshi"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Age of information (AoI) has been widely accepted as a measure quantifying\nfreshness of status information in real-time status update systems. In many of\nsuch systems, multiple sources share a limited network resource and therefore\nthe AoIs defined for the individual sources should be correlated with each\nother. However, there are not found any results studying the correlation of two\nor more AoIs in a status update system with multiple sources. In this work, we\nconsider a multi-source system sharing a common service facility and provide a\nframework to investigate joint performance of the multiple AoIs. We then apply\nour framework to a simple pushout server with multiple sources and derive a\nclosed-form formula of the joint Laplace transform of the AoIs in the case with\nindependent M/G inputs. We further show some properties of the correlation\ncoefficient of AoIs in the two-source system.\n",
        "pdf_link": "http://arxiv.org/pdf/2006.03404v1"
    },
    {
        "title": "Beyond Scaling: Calculable Error Bounds of the Power-of-Two-Choices\n  Mean-Field Model in Heavy-Traffic",
        "authors": [
            "Fnu Hairi",
            "Xin Liu",
            "Lei Ying"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  This paper provides a recipe for deriving calculable approximation errors of\nmean-field models in heavy-traffic with the focus on the well-known load\nbalancing algorithm -- power-of-two-choices (Po2). The recipe combines Stein's\nmethod for linearized mean-field models and State Space Concentration (SSC)\nbased on geometric tail bounds. In particular, we divide the state space into\ntwo regions, a neighborhood near the mean-field equilibrium and the complement\nof that. We first use a tail bound to show that the steady-state probability\nbeing outside the neighborhood is small. Then, we use a linearized mean-field\nmodel and Stein's method to characterize the generator difference, which\nprovides the dominant term of the approximation error. From the dominant term,\nwe are able to obtain an asymptotically-tight bound and a nonasymptotic upper\nbound, both are calculable bounds, not order-wise scaling results like most\nresults in the literature. Finally, we compared the theoretical bounds with\nnumerical evaluations to show the effectiveness of our results. We note that\nthe simulation results show that both bounds are valid even for small size\nsystems such as a system with only ten servers.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.06613v2"
    },
    {
        "title": "On Non-Markovian Performance Models",
        "authors": [
            "Andras Farago"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We present an approach that can be useful when the network or system\nperformance is described by a model that is not Markovian. Although most\nperformance models are based on Markov chains or Markov processes, in some\ncases the Markov property does not hold. This can occur, for example, when the\nsystem exhibits long range dependencies. For such situations, and other\nnon-Markovian cases, our method may provide useful help.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.07152v1"
    },
    {
        "title": "Minimizing Maintenance Cost Involving Flow-time and Tardiness Penalty\n  with Unequal Release Dates",
        "authors": [
            "K. Adjallah",
            "K. Adzakpa"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  This paper proposes important and useful results relating to the minimization\nof the sum of the flow-time and the tardiness of tasks or jobs with unequal\nrelease dates (occurrence date), with application to maintenance planning and\nscheduling. Firstly, the policy of real-time maintenance is defined for\nminimizing the cost of tardiness and critical states. The required local\noptimality rule (FTR) is proved, in order to minimize the sum or the linear\ncombination of the tasks' flow-time and tardiness costs. This rule has served\nto design a scheduling algorithm, with O(n 3) complexity when it is applied to\nschedule a set of n tasks on one processor. To evaluate its performance, the\nresults are compared to a lower bound that is provided, in a numerical case\nstudy. Using this algorithm in combination with the tasks urgency criterion, a\nreal-time algorithm is developed to schedule the tasks on q parallel\nprocessors. This latter algorithm is finally applied to schedule and assign\npreventive maintenance tasks to processors in the case of a distributed system.\nIts efficiency enables, as shown in the numerical example, to minimize the cost\nof preventive maintenance tasks, expressed as the sum of the tasks tardiness\nand flow-time. This corresponds to costs of critical states and of tardiness of\npreventive maintenance.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08297v1"
    },
    {
        "title": "Optimal Hyper-Scalable Load Balancing with a Strict Queue Limit",
        "authors": [
            "Mark van der Boor",
            "Sem Borst",
            "Johan van Leeuwaarden"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Load balancing plays a critical role in efficiently dispatching jobs in\nparallel-server systems such as cloud networks and data centers. A fundamental\nchallenge in the design of load balancing algorithms is to achieve an optimal\ntrade-off between delay performance and implementation overhead (e.g.\ncommunication or memory usage). This trade-off has primarily been studied so\nfar from the angle of the amount of overhead required to achieve asymptotically\noptimal performance, particularly vanishing delay in large-scale systems. In\ncontrast, in the present paper, we focus on an arbitrarily sparse communication\nbudget, possibly well below the minimum requirement for vanishing delay,\nreferred to as the hyper-scalable operating region. Furthermore, jobs may only\nbe admitted when a specific limit on the queue position of the job can be\nguaranteed.\n  The centerpiece of our analysis is a universal upper bound for the achievable\nthroughput of any dispatcher-driven algorithm for a given communication budget\nand queue limit. We also propose a specific hyper-scalable scheme which can\noperate at any given message rate and enforce any given queue limit, while\nallowing the server states to be captured via a closed product-form network, in\nwhich servers act as customers traversing various nodes. The product-form\ndistribution is leveraged to prove that the bound is tight and that the\nproposed hyper-scalable scheme is throughput-optimal in a many-server regime\ngiven the communication and queue limit constraints. Extensive simulation\nexperiments are conducted to illustrate the results.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.08357v1"
    },
    {
        "title": "Performance Comparison for Scientific Computations on the Edge via\n  Relative Performance",
        "authors": [
            "Aravind Sankaran",
            "Paolo Bientinesi"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  In a typical Internet-of-Things setting that involves scientific\napplications, a target computation can be evaluated in many different ways\ndepending on the split of computations among various devices. On the one hand,\ndifferent implementations (or algorithms)--equivalent from a mathematical\nperspective--might exhibit significant difference in terms of performance. On\nthe other hand, some of the implementations are likely to show similar\nperformance characteristics. In this paper, we focus on analyzing the\nperformance of a given set of algorithms by clustering them into performance\nclasses. To this end, we use a measurement-based approach to evaluate and score\nalgorithms based on pair-wise comparisons; we refer to this approach\nas\"Relative performance analysis\". Each comparison yields one of three\noutcomes: one algorithm can be \"better\", \"worse\", or \"equivalent\" to another;\nthose algorithms evaluating to have equivalent performance are merged into the\nsame performance class. We show that our clustering methodology facilitates\nalgorithm selection with respect to more than one metric; for instance, from\nthe subset of equivalently fast algorithms, one could then select an algorithm\nthat consumes the least energy on a certain device.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.12740v2"
    },
    {
        "title": "Modified Erlang loss system for cognitive wireless networks",
        "authors": [
            "E. V. Morozov",
            "S. S. Rogozin",
            "H. Q. Nguyen",
            "T. Phung-Duc"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  This paper considers a modified Erlang loss system for cognitive wireless\nnetworks and related applications. A primary user has preemptive priority over\nsecondary users and the primary customer is lost if upon arrival all the\nchannels are used by other primary users. Secondary users cognitively use idle\nchannels and they can wait at an infinite buffer in cases idle channels are not\navailable upon arrival or they are interrupted by primary users. We obtain\nexplicit stability condition for the cases where arrival processes of primary\nusers and secondary users follow Poisson processes and their service times\nfollow two distinct arbitrary distributions. The stability condition is\ninsensitive to the service time distributions and implies the maximal\nthroughout of secondary users. For a special case of exponential service time\ndistributions, we analyze in depth to show the effect of parameters on the\ndelay performance and the mean number of interruptions of secondary users. Our\nsimulations for distributions rather than exponential reveal that the mean\nnumber of terminations for secondary users is less sensitive to the service\ntime distribution of primary users.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03222v1"
    },
    {
        "title": "A Survey of Stability Results for Redundancy Systems",
        "authors": [
            "Elene Anton",
            "Urtzi Ayesta",
            "Matthieu Jonckheere",
            "Ina Maria Verloop"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Redundancy mechanisms consist in sending several copies of a same job to a\nsubset of servers. It constitutes one of the most promising ways to exploit\ndiversity in multiservers applications. However, its pros and cons are still\nnot sufficiently understood in the context of realistic models with generic\nstatistical properties of service-times distributions and correlation\nstructures of copies. We aim at giving a survey of recent results concerning\nthe stability-arguably the first benchmark of performance-of systems with\ncancel-oncompletion redundancy. We also point out open questions and\nconjectures.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.10942v1"
    },
    {
        "title": "ReaDmE: Read-Rate Based Dynamic Execution Scheduling for Intermittent\n  RF-Powered Devices",
        "authors": [
            "Yang Su",
            "Damith C. Ranasinghe"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  This paper presents a method for remotely and dynamically determining the\nexecution schedule of long-running tasks on intermittently powered devices such\nas computational RFID. Our objective is to prevent brown-out events caused by\nsudden power-loss due to the intermittent nature of the powering channel. We\nformulate, validate and demonstrate that the read-rate measured from an RFID\nreader (number of successful interrogations per second) can provide an adequate\nmeans of estimating the powering channel condition for passively powered CRFID\ndevices. This method is attractive because it can be implemented without\nimposing an added burden on the device or requiring additional hardware. We\nfurther propose ReaDmE, a dynamic execution scheduling scheme to mitigate\nbrownout events to support long-run execution of complex tasks, such as\ncryptographic algorithms, on CRFID. Experimental results demonstrate that the\nReaDmE method can improve CRFID's long-run execution success rate by 20% at the\ncritical operational range or reduce time overhead by up to 23% compared to\nprevious execution scheduling methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.14404v2"
    },
    {
        "title": "Database management system performance comparisons: A systematic\n  literature review",
        "authors": [
            "Toni Taipalus"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Efficiency has been a pivotal aspect of the software industry since its\ninception, as a system that serves the end-user fast, and the service provider\ncost-efficiently benefits all parties. A database management system (DBMS) is\nan integral part of effectively all software systems, and therefore it is\nlogical that different studies have compared the performance of different DBMSs\nin hopes of finding the most efficient one. This study systematically\nsynthesizes the results and approaches of studies that compare DBMS performance\nand provides recommendations for industry and research. The results show that\nperformance is usually tested in a way that does not reflect real-world use\ncases, and that tests are typically reported in insufficient detail for\nreplication or for drawing conclusions from the stated results.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.01095v3"
    },
    {
        "title": "Reengineering multi tiered enterprise business applications for\n  performance enhancement and reciprocal or rectangular hyperbolic relation of\n  variation of data transportation time with row pre-fetch size of relational\n  database drivers",
        "authors": [
            "Sridhar Sowmiyanarayanan"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Reengineering multi tiered enterprise business applications for performance\nenhancement and reciprocal or rectangular hyperbolic relation of variation of\ndata transportation time with row pre-fetch size of relational database drivers\n",
        "pdf_link": "http://arxiv.org/pdf/1201.4655v1"
    },
    {
        "title": "A Brief Review on Models for Performance Evaluation in DSS Architecture",
        "authors": [
            "Ghassem Tofighi",
            "Kaamran Raahemifar",
            "Anastasios N. Venetsanopoulos"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Distributed Software Systems are used these days by many people in the real\ntime operations and modern enterprise applications. One of the most important\nand essential attributes of measurements for the quality of service of\ndistributed software is performance. Performance models can be employed at\nearly stages of the software development cycle to characterize the quantitative\nbehavior of software systems. In this research, performance models based on\nfuzzy logic approach, queuing network approach and Petri net approach have been\nreviewed briefly. One of the most common ways in performance analysis of\ndistributed software systems is translating the UML diagrams to mathematical\nmodeling languages for the description of distributed systems such as queuing\nnetworks or Petri nets. In this paper, some of these approaches are reviewed\nbriefly. Attributes which are used for performance modeling in the literature\nare mostly machine based. On the other hand, end users and client parameters\nfor performance evaluation are not covered extensively. In this way, future\nresearch could be based on developing hybrid models to capture user decision\nvariables which make system performance evaluation more user driven.\n",
        "pdf_link": "http://arxiv.org/pdf/1401.6020v1"
    },
    {
        "title": "Analysis and Approximation of Dual Tandem Queues with Finite Buffer\n  Capacity",
        "authors": [
            "Kan Wu",
            "Ning Zhao"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Tandem queues with finite buffer capacity commonly exist in practical\napplications. By viewing a tandem queue as an integrated system, an innovative\napproach has been developed to analyze its performance through the insight from\nreduction method. In our approach, the starvation at the bottleneck caused by\nservice time randomness is modeled and captured by interruptions. Fundamental\nproperties of tandem queues with finite buffer capacity are examined. We show\nthat in general system service rate of a dual tandem queue with finite buffer\ncapacity is equal or smaller than its bottleneck service rate, and virtual\ninterruptions, which are the extra idle period at the bottleneck caused by the\nnon-bottlenecks, depend on arrival rates. Hence, system service rate is a\nfunction of arrival rate when the buffer capacity of a tandem queue is finite.\nApproximation for the mean queue time of a dual tandem queue has been developed\nthrough the concept of virtual interruptions.\n",
        "pdf_link": "http://arxiv.org/pdf/1407.3267v2"
    },
    {
        "title": "Efficient HTTP based I/O on very large datasets for high performance\n  computing with the libdavix library",
        "authors": [
            "Adrien Devresse",
            "Fabrizio Furano"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Remote data access for data analysis in high performance computing is\ncommonly done with specialized data access protocols and storage systems. These\nprotocols are highly optimized for high throughput on very large datasets,\nmulti-streams, high availability, low latency and efficient parallel I/O. The\npurpose of this paper is to describe how we have adapted a generic protocol,\nthe Hyper Text Transport Protocol (HTTP) to make it a competitive alternative\nfor high performance I/O and data analysis applications in a global computing\ngrid: the Worldwide LHC Computing Grid. In this work, we first analyze the\ndesign differences between the HTTP protocol and the most common high\nperformance I/O protocols, pointing out the main performance weaknesses of\nHTTP. Then, we describe in detail how we solved these issues. Our solutions\nhave been implemented in a toolkit called davix, available through several\nrecent Linux distributions. Finally, we describe the results of our benchmarks\nwhere we compare the performance of davix against a HPC specific protocol for a\ndata analysis use case.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.4168v1"
    },
    {
        "title": "Quantifying performance bottlenecks of stencil computations using the\n  Execution-Cache-Memory model",
        "authors": [
            "Holger Stengel",
            "Jan Treibig",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Stencil algorithms on regular lattices appear in many fields of computational\nscience, and much effort has been put into optimized implementations. Such\nactivities are usually not guided by performance models that provide estimates\nof expected speedup. Understanding the performance properties and bottlenecks\nby performance modeling enables a clear view on promising optimization\nopportunities. In this work we refine the recently developed\nExecution-Cache-Memory (ECM) model and use it to quantify the performance\nbottlenecks of stencil algorithms on a contemporary Intel processor. This\nincludes applying the model to arrive at single-core performance and\nscalability predictions for typical corner case stencil loop kernels. Guided by\nthe ECM model we accurately quantify the significance of \"layer conditions,\"\nwhich are required to estimate the data traffic through the memory hierarchy,\nand study the impact of typical optimization approaches such as spatial\nblocking, strength reduction, and temporal blocking for their expected\nbenefits. We also compare the ECM model to the widely known Roofline model.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.5010v2"
    },
    {
        "title": "On Bootstrapping Machine Learning Performance Predictors via Analytical\n  Models",
        "authors": [
            "Diego Didona",
            "Paolo Romano"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Performance modeling typically relies on two antithetic methodologies: white\nbox models, which exploit knowledge on system's internals and capture its\ndynamics using analytical approaches, and black box techniques, which infer\nrelations among the input and output variables of a system based on the\nevidences gathered during an initial training phase. In this paper we\ninvestigate a technique, which we name Bootstrapping, which aims at reconciling\nthese two methodologies and at compensating the cons of the one with the pros\nof the other. We thoroughly analyze the design space of this gray box modeling\ntechnique, and identify a number of algorithmic and parametric trade-offs which\nwe evaluate via two realistic case studies, a Key-Value Store and a Total Order\nBroadcast service.\n",
        "pdf_link": "http://arxiv.org/pdf/1410.5102v1"
    },
    {
        "title": "On the Scalability of Data Reduction Techniques in Current and Upcoming\n  HPC Systems from an Application Perspective",
        "authors": [
            "Axel Huebl",
            "Rene Widera",
            "Felix Schmitt",
            "Alexander Matthes",
            "Norbert Podhorszki",
            "Jong Youl Choi",
            "Scott Klasky",
            "Michael Bussmann"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  We implement and benchmark parallel I/O methods for the fully-manycore driven\nparticle-in-cell code PIConGPU. Identifying throughput and overall I/O size as\na major challenge for applications on today's and future HPC systems, we\npresent a scaling law characterizing performance bottlenecks in\nstate-of-the-art approaches for data reduction. Consequently, we propose,\nimplement and verify multi-threaded data-transformations for the I/O library\nADIOS as a feasible way to trade underutilized host-side compute potential on\nheterogeneous systems for reduced I/O latency.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.00522v1"
    },
    {
        "title": "Bolt: Accelerated Data Mining with Fast Vector Compression",
        "authors": [
            "Davis W Blalock",
            "John V Guttag"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Vectors of data are at the heart of machine learning and data mining.\nRecently, vector quantization methods have shown great promise in reducing both\nthe time and space costs of operating on vectors. We introduce a vector\nquantization algorithm that can compress vectors over 12x faster than existing\ntechniques while also accelerating approximate vector operations such as\ndistance and dot product computations by up to 10x. Because it can encode over\n2GB of vectors per second, it makes vector quantization cheap enough to employ\nin many more circumstances. For example, using our technique to compute\napproximate dot products in a nested loop can multiply matrices faster than a\nstate-of-the-art BLAS implementation, even when our algorithm must first\ncompress the matrices.\n  In addition to showing the above speedups, we demonstrate that our approach\ncan accelerate nearest neighbor search and maximum inner product search by over\n100x compared to floating point operations and up to 10x compared to other\nvector quantization methods. Our approximate Euclidean distance and dot product\ncomputations are not only faster than those of related algorithms with slower\nencodings, but also faster than Hamming distance computations, which have\ndirect hardware support on the tested platforms. We also assess the errors of\nour algorithm's approximate distances and dot products, and find that it is\ncompetitive with existing, slower vector quantization algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.10283v1"
    },
    {
        "title": "A Stochastic Model for File Lifetime and Security in Data Center\n  Networks",
        "authors": [
            "Quan-Lin Li",
            "Fan-Qi Ma",
            "Jing-Yu Ma"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Data center networks are an important infrastructure in various applications\nof modern information technologies. Note that each data center always has a\nfinite lifetime, thus once a data center fails, then it will lose all its\nstorage files and useful information. For this, it is necessary to replicate\nand copy each important file into other data centers such that this file can\nincrease its lifetime of staying in a data center network. In this paper, we\ndescribe a large-scale data center network with a file d-threshold policy,\nwhich is to replicate each important file into at most d-1 other data centers\nsuch that this file can maintain in the data center network under a given level\nof data security in the long-term. To this end, we develop three relevant\nMarkov processes to propose two effective methods for assessing the file\nlifetime and data security. By using the RG-factorizations, we show that the\ntwo methods are used to be able to more effectively evaluate the file lifetime\nof large-scale data center networks. We hope the methodology and results given\nin this paper are applicable in the file lifetime study of more general data\ncenter networks with replication mechanism.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.01151v1"
    },
    {
        "title": "Heavy-Traffic Insensitive Bounds for Weighted Proportionally Fair\n  Bandwidth Sharing Policies",
        "authors": [
            "Weina Wang",
            "Siva Theja Maguluri",
            "R. Srikant",
            "Lei Ying"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  We consider a connection-level model proposed by Massouli\\'{e} and Roberts\nfor bandwidth sharing among file transfer flows in a communication network. We\nstudy weighted proportionally fair sharing policies and establish explicit-form\nbounds on the weighted sum of the expected numbers of flows on different routes\nin heavy traffic. The bounds are linear in the number of critically loaded\nlinks in the network, and they hold for a class of phase-type file-size\ndistributions; i.e., the bounds are heavy-traffic insensitive to the\ndistributions in this class. Our approach is Lyapunov-drift based, which is\ndifferent from the widely used diffusion approximation approach. A key\ntechnique we develop is to construct a novel inner product in the state space,\nwhich then allows us to obtain a multiplicative type of state-space collapse in\nsteady state. Furthermore, this state-space collapse result implies the\ninterchange of limits as a by-product for the diffusion approximation of the\nequal-weight case under phase-type file-size distributions, demonstrating the\nheavy-traffic insensitivity of the stationary distribution.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.02120v2"
    },
    {
        "title": "Compiler Enhanced Scheduling for OpenMP for Heterogeneous\n  Multiprocessors",
        "authors": [
            "Jyothi Krishna V S",
            "Shankar Balachandran"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Scheduling in Asymmetric Multicore Processors (AMP), a special case of\nHeterogeneous Multiprocessors, is a widely studied topic. The scheduling\ntechniques which are mostly runtime do not usually consider parallel\nprogramming pattern used in parallel programming frameworks like OpenMP. On the\nother hand, current compilers for these parallel programming platforms are\nhardware oblivious which prevent any compile-time optimization for platforms\nlike big.LITTLE and has to completely rely on runtime optimization. In this\npaper, we propose a hardware-aware Compiler Enhanced Scheduling (CES) where the\ncommon compiler transformations are coupled with compiler added scheduling\ncommands to take advantage of the hardware asymmetry and improve the runtime\nefficiency. We implement a compiler for OpenMP and demonstrate its efficiency\nin Samsung Exynos with big.LITTLE architecture. On an average, we see 18%\nreduction in runtime and 14% reduction in energy consumption in standard NPB\nand FSU benchmarks with CES across multiple frequencies and core configurations\nin big.LITTLE.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.06074v1"
    },
    {
        "title": "Persistent Stochastic Non-Interference",
        "authors": [
            "Jane Hillston",
            "Carla Piazza",
            "Sabina Rossi"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  In this paper we present an information flow security property for\nstochastic, cooperating, processes expressed as terms of the Performance\nEvaluation Process Algebra (PEPA). We introduce the notion of Persistent\nStochastic Non-Interference (PSNI) based on the idea that every state reachable\nby a process satisfies a basic Stochastic Non-Interference (SNI) property. The\nstructural operational semantics of PEPA allows us to give two\ncharacterizations of PSNI: the first involves a single bisimulation-like\nequivalence check, while the second is formulated in terms of unwinding\nconditions. The observation equivalence at the base of our definition relies on\nthe notion of lumpability and ensures that, for a secure process P, the steady\nstate probability of observing the system being in a specific state P' is\nindependent from its possible high level interactions.\n",
        "pdf_link": "http://arxiv.org/pdf/1808.08650v1"
    },
    {
        "title": "Automated Instruction Stream Throughput Prediction for Intel and AMD\n  Microarchitectures",
        "authors": [
            "Jan Laukemann",
            "Julian Hammer",
            "Johannes Hofmann",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  An accurate prediction of scheduling and execution of instruction streams is\na necessary prerequisite for predicting the in-core performance behavior of\nthroughput-bound loop kernels on out-of-order processor architectures. Such\npredictions are an indispensable component of analytical performance models,\nsuch as the Roofline and the Execution-Cache-Memory (ECM) model, and allow a\ndeep understanding of the performance-relevant interactions between hardware\narchitecture and loop code. We present the Open Source Architecture Code\nAnalyzer (OSACA), a static analysis tool for predicting the execution time of\nsequential loops comprising x86 instructions under the assumption of an\ninfinite first-level cache and perfect out-of-order scheduling. We show the\nprocess of building a machine model from available documentation and\nsemi-automatic benchmarking, and carry it out for the latest Intel Skylake and\nAMD Zen micro-architectures. To validate the constructed models, we apply them\nto several assembly kernels and compare runtime predictions with actual\nmeasurements. Finally we give an outlook on how the method may be generalized\nto new architectures.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.00912v2"
    },
    {
        "title": "Linux-Tomcat Application Performance on Amazon AWS",
        "authors": [
            "Neil J. Gunther",
            "Mohit Chawla"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  The need for Linux system administrators to do performance management has\nreturned with a vengeance. Why? The cloud. Resource consumption in the cloud is\nall about pay-as-you-go. This article shows you how performance models can find\nthe most cost-effective deployment of an application on Amazon's cloud.\n",
        "pdf_link": "http://arxiv.org/pdf/1811.12341v1"
    },
    {
        "title": "Towards Automatic Transformation of Legacy Scientific Code into OpenCL\n  for Optimal Performance on FPGAs",
        "authors": [
            "Wim Vanderbauwhede",
            "Syed Waqar Nabi"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  There is a large body of legacy scientific code written in languages like\nFortran that is not optimised to get the best performance out of heterogeneous\nacceleration devices like GPUs and FPGAs, and manually porting such code into\nparallel languages frameworks like OpenCL requires considerable effort. We are\nworking towards developing a turn-key, self-optimising compiler for\naccelerating scientific applications, that can automatically transform legacy\ncode into a solution for heterogeneous targets. In this paper we focus on FPGAs\nas the acceleration devices, and carry out our discussion in the context of the\nOpenCL programming framework. We show a route to automatic creation of kernels\nwhich are optimised for execution in a \"streaming\" fashion, which gives optimal\nperformance on FPGAs. We use a 2D shallow-water model as an illustration;\nspecifically we show how the use of \\emph{channels} to communicate directly\nbetween peer kernels and the use of on-chip memory to create stencil buffers\ncan lead to significant performance improvements. Our results show better FPGA\nperformance against a baseline CPU implementation, and better energy-efficiency\nagainst both CPU and GPU implementations.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.00416v2"
    },
    {
        "title": "Star sampling with and without replacement",
        "authors": [
            "Jonathan Stokes",
            "Steven Weber"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Star sampling (SS) is a random sampling procedure on a graph wherein each\nsample consists of a randomly selected vertex (the star center) and its one-hop\nneighbors (the star endpoints). We consider the use of star sampling to find\nany member of an arbitrary target set of vertices in a graph, where the figure\nof merit (cost) is either the expected number of samples (unit cost) or the\nexpected number of star centers plus star endpoints (linear cost) until a\nvertex in the target set is encountered, either as a star center or as a star\npoint. We analyze this performance measure on three related star sampling\nparadigms: SS with replacement (SSR), SS without center replacement (SSC), and\nSS without star replacement (SSS). We derive exact and approximate expressions\nfor the expected unit and linear costs of SSR, SSC, and SSS on Erdos-Renyi (ER)\ngraphs. Our results show there is i) little difference in unit cost, but ii)\nsignificant difference in linear cost, across the three paradigms. Although our\nresults are derived for ER graphs, experiments on \"real-world\" graphs suggest\nour performance expressions are reasonably accurate for non-ER graphs.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.03393v3"
    },
    {
        "title": "Analytic Performance Modeling and Analysis of Detailed Neuron\n  Simulations",
        "authors": [
            "Francesco Cremonesi",
            "Georg Hager",
            "Gerhard Wellein",
            "Felix Schürmann"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Big science initiatives are trying to reconstruct and model the brain by\nattempting to simulate brain tissue at larger scales and with increasingly more\nbiological detail than previously thought possible. The exponential growth of\nparallel computer performance has been supporting these developments, and at\nthe same time maintainers of neuroscientific simulation code have strived to\noptimally and efficiently exploit new hardware features. Current state of the\nart software for the simulation of biological networks has so far been\ndeveloped using performance engineering practices, but a thorough analysis and\nmodeling of the computational and performance characteristics, especially in\nthe case of morphologically detailed neuron simulations, is lacking. Other\ncomputational sciences have successfully used analytic performance engineering\nand modeling methods to gain insight on the computational properties of\nsimulation kernels, aid developers in performance optimizations and eventually\ndrive co-design efforts, but to our knowledge a model-based performance\nanalysis of neuron simulations has not yet been conducted.\n  We present a detailed study of the shared-memory performance of\nmorphologically detailed neuron simulations based on the Execution-Cache-Memory\n(ECM) performance model. We demonstrate that this model can deliver accurate\npredictions of the runtime of almost all the kernels that constitute the neuron\nmodels under investigation. The gained insight is used to identify the main\ngoverning mechanisms underlying performance bottlenecks in the simulation. The\nimplications of this analysis on the optimization of neural simulation software\nand eventually co-design of future hardware architectures are discussed. In\nthis sense, our work represents a valuable conceptual and quantitative\ncontribution to understanding the performance properties of biological networks\nsimulations.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.05344v1"
    },
    {
        "title": "RegDem: Increasing GPU Performance via Shared Memory Register Spilling",
        "authors": [
            "Putt Sakdhnagool",
            "Amit Sabne",
            "Rudolf Eigenmann"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  GPU utilization, measured as occupancy, is limited by the parallel threads'\ncombined usage of on-chip resources, such as registers and the\nprogrammer-managed shared memory. Higher resource demand means lower effective\nparallel thread count, and therefore lower program performance. Our\ninvestigation found that registers are often the occupancy limiters.\n  The de-facto nvcc compiler-based approach spills excessive registers to the\noff-chip memory, ignoring the shared memory and leaving the on-chip resources\nunderutilized. To mitigate the register demand, this paper presents a binary\ntranslation technique, called RegDem, that spills excessive registers to the\nunderutilized shared memory by transforming the GPU assembly code (SASS). Most\nGPU programs do not fully use shared memory, thus allowing RegDem to use it for\nregister spilling. The higher occupancy achieved by RegDem outweighs the\nslightly higher cost of accessing shared memory instead of placing data in\nregisters. The paper also presents a compile-time performance predictor that\nmodels instructions stalls to choose the best version from a set of program\nvariants. Cumulatively, these techniques outperform the nvcc compiler with a 9%\ngeometric mean, the highest observed being 18%.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.02894v1"
    },
    {
        "title": "Guidelines for benchmarking of optimization approaches for fitting\n  mathematical models",
        "authors": [
            "Clemens Kreutz"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Insufficient performance of optimization approaches for fitting of\nmathematical models is still a major bottleneck in systems biology. In this\nmanuscript, the reasons and methodological challenges are summarized as well as\ntheir impact in benchmark studies. Important aspects for increasing evidence of\noutcomes of benchmark analyses are discussed. Based on general guidelines for\nbenchmarking in computational biology, a collection of tailored guidelines is\npresented for performing informative and unbiased benchmarking of\noptimization-based fitting approaches. Comprehensive benchmark studies based on\nthese recommendations are urgently required for establishing of a robust and\nreliable methodology for the systems biology community.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.03427v1"
    },
    {
        "title": "Quantitative Impact Evaluation of an Abstraction Layer for Data Stream\n  Processing Systems",
        "authors": [
            "Guenter Hesse",
            "Christoph Matthies",
            "Kelvin Glass",
            "Johannes Huegle",
            "Matthias Uflacker"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  With the demand to process ever-growing data volumes, a variety of new data\nstream processing frameworks have been developed. Moving an implementation from\none such system to another, e.g., for performance reasons, requires adapting\nexisting applications to new interfaces. Apache Beam addresses these high\nsubstitution costs by providing an abstraction layer that enables executing\nprograms on any of the supported streaming frameworks. In this paper, we\npresent a novel benchmark architecture for comparing the performance impact of\nusing Apache Beam on three streaming frameworks: Apache Spark Streaming, Apache\nFlink, and Apache Apex. We find significant performance penalties when using\nApache Beam for application development in the surveyed systems. Overall, usage\nof Apache Beam for the examined streaming applications caused a high variance\nof query execution times with a slowdown of up to a factor of 58 compared to\nqueries developed without the abstraction layer. All developed benchmark\nartifacts are publicly available to ensure reproducible results.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.08302v1"
    },
    {
        "title": "MDS coding is better than replication for job completion times",
        "authors": [
            "Ken Duffy",
            "Seva Shneer"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In a multi-server system, how can one get better performance than random\nassignment of jobs to servers if queue-states cannot be queried by the\ndispatcher? A replication strategy has recently been proposed where $d$ copies\nof each arriving job are sent to servers chosen at random. The job's completion\ntime is the first time that the service of any of its copies is complete. On\ncompletion, redundant copies of the job are removed from other queues so as not\nto overburden the system.\n  For digital jobs, where the objects to be served can be algebraically\nmanipulated, and for servers whose output is a linear function of their input,\nhere we consider an alternate strategy: Maximum Distance Separable (MDS) codes.\nFor every batch of $n$ digital jobs that arrive, $n+m$ linear combinations are\ncreated over the reals or a large finite field, and each coded job is sent to a\nrandom server. The batch completion time is the first time that any $n$ of the\n$n+m$ coded jobs are served, as the evaluation of $n$ original jobs can be\nrecovered by Gaussian elimination. If redundant jobs can be removed from queues\non batch completion, we establish that in order to get the improved\nresponse-time performance of sending $d$ copies of each of $n$ jobs via the\nreplication strategy, with the MDS methodology it suffices to send $n+d$ jobs.\nThat is, while replication is multiplicative, MDS is linear.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11052v2"
    },
    {
        "title": "How to Staff When Customers Arrive in Batches",
        "authors": [
            "Andrew Daw",
            "Robert C. Hampshire",
            "Jamol Pender"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In many different settings, requests for service can arrive in near or true\nsimultaneity with one another. This creates batches of arrivals to the\nunderlying queueing system. In this paper, we study the staffing problem for\nthe batch arrival queue. We show that batches place a dangerous and deceptive\nstress on services, requiring a high amount of resources and exhibiting a\nfundamentally larger tail in those demands. This uncovers a service regime in\nwhich a system with large batch arrivals may have low utilization but will\nstill have non-trivial waiting. Methodologically, these staffing results follow\nfrom novel large batch and large batch-and-rate limits of the multi-server\nqueueing model. In the large batch limit, we establish the first formal\nconnection between general multi-server queues and storage processes, another\nfamily of stochastic models. By consequence, we show that the batch scaled\nqueue length process is not asymptotically normal, and that, in fact, the fluid\nand diffusion-type limits coincide. Hence, the (safety) staffing of this system\nmust be directly proportional to the batch size just to achieve a\nnon-degenerate probability of wait. In exhibition of the existence and insights\nof this large batch regime, we apply our results to data on Covid-19 contact\ntracing in New York City. In doing so, we identify significant benefits\nproduced by the tracing agency's decision to staff above national\nrecommendations, and we also demonstrate that there may have been an\nopportunity to further improve the operation by optimizing the arrival pattern\nin the public health data pipeline.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12650v4"
    },
    {
        "title": "Modeling Shared Cache Performance of OpenMP Programs using Reuse\n  Distance",
        "authors": [
            "Atanu Barai",
            "Gopinath Chennupati",
            "Nandakishore Santhi",
            "Abdel-Hameed A. Badawy",
            "Stephan Eidenbenz"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Performance modeling of parallel applications on multicore computers remains\na challenge in computational co-design due to the complex design of multicore\nprocessors including private and shared memory hierarchies. We present a\nScalable Analytical Shared Memory Model to predict the performance of parallel\napplications that runs on a multicore computer and shares the same level of\ncache in the hierarchy. This model uses a computationally efficient,\nprobabilistic method to predict the reuse distance profiles, where reuse\ndistance is a hardware architecture-independent measure of the patterns of\nvirtual memory accesses. It relies on a stochastic, static basic block-level\nanalysis of reuse profiles measured from the memory traces of applications ran\nsequentially on small instances rather than using a multi-threaded trace. The\nresults indicate that the hit-rate predictions on the shared cache are\naccurate.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.12666v1"
    },
    {
        "title": "Optimising energy and overhead for large parameter space simulations",
        "authors": [
            "Alexander J. M. Kell",
            "Matthew Forshaw",
            "A. Stephen McGough"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Many systems require optimisation over multiple objectives, where objectives\nare characteristics of the system such as energy consumed or increase in time\nto perform the work. Optimisation is performed by selecting the `best' set of\ninput parameters to elicit the desired objectives. However, the parameter\nsearch space can often be far larger than can be searched in a reasonable time.\nAdditionally, the objectives are often mutually exclusive -- leading to a\ndecision being made as to which objective is more important or optimising over\na combination of the objectives. This work is an application of a Genetic\nAlgorithm to identify the Pareto frontier for finding the optimal parameter\nsets for all combinations of objectives. A Pareto frontier can be used to\nidentify the sets of optimal parameters for which each is the `best' for a\ngiven combination of objectives -- thus allowing decisions to be made with full\nknowledge. We demonstrate this approach for the HTC-Sim simulation system in\nthe case where a Reinforcement Learning scheduler is tuned for the two\nobjectives of energy consumption and task overhead. Demonstrating that this\napproach can reduce the energy consumed by ~36% over previously published work\nwithout significantly increasing the overhead.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.02516v1"
    },
    {
        "title": "Characterizing Deep Learning Training Workloads on Alibaba-PAI",
        "authors": [
            "Mengdi Wang",
            "Chen Meng",
            "Guoping Long",
            "Chuan Wu",
            "Jun Yang",
            "Wei Lin",
            "Yangqing Jia"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Modern deep learning models have been exploited in various domains, including\ncomputer vision (CV), natural language processing (NLP), search and\nrecommendation. In practical AI clusters, workloads training these models are\nrun using software frameworks such as TensorFlow, Caffe, PyTorch and CNTK. One\ncritical issue for efficiently operating practical AI clouds, is to\ncharacterize the computing and data transfer demands of these workloads, and\nmore importantly, the training performance given the underlying software\nframework and hardware configurations. In this paper, we characterize deep\nlearning training workloads from Platform of Artificial Intelligence (PAI) in\nAlibaba. We establish an analytical framework to investigate detailed execution\ntime breakdown of various workloads using different training architectures, to\nidentify performance bottleneck. Results show that weight/gradient\ncommunication during training takes almost 62% of the total execution time\namong all our workloads on average. The computation part, involving both GPU\ncomputing and memory access, are not the biggest bottleneck based on collective\nbehavior of the workloads. We further evaluate attainable performance of the\nworkloads on various potential software/hardware mappings, and explore\nimplications on software architecture selection and hardware configurations. We\nidentify that 60% of PS/Worker workloads can be potentially sped up when ported\nto the AllReduce architecture exploiting the high-speed NVLink for GPU\ninterconnect, and on average 1.7X speedup can be achieved when Ethernet\nbandwidth is upgraded from 25 Gbps to 100 Gbps.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.05930v1"
    },
    {
        "title": "AI Benchmark: All About Deep Learning on Smartphones in 2019",
        "authors": [
            "Andrey Ignatov",
            "Radu Timofte",
            "Andrei Kulik",
            "Seungsoo Yang",
            "Ke Wang",
            "Felix Baum",
            "Max Wu",
            "Lirong Xu",
            "Luc Van Gool"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The performance of mobile AI accelerators has been evolving rapidly in the\npast two years, nearly doubling with each new generation of SoCs. The current\n4th generation of mobile NPUs is already approaching the results of\nCUDA-compatible Nvidia graphics cards presented not long ago, which together\nwith the increased capabilities of mobile deep learning frameworks makes it\npossible to run complex and deep AI models on mobile devices. In this paper, we\nevaluate the performance and compare the results of all chipsets from Qualcomm,\nHiSilicon, Samsung, MediaTek and Unisoc that are providing hardware\nacceleration for AI inference. We also discuss the recent changes in the\nAndroid ML pipeline and provide an overview of the deployment of deep learning\nmodels on mobile devices. All numerical results provided in this paper can be\nfound and are regularly updated on the official project website:\nhttp://ai-benchmark.com.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.06663v1"
    },
    {
        "title": "Kriptosare.gen, a dockerized Bitcoin testbed: analysis of server\n  performance",
        "authors": [
            "Francesco Zola",
            "Cristina Pérez-Solá",
            "Jon Egaña Zubia",
            "Maria Eguimendia",
            "Jordi Herrera-Joancomartí"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Bitcoin is a peer-to-peer distributed cryptocurrency system, that keeps all\ntransaction history in a public ledger known as blockchain. The Bitcoin network\nis implicitly pseudoanonymous and its nodes are controlled by independent\nentities making network analysis difficult. This calls for the development of a\nfully controlled testing environment.\n  This paper presents Kriptosare.gen, a dockerized automatized Bitcoin testbed,\nfor deploying full-scale custom Bitcoin networks. The testbed is deployed in a\nsingle machine executing four different experiments, each one with different\nnetwork configuration. We perform a cost analysis to investigate how the\nresources are related with network parameters and provide experimental data\nquantifying the amount of computational resources needed to run the different\ntypes of simulations. Obtained results demonstrate that it is possible to run\nthe testbed with a configuration similar to a real Bitcoin system.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.07377v1"
    },
    {
        "title": "Delay-optimal policies in partial fork-join systems with redundancy and\n  random slowdowns",
        "authors": [
            "Martin Zubeldia"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We consider a large distributed service system consisting of $n$ homogeneous\nservers with infinite capacity FIFO queues. Jobs arrive as a Poisson process of\nrate $\\lambda n/k_n$ (for some positive constant $\\lambda$ and integer $k_n$).\nEach incoming job consists of $k_n$ identical tasks that can be executed in\nparallel, and that can be encoded into at least $k_n$ \"replicas\" of the same\nsize (by introducing redundancy) so that the job is considered to be completed\nwhen any $k_n$ replicas associated with it finish their service. Moreover, we\nassume that servers can experience random slowdowns in their processing rate so\nthat the service time of a replica is the product of its size and a random\nslowdown.\n  First, we assume that the server slowdowns are shifted exponential and\nindependent of the replica sizes. In this setting we show that the delay of a\ntypical job is asymptotically minimized (as $n\\to\\infty$) when the number of\nreplicas per task is a constant that only depends on the arrival rate\n$\\lambda$, and on the expected slowdown of servers.\n  Second, we introduce a new model for the server slowdowns in which larger\ntasks experience less variable slowdowns than smaller tasks. In this setting we\nshow that, under the class of policies where all replicas start their service\nat the same time, the delay of a typical job is asymptotically minimized (as\n$n\\to\\infty$) when the number of replicas per task is made to depend on the\nactual size of the tasks being replicated, with smaller tasks being replicated\nmore than larger tasks.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.09602v1"
    },
    {
        "title": "Direct N-body application on low-power and energy-efficient parallel\n  architectures",
        "authors": [
            "D. Goz",
            "G. Ieronymakis",
            "V. Papaefstathiou",
            "N. Dimou",
            "S. Bertocco",
            "A. Ragagnin",
            "L. Tornatore",
            "G. Taffoni",
            "I. Coretti"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The aim of this work is to quantitatively evaluate the impact of computation\non the energy consumption on ARM MPSoC platforms, exploiting CPUs, embedded\nGPUs and FPGAs. One of them possibly represents the future of High Performance\nComputing systems: a prototype of an Exascale supercomputer. Performance and\nenergy measurements are made using a state-of-the-art direct $N$-body code from\nthe astrophysical domain. We provide a comparison of the time-to-solution and\nenergy delay product metrics, for different software configurations. We have\nshown that FPGA technologies can be used for application kernel acceleration\nand are emerging as a promising alternative to \"traditional\" technologies for\nHPC, which purely focus on peak-performance than on power-efficiency.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.14496v1"
    },
    {
        "title": "Towards Green Computing: A Survey of Performance and Energy Efficiency\n  of Different Platforms using OpenCL",
        "authors": [
            "Philip Heinisch",
            "Katharina Ostaszewski",
            "Hendrik Ranocha"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  When considering different hardware platforms, not just the time-to-solution\ncan be of importance but also the energy necessary to reach it. This is not\nonly the case with battery powered and mobile devices but also with\nhigh-performance parallel cluster systems due to financial and practical limits\non power consumption and cooling. Recent developments in hard- and software\nhave given programmers the ability to run the same code on a range of different\ndevices giving rise to the concept of heterogeneous computing. Many of these\ndevices are optimized for certain types of applications. To showcase the\ndifferences and give a basic outlook on the applicability of different\narchitectures for specific problems, the cross-platform OpenCL framework was\nused to compare both time- and energy-to-solution. A large set of devices\nranging from ARM processors to server CPUs and consumer and enterprise level\nGPUs has been used with different benchmarking testcases taken from applied\nresearch applications. While the results show the overall advantages of GPUs in\nterms of both runtime and energy efficiency compared to CPUs, ARM devices show\npotential for certain applications in massively parallel systems. This study\nalso highlights how OpenCL enables the use of the same codebase on many\ndifferent systems and hardware platforms without specific code adaptations.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.03794v1"
    },
    {
        "title": "Benchmarking TinyML Systems: Challenges and Direction",
        "authors": [
            "Colby R. Banbury",
            "Vijay Janapa Reddi",
            "Max Lam",
            "William Fu",
            "Amin Fazel",
            "Jeremy Holleman",
            "Xinyuan Huang",
            "Robert Hurtado",
            "David Kanter",
            "Anton Lokhmotov",
            "David Patterson",
            "Danilo Pau",
            "Jae-sun Seo",
            "Jeff Sieracki",
            "Urmish Thakker",
            "Marian Verhelst",
            "Poonam Yadav"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Recent advancements in ultra-low-power machine learning (TinyML) hardware\npromises to unlock an entirely new class of smart applications. However,\ncontinued progress is limited by the lack of a widely accepted benchmark for\nthese systems. Benchmarking allows us to measure and thereby systematically\ncompare, evaluate, and improve the performance of systems and is therefore\nfundamental to a field reaching maturity. In this position paper, we present\nthe current landscape of TinyML and discuss the challenges and direction\ntowards developing a fair and useful hardware benchmark for TinyML workloads.\nFurthermore, we present our four benchmarks and discuss our selection\nmethodology. Our viewpoints reflect the collective thoughts of the TinyMLPerf\nworking group that is comprised of over 30 organizations.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.04821v4"
    },
    {
        "title": "In Datacenter Performance, The Only Constant Is Change",
        "authors": [
            "Dmitry Duplyakin",
            "Alexandru Uta",
            "Aleksander Maricq",
            "Robert Ricci"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  All computing infrastructure suffers from performance variability, be it\nbare-metal or virtualized. This phenomenon originates from many sources: some\ntransient, such as noisy neighbors, and others more permanent but sudden, such\nas changes or wear in hardware, changes in the underlying hypervisor stack, or\neven undocumented interactions between the policies of the computing resource\nprovider and the active workloads. Thus, performance measurements obtained on\nclouds, HPC facilities, and, more generally, datacenter environments are almost\nguaranteed to exhibit performance regimes that evolve over time, which leads to\nundesirable nonstationarities in application performance. In this paper, we\npresent our analysis of performance of the bare-metal hardware available on the\nCloudLab testbed where we focus on quantifying the evolving performance regimes\nusing changepoint detection. We describe our findings, backed by a dataset with\nnearly 6.9M benchmark results collected from over 1600 machines over a period\nof 2 years and 9 months. These findings yield a comprehensive characterization\nof real-world performance variability patterns in one computing facility, a\nmethodology for studying such patterns on other infrastructures, and contribute\nto a better understanding of performance variability in general.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.04824v1"
    },
    {
        "title": "How Fast Can We Insert? An Empirical Performance Evaluation of Apache\n  Kafka",
        "authors": [
            "Guenter Hesse",
            "Christoph Matthies",
            "Matthias Uflacker"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Message brokers see widespread adoption in modern IT landscapes, with Apache\nKafka being one of the most employed platforms. These systems feature\nwell-defined APIs for use and configuration and present flexible solutions for\nvarious data storage scenarios. Their ability to scale horizontally enables\nusers to adapt to growing data volumes and changing environments. However, one\nof the main challenges concerning message brokers is the danger of them\nbecoming a bottleneck within an IT architecture. To prevent this, knowledge\nabout the amount of data a message broker using a specific configuration can\nhandle needs to be available. In this paper, we propose a monitoring\narchitecture for message brokers and similar Java Virtual Machine-based\nsystems. We present a comprehensive performance analysis of the popular Apache\nKafka platform using our approach. As part of the benchmark, we study selected\ndata ingestion scenarios with respect to their maximum data ingestion rates.\nThe results show that we can achieve an ingestion rate of about 420,000\nmessages/second on the used commodity hardware and with the developed data\nsender tool.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.06452v3"
    },
    {
        "title": "Towards High Performance, Portability, and Productivity: Lightweight\n  Augmented Neural Networks for Performance Prediction",
        "authors": [
            "Ajitesh Srivastava",
            "Naifeng Zhang",
            "Rajgopal Kannan",
            "Viktor K. Prasanna"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Writing high-performance code requires significant expertise in the\nprogramming language, compiler optimizations, and hardware knowledge. This\noften leads to poor productivity and portability and is inconvenient for a\nnon-programmer domain-specialist such as a Physicist. More desirable is a\nhigh-level language where the domain-specialist simply specifies the workload\nin terms of high-level operations (e.g., matrix-multiply(A, B)), and the\ncompiler identifies the best implementation fully utilizing the heterogeneous\nplatform. For creating a compiler that supports productivity, portability, and\nperformance simultaneously, it is crucial to predict the performance of various\navailable implementations (variants) of the dominant operations (kernels)\ncontained in the workload on various hardware to decide (a) which variant\nshould be chosen for each kernel in the workload, and (b) on which hardware\nresource the variant should run. To enable the performance prediction, we\npropose lightweight augmented neural networks for arbitrary combinations of\nkernel-variant-hardware. A key innovation is utilizing the mathematical\ncomplexity of the kernels as a feature to achieve higher accuracy. These models\nare compact to reduce training time and fast inference during compile-time and\nrun-time. Using models with less than 75 parameters, and only 250 training data\ninstances, we are able to obtain a low MAPE of 3%, significantly outperforming\ntraditional feed-forward neural networks on 48 kernel-variant-hardware\ncombinations. We further demonstrate that our variant-selection approach can be\nused in Halide implementations to obtain up to 1.7x speedup over Halide's\nauto-scheduler.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.07497v2"
    },
    {
        "title": "Optimal Multiserver Scheduling with Unknown Job Sizes in Heavy Traffic",
        "authors": [
            "Ziv Scully",
            "Isaac Grosof",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We consider scheduling to minimize mean response time of the M/G/k queue with\nunknown job sizes. In the single-server case, the optimal policy is the Gittins\npolicy, but it is not known whether Gittins or any other policy is optimal in\nthe multiserver case. Exactly analyzing the M/G/k under any scheduling policy\nis intractable, and Gittins is a particularly complicated policy that is hard\nto analyze even in the single-server case.\n  In this work we introduce monotonic Gittins (M-Gittins), a new variation of\nthe Gittins policy, and show that it minimizes mean response time in the\nheavy-traffic M/G/k for a wide class of finite-variance job size distributions.\nWe also show that the monotonic shortest expected remaining processing time\n(M-SERPT) policy, which is simpler than M-Gittins, is a 2-approximation for\nmean response time in the heavy traffic M/G/k under similar conditions. These\nresults constitute the most general optimality results to date for the M/G/k\nwith unknown job sizes. Our techniques build upon work by Grosof et al., who\nstudy simple policies, such as SRPT, in the M/G/k; Bansal et al., Kamphorst and\nZwart, and Lin et al., who analyze mean response time scaling of simple\npolicies in the heavy-traffic M/G/1; and Aalto et al. and Scully et al., who\ncharacterize and analyze the Gittins policy in the M/G/1.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.13232v3"
    },
    {
        "title": "Static vs accumulating priorities in healthcare queues under heavy loads",
        "authors": [
            "Binyamin Oz",
            "Seva Shneer",
            "Ilze Ziedins"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Amid unprecedented times caused by COVID-19, healthcare systems all over the\nworld are strained to the limits of, or even beyond, capacity. A similar event\nis experienced by some healthcare systems regularly, due to for instance\nseasonal spikes in the number of patients. We model this as a queueing system\nin heavy traffic (where the arrival rate is approaching the service rate from\nbelow) or in overload (where the arrival rate exceeds the service rate). In\nboth cases we assume that customers (patients) may have different priorities\nand we consider two popular service disciplines: static priorities and\naccumulating priorities. It has been shown that the latter allows for patients\nof all classes to be seen in a timely manner as long as the system is stable.\nWe demonstrate however that if accumulating priorities are used in the heavy\ntraffic or overload regime, then all patients, including those with the highest\npriority, will experience very long waiting times. If on the other hand static\npriorities are applied, then one can ensure that the highest-priority patients\nwill be seen in a timely manner even in overloaded systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2003.14087v2"
    },
    {
        "title": "Function-as-a-Service Performance Evaluation: A Multivocal Literature\n  Review",
        "authors": [
            "Joel Scheuner",
            "Philipp Leitner"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Function-as-a-Service (FaaS) is one form of the serverless cloud computing\nparadigm and is defined through FaaS platforms (e.g., AWS Lambda) executing\nevent-triggered code snippets (i.e., functions). Many studies that empirically\nevaluate the performance of such FaaS platforms have started to appear but we\nare currently lacking a comprehensive understanding of the overall domain. To\naddress this gap, we conducted a multivocal literature review (MLR) covering\n112 studies from academic (51) and grey (61) literature. We find that existing\nwork mainly studies the AWS Lambda platform and focuses on micro-benchmarks\nusing simple functions to measure CPU speed and FaaS platform overhead (i.e.,\ncontainer cold starts). Further, we discover a mismatch between academic and\nindustrial sources on tested platform configurations, find that function\ntriggers remain insufficiently studied, and identify HTTP API gateways and\ncloud storages as the most used external service integrations. Following\nexisting guidelines on experimentation in cloud systems, we discover many flaws\nthreatening the reproducibility of experiments presented in the surveyed\nstudies. We conclude with a discussion of gaps in literature and highlight\nmethodological suggestions that may serve to improve future FaaS performance\nevaluation studies.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.03276v3"
    },
    {
        "title": "Energy Predictive Models for Convolutional Neural Networks on Mobile\n  Platforms",
        "authors": [
            "Crefeda Faviola Rodrigues",
            "Graham Riley",
            "Mikel Lujan"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Energy use is a key concern when deploying deep learning models on mobile and\nembedded platforms. Current studies develop energy predictive models based on\napplication-level features to provide researchers a way to estimate the energy\nconsumption of their deep learning models. This information is useful for\nbuilding resource-aware models that can make efficient use of the hard-ware\nresources. However, previous works on predictive modelling provide little\ninsight into the trade-offs involved in the choice of features on the final\npredictive model accuracy and model complexity. To address this issue, we\nprovide a comprehensive analysis of building regression-based predictive models\nfor deep learning on mobile devices, based on empirical measurements gathered\nfrom the SyNERGY framework.Our predictive modelling strategy is based on two\ntypes of predictive models used in the literature:individual layers and\nlayer-type. Our analysis of predictive models show that simple layer-type\nfeatures achieve a model complexity of 4 to 32 times less for convolutional\nlayer predictions for a similar accuracy compared to predictive models using\nmore complex features adopted by previous approaches. To obtain an overall\nenergy estimate of the inference phase, we build layer-type predictive models\nfor the fully-connected and pooling layers using 12 representative\nConvolutional NeuralNetworks (ConvNets) on the Jetson TX1 and the Snapdragon\n820using software backends such as OpenBLAS, Eigen and CuDNN. We obtain an\naccuracy between 76% to 85% and a model complexity of 1 for the overall energy\nprediction of the test ConvNets across different hardware-software\ncombinations.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.05137v1"
    },
    {
        "title": "GAPP: A Fast Profiler for Detecting Serialization Bottlenecks in\n  Parallel Linux Applications",
        "authors": [
            "Reena Nair",
            "Tony Field"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We present a parallel profiling tool, GAPP, that identifies serialization\nbottlenecks in parallel Linux applications arising from load imbalance or\ncontention for shared resources . It works by tracing kernel context switch\nevents using kernel probes managed by the extended Berkeley Packet Filter\n(eBPF) framework. The overhead is thus extremely low (an average 4% run time\noverhead for the applications explored), the tool requires no program\ninstrumentation and works for a variety of serialization bottlenecks. We\nevaluate GAPP using the Parsec3.0 benchmark suite and two large open-source\nprojects: MySQL and Nektar++ (a spectral/hp element framework). We show that\nGAPP is able to reveal a wide range of bottleneck-related performance issues,\nfor example arising from synchronization primitives, busy-wait loops, memory\noperations, thread imbalance and resource contention.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.05628v1"
    },
    {
        "title": "Comparisons of Algorithms in Big Data Processing",
        "authors": [
            "Amirali Daghighi",
            "Jim Q. Chen"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Parallel computing is the fundamental base for MapReduce framework in Hadoop.\nEach data chunk is replicated over 3 servers for increasing availability of\ndata and decreasing probability of data loss. Hence, the 3 servers that have\nMap task stored on their disk are fastest servers to process them, which are\ncalled local servers. All servers in the same rack as local servers are called\nrack-local servers that are slower than local servers since data chunk\nassociated with Map task should be fetched through top of the rack switch. All\nother servers are called remote servers that are slowest servers since they\nneed to fetch data from a local server in another rack, so data should be\ntransmitted through at least 2 top of rack switches and a core switch. Note\nthat number of switches in path of data transfer depends on internal network\nstructure of data centers. The First-In-First-Out (FIFO) and Hadoop Fair\nScheduler (HFS) algorithms do not take rack structure of data centers into\naccount, so they are known to not be heavy-traffic delay optimal or even\nthroughput optimal. The recent advances on scheduling for data centers\nconsidering rack structure of them and heterogeneity of servers resulted in\nstate-of-the-art Balanced-PANDAS algorithm that outperforms classic MaxWeight\nalgorithm. In both Balanced-PANDAS and MaxWeight algorithms, processing rate of\nlocal, rack-local, and remote servers are assumed to be known. However, with\nthe change of traffic over time in addition to estimation errors of processing\nrates, it is not realistic to consider processing rates to be known. In this\nwork, we study robustness of Balanced-PANDAS and MaxWeight algorithms in terms\nof inaccurate estimations of processing rates. We observe that Balanced-PANDAS\nis not as sensitive as MaxWeight on the accuracy of processing rates, making it\nmore appealing to use in data centers.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06254v2"
    },
    {
        "title": "Scalability of High-Performance PDE Solvers",
        "authors": [
            "Paul Fischer",
            "Misun Min",
            "Thilina Rathnayake",
            "Som Dutta",
            "Tzanio Kolev",
            "Veselin Dobrev",
            "Jean-Sylvain Camier",
            "Martin Kronbichler",
            "Tim Warburton",
            "Kasia Swirydowicz",
            "Jed Brown"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Performance tests and analyses are critical to effective HPC software\ndevelopment and are central components in the design and implementation of\ncomputational algorithms for achieving faster simulations on existing and\nfuture computing architectures for large-scale application problems. In this\npaper, we explore performance and space-time trade-offs for important\ncompute-intensive kernels of large-scale numerical solvers for PDEs that govern\na wide range of physical applications. We consider a sequence of PDE- motivated\nbake-off problems designed to establish best practices for efficient high-order\nsimulations across a variety of codes and platforms. We measure peak\nperformance (degrees of freedom per second) on a fixed number of nodes and\nidentify effective code optimization strategies for each architecture. In\naddition to peak performance, we identify the minimum time to solution at 80%\nparallel efficiency. The performance analysis is based on spectral and p-type\nfinite elements but is equally applicable to a broad spectrum of numerical PDE\ndiscretizations, including finite difference, finite volume, and h-type finite\nelements.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.06722v1"
    },
    {
        "title": "Demonstrating a Pre-Exascale, Cost-Effective Multi-Cloud Environment for\n  Scientific Computing",
        "authors": [
            "I. Sfiligoi",
            "D. Schultz",
            "B. Riedel",
            "F. Wuerthwein",
            "S. Barnet",
            "V. Brik"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Scientific computing needs are growing dramatically with time and are\nexpanding in science domains that were previously not compute intensive. When\ncompute workflows spike well in excess of the capacity of their local compute\nresource, capacity should be temporarily provisioned from somewhere else to\nboth meet deadlines and to increase scientific output. Public Clouds have\nbecome an attractive option due to their ability to be provisioned with minimal\nadvance notice. The available capacity of cost-effective instances is not well\nunderstood. This paper presents expanding the IceCube's production HTCondor\npool using cost-effective GPU instances in preemptible mode gathered from the\nthree major Cloud providers, namely Amazon Web Services, Microsoft Azure and\nthe Google Cloud Platform. Using this setup, we sustained for a whole workday\nabout 15k GPUs, corresponding to around 170 PFLOP32s, integrating over one\nEFLOP32 hour worth of science output for a price tag of about $60k. In this\npaper, we provide the reasoning behind Cloud instance selection, a description\nof the setup and an analysis of the provisioned resources, as well as a short\ndescription of the actual science output of the exercise.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09492v1"
    },
    {
        "title": "Communication-Aware Scheduling of Precedence-Constrained Tasks on\n  Related Machines",
        "authors": [
            "Yu Su",
            "Xiaoqi Ren",
            "Shai Vardi",
            "Adam Wierman"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Scheduling precedence-constrained tasks is a classical problem that has been\nstudied for more than fifty years. However, little progress has been made in\nthe setting where there are communication delays between tasks. Results for the\ncase of identical machines were derived nearly thirty years ago, and yet no\nresults for related machines have followed. In this work, we propose a new\nscheduler, Generalized Earliest Time First (GETF), and provide the first\nprovable, worst-case approximation guarantees for the goals of minimizing both\nthe makespan and total weighted completion time of tasks with precedence\nconstraints on related machines with machine-dependent communication times.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14639v1"
    },
    {
        "title": "Stability, memory, and messaging tradeoffs in heterogeneous service\n  systems",
        "authors": [
            "David Gamarnik",
            "John N. Tsitsiklis",
            "Martin Zubeldia"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We consider a heterogeneous distributed service system, consisting of $n$\nservers with unknown and possibly different processing rates. Jobs with unit\nmean and independent processing times arrive as a renewal process of rate\n$\\lambda n$, with $0<\\lambda<1$, to the system. Incoming jobs are immediately\ndispatched to one of several queues associated with the $n$ servers. We assume\nthat the dispatching decisions are made by a central dispatcher endowed with a\nfinite memory, and with the ability to exchange messages with the servers.\n  We study the fundamental resource requirements (memory bits and message\nexchange rate) in order for a dispatching policy to be {\\bf maximally stable},\ni.e., stable whenever the processing rates are such that the arrival rate is\nless than the total available processing rate. First, for the case of Poisson\narrivals and exponential service times, we present a policy that is maximally\nstable while using a positive (but arbitrarily small) message rate, and\n$\\log_2(n)$ bits of memory. Second, we show that within a certain broad class\nof policies, a dispatching policy that exchanges $o\\big(n^2\\big)$ messages per\nunit of time, and with $o(\\log(n))$ bits of memory, cannot be maximally stable.\nThus, as long as the message rate is not too excessive, a logarithmic memory is\nnecessary and sufficient for maximal stability.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.07219v1"
    },
    {
        "title": "Machine Learning Enabled Scalable Performance Prediction of Scientific\n  Codes",
        "authors": [
            "Gopinath Chennupati",
            "Nandakishore Santhi",
            "Phill Romero",
            "Stephan Eidenbenz"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We present the Analytical Memory Model with Pipelines (AMMP) of the\nPerformance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and\nhardware architecture parameters as input, predicts runtime of that code on the\ntarget hardware platform, which is defined in the input parameters. PPT-AMMP\ntransforms the code to an (architecture-independent) intermediate\nrepresentation, then (i) analyzes the basic block structure of the code, (ii)\nprocesses architecture-independent virtual memory access patterns that it uses\nto build memory reuse distance distribution models for each basic block, (iii)\nruns detailed basic-block level simulations to determine hardware pipeline\nusage.\n  PPT-AMMP uses machine learning and regression techniques to build the\nprediction models based on small instances of the input code, then integrates\ninto a higher-order discrete-event simulation model of PPT running on Simian\nPDES engine. We validate PPT-AMMP on four standard computational physics\nbenchmarks, finally present a use case of hardware parameter sensitivity\nanalysis to identify bottleneck hardware resources on different code inputs. We\nfurther extend PPT-AMMP to predict the performance of scientific application\n(radiation transport), SNAP. We analyze the application of multi-variate\nregression models that accurately predict the reuse profiles and the basic\nblock counts. The predicted runtimes of SNAP when compared to that of actual\ntimes are accurate.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.04212v2"
    },
    {
        "title": "Discriminating Equivalent Algorithms via Relative Performance",
        "authors": [
            "Aravind Sankaran",
            "Paolo Bientinesi"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  In scientific computing, it is common that a mathematical expression can be\ncomputed by many different algorithms (sometimes over hundreds), each\nidentifying a specific sequence of library calls. Although mathematically\nequivalent, those algorithms might exhibit significant differences in terms of\nperformance. However in practice, due to fluctuations, there is not one\nalgorithm that consistently performs noticeably better than the rest. For this\nreason, with this work we aim to identify not the one best algorithm, but the\nsubset of algorithms that are reliably faster than the rest. To this end,\ninstead of using the usual approach of quantifying the performance of an\nalgorithm in absolute terms, we present a measurement-based clustering approach\nto sort the algorithms into equivalence (or performance) classes using\npair-wise comparisons. We show that this approach, based on relative\nperformance, leads to robust identification of the fastest algorithms even\nunder noisy system conditions. Furthermore, it enables the development of\npractical machine learning models for automatic algorithm selection.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.07226v3"
    },
    {
        "title": "Performance Assessment of OpenMP Compilers Targeting NVIDIA V100 GPUs",
        "authors": [
            "Joshua Hoke Davis",
            "Christopher Daley",
            "Swaroop Pophale",
            "Thomas Huber",
            "Sunita Chandrasekaran",
            "Nicholas J. Wright"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Heterogeneous systems are becoming increasingly prevalent. In order to\nexploit the rich compute resources of such systems, robust programming models\nare needed for application developers to seamlessly migrate legacy code from\ntoday's systems to tomorrow's. Over the past decade and more, directives have\nbeen established as one of the promising paths to tackle programmatic\nchallenges on emerging systems. This work focuses on applying and demonstrating\nOpenMP offloading directives on five proxy applications. We observe that the\nperformance varies widely from one compiler to the other; a crucial aspect of\nour work is reporting best practices to application developers who use OpenMP\noffloading compilers. While some issues can be worked around by the developer,\nthere are other issues that must be reported to the compiler vendors. By\nrestructuring OpenMP offloading directives, we gain an 18x speedup for the su3\nproxy application on NERSC's Cori system when using the Clang compiler, and a\n15.7x speedup by switching max reductions to add reductions in the laplace\nmini-app when using the Cray-llvm compiler on Cori.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.09454v3"
    },
    {
        "title": "Self-Learning Threshold-Based Load Balancing",
        "authors": [
            "Diego Goldsztajn",
            "Sem C. Borst",
            "Johan S. H. van Leeuwaarden",
            "Debankur Mukherjee",
            "Philip A. Whiting"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We consider a large-scale service system where incoming tasks have to be\ninstantaneously dispatched to one out of many parallel server pools. The\nuser-perceived performance degrades with the number of concurrent tasks and the\ndispatcher aims at maximizing the overall quality-of-service by balancing the\nload through a simple threshold policy. We demonstrate that such a policy is\noptimal on the fluid and diffusion scales, while only involving a small\ncommunication overhead, which is crucial for large-scale deployments. In order\nto set the threshold optimally, it is important, however, to learn the load of\nthe system, which may be unknown. For that purpose, we design a control rule\nfor tuning the threshold in an online manner. We derive conditions which\nguarantee that this adaptive threshold settles at the optimal value, along with\nestimates for the time until this happens. In addition, we provide numerical\nexperiments which support the theoretical results and further indicate that our\npolicy copes effectively with time-varying demand patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15525v4"
    },
    {
        "title": "Poster: Benchmarking Financial Data Feed Systems",
        "authors": [
            "Manuel Coenen",
            "Christoph Wagner",
            "Alexander Echler",
            "Sebastian Frischbier"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Data-driven solutions for the investment industry require event-based backend\nsystems to process high-volume financial data feeds with low latency, high\nthroughput, and guaranteed delivery modes.\n  At vwd we process an average of 18 billion incoming event notifications from\n500+ data sources for 30 million symbols per day and peak rates of 1+ million\nnotifications per second using custom-built platforms that keep audit logs of\nevery event.\n  We currently assess modern open source event-processing platforms such as\nKafka, NATS, Redis, Flink or Storm for the use in our ticker plant to reduce\nthe maintenance effort for cross-cutting concerns and leverage hybrid\ndeployment models. For comparability and repeatability we benchmark candidates\nwith a standardized workload we derived from our real data feeds.\n  We have enhanced an existing light-weight open source benchmarking tool in\nits processing, logging, and reporting capabilities to cope with our workloads.\nThe resulting tool wrench can simulate workloads or replay snapshots in volume\nand dynamics like those we process in our ticker plant. We provide the tool as\nopen source.\n  As part of ongoing work we contribute details on (a) our workload and\nrequirements for benchmarking candidate platforms for financial feed\nprocessing; (b) the current state of the tool wrench.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.15534v1"
    },
    {
        "title": "Resource Allocation in One-dimensional Distributed Service Networks with\n  Applications",
        "authors": [
            "Nitish K. Panigrahy",
            "Prithwish Basu",
            "Philippe Nain",
            "Don Towsley",
            "Ananthram Swami",
            "Kevin S. Chan",
            "Kin K. Leung"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We consider assignment policies that allocate resources to users, where both\nresources and users are located on a one-dimensional line. First, we consider\nunidirectional assignment policies that allocate resources only to users\nlocated to their left. We propose the Move to Right (MTR) policy, which scans\nfrom left to right assigning nearest rightmost available resource to a user,\nand contrast it to the Unidirectional Gale-Shapley (UGS) matching policy. While\nboth policies among all unidirectional policies, minimize the expected distance\ntraveled by a request (request distance), MTR is fairer. Moreover, we show that\nwhen user and resource locations are modeled by statistical point processes,\nand resources are allowed to satisfy more than one user, the spatial system\nunder unidirectional policies can be mapped into bulk service queueing systems,\nthus allowing the application of many queueing theory results that yield closed\nform expressions. As we consider a case where different resources can satisfy\ndifferent numbers of users, we also generate new results for bulk service\nqueues. We also consider bidirectional policies where there are no directional\nrestrictions on resource allocation and develop an algorithm for computing the\noptimal assignment which is more efficient than known algorithms in the\nliterature when there are more resources than users. Numerical evaluation of\nperformance of unidirectional and bidirectional allocation schemes yields\ndesign guidelines beneficial for resource placement. \\np{Finally, we present a\nheuristic algorithm, which leverages the optimal dynamic programming scheme for\none-dimensional inputs to obtain approximate solutions to the optimal\nassignment problem for the two-dimensional scenario and empirically yields\nrequest distances within a constant factor of the optimal solution.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.04893v1"
    },
    {
        "title": "RL-QN: A Reinforcement Learning Framework for Optimal Control of\n  Queueing Systems",
        "authors": [
            "Bai Liu",
            "Qiaomin Xie",
            "Eytan Modiano"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  With the rapid advance of information technology, network systems have become\nincreasingly complex and hence the underlying system dynamics are often unknown\nor difficult to characterize. Finding a good network control policy is of\nsignificant importance to achieve desirable network performance (e.g., high\nthroughput or low delay). In this work, we consider using model-based\nreinforcement learning (RL) to learn the optimal control policy for queueing\nnetworks so that the average job delay (or equivalently the average queue\nbacklog) is minimized. Traditional approaches in RL, however, cannot handle the\nunbounded state spaces of the network control problem. To overcome this\ndifficulty, we propose a new algorithm, called Reinforcement Learning for\nQueueing Networks (RL-QN), which applies model-based RL methods over a finite\nsubset of the state space, while applying a known stabilizing policy for the\nrest of the states. We establish that the average queue backlog under RL-QN\nwith an appropriately constructed subset can be arbitrarily close to the\noptimal result. We evaluate RL-QN in dynamic server allocation, routing and\nswitching problems. Simulation results show that RL-QN minimizes the average\nqueue backlog effectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.07401v2"
    },
    {
        "title": "Zero Queueing for Multi-Server Jobs",
        "authors": [
            "Weina Wang",
            "Qiaomin Xie",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Cloud computing today is dominated by multi-server jobs. These are jobs that\nrequest multiple servers simultaneously and hold onto all of these servers for\nthe duration of the job. Multi-server jobs add a lot of complexity to the\ntraditional one-job-per-server model: an arrival might not \"fit\" into the\navailable servers and might have to queue, blocking later arrivals and leaving\nservers idle. From a queueing perspective, almost nothing is understood about\nmulti-server job queueing systems; even understanding the exact stability\nregion is a very hard problem.\n  In this paper, we investigate a multi-server job queueing model under scaling\nregimes where the number of servers in the system grows. Specifically, we\nconsider a system with multiple classes of jobs, where jobs from different\nclasses can request different numbers of servers and have different service\ntime distributions, and jobs are served in first-come-first-served order. The\nmulti-server job model opens up new scaling regimes where both the number of\nservers that a job needs and the system load scale with the total number of\nservers. Within these scaling regimes, we derive the first results on\nstability, queueing probability, and the transient analysis of the number of\njobs in the system for each class. In particular we derive sufficient\nconditions for zero queueing. Our analysis introduces a novel way of extracting\ninformation from the Lyapunov drift, which can be applicable to a broader scope\nof problems in queueing systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.10521v2"
    },
    {
        "title": "Pinpointing the Memory Behaviors of DNN Training",
        "authors": [
            "Jiansong Li",
            "Xiao Dong",
            "Guangli Li",
            "Peng Zhao",
            "Xueying Wang",
            "Xiaobing Chen",
            "Xianzhi Yu",
            "Yongxin Yang",
            "Zihan Jiang",
            "Wei Cao",
            "Lei Liu",
            "Xiaobing Feng"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The training of deep neural networks (DNNs) is usually memory-hungry due to\nthe limited device memory capacity of DNN accelerators. Characterizing the\nmemory behaviors of DNN training is critical to optimize the device memory\npressures. In this work, we pinpoint the memory behaviors of each device memory\nblock of GPU during training by instrumenting the memory allocators of the\nruntime system. Our results show that the memory access patterns of device\nmemory blocks are stable and follow an iterative fashion. These observations\nare useful for the future optimization of memory-efficient training from the\nperspective of raw memory access patterns.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.00258v1"
    },
    {
        "title": "DJXPerf: Identifying Memory Inefficiencies via Object-centric Profiling\n  for Java",
        "authors": [
            "Bolun Li",
            "Pengfei Su",
            "Milind Chabbi",
            "Shuyin Jiao",
            "Xu Liu"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Java is the \"go-to\" programming language choice for developing scalable\nenterprise cloud applications. In such systems, even a few percent CPU time\nsavings can offer a significant competitive advantage and cost saving. Although\nperformance tools abound in Java, those that focus on the data locality in the\nmemory hierarchy are rare.\n  In this paper, we present DJXPerf, a lightweight, object-centric memory\nprofiler for Java, which associates memory-hierarchy performance metrics (e.g.,\ncache/TLB misses) with Java objects. DJXPerf uses statistical sampling of\nhardware performance monitoring counters to attribute metrics to not only\nsource code locations but also Java objects. DJXPerf presents Java object\nallocation contexts combined with their usage contexts and presents them\nordered by the poor locality behaviors. DJXPerf's performance measurement,\nobject attribution, and presentation techniques guide optimizing object\nallocation, layout, and access patterns. DJXPerf incurs only ~8% runtime\noverhead and ~5% memory overhead on average, requiring no modifications to\nhardware, OS, Java virtual machine, or application source code, which makes it\nattractive to use in production. Guided by DJXPerf, we study and optimize a\nnumber of Java and Scala programs, including well-known benchmarks and\nreal-world applications, and demonstrate significant speedups.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.03388v1"
    },
    {
        "title": "PPT-Multicore: Performance Prediction of OpenMP applications using Reuse\n  Profiles and Analytical Modeling",
        "authors": [
            "Atanu Barai",
            "Yehia Arafa",
            "Abdel-Hameed Badawy",
            "Gopinath Chennupati",
            "Nandakishore Santhi",
            "Stephan Eidenbenz"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  We present PPT-Multicore, an analytical model embedded in the Performance\nPrediction Toolkit (PPT) to predict parallel application performance running on\na multicore processor. PPT-Multicore builds upon our previous work towards a\nmulticore cache model. We extract LLVM basic block labeled memory trace using\nan architecture-independent LLVM-based instrumentation tool only once in an\napplication's lifetime. The model uses the memory trace and other parameters\nfrom an instrumented sequentially executed binary. We use a probabilistic and\ncomputationally efficient reuse profile to predict the cache hit rates and\nruntimes of OpenMP programs' parallel sections. We model Intel's Broadwell,\nHaswell, and AMD's Zen2 architectures and validate our framework using\ndifferent applications from PolyBench and PARSEC benchmark suites. The results\nshow that PPT-Multicore can predict cache hit rates with an overall average\nerror rate of 1.23% while predicting the runtime with an error rate of 9.08%.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.05102v1"
    },
    {
        "title": "NekRS, a GPU-Accelerated Spectral Element Navier-Stokes Solver",
        "authors": [
            "Paul Fischer",
            "Stefan Kerkemeier",
            "Misun Min",
            "Yu-Hsiang Lan",
            "Malachi Phillips",
            "Thilina Rathnayake",
            "Elia Merzari",
            "Ananias Tomboulides",
            "Ali Karakus",
            "Noel Chalmers",
            "Tim Warburton"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The development of NekRS, a GPU-oriented thermal-fluids simulation code based\non the spectral element method (SEM) is described. For performance portability,\nthe code is based on the open concurrent compute abstraction and leverages\nscalable developments in the SEM code Nek5000 and in libParanumal, which is a\nlibrary of high-performance kernels for high-order discretizations and\nPDE-based miniapps. Critical performance sections of the Navier-Stokes time\nadvancement are addressed. Performance results on several platforms are\npresented, including scaling to 27,648 V100s on OLCF Summit, for calculations\nof up to 60B gridpoints.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.05829v1"
    },
    {
        "title": "Comparison of the FCFS and PS discipline in Redundancy Systems",
        "authors": [
            "Youri Raaijmakers"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  We consider the c.o.c. redundancy system with $N$ parallel servers where\nincoming jobs are immediately replicated to $d$ servers chosen uniformly at\nrandom (without replacement). A job finishes service as soon as the first\nreplica is completed, after which all the remaining replicas are abandoned. We\ncompare the performance of the first-come first-served (FCFS) and\nprocessor-sharing (PS) discipline based on the stability condition, the tail\nbehavior of the latency and the expected latency.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.07458v1"
    },
    {
        "title": "Age of information without service preemption",
        "authors": [
            "George Kesidis",
            "Takis Konstantopoulos",
            "Michael A. Zazanis"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  When designing a message transmission system, from the point of view of\nmaking sure that the information transmitted is as fresh as possible, two rules\nof thumb seem reasonable: use small buffers and adopt a last-in-first-out\npolicy. In this paper, we measure freshness of information using the \"age of\ninformation\" performance measure. Considering it as a stochastic process\noperating in a stationary regime, we compute not just the first moment but the\nwhole marginal distribution of the age of information (something important in\napplications) for two well-performing systems. In neither case do we allow for\npreemption of the message being processed because this may be difficult to\nimplement in practice. We assume that the arrival process is Poisson and that\nthe messages have independent sizes (service times) with common distribution.\nWe use Palm and Markov-renewal theory to derive explicit results for Laplace\ntransforms. In particular, this approach can be used to analyze more complex\nlast-in-first-out systems with larger buffer sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.08050v3"
    },
    {
        "title": "Asymptotic analysis of the sojourn time of a batch in an $M^{[X]}/M/1$\n  Processor Sharing Queue",
        "authors": [
            "Fabrice Guillemin",
            "Alain Simonian",
            "Ridha Nasri",
            "Veronica Quintuna Rodriguez"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  In this paper, we exploit results obtained in an earlier study for the\nLaplace transform of the sojourn time $\\Omega$ of an entire batch in the\n$M^{[X]}/M/1$ Processor Sharing (PS) queue in order to derive the asymptotic\nbehavior of the complementary probability distribution function of this random\nvariable, namely the behavior of $P(\\Omega>x)$ when $x$ tends to infinity. We\nprecisely show that up to a multiplying factor, the behavior of $P(\\Omega>x)$\nfor large $x$ is of the same order of magnitude as $P(\\omega>x)$, where\n$\\omega$ is the sojourn time of an arbitrary job is the system. From a\npractical point of view, this means that if a system has to be dimensioned to\nguarantee processing time for jobs then the system can also guarantee\nprocessing times for entire batches by introducing a marginal amount of\nprocessing capacity.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.09273v1"
    },
    {
        "title": "Stability and Optimization of Speculative Queueing Networks",
        "authors": [
            "Jonatha Anselmi",
            "Neil Walton"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  We provide a queueing-theoretic framework for job replication schemes based\non the principle \"\\emph{replicate a job as soon as the system detects it as a\n\\emph{straggler}}\". This is called job \\emph{speculation}. Recent works have\nanalyzed {replication} on arrival, which we refer to as \\emph{replication}.\nReplication is motivated by its implementation in Google's BigTable. However,\nsystems such as Apache Spark and Hadoop MapReduce implement speculative job\nexecution. The performance and optimization of speculative job execution is not\nwell understood. To this end, we propose a queueing network model for load\nbalancing where each server can speculate on the execution time of a job.\nSpecifically, each job is initially assigned to a single server by a frontend\ndispatcher. Then, when its execution begins, the server sets a timeout. If the\njob completes before the timeout, it leaves the network, otherwise the job is\nterminated and relaunched or resumed at another server where it will complete.\nWe provide a necessary and sufficient condition for the stability of\nspeculative queueing networks with heterogeneous servers, general job sizes and\nscheduling disciplines. We find that speculation can increase the stability\nregion of the network when compared with standard load balancing models and\nreplication schemes. We provide general conditions under which timeouts\nincrease the size of the stability region and derive a formula for the optimal\nspeculation time, i.e., the timeout that minimizes the load induced through\nspeculation. We compare speculation with redundant-$d$ and\nredundant-to-idle-queue-$d$ rules under an $S\\& X$ model. For light loaded\nsystems, redundancy schemes provide better response times. However, for\nmoderate to heavy loadings, redundancy schemes can lose capacity and have\nmarkedly worse response times when compared with a speculative scheme.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.10426v2"
    },
    {
        "title": "Age of information distribution under dynamic service preemption",
        "authors": [
            "George Kesidis",
            "Takis Konstantopoulos",
            "Michael A. Zazanis"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Age of Information (AoI) has emerged as an important quality-of-service\nmeasure for applications that prioritize delivery of the freshest information,\ne.g., virtual or augmented reality over mobile devices and wireless sensor\nnetworks used in the control of cyber-physical systems. We derive the Laplace\ntransform of the stationary AoI for the M/GI/1/2 system with a \"dynamic\"\nservice preemption and pushout policy depending on the existing service time of\nthe in-service message. Thus, our system generalizes both the static M/GI/1/2\nqueue-pushout system without service preemption and the M/GI/1/1 bufferless\nsystem with service preemption - two systems considered to provide very good\nAoI performance. Based on our analysis, for a service-time distribution that is\na mixture of deterministic and exponential, we numerically show that the\ndynamic policy has lower mean AoI than that of these two static policies and\nalso that of the well studied M/GI/1/1 blocking system.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.11393v3"
    },
    {
        "title": "Nudge: Stochastically Improving upon FCFS",
        "authors": [
            "Isaac Grosof",
            "Kunhe Yang",
            "Ziv Scully",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The First-Come First-Served (FCFS) scheduling policy is the most popular\nscheduling algorithm used in practice. Furthermore, its usage is theoretically\nvalidated: for light-tailed job size distributions, FCFS has weakly optimal\nasymptotic tail of response time. But what if we don't just care about the\nasymptotic tail? What if we also care about the 99th percentile of response\ntime, or the fraction of jobs that complete in under one second? Is FCFS still\nbest? Outside of the asymptotic regime, only loose bounds on the tail of FCFS\nare known, and optimality is completely open.\n  In this paper, we introduce a new policy, Nudge, which is the first policy to\nprovably stochastically improve upon FCFS. We prove that Nudge simultaneously\nimproves upon FCFS at every point along the tail, for light-tailed job size\ndistributions. As a result, Nudge outperforms FCFS for every moment and every\npercentile of response time. Moreover, Nudge provides a multiplicative\nimprovement over FCFS in the asymptotic tail. This resolves a long-standing\nopen problem by showing that, counter to previous conjecture, FCFS is not\nstrongly asymptotically optimal.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.01492v1"
    },
    {
        "title": "Age of Information for Small Buffer Systems",
        "authors": [
            "George Kesidis",
            "Takis Konstantopoulos",
            "Michael Zazanis"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Consider a message processing system whose objective is to produce the most\ncurrent information as measured by the quantity known as \"age of information\".\nWe have argued in previous papers that if we are allowed to design the message\nprocessing policy ad libitum, we should keep a small buffer and operate\naccording to a LIFO policy. In this small note we provide an analysis for the\nAoI of the P_m system which uses a buffer of size m, a single server, operating\nwithout service preemption and in a LIFO manner for stored messages. Analytical\nexpressions for the mean (or even distribution) of the AoI in steady-state are\npossible but with the aid computer algebra. We explain the the analysis for\nm=3.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.08473v1"
    },
    {
        "title": "AutoTune: Improving End-to-end Performance and Resource Efficiency for\n  Microservice Applications",
        "authors": [
            "Michael Alan Chang",
            "Aurojit Panda",
            "Hantao Wang",
            "Yuancheng Tsai",
            "Rahul Balakrishnan",
            "Scott Shenker"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Most large web-scale applications are now built by composing collections\n(from a few up to 100s or 1000s) of microservices. Operators need to decide how\nmany resources are allocated to each microservice, and these allocations can\nhave a large impact on application performance. Manually determining\nallocations that are both cost-efficient and meet performance requirements is\nchallenging, even for experienced operators. In this paper we present AutoTune,\nan end-to-end tool that automatically minimizes resource utilization while\nmaintaining good application performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2106.10334v2"
    },
    {
        "title": "On the Quantum Performance Evaluation of Two Distributed Quantum\n  Architectures",
        "authors": [
            "Gayane Vardoyan",
            "Matthew Skrzypczyk",
            "Stephanie Wehner"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Distributed quantum applications impose requirements on the quality of the\nquantum states that they consume. When analyzing architecture implementations\nof quantum hardware, characterizing this quality forms an important factor in\nunderstanding their performance. Fundamental characteristics of quantum\nhardware lead to inherent tradeoffs between the quality of states and\ntraditional performance metrics such as throughput. Furthermore, any real-world\nimplementation of quantum hardware exhibits time-dependent noise that degrades\nthe quality of quantum states over time. Here, we study the performance of two\npossible architectures for interfacing a quantum processor with a quantum\nnetwork. The first corresponds to the current experimental state of the art in\nwhich the same device functions both as a processor and a network device. The\nsecond corresponds to a future architecture that separates these two functions\nover two distinct devices. We model these architectures as Markov chains and\ncompare their quality of executing quantum operations and producing entangled\nquantum states as functions of their memory lifetimes, as well as the time that\nit takes to perform various operations within each architecture. As an\nillustrative example, we apply our analysis to architectures based on\nNitrogen-Vacancy centers in diamond, where we find that for present-day device\nparameters one architecture is more suited to computation-heavy applications,\nand the other for network-heavy ones. Besides the detailed study of these\narchitectures, a novel contribution of our work are several formulas that\nconnect an understanding of waiting time distributions to the decay of quantum\nquality over time for the most common noise models employed in quantum\ntechnologies. This provides a valuable new tool for performance evaluation\nexperts, and its applications extend beyond the two architectures studied in\nthis work.\n",
        "pdf_link": "http://arxiv.org/pdf/2107.12246v2"
    },
    {
        "title": "Theoretical Analysis and Evaluation of NoCs with Weighted Round-Robin\n  Arbitration",
        "authors": [
            "Sumit K. Mandal",
            "Jie Tong",
            "Raid Ayoub",
            "Michael Kishinevsky",
            "Ahmed Abousamra",
            "Umit Y. Ogras"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Fast and accurate performance analysis techniques are essential in early\ndesign space exploration and pre-silicon evaluations, including software\neco-system development. In particular, on-chip communication continues to play\nan increasingly important role as the many-core processors scale up. This paper\npresents the first performance analysis technique that targets networks-on-chip\n(NoCs) that employ weighted round-robin (WRR) arbitration. Besides fairness,\nWRR arbitration provides flexibility in allocating bandwidth proportionally to\nthe importance of the traffic classes, unlike basic round-robin and\npriority-based arbitration. The proposed approach first estimates the effective\nservice time of the packets in the queue due to WRR arbitration. Then, it uses\nthe effective service time to compute the average waiting time of the packets.\nNext, we incorporate a decomposition technique to extend the analytical model\nto handle NoC of any size. The proposed approach achieves less than 5% error\nwhile executing real applications and 10% error under challenging synthetic\ntraffic with different burstiness levels.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.09534v2"
    },
    {
        "title": "On the Representation of Correlated Exponential Distributions by Phase\n  Type Distributions",
        "authors": [
            "Peter Buchholz"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  In this paper we present results for bivariate exponential distributions\nwhich are represented by phase type distributions. The paper extends results\nfrom previous publications [5, 14] on this topic by introducing new\nrepresentations that require a smaller number of phases to reach some\ncorrelation coefficient and introduces different ways to describe correlation\nbetween exponentially distributed random variables. Furthermore, it is shown\nhow Markovian Arrival Processes (MAPs) with exponential marginal distribution\ncan be generated from the phase type representations of exponential\ndistributions and how the results for exponential distributions can be applied\nto define correlated hyperexponential or Erlang distributions. As application\nexamples we analyze two queueing models with correlated inter-arrival and\nservice times.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.12223v1"
    },
    {
        "title": "Leveraging Transprecision Computing for Machine Vision Applications at\n  the Edge",
        "authors": [
            "Umar Ibrahim Minhas",
            "Lev Mukhanov",
            "Georgios Karakonstantis",
            "Hans Vandierendonck",
            "Roger Woods"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Machine vision tasks present challenges for resource constrained edge\ndevices, particularly as they execute multiple tasks with variable workloads. A\nrobust approach that can dynamically adapt in runtime while maintaining the\nmaximum quality of service (QoS) within resource constraints, is needed. The\npaper presents a lightweight approach that monitors the runtime workload\nconstraint and leverages accuracy-throughput trade-off. Optimisation techniques\nare included which find the configurations for each task for optimal accuracy,\nenergy and memory and manages transparent switching between configurations. For\nan accuracy drop of 1%, we show a 1.6x higher achieved frame processing rate\nwith further improvements possible at lower accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.12914v1"
    },
    {
        "title": "Performance Analysis of CP2K Code for Ab Initio Molecular Dynamics",
        "authors": [
            "Dewi Yokelson",
            "Nikolay V. Tkachenko",
            "Robert Robey",
            "Ying Wai Li",
            "Pavel A. Dub"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Using a realistic molecular catalyst system, we conduct scaling studies of ab\ninitio molecular dynamics simulations using the CP2K code on both Intel Xeon\nCPU and NVIDIA V100 GPU architectures. We explore using process placement and\naffinity to gain additional performance improvements. We also use statistical\nmethods to understand performance changes in spite of the variability in\nruntime for each molecular dynamics timestep. We found ideal conditions for CPU\nruns included at least four MPI ranks per node, bound evenly across each\nsocket, and fully utilizing processing cores with one OpenMP thread per core,\nno benefit was shown from reserving cores for the system. The CPU-only\nsimulations scaled at 70% or more of the ideal scaling up to 10 compute nodes,\nafter which the returns began to diminish more quickly. Simulations on a single\n40-core node with two NVIDIA V100 GPUs for acceleration achieved over 3.7x\nspeedup compared to the fastest single 36-core node CPU-only version, and\nshowed 13% speedup over the fastest time we achieved across five CPU-only\nnodes.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.04536v1"
    },
    {
        "title": "Sharp Waiting-Time Bounds for Multiserver Jobs",
        "authors": [
            "Yige Hong",
            "Weina Wang"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Multiserver jobs, which are jobs that occupy multiple servers simultaneously\nduring service, are prevalent in today's computing clusters. But little is\nknown about the delay performance of systems with multiserver jobs. We consider\nqueueing models for multiserver jobs in scaling regimes where the system load\nbecomes heavy and meanwhile the total number of servers in the system and the\nnumber of servers that a job needs become large. Prior work has derived upper\nbounds on the queueing probability in this scaling regime. However, without\nproper lower bounds, the existing results cannot be used to differentiate\nbetween policies. In this paper, we study the delay performance by establishing\nsharp bounds on the mean waiting time of multiserver jobs, where the waiting\ntime of a job is the time spent in queueing rather than in service. We first\ncharacterize the exact order of the mean waiting time under the\nFirst-Come-First-Serve (FCFS) policy. Then we prove a lower bound on the mean\nwaiting time of all policies, which has an order gap with the mean waiting time\nunder FCFS. Finally, we show that the lower bound is achievable under a\npriority policy that we call Smallest-Need-First (SNF).\n",
        "pdf_link": "http://arxiv.org/pdf/2109.05343v3"
    },
    {
        "title": "When Does the Gittins Policy Have Asymptotically Optimal Response Time\n  Tail?",
        "authors": [
            "Ziv Scully",
            "Lucas van Kreveld"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  We consider scheduling in the M/G/1 queue with unknown job sizes. It is known\nthat the Gittins policy minimizes mean response time in this setting. However,\nthe behavior of the tail of response time under Gittins is poorly understood,\neven in the large-response-time limit. Characterizing Gittins's asymptotic tail\nbehavior is important because if Gittins has optimal tail asymptotics, then it\nsimultaneously provides optimal mean response time and good tail performance.\n  In this work, we give the first comprehensive account of Gittins's asymptotic\ntail behavior. For heavy-tailed job sizes, we find that Gittins always has\nasymptotically optimal tail. The story for light-tailed job sizes is less\nclear-cut: Gittins's tail can be optimal, pessimal, or in between. To remedy\nthis, we show that a modification of Gittins avoids pessimal tail behavior\nwhile achieving near-optimal mean response time.\n",
        "pdf_link": "http://arxiv.org/pdf/2110.06326v3"
    },
    {
        "title": "Mean Field and Refined Mean Field Approximations for Heterogeneous\n  Systems: It Works!",
        "authors": [
            "Sebastian Allmeier",
            "Nicolas Gast"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Mean field approximation is a powerful technique to study the performance of\nlarge stochastic systems represented as $n$ interacting objects. Applications\ninclude load balancing models, epidemic spreading, cache replacement policies,\nor large-scale data centers. Mean field approximation is asymptotically exact\nfor systems composed of $n$ homogeneous objects under mild conditions. In this\npaper, we study what happens when objects are heterogeneous. This can represent\nservers with different speeds or contents with different popularities. We\ndefine an interaction model that allows obtaining asymptotic convergence\nresults for stochastic systems with heterogeneous object behavior, and show\nthat the error of the mean field approximation is of order $O(1/n)$. More\nimportantly, we show how to adapt the refined mean field approximation,\ndeveloped by Gast et al. 2019, and show that the error of this approximation is\nreduced to $O(1/n^2)$. To illustrate the applicability of our result, we\npresent two examples. The first addresses a list-based cache replacement model\nRANDOM($m$), which is an extension of the RANDOM policy. The second is a\nheterogeneous supermarket model. These examples show that the proposed\napproximations are computationally tractable and very accurate. They also show\nthat for moderate system sizes ($n\\approx30$) the refined mean field\napproximation tends to be more accurate than simulations for any reasonable\nsimulation time.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.01594v1"
    },
    {
        "title": "Optimisation of job scheduling for supercomputers with burst buffers",
        "authors": [
            "Jan Kopanski"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The ever-increasing gap between compute and I/O performance in HPC platforms,\ntogether with the development of novel NVMe storage devices (NVRAM), led to the\nemergence of the burst buffer concept - an intermediate persistent storage\nlayer logically positioned between random-access main memory and a parallel\nfile system. Since the appearance of this technology, numerous supercomputers\nhave been equipped with burst buffers exploring various architectures. Despite\nthe development of real-world architectures as well as research concepts,\nResource and Job Management Systems, such as Slurm, provide only marginal\nsupport for scheduling jobs with burst buffer requirements. This research is\nprimarily motivated by the alerting observation that burst buffers are omitted\nfrom reservations in the procedure of backfilling in existing job schedulers.\nIn this dissertation, we forge a detailed supercomputer simulator based on\nBatsim and SimGrid, which is capable of simulating I/O contention and I/O\ncongestion effects. Due to the lack of publicly available workloads with burst\nbuffer requests, we create a burst buffer request distribution model derived\nfrom Parallel Workload Archive logs. We investigate the impact of burst buffer\nreservations on the overall efficiency of online job scheduling for canonical\nalgorithms: First-Come-First-Served (FCFS) and Shortest-Job-First (SJF)\nEASY-backfilling. Our results indicate that the lack of burst buffer\nreservations in backfilling may significantly deteriorate the performance of\nscheduling. [...] Furthermore, this lack of reservations may cause the\nstarvation of medium-size and wide jobs. Finally, we propose a\nburst-buffer-aware plan-based scheduling algorithm with simulated annealing\noptimisation, which improves the mean waiting time by over 20% and mean bounded\nslowdown by 27% compared to the SJF EASY-backfilling.\n",
        "pdf_link": "http://arxiv.org/pdf/2111.10200v1"
    },
    {
        "title": "A Foreground-Background queueing model with speed or capacity modulation",
        "authors": [
            "Andrea Marin",
            "Isi Mitrani"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The models studied in the steady state involve two queues which are served\neither by a single server whose speed depends on the number of jobs present, or\nby several parallel servers whose number may be controlled dynamically. Job\nservice times have a two-phase Coxian distribution and the second phase is\ngiven lower priority than the first. The trade-offs between holding costs and\nenergy consumption costs are examined by means of a suitable cost functions.\nTwo different two-dimensional Markov process are solved exactly. The solutions\nare used in several numerical experiments. Some counter-intuitive results are\nobserved.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.01058v1"
    },
    {
        "title": "Public Release and Validation of SPEC CPU2017 PinPoints",
        "authors": [
            "Haiyang Han",
            "Nikos Hardavellas"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Phase-based statistical sampling methods such as SimPoints have proven to be\neffective at dramatically reducing the long time for architectural simulators\nto run large workloads such as SPEC CPU2017. However, generating and validating\nthem is a long and tenuous process. While checkpoints of program phases, or\n\"pinballs\", of SPEC CPU2017 have been collected by other researchers and shared\nwith the research community, they are outdated and produce errors when used\nwith the latest versions of the Sniper architectural simulator. To facilitate\nour own research as well as contribute to the community, we collect and\nvalidate our own pinballs for the SPEC CPU2017 SPECspeed suite and release them\nto the public domain. In this work we document our methodology, the hardware\nand software details of the collection process, and our validation results. In\nterms of CPI, our pinballs have an average error rate of 12% when compared with\nthe native whole-program benchmark execution.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.06981v1"
    },
    {
        "title": "Distilling the Real Cost of Production Garbage Collectors",
        "authors": [
            "Zixian Cai",
            "Stephen M. Blackburn",
            "Michael D. Bond",
            "Martin Maas"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Abridged abstract: despite the long history of garbage collection (GC) and\nits prevalence in modern programming languages, there is surprisingly little\nclarity about its true cost. Without understanding their cost, crucial\ntradeoffs made by garbage collectors (GCs) go unnoticed. This can lead to\nmisguided design constraints and evaluation criteria used by GC researchers and\nusers, hindering the development of high-performance, low-cost GCs. In this\npaper, we develop a methodology that allows us to empirically estimate the cost\nof GC for any given set of metrics. By distilling out the explicitly\nidentifiable GC cost, we estimate the intrinsic application execution cost\nusing different GCs. The minimum distilled cost forms a baseline. Subtracting\nthis baseline from the total execution costs, we can then place an empirical\nlower bound on the absolute costs of different GCs. Using this methodology, we\nstudy five production GCs in OpenJDK 17, a high-performance Java runtime. We\nmeasure the cost of these collectors, and expose their respective key\nperformance tradeoffs. We find that with a modestly sized heap, production GCs\nincur substantial overheads across a diverse suite of modern benchmarks,\nspending at least 7-82% more wall-clock time and 6-92% more CPU cycles relative\nto the baseline cost. We show that these costs can be masked by concurrency and\ngenerous provisioning of memory/compute. In addition, we find that newer\nlow-pause GCs are significantly more expensive than older GCs, and,\nsurprisingly, sometimes deliver worse application latency than stop-the-world\nGCs. Our findings reaffirm that GC is by no means a solved problem and that a\nlow-cost, low-latency GC remains elusive. We recommend adopting the\ndistillation methodology together with a wider range of cost metrics for future\nGC evaluations.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.07880v2"
    },
    {
        "title": "FSpGEMM: An OpenCL-based HPC Framework for Accelerating General Sparse\n  Matrix-Matrix Multiplication on FPGAs",
        "authors": [
            "Erfan Bank Tavakoli",
            "Michael Riera",
            "Masudul Hassan Quraishi",
            "Fengbo Ren"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  General sparse matrix-matrix multiplication (SpGEMM) is an integral part of\nmany scientific computing, high-performance computing (HPC), and graph analytic\napplications. This paper presents a new compressed sparse vector (CSV) format\nfor representing sparse matrices and FSpGEMM, an OpenCL-based HPC framework for\naccelerating general sparse matrix-matrix multiplication on FPGAs. The proposed\nFSpGEMM framework includes an FPGA kernel implementing a throughput-optimized\nhardware architecture based on Gustavson's algorithm and a host program\nimplementing pre-processing functions for converting input matrices to the CSV\nformat tailored for the proposed architecture. FSpGEMM utilizes a new buffering\nscheme tailored to Gustavson's algorithm. We compare FSpGEMM implemented on an\nIntel Arria 10 GX FPGA development board with Intel Math Kernel Library (MKL)\nimplemented on an Intel Xeon E5-2637 CPU and cuSPARSE on an NVIDIA GTX TITAN X\nGPU, respectively, for multiplying a set of sparse matrices selected from\nSuiteSparse Matrix Collection. The experiment results show that the proposed\nFSpGEMM solution achieves on average 4.9x and 1.7x higher performance with\n31.9x and 13.1x lower energy consumption per SpGEMM computation than the CPU\nand GPU implementations, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.10037v1"
    },
    {
        "title": "Using Silent Writes in Low-Power Traffic-Aware ECC",
        "authors": [
            "Mostafa Kishani",
            "Amirali Baniasadi",
            "Hossein Pedram"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Using Error Detection Code (EDC) and Error Correction Code (ECC) is a\nnoteworthy way to increase cache memories robustness against soft errors. EDC\nenables detecting errors in cache memory while ECC is used to correct erroneous\ncache blocks. ECCs are often costly as they impose considerable area and energy\noverhead on cache memory. Reducing this overhead has been the subject of many\nstudies. In particular, a previous study has suggested mapping ECC to the main\nmemory at the expense of high cache traffic and energy. A major source of this\nexcessive traffic and energy is the high frequency of cache writes. In this\nwork, we show that a significant portion of cache writes are silent, i.e., they\nwrite the same data already existing. We build on this observation and\nintroduce Traffic-aware ECC (or simply TCC). TCC detects silent writes by an\nefficient mechanism. Once such writes are detected updating their ECC is\navoided effectively reducing L2 cache traffic and access frequency. Using our\nsolution, we reduce L2 cache access frequency by 8% while maintaining\nperformance. We reduce L2 cache dynamic and overall cache energy by up to 32%\nand 8%, respectively. Furthermore, TCC reduces L2 cache miss rate by 3%.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.12667v1"
    },
    {
        "title": "Calipers: A Criticality-aware Framework for Modeling Processor\n  Performance",
        "authors": [
            "Hossein Golestani",
            "Rathijit Sen",
            "Vinson Young",
            "Gagan Gupta"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Computer architecture design space is vast and complex. Tools are needed to\nexplore new ideas and gain insights quickly, with low efforts and at a desired\naccuracy. We propose Calipers, a criticality-based framework to model key\nabstractions of complex architectures and a program's execution using dynamic\nevent-dependence graphs. By applying graph algorithms, Calipers can track\ninstruction and event dependencies, compute critical paths, and analyze\narchitecture bottlenecks. By manipulating the graph, Calipers enables\narchitects to investigate a wide range of Instruction Set Architecture (ISA)\nand microarchitecture design choices/\"what-if\" scenarios during both early- and\nlate-stage design space exploration without recompiling and rerunning the\nprogram. Calipers can model in-order and out-of-order microarchitectures,\nstructural hazards, and different types of ISAs, and can evaluate multiple\nideas in a single run. Modeling algorithms are described in detail.\n  We apply Calipers to explore and gain insights in complex microarchitectural\nand ISA ideas for RISC and EDGE processors, at lower effort than cycle-accurate\nsimulators and with comparable accuracy. For example, among a variety of\ninvestigations presented in the paper, experiments show that targeting only a\nfraction of critical loads can help realize most benefits of value prediction.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.05884v1"
    },
    {
        "title": "Optimization of Traffic Control in MMAP[k]/PH[k]/S Catastrophic Queueing\n  Model with PH Retrial Times and Preemptive Repeat Policy",
        "authors": [
            "Raina Raj",
            "Vidyottama Jain"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  The presented study elaborates a multi-server catastrophic retrial queueing\nmodel considering preemptive repeat priority policy with phase-type (PH)\ndistributed retrial times. For the sake of comprehension, the scenario of model\noperation prior and later to the occurrence of the disaster is referred to as\nthe normal scenario and as the catastrophic scenario, respectively. In both\nscenarios, the arrival and service processes of all types of calls follow\nmarked Markovian arrival process (MMAP) and PH distribution with distinct\nparameters, respectively. In the normal scenario, the incoming heterogeneous\ncalls are categorized as handoff calls and new calls. An arriving new call will\nbe blocked when all the channels are occupied, and consequently, will join the\norbit (virtual space) of infinite capacity. From the orbit, the blocked new\ncall can either retry for the service or exit the system following PH\ndistribution. Whereas, an arriving handoff call is given preemptive repeat\npriority over a new call in service when all the channels are occupied and at\nleast one of the channel is occupied with a new call otherwise the handoff call\nis dropped, and consequently, this preempted new call will join the orbit. In\nthe catastrophic scenario, when a disaster causes the shut down of the entire\nsystem and failure of all functioning channels, a set of backup channels is\nquickly deployed to restore services. The Markov chain's ergodicity criteria\nare established by demonstrating that it belongs to the class of asymptotically\nquasi-Toeplitz Markov chains (AQTMC). For the approximate computation of the\nstationary distribution, a new approach is developed.\n  An optimization problem to obtain optimal value of total number of backup\nchannels has been formulated and dealt by employing non dominated sorting\ngenetic algorithm-II (NSGA-II) approach.\n",
        "pdf_link": "http://arxiv.org/pdf/2201.08182v1"
    },
    {
        "title": "Analysis of Two-Station Polling Queues with Setups using Continuous Time\n  Markov Chain",
        "authors": [
            "Ravi Suman",
            "Ananth Krishnamurthy"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  The paper analyzes the performance of tandem network of polling queue with\nsetups. For a system with two-products and two-stations, we propose a new\napproach based on a partially-collapsible state-space characterization to\nreduce state-space complexity. In this approach, the size of the state-space is\nvaried depending on the information needed to determine buffer levels and\nwaiting times. We evaluate system performance under different system setting\nand comment on the numerical accuracy of the approach as well as provide\nmanagerial insights. Numerical results show that approach yields reliable\nestimates of the performance measures. We also show how product and station\nasymmetry significantly affect the systems performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.10045v1"
    },
    {
        "title": "Queue input estimation from discrete workload observations",
        "authors": [
            "Liron Ravner"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  This note considers the problem of statistical inference of the parameters of\nthe input process to a queue from periodic workload observations. The main\nfocus is the open problem of constructing statistically efficient estimators\nfor a given observation scheme, in the sense of minimizing the asymptotic\nvariance of the estimation error.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.10750v1"
    },
    {
        "title": "Shisha: Online scheduling of CNN pipelines on heterogeneous\n  architectures",
        "authors": [
            "Pirah Noor Soomro",
            "Mustafa Abduljabbar",
            "Jeronimo Castrillon",
            "Miquel Pericàs"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Chiplets have become a common methodology in modern chip design. Chiplets\nimprove yield and enable heterogeneity at the level of cores, memory subsystem\nand the interconnect. Convolutional Neural Networks (CNNs) have high\ncomputational, bandwidth and memory capacity requirements owing to the\nincreasingly large amount of weights. Thus to exploit chiplet-based\narchitectures, CNNs must be optimized in terms of scheduling and workload\ndistribution among computing resources. We propose Shisha, an online approach\nto generate and schedule parallel CNN pipelines on chiplet architectures.\nShisha targets heterogeneity in compute performance and memory bandwidth and\ntunes the pipeline schedule through a fast online exploration technique. We\ncompare Shisha with Simulated Annealing, Hill Climbing and Pipe-Search. On\naverage, the convergence time is improved by ~35x in Shisha compared to other\nexploration algorithms. Despite the quick exploration, Shisha's solution is\noften better than that of other heuristic exploration algorithms.\n",
        "pdf_link": "http://arxiv.org/pdf/2202.11575v2"
    },
    {
        "title": "Machine Learning for CUDA+MPI Design Rules",
        "authors": [
            "Carl Pearson",
            "Aurya Javeed",
            "Karen Devine"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  We present a new strategy for automatically exploring the design space of key\nCUDA+MPI programs and providing design rules that discriminate slow from fast\nimplementations. In such programs, the order of operations (e.g., GPU kernels,\nMPI communication) and assignment of operations to resources (e.g., GPU\nstreams) makes the space of possible designs enormous. Systems experts have the\ntask of redesigning and reoptimizing these programs to effectively utilize each\nnew platform. This work provides a prototype tool to reduce that burden.\n  In our approach, a directed acyclic graph of CUDA and MPI operations defines\nthe design space for the program. Monte-Carlo tree search discovers regions of\nthe design space that have large impact on the program's performance. A\nsequence-to-vector transformation defines features for each explored\nimplementation, and each implementation is assigned a class label according to\nits relative performance. A decision tree is trained on the features and labels\nto produce design rules for each class; these rules can be used by systems\nexperts to guide their implementations. We demonstrate our strategy using a key\nkernel from scientific computing -- sparse-matrix vector multiplication -- on a\nplatform with multiple MPI ranks and GPU streams.\n",
        "pdf_link": "http://arxiv.org/pdf/2203.02530v2"
    },
    {
        "title": "Efficient scheduling in redundancy systems with general service times",
        "authors": [
            "Elene Anton",
            "Rhonda Righter",
            "Ina Maria Verloop"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  We characterize the impact of scheduling policies on the mean response time\nin nested systems with cancel-on-complete redundancy. We consider not only\nredundancy-oblivious policies, such as FCFS and ROS, but also redundancy-aware\npolicies of the form $\\Pi_1-\\Pi_2$, where $\\Pi_1$ discriminates among job\nclasses (e.g., least-redundant-first (LRF), most-redundant-first (MRF)) and\n$\\Pi_2$ discriminates among jobs of the same class. Assuming that jobs have\nindependent and identically distributed (i.i.d.) copies, we prove the\nfollowing: (i) When jobs have exponential service times, LRF policies\noutperform any other policy. (ii) When service times are New-Worse-than-Used,\nMRF-FCFS outperforms LRF-FCFS as the variability of the service time grows\ninfinitely large. (iii) When service times are New-Better-than-Used, LRF-ROS\n(resp. MRF-ROS) outperforms LRF-FCFS (resp. MRF-FCFS) in a two-server system.\nStatement (iii) also holds when job sizes follow a general distribution and\nhave identical copies (all the copies of a job have the same size). Moreover,\nwe show via simulation that, for a large class of redundancy systems,\nredundancy-aware policies can considerably improve the mean response time\ncompared to redundancy-oblivious policies. We also explore the effect of\nredundancy on the stability region.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.10164v1"
    },
    {
        "title": "TPU-KNN: K Nearest Neighbor Search at Peak FLOP/s",
        "authors": [
            "Felix Chern",
            "Blake Hechtman",
            "Andy Davis",
            "Ruiqi Guo",
            "David Majnemer",
            "Sanjiv Kumar"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  This paper presents a novel nearest neighbor search algorithm achieving TPU\n(Google Tensor Processing Unit) peak performance, outperforming\nstate-of-the-art GPU algorithms with similar level of recall. The design of the\nproposed algorithm is motivated by an accurate accelerator performance model\nthat takes into account both the memory and instruction bottlenecks. Our\nalgorithm comes with an analytical guarantee of recall in expectation and does\nnot require maintaining sophisticated index data structure or tuning, making it\nsuitable for applications with frequent updates. Our work is available in the\nopen-source package of Jax and Tensorflow on TPU.\n",
        "pdf_link": "http://arxiv.org/pdf/2206.14286v2"
    },
    {
        "title": "Diverse Adaptive Bulk Search: a Framework for Solving QUBO Problems on\n  Multiple GPUs",
        "authors": [
            "Koji Nakano",
            "Daisuke Takafuji",
            "Yasuaki Ito",
            "Takashi Yazane",
            "Junko Yano",
            "Shiro Ozaki",
            "Ryota Katsuki",
            "Rie Mori"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Quadratic Unconstrained Binary Optimization (QUBO) is a combinatorial\noptimization to find an optimal binary solution vector that minimizes the\nenergy value defined by a quadratic formula of binary variables in the vector.\nAs many NP-hard problems can be reduced to QUBO problems, considerable research\nhas gone into developing QUBO solvers running on various computing platforms\nsuch as quantum devices, ASICs, FPGAs, GPUs, and optical fibers. This paper\npresents a framework called Diverse Adaptive Bulk Search (DABS), which has the\npotential to find optimal solutions of many types of QUBO problems. Our DABS\nsolver employs a genetic algorithm-based search algorithm featuring three\ndiverse strategies: multiple search algorithms, multiple genetic operations,\nand multiple solution pools. During the execution of the solver, search\nalgorithms and genetic operations that succeeded in finding good solutions are\nautomatically selected to obtain better solutions. Moreover, search algorithms\ntraverse between different solution pools to find good solutions. We have\nimplemented our DABS solver to run on multiple GPUs. Experimental evaluations\nusing eight NVIDIA A100 GPUs confirm that our DABS solver succeeds in finding\noptimal or potentially optimal solutions for three types of QUBO problems.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.03069v2"
    },
    {
        "title": "Automated Cause Analysis of Latency Outliers Using System-Level\n  Dependency Graphs",
        "authors": [
            "Sneh Patel",
            "Brendan Park",
            "Naser Ezzati-Jivan",
            "Quentin Fournier"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Detecting performance issues and identifying their root causes in the runtime\nis a challenging task. Typically, developers use methods such as logging and\ntracing to identify bottlenecks. These solutions are, however, not ideal as\nthey are time-consuming and require manual effort. In this paper, we propose a\nmethod to automate the task of detecting latency outliers using system-level\ntraces and then comparing them to identify the root cause(s). Our method makes\nuse of dependency graphs to show internal interactions between threads and\nsystem resources. With these graphs, one can pinpoint where performance issues\noccur. However, a single trace can be composed of a large number of requests,\neach generating one graph. To automate the task of identifying outliers within\nthe dataset, we use machine learning density-based models and statistical\ncalculations such as -score. Our evaluation shows an accuracy greater than 97 %\non outlier detection, making them appropriate for in-production servers and\nindustry-level use cases.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.06515v1"
    },
    {
        "title": "Markovian queues with Poisson control",
        "authors": [
            "R. Núñez-Queija",
            "B. J. Prabhu",
            "J. A. C. Resing"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  We investigate Markovian queues that are examined by a controller at random\ntimes determined by a Poisson process. Upon examination, the controller sets\nthe service speed to be equal to the minimum of the current number of customers\nin the queue and a certain maximum service speed; this service speed prevails\nuntil the next examination time. We study the resulting two-dimensional Markov\nprocess of queue length and server speed, in particular two regimes with time\nscale separation, specifically for infinitely frequent and infinitely long\nexamination times. In the intermediate regime the analysis proves to be\nextremely challenging. To gain further insight into the model dynamics we then\nanalyse two variants of the model in which the controller is just an observer\nand does not change the speed of the server.\n",
        "pdf_link": "http://arxiv.org/pdf/2208.10198v2"
    },
    {
        "title": "Near-Optimal Stochastic Bin-Packing in Large Service Systems with\n  Time-Varying Item Sizes",
        "authors": [
            "Yige Hong",
            "Qiaomin Xie",
            "Weina Wang"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  In modern computing systems, jobs' resource requirements often vary over\ntime. Accounting for this temporal variability during job scheduling is\nessential for meeting performance goals. However, theoretical understanding on\nhow to schedule jobs with time-varying resource requirements is limited.\nMotivated by this gap, we propose a \\emph{new setting} of the stochastic\nbin-packing problem in service systems that allows for \\emph{time-varying} job\nresource requirements, also referred to as `item sizes' in traditional\nbin-packing terms. In this setting, a job or `item' must be dispatched to a\nserver or `bin' upon arrival. Its resource requirement may vary over time while\nin service, following a Markovian assumption. Once the job's service is\ncomplete, it departs from the system. Our goal is to minimize the expected\nnumber of active servers, or `non-empty bins', in steady state.\n  Under our problem formulation, we develop a job dispatch policy, named\nJoin-Reqesting-Server (JRS). Broadly, JRS lets each server independently\nevaluate its current job configuration and decide whether to accept additional\njobs, balancing the competing objectives of maximizing throughput and\nminimizing the risk of resource capacity overruns. The JRS dispatcher then\nutilizes these individual evaluations to decide which server to dispatch each\narriving job to. The theoretical performance guarantee of JRS is in the\nasymptotic regime where the job arrival rate scales large linearly with respect\nto a scaling factor $r$. We show that JRS achieves an additive optimality gap\nof $O(\\sqrt{r})$ in the objective value, where the optimal objective value is\n$\\Theta(r)$. When specialized to constant job resource requirements, our result\nimproves upon the state-of-the-art $o(r)$ optimality gap. Our technical\napproach highlights a novel policy conversion framework that reduces the policy\ndesign problem into a single-server problem.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.04123v4"
    },
    {
        "title": "Analysis of Reinforcement Learning for determining task replication in\n  workflows",
        "authors": [
            "Andrew Stephen McGough",
            "Matthew Forshaw"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Executing workflows on volunteer computing resources where individual tasks\nmay be forced to relinquish their resource for the resource's primary use leads\nto unpredictability and often significantly increases execution time. Task\nreplication is one approach that can ameliorate this challenge. This comes at\nthe expense of a potentially significant increase in system load and energy\nconsumption. We propose the use of Reinforcement Learning (RL) such that a\nsystem may `learn' the `best' number of replicas to run to increase the number\nof workflows which complete promptly whilst minimising the additional workload\non the system when replicas are not beneficial. We show, through simulation,\nthat we can save 34% of the energy consumption using RL compared to a fixed\nnumber of replicas with only a 4% decrease in workflows achieving a pre-defined\noverhead bound.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.13531v1"
    },
    {
        "title": "Inference Latency Prediction at the Edge",
        "authors": [
            "Zhuojin Li",
            "Marco Paolieri",
            "Leana Golubchik"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  With the growing workload of inference tasks on mobile devices,\nstate-of-the-art neural architectures (NAs) are typically designed through\nNeural Architecture Search (NAS) to identify NAs with good tradeoffs between\naccuracy and efficiency (e.g., latency). Since measuring the latency of a huge\nset of candidate architectures during NAS is not scalable, approaches are\nneeded for predicting end-to-end inference latency on mobile devices. Such\npredictions are challenging due to hardware heterogeneity, optimizations\napplied by ML frameworks, and the diversity of neural architectures. Motivated\nby these challenges, in this paper, we first quantitatively assess\ncharacteristics of neural architectures and mobile devices that have\nsignificant effects on inference latency. Based on this assessment, we propose\na latency prediction framework which addresses these challenges by developing\noperation-wise latency predictors, under a variety of settings and a number of\nhardware devices, with multi-core CPUs and GPUs, achieving high accuracy in\nend-to-end latency prediction, as shown by our comprehensive evaluations. To\nillustrate that our approach does not require expensive data collection, we\nalso show that accurate predictions can be achieved on real-world NAs using\nonly small amounts of profiling data.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.02620v1"
    },
    {
        "title": "Stochastic Network Calculus with Localized Application of Martingales",
        "authors": [
            "Anne Bouillard"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Stochastic Network Calculus is a probabilistic method to compute performance\nbounds in networks, such as end-to-end delays. It relies on the analysis of\nstochastic processes using formalism of (Deterministic) Network Calculus.\nHowever, unlike the deterministic theory, the computed bounds are usually very\nloose compared to the simulation. This is mainly due to the intensive use of\nthe Boole's inequality. On the other hand, analyses based on martingales can\nachieve tight bounds, but until now, they have not been applied to sequences of\nservers. In this paper, we improve the accuracy of Stochastic Network Calculus\nby combining this martingale analysis with a recent Stochastic Network Calculus\nresults based on the Pay-Multiplexing-Only-Once property, well-known from the\nDeterministic Network calculus. We exhibit a non-trivial class of networks that\ncan benefit from this analysis and compare our bounds with simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.05657v2"
    },
    {
        "title": "Using Microbenchmark Suites to Detect Application Performance Changes",
        "authors": [
            "Martin Grambow",
            "Denis Kovalev",
            "Christoph Laaber",
            "Philipp Leitner",
            "David Bermbach"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Software performance changes are costly and often hard to detect pre-release.\nSimilar to software testing frameworks, either application benchmarks or\nmicrobenchmarks can be integrated into quality assurance pipelines to detect\nperformance changes before releasing a new application version. Unfortunately,\nextensive benchmarking studies usually take several hours which is problematic\nwhen examining dozens of daily code changes in detail; hence, trade-offs have\nto be made. Optimized microbenchmark suites, which only include a small subset\nof the full suite, are a potential solution for this problem, given that they\nstill reliably detect the majority of the application performance changes such\nas an increased request latency. It is, however, unclear whether\nmicrobenchmarks and application benchmarks detect the same performance problems\nand one can be a proxy for the other.\n  In this paper, we explore whether microbenchmark suites can detect the same\napplication performance changes as an application benchmark. For this, we run\nextensive benchmark experiments with both the complete and the optimized\nmicrobenchmark suites of the two time-series database systems InuxDB and\nVictoriaMetrics and compare their results to the results of corresponding\napplication benchmarks. We do this for 70 and 110 commits, respectively. Our\nresults show that it is possible to detect application performance changes\nusing an optimized microbenchmark suite if frequent false-positive alarms can\nbe tolerated.\n",
        "pdf_link": "http://arxiv.org/pdf/2212.09515v1"
    },
    {
        "title": "Performance Characterization of using Quantization for DNN Inference on\n  Edge Devices: Extended Version",
        "authors": [
            "Hyunho Ahn",
            "Tian Chen",
            "Nawras Alnaasan",
            "Aamir Shafi",
            "Mustafa Abduljabbar",
            "Hari Subramoni",
            "Dhabaleswar K.",
            " Panda"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Quantization is a popular technique used in Deep Neural Networks (DNN)\ninference to reduce the size of models and improve the overall numerical\nperformance by exploiting native hardware. This paper attempts to conduct an\nelaborate performance characterization of the benefits of using quantization\ntechniques -- mainly FP16/INT8 variants with static and dynamic schemes --\nusing the MLPerf Edge Inference benchmarking methodology. The study is\nconducted on Intel x86 processors and Raspberry Pi device with ARM processor.\nThe paper uses a number of DNN inference frameworks, including OpenVINO (for\nIntel CPUs only), TensorFlow Lite (TFLite), ONNX, and PyTorch with MobileNetV2,\nVGG-19, and DenseNet-121. The single-stream, multi-stream, and offline\nscenarios of the MLPerf Edge Inference benchmarks are used for measuring\nlatency and throughput in our experiments. Our evaluation reveals that OpenVINO\nand TFLite are the most optimized frameworks for Intel CPUs and Raspberry Pi\ndevice, respectively. We observe no loss in accuracy except for the static\nquantization techniques. We also observed the benefits of using quantization\nfor these optimized frameworks. For example, INT8-based quantized models\ndeliver $3.3\\times$ and $4\\times$ better performance over FP32 using OpenVINO\non Intel CPU and TFLite on Raspberry Pi device, respectively, for the MLPerf\noffline scenario. To the best of our knowledge, this paper is the first one\nthat presents a unique characterization study characterizing the impact of\nquantization for a range of DNN inference frameworks -- including OpenVINO,\nTFLite, PyTorch, and ONNX -- on Intel x86 processors and Raspberry Pi device\nwith ARM processor using the MLPerf Edge Inference benchmark methodology.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.05016v1"
    },
    {
        "title": "I Tag, You Tag, Everybody Tags!",
        "authors": [
            "Hazem Ibrahim",
            "Rohail Asim",
            "Matteo Varvello",
            "Yasir Zaki"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Location tags are designed to track personal belongings. Nevertheless, there\nhas been anecdotal evidence that location tags are also misused to stalk\npeople. Tracking is achieved locally, e.g., via Bluetooth with a paired phone,\nand remotely, by piggybacking on location-reporting devices which come into\nproximity of a tag. This paper studies the performance of the two most popular\nlocation tags (Apple's AirTag and Samsung's SmartTag) through controlled\nexperiments - with a known large distribution of location-reporting devices -\nas well as in-the-wild experiments - with no control on the number and kind of\nreporting devices encountered, thus emulating real-life use-cases. We find that\nboth tags achieve similar performance, e.g., they are located 55% of the times\nin about 10 minutes within a 100 m radius. It follows that real time stalking\nto a precise location via location tags is impractical, even when both tags are\nconcurrently deployed which achieves comparable accuracy in half the time.\nNevertheless, half of a victim's exact movements can be backtracked accurately\n(10m error) with just a one-hour delay, which is still perilous information in\nthe possession of a stalker.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.06073v2"
    },
    {
        "title": "Analyzing the Performance of the Inter-Blockchain Communication Protocol",
        "authors": [
            "Joao Otavio Chervinski",
            "Diego Kreutz",
            "Xiwei Xu",
            "Jiangshan Yu"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  With the increasing demand for communication between blockchains, improving\nthe performance of cross-chain communication protocols becomes an emerging\nchallenge. We take a first step towards analyzing the limitations of\ncross-chain communication protocols by comprehensively evaluating Cosmos\nNetwork's Inter-Blockchain Communication Protocol. To achieve our goal we\nintroduce a novel framework to guide empirical evaluations of cross-chain\ncommunication protocols. We implement an instance of our framework as a tool to\nevaluate the IBC protocol. Our findings highlight several challenges, such as\nhigh transaction confirmation latency, bottlenecks in the blockchain's RPC\nimplementation and concurrency issues that hinder the scalability of the\ncross-chain message relayer. We also demonstrate how to reduce the time\nrequired to complete cross-chain transfers by up to 70% when submitting large\namounts of transfers. Finally, we discuss challenges faced during deployment\nwith the objective of contributing to the development and advancement of\ncross-chain communication.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.10844v2"
    },
    {
        "title": "Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices",
        "authors": [
            "Yan Sun",
            "Yifan Yuan",
            "Zeduo Yu",
            "Reese Kuper",
            "Chihun Song",
            "Jinghan Huang",
            "Houxiang Ji",
            "Siddharth Agarwal",
            "Jiaqi Lou",
            "Ipoom Jeong",
            "Ren Wang",
            "Jung Ho Ahn",
            "Tianyin Xu",
            "Nam Sung Kim"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  The ever-growing demands for memory with larger capacity and higher bandwidth\nhave driven recent innovations on memory expansion and disaggregation\ntechnologies based on Compute eXpress Link (CXL). Especially, CXL-based memory\nexpansion technology has recently gained notable attention for its ability not\nonly to economically expand memory capacity and bandwidth but also to decouple\nmemory technologies from a specific memory interface of the CPU. However, since\nCXL memory devices have not been widely available, they have been emulated\nusing DDR memory in a remote NUMA node. In this paper, for the first time, we\ncomprehensively evaluate a true CXL-ready system based on the latest\n4th-generation Intel Xeon CPU with three CXL memory devices from different\nmanufacturers. Specifically, we run a set of microbenchmarks not only to\ncompare the performance of true CXL memory with that of emulated CXL memory but\nalso to analyze the complex interplay between the CPU and CXL memory in depth.\nThis reveals important differences between emulated CXL memory and true CXL\nmemory, some of which will compel researchers to revisit the analyses and\nproposals from recent work. Next, we identify opportunities for\nmemory-bandwidth-intensive applications to benefit from the use of CXL memory.\nLastly, we propose a CXL-memory-aware dynamic page allocation policy, Caption\nto more efficiently use CXL memory as a bandwidth expander. We demonstrate that\nCaption can automatically converge to an empirically favorable percentage of\npages allocated to CXL memory, which improves the performance of\nmemory-bandwidth-intensive applications by up to 24% when compared to the\ndefault page allocation policy designed for traditional NUMA systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.15375v4"
    },
    {
        "title": "An Analysis of the Completion Time of the BB84 Protocol",
        "authors": [
            "Sounak Kar",
            "Jean-Yves Le Boudec"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  The BB84 QKD protocol is based on the idea that the sender and the receiver\ncan reconcile a certain fraction of the teleported qubits to detect\neavesdropping or noise and decode the rest to use as a private key. Under the\npresent hardware infrastructure, decoherence of quantum states poses a\nsignificant challenge to performing perfect or efficient teleportation, meaning\nthat a teleportation-based protocol must be run multiple times to observe\nsuccess. Thus, performance analyses of such protocols usually consider the\ncompletion time, i.e., the time until success, rather than the duration of a\nsingle attempt. Moreover, due to decoherence, the success of an attempt is in\ngeneral dependent on the duration of individual phases of that attempt, as\nquantum states must wait in memory while the success or failure of a generation\nphase is communicated to the relevant parties. In this work, we do a\nperformance analysis of the completion time of the BB84 protocol in a setting\nwhere the sender and the receiver are connected via a single quantum repeater\nand the only quantum channel between them does not see any adversarial attack.\nAssuming certain distributional forms for the generation and communication\nphases of teleportation, we provide a method to compute the MGF of the\ncompletion time and subsequently derive an estimate of the CDF and a bound on\nthe tail probability. This result helps us gauge the (tail) behaviour of the\ncompletion time in terms of the parameters characterising the elementary phases\nof teleportation, without having to run the protocol multiple times. We also\nprovide an efficient simulation scheme to generate the completion time, which\nrelies on expressing the completion time in terms of aggregated teleportation\ntimes. We numerically compare our approach with a full-scale simulation and\nobserve good agreement between them.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.10218v1"
    },
    {
        "title": "LightningSim: Fast and Accurate Trace-Based Simulation for High-Level\n  Synthesis",
        "authors": [
            "Rishov Sarkar",
            "Cong Hao"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  High-Level Synthesis allows hardware designers to create complex RTL designs\nusing C/C++. The traditional HLS workflow involves iterations of C/C++\nsimulation for partial functional verification and HLS synthesis for coarse\ntiming estimates. However, neither C/C++ simulation nor HLS synthesis estimates\ncan account for complex behaviors like FIFO interactions and pipeline stalls,\nthereby obscuring problems like deadlocks and latency overheads. Such problems\nare revealed only through C/RTL co-simulation, which is typically orders of\nmagnitude slower than either C/C++ simulation or HLS synthesis, far too slow to\nintegrate into the edit-run development cycle. Addressing this, we propose\nLightningSim, a fast simulation tool for HLS that combines the speed of native\nC/C++ with the accuracy of C/RTL co-simulation. LightningSim directly operates\non the LLVM intermediate representation (IR) code and accurately simulates a\nhardware design's dynamic behavior. First, it traces LLVM IR execution to\ncapture the run-time information; second, it maps the static HLS scheduling\ninformation to the trace to simulate the dynamic behavior; third, it calculates\nstalls and deadlocks from inter-function interactions to get precise cycle\ncounts. Evaluated on 33 benchmarks, LightningSim produces 99.9%-accurate timing\nestimates up to 95x faster than RTL simulation. Our code is publicly available\non GitHub.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.11219v1"
    },
    {
        "title": "Performance of the Gittins Policy in the G/G/1 and G/G/k, With and\n  Without Setup Times",
        "authors": [
            "Yige Hong",
            "Ziv Scully"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  How should we schedule jobs to minimize mean queue length? In the preemptive\nM/G/1 queue, we know the optimal policy is the Gittins policy, which uses any\navailable information about jobs' remaining service times to dynamically\nprioritize jobs. For models more complex than the M/G/1, optimal scheduling is\ngenerally intractable. This leads us to ask: beyond the M/G/1, does Gittins\nstill perform well? Recent results show Gittins performs well in the M/G/k,\nmeaning that its additive suboptimality gap is bounded by an expression which\nis negligible in heavy traffic. But allowing multiple servers is just one way\nto extend the M/G/1, and most other extensions remain open. Does Gittins still\nperform well with non-Poisson arrival processes? Or if servers require setup\ntimes when transitioning from idle to busy? In this paper, we give the first\nanalysis of the Gittins policy that can handle any combination of (a) multiple\nservers, (b) non-Poisson arrivals, and (c) setup times. Our results thus cover\nthe G/G/1 and G/G/k, with and without setup times, bounding Gittins's\nsuboptimality gap in each case. Each of (a), (b), and (c) adds a term to our\nbound, but all the terms are negligible in heavy traffic, thus implying\nGittins's heavy-traffic optimality in all the systems we consider. Another\nconsequence of our results is that Gittins is optimal in the M/G/1 with setup\ntimes at all loads.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.13231v3"
    },
    {
        "title": "A Data-Driven Approach to Lightweight DVFS-Aware Counter-Based Power\n  Modeling for Heterogeneous Platforms",
        "authors": [
            "Sergio Mazzola",
            "Thomas Benz",
            "Björn Forsberg",
            "Luca Benini"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Computing systems have shifted towards highly parallel and heterogeneous\narchitectures to tackle the challenges imposed by limited power budgets. These\narchitectures must be supported by novel power management paradigms addressing\nthe increasing design size, parallelism, and heterogeneity while ensuring high\naccuracy and low overhead. In this work, we propose a systematic, automated,\nand architecture-agnostic approach to accurate and lightweight DVFS-aware\nstatistical power modeling of the CPU and GPU sub-systems of a heterogeneous\nplatform, driven by the sub-systems' local performance monitoring counters\n(PMCs). Counter selection is guided by a generally applicable statistical\nmethod that identifies the minimal subsets of counters robustly correlating to\npower dissipation. Based on the selected counters, we train a set of\nlightweight, linear models characterizing each sub-system over a range of\nfrequencies. Such models compose a lookup-table-based system-level model that\nefficiently captures the non-linearity of power consumption, showing desirable\nresponsiveness and decomposability. We validate the system-level model on real\nhardware by measuring the total energy consumption of an NVIDIA Jetson AGX\nXavier platform over a set of benchmarks. The resulting average estimation\nerror is 1.3%, with a maximum of 3.1%. Furthermore, the model shows a maximum\nevaluation runtime of 500 ns, thus implying a negligible impact on system\nutilization and applicability to online dynamic power management (DPM).\n",
        "pdf_link": "http://arxiv.org/pdf/2305.06782v1"
    },
    {
        "title": "Case Study for Running Memory-Bound Kernels on RISC-V CPUs",
        "authors": [
            "Valentin Volokitin",
            "Evgeny Kozinov",
            "Valentina Kustikova",
            "Alexey Liniov",
            "Iosif Meyerov"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  The emergence of a new, open, and free instruction set architecture, RISC-V,\nhas heralded a new era in microprocessor architectures. Starting with\nlow-power, low-performance prototypes, the RISC-V community has a good chance\nof moving towards fully functional high-end microprocessors suitable for\nhigh-performance computing. Achieving progress in this direction requires\ncomprehensive development of the software environment, namely operating\nsystems, compilers, mathematical libraries, and approaches to performance\nanalysis and optimization. In this paper, we analyze the performance of two\navailable RISC-V devices when executing three memory-bound applications: a\nwidely used STREAM benchmark, an in-place dense matrix transposition algorithm,\nand a Gaussian Blur algorithm. We show that, compared to x86 and ARM CPUs,\nRISC-V devices are still expected to be inferior in terms of computation time\nbut are very good in resource utilization. We also demonstrate that\nwell-developed memory optimization techniques for x86 CPUs improve the\nperformance on RISC-V CPUs. Overall, the paper shows the potential of RISC-V as\nan alternative architecture for high-performance computing.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.09266v1"
    },
    {
        "title": "Accelerating Machine Learning Queries with Linear Algebra Query\n  Processing",
        "authors": [
            "Wenbo Sun",
            "Asterios Katsifodimos",
            "Rihan Hai"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  The rapid growth of large-scale machine learning (ML) models has led numerous\ncommercial companies to utilize ML models for generating predictive results to\nhelp business decision-making. As two primary components in traditional\npredictive pipelines, data processing, and model predictions often operate in\nseparate execution environments, leading to redundant engineering and\ncomputations. Additionally, the diverging mathematical foundations of data\nprocessing and machine learning hinder cross-optimizations by combining these\ntwo components, thereby overlooking potential opportunities to expedite\npredictive pipelines.\n  In this paper, we propose an operator fusing method based on GPU-accelerated\nlinear algebraic evaluation of relational queries. Our method leverages linear\nalgebra computation properties to merge operators in machine learning\npredictions and data processing, significantly accelerating predictive\npipelines by up to 317x. We perform a complexity analysis to deliver\nquantitative insights into the advantages of operator fusion, considering\nvarious data and model dimensions. Furthermore, we extensively evaluate matrix\nmultiplication query processing utilizing the widely-used Star Schema\nBenchmark. Through comprehensive evaluations, we demonstrate the effectiveness\nand potential of our approach in improving the efficiency of data processing\nand machine learning workloads on modern hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.08367v2"
    },
    {
        "title": "Machine Learning-driven Autotuning of Graphics Processing Unit\n  Accelerated Computational Fluid Dynamics for Enhanced Performance",
        "authors": [
            "Weicheng Xue",
            "Christohper John Roy"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Optimizing the performance of computational fluid dynamics (CFD) applications\naccelerated by graphics processing units (GPUs) is crucial for efficient\nsimulations. In this study, we employed a machine learning-based autotuning\ntechnique to optimize 14 key parameters related to GPU kernel scheduling,\nincluding the number of thread blocks and threads within a block. Our approach\nutilizes fully connected neural networks as the underlying machine learning\nmodel, with the tuning parameters as inputs to the neural networks and the\nactual execution time of a simulation as the outputs. To assess the\neffectiveness of our autotuning approach, we conducted experiments on three\ndifferent types of GPUs, with computational speeds ranging from low to high. We\nperformed independent training for each GPU model and also explored combined\ntraining across multiple GPU models. By leveraging artificial neural networks,\nour autotuning technique achieved remarkable results in tuning a wide range of\nparameters, leading to enhanced performance for a CFD code. Importantly, our\napproach demonstrated its efficacy while requiring only a small fraction of\nsamples from the large parameter search space. This efficiency is attributed to\nthe effectiveness of the fully connected neural networks in capturing the\ncomplex relationships between the parameter settings and the resulting\nperformance. Overall, our study showcases the potential of machine learning,\nspecifically fully connected neural networks, in autotuning GPU-accelerated CFD\ncodes. By leveraging this approach, researchers and practitioners can achieve\nhigh performance in scientific simulations with optimized parameter\nconfigurations.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.14011v3"
    },
    {
        "title": "Benchmarking Performance of Deep Learning Model for Material\n  Segmentation on Two HPC Systems",
        "authors": [
            "Warren R. Williams",
            "S. Ross Glandon",
            "Luke L. Morris",
            "Jing-Ru C. Cheng"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Performance Benchmarking of HPC systems is an ongoing effort that seeks to\nprovide information that will allow for increased performance and improve the\njob schedulers that manage these systems. We develop a benchmarking tool that\nutilizes machine learning models and gathers performance data on\nGPU-accelerated nodes while they perform material segmentation analysis. The\nbenchmark uses a ML model that has been converted from Caffe to PyTorch using\nthe MMdnn toolkit and the MINC-2500 dataset. Performance data is gathered on\ntwo ERDC DSRC systems, Onyx and Vulcanite. The data reveals that while\nVulcanite has faster model times in a large number of benchmarks, and it is\nalso more subject to some environmental factors that can cause performances\nslower than Onyx. In contrast the model times from Onyx are consistent across\nbenchmarks.\n",
        "pdf_link": "http://arxiv.org/pdf/2307.14921v1"
    },
    {
        "title": "Evaluation of ARM CPUs for IceCube available through Google Kubernetes\n  Engine",
        "authors": [
            "Igor Sfiligoi",
            "David Schultz",
            "Benedikt Riedel",
            "Frank Würthwein"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  The IceCube experiment has substantial simulation needs and is in continuous\nsearch for the most cost-effective ways to satisfy them. The most CPU-intensive\npart relies on CORSIKA, a cosmic ray air shower simulation. Historically,\nIceCube relied exclusively on x86-based CPUs, like Intel Xeon and AMD EPYC, but\nrecently server-class ARM-based CPUs are also becoming available, both on-prem\nand in the cloud. In this paper we present our experience in running a sample\nCORSIKA simulation on both ARM and x86 CPUs available through Google Kubernetes\nEngine (GKE). We used the production binaries for the x86 instances, but had to\nbuild the binaries for ARM instances from source code, which turned out to be\nmostly painless. Our benchmarks show that ARM-based CPUs in GKE were not only\nthe most cost-effective but were also the fastest in absolute terms in all the\ntested configurations. While the advantage is not drastic, about 20% in\ncost-effectiveness and less than 10% in absolute terms, it is still large\nenough to warrant an investment in ARM support for IceCube.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.03678v1"
    },
    {
        "title": "Algebraic Reasoning About Timeliness",
        "authors": [
            "Seyed Hossein Haeri",
            "Peter W. Thompson",
            "Peter Van Roy",
            "Magne Haveraaen",
            "Neil J. Davies",
            "Mikhail Barash",
            "Kevin Hammond",
            "James Chapman"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Designing distributed systems to have predictable performance under high load\nis difficult because of resource exhaustion, non-linearity, and stochastic\nbehaviour. Timeliness, i.e., delivering results within defined time bounds, is\na central aspect of predictable performance. In this paper, we focus on\ntimeliness using the DELTA-Q Systems Development paradigm (DELTA-QSD, developed\nby PNSol), which computes timeliness by modelling systems observationally using\nso-called outcome expressions. An outcome expression is a compositional\ndefinition of a system's observed behaviour in terms of its basic operations.\nGiven the behaviour of the basic operations, DELTA-QSD efficiently computes the\nstochastic behaviour of the whole system including its timeliness.\n  This paper formally proves useful algebraic properties of outcome expressions\nw.r.t. timeliness. We prove the different algebraic structures the set of\noutcome expressions form with the different DELTA-QSD operators and demonstrate\nwhy those operators do not form richer structures. We prove or disprove the set\nof all possible distributivity results on outcome expressions. On our way for\ndisproving 8 of those distributivity results, we develop a technique called\nproperisation, which gives rise to the first body of maths for improper random\nvariables. Finally, we also prove 14 equivalences that have been used in the\npast in the practice of DELTA-QSD.\n  An immediate benefit is rewrite rules that can be used for design exploration\nunder established timeliness equivalence. This work is part of an ongoing\nproject to disseminate and build tool support for DELTA-QSD. The ability to\nrewrite outcome expressions is essential for efficient tool support.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.10654v1"
    },
    {
        "title": "Semi-static Conditions in Low-latency C++ for High Frequency Trading:\n  Better than Branch Prediction Hints",
        "authors": [
            "Paul Alexander Bilokon",
            "Maximilian Lucuta",
            "Erez Shermer"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Conditional branches pose a challenge for code optimisation, particularly in\nlow latency settings. For better performance, processors leverage dedicated\nhardware to predict the outcome of a branch and execute the following\ninstructions speculatively, a powerful optimisation. Modern branch predictors\nemploy sophisticated algorithms and heuristics that utilise historical data and\npatterns to make predictions, and often, are extremely effective at doing so.\nConsequently, programmers may inadvertently underestimate the cost of\nmisprediction when benchmarking code with synthetic data that is either too\nshort or too predictable. While eliminating branches may not always be\nfeasible, C++20 introduced the [[likely]] and [[unlikely]] attributes that\nenable the compiler to perform spot optimisations on assembly code associated\nwith likely execution paths. Can we do better than this?\n  This work presents the development of a novel language construct, referred to\nas a semi-static condition, which enables programmers to dynamically modify the\ndirection of a branch at run-time by modifying the assembly code within the\nunderlying executable. Subsequently, we explore scenarios where the use of\nsemi-static conditions outperforms traditional conditional branching,\nhighlighting their potential applications in real-time machine learning and\nhigh-frequency trading. Throughout the development process, key considerations\nof performance, portability, syntax, and security were taken into account.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.14185v1"
    },
    {
        "title": "C++ Design Patterns for Low-latency Applications Including\n  High-frequency Trading",
        "authors": [
            "Paul Bilokon",
            "Burak Gunduz"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  This work aims to bridge the existing knowledge gap in the optimisation of\nlatency-critical code, specifically focusing on high-frequency trading (HFT)\nsystems. The research culminates in three main contributions: the creation of a\nLow-Latency Programming Repository, the optimisation of a market-neutral\nstatistical arbitrage pairs trading strategy, and the implementation of the\nDisruptor pattern in C++. The repository serves as a practical guide and is\nenriched with rigorous statistical benchmarking, while the trading strategy\noptimisation led to substantial improvements in speed and profitability. The\nDisruptor pattern showcased significant performance enhancement over\ntraditional queuing methods. Evaluation metrics include speed, cache\nutilisation, and statistical significance, among others. Techniques like Cache\nWarming and Constexpr showed the most significant gains in latency reduction.\nFuture directions involve expanding the repository, testing the optimised\ntrading algorithm in a live trading environment, and integrating the Disruptor\npattern with the trading algorithm for comprehensive system benchmarking. The\nwork is oriented towards academics and industry practitioners seeking to\nimprove performance in latency-sensitive applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.04259v1"
    },
    {
        "title": "SPEChpc 2021 Benchmarks on Ice Lake and Sapphire Rapids Infiniband\n  Clusters: A Performance and Energy Case Study",
        "authors": [
            "Ayesha Afzal",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  In this work, fundamental performance, power, and energy characteristics of\nthe full SPEChpc 2021 benchmark suite are assessed on two different clusters\nbased on Intel Ice Lake and Sapphire Rapids CPUs using the MPI-only codes'\nvariants. We use memory bandwidth, data volume, and scalability metrics in\norder to categorize the benchmarks and pinpoint relevant performance and\nscalability bottlenecks on the node and cluster levels. Common patterns such as\nmemory bandwidth limitation, dominating communication and synchronization\noverhead, MPI serialization, superlinear scaling, and alignment issues could be\nidentified, in isolation or in combination, showing that SPEChpc 2021 is\nrepresentative of many HPC workloads. Power dissipation and energy measurements\nindicate that the modern Intel server CPUs have such a high idle power level\nthat race-to-idle is the paramount strategy for energy to solution and\nenergy-delay product minimization. On the chip level, only memory-bound code\nshows a clear advantage of Sapphire Rapids compared to Ice Lake in terms of\nenergy to solution.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.05373v3"
    },
    {
        "title": "Comparative evaluation of bandwidth-bound applications on the Intel Xeon\n  CPU MAX Series",
        "authors": [
            "Istvan Z Reguly"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  In this paper we explore the performance of Intel Xeon MAX CPU Series,\nrepresenting the most significant new variation upon the classical CPU\narchitecture since the Intel Xeon Phi Processor. Given the availability of a\nlarge on-package high-bandwidth memory, the bandwidth-to-compute ratio has\nsignificantly shifted compared to other CPUs on the market. Since a large\nfraction of HPC workloads are sensitive to the available bandwidth, we explore\nhow this architecture performs on a selection of HPC proxies and applications\nthat are mostly sensitive to bandwidth, and how it compares to the previous 3rd\ngeneration Intel Xeon Scalable processors (codenamed Ice Lake) and an AMD EPYC\n7003 Series Processor with 3D V-Cache Technology (codenamed Milan-X). We\nexplore performance with different parallel implementations (MPI, MPI+OpenMP,\nMPI+SYCL), compiled with different compilers and flags, and executed with or\nwithout hyperthreading. We show how performance bottlenecks are shifted from\nbandwidth to communication latencies for some applications, and demonstrate\nspeedups compared to the previous generation between 2.0x-4.3x.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.09084v1"
    },
    {
        "title": "Evaluating the performance portability of SYCL across CPUs and GPUs on\n  bandwidth-bound applications",
        "authors": [
            "Istvan Z Reguly"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  In this paper, we evaluate the portability of the SYCL programming model on\nsome of the latest CPUs and GPUs from a wide range of vendors, utilizing the\ntwo main compilers: DPC++ and hipSYCL/OpenSYCL. Both compilers currently\nsupport GPUs from all three major vendors; we evaluate performance on the\nIntel(R) Data Center GPU Max 1100, the NVIDIA A100 GPU, and the AMD MI250X GPU.\nSupport on CPUs currently is less established, with DPC++ only supporting x86\nCPUs through OpenCL, however, OpenSYCL does have an OpenMP backend capable of\ntargeting all modern CPUs; we benchmark the Intel Xeon Platinum 8360Y Processor\n(Ice Lake), the AMD EPYC 9V33X (Genoa-X), and the Ampere Altra platforms. We\nstudy a range of primarily bandwidth-bound applications implemented using the\nOPS and OP2 DSLs, evaluate different formulations in SYCL, and contrast their\nperformance to \"native\" programming approaches where available\n(CUDA/HIP/OpenMP). On GPU architectures SCYL on average even slightly\noutperforms native approaches, while on CPUs it falls behind - highlighting a\ncontinued need for improving CPU performance. While SYCL does not solve all the\nchallenges of performance portability (e.g. needing different algorithms on\ndifferent hardware), it does provide a single programming model and ecosystem\nto target most current HPC architectures productively.\n",
        "pdf_link": "http://arxiv.org/pdf/2309.10075v1"
    },
    {
        "title": "Accelerating Deep Neural Network guided MCTS using Adaptive Parallelism",
        "authors": [
            "Yuan Meng",
            "Qian Wang",
            "Tianxin Zu",
            "Viktor Prasanna"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Deep Neural Network guided Monte-Carlo Tree Search (DNN-MCTS) is a powerful\nclass of AI algorithms. In DNN-MCTS, a Deep Neural Network model is trained\ncollaboratively with a dynamic Monte-Carlo search tree to guide the agent\ntowards actions that yields the highest returns. While the DNN operations are\nhighly parallelizable, the search tree operations involved in MCTS are\nsequential and often become the system bottleneck. Existing MCTS parallel\nschemes on shared-memory multi-core CPU platforms either exploit data\nparallelism but sacrifice memory access latency, or take advantage of local\ncache for low-latency memory accesses but constrain the tree search to a single\nthread. In this work, we analyze the tradeoff of these parallel schemes and\ndevelop performance models for both parallel schemes based on the application\nand hardware parameters. We propose a novel implementation that addresses the\ntradeoff by adaptively choosing the optimal parallel scheme for the MCTS\ncomponent on the CPU. Furthermore, we propose an efficient method for searching\nthe optimal communication batch size as the MCTS component on the CPU\ninterfaces with DNN operations offloaded to an accelerator (GPU). Using a\nrepresentative DNN-MCTS algorithm - Alphazero on board game benchmarks, we show\nthat the parallel framework is able to adaptively generate the best-performing\nparallel implementation, leading to a range of $1.5\\times - 3\\times$ speedup\ncompared with the baseline methods on CPU and CPU-GPU platforms.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.05313v1"
    },
    {
        "title": "Memory Efficient Multithreaded Incremental Segmented Sieve Algorithm",
        "authors": [
            "Evan Ning",
            "David Kaeli"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Prime numbers are fundamental in number theory and play a significant role in\nvarious areas, from pure mathematics to practical applications, including\ncryptography. In this contribution, we introduce a multithreaded implementation\nof the Segmented Sieve algorithm. In our implementation, instead of handling\nlarge prime ranges in one iteration, the sieving process is broken down\nincrementally, which theoretically eliminates the challenges of working with\nlarge numbers, and can reduce memory usage, providing overall more efficient\nmulti-core utilization over extended computations.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.17746v1"
    },
    {
        "title": "Benchmarking GPUs on SVBRDF Extractor Model",
        "authors": [
            "Narayan Kandel",
            "Melanie Lambert"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  With the maturity of deep learning, its use is emerging in every field. Also,\nas different types of GPUs are becoming more available in the markets, it\ncreates a difficult decision for users. How can users select GPUs to achieve\noptimal performance for a specific task? Analysis of GPU architecture is well\nstudied, but existing works that benchmark GPUs do not study tasks for networks\nwith significantly larger input. In this work, we tried to differentiate the\nperformance of different GPUs on neural network models that operate on bigger\ninput images (256x256).\n",
        "pdf_link": "http://arxiv.org/pdf/2310.19816v1"
    },
    {
        "title": "PROMPT: A Fast and Extensible Memory Profiling Framework",
        "authors": [
            "Ziyang Xu",
            "Yebin Chon",
            "Yian Su",
            "Zujun Tan",
            "Sotiris Apostolakis",
            "Simone Campanoni",
            "David I. August"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Memory profiling captures programs' dynamic memory behavior, assisting\nprogrammers in debugging, tuning, and enabling advanced compiler optimizations\nlike speculation-based automatic parallelization. As each use case demands its\nunique program trace summary, various memory profiler types have been\ndeveloped. Yet, designing practical memory profilers often requires extensive\ncompiler expertise, adeptness in program optimization, and significant\nimplementation efforts. This often results in a void where aspirations for fast\nand robust profilers remain unfulfilled. To bridge this gap, this paper\npresents PROMPT, a pioneering framework for streamlined development of fast\nmemory profilers. With it, developers only need to specify profiling events and\ndefine the core profiling logic, bypassing the complexities of custom\ninstrumentation and intricate memory profiling components and optimizations.\nTwo state-of-the-art memory profilers were ported with PROMPT while all\nfeatures preserved. By focusing on the core profiling logic, the code was\nreduced by more than 65% and the profiling speed was improved by 5.3x and 7.1x\nrespectively. To further underscore PROMPT's impact, a tailored memory\nprofiling workflow was constructed for a sophisticated compiler optimization\nclient. In just 570 lines of code, this redesigned workflow satisfies the\nclient's memory profiling needs while achieving more than 90% reduction in\nprofiling time and improved robustness compared to the original profilers.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.03263v1"
    },
    {
        "title": "Enhancing Performance Monitoring in C/C++ Programs with EDPM: A\n  Domain-Specific Language for Performance Monitoring",
        "authors": [
            "David Weisskopf Holmqvist",
            "Suejb Memeti"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  The utilization of performance monitoring probes is a valuable tool for\nprogrammers to gather performance data. However, the manual insertion of these\nprobes can result in an increase in code size, code obfuscation, and an added\nburden of learning different APIs associated with performance monitoring tools.\nTo mitigate these issues, EDPM, an embedded domain-specific language, was\ndeveloped to provide a higher level of abstraction for annotating regions of\ncode that require instrumentation in C and C++ programs. This paper presents\nthe design and implementation of EDPM and compares it to the well-known tool\nPAPI, in terms of required lines of code, flexibility in configuring regions,\nand performance overhead. The results of this study demonstrate that EDPM is a\nlow-resolution profiling tool that offers a reduction in required lines of code\nand enables programmers to express various configurations of regions.\nFurthermore, the design of EDPM is such that its pragmas are ignored by the\nstandard compiler, allowing for seamless integration into existing software\nprocesses without disrupting build systems or increasing the size of the\nexecutable. Additionally, the design of the EDPM pre-compiler allows for the\nextension of available performance counters while maintaining a high level of\nabstraction for programmers. Therefore, EDPM offers a promising solution to\nsimplify and optimize performance monitoring in C and C++ programs.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.03535v1"
    },
    {
        "title": "Autotuning by Changing Directives and Number of Threads in OpenMP using\n  ppOpen-AT",
        "authors": [
            "Toma Sakurai",
            "Satoshi Ohshima",
            "Takahiro Katagiri",
            "Toru Nagai"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Recently, computers have diversified architectures. To achieve high numerical\ncalculation software performance, it is necessary to tune the software\naccording to the target computer architecture. However, code optimization for\neach environment is difficult unless it is performed by a specialist who knows\ncomputer architectures well. By applying autotuning (AT), the tuning effort can\nbe reduced. Optimized implementation by AT that enhances computer performance\ncan be used even by non-experts. In this research, we propose a technique for\nAT for programs using open multi-processing (OpenMP). We propose an AT method\nusing an AT language that changes the OpenMP optimized loop and dynamically\nchanges the number of threads in OpenMP according to computational kernels.\nPerformance evaluation was performed using the Fujitsu PRIMEHPC FX100, which is\na K-computer type supercomputer installed at the Information Technology Center,\nNagoya University. As a result, we found there was a performance increase of\n1.801 times that of the original code in a plasma turbulence analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/2312.05779v1"
    },
    {
        "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and\n  DeepSpeed-Inference",
        "authors": [
            "Connor Holmes",
            "Masahiro Tanaka",
            "Michael Wyatt",
            "Ammar Ahmad Awan",
            "Jeff Rasley",
            "Samyam Rajbhandari",
            "Reza Yazdani Aminabadi",
            "Heyang Qin",
            "Arash Bakhtiari",
            "Lev Kurilenko",
            "Yuxiong He"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The deployment and scaling of large language models (LLMs) have become\ncritical as they permeate various applications, demanding high-throughput and\nlow-latency serving systems. Existing frameworks struggle to balance these\nrequirements, especially for workloads with long prompts. This paper introduces\nDeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and\ngeneration composition strategy, to deliver up to 2.3x higher effective\nthroughput, 2x lower latency on average, and up to 3.7x lower (token-level)\ntail latency, compared to state-of-the-art systems like vLLM. We leverage a\nsynergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an\nefficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced\nimplementation supports a range of models and offers both non-persistent and\npersistent deployment options, catering to diverse user scenarios from\ninteractive sessions to long-running applications. We present a detailed\nbenchmarking methodology, analyze the performance through latency-throughput\ncurves, and investigate scalability via load balancing. Our evaluations\ndemonstrate substantial improvements in throughput and latency across various\nmodels and hardware configurations. We discuss our roadmap for future\nenhancements, including broader model support and new hardware backends. The\nDeepSpeed-FastGen code is readily available for community engagement and\ncontribution.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.08671v1"
    },
    {
        "title": "HeteGen: Heterogeneous Parallel Inference for Large Language Models on\n  Resource-Constrained Devices",
        "authors": [
            "Xuanlei Zhao",
            "Bin Jia",
            "Haotian Zhou",
            "Ziming Liu",
            "Shenggan Cheng",
            "Yang You"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  In recent times, the emergence of Large Language Models (LLMs) has resulted\nin increasingly larger model size, posing challenges for inference on\nlow-resource devices. Prior approaches have explored offloading to facilitate\nlow-memory inference but often suffer from efficiency due to I/O bottlenecks.\nTo achieve low-latency LLMs inference on resource-constrained devices, we\nintroduce HeteGen, a novel approach that presents a principled framework for\nheterogeneous parallel computing using CPUs and GPUs. Based on this framework,\nHeteGen further employs heterogeneous parallel computing and asynchronous\noverlap for LLMs to mitigate I/O bottlenecks. Our experiments demonstrate a\nsubstantial improvement in inference speed, surpassing state-of-the-art methods\nby over 317% at most.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.01164v1"
    },
    {
        "title": "Tail Optimality and Performance Analysis of the Nudge-M Scheduling\n  Algorithm",
        "authors": [
            "Nils Charlet",
            "Benny Van Houdt"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Recently it was shown that the response time of First-Come-First-Served\n(FCFS) scheduling can be stochastically and asymptotically improved upon by the\n{\\it Nudge} scheduling algorithm in case of light-tailed job size\ndistributions. Such improvements are feasible even when the jobs are\npartitioned into two types and the scheduler only has information about the\ntype of incoming jobs (but not their size).\n  In this paper we introduce Nudge-$M$ scheduling, where basically any incoming\ntype-1 job is allowed to pass any type-2 job that is still waiting in the queue\ngiven that it arrived as one of the last $M$ jobs. We prove that Nudge-$M$ has\nan asymptotically optimal response time within a large family of Nudge\nscheduling algorithms when job sizes are light-tailed. Simple explicit results\nfor the asymptotic tail improvement ratio (ATIR) of Nudge-$M$ over FCFS are\nderived as well as explicit results for the optimal parameter $M$. An\nexpression for the ATIR that only depends on the type-1 and type-2 mean job\nsizes and the fraction of type-1 jobs is presented in the heavy traffic\nsetting.\n  The paper further presents a numerical method to compute the response time\ndistribution and mean response time of Nudge-$M$ scheduling provided that the\njob size distribution of both job types follows a phase-type distribution (by\nmaking use of the framework of Markov modulated fluid queues with jumps).\n",
        "pdf_link": "http://arxiv.org/pdf/2403.06588v2"
    },
    {
        "title": "Implementing and Evaluating E2LSH on Storage",
        "authors": [
            "Yu Nakanishi",
            "Kazuhiro Hiwada",
            "Yosuke Bando",
            "Tomoya Suzuki",
            "Hirotsugu Kajihara",
            "Shintaro Sano",
            "Tatsuro Endo",
            "Tatsuo Shiozawa"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Locality sensitive hashing (LSH) is one of the widely-used approaches to\napproximate nearest neighbor search (ANNS) in high-dimensional spaces. The\nfirst work on LSH for the Euclidean distance, E2LSH, showed how ANNS can be\nsolved efficiently at a sublinear query time in the database size with\ntheoretically-guaranteed accuracy, although it required a large hash index\nsize. Since then, several LSH variants having much smaller index sizes have\nbeen proposed. Their query time is linear or superlinear, but they have been\nshown to run effectively faster because they require fewer I/Os when the index\nis stored on hard disk drives and because they also permit in-memory execution\nwith modern DRAM capacity.\n  In this paper, we show that E2LSH is regaining the advantage in query speed\nwith the advent of modern flash storage devices such as solid-state drives\n(SSDs). We evaluate E2LSH on a modern single-node computing environment and\nanalyze its computational cost and I/O cost, from which we derive storage\nperformance requirements for its external memory execution. Our analysis\nindicates that E2LSH on a single consumer-grade SSD can run faster than the\nstate-of-the-art small-index methods executed in-memory. It also indicates that\nE2LSH with emerging high-performance storage devices and interfaces can\napproach in-memory E2LSH speeds. We implement a simple adaptation of E2LSH to\nexternal memory, E2LSH-on-Storage (E2LSHoS), and evaluate it for practical\nlarge datasets of up to one billion objects using different combinations of\nmodern storage devices and interfaces. We demonstrate that our E2LSHoS\nimplementation runs much faster than small-index methods and can approach\nin-memory E2LSH speeds, and also that its query time scales sublinearly with\nthe database size beyond the index size limit of in-memory E2LSH.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.16404v1"
    },
    {
        "title": "Multilevel Modeling as a Methodology for the Simulation of Human\n  Mobility",
        "authors": [
            "Luca Serena",
            "Moreno Marzolla",
            "Gabriele D'Angelo",
            "Stefano Ferretti"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Multilevel modeling is increasingly relevant in the context of modelling and\nsimulation since it leads to several potential benefits, such as software reuse\nand integration, the split of semantically separated levels into sub-models,\nthe possibility to employ different levels of detail, and the potential for\nparallel execution. The coupling that inevitably exists between the sub-models,\nhowever, implies the need for maintaining consistency between the various\ncomponents, more so when different simulation paradigms are employed (e.g.,\nsequential vs parallel, discrete vs continuous). In this paper we argue that\nmultilevel modelling is well suited for the simulation of human mobility, since\nit naturally leads to the decomposition of the model into two layers, the\n\"micro\" and \"macro\" layer, where individual entities (micro) and long-range\ninteractions (macro) are described. In this paper we investigate the challenges\nof multilevel modeling, and describe some preliminary results using prototype\nimplementations of multilayer simulators in the context of epidemic diffusion\nand vehicle pollution.\n",
        "pdf_link": "http://arxiv.org/pdf/2403.16745v1"
    },
    {
        "title": "Asymptotically Optimal Scheduling of Multiple Parallelizable Job Classes",
        "authors": [
            "Benjamin Berg",
            "Benjamin Moseley",
            "Weina Wang",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Many modern computing workloads are composed of parallelizable jobs. A single\nparallelizable job can be completed more quickly if it is run on additional\nservers, however each job is typically limited in the number of servers it can\nrun on (its parallelizability level). A job's parallelizability level is\ndetermined by the type of computation the job performs and how it was\nimplemented. As a result, a single workload of parallelizable jobs generally\nconsists of multiple $\\textit{job classes}$, where jobs from different classes\nmay have different parallelizability levels. The inherent sizes of jobs from\ndifferent classes may also be vastly different.\n  This paper considers the important, practical problem of how to schedule an\narbitrary number of classes of parallelizable jobs. Here, each class of jobs\nhas an associated job size distribution and parallelizability level. Given a\nlimited number of servers, $k$, we ask how to allocate the $k$ servers across a\nstream of arriving jobs in order to minimize the $\\textit{mean response time}$\n-- the average time from when a job arrives to the system until it is\ncompleted.\n  The problem of optimal scheduling in multiserver systems is known to be\ndifficult, even when jobs are not parallelizable. To solve the harder problem\nof scheduling multiple classes of parallelizable jobs, we turn to asymptotic\nscaling regimes. We find that in lighter-load regimes (i.e., Sub-Halfin-Whitt),\nthe optimal allocation algorithm is Least-Parallelizable-First (LPF), a policy\nthat prioritizes jobs from the least parallelizable job classes. By contrast,\nwe also find that in the heavier-load regimes (i.e., Super-NDS), the optimal\nallocation algorithm prioritizes the jobs with the Shortest Expected Remaining\nProcessing Time (SERPT). We also develop scheduling policies that perform\noptimally when the scaling regime is not known to the system a priori.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.00346v1"
    },
    {
        "title": "Strongly Tail-Optimal Scheduling in the Light-Tailed M/G/1",
        "authors": [
            "George Yu",
            "Ziv Scully"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  We study the problem of scheduling jobs in a queueing system, specifically an\nM/G/1 with light-tailed job sizes, to asymptotically optimize the response time\ntail. This means scheduling to make $\\mathbf{P}[T > t]$, the chance a job's\nresponse time exceeds $t$, decay as quickly as possible in the $t \\to \\infty$\nlimit. For some time, the best known policy was First-Come First-Served (FCFS),\nwhich has an asymptotically exponential tail: $\\mathbf{P}[T > t] \\sim C\ne^{-\\gamma t}$. FCFS achieves the optimal *decay rate* $\\gamma$, but its *tail\nconstant* $C$ is suboptimal. Only recently have policies that improve upon\nFCFS's tail constant been discovered. But it is unknown what the optimal tail\nconstant is, let alone what policy might achieve it.\n  In this paper, we derive a closed-form expression for the optimal tail\nconstant $C$, and we introduce *$\\gamma$-Boost*, a new policy that achieves\nthis optimal tail constant. Roughly speaking, $\\gamma$-Boost operates similarly\nto FCFS, but it pretends that small jobs arrive earlier than their true arrival\ntimes. This significantly reduces the response time of small jobs without\nunduly delaying large jobs, improving upon FCFS's tail constant by up to 50%\nwith only moderate job size variability, with even larger improvements for\nhigher variability. While these results are for systems with full job size\ninformation, we also introduce and analyze a version of $\\gamma$-Boost that\nworks in settings with partial job size information, showing it too achieves\nsignificant gains over FCFS. Finally, we show via simulation that\n$\\gamma$-Boost has excellent practical performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.08826v2"
    },
    {
        "title": "LightningSimV2: Faster and Scalable Simulation for High-Level Synthesis\n  via Graph Compilation and Optimization",
        "authors": [
            "Rishov Sarkar",
            "Rachel Paul",
            "Cong Hao"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  High-Level Synthesis (HLS) enables rapid prototyping of complex hardware\ndesigns by translating C or C++ code to low-level RTL code. However, the\ntesting and evaluation of HLS designs still typically rely on slow RTL-level\nsimulators that can take hours to provide feedback, especially for complex\ndesigns. A recent work, LightningSim, helps to solve this problem by providing\na simulation workflow one to two orders of magnitude faster than RTL\nsimulation. However, it still exhibits inefficiencies due to several types of\nredundant computation, making it slow for large design simulation and design\nspace exploration. Addressing these inefficiencies, we introduce\nLightningSimV2, a much faster and scalable simulation tool. LightningSimV2\nfeatures three main innovations. First, we perform compile-time static\nanalysis, exploiting the repetitive structures in HLS designs, e.g., loops, to\nreduce the simulation workload. Second, we propose a novel graph-based\nsimulation approach, with decoupled simulation graph construction step and\ngraph traversal step, significantly reducing repeated computation. Third,\nbenefiting from the decoupled approach, LightningSimV2 can perform incremental\nstall analysis extremely fast, enabling highly efficient design space\nexploration of large numbers of complex hardware parameters, e.g., optimal FIFO\ndepths. Moreover, the DSE is well-suited for parallel computing, further\nimproving the DSE efficiency. Compared with LightningSim, LightningSimV2\nachieves up to 3.5x speedup in full simulation and up to 577x speed up for\nincremental DSE. Our code is open-source on GitHub at\nhttps://github.com/sharc-lab/LightningSim/tree/v0.2.0.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.09471v2"
    },
    {
        "title": "Parallel Implementations Assessment of a Spatial-Spectral Classifier for\n  Hyperspectral Clinical Applications",
        "authors": [
            "Raquel Lazcano",
            "Daniel Madroñal",
            "Giordana Florimbi",
            "Jaime Sancho",
            "Sergio Sanchez",
            "Raquel Leon",
            "Himar Fabelo",
            "Samuel Ortega",
            "Emanuele Torti",
            "Ruben Salvador",
            "Margarita Marrero-Martin",
            "Francesco Leporati",
            "Eduardo Juarez",
            "Gustavo M Callico",
            "Cesar Sanz"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Hyperspectral (HS) imaging presents itself as a non-contact, non-ionizing and\nnon-invasive technique, proven to be suitable for medical diagnosis. However,\nthe volume of information contained in these images makes difficult providing\nthe surgeon with information about the boundaries in real-time. To that end,\nHigh-Performance-Computing (HPC) platforms become necessary. This paper\npresents a comparison between the performances provided by five different HPC\nplatforms while processing a spatial-spectral approach to classify HS images,\nassessing their main benefits and drawbacks. To provide a complete study, two\ndifferent medical applications, with two different requirements, have been\nanalyzed. The first application consists of HS images taken from neurosurgical\noperations; the second one presents HS images taken from dermatological\ninterventions. While the main constraint for neurosurgical applications is the\nprocessing time, in other environments, as the dermatological one, other\nrequirements can be considered. In that sense, energy efficiency is becoming a\nmajor challenge, since this kind of applications are usually developed as\nhand-held devices, thus depending on the battery capacity. These requirements\nhave been considered to choose the target platforms: on the one hand, three of\nthe most powerful Graphic Processing Units (GPUs) available in the market; and,\non the other hand, a low-power GPU and a manycore architecture, both\nspecifically thought for being used in battery-dependent environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.10631v1"
    },
    {
        "title": "Analysis of Markovian Arrivals and Service with Applications to\n  Intermittent Overload",
        "authors": [
            "Isaac Grosof",
            "Yige Hong",
            "Mor Harchol-Balter"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  In many important real-world queueing settings, arrival and service rates\nfluctuate over time. We consider the MAMS system, where the arrival and service\nrates each vary according to an arbitrary finite-state Markov chain, allowing\nintermittent overload to be modeled. This model has been extensively studied,\nand we derive results matching those found in the literature via a somewhat\nnovel framework.\n  We derive a characterization of mean queue length in the MAMS system, with\nexplicit bounds for all arrival and service chains at all loads, using our new\nframework. Our bounds are tight in heavy traffic. We prove even stronger bounds\nfor the important special case of two-level arrivals with intermittent\noverload.\n  Our framework is based around the concepts of relative arrivals and relative\ncompletions, which have previously been used in studying the MAMS system, under\ndifferent names. These quantities allow us to tractably capture the transient\ncorrelational effect of the arrival and service processes on the mean queue\nlength.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.04102v4"
    },
    {
        "title": "Enabling full-speed random access to the entire memory on the A100 GPU",
        "authors": [
            "Alden Walker"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  We describe some features of the A100 memory architecture. In particular, we\ngive a technique to reverse-engineer some hardware layout information. Using\nthis information, we show how to avoid TLB issues to obtain full-speed random\nHBM access to the entire memory, as long as we constrain any particular thread\nto a reduced access window of less than 64GB.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.11425v1"
    },
    {
        "title": "Response time in a pair of processor sharing queues with\n  Join-the-Shortest-Queue scheduling",
        "authors": [
            "Julianna Bor",
            "Peter G Harrison"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Join-the-Shortest-Queue (JSQ) is the scheduling policy of choice for many\nnetwork providers, cloud servers and traffic management systems, where\nindividual queues are served under processor sharing (PS) queueing discipline.\nA numerical solution for the response time distribution in two parallel PS\nqueues with JSQ scheduling is derived for the first time. Using the generating\nfunction method, two partial differential equations (PDEs) are obtained\ncorresponding to conditional response times, where the conditioning is on a\nparticular traced task joining the first or the second queue. These PDEs are\nfunctional equations that contain partial generating functions and their\npartial derivatives, and therefore cannot be solved by commonly used\ntechniques. We are able to solve these PDEs numerically with good accuracy and\nperform the deconditioning with respect to the queue-length probabilities by\nevaluating a certain complex integral. Numerical results for the density and\nthe first four moments compare well against regenerative simulation with\n500,000 regeneration cycles.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.11927v1"
    },
    {
        "title": "Exploring and Evaluating Real-world CXL: Use Cases and System Adoption",
        "authors": [
            "Jie Liu",
            "Xi Wang",
            "Jianbo Wu",
            "Shuangyan Yang",
            "Jie Ren",
            "Bhanu Shankar",
            "Dong Li"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Compute eXpress Link (CXL) is emerging as a promising memory interface\ntechnology. Because of the common unavailiability of CXL devices, the\nperformance of the CXL memory is largely unknown. What are the use cases for\nthe CXL memory? What are the impacts of the CXL memory on application\nperformance? How to use the CXL memory in combination with existing memory\ncomponents? In this work, we study the performance of three genuine CXL\nmemory-expansion cards from different vendors. We characterize the basic\nperformance of the CXL memory, study how HPC applications and large language\nmodels can benefit from the CXL memory, and study the interplay between memory\ntiering and page interleaving. We also propose a novel data object-level\ninterleaving policy to match the interleaving policy with memory access\npatterns. We reveal the challenges and opportunities of using the CXL memory.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.14209v1"
    },
    {
        "title": "An Online Probabilistic Distributed Tracing System",
        "authors": [
            "M. Toslali",
            "S. Qasim",
            "S. Parthasarathy",
            "F. A. Oliveira",
            "H. Huang",
            "G. Stringhini",
            "Z. Liu",
            "A. K. Coskun"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Distributed tracing has become a fundamental tool for diagnosing performance\nissues in the cloud by recording causally ordered, end-to-end workflows of\nrequest executions. However, tracing in production workloads can introduce\nsignificant overheads due to the extensive instrumentation needed for\nidentifying performance variations. This paper addresses the trade-off between\nthe cost of tracing and the utility of the \"spans\" within that trace through\nAstraea, an online probabilistic distributed tracing system. Astraea is based\non our technique that combines online Bayesian learning and multi-armed bandit\nframeworks. This formulation enables Astraea to effectively steer tracing\ntowards the useful instrumentation needed for accurate performance diagnosis.\nAstraea localizes performance variations using only 10-28% of available\ninstrumentation, markedly reducing tracing overhead, storage, compute costs,\nand trace analysis time.\n",
        "pdf_link": "http://arxiv.org/pdf/2405.15645v1"
    },
    {
        "title": "Towards CPU Performance Prediction: New Challenge Benchmark Dataset and\n  Novel Approach",
        "authors": [
            "Xiaoman Liu"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The server central processing unit (CPU) market continues to exhibit robust\ndemand due to the rising global need for computing power. Against this\nbackdrop, CPU benchmark performance prediction is crucial for architecture\ndesigners. It offers profound insights for optimizing system designs and\nsignificantly reduces the time required for benchmark testing. However, the\ncurrent research suffers from a lack of a unified, standard and a comprehensive\ndataset covering various CPU benchmark suites on real machines. Additionally,\nthe traditional simulation-based methods suffer from slow simulation speeds.\nFurthermore, traditional machine learning approaches not only struggle to\nprocess complex features across various hardware configurations but also fall\nshort in achieving sufficient accuracy.\n  To bridge these gaps, we firstly perform a streamlined data preprocessing and\nreorganize our in-house datasets gathered from a variety CPU models of 4th\nGeneration Intel Xeon Scalable Processors on various benchmark suites. We then\npropose Nova CPU Performance Predictor (NCPP), a deep learning model with\nattention mechanisms, specifically designed to predict CPU performance across\nvarious benchmarks. Our model effectively captures key hardware configurations\naffecting performance in across various benchmarks. Moreover, we compare eight\nmainstream machine learning methods, demonstrating the significant advantages\nof our model in terms of accuracy and explainability over existing approaches.\nFinally, our results provide new perspectives and practical strategies for\nhardware designers. To foster further research and collaboration, we\n\\textit{\\textbf{open-source}} the model\n\\url{https://github.com/xiaoman-liu/NCPP}\n",
        "pdf_link": "http://arxiv.org/pdf/2407.03385v3"
    },
    {
        "title": "CEBench: A Benchmarking Toolkit for the Cost-Effectiveness of LLM\n  Pipelines",
        "authors": [
            "Wenbo Sun",
            "Jiaqi Wang",
            "Qiming Guo",
            "Ziyu Li",
            "Wenlu Wang",
            "Rihan Hai"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Online Large Language Model (LLM) services such as ChatGPT and Claude 3 have\ntransformed business operations and academic research by effortlessly enabling\nnew opportunities. However, due to data-sharing restrictions, sectors such as\nhealthcare and finance prefer to deploy local LLM applications using costly\nhardware resources. This scenario requires a balance between the effectiveness\nadvantages of LLMs and significant financial burdens. Additionally, the rapid\nevolution of models increases the frequency and redundancy of benchmarking\nefforts. Existing benchmarking toolkits, which typically focus on\neffectiveness, often overlook economic considerations, making their findings\nless applicable to practical scenarios. To address these challenges, we\nintroduce CEBench, an open-source toolkit specifically designed for\nmulti-objective benchmarking that focuses on the critical trade-offs between\nexpenditure and effectiveness required for LLM deployments. CEBench allows for\neasy modifications through configuration files, enabling stakeholders to\neffectively assess and optimize these trade-offs. This strategic capability\nsupports crucial decision-making processes aimed at maximizing effectiveness\nwhile minimizing cost impacts. By streamlining the evaluation process and\nemphasizing cost-effectiveness, CEBench seeks to facilitate the development of\neconomically viable AI solutions across various industries and research fields.\nThe code and demonstration are available in\n\\url{https://github.com/amademicnoboday12/CEBench}.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.12797v1"
    },
    {
        "title": "Understanding the Impact of Synchronous, Asynchronous, and Hybrid\n  In-Situ Techniques in Computational Fluid Dynamics Applications",
        "authors": [
            "Yi Ju",
            "Adalberto Perez",
            "Stefano Markidis",
            "Philipp Schlatter",
            "Erwin Laure"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  High-Performance Computing (HPC) systems provide input/output (IO)\nperformance growing relatively slowly compared to peak computational\nperformance and have limited storage capacity. Computational Fluid Dynamics\n(CFD) applications aiming to leverage the full power of Exascale HPC systems,\nsuch as the solver Nek5000, will generate massive data for further processing.\nThese data need to be efficiently stored via the IO subsystem. However, limited\nIO performance and storage capacity may result in performance, and thus\nscientific discovery, bottlenecks. In comparison to traditional post-processing\nmethods, in-situ techniques can reduce or avoid writing and reading the data\nthrough the IO subsystem, promising to be a solution to these problems. In this\npaper, we study the performance and resource usage of three in-situ use cases:\ndata compression, image generation, and uncertainty quantification. We\nfurthermore analyze three approaches when these in-situ tasks and the\nsimulation are executed synchronously, asynchronously, or in a hybrid manner.\nIn-situ compression can be used to reduce the IO time and storage requirements\nwhile maintaining data accuracy. Furthermore, in-situ visualization and\nanalysis can save Terabytes of data from being routed through the IO subsystem\nto storage. However, the overall efficiency is crucially dependent on the\ncharacteristics of both, the in-situ task and the simulation. In some cases,\nthe overhead introduced by the in-situ tasks can be substantial. Therefore, it\nis essential to choose the proper in-situ approach, synchronous, asynchronous,\nor hybrid, to minimize overhead and maximize the benefits of concurrent\nexecution.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.20717v1"
    },
    {
        "title": "In-Situ Techniques on GPU-Accelerated Data-Intensive Applications",
        "authors": [
            "Yi Ju",
            "Mingshuai Li",
            "Adalberto Perez",
            "Laura Bellentani",
            "Niclas Jansson",
            "Stefano Markidis",
            "Philipp Schlatter",
            "Erwin Laure"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The computational power of High-Performance Computing (HPC) systems is\nconstantly increasing, however, their input/output (IO) performance grows\nrelatively slowly, and their storage capacity is also limited. This unbalance\npresents significant challenges for applications such as Molecular Dynamics\n(MD) and Computational Fluid Dynamics (CFD), which generate massive amounts of\ndata for further visualization or analysis. At the same time, checkpointing is\ncrucial for long runs on HPC clusters, due to limited walltimes and/or failures\nof system components, and typically requires the storage of large amount of\ndata. Thus, restricted IO performance and storage capacity can lead to\nbottlenecks for the performance of full application workflows (as compared to\ncomputational kernels without IO). In-situ techniques, where data is further\nprocessed while still in memory rather to write it out over the I/O subsystem,\ncan help to tackle these problems. In contrast to traditional post-processing\nmethods, in-situ techniques can reduce or avoid the need to write or read data\nvia the IO subsystem. They offer a promising approach for applications aiming\nto leverage the full power of large scale HPC systems. In-situ techniques can\nalso be applied to hybrid computational nodes on HPC systems consisting of\ngraphics processing units (GPUs) and central processing units (CPUs). On one\nnode, the GPUs would have significant performance advantages over the CPUs.\nTherefore, current approaches for GPU-accelerated applications often focus on\nmaximizing GPU usage, leaving CPUs underutilized. In-situ tasks using CPUs to\nperform data analysis or preprocess data concurrently to the running\nsimulation, offer a possibility to improve this underutilization.\n",
        "pdf_link": "http://arxiv.org/pdf/2407.20731v1"
    },
    {
        "title": "Understanding Power Consumption Metric on Heterogeneous Memory Systems",
        "authors": [
            "Andrès Rubio Proaño",
            "Kento Sato"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Contemporary memory systems contain a variety of memory types, each\npossessing distinct characteristics. This trend empowers applications to opt\nfor memory types aligning with developer's desired behavior. As a result,\ndevelopers gain flexibility to tailor their applications to specific needs,\nfactoring in attributes like latency, bandwidth, and power consumption. Our\nresearch centers on the aspect of power consumption within memory systems. We\nintroduce an approach that equips developers with comprehensive insights into\nthe power consumption of individual memory types. Additionally, we propose an\nordered hierarchy of memory types. Through this methodology, developers can\nmake informed decisions for efficient memory usage aligned with their unique\nrequirements.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.06579v1"
    },
    {
        "title": "Application Research On Real-Time Perception Of Device Performance\n  Status",
        "authors": [
            "Zhe Wang",
            "Zhen Wang",
            "Jianwen Wu",
            "Wangzhong Xiao",
            "Yidong Chen",
            "Zihua Feng",
            "Dian Yang",
            "Hongchen Liu",
            "Bo Liang",
            "Jiaojiao Fu"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  In order to accurately identify the performance status of mobile devices and\nfinely adjust the user experience, a real-time performance perception\nevaluation method based on TOPSIS (Technique for Order Preference by Similarity\nto Ideal Solution) combined with entropy weighting method and time series model\nconstruction was studied. After collecting the performance characteristics of\nvarious mobile devices, the device performance profile was fitted by using PCA\n(principal component analysis) dimensionality reduction and feature engineering\nmethods such as descriptive time series analysis. The ability of performance\nfeatures and profiles to describe the real-time performance status of devices\nwas understood and studied by applying the TOPSIS method and multi-level\nweighting processing. A time series model was constructed for the feature set\nunder objective weighting, and multiple sensitivity (real-time, short-term,\nlong-term) performance status perception results were provided to obtain\nreal-time performance evaluation data and long-term stable performance\nprediction data. Finally, by configuring dynamic AB experiments and overlaying\nfine-grained power reduction strategies, the usability of the method was\nverified, and the accuracy of device performance status identification and\nprediction was compared with the performance of the profile features including\ndimensionality reduction time series modeling, TOPSIS method and entropy\nweighting method, subjective weighting, HMA method. The results show that\naccurate real-time performance perception results can greatly enhance business\nvalue, and this research has application effectiveness and certain\nforward-looking significance.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.03218v1"
    },
    {
        "title": "Microarchitectural comparison and in-core modeling of state-of-the-art\n  CPUs: Grace, Sapphire Rapids, and Genoa",
        "authors": [
            "Jan Laukemann",
            "Georg Hager",
            "Gerhard Wellein"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  With Nvidia's release of the Grace Superchip, all three big semiconductor\ncompanies in HPC (AMD, Intel, Nvidia) are currently competing in the race for\nthe best CPU. In this work we analyze the performance of these state-of-the-art\nCPUs and create an accurate in-core performance model for their\nmicroarchitectures Zen 4, Golden Cove, and Neoverse V2, extending the Open\nSource Architecture Code Analyzer (OSACA) tool and comparing it with LLVM-MCA.\nStarting from the peculiarities and up- and downsides of a single core, we\nextend our comparison by a variety of microbenchmarks and the capabilities of a\nfull node. The \"write-allocate (WA) evasion\" feature, which can automatically\nreduce the memory traffic caused by write misses, receives special attention;\nwe show that the Grace Superchip has a next-to-optimal implementation of WA\nevasion, and that the only way to avoid write allocates on Zen 4 is the\nexplicit use of non-temporal stores.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08108v1"
    },
    {
        "title": "Temporal Load Imbalance on Ondes3D Seismic Simulator for Different\n  Multicore Architectures",
        "authors": [
            "Ana Luisa Veroneze Solórzano",
            "Philippe Olivier Alexandre Navaux",
            "Lucas Mello Schnorr"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The variety of today's multicore architectures motivates researchers to\nexplore parallel scientific applications on different platforms. Load imbalance\nis one performance issue that can prejudice parallel applications from\nexploiting the computational power of these platforms. Ondes3D is a scientific\napplication for seismic wave simulation used to assess the geological impact of\nearthquakes. Its parallelism relies on applying a regular domain decomposition\nin the geological domain provided and distributing each sub-domain to MPI\nranks. Previous works investigate the significant spatial and temporal\nimbalance in Ondes3D and suggest new parallelization and load balancing\ntechniques to minimize them. However, none explored its execution on different\narchitectures. Our paper evaluates the performance of Ondes3D for two\nearthquake scenarios on eight different multicore architectures, including\nIntel, AMD, and ARM processors. We measure the load distribution per MPI rank,\nevaluate the temporal load imbalance, and compare the execution of the\napplication's kernels. Our results show that the temporal load imbalance in\nOndes3D depends on the architecture chosen, with some platforms minimizing such\nimbalance more effectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.11392v1"
    },
    {
        "title": "Plug-and-Play Performance Estimation for LLM Services without Relying on\n  Labeled Data",
        "authors": [
            "Can Wang",
            "Dianbo Sui",
            "Hongliang Sun",
            "Hao Ding",
            "Bolin Zhang",
            "Zhiying Tu"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Large Language Model (LLM) services exhibit impressive capability on\nunlearned tasks leveraging only a few examples by in-context learning (ICL).\nHowever, the success of ICL varies depending on the task and context, leading\nto heterogeneous service quality. Directly estimating the performance of LLM\nservices at each invocation can be laborious, especially requiring abundant\nlabeled data or internal information within the LLM. This paper introduces a\nnovel method to estimate the performance of LLM services across different tasks\nand contexts, which can be \"plug-and-play\" utilizing only a few unlabeled\nsamples like ICL. Our findings suggest that the negative log-likelihood and\nperplexity derived from LLM service invocation can function as effective and\nsignificant features. Based on these features, we utilize four distinct\nmeta-models to estimate the performance of LLM services. Our proposed method is\ncompared against unlabeled estimation baselines across multiple LLM services\nand tasks. And it is experimentally applied to two scenarios, demonstrating its\neffectiveness in the selection and further optimization of LLM services.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.07737v1"
    },
    {
        "title": "ALISE: Accelerating Large Language Model Serving with Speculative\n  Scheduling",
        "authors": [
            "Youpeng Zhao",
            "Jun Wang"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Large Language Models (LLMs) represent a revolutionary advancement in the\ncontemporary landscape of artificial general intelligence (AGI). As exemplified\nby ChatGPT, LLM-based applications necessitate minimal response latency and\nmaximal throughput for inference serving. However, due to the unpredictability\nof LLM execution, the first-come-first-serve (FCFS) scheduling policy employed\nby current LLM serving systems suffers from head-of-line (HoL) blocking issues\nand long job response times.\n  In this paper, we propose a new efficient LLM inference serving framework,\nnamed ALISE. The key design paradigm of ALISE is to leverage a novel\nspeculative scheduler by estimating the execution time for each job and\nexploiting such prior knowledge to assign appropriate job priority orders, thus\nminimizing potential queuing delays for heterogeneous workloads. Furthermore,\nto mitigate the memory overhead of the intermediate key-value (KV) cache, we\nemploy a priority-based adaptive memory management protocol and\nquantization-based compression techniques. Evaluations demonstrate that in\ncomparison to the state-of-the-art solution vLLM, ALISE improves the throughput\nof inference serving by up to 1.8x and 2.1x under the same latency constraint\non the Alpaca and ShareGPT datasets, respectively.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.23537v1"
    },
    {
        "title": "Efficient Performance Analysis of Modular Rewritable Petri Nets",
        "authors": [
            "Lorenzo Capra",
            "Marco Gribaudo"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Petri Nets (PN) are extensively used as a robust formalism to model\nconcurrent and distributed systems; however, they encounter difficulties in\naccurately modeling adaptive systems. To address this issue, we defined\nrewritable PT nets (RwPT) using Maude, a declarative language that ensures\nconsistent rewriting logic semantics. Recently, we proposed a modular approach\nthat employs algebraic operators to build extensive RwPT models. This\nmethodology uses composite node labeling to maintain hierarchical organization\nthrough net rewrites and has been shown to be effective. Once stochastic\nparameters are integrated into the formalism, we introduce an automated\nprocedure to derive a lumped CTMC from the quotient graph generated by a\nmodular RwPT model. To demonstrate the effectiveness of our method, we present\na fault-tolerant manufacturing system as a case study.\n",
        "pdf_link": "http://arxiv.org/pdf/2410.23762v1"
    },
    {
        "title": "DeepContext: A Context-aware, Cross-platform, and Cross-framework Tool\n  for Performance Profiling and Analysis of Deep Learning Workloads",
        "authors": [
            "Qidong Zhao",
            "Hao Wu",
            "Yuming Hao",
            "Zilingfeng Ye",
            "Jiajia Li",
            "Xu Liu",
            "Keren Zhou"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Effective performance profiling and analysis are essential for optimizing\ntraining and inference of deep learning models, especially given the growing\ncomplexity of heterogeneous computing environments. However, existing tools\noften lack the capability to provide comprehensive program context information\nand performance optimization insights for sophisticated interactions between\nCPUs and GPUs. This paper introduces DeepContext, a novel profiler that links\nprogram contexts across high-level Python code, deep learning frameworks,\nunderlying libraries written in C/C++, as well as device code executed on GPUs.\nDeepContext incorporates measurements of both coarse- and fine-grained\nperformance metrics for major deep learning frameworks, such as PyTorch and\nJAX, and is compatible with GPUs from both Nvidia and AMD, as well as various\nCPU architectures, including x86 and ARM. In addition, DeepContext integrates a\nnovel GUI that allows users to quickly identify hotpots and an innovative\nautomated performance analyzer that suggests users with potential optimizations\nbased on performance metrics and program context. Through detailed use cases,\nwe demonstrate how DeepContext can help users identify and analyze performance\nissues to enable quick and effective optimization of deep learning workloads.\nWe believe Deep Context is a valuable tool for users seeking to optimize\ncomplex deep learning workflows across multiple compute environments.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.02797v1"
    },
    {
        "title": "The Effect of Scheduling and Preemption on the Efficiency of LLM\n  Inference Serving",
        "authors": [
            "Kyoungmin Kim",
            "Kijae Hong",
            "Caglar Gulcehre",
            "Anastasia Ailamaki"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The growing usage of Large Language Models (LLMs) highlights the demands and\nchallenges in scalable LLM inference systems, affecting deployment and\ndevelopment processes. On the deployment side, there is a lack of comprehensive\nanalysis on the conditions under which a particular scheduler performs better\nor worse, with performance varying substantially across different schedulers,\nhardware, models, and workloads. Manually testing each configuration on GPUs\ncan be prohibitively expensive. On the development side, unpredictable\nperformance and unknown upper limits can lead to inconclusive trial-and-error\nprocesses, consuming resources on ideas that end up ineffective. To address\nthese challenges, we introduce INFERMAX, an analytical framework that uses\ninference cost models to compare various schedulers, including an optimal\nscheduler formulated as a constraint satisfaction problem (CSP) to establish an\nupper bound on performance. Our framework offers in-depth analysis and raises\nessential questions, challenging assumptions and exploring opportunities for\nmore efficient scheduling. Notably, our findings indicate that preempting\nrequests can reduce GPU costs by 30% compared to avoiding preemptions at all.\nWe believe our methods and insights will facilitate the cost-effective\ndeployment and development of scalable, efficient inference systems and pave\nthe way for cost-based scheduling.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07447v2"
    },
    {
        "title": "A Performance Analysis of BFT Consensus for Blockchains",
        "authors": [
            "J. D. Chan",
            "Y. C. Tay",
            "Brian R. Z. Yen"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Distributed ledgers are common in the industry. Some of them can use\nblockchains as their underlying infrastructure. A blockchain requires\nparticipants to agree on its contents. This can be achieved via a consensus\nprotocol, and several BFT (Byzantine Fault Tolerant) protocols have been\nproposed for this purpose. How do these protocols differ in performance? And\nhow is this difference affected by the communication network? Moreover, such a\nprotocol would need a timer to ensure progress, but how should the timer be\nset?\n  This paper presents an analytical model to address these and related issues\nin the case of crash faults. Specifically, it focuses on two consensus\nprotocols (Istanbul BFT and HotStuff) and two network topologies (Folded-Clos\nand Dragonfly). The model provides closed-form expressions for analyzing how\nthe timer value and number of participants, faults and switches affect the\nconsensus time. The formulas and analyses are validated with simulations. The\nconclusion offers some tips for analytical modeling of such protocols.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.07622v1"
    },
    {
        "title": "Examem: Low-Overhead Memory Instrumentation for Intelligent Memory\n  Systems",
        "authors": [
            "Ashwin Poduval",
            "Hayden Coffey",
            "Michael Swift"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Memory performance is often the main bottleneck in modern computing systems.\nIn recent years, researchers have attempted to scale the memory wall by\nleveraging new technology such as CXL, HBM, and in- and near-memory processing.\nDevelopers optimizing for such hardware need to understand how target\napplications perform to fully take advantage of these systems. Existing\nsoftware and hardware performance introspection techniques are ill-suited for\nthis purpose due to one or more of the following factors: coarse-grained\nmeasurement, inability to offer data needed to debug key issues, high runtime\noverhead, and hardware dependence. The heightened integration between compute\nand memory in many proposed systems offers an opportunity to extend compiler\nsupport for this purpose.\n  We have developed Examem, a memory performance introspection framework based\non the LLVM compiler infrastructure. Examem supports developer annotated\nregions in code, allowing for targeted instrumentation of kernels. Examem\nsupports hardware performance counters when available, in addition to software\ninstrumentation. It statically records information about the instruction mix of\nthe code and adds dynamic instrumentation to produce estimated memory bandwidth\nfor an instrumented region at runtime. This combined approach keeps runtime\noverhead low while remaining accurate, with a geomean overhead under 10% and a\ngeomean byte accuracy of 93%. Finally, our instrumentation is performed using\nan LLVM IR pass, which is target agnostic, and we have applied it to four ISAs.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.12583v1"
    },
    {
        "title": "Automating Energy-Efficient GPU Kernel Generation: A Fast Search-Based\n  Compilation Approach",
        "authors": [
            "Yijia Zhang",
            "Zhihong Gou",
            "Shijie Cao",
            "Weigang Feng",
            "Sicheng Zhang",
            "Guohao Dai",
            "Ningyi Xu"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Deep Neural Networks (DNNs) have revolutionized various fields, but their\ndeployment on GPUs often leads to significant energy consumption. Unlike\nexisting methods for reducing GPU energy consumption, which are either\nhardware-inflexible or limited by workload constraints, this paper addresses\nthe problem at the GPU kernel level. We propose a novel search-based\ncompilation method to generate energy-efficient GPU kernels by incorporating\nenergy efficiency into the search process. To accelerate the energy evaluation\nprocess, we develop an accurate energy cost model based on high-level kernel\nfeatures. Furthermore, we introduce a dynamic updating strategy for the energy\ncost model, reducing the need for on-device energy measurements and\naccelerating the search process. Our evaluation demonstrates that the proposed\napproach can generate GPU kernels with up to 21.69% reduced energy consumption\nwhile maintaining low latency.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.18873v1"
    },
    {
        "title": "MLKAPS: Machine Learning and Adaptive Sampling for HPC Kernel\n  Auto-tuning",
        "authors": [
            "Mathys Jam",
            "Eric Petit",
            "Pablo de Oliveira Castro",
            "David Defour",
            "Greg Henry",
            "William Jalby"
        ],
        "category": "cs.PF",
        "published_year": "2025",
        "summary": "  Many High-Performance Computing (HPC) libraries rely on decision trees to\nselect the best kernel hyperparameters at runtime,depending on the input and\nenvironment. However, finding optimized configurations for each input and\nenvironment is challengingand requires significant manual effort and\ncomputational resources. This paper presents MLKAPS, a tool that automates this\ntask usingmachine learning and adaptive sampling techniques. MLKAPS generates\ndecision trees that tune HPC kernels' design parameters toachieve efficient\nperformance for any user input. MLKAPS scales to large input and design spaces,\noutperforming similar state-of-the-artauto-tuning tools in tuning time and mean\nspeedup. We demonstrate the benefits of MLKAPS on the highly optimized Intel\nMKLdgetrf LU kernel and show that MLKAPS finds blindspots in the manual tuning\nof HPC experts. It improves over 85% of the inputswith a geomean speedup of\nx1.30. On the Intel MKL dgeqrf QR kernel, MLKAPS improves performance on 85% of\nthe inputs with ageomean speedup of x1.18.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.05811v1"
    },
    {
        "title": "Performance and Scalability Models for a Hypergrowth e-Commerce Web Site",
        "authors": [
            "Neil J. Gunther"
        ],
        "category": "cs.PF",
        "published_year": "2000",
        "summary": "  The performance of successful Web-based e-commerce services has all the\nallure of a roller-coaster ride: accelerated fiscal growth combined with the\never-present danger of running out of server capacity. This chapter presents a\ncase study based on the author's own capacity planning engagement with one of\nthe hottest e-commerce Web sites in the world. Several spreadsheet techniques\nare presented for forecasting both short-term and long-term trends in the\nconsumption of server capacity. Two new performance metrics are introduced for\nsite planning and procurement: the effective demand, and the doubling period.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0012022v1"
    },
    {
        "title": "On Conditional Branches in Optimal Decision Trees",
        "authors": [
            "Michael B. Baer"
        ],
        "category": "cs.PF",
        "published_year": "2006",
        "summary": "  The decision tree is one of the most fundamental programming abstractions. A\ncommonly used type of decision tree is the alphabetic binary tree, which uses\n(without loss of generality) ``less than'' versus ''greater than or equal to''\ntests in order to determine one of $n$ outcome events. The process of finding\nan optimal alphabetic binary tree for a known probability distribution on\noutcome events usually has the underlying assumption that the cost (time) per\ndecision is uniform and thus independent of the outcome of the decision. This\nassumption, however, is incorrect in the case of software to be optimized for a\ngiven microprocessor, e.g., in compiling switch statements or in fine-tuning\nprogram bottlenecks. The operation of the microprocessor generally means that\nthe cost for the more likely decision outcome can or will be less -- often far\nless -- than the less likely decision outcome. Here we formulate a variety of\n$O(n^3)$-time $O(n^2)$-space dynamic programming algorithms to solve such\noptimal binary decision tree problems, optimizing for the behavior of\nprocessors with predictive branch capabilities, both static and dynamic. In the\nstatic case, we use existing results to arrive at entropy-based performance\nbounds. Solutions to this formulation are often faster in practice than\n``optimal'' decision trees as formulated in the literature, and, for small\nproblems, are easily worth the extra complexity in finding the better solution.\nThis can be applied in fast implementation of decoding Huffman codes.\n",
        "pdf_link": "http://arxiv.org/pdf/cs/0611037v2"
    },
    {
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,\n  Programs and Systems - Report on the Workshop ICOOOLPS'2006 at ECOOP'06",
        "authors": [
            "Roland Ducournau",
            "Etienne Gagnon",
            "Chandra Krintz",
            "Philippe Mulet",
            "Jan Vitek",
            "Olivier Zendra"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  ICOOOLPS'2006 was the first edition of ECOOP-ICOOOLPS workshop. It intended\nto bring researchers and practitioners both from academia and industry\ntogether, with a spirit of openness, to try and identify and begin to address\nthe numerous and very varied issues of optimization. This succeeded, as can be\nseen from the papers, the attendance and the liveliness of the discussions that\ntook place during and after the workshop, not to mention a few new cooperations\nor postdoctoral contracts. The 22 talented people from different groups who\nparticipated were unanimous to appreciate this first edition and recommend that\nICOOOLPS be continued next year. A community is thus beginning to form, and\nshould be reinforced by a second edition next year, with all the improvements\nthis first edition made emerge.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.2887v1"
    },
    {
        "title": "Effects of Non-Identical Rayleigh Fading on Differential Unitary\n  Space-Time Modulation",
        "authors": [
            "Meixia Tao"
        ],
        "category": "cs.PF",
        "published_year": "2007",
        "summary": "  This paper has been withdrawn by the author.\n",
        "pdf_link": "http://arxiv.org/pdf/0710.3283v2"
    },
    {
        "title": "Performability Aspects of the Atlas Vo; Using Lmbench Suite",
        "authors": [
            "Fotis Georgatos",
            "John Kouvakis",
            "John Kouretis"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  The ATLAS Virtual Organization is grid's largest Virtual Organization which\nis currently in full production stage. Hereby a case is being made that a user\nworking within that VO is going to face a wide spectrum of different systems,\nwhose heterogeneity is enough to count as \"orders of magnitude\" according to a\nnumber of metrics; including integer/float operations, memory throughput\n(STREAM) and communication latencies. Furthermore, the spread of performance\ndoes not appear to follow any known distribution pattern, which is demonstrated\nin graphs produced during May 2007 measurements. It is implied that the current\npractice where either \"all-WNs-are-equal\" or, the alternative of SPEC-based\nrating used by LCG/EGEE is an oversimplification which is inappropriate and\nexpensive from an operational point of view, therefore new techniques are\nneeded for optimal grid resources allocation.\n",
        "pdf_link": "http://arxiv.org/pdf/0805.2949v1"
    },
    {
        "title": "A Call-Graph Profiler for GNU Octave",
        "authors": [
            "Muthiah Annamalai",
            "Leela Velusamy"
        ],
        "category": "cs.PF",
        "published_year": "2008",
        "summary": "  We report the design and implementation of a call-graph profiler for GNU\nOctave, a numerical computing platform. GNU Octave simplifies matrix\ncomputation for use in modeling or simulation. Our work provides a call-graph\nprofiler, which is an improvement on the flat profiler. We elaborate design\nconstraints of building a profiler for numerical computation, and benchmark the\nprofiler by comparing it to the rudimentary timer start-stop (tic-toc)\nmeasurements, for a similar set of programs. The profiler code provides clean\ninterfaces to internals of GNU Octave, for other (newer) profiling tools on GNU\nOctave.\n",
        "pdf_link": "http://arxiv.org/pdf/0810.3468v1"
    },
    {
        "title": "Stability of Finite Population ALOHA with Variable Packets",
        "authors": [
            "Predrag R. Jelenkovic",
            "Jian Tan"
        ],
        "category": "cs.PF",
        "published_year": "2009",
        "summary": "  ALOHA is one of the most basic Medium Access Control (MAC) protocols and\nrepresents a foundation for other more sophisticated distributed and\nasynchronous MAC protocols, e.g., CSMA. In this paper, unlike in the\ntraditional work that focused on mean value analysis, we study the\ndistributional properties of packet transmission delays over an ALOHA channel.\nWe discover a new phenomenon showing that a basic finite population ALOHA model\nwith variable size (exponential) packets is characterized by power law\ntransmission delays, possibly even resulting in zero throughput. These results\nare in contrast to the classical work that shows exponential delays and\npositive throughput for finite population ALOHA with fixed packets.\nFurthermore, we characterize a new stability condition that is entirely derived\nfrom the tail behavior of the packet and backoff distributions that may not be\ndetermined by mean values. The power law effects and the possible instability\nmight be diminished, or perhaps eliminated, by reducing the variability of\npackets. However, we show that even a slotted (synchronized) ALOHA with packets\nof constant size can exhibit power law delays when the number of active users\nis random. From an engineering perspective, our results imply that the\nvariability of packet sizes and number of active users need to be taken into\nconsideration when designing robust MAC protocols, especially for ad-hoc/sensor\nnetworks where other factors, such as link failures and mobility, might further\ncompound the problem.\n",
        "pdf_link": "http://arxiv.org/pdf/0902.4481v2"
    },
    {
        "title": "The Missing Piece Syndrome in Peer-to-Peer Communication",
        "authors": [
            "Bruce Hajek",
            "Ji Zhu"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  Typical protocols for peer-to-peer file sharing over the Internet divide\nfiles to be shared into pieces. New peers strive to obtain a complete\ncollection of pieces from other peers and from a seed. In this paper we\ninvestigate a problem that can occur if the seeding rate is not large enough.\nThe problem is that, even if the statistics of the system are symmetric in the\npieces, there can be symmetry breaking, with one piece becoming very rare. If\npeers depart after obtaining a complete collection, they can tend to leave\nbefore helping other peers receive the rare piece. Assuming that peers arrive\nwith no pieces, there is a single seed, random peer contacts are made, random\nuseful pieces are downloaded, and peers depart upon receiving the complete\nfile, the system is stable if the seeding rate (in pieces per time unit) is\ngreater than the arrival rate, and is unstable if the seeding rate is less than\nthe arrival rate. The result persists for any piece selection policy that\nselects from among useful pieces, such as rarest first, and it persists with\nthe use of network coding.\n",
        "pdf_link": "http://arxiv.org/pdf/1002.3493v2"
    },
    {
        "title": "Magnetohydrodynamics on Heterogeneous architectures: a performance\n  comparison",
        "authors": [
            "Bijia Pang",
            "Ue-li Pen",
            "Michael Perrone"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  We present magneto-hydrodynamic simulation results for heterogeneous systems.\nHeterogeneous architectures combine high floating point performance many-core\nunits hosted in conventional server nodes. Examples include Graphics Processing\nUnits (GPU's) and Cell. They have potentially large gains in performance, at\nmodest power and monetary cost. We implemented a magneto-hydrodynamic (MHD)\nsimulation code on a variety of heterogeneous and multi-core architectures ---\nmulti-core x86, Cell, Nvidia and ATI GPU --- in different languages, FORTRAN,\nC, Cell, CUDA and OpenCL. We present initial performance results for these\nsystems. To our knowledge, this is the widest comparison of heterogeneous\nsystems for MHD simulations. We review the different challenges faced in each\narchitecture, and potential bottlenecks. We conclude that substantial gains in\nperformance over traditional systems are possible, and in particular that is\npossible to extract a greater percentage of peak theoretical performance from\nsome systems when compared to x86 architectures.\n",
        "pdf_link": "http://arxiv.org/pdf/1004.1680v1"
    },
    {
        "title": "A Performance Comparison of CUDA and OpenCL",
        "authors": [
            "Kamran Karimi",
            "Neil G. Dickson",
            "Firas Hamze"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is\nan open standard that can be used to program CPUs, GPUs, and other devices from\ndifferent vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL\npromises a portable language for GPU programming, its generality may entail a\nperformance penalty. In this paper, we use complex, near-identical kernels from\na Quantum Monte Carlo application to compare the performance of CUDA and\nOpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel\nto an OpenCL kernel involves minimal modifications. Making such a kernel\ncompile with ATI's build tools involves more modifications. Our performance\ntests measure and compare data transfer times to and from the GPU, kernel\nexecution times, and end-to-end application execution times for both CUDA and\nOpenCL.\n",
        "pdf_link": "http://arxiv.org/pdf/1005.2581v3"
    },
    {
        "title": "Mantis: Predicting System Performance through Program Analysis and\n  Modeling",
        "authors": [
            "Byung-Gon Chun",
            "Ling Huang",
            "Sangmin Lee",
            "Petros Maniatis",
            "Mayur Naik"
        ],
        "category": "cs.PF",
        "published_year": "2010",
        "summary": "  We present Mantis, a new framework that automatically predicts program\nperformance with high accuracy. Mantis integrates techniques from programming\nlanguage and machine learning for performance modeling, and is a radical\ndeparture from traditional approaches. Mantis extracts program features, which\nare information about program execution runs, through program instrumentation.\nIt uses machine learning techniques to select features relevant to performance\nand creates prediction models as a function of the selected features. Through\nprogram analysis, it then generates compact code slices that compute these\nfeature values for prediction. Our evaluation shows that Mantis can achieve\nmore than 93% accuracy with less than 10% training data set, which is a\nsignificant improvement over models that are oblivious to program features. The\nsystem generates code slices that are cheap to compute feature values.\n",
        "pdf_link": "http://arxiv.org/pdf/1010.0019v1"
    },
    {
        "title": "Optimal Power Cost Management Using Stored Energy in Data Centers",
        "authors": [
            "Rahul Urgaonkar",
            "Bhuvan Urgaonkar",
            "Michael J. Neely",
            "Anand Sivasubramaniam"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Since the electricity bill of a data center constitutes a significant portion\nof its overall operational costs, reducing this has become important. We\ninvestigate cost reduction opportunities that arise by the use of uninterrupted\npower supply (UPS) units as energy storage devices. This represents a deviation\nfrom the usual use of these devices as mere transitional fail-over mechanisms\nbetween utility and captive sources such as diesel generators. We consider the\nproblem of opportunistically using these devices to reduce the time average\nelectric utility bill in a data center. Using the technique of Lyapunov\noptimization, we develop an online control algorithm that can optimally exploit\nthese devices to minimize the time average cost. This algorithm operates\nwithout any knowledge of the statistics of the workload or electricity cost\nprocesses, making it attractive in the presence of workload and pricing\nuncertainties. An interesting feature of our algorithm is that its deviation\nfrom optimality reduces as the storage capacity is increased. Our work opens up\na new area in data center power management.\n",
        "pdf_link": "http://arxiv.org/pdf/1103.3099v2"
    },
    {
        "title": "Non-equilibrium Information Envelopes and the\n  Capacity-Delay-Error-Tradeoff of Source Coding",
        "authors": [
            "Ralf Lübben",
            "Markus Fidler"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  This paper develops an envelope-based approach to establish a link between\ninformation and queueing theory. Unlike classical, equilibrium information\ntheory, information envelopes focus on the dynamics of sources and coders,\nusing functions of time that bound the number of bits generated. In the limit\nthe information envelopes converge to the average behavior and recover the\nentropy of a source, respectively, the average codeword length of a coder. In\ncontrast, on short time scales and for sources with memory it is shown that\nlarge deviations from known equilibrium results occur with non-negligible\nprobability. These can cause significant network delays. Compared to well-known\ntraffic models from queueing theory, information envelopes consider the\nfunctioning of information sources and coders, avoiding a priori assumptions,\nsuch as exponential traffic, or empirical, trace-based traffic models. Using\nresults from the stochastic network calculus, the envelopes yield a\ncharacterization of the operating points of source coders by the triplet of\ncapacity, delay, and error. In the limit, assuming an optimal coder the\nrequired capacity approaches the entropy with arbitrarily small probability of\nerror if infinitely large delays are permitted. We derive a corresponding\ncharacterization of channels and prove that the model has the desirable\nproperty of additivity, that allows analyzing coders and channels separately.\n",
        "pdf_link": "http://arxiv.org/pdf/1107.3087v1"
    },
    {
        "title": "Performance Analysis of Sequential Method for Handover in Cognitive\n  Radio Systems",
        "authors": [
            "Hossein Shokri-Ghadikolaei",
            "Mohammad Mozaffari",
            "Masoumeh Nasiri-Kenari"
        ],
        "category": "cs.PF",
        "published_year": "2011",
        "summary": "  Powerful spectrum handover schemes enable cognitive radios (CRs) to use\ntransmission opportunities in primary users' channels appropriately. In this\npaper, we consider the cognitive access of primary channels by a secondary\nuser. We evaluate the average detection time and the maximum achievable average\nthroughput of the secondary user when the sequential method for hand-over\n(SMHO) is used. We assume that a prior knowledge of the primary users' presence\nand absence probabilities are available. When investigating the maximum\nachievable throughput of the secondary user, we end into an optimization\nproblem, in which the optimum value of sensing time must be selected. In our\noptimization problem, we take into account the spectrum hand over due to false\ndetection of the primary user. We also propose a weighted based hand-over\n(WBHO) scheme in which the impacts of channels conditions and primary users'\npresence probability are considered. This Spectrum handover scheme provides\nhigher average throughput for the SU than the SMHO method. The tradeoff between\nthe maximum achievable throughput and consumed energy is discussed, and finally\nan energy efficient optimization formulation for finding a proper sensing time\nis provided.\n",
        "pdf_link": "http://arxiv.org/pdf/1111.1926v2"
    },
    {
        "title": "Eigenvalue-based Cyclostationary Spectrum Sensing Using Multiple\n  Antennas",
        "authors": [
            "Paulo Urriza",
            "Eric Rebeiz",
            "Danijela Cabric"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  In this paper, we propose a signal-selective spectrum sensing method for\ncognitive radio networks and specifically targeted for receivers with\nmultiple-antenna capability. This method is used for detecting the presence or\nabsence of primary users based on the eigenvalues of the cyclic covariance\nmatrix of received signals. In particular, the cyclic correlation significance\ntest is used to detect a specific signal-of-interest by exploiting knowledge of\nits cyclic frequencies. The analytical threshold for achieving constant false\nalarm rate using this detection method is presented, verified through\nsimulations, and shown to be independent of both the number of samples used and\nthe noise variance, effectively eliminating the dependence on accurate noise\nestimation. The proposed method is also shown, through numerical simulations,\nto outperform existing multiple-antenna cyclostationary-based spectrum sensing\nalgorithms under a quasi-static Rayleigh fading channel, in both spatially\ncorrelated and uncorrelated noise environments. The algorithm also has\nsignificantly lower computational complexity than these other approaches.\n",
        "pdf_link": "http://arxiv.org/pdf/1210.8176v1"
    },
    {
        "title": "Accelerating a Cloud-Based Software GNSS Receiver",
        "authors": [
            "Kamran Karimi",
            "Aleks G. Pamir",
            "M. Haris Afzal"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  In this paper we discuss ways to reduce the execution time of a software\nGlobal Navigation Satellite System (GNSS) receiver that is meant for offline\noperation in a cloud environment. Client devices record satellite signals they\nreceive, and send them to the cloud, to be processed by this software. The goal\nof this project is for each client request to be processed as fast as possible,\nbut also to increase total system throughput by making sure as many requests as\npossible are processed within a unit of time. The characteristics of our\napplication provided both opportunities and challenges for increasing\nperformance. We describe the speedups we obtained by enabling the software to\nexploit multi-core CPUs and GPGPUs. We mention which techniques worked for us\nand which did not. To increase throughput, we describe how we control the\nresources allocated to each invocation of the software to process a client\nrequest, such that multiple copies of the application can run at the same time.\nWe use the notion of effective running time to measure the system's throughput\nwhen running multiple instances at the same time, and show how we can determine\nwhen the system's computing resources have been saturated.\n",
        "pdf_link": "http://arxiv.org/pdf/1309.0052v2"
    },
    {
        "title": "Approximate analysis of biological systems by hybrid switching jump\n  diffusion",
        "authors": [
            "Alessio Angius",
            "Gianfranco Balbo",
            "Marco Beccuti",
            "Enrico Bibbona",
            "Andras Horvath",
            "Roberta Sirovich"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  In this paper we consider large state space continuous time Markov chains\n(MCs) arising in the field of systems biology. For density dependent families\nof MCs that represent the interaction of large groups of identical objects,\nKurtz has proposed two kinds of approximations. One is based on ordinary\ndifferential equations, while the other uses a diffusion process. The\ncomputational cost of the deterministic approximation is significantly lower,\nbut the diffusion approximation retains stochasticity and is able to reproduce\nrelevant random features like variance, bimodality, and tail behavior. In a\nrecent paper, for particular stochastic Petri net models, we proposed a jump\ndiffusion approximation that aims at being applicable beyond the limits of\nKurtz's diffusion approximation, namely when the process reaches the boundary\nwith non-negligible probability. Other limitations of the diffusion\napproximation in its original form are that it can provide inaccurate results\nwhen the number of objects in some groups is often or constantly low and that\nit can be applied only to pure density dependent Markov chains. In order to\novercome these drawbacks, in this paper we propose to apply the jump-diffusion\napproximation only to those components of the model that are in density\ndependent form and are associated with high population levels. The remaining\ncomponents are treated as discrete quantities. The resulting process is a\nhybrid switching jump diffusion. We show that the stochastic differential\nequations that characterize this process can be derived automatically both from\nthe description of the original Markov chains or starting from a higher level\ndescription language, like stochastic Petri nets. The proposed approach is\nillustrated on three models: one modeling the so called crazy clock reaction,\none describing viral infection kinetics and the last considering transcription\nregulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.1352v2"
    },
    {
        "title": "Extended Differential Aggregations in Process Algebra for Performance\n  and Biology",
        "authors": [
            "Max Tschaikowski",
            "Mirco Tribastone"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  We study aggregations for ordinary differential equations induced by fluid\nsemantics for Markovian process algebra which can capture the dynamics of\nperformance models and chemical reaction networks. Whilst previous work has\nrequired perfect symmetry for exact aggregation, we present approximate fluid\nlumpability, which makes nearby processes perfectly symmetric after a\nperturbation of their parameters. We prove that small perturbations yield\nnearby differential trajectories. Numerically, we show that many heterogeneous\nprocesses can be aggregated with negligible errors.\n",
        "pdf_link": "http://arxiv.org/pdf/1406.2067v1"
    },
    {
        "title": "Performance comparison between Java and JNI for optimal implementation\n  of computational micro-kernels",
        "authors": [
            "Nassim A. Halli",
            "Henri-Pierre Charles",
            "Jean-François Mehaut"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  General purpose CPUs used in high performance computing (HPC) support a\nvector instruction set and an out-of-order engine dedicated to increase the\ninstruction level parallelism. Hence, related optimizations are currently\ncritical to improve the performance of applications requiring numerical\ncomputation. Moreover, the use of a Java run-time environment such as the\nHotSpot Java Virtual Machine (JVM) in high performance computing is a promising\nalternative. It benefits from its programming flexibility, productivity and the\nperformance is ensured by the Just-In-Time (JIT) compiler. Though, the JIT\ncompiler suffers from two main drawbacks. First, the JIT is a black box for\ndevelopers. We have no control over the generated code nor any feedback from\nits optimization phases like vectorization. Secondly, the time constraint\nnarrows down the degree of optimization compared to static compilers like GCC\nor LLVM. So, it is compelling to use statically compiled code since it benefits\nfrom additional optimization reducing performance bottlenecks. Java enables to\ncall native code from dynamic libraries through the Java Native Interface\n(JNI). Nevertheless, JNI methods are not inlined and require an additional cost\nto be invoked compared to Java ones. Therefore, to benefit from better static\noptimization, this call overhead must be leveraged by the amount of computation\nperformed at each JNI invocation. In this paper we tackle this problem and we\npropose to do this analysis for a set of micro-kernels. Our goal is to select\nthe most efficient implementation considering the amount of computation defined\nby the calling context. We also investigate the impact on performance of\nseveral different optimization schemes which are vectorization, out-of-order\noptimization, data alignment, method inlining and the use of native memory for\nJNI methods.\n",
        "pdf_link": "http://arxiv.org/pdf/1412.6765v1"
    },
    {
        "title": "GraphMat: High performance graph analytics made productive",
        "authors": [
            "Narayanan Sundaram",
            "Nadathur Rajagopalan Satish",
            "Md Mostofa Ali Patwary",
            "Subramanya R Dulloor",
            "Satya Gautam Vadlamudi",
            "Dipankar Das",
            "Pradeep Dubey"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Given the growing importance of large-scale graph analytics, there is a need\nto improve the performance of graph analysis frameworks without compromising on\nproductivity. GraphMat is our solution to bridge this gap between a\nuser-friendly graph analytics framework and native, hand-optimized code.\nGraphMat functions by taking vertex programs and mapping them to high\nperformance sparse matrix operations in the backend. We get the productivity\nbenefits of a vertex programming framework without sacrificing performance.\nGraphMat is in C++, and we have been able to write a diverse set of graph\nalgorithms in this framework with the same effort compared to other vertex\nprogramming frameworks. GraphMat performs 1.2-7X faster than high performance\nframeworks such as GraphLab, CombBLAS and Galois. It achieves better multicore\nscalability (13-15X on 24 cores) than other frameworks and is 1.2X off native,\nhand-optimized code on a variety of different graph algorithms. Since GraphMat\nperformance depends mainly on a few scalable and well-understood sparse matrix\noperations, GraphMatcan naturally benefit from the trend of increasing\nparallelism on future hardware.\n",
        "pdf_link": "http://arxiv.org/pdf/1503.07241v1"
    },
    {
        "title": "Brewing Analytics Quality for Cloud Performance",
        "authors": [
            "Li Chen",
            "Pooja Jain",
            "Kingsum Chow",
            "Emad Guirguis",
            "Tony Wu"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  Cloud computing has become increasingly popular. Many options of cloud\ndeployments are available. Testing cloud performance would enable us to choose\na cloud deployment based on the requirements. In this paper, we present an\ninnovative process, implemented in software, to allow us to assess the quality\nof the cloud performance data. The process combines performance data from\nmultiple machines, spanning across user experience data, workload performance\nmetrics, and readily available system performance data. Furthermore, we discuss\nthe major challenges of bringing raw data into tidy data formats in order to\nenable subsequent analysis, and describe how our process has several layers of\nassessment to validate the quality of the data processing procedure. We present\na case study to demonstrate the effectiveness of our proposed process, and\nconclude our paper with several future research directions worth investigating.\n",
        "pdf_link": "http://arxiv.org/pdf/1509.00095v1"
    },
    {
        "title": "On the Rate of Convergence of Mean-Field Models: Stein's Method Meets\n  the Perturbation Theory",
        "authors": [
            "Lei Ying"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  This paper studies the rate of convergence of a family of continuous-time\nMarkov chains (CTMC) to a mean-field model. When the mean-field model is a\nfinite-dimensional dynamical system with a unique equilibrium point, an\nanalysis based on Stein's method and the perturbation theory shows that under\nsome mild conditions, the stationary distributions of CTMCs converge (in the\nmean-square sense) to the equilibrium point of the mean-field model if the\nmean-field model is globally asymptotically stable and locally exponentially\nstable. In particular, the mean square difference between the $M$th CTMC in the\nsteady state and the equilibrium point of the mean-field system is $O(1/M),$\nwhere $M$ is the size of the $M$th CTMC. This approach based on Stein's method\nprovides a new framework for studying the convergence of CTMCs to their\nmean-field limit by mainly looking into the stability of the mean-field model,\nwhich is a deterministic system and is often easier to analyze than the CTMCs.\nMore importantly, this approach quantifies the rate of convergence, which\nreveals the approximation error of using mean-field models for approximating\nfinite-size systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1510.00761v1"
    },
    {
        "title": "Ergodic Theory for Controlled Markov Chains with Stationary Inputs",
        "authors": [
            "Yue Chen",
            "Ana Bušić",
            "Sean Meyn"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Consider a stochastic process $\\{X(t)\\}$ on a finite state space $ {\\sf\nX}=\\{1,\\dots, d\\}$. It is conditionally Markov, given a real-valued `input\nprocess' $\\{\\zeta(t)\\}$. This is assumed to be small, which is modeled through\nthe scaling, \\[ \\zeta_t = \\varepsilon \\zeta^1_t, \\qquad 0\\le \\varepsilon \\le\n1\\,, \\] where $\\{\\zeta^1(t)\\}$ is a bounded stationary process. The following\nconclusions are obtained, subject to smoothness assumptions on the controlled\ntransition matrix and a mixing condition on $\\{\\zeta(t)\\}$:\n  (i) A stationary version of the process is constructed, that is coupled with\na stationary version of the Markov chain $\\{X^\\bullet$(t)\\}obtained with\n$\\{\\zeta(t)\\}\\equiv 0$. The triple $(\\{X(t)\\}, \\{X^\\bullet(t)\\},\\{\\zeta(t)\\})$\nis a jointly stationary process satisfying \\[ {\\sf P}\\{X(t) \\neq X^\\bullet(t)\\}\n= O(\\varepsilon) \\] Moreover, a second-order Taylor-series approximation is\nobtained: \\[ {\\sf P}\\{X(t) =i \\} ={\\sf P}\\{X^\\bullet(t) =i \\} + \\varepsilon^2\n\\varrho(i) + o(\\varepsilon^2),\\quad 1\\le i\\le d, \\] with an explicit formula\nfor the vector $\\varrho\\in\\mathbb{R}^d$.\n  (ii) For any $m\\ge 1$ and any function $f\\colon \\{1,\\dots,d\\}\\times\n\\mathbb{R}\\to\\mathbb{R}^m$, the stationary stochastic process $Y(t) =\nf(X(t),\\zeta(t))$ has a power spectral density $\\text{S}_f$ that admits a\nsecond order Taylor series expansion: A function $\\text{S}^{(2)}_f\\colon\n[-\\pi,\\pi] \\to \\mathbb{C}^{ m\\times m}$ is constructed such that \\[\n\\text{S}_f(\\theta) = \\text{S}^\\bullet_f(\\theta) + \\varepsilon^2\n\\text{S}_f^{(2)}(\\theta) + o(\\varepsilon^2),\\quad \\theta\\in [-\\pi,\\pi] . \\] An\nexplicit formula for the function $\\text{S}_f^{(2)}$ is obtained, based in part\non the bounds in (i).\n  The results are illustrated using a version of the timing channel of\nAnantharam and Verdu.\n",
        "pdf_link": "http://arxiv.org/pdf/1604.04013v2"
    },
    {
        "title": "A Unified Framework for Analyzing Closed Queueing Networks in Bike\n  Sharing Systems",
        "authors": [
            "Quan-Lin Li",
            "Rui-Na Fan",
            "Jing-Yu Ma"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  During the last decade bike sharing systems have emerged as a public\ntransport mode in urban short trips in more than 500 major cities around the\nworld. For the mobility service mode, many challenges from its operations are\nnot well addressed yet, for example, how to develop the bike sharing systems to\nbe able to effectively satisfy the fluctuating demands both for bikes and for\nvacant lockers. To this end, it is a key to give performance analysis of the\nbike sharing systems. This paper first describes a large-scale bike sharing\nsystem. Then the bike sharing system is abstracted as a closed queueing network\nwith multi-class customers, where the virtual customers and the virtual nodes\nare set up, and the service rates as well as the relative arrival rates are\nestablished. Finally, this paper gives a product-form solution to the steady\nstate joint probabilities of queue lengths, and gives performance analysis of\nthe bike sharing system. Therefore, this paper provides a unified framework for\nanalyzing closed queueing networks in the study of bike sharing systems. We\nhope the methodology and results of this paper can be applicable in the study\nof more general bike sharing systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1606.04805v1"
    },
    {
        "title": "A survey of sparse matrix-vector multiplication performance on large\n  matrices",
        "authors": [
            "Max Grossman",
            "Christopher Thiele",
            "Mauricio Araya-Polo",
            "Florian Frank",
            "Faruk O. Alpak",
            "Vivek Sarkar"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  We contribute a third-party survey of sparse matrix-vector (SpMV) product\nperformance on industrial-strength, large matrices using: (1) The SpMV\nimplementations in Intel MKL, the Trilinos project (Tpetra subpackage), the\nCUSPARSE library, and the CUSP library, each running on modern architectures.\n(2) NVIDIA GPUs and Intel multi-core CPUs (supported by each software package).\n(3) The CSR, BSR, COO, HYB, and ELL matrix formats (supported by each software\npackage).\n",
        "pdf_link": "http://arxiv.org/pdf/1608.00636v1"
    },
    {
        "title": "Transient performance analysis of zero-attracting LMS",
        "authors": [
            "Jie Chen",
            "Cedric Richard",
            "Yingying Song",
            "David Brie"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  Zero-attracting least-mean-square (ZA-LMS) algorithm has been widely used for\nonline sparse system identification. It combines the LMS framework and\n$\\ell_1$-norm regularization to promote sparsity, and relies on subgradient\niterations. Despite the significant interest in ZA-LMS, few works analyzed its\ntransient behavior. The main difficulty lies in the nonlinearity of the update\nrule. In this work, a detailed analysis in the mean and mean-square sense is\ncarried out in order to examine the behavior of the algorithm. Simulation\nresults illustrate the accuracy of the model and highlight its performance\nthrough comparisons with an existing model.\n",
        "pdf_link": "http://arxiv.org/pdf/1608.07046v1"
    },
    {
        "title": "CLTune: A Generic Auto-Tuner for OpenCL Kernels",
        "authors": [
            "Cedric Nugteren",
            "Valeriu Codreanu"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  This work presents CLTune, an auto-tuner for OpenCL kernels. It evaluates and\ntunes kernel performance of a generic, user-defined search space of possible\nparameter-value combinations. Example parameters include the OpenCL workgroup\nsize, vector data-types, tile sizes, and loop unrolling factors. CLTune can be\nused in the following scenarios: 1) when there are too many tunable parameters\nto explore manually, 2) when performance portability across OpenCL devices is\ndesired, or 3) when the optimal parameters change based on input argument\nvalues (e.g. matrix dimensions). The auto-tuner is generic, easy to use,\nopen-source, and supports multiple search strategies including simulated\nannealing and particle swarm optimisation. CLTune is evaluated on two GPU\ncase-studies inspired by the recent successes in deep learning: 2D convolution\nand matrix-multiplication (GEMM). For 2D convolution, we demonstrate the need\nfor auto-tuning by optimizing for different filter sizes, achieving performance\non-par or better than the state-of-the-art. For matrix-multiplication, we use\nCLTune to explore a parameter space of more than two-hundred thousand\nconfigurations, we show the need for device-specific tuning, and outperform the\nclBLAS library on NVIDIA, AMD and Intel GPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/1703.06503v1"
    },
    {
        "title": "Deep Learning at 15PF: Supervised and Semi-Supervised Classification for\n  Scientific Data",
        "authors": [
            "Thorsten Kurth",
            "Jian Zhang",
            "Nadathur Satish",
            "Ioannis Mitliagkas",
            "Evan Racah",
            "Mostofa Ali Patwary",
            "Tareq Malas",
            "Narayanan Sundaram",
            "Wahid Bhimji",
            "Mikhail Smorkalov",
            "Jack Deslippe",
            "Mikhail Shiryaev",
            "Srinivas Sridharan",
            " Prabhat",
            "Pradeep Dubey"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  This paper presents the first, 15-PetaFLOP Deep Learning system for solving\nscientific pattern classification problems on contemporary HPC architectures.\nWe develop supervised convolutional architectures for discriminating signals in\nhigh-energy physics data as well as semi-supervised architectures for\nlocalizing and classifying extreme weather in climate data. Our\nIntelcaffe-based implementation obtains $\\sim$2TFLOP/s on a single Cori\nPhase-II Xeon-Phi node. We use a hybrid strategy employing synchronous\nnode-groups, while using asynchronous communication across groups. We use this\nstrategy to scale training of a single model to $\\sim$9600 Xeon-Phi nodes;\nobtaining peak performance of 11.73-15.07 PFLOP/s and sustained performance of\n11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art\nclassification accuracy on a dataset with 10M images, exceeding that achieved\nby selections on high-level physics-motivated features. Our semi-supervised\narchitecture successfully extracts weather patterns in a 15TB climate dataset.\nOur results demonstrate that Deep Learning can be optimized and scaled\neffectively on many-core, HPC systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1708.05256v1"
    },
    {
        "title": "Report: Performance comparison between C2075 and P100 GPU cards using\n  cosmological correlation functions",
        "authors": [
            "Miguel Cárdenas-Montes",
            "Iván Méndez-Jiménez",
            "Juan José Rodríguez-Vázquez",
            "José María Hernández Calama"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  In this report, some cosmological correlation functions are used to evaluate\nthe differential performance between C2075 and P100 GPU cards. In the past, the\ncorrelation functions used in this work have been widely studied and exploited\non some previous GPU architectures. The analysis of the performance indicates\nthat a speedup in the range from 13 to 15 is achieved without any additional\noptimization process for the P100 card.\n",
        "pdf_link": "http://arxiv.org/pdf/1709.03264v1"
    },
    {
        "title": "A General Formula for the Stationary Distribution of the Age of\n  Information and Its Application to Single-Server Queues",
        "authors": [
            "Yoshiaki Inoue",
            "Hiroyuki Masuyama",
            "Tetsuya Takine",
            "Toshiyuki Tanaka"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  This paper considers the stationary distribution of the age of information\n(AoI) in information update systems. We first derive a general formula for the\nstationary distribution of the AoI, which holds for a wide class of information\nupdate systems. The formula indicates that the stationary distribution of the\nAoI is given in terms of the stationary distributions of the system delay and\nthe peak AoI. To demonstrate its applicability and usefulness, we analyze the\nAoI in single-server queues with four different service disciplines: first-come\nfirst-served (FCFS), preemptive last-come first-served (LCFS), and two variants\nof non-preemptive LCFS service disciplines. For the FCFS and the preemptive\nLCFS service disciplines, the GI/GI/1, M/GI/1, and GI/M/1 queues are\nconsidered, and for the non-preemptive LCFS service disciplines, the M/GI/1 and\nGI/M/1 queues are considered. With these results, we further show comparison\nresults for the mean AoI's in the M/GI/1 and GI/M/1 queues under those service\ndisciplines.\n",
        "pdf_link": "http://arxiv.org/pdf/1804.06139v2"
    },
    {
        "title": "Online normalizer calculation for softmax",
        "authors": [
            "Maxim Milakov",
            "Natalia Gimelshein"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  The Softmax function is ubiquitous in machine learning, multiple previous\nworks suggested faster alternatives for it. In this paper we propose a way to\ncompute classical Softmax with fewer memory accesses and hypothesize that this\nreduction in memory accesses should improve Softmax performance on actual\nhardware. The benchmarks confirm this hypothesis: Softmax accelerates by up to\n1.3x and Softmax+TopK combined and fused by up to 5x.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.02867v2"
    },
    {
        "title": "Adaptive Selection of Deep Learning Models on Embedded Systems",
        "authors": [
            "Ben Taylor",
            "Vicent Sanz Marco",
            "Willy Wolff",
            "Yehia Elkhatib",
            "Zheng Wang"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  The recent ground-breaking advances in deep learning networks ( DNNs ) make\nthem attractive for embedded systems. However, it can take a long time for DNNs\nto make an inference on resource-limited embedded devices. Offloading the\ncomputation into the cloud is often infeasible due to privacy concerns, high\nlatency, or the lack of connectivity. As such, there is a critical need to find\na way to effectively execute the DNN models locally on the devices. This paper\npresents an adaptive scheme to determine which DNN model to use for a given\ninput, by considering the desired accuracy and inference time. Our approach\nemploys machine learning to develop a predictive model to quickly select a\npre-trained DNN to use for a given input and the optimization constraint. We\nachieve this by first training off-line a predictive model, and then use the\nlearnt model to select a DNN model to use for new, unseen inputs. We apply our\napproach to the image classification task and evaluate it on a Jetson TX2\nembedded deep learning platform using the ImageNet ILSVRC 2012 validation\ndataset. We consider a range of influential DNN models. Experimental results\nshow that our approach achieves a 7.52% improvement in inference accuracy, and\na 1.8x reduction in inference time over the most-capable single DNN model.\n",
        "pdf_link": "http://arxiv.org/pdf/1805.04252v1"
    },
    {
        "title": "uops.info: Characterizing Latency, Throughput, and Port Usage of\n  Instructions on Intel Microarchitectures",
        "authors": [
            "Andreas Abel",
            "Jan Reineke"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Modern microarchitectures are some of the world's most complex man-made\nsystems. As a consequence, it is increasingly difficult to predict, explain,\nlet alone optimize the performance of software running on such\nmicroarchitectures. As a basis for performance predictions and optimizations,\nwe would need faithful models of their behavior, which are, unfortunately,\nseldom available.\n  In this paper, we present the design and implementation of a tool to\nconstruct faithful models of the latency, throughput, and port usage of x86\ninstructions. To this end, we first discuss common notions of instruction\nthroughput and port usage, and introduce a more precise definition of latency\nthat, in contrast to previous definitions, considers dependencies between\ndifferent pairs of input and output operands. We then develop novel algorithms\nto infer the latency, throughput, and port usage based on\nautomatically-generated microbenchmarks that are more accurate and precise than\nexisting work.\n  To facilitate the rapid construction of optimizing compilers and tools for\nperformance prediction, the output of our tool is provided in a\nmachine-readable format. We provide experimental results for processors of all\ngenerations of Intel's Core architecture, i.e., from Nehalem to Coffee Lake,\nand discuss various cases where the output of our tool differs considerably\nfrom prior work.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.04610v3"
    },
    {
        "title": "Learning with Analytical Models",
        "authors": [
            "Huda Ibeid",
            "Siping Meng",
            "Oliver Dobon",
            "Luke Olson",
            "William Gropp"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  To understand and predict the performance of scientific applications, several\nanalytical and machine learning approaches have been proposed, each having its\nadvantages and disadvantages. In this paper, we propose and validate a hybrid\napproach for performance modeling and prediction, which combines analytical and\nmachine learning models. The proposed hybrid model aims to minimize prediction\ncost while providing reasonable prediction accuracy. Our validation results\nshow that the hybrid model is able to learn and correct the analytical models\nto better match the actual performance. Furthermore, the proposed hybrid model\nimproves the prediction accuracy in comparison to pure machine learning\ntechniques while using small training datasets, thus making it suitable for\nhardware and workload changes.\n",
        "pdf_link": "http://arxiv.org/pdf/1810.11772v2"
    },
    {
        "title": "Speeding up Deep Learning with Transient Servers",
        "authors": [
            "Shijian Li",
            "Robert J. Walls",
            "Lijie Xu",
            "Tian Guo"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Distributed training frameworks, like TensorFlow, have been proposed as a\nmeans to reduce the training time of deep learning models by using a cluster of\nGPU servers. While such speedups are often desirable---e.g., for rapidly\nevaluating new model designs---they often come with significantly higher\nmonetary costs due to sublinear scalability. In this paper, we investigate the\nfeasibility of using training clusters composed of cheaper transient GPU\nservers to get the benefits of distributed training without the high costs.\n  We conduct the first large-scale empirical analysis, launching more than a\nthousand GPU servers of various capacities, aimed at understanding the\ncharacteristics of transient GPU servers and their impact on distributed\ntraining performance. Our study demonstrates the potential of transient servers\nwith a speedup of 7.7X with more than 62.9% monetary savings for some cluster\nconfigurations. We also identify a number of important challenges and\nopportunities for redesigning distributed training frameworks to be\ntransient-aware. For example, the dynamic cost and availability characteristics\nof transient servers suggest the need for frameworks to dynamically change\ncluster configurations to best take advantage of current conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/1903.00045v2"
    },
    {
        "title": "Toward an End-to-End Auto-tuning Framework in HPC PowerStack",
        "authors": [
            "Xingfu Wu",
            "Aniruddha Marathe",
            "Siddhartha Jana",
            "Ondrej Vysocky",
            "Jophin John",
            "Andrea Bartolini",
            "Lubomir Riha",
            "Michael Gerndt",
            "Valerie Taylor",
            "Sridutt Bhalachandra"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Efficiently utilizing procured power and optimizing performance of scientific\napplications under power and energy constraints are challenging. The HPC\nPowerStack defines a software stack to manage power and energy of\nhigh-performance computing systems and standardizes the interfaces between\ndifferent components of the stack. This survey paper presents the findings of a\nworking group focused on the end-to-end tuning of the PowerStack. First, we\nprovide a background on the PowerStack layer-specific tuning efforts in terms\nof their high-level objectives, the constraints and optimization goals,\nlayer-specific telemetry, and control parameters, and we list the existing\nsoftware solutions that address those challenges. Second, we propose the\nPowerStack end-to-end auto-tuning framework, identify the opportunities in\nco-tuning different layers in the PowerStack, and present specific use cases\nand solutions. Third, we discuss the research opportunities and challenges for\ncollective auto-tuning of two or more management layers (or domains) in the\nPowerStack. This paper takes the first steps in identifying and aggregating the\nimportant R&D challenges in streamlining the optimization efforts across the\nlayers of the PowerStack.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.06571v1"
    },
    {
        "title": "Performance portability through machine learning guided kernel selection\n  in SYCL libraries",
        "authors": [
            "John Lawson"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Automatically tuning parallel compute kernels allows libraries and frameworks\nto achieve performance on a wide range of hardware, however these techniques\nare typically focused on finding optimal kernel parameters for particular input\nsizes and parameters. General purpose compute libraries must be able to cater\nto all inputs and parameters provided by a user, and so these techniques are of\nlimited use. Additionally, parallel programming frameworks such as SYCL require\nthat the kernels be deployed in a binary format embedded within the library. As\nsuch it is impractical to deploy a large number of possible kernel\nconfigurations without inflating the library size.\n  Machine learning methods can be used to mitigate against both of these\nproblems and provide performance for general purpose routines with a limited\nnumber of kernel configurations. We show that unsupervised clustering methods\ncan be used to select a subset of the possible kernels that should be deployed\nand that simple classification methods can be trained to select from these\nkernels at runtime to give good performance. As these techniques are fully\nautomated, relying only on benchmark data, the tuning process for new hardware\nor problems does not require any developer effort or expertise.\n",
        "pdf_link": "http://arxiv.org/pdf/2008.13145v1"
    },
    {
        "title": "A Linear Algebra Approach to Fast DNA Mixture Analysis Using GPUs",
        "authors": [
            "Siddharth Samsi",
            "Brian Helfer",
            "Jeremy Kepner",
            "Albert Reuther",
            "Darrell O. Ricke"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Analysis of DNA samples is an important step in forensics, and the speed of\nanalysis can impact investigations. Comparison of DNA sequences is based on the\nanalysis of short tandem repeats (STRs), which are short DNA sequences of 2-5\nbase pairs. Current forensics approaches use 20 STR loci for analysis. The use\nof single nucleotide polymorphisms (SNPs) has utility for analysis of complex\nDNA mixtures. The use of tens of thousands of SNPs loci for analysis poses\nsignificant computational challenges because the forensic analysis scales by\nthe product of the loci count and number of DNA samples to be analyzed. In this\npaper, we discuss the implementation of a DNA sequence comparison algorithm by\nre-casting the algorithm in terms of linear algebra primitives. By developing\nan overloaded matrix multiplication approach to DNA comparisons, we can\nleverage advances in GPU hardware and algoithms for Dense Generalized\nMatrix-Multiply (DGEMM) to speed up DNA sample comparisons. We show that it is\npossible to compare 2048 unknown DNA samples with 20 million known samples in\nunder 6 seconds using a NVIDIA K80 GPU.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.00516v1"
    },
    {
        "title": "Benchmarking Data Analysis and Machine Learning Applications on the\n  Intel KNL Many-Core Processor",
        "authors": [
            "Chansup Byun",
            "Jeremy Kepner",
            "William Arcand",
            "David Bestor",
            "Bill Bergeron",
            "Vijay Gadepally",
            "Michael Houle",
            "Matthew Hubbell",
            "Michael Jones",
            "Anna Klein",
            "Peter Michaleas",
            "Lauren Milechin",
            "Julie Mullen",
            "Andrew Prout",
            "Antonio Rosa",
            "Siddharth Samsi",
            "Charles Yee",
            "Albert Reuther"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Knights Landing (KNL) is the code name for the second-generation Intel Xeon\nPhi product family. KNL has generated significant interest in the data analysis\nand machine learning communities because its new many-core architecture targets\nboth of these workloads. The KNL many-core vector processor design enables it\nto exploit much higher levels of parallelism. At the Lincoln Laboratory\nSupercomputing Center (LLSC), the majority of users are running data analysis\napplications such as MATLAB and Octave. More recently, machine learning\napplications, such as the UC Berkeley Caffe deep learning framework, have\nbecome increasingly important to LLSC users. Thus, the performance of these\napplications on KNL systems is of high interest to LLSC users and the broader\ndata analysis and machine learning communities. Our data analysis benchmarks of\nthese application on the Intel KNL processor indicate that single-core\ndouble-precision generalized matrix multiply (DGEMM) performance on KNL systems\nhas improved by ~3.5x compared to prior Intel Xeon technologies. Our data\nanalysis applications also achieved ~60% of the theoretical peak performance.\nAlso a performance comparison of a machine learning application, Caffe, between\nthe two different Intel CPUs, Xeon E5 v3 and Xeon Phi 7210, demonstrated a 2.7x\nimprovement on a KNL node.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.03515v1"
    },
    {
        "title": "Cloud-based or On-device: An Empirical Study of Mobile Deep Inference",
        "authors": [
            "Tian Guo"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Modern mobile applications are benefiting significantly from the advancement\nin deep learning, e.g., implementing real-time image recognition and\nconversational system. Given a trained deep learning model, applications\nusually need to perform a series of matrix operations based on the input data,\nin order to infer possible output values. Because of computational complexity\nand size constraints, these trained models are often hosted in the cloud. To\nutilize these cloud-based models, mobile apps will have to send input data over\nthe network. While cloud-based deep learning can provide reasonable response\ntime for mobile apps, it restricts the use case scenarios, e.g. mobile apps\nneed to have network access. With mobile specific deep learning optimizations,\nit is now possible to employ on-device inference. However, because mobile\nhardware, such as GPU and memory size, can be very limited when compared to its\ndesktop counterpart, it is important to understand the feasibility of this new\non-device deep learning inference architecture. In this paper, we empirically\nevaluate the inference performance of three Convolutional Neural Networks\n(CNNs) using a benchmark Android application we developed. Our measurement and\nanalysis suggest that on-device inference can cost up to two orders of\nmagnitude greater response time and energy when compared to cloud-based\ninference, and that loading model and computing probability are two performance\nbottlenecks for on-device deep inferences.\n",
        "pdf_link": "http://arxiv.org/pdf/1707.04610v2"
    },
    {
        "title": "Asymptotically Optimal Load Balancing in Large-scale Heterogeneous\n  Systems with Multiple Dispatchers",
        "authors": [
            "Xingyu Zhou",
            "Ness Shroff",
            "Adam Wierman"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We consider the load balancing problem in large-scale heterogeneous systems\nwith multiple dispatchers. We introduce a general framework called\nLocal-Estimation-Driven (LED). Under this framework, each dispatcher keeps\nlocal (possibly outdated) estimates of queue lengths for all the servers, and\nthe dispatching decision is made purely based on these local estimates. The\nlocal estimates are updated via infrequent communications between dispatchers\nand servers. We derive sufficient conditions for LED policies to achieve\nthroughput optimality and delay optimality in heavy-traffic, respectively.\nThese conditions directly imply delay optimality for many previous local-memory\nbased policies in heavy traffic. Moreover, the results enable us to design new\ndelay optimal policies for heterogeneous systems with multiple dispatchers.\nFinally, the heavy-traffic delay optimality of the LED framework directly\nresolves a recent open problem on how to design optimal load balancing schemes\nusing delayed information.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.08908v1"
    },
    {
        "title": "Learning Queuing Networks by Recurrent Neural Networks",
        "authors": [
            "Giulio Garbi",
            "Emilio Incerto",
            "Mirco Tribastone"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  It is well known that building analytical performance models in practice is\ndifficult because it requires a considerable degree of proficiency in the\nunderlying mathematics. In this paper, we propose a machine-learning approach\nto derive performance models from data. We focus on queuing networks, and\ncrucially exploit a deterministic approximation of their average dynamics in\nterms of a compact system of ordinary differential equations. We encode these\nequations into a recurrent neural network whose weights can be directly related\nto model parameters. This allows for an interpretable structure of the neural\nnetwork, which can be trained from system measurements to yield a white-box\nparameterized model that can be used for prediction purposes such as what-if\nanalyses and capacity planning. Using synthetic models as well as a real case\nstudy of a load-balancing system, we show the effectiveness of our technique in\nyielding models with high predictive power.\n",
        "pdf_link": "http://arxiv.org/pdf/2002.10788v1"
    },
    {
        "title": "Control of parallel non-observable queues: asymptotic equivalence and\n  optimality of periodic policies",
        "authors": [
            "Jonatha Anselmi",
            "Bruno Gaujal",
            "Tommaso Nesti"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  We consider a queueing system composed of a dispatcher that routes\ndeterministically jobs to a set of non-observable queues working in parallel.\nIn this setting, the fundamental problem is which policy should the dispatcher\nimplement to minimize the stationary mean waiting time of the incoming jobs. We\npresent a structural property that holds in the classic scaling of the system\nwhere the network demand (arrival rate of jobs) grows proportionally with the\nnumber of queues. Assuming that each queue of type $r$ is replicated $k$ times,\nwe consider a set of policies that are periodic with period $k \\sum_r p_r$ and\nsuch that exactly $p_r$ jobs are sent in a period to each queue of type $r$.\nWhen $k\\to\\infty$, our main result shows that all the policies in this set are\nequivalent, in the sense that they yield the same mean stationary waiting time,\nand optimal, in the sense that no other policy having the same aggregate\narrival rate to \\emph{all} queues of a given type can do better in minimizing\nthe stationary mean waiting time. This property holds in a strong probabilistic\nsense. Furthermore, the limiting mean waiting time achieved by our policies is\na convex function of the arrival rate in each queue, which facilitates the\ndevelopment of a further optimization aimed at solving the fundamental problem\nabove for large systems.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.4547v3"
    },
    {
        "title": "Degradation Analysis of Probabilistic Parallel Choice Systems",
        "authors": [
            "Avinash Saxena",
            "Shrisha Rao"
        ],
        "category": "cs.PF",
        "published_year": "2014",
        "summary": "  Degradation analysis is used to analyze the useful lifetimes of systems,\ntheir failure rates, and various other system parameters like mean time to\nfailure (MTTF), mean time between failures (MTBF), and the system failure rate\n(SFR). In many systems, certain possible parallel paths of execution that have\ngreater chances of success are preferred over others. Thus we introduce here\nthe concept of probabilistic parallel choice. We use binary and $n$-ary\nprobabilistic choice operators in describing the selections of parallel paths.\nThese binary and $n$-ary probabilistic choice operators are considered so as to\nrepresent the complete system (described as a series-parallel system) in terms\nof the probabilities of selection of parallel paths and their relevant\nparameters. Our approach allows us to derive new and generalized formulae for\nsystem parameters like MTTF, MTBF, and SFR. We use a generalized exponential\ndistribution, allowing distinct installation times for individual components,\nand use this model to derive expressions for such system parameters.\n",
        "pdf_link": "http://arxiv.org/pdf/1404.5406v1"
    },
    {
        "title": "Straggler Mitigation by Delayed Relaunch of Tasks",
        "authors": [
            "Mehmet Fatih Aktas",
            "Pei Peng",
            "Emina Soljanin"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Redundancy for straggler mitigation, originally in data download and more\nrecently in distributed computing context, has been shown to be effective both\nin theory and practice. Analysis of systems with redundancy has drawn\nsignificant attention and numerous papers have studied pain and gain of\nredundancy under various service models and assumptions on the straggler\ncharacteristics. We here present a cost (pain) vs. latency (gain) analysis of\nusing simple replication or erasure coding for straggler mitigation in\nexecuting jobs with many tasks. We quantify the effect of the tail of task\nexecution times and discuss tail heaviness as a decisive parameter for the cost\nand latency of using redundancy. Specifically, we find that coded redundancy\nachieves better cost vs. latency tradeoff than simple replication and can yield\nreduction in both cost and latency under less heavy tailed execution times. We\nshow that delaying redundancy is not effective in reducing cost and that\ndelayed relaunch of stragglers can yield significant reduction in cost and\nlatency. We validate these observations by comparing with the simulations that\nuse empirical distributions extracted from Google cluster data.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.00414v1"
    },
    {
        "title": "Effective Straggler Mitigation: Which Clones Should Attack and When?",
        "authors": [
            "Mehmet Fatih Aktas",
            "Pei Peng",
            "Emina Soljanin"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Redundancy for straggler mitigation, originally in data download and more\nrecently in distributed computing context, has been shown to be effective both\nin theory and practice. Analysis of systems with redundancy has drawn\nsignificant attention and numerous papers have studied pain and gain of\nredundancy under various service models and assumptions on the straggler\ncharacteristics. We here present a cost (pain) vs. latency (gain) analysis of\nusing simple replication or erasure coding for straggler mitigation in\nexecuting jobs with many tasks. We quantify the effect of the tail of task\nexecution times and discuss tail heaviness as a decisive parameter for the cost\nand latency of using redundancy. Specifically, we find that coded redundancy\nachieves better cost vs. latency and allows for greater achievable latency and\ncost tradeoff region compared to replication and can yield reduction in both\ncost and latency under less heavy tailed execution times. We show that delaying\nredundancy is not effective in reducing cost.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.00748v1"
    },
    {
        "title": "Optimal DNN Primitive Selection with Partitioned Boolean Quadratic\n  Programming",
        "authors": [
            "Andrew Anderson",
            "David Gregg"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  Deep Neural Networks (DNNs) require very large amounts of computation both\nfor training and for inference when deployed in the field. Many different\nalgorithms have been proposed to implement the most computationally expensive\nlayers of DNNs. Further, each of these algorithms has a large number of\nvariants, which offer different trade-offs of parallelism, data locality,\nmemory footprint, and execution time. In addition, specific algorithms operate\nmuch more efficiently on specialized data layouts and formats.\n  We state the problem of optimal primitive selection in the presence of data\nformat transformations, and show that it is NP-hard by demonstrating an\nembedding in the Partitioned Boolean Quadratic Assignment problem (PBQP).\n  We propose an analytic solution via a PBQP solver, and evaluate our approach\nexperimentally by optimizing several popular DNNs using a library of more than\n70 DNN primitives, on an embedded platform and a general purpose platform. We\nshow experimentally that significant gains are possible versus the state of the\nart vendor libraries by using a principled analytic solution to the problem of\nlayout selection in the presence of data format transformations.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.01079v2"
    },
    {
        "title": "BestConfig: Tapping the Performance Potential of Systems via Automatic\n  Configuration Tuning",
        "authors": [
            "Yuqing Zhu",
            "Jianxun Liu",
            "Mengying Guo",
            "Yungang Bao",
            "Wenlong Ma",
            "Zhuoyue Liu",
            "Kunpeng Song",
            "Yingchun Yang"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  An ever increasing number of configuration parameters are provided to system\nusers. But many users have used one configuration setting across different\nworkloads, leaving untapped the performance potential of systems. A good\nconfiguration setting can greatly improve the performance of a deployed system\nunder certain workloads. But with tens or hundreds of parameters, it becomes a\nhighly costly task to decide which configuration setting leads to the best\nperformance. While such task requires the strong expertise in both the system\nand the application, users commonly lack such expertise.\n  To help users tap the performance potential of systems, we present\nBestConfig, a system for automatically finding a best configuration setting\nwithin a resource limit for a deployed system under a given application\nworkload. BestConfig is designed with an extensible architecture to automate\nthe configuration tuning for general systems. To tune system configurations\nwithin a resource limit, we propose the divide-and-diverge sampling method and\nthe recursive bound-and-search algorithm. BestConfig can improve the throughput\nof Tomcat by 75%, that of Cassandra by 63%, that of MySQL by 430%, and reduce\nthe running time of Hive join job by about 50% and that of Spark join job by\nabout 80%, solely by configuration adjustment.\n",
        "pdf_link": "http://arxiv.org/pdf/1710.03439v1"
    },
    {
        "title": "Optimizing Redundancy Levels in Master-Worker Compute Clusters for\n  Straggler Mitigation",
        "authors": [
            "Mehmet Fatih Aktas",
            "Emina Soljanin"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Runtime variability in computing systems causes some tasks to straggle and\ntake much longer than expected to complete. These straggler tasks are known to\nsignificantly slowdown distributed computation. Job execution with speculative\nexecution of redundant tasks has been the most widely deployed technique for\nmitigating the impact of stragglers, and many recent theoretical papers have\nstudied the advantages and disadvantages of using redundancy under various\nsystem and service models. However, no clear guidelines could yet be found on\nwhen, for which jobs, and how much redundancy should be employed in\nMaster-Worker compute clusters, which is the most widely adopted architecture\nin modern compute systems. We are concerned with finding a strategy for\nscheduling jobs with redundancy that works well in practice. This is a complex\noptimization problem, which we address in stages. We first use Reinforcement\nLearning (RL) techniques to learn good scheduling principles from realistic\nexperience. Building on these principles, we derive a simple scheduling policy\nand present an approximate analysis of its performance. Specifically, we derive\nexpressions to decide when and which jobs should be scheduled with how much\nredundancy. We show that policy that we devise in this way performs as good as\nthe more complex policies that are derived by RL. Finally, we extend our\napproximate analysis to the case when system employs the other widely deployed\nremedy for stragglers, which is relaunching straggler tasks after waiting some\ntime. We show that scheduling with redundancy significantly outperforms\nstraggler relaunch policy when the offered load on the system is low or\nmoderate, and performs slightly worse when the offered load is very high.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.05345v1"
    },
    {
        "title": "ALTIS: Modernizing GPGPU Benchmarking",
        "authors": [
            "Bodun Hu",
            "Christopher J. Rossbach"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This paper presents Altis, a benchmark suite for modern GPGPU computing.\nPrevious benchmark suites such as Rodinia and SHOC have served the research\ncommunity well, but were developed years ago when hardware was more limited,\nsoftware supported fewer features, and production hardware-accelerated\nworkloads were scarce. Since that time, GPU compute density and memory capacity\nhas grown exponentially, programmability features such as unified memory,\ndemand paging, and HyperQ have matured, and new workloads such as deep neural\nnetworks (DNNs), graph analytics, and crypto-currencies have emerged in\nproduction environments, stressing the hardware and software in ways that\nprevious benchmarks did not anticipate. Drawing inspiration from Rodinia and\nSHOC, Altis is a benchmark suite designed for modern GPU architectures and\nmodern GPU runtimes, representing a diverse set of application domains. By\nadopting and extending applications from Rodinia and SHOC, adding new\napplications, and focusing on CUDA platforms, Altis better represents modern\nGPGPU workloads to enable support GPGPU research in both architecture and\nsystem software.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.10347v2"
    },
    {
        "title": "Straggler Mitigation at Scale",
        "authors": [
            "Mehmet Fatih Aktas",
            "Emina Soljanin"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Runtime performance variability at the servers has been a major issue,\nhindering the predictable and scalable performance in modern distributed\nsystems. Executing requests or jobs redundantly over multiple servers has been\nshown to be effective for mitigating variability, both in theory and practice.\nSystems that employ redundancy has drawn significant attention, and numerous\npapers have analyzed the pain and gain of redundancy under various service\nmodels and assumptions on the runtime variability. This paper presents a cost\n(pain) vs. latency (gain) analysis of executing jobs of many tasks by employing\nreplicated or erasure coded redundancy. Tail heaviness of service time\nvariability is decisive on the pain and gain of redundancy and we quantify its\neffect by deriving expressions for the cost and latency. Specifically, we try\nto answer four questions: 1) How do replicated and coded redundancy compare in\nthe cost vs. latency tradeoff? 2) Can we introduce redundancy after waiting\nsome time and expect to reduce the cost? 3) Can relaunching the tasks that\nappear to be straggling after some time help to reduce cost and/or latency? 4)\nIs it effective to use redundancy and relaunching together? We validate the\nanswers we found for each of the questions via simulations that use empirical\ndistributions extracted from a Google cluster data.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.10664v2"
    },
    {
        "title": "Pinpointing Performance Inefficiencies in Java",
        "authors": [
            "Pengfei Su",
            "Qingsen Wang",
            "Milind Chabbi",
            "Xu Liu"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Many performance inefficiencies such as inappropriate choice of algorithms or\ndata structures, developers' inattention to performance, and missed compiler\noptimizations show up as wasteful memory operations. Wasteful memory operations\nare those that produce/consume data to/from memory that may have been avoided.\nWe present, JXPerf, a lightweight performance analysis tool for pinpointing\nwasteful memory operations in Java programs. Traditional byte-code\ninstrumentation for such analysis (1) introduces prohibitive overheads and (2)\nmisses inefficiencies in machine code generation. JXPerf overcomes both of\nthese problems. JXPerf uses hardware performance monitoring units to sample\nmemory locations accessed by a program and uses hardware debug registers to\nmonitor subsequent accesses to the same memory. The result is a lightweight\nmeasurement at machine-code level with attribution of inefficiencies to their\nprovenance: machine and source code within full calling contexts. JXPerf\nintroduces only 7% runtime overhead and 7% memory overhead making it useful in\nproduction. Guided by JXPerf, we optimize several Java applications by\nimproving code generation and choosing superior data structures and algorithms,\nwhich yield significant speedups.\n",
        "pdf_link": "http://arxiv.org/pdf/1906.12066v1"
    },
    {
        "title": "tinyMD: A Portable and Scalable Implementation for Pairwise Interactions\n  Simulations",
        "authors": [
            "Rafael Ravedutti L. Machado",
            "Jonas Schmitt",
            "Sebastian Eibl",
            "Jan Eitzinger",
            "Roland Leißa",
            "Sebastian Hack",
            "Arsène Pérard-Gayot",
            "Richard Membarth",
            "Harald Köstler"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  This paper investigates the suitability of the AnyDSL partial evaluation\nframework to implement tinyMD: an efficient, scalable, and portable simulation\nof pairwise interactions among particles. We compare tinyMD with the miniMD\nproxy application that scales very well on parallel supercomputers. We discuss\nthe differences between both implementations and contrast miniMD's performance\nfor single-node CPU and GPU targets, as well as its scalability on SuperMUC-NG\nand Piz Daint supercomputers. Additionaly, we demonstrate tinyMD's flexibility\nby coupling it with the waLBerla multi-physics framework. This allow us to\nexecute tinyMD simulations using the load-balancing mechanism implemented in\nwaLBerla.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.07400v1"
    },
    {
        "title": "ReLeaSER: A Reinforcement Learning Strategy for Optimizing Utilization\n  Of Ephemeral Cloud Resources",
        "authors": [
            "Mohamed Handaoui",
            "Jean-Emile Dartois",
            "Jalil Boukhobza",
            "Olivier Barais",
            "Laurent d'Orazio"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Cloud data center capacities are over-provisioned to handle demand peaks and\nhardware failures which leads to low resources' utilization. One way to improve\nresource utilization and thus reduce the total cost of ownership is to offer\nunused resources (referred to as ephemeral resources) at a lower price.\nHowever, reselling resources needs to meet the expectations of its customers in\nterms of Quality of Service. The goal is so to maximize the amount of reclaimed\nresources while avoiding SLA penalties. To achieve that, cloud providers have\nto estimate their future utilization to provide availability guarantees. The\nprediction should consider a safety margin for resources to react to\nunpredictable workloads. The challenge is to find the safety margin that\nprovides the best trade-off between the amount of resources to reclaim and the\nrisk of SLA violations. Most state-of-the-art solutions consider a fixed safety\nmargin for all types of metrics (e.g., CPU, RAM). However, a unique fixed\nmargin does not consider various workloads variations over time which may lead\nto SLA violations or/and poor utilization. In order to tackle these challenges,\nwe propose ReLeaSER, a Reinforcement Learning strategy for optimizing the\nephemeral resources' utilization in the cloud. ReLeaSER dynamically tunes the\nsafety margin at the host-level for each resource metric. The strategy learns\nfrom past prediction errors (that caused SLA violations). Our solution reduces\nsignificantly the SLA violation penalties on average by 2.7x and up to 3.4x. It\nalso improves considerably the CPs' potential savings by 27.6% on average and\nup to 43.6%.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.11208v4"
    },
    {
        "title": "Performance Modeling of Streaming Kernels and Sparse Matrix-Vector\n  Multiplication on A64FX",
        "authors": [
            "Christie L. Alappat",
            "Jan Laukemann",
            "Thomas Gruber",
            "Georg Hager",
            "Gerhard Wellein",
            "Nils Meyer",
            "Tilo Wettig"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The A64FX CPU powers the current number one supercomputer on the Top500 list.\nAlthough it is a traditional cache-based multicore processor, its peak\nperformance and memory bandwidth rival accelerator devices. Generating\nefficient code for such a new architecture requires a good understanding of its\nperformance features. Using these features, we construct the\nExecution-Cache-Memory (ECM) performance model for the A64FX processor in the\nFX700 supercomputer and validate it using streaming loops. We also identify\narchitectural peculiarities and derive optimization hints. Applying the ECM\nmodel to sparse matrix-vector multiplication (SpMV), we motivate why the CRS\nmatrix storage format is inappropriate and how the SELL-C-sigma format with\nsuitable code optimizations can achieve bandwidth saturation for SpMV.\n",
        "pdf_link": "http://arxiv.org/pdf/2009.13903v1"
    },
    {
        "title": "Redundant Loads: A Software Inefficiency Indicator",
        "authors": [
            "Pengfei Su",
            "Shasha Wen",
            "Hailong Yang",
            "Milind Chabbi",
            "Xu Liu"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Modern software packages have become increasingly complex with millions of\nlines of code and references to many external libraries. Redundant operations\nare a common performance limiter in these code bases. Missed compiler\noptimization opportunities, inappropriate data structure and algorithm choices,\nand developers' inattention to performance are some common reasons for the\nexistence of redundant operations. Developers mainly depend on compilers to\neliminate redundant operations. However, compilers' static analysis often\nmisses optimization opportunities due to ambiguities and limited analysis\nscope; automatic optimizations to algorithmic and data structural problems are\nout of scope.\n  We develop LoadSpy, a whole-program profiler to pinpoint redundant memory\nload operations, which are often a symptom of many redundant operations. The\nstrength of LoadSpy exists in identifying and quantifying redundant load\noperations in programs and associating the redundancies with program execution\ncontexts and scopes to focus developers' attention on problematic code. LoadSpy\nworks on fully optimized binaries, adopts various optimization techniques to\nreduce its overhead, and provides a rich graphic user interface, which make it\na complete developer tool. Applying LoadSpy showed that a large fraction of\nredundant loads is common in modern software packages despite highest levels of\nautomatic compiler optimizations. Guided by LoadSpy, we optimize several\nwell-known benchmarks and real-world applications, yielding significant\nspeedups.\n",
        "pdf_link": "http://arxiv.org/pdf/1902.05462v1"
    },
    {
        "title": "On Linear Learning with Manycore Processors",
        "authors": [
            "Eliza Wszola",
            "Celestine Mendler-Dünner",
            "Martin Jaggi",
            "Markus Püschel"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  A new generation of manycore processors is on the rise that offers dozens and\nmore cores on a chip and, in a sense, fuses host processor and accelerator. In\nthis paper we target the efficient training of generalized linear models on\nthese machines. We propose a novel approach for achieving parallelism which we\ncall Heterogeneous Tasks on Homogeneous Cores (HTHC). It divides the problem\ninto multiple fundamentally different tasks, which themselves are parallelized.\nFor evaluation, we design a detailed, architecture-cognizant implementation of\nour scheme on a recent 72-core Knights Landing processor that is adaptive to\nthe cache, memory, and core structure. Our library efficiently supports dense\nand sparse datasets as well as 4-bit quantized data for further possible gains\nin performance. We show benchmarks for Lasso and SVM with different data sets\nagainst straightforward parallel implementations and prior software. In\nparticular, for Lasso on dense data, we improve the state-of-the-art by an\norder of magnitude.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.00626v6"
    },
    {
        "title": "Performance Analysis of Deep Learning Workloads on Leading-edge Systems",
        "authors": [
            "Yihui Ren",
            "Shinjae Yoo",
            "Adolfy Hoisie"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This work examines the performance of leading-edge systems designed for\nmachine learning computing, including the NVIDIA DGX-2, Amazon Web Services\n(AWS) P3, IBM Power System Accelerated Compute Server AC922, and a\nconsumer-grade Exxact TensorEX TS4 GPU server. Representative deep learning\nworkloads from the fields of computer vision and natural language processing\nare the focus of the analysis. Performance analysis is performed along with a\nnumber of important dimensions. Performance of the communication interconnects\nand large and high-throughput deep learning models are considered. Different\npotential use models for the systems as standalone and in the cloud also are\nexamined. The effect of various optimization of the deep learning models and\nsystem configurations is included in the analysis.\n",
        "pdf_link": "http://arxiv.org/pdf/1905.08764v2"
    },
    {
        "title": "Scheduling in the Presence of Data Intensive Compute Jobs",
        "authors": [
            "Amir Behrouzi-Far",
            "Emina Soljanin"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We study the performance of non-adaptive scheduling policies in computing\nsystems with multiple servers. Compute jobs are mostly regular, with modest\nservice requirements. However, there are sporadic data intensive jobs, whose\nexpected service time is much higher than that of the regular jobs. Forthis\nmodel, we are interested in the effect of scheduling policieson the average\ntime a job spends in the system. To this end, we introduce two performance\nindicators in a simplified, only-arrival system. We believe that these\nperformance indicators are good predictors of the relative performance of the\npolicies in the queuing system, which is supported by simulations results.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.03348v2"
    },
    {
        "title": "Download Time Analysis for Distributed Storage Codes with Locality and\n  Availability",
        "authors": [
            "Mehmet Fatih Aktas",
            "Swanand Kadhe",
            "Emina Soljanin",
            "Alex Sprintson"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The paper presents techniques for analyzing the expected download time in\ndistributed storage systems that employ systematic availability codes. These\ncodes provide access to hot data through the systematic server containing the\nobject and multiple recovery groups. When a request for an object is received,\nit can be replicated (forked) to the systematic server and all recovery groups.\nWe first consider the low-traffic regime and present the close-form expression\nfor the download time. By comparison across systems with availability, maximum\ndistance separable (MDS), and replication codes, we demonstrate that\navailability codes can reduce download time in some settings but are not always\noptimal. In the high-traffic regime, the system consists of multiple\ninter-dependent Fork-Join queues, making exact analysis intractable.\nAccordingly, we present upper and lower bounds on the download time, and an\nM/G/1 queue approximation for several cases of interest. Via extensive\nnumerical simulations, we evaluate our bounds and demonstrate that the M/G/1\nqueue approximation has a high degree of accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/1912.09765v4"
    },
    {
        "title": "Optimal Scheduling for Maximizing Information Freshness & System\n  Performance in Industrial Cyber-Physical Systems",
        "authors": [
            "Devarpita Sinha",
            "Rajarshi Roy"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Age of Information is a newly introduced metric, getting vivid attention for\nmeasuring the freshness of information in real-time networks. This parameter\nhas evolved to guarantee the reception of timely information from the latest\nstatus update, received by a user from any real-time application. In this\npaper, we study a centralized, closed-loop, networked controlled industrial\nwireless sensor-actuator network for cyber-physical production systems. Here,\nwe jointly address the problem of transmission scheduling of sensor updates and\nthe restoration of an information flow-line after any real-time update having\nhard-deadline drops from it, resulting a break in the loop. Unlike existing\nreal-time scheduling policies that only ensure timely updates, this work aims\nto accomplish both the time-sensitivity and data freshness in new and\nregenerative real-time updates in terms of the age of information. Here, the\ncoexistence of both cyber and physical units and their individual requirements\nfor providing the quality of service to the system, as a whole, seems to be one\nof the major challenges to handle. In this work, minimization of staleness of\nthe time-critical updates to extract maximum utilization out of its information\ncontent and its effects on other network performances are thoroughly\ninvestigated. A greedy scheduling policy called Deadline-aware highest latency\nfirst has been used to solve this problem; its performance optimality is proved\nanalytically. Finally, our claim is validated by comparing the results obtained\nby our algorithm with those of other popular scheduling policies through\nextensive simulations.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.04218v2"
    },
    {
        "title": "Detecting State Transitions of a Markov Source: Sampling Frequency and\n  Age Trade-off",
        "authors": [
            "Jaya Prakash Champati",
            "Mikael Skoglund",
            "James Gross"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We consider a finite-state Discrete-Time Markov Chain (DTMC) source that can\nbe sampled for detecting the events when the DTMC transits to a new state. Our\ngoal is to study the trade-off between sampling frequency and staleness in\ndetecting the events. We argue that, for the problem at hand, using Age of\nInformation (AoI) for quantifying the staleness of a sample is conservative and\ntherefore, introduce \\textit{age penalty} for this purpose. We study two\noptimization problems: minimize average age penalty subject to an average\nsampling frequency constraint, and minimize average sampling frequency subject\nto an average age penalty constraint; both are Constrained Markov Decision\nProblems. We solve them using linear programming approach and compute Markov\npolicies that are optimal among all causal policies. Our numerical results\ndemonstrate that the computed Markov policies not only outperform optimal\nperiodic sampling policies, but also achieve sampling frequencies close to or\nlower than that of an optimal clairvoyant (non-causal) sampling policy, if a\nsmall age penalty is allowed.\n",
        "pdf_link": "http://arxiv.org/pdf/2001.10381v2"
    },
    {
        "title": "A review of analytical performance modeling and its role in computer\n  engineering and science",
        "authors": [
            "Y. C. Tay"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  This article is a review of analytical performance modeling for computer\nsystems. It discusses the motivation for this area of research, examines key\nissues, introduces some ideas, illustrates how it is applied, and points out a\nrole that it can play in developing Computer Science.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.13144v1"
    },
    {
        "title": "Quantum Monte Carlo for large chemical systems: Implementing efficient\n  strategies for petascale platforms and beyond",
        "authors": [
            "Anthony Scemama",
            "Michel Caffarel",
            "Emmanuel Oseret",
            "William Jalby"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Various strategies to implement efficiently QMC simulations for large\nchemical systems are presented. These include: i.) the introduction of an\nefficient algorithm to calculate the computationally expensive Slater matrices.\nThis novel scheme is based on the use of the highly localized character of\natomic Gaussian basis functions (not the molecular orbitals as usually done),\nii.) the possibility of keeping the memory footprint minimal, iii.) the\nimportant enhancement of single-core performance when efficient optimization\ntools are employed, and iv.) the definition of a universal, dynamic,\nfault-tolerant, and load-balanced computational framework adapted to all kinds\nof computational platforms (massively parallel machines, clusters, or\ndistributed grids). These strategies have been implemented in the QMC=Chem code\ndeveloped at Toulouse and illustrated with numerical applications on small\npeptides of increasing sizes (158, 434, 1056 and 1731 electrons). Using 10k-80k\ncomputing cores of the Curie machine (GENCI-TGCC-CEA, France) QMC=Chem has been\nshown to be capable of running at the petascale level, thus demonstrating that\nfor this machine a large part of the peak performance can be achieved.\nImplementation of large-scale QMC simulations for future exascale platforms\nwith a comparable level of efficiency is expected to be feasible.\n",
        "pdf_link": "http://arxiv.org/pdf/1209.6630v2"
    },
    {
        "title": "Crossing the Architectural Barrier: Evaluating Representative Regions of\n  Parallel HPC Applications",
        "authors": [
            "Alexandra Ferreron",
            "Radhika Jagtap",
            "Sascha Bischoff",
            "Roxana Rusitoru"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Exascale computing will get mankind closer to solving important social,\nscientific and engineering problems. Due to high prototyping costs, High\nPerformance Computing (HPC) system architects make use of simulation models for\ndesign space exploration and hardware-software co-design. However, as HPC\nsystems reach exascale proportions, the cost of simulation increases, since\nsimulators themselves are largely single-threaded. Tools for selecting\nrepresentative parts of parallel applications to reduce running costs are\nwidespread, e.g., BarrierPoint achieves this by analysing, in simulation,\nabstract characteristics such as basic blocks and reuse distances. However,\narchitectures new to HPC have a limited set of tools available.\n  In this work, we provide an independent cross-architectural evaluation on\nreal hardware - across Intel and ARM - of the BarrierPoint methodology, when\napplied to parallel HPC proxy applications. We present both cases: when the\nmethodology can be applied and when it cannot. In the former case, results show\nthat we can predict the performance of full application execution by running\nshorter representative sections. In the latter case, we dive into the\nunderlying issues and suggest improvements. We demonstrate a total simulation\ntime reduction of up to 178x, whilst keeping the error below 2.3% for both\ncycles and instructions.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.09584v1"
    },
    {
        "title": "Dispatching to Parallel Servers: Solutions of Poisson's Equation for\n  First-Policy Improvement",
        "authors": [
            "Olivier Bilenne"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Policy iteration techniques for multiple-server dispatching rely on the\ncomputation of value functions. In this context, we consider the\ncontinuous-space M/G/1-FCFS queue endowed with an arbitrarily-designed cost\nfunction for the waiting times of the incoming jobs. The associated relative\nvalue function is a solution of Poisson's equation for Markov chains, which in\nthis work we solve in the Laplace transform domain by considering an ancillary,\nunderlying stochastic process extended to (imaginary) negative backlog states.\nThis construction enables us to issue closed-form relative value functions for\npolynomial and exponential cost functions and for piecewise compositions of the\nlatter, in turn permitting the derivation of interval bounds for the relative\nvalue function in the form of power series or trigonometric sums. We review\nvarious cost approximation schemes and assess the convergence of the interval\nbounds these induce on the relative value function. Namely: Taylor expansions\n(divergent, except for a narrow class of entire functions with low orders of\ngrowth), and uniform approximation schemes (polynomials, trigonometric), which\nachieve optimal convergence rates over finite intervals. This study addresses\nall the steps to implementing dispatching policies for systems of parallel\nservers, from the specification of general cost functions towards the\ncomputation of interval bounds for the relative value functions and the exact\nimplementation of the first-policy improvement step.\n",
        "pdf_link": "http://arxiv.org/pdf/1803.10688v5"
    },
    {
        "title": "A refined mean field approximation of synchronous discrete-time\n  population models",
        "authors": [
            "Nicolas Gast",
            "Diego Latella",
            "Mieke Massink"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  Mean field approximation is a popular method to study the behaviour of\nstochastic models composed of a large number of interacting objects. When the\nobjects are asynchronous, the mean field approximation of a population model\ncan be expressed as an ordinary differential equation. When the objects are\n(clock-) synchronous the mean field approximation is a discrete time dynamical\nsystem. We focus on the latter.We study the accuracy of mean field\napproximation when this approximation is a discrete-time dynamical system. We\nextend a result that was shown for the continuous time case and we prove that\nexpected performance indicators estimated by mean field approximation are\n$O(1/N)$-accurate. We provide simple expressions to effectively compute the\nasymptotic error of mean field approximation, for finite time-horizon and\nsteady-state, and we use this computed error to propose what we call a\n\\emph{refined} mean field approximation. We show, by using a few numerical\nexamples, that this technique improves the quality of approximation compared to\nthe classical mean field approximation, especially for relatively small\npopulation sizes.\n",
        "pdf_link": "http://arxiv.org/pdf/1807.08585v1"
    },
    {
        "title": "Defence Efficiency",
        "authors": [
            "Gleb Polevoy"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In order to automate actions, such as defences against network attacks, one\nneeds to quantify their efficiency. This can subsequently be used in\npost-evaluation, learning, etc. In order to quantify the defence efficiency as\na function of the impact of the defence and its total cost, we present several\nnatural requirements from such a definition of efficiency and provide a natural\ndefinition that complies with these requirements. Next, we precisely\ncharacterize our definition of efficiency by the axiomatic approach; namely, we\nstrengthen the original requirements from such a definition and prove that the\ngiven definition is the unique definition that satisfies those requirements.\nFinally, we generalize the definition to the case of any number of input\nvariables in two natural ways, and compare these generalizations.\n",
        "pdf_link": "http://arxiv.org/pdf/1904.07141v1"
    },
    {
        "title": "Analytical Performance Models for NoCs with Multiple Priority Traffic\n  Classes",
        "authors": [
            "Sumit K. Mandal",
            "Raid Ayoub",
            "Michael Kishinevsky",
            "Umit Y. Ogras"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Networks-on-chip (NoCs) have become the standard for interconnect solutions\nin industrial designs ranging from client CPUs to many-core\nchip-multiprocessors. Since NoCs play a vital role in system performance and\npower consumption, pre-silicon evaluation environments include cycle-accurate\nNoC simulators. Long simulations increase the execution time of evaluation\nframeworks, which are already notoriously slow, and prohibit design-space\nexploration. Existing analytical NoC models, which assume fair arbitration,\ncannot replace these simulations since industrial NoCs typically employ\npriority schedulers and multiple priority classes. To address this limitation,\nwe propose a systematic approach to construct priority-aware analytical\nperformance models using micro-architecture specifications and input traffic.\nOur approach consists of developing two novel transformations of queuing system\nand designing an algorithm which iteratively uses these two transformations to\nestimate end-to-end latency. Our approach decomposes the given NoC into\nindividual queues with modified service time to enable accurate and scalable\nlatency computations. Specifically, we introduce novel transformations along\nwith an algorithm that iteratively applies these transformations to decompose\nthe queuing system. Experimental evaluations using real architectures and\napplications show high accuracy of 97% and up to 2.5x speedup in full-system\nsimulation.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02408v2"
    },
    {
        "title": "Redundancy Scheduling in Systems with Bi-Modal Job Service Time\n  Distribution",
        "authors": [
            "Amir Behrouzi-Far",
            "Emina Soljanin"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Queuing systems with redundant requests have drawn great attention because of\ntheir promise to reduce the job completion time and variability. Despite a\nlarge body of work on the topic, we are still far from fully understanding the\nbenefits of redundancy in practice. We here take one step towards practical\nsystems by studying queuing systems with bi-modal job service time\ndistribution. Such distributions have been observed in practice, as can be seen\nin, e.g., Google cluster traces. We develop an analogy to a classical urns and\nballs problem, and use it to study the queuing time performance of two\nnon-adaptive classical scheduling policies: random and round-robin. We\nintroduce new performance indicators in the analogous model, and argue that\nthey are good predictors of the queuing time in non-adaptive scheduling\npolicies. We then propose a non-adaptive scheduling policy that is based on\ncombinatorial designs, and show that it has better performance indicators.\nSimulations confirm that the proposed scheduling policy, as the performance\nindicators suggest, reduces the queuing times compared to random and\nround-robin scheduling.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02415v2"
    },
    {
        "title": "A Repairable System Supported by Two Spare Units and Serviced by Two\n  Types of Repairers",
        "authors": [
            "Vahid Andalib",
            "Jyotirmoy Sarkar"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  We study a one-unit repairable system, supported by two identical spare units\non cold standby, and serviced by two types of repairers. The model applies, for\ninstance, to ANSI (American National Standard Institute) centrifugal pumps in a\nchemical plant. The failed unit undergoes repair either by an in-house repairer\nwithin a random or deterministic patience time, or else by a visiting expert\nrepairer. The expert repairs one or all failed units before leaving, and does\nso faster but at a higher cost rate than the regular repairer. Four models\narise depending on the number of repairs done by the expert and the nature of\nthe patience time. We compare these models based on the limiting availability\n$A_{\\infty}$, and the limiting profit per unit time $\\omega$, using semi-Markov\nprocesses, when all distributions are exponential. As anticipated, to maximize\n$A_{\\infty}$, the expert should repair all failed units. To maximize $\\omega$,\na suitably chosen deterministic patience time is better than a random patience\ntime. Furthermore, given all cost parameters, we determine the optimum number\nof repairs the expert should complete, and the optimum patience time given to\nthe regular repairer in order to maximize $\\omega$.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02547v1"
    },
    {
        "title": "HPC AI500: A Benchmark Suite for HPC AI Systems",
        "authors": [
            "Zihan Jiang",
            "Wanling Gao",
            "Lei Wang",
            "Xingwang Xiong",
            "Yuchen Zhang",
            "Xu Wen",
            "Chunjie Luo",
            "Hainan Ye",
            "Yunquan Zhang",
            "Shengzhong Feng",
            "Kenli Li",
            "Weijia Xu",
            "Jianfeng Zhan"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In recent years, with the trend of applying deep learning (DL) in high\nperformance scientific computing, the unique characteristics of emerging DL\nworkloads in HPC raise great challenges in designing, implementing HPC AI\nsystems. The community needs a new yard stick for evaluating the future HPC\nsystems. In this paper, we propose HPC AI500 --- a benchmark suite for\nevaluating HPC systems that running scientific DL workloads. Covering the most\nrepresentative scientific fields, each workload from HPC AI500 is based on\nreal-world scientific DL applications. Currently, we choose 14 scientific DL\nbenchmarks from perspectives of application scenarios, data sets, and software\nstack. We propose a set of metrics for comprehensively evaluating the HPC AI\nsystems, considering both accuracy, performance as well as power and cost. We\nprovide a scalable reference implementation of HPC AI500. HPC AI500 is a part\nof the open-source AIBench project, the specification and source code are\npublicly available from \\url{http://www.benchcouncil.org/AIBench/index.html}.\n",
        "pdf_link": "http://arxiv.org/pdf/1908.02607v3"
    },
    {
        "title": "Rule Designs for Optimal Online Game Matchmaking",
        "authors": [
            "Mingkuan Xu",
            "Yang Yu",
            "Chenye Wu"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Online games are the most popular form of entertainment among youngsters as\nwell as elders. Recognized as e-Sports, they may become an official part of the\nOlympic Games by 2020. However, a long waiting time for matchmaking will\nlargely affect players' experiences. We examine different matchmaking\nmechanisms for 2v2 games. By casting the mechanisms into a queueing theoretic\nframework, we decompose the rule design process into a sequence of decision\nmaking problems, and derive the optimal mechanism with minimum expected waiting\ntime. We further the result by exploring additional static as well as dynamic\nrule designs' impacts. In the static setting, we consider the game allows\nplayers to choose sides before the battle. In the dynamic setting, we consider\nthe game offers multiple zones for players of different skill levels. In both\nsettings, we examine the value of choice-free players. Closed form expressions\nfor the expected waiting time in different settings illuminate the guidelines\nfor online game rule designs.\n",
        "pdf_link": "http://arxiv.org/pdf/1911.11852v1"
    },
    {
        "title": "Non-Asymptotic Performance Analysis of Size-Based Routing Policies",
        "authors": [
            "E. Bachmat",
            "J. Doncel"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We investigate the performance of two size-based routing policies: the Size\nInterval Task Assignment (SITA) and Task Assignment based on Guessing Size\n(TAGS). We consider a system with two servers and Bounded Pareto distributed\njob sizes with tail parameter 1 where the difference between the size of the\nlargest and the smallest job is finite. We show that the ratio between the mean\nwaiting time of TAGS over the mean waiting time of SITA is unbounded when the\nlargest job size is large and the arrival rate times the largest job size is\nless than one. We provide numerical experiments that show that our theoretical\nfindings extend to Bounded Pareto distributed job sizes with tail parameter\ndifferent to 1.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.03695v1"
    },
    {
        "title": "Exponential Convergence Rate for the Asymptotic Optimality of Whittle\n  Index Policy",
        "authors": [
            "Nicolas Gast",
            "Bruno Gaujal",
            "Chen Yan"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We evaluate the performance of Whittle index policy for restless Markovian\nbandits, when the number of bandits grows. It is proven in [30] that this\nperformance is asymptotically optimal if the bandits are indexable and the\nassociated deterministic system has a global attractor fixed point. In this\npaper we show that, under the same conditions, the convergence rate is\nexponential in the number of bandits, unless the fixed point is singular (to be\ndefined later). Our proof is based on the nature of the deterministic equation\ngoverning the stochastic system: We show that it is a piecewise affine\ncontinuous dynamical system inside the simplex of the empirical measure of the\nbandits. Using simulations and numerical solvers, we also investigate the cases\nwhere the conditions for the exponential rate theorem are violated, notably\nwhen attracting limit cycles appear, or when the fixed point is singular. We\nillustrate our theorem on a Markovian fading channel model, which has been well\nstudied in the literature. Finally, we extend our synchronous model results to\nthe asynchronous model.\n",
        "pdf_link": "http://arxiv.org/pdf/2012.09064v1"
    },
    {
        "title": "An In-Depth Investigation of the Performance Characteristics of\n  Hyperledger Fabric",
        "authors": [
            "Tobias Guggenberger",
            "Johannes Sedlmeir",
            "Gilbert Fridgen",
            "André Luckow"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Private permissioned blockchains are deployed in ever greater numbers to\nfacilitate cross-organizational processes in various industries, particularly\nin supply chain management. One popular example of this trend is Hyperledger\nFabric. Compared to public permissionless blockchains, it promises improved\nperformance and provides certain features that address key requirements of\nenterprises. However, also permissioned blockchains are still not as scalable\nas centralized systems, and due to the scarcity of theoretical results and\nempirical data, their real-world performance cannot be predicted with the\nnecessary precision. We intend to address this issue by conducting an in-depth\nperformance analysis of Hyperledger Fabric. The paper presents a detailed\ncompilation of various performance characteristics using an enhanced version of\nthe Distributed Ledger Performance Scan (DLPS). Researchers and practitioners\nalike can use the various performance properties identified and discussed as\nguidelines to better configure and implement their Hyperledger Fabric network.\nLikewise, they are encouraged to use the DLPS framework to conduct their\nmeasurements.\n",
        "pdf_link": "http://arxiv.org/pdf/2102.07731v2"
    },
    {
        "title": "Consensus in Blockchain Systems with Low Network Throughput: A\n  Systematic Mapping Study",
        "authors": [
            "Henrik Knudsen",
            "Jakob Svennevik Notland",
            "Peter Halland Haro",
            "Truls Bakkejord Ræder",
            "Jingyue Li"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Blockchain technologies originate from cryptocurrencies. Thus, most\nblockchain technologies assume an environment with a fast and stable network.\nHowever, in some blockchain-based systems, e.g., supply chain management (SCM)\nsystems, some Internet of Things (IOT) nodes can only rely on the low-quality\nnetwork sometimes to achieve consensus. Thus, it is critical to understand the\napplicability of existing consensus algorithms in such environments. We\nperformed a systematic mapping study to evaluate and compare existing consensus\nmechanisms' capability to provide integrity and security with varying network\nproperties. Our study identified 25 state-of-the-art consensus algorithms from\npublished and preprint literature. We categorized and compared the consensus\nalgorithms qualitatively based on established performance and integrity metrics\nand well-known blockchain security issues. Results show that consensus\nalgorithms rely on the synchronous network for correctness cannot provide the\nexpected integrity. Such consensus algorithms may also be vulnerable to\ndistributed-denial-of-service (DDOS) and routing attacks, given limited network\nthroughput. Conversely, asynchronous consensus algorithms, e.g.,\nHoney-BadgerBFT, are deemed more robust against many of these attacks and may\nprovide high integrity in asynchrony events.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.02916v1"
    },
    {
        "title": "ECM modeling and performance tuning of SpMV and Lattice QCD on A64FX",
        "authors": [
            "Christie Alappat",
            "Nils Meyer",
            "Jan Laukemann",
            "Thomas Gruber",
            "Georg Hager",
            "Gerhard Wellein",
            "Tilo Wettig"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The A64FX CPU is arguably the most powerful Arm-based processor design to\ndate. Although it is a traditional cache-based multicore processor, its peak\nperformance and memory bandwidth rival accelerator devices. A good\nunderstanding of its performance features is of paramount importance for\ndevelopers who wish to leverage its full potential. We present an architectural\nanalysis of the A64FX used in the Fujitsu FX1000 supercomputer at a level of\ndetail that allows for the construction of Execution-Cache-Memory (ECM)\nperformance models for steady-state loops. In the process we identify\narchitectural peculiarities that point to viable generic optimization\nstrategies. After validating the model using simple streaming loops we apply\nthe insight gained to sparse matrix-vector multiplication (SpMV) and the domain\nwall (DW) kernel from quantum chromodynamics (QCD). For SpMV we show why the\nCRS matrix storage format is not a good practical choice on this architecture\nand how the SELL-C-sigma format can achieve bandwidth saturation. For the DW\nkernel we provide a cache-reuse analysis and show how an appropriate choice of\ndata layout for complex arrays can realize memory-bandwidth saturation in this\ncase as well. A comparison with state-of-the-art high-end Intel Cascade Lake AP\nand Nvidia V100 systems puts the capabilities of the A64FX into perspective. We\nalso explore the potential for power optimizations using the tuning knobs\nprovided by the Fugaku system, achieving energy savings of about 31% for SpMV\nand 18% for DW.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.03013v2"
    },
    {
        "title": "Characterization of the Gittins index for sequential multistage jobs",
        "authors": [
            "Samuli Aalto"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The optimal scheduling problem in single-server queueing systems is a classic\nproblem in queueing theory. The Gittins index policy is known to be the optimal\npreemptive nonanticipating policy (both for the open version of the problem\nwith Poisson arrivals and the closed version without arrivals) minimizing the\nexpected holding costs. While the Gittins index is thoroughly characterized for\nordinary jobs whose state is described by the attained service, it is not at\nall the case with jobs that have more complex structure. Recently, a class of\nsuch jobs, the multistage jobs, were introduced, and it was shown that the\ncomputation of Gittins index of a multistage job reduces into separable\ncomputations for the individual stages. The characterization is, however,\nindirect in the sense that it relies on the recursion for an auxiliary function\n(so called SJP function) and not for the Gittins index itself. In this paper,\nwe answer the natural question: Is it possible to compute the Gittins index for\na multistage job more directly by recursively combining the Gittins indexes of\nits individual stages? According to our results, it seems to be possible, at\nleast, for sequential multistage jobs that have a fixed (deterministic)\nsequence of stages. We prove this for sequential two-stage jobs that have\nmonotonous hazard rates in both stages, but our numerical experiments give an\nindication that the result could possibly be generalized to any sequential\nmultistage jobs. Our approach, in this paper, is based on the Whittle index\noriginally developed in the context of restless bandits.\n",
        "pdf_link": "http://arxiv.org/pdf/2103.10646v1"
    },
    {
        "title": "AttMEMO : Accelerating Transformers with Memoization on Big Memory\n  Systems",
        "authors": [
            "Yuan Feng",
            "Hyeran Jeon",
            "Filip Blagojevic",
            "Cyril Guyot",
            "Qing Li",
            "Dong Li"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Transformer models gain popularity because of their superior inference\naccuracy and inference throughput. However, the transformer is\ncomputation-intensive, causing a long inference time. The existing works on\ntransformer inference acceleration have limitations caused by either the\nmodification of transformer architectures or the need of specialized hardware.\nIn this paper, we identify the opportunities of using memoization to accelerate\nthe self-attention mechanism in transformers without the above limitations.\nBuilt upon a unique observation that there is rich similarity in attention\ncomputation across inference sequences, we build a memoization database that\nleverages the emerging big memory system. We introduce a novel embedding\ntechnique to find semantically similar inputs to identify computation\nsimilarity. We also introduce a series of techniques such as memory mapping and\nselective memoization to avoid memory copy and unnecessary overhead. We enable\n22% inference-latency reduction on average (up to 68%) with negligible loss in\ninference accuracy.\n",
        "pdf_link": "http://arxiv.org/pdf/2301.09262v2"
    },
    {
        "title": "Cross-entropy optimisation of importance sampling parameters for\n  statistical model checking",
        "authors": [
            "Cyrille Jégourel",
            "Axel Legay",
            "Sean Sedwards"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  Statistical model checking avoids the exponential growth of states associated\nwith probabilistic model checking by estimating properties from multiple\nexecutions of a system and by giving results within confidence bounds. Rare\nproperties are often very important but pose a particular challenge for\nsimulation-based approaches, hence a key objective under these circumstances is\nto reduce the number and length of simulations necessary to produce a given\nlevel of confidence. Importance sampling is a well-established technique that\nachieves this, however to maintain the advantages of statistical model checking\nit is necessary to find good importance sampling distributions without\nconsidering the entire state space.\n  Motivated by the above, we present a simple algorithm that uses the notion of\ncross-entropy to find the optimal parameters for an importance sampling\ndistribution. In contrast to previous work, our algorithm uses a low\ndimensional vector of parameters to define this distribution and thus avoids\nthe often intractable explicit representation of a transition matrix. We show\nthat our parametrisation leads to a unique optimum and can produce many orders\nof magnitude improvement in simulation efficiency. We demonstrate the efficacy\nof our methodology by applying it to models from reliability engineering and\nbiochemistry.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.5229v1"
    },
    {
        "title": "A Note on Disk Drag Dynamics",
        "authors": [
            "Neil J. Gunther"
        ],
        "category": "cs.PF",
        "published_year": "2012",
        "summary": "  The electrical power consumed by typical magnetic hard disk drives (HDD) not\nonly increases linearly with the number of spindles but, more significantly, it\nincreases as very fast power-laws of speed (RPM) and diameter. Since the\ntheoretical basis for this relationship is neither well-known nor readily\naccessible in the literature, we show how these exponents arise from\naerodynamic disk drag and discuss their import for green storage capacity\nplanning.\n",
        "pdf_link": "http://arxiv.org/pdf/1201.6402v1"
    },
    {
        "title": "Solving the Klein-Gordon equation using Fourier spectral methods: A\n  benchmark test for computer performance",
        "authors": [
            "S. Aseeri",
            "O. Batrašev",
            "M. Icardi",
            "B. Leu",
            "A. Liu",
            "N. Li",
            "B. K. Muite",
            "E. Müller",
            "B. Palen",
            "M. Quell",
            "H. Servat",
            "P. Sheth",
            "R. Speck",
            "M. Van Moer",
            "J. Vienne"
        ],
        "category": "cs.PF",
        "published_year": "2015",
        "summary": "  The cubic Klein-Gordon equation is a simple but non-trivial partial\ndifferential equation whose numerical solution has the main building blocks\nrequired for the solution of many other partial differential equations. In this\nstudy, the library 2DECOMP&FFT is used in a Fourier spectral scheme to solve\nthe Klein-Gordon equation and strong scaling of the code is examined on\nthirteen different machines for a problem size of 512^3. The results are useful\nin assessing likely performance of other parallel fast Fourier transform based\nprograms for solving partial differential equations. The problem is chosen to\nbe large enough to solve on a workstation, yet also of interest to solve\nquickly on a supercomputer, in particular for parametric studies. Unlike other\nhigh performance computing benchmarks, for this problem size, the time to\nsolution will not be improved by simply building a bigger supercomputer.\n",
        "pdf_link": "http://arxiv.org/pdf/1501.04552v1"
    },
    {
        "title": "PageRank Pipeline Benchmark: Proposal for a Holistic System Benchmark\n  for Big-Data Platforms",
        "authors": [
            "Patrick Dreher",
            "Chansup Byun",
            "Chris Hill",
            "Vijay Gadepally",
            "Bradley Kuszmaul",
            "Jeremy Kepner"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  The rise of big data systems has created a need for benchmarks to measure and\ncompare the capabilities of these systems. Big data benchmarks present unique\nscalability challenges. The supercomputing community has wrestled with these\nchallenges for decades and developed methodologies for creating rigorous\nscalable benchmarks (e.g., HPC Challenge). The proposed PageRank pipeline\nbenchmark employs supercomputing benchmarking methodologies to create a\nscalable benchmark that is reflective of many real-world big data processing\nsystems. The PageRank pipeline benchmark builds on existing prior scalable\nbenchmarks (Graph500, Sort, and PageRank) to create a holistic benchmark with\nmultiple integrated kernels that can be run together or independently. Each\nkernel is well defined mathematically and can be implemented in any programming\nenvironment. The linear algebraic nature of PageRank makes it well suited to\nbeing implemented using the GraphBLAS standard. The computations are simple\nenough that performance predictions can be made based on simple computing\nhardware models. The surrounding kernels provide the context for each kernel\nthat allows rigorous definition of both the input and the output for each\nkernel. Furthermore, since the proposed PageRank pipeline benchmark is scalable\nin both problem size and hardware, it can be used to measure and quantitatively\ncompare a wide range of present day and future systems. Serial implementations\nin C++, Python, Python with Pandas, Matlab, Octave, and Julia have been\nimplemented and their single threaded performance has been measured.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.01876v2"
    },
    {
        "title": "Group-Server Queues",
        "authors": [
            "Quan-Lin Li",
            "Jing-Yu Ma",
            "Mingzhou Xie",
            "Li Xia"
        ],
        "category": "cs.PF",
        "published_year": "2017",
        "summary": "  By analyzing energy-efficient management of data centers, this paper proposes\nand develops a class of interesting {\\it Group-Server Queues}, and establishes\ntwo representative group-server queues through loss networks and impatient\ncustomers, respectively. Furthermore, such two group-server queues are given\nmodel descriptions and necessary interpretation. Also, simple mathematical\ndiscussion is provided, and simulations are made to study the expected queue\nlengths, the expected sojourn times and the expected virtual service times. In\naddition, this paper also shows that this class of group-server queues are\noften encountered in many other practical areas including communication\nnetworks, manufacturing systems, transportation networks, financial networks\nand healthcare systems. Note that the group-server queues are always used to\ndesign effectively dynamic control mechanisms through regrouping and\nrecombining such many servers in a large-scale service system by means of, for\nexample, bilateral threshold control, and customers transfer to the buffer or\nserver groups. This leads to the large-scale service system that is divided\ninto several adaptive and self-organizing subsystems through scheduling of\nbatch customers and regrouping of service resources, which make the middle\nlayer of this service system more effectively managed and strengthened under a\ndynamic, real-time and even reward optimal framework. Based on this,\nperformance of such a large-scale service system may be improved greatly in\nterms of introducing and analyzing such group-server queues. Therefore, not\nonly analysis of group-server queues is regarded as a new interesting research\ndirection, but there also exists many theoretical challenges, basic\ndifficulties and open problems in the area of queueing networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1706.03341v2"
    },
    {
        "title": "Predicting the confirmation time of Bitcoin transactions",
        "authors": [
            "David Koops"
        ],
        "category": "cs.PF",
        "published_year": "2018",
        "summary": "  We study the probabilistic distribution of the confirmation time of Bitcoin\ntransactions, conditional on the current memory pool (i.e., the queue of\ntransactions awaiting confirmation). The results of this paper are particularly\ninteresting for users that want to make a Bitcoin transaction during\n`heavy-traffic situations', when the transaction demand exceeds the block\ncapacity. In such situations, Bitcoin users tend to bid up the transaction\nfees, in order to gain priority over other users that pay a lower fee. We argue\nthat the time until a Bitcoin transaction is confirmed can be modelled as a\nparticular stochastic fluid queueing process (to be precise: a\nCram\\'er-Lundberg process). We approximate the queueing process in two\ndifferent ways. The first approach leads to a lower bound on the confirmation\nprobability, which becomes increasingly tight as traffic decreases. The second\napproach relies on a diffusion approximation with a continuity correction,\nwhich becomes increasingly accurate as traffic intensifies. The accuracy of the\napproximations under different traffic loads are evaluated in a simulation\nstudy.\n",
        "pdf_link": "http://arxiv.org/pdf/1809.10596v1"
    },
    {
        "title": "Towards the Tradeoff Between Service Performance and Information\n  Freshness",
        "authors": [
            "Zhongdong Liu",
            "Bo Ji"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  The last decade has witnessed an unprecedented growth in the demand for\ndata-driven real-time services. These services are fueled by emerging\napplications that require rapidly injecting data streams and computing updated\nanalytics results in real-time. In many of such applications, the computing\nresources are often shared for processing both updates from information sources\nand queries from end users. This requires joint scheduling of updates and\nqueries because the service provider needs to make a critical decision upon\nreceiving a user query: either it responds immediately with currently available\nbut possibly stale information, or it first processes new updates and then\nresponds with fresher information. Hence, the tradeoff between service\nperformance and information freshness naturally arises in this context. To that\nend, we propose a simple single-server two-queue model that captures the\ncoupled scheduling of updates and queries and aim to design scheduling policies\nthat can properly address the important tradeoff between performance and\nfreshness. Specifically, we consider the response time as a performance metric\nand the Age of Information (AoI) as a freshness metric. After demonstrating the\nlimitations of the simplest FCFS policy, we propose two threshold-based\npolicies: the Query-k policy that prioritizes queries and the Update-k policy\nthat prioritizes updates. Then, we rigorously analyze both the response time\nand the Peak AoI (PAoI) of the threshold-based policies. Further, we propose\nthe Joint-(M,N) policy, which allows flexibly prioritizing updates or queries\nthrough choosing different values of two thresholds M and N. Finally, we\nconduct simulations to evaluate the response time and the PAoI of the proposed\npolicies. The results show that our proposed threshold-based policies can\neffectively control the balance between performance and freshness.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.00826v2"
    },
    {
        "title": "Optimal Asynchronous Dynamic Policies in Energy-Efficient Data Centers",
        "authors": [
            "Jing-Yu Ma",
            "Quan-Lin Li",
            "Li Xia"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  In this paper, we use a Markov decision process to find optimal asynchronous\npolicy of an energy-efficient data center with two groups of heterogeneous\nservers, a finite buffer, and a fast setup process at sleep state. Servers in\nGroup 1 always work. Servers in Group 2 may either work or sleep, and a fast\nsetup process occurs when server's states are changed from sleep to work. In\nsuch a data center, an asynchronous dynamic policy is designed as two\nsub-policies: The setup policy and the sleep policy, which determine the switch\nrule between the work and sleep states for the servers in Group 2. To analyze\nthe optimal asynchronous dynamic policy, we apply the Markov decision process\nto establish a policy-based Poisson equation, which provides expression for the\nunique solution of the performance potential by means of the RG-factorization.\nBased on this, we can characterize the monotonicity and optimality of the\nlong-run average profit of the data center with respect to the asynchronous\ndynamic policy under different service prices. Furthermore, we prove that the\nbang-bang control is always optimal for this optimization problem, and supports\na threshold-type dynamic control in the energy-efficient data center. We hope\nthat the methodology and results derived in this paper can shed light to the\nstudy of more general energy-efficient data centers.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.03371v2"
    },
    {
        "title": "Anonymity Mixes as (Partial) Assembly Queues: Modeling and Analysis",
        "authors": [
            "Mehmet Fatih Aktas",
            "Emina Soljanin"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Anonymity platforms route the traffic over a network of special routers that\nare known as mixes and implement various traffic disruption techniques to hide\nthe communicating users' identities. Batch mixes in particular anonymize\ncommunicating peers by allowing message exchange to take place only after a\nsufficient number of messages (a batch) accumulate, thus introducing delay. We\nintroduce a queueing model for batch mix and study its delay properties. Our\nanalysis shows that delay of a batch mix grows quickly as the batch size gets\nclose to the number of senders connected to the mix. We then propose a\nrandomized batch mixing strategy and show that it achieves much better delay\nscaling in terms of the batch size. However, randomization is shown to reduce\nthe anonymity preserving capabilities of the mix. We also observe that queueing\nmodels are particularly useful to study anonymity metrics that are more\npractically relevant such as the time-to-deanonymize metric.\n",
        "pdf_link": "http://arxiv.org/pdf/1907.11603v1"
    },
    {
        "title": "Semi-Analytical Model for Design and Analysis of On-Orbit Servicing\n  Architecture",
        "authors": [
            "Koki Ho",
            "Hai Wang",
            "Paul A. DeTrempe",
            "Tristan Sarton du Jonchay",
            "Kento Tomita"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Robotic on-orbit servicing (OOS) is expected to be a key technology and\nconcept for future sustainable space exploration. This paper develops a\nsemi-analytical model for OOS systems analysis, responding to the growing needs\nand ongoing trend of robotic OOS. An OOS infrastructure system is considered\nwhose goal is to provide responsive services to the random failures of a set of\ncustomer modular satellites distributed in space (e.g., at the geosynchronous\nequatorial orbit). The considered OOS architecture is comprised of a servicer\nthat travels and provides module-replacement services to the customer\nsatellites, an on-orbit depot to store the spares, and a series of launch\nvehicles to replenish the depot. The OOS system performance is analyzed by\nevaluating the mean waiting time before service completion for a given failure\nand its relationship with the depot capacity. Leveraging the queueing theory\nand inventory management methods, the developed semi-analytical model is\ncapable of analyzing the OOS system performance without relying on\ncomputationally costly simulations. The effectiveness of the proposed model is\ndemonstrated using a case study compared with simulation results. This paper is\nexpected to provide a critical step to push the research frontier of\nanalytical/semi-analytical models development for complex space systems design.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.03169v3"
    },
    {
        "title": "ClassyTune: A Performance Auto-Tuner for Systems in the Cloud",
        "authors": [
            "Yuqing Zhu",
            "Jianxun Liu"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  Performance tuning can improve the system performance and thus enable the\nreduction of cloud computing resources needed to support an application. Due to\nthe ever increasing number of parameters and complexity of systems, there is a\nnecessity to automate performance tuning for the complicated systems in the\ncloud. The state-of-the-art tuning methods are adopting either the\nexperience-driven tuning approach or the data-driven one. Data-driven tuning is\nattracting increasing attentions, as it has wider applicability. But existing\ndata-driven methods cannot fully address the challenges of sample scarcity and\nhigh dimensionality simultaneously. We present ClassyTune, a data-driven\nautomatic configuration tuning tool for cloud systems. ClassyTune exploits the\nmachine learning model of classification for auto-tuning. This exploitation\nenables the induction of more training samples without increasing the input\ndimension. Experiments on seven popular systems in the cloud show that\nClassyTune can effectively tune system performance to seven times higher for\nhigh-dimensional configuration space, outperforming expert tuning and the\nstate-of-the-art auto-tuning solutions. We also describe a use case in which\nperformance tuning enables the reduction of 33% computing resources needed to\nrun an online stateless service.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.05482v1"
    },
    {
        "title": "Evaluating Load Balancing Performance in Distributed Storage with\n  Redundancy",
        "authors": [
            "Mehmet Fatih Aktas",
            "Amir Behrouzi-Far",
            "Emina Soljanin",
            "Philip Whiting"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  To facilitate load balancing, distributed systems store data redundantly. We\nevaluate the load balancing performance of storage schemes in which each object\nis stored at $d$ different nodes, and each node stores the same number of\nobjects. In our model, the load offered for the objects is sampled uniformly at\nrandom from all the load vectors with a fixed cumulative value. We find that\nthe load balance in a system of $n$ nodes improves multiplicatively with $d$ as\nlong as $d = o\\left(\\log(n)\\right)$, and improves exponentially once $d =\n\\Theta\\left(\\log(n)\\right)$. We show that the load balance improves in the same\nway with $d$ when the service choices are created with XOR's of $r$ objects\nrather than object replicas. In such redundancy schemes, storage overhead is\nreduced multiplicatively by $r$. However, recovery of an object requires\ndownloading content from $r$ nodes. At the same time, the load balance\nincreases additively by $r$. We express the system's load balance in terms of\nthe maximal spacing or maximum of $d$ consecutive spacings between the ordered\nstatistics of uniform random variables. Using this connection and the limit\nresults on the maximal $d$-spacings, we derive our main results.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.05791v3"
    },
    {
        "title": "Using HEP experiment workflows for the benchmarking and accounting of\n  WLCG computing resources",
        "authors": [
            "Andrea Valassi",
            "Manfred Alef",
            "Jean-Michel Barbet",
            "Olga Datskova",
            "Riccardo De Maria",
            "Miguel Fontes Medeiros",
            "Domenico Giordano",
            "Costin Grigoras",
            "Christopher Hollowell",
            "Martina Javurkova",
            "Viktor Khristenko",
            "David Lange",
            "Michele Michelotto",
            "Lorenzo Rinaldi",
            "Andrea Sciabà",
            "Cas Van Der Laan"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Benchmarking of CPU resources in WLCG has been based on the HEP-SPEC06 (HS06)\nsuite for over a decade. It has recently become clear that HS06, which is based\non real applications from non-HEP domains, no longer describes typical HEP\nworkloads. The aim of the HEP-Benchmarks project is to develop a new benchmark\nsuite for WLCG compute resources, based on real applications from the LHC\nexperiments. By construction, these new benchmarks are thus guaranteed to have\na score highly correlated to the throughputs of HEP applications, and a CPU\nusage pattern similar to theirs. Linux containers and the CernVM-FS filesystem\nare the two main technologies enabling this approach, which had been considered\nimpossible in the past. In this paper, we review the motivation, implementation\nand outlook of the new benchmark suite.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.01609v2"
    },
    {
        "title": "Flattening the Curve: Insights From Queueing Theory",
        "authors": [
            "Sergio Palomo",
            "Jamol Pender",
            "William Massey",
            "Robert C. Hampshire"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The worldwide outbreak of the coronavirus was first identified in 2019 in\nWuhan, China. Since then, the disease has spread worldwide. As it currently\nspreading in the United States, policy makers, public health officials and\ncitizens are racing to understand the impact of this virus on the United States\nhealthcare system. They fear that the rapid influx of patients will overwhelm\nthe healthcare system leading to unnecessary fatalities. Most countries and\nstates in America have introduced mitigation strategies, such as social\ndistancing, to decrease the rate of newly infected people, i.e. flattening the\ncurve.In this paper, we analyze the time evolution of the number of people\nhospitalized due to the coronavirus using the methods of queueing theory. Given\nthat the rate of new infections varies over time as the pandemic evolves, we\nmodel the number of coronavirus patients as a dynamical system based on the\ntheory of infinite server queues with non-stationary Poisson arrival rates.\nWith this model we are able to quantify how flattening the curve affects the\npeak demand for hospital resources. This allows us to characterize how\naggressively society must flatten the curve in order to avoid overwhelming the\ncapacity of healthcare system. We also demonstrate how flattening the curve\nimpacts the elapsed time between the peak rate of hospitalizations and the time\nof the peak demand for the hospital resources. Finally, we present empirical\nevidence from China, South Korea, Italy and the United States that supports the\ninsights from the model.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.09645v1"
    },
    {
        "title": "A Lower Bound on the stability region of Redundancy-d with FIFO service\n  discipline",
        "authors": [
            "Gal Mendelson"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Redundancy-d (R(d)) is a load balancing method used to route incoming jobs to\nK servers, each with its own queue. Every arriving job is replicated into\n2<=d<=K tasks, which are then routed to d servers chosen uniformly at random.\nWhen the first task finishes service, the remaining d-1 tasks are cancelled and\nthe job departs the system.\n  Despite the fact that R(d) is known, under certain conditions, to\nsubstantially improve job completion times compared to not using redundancy at\nall, little is known on a more fundamental performance criterion: what is the\nset of arrival rates under which the R(d) queueing system with FIFO service\ndiscipline is stable? In this context, due to the complex dynamics of systems\nwith redundancy and cancellations, existing results are scarce and are limited\nto very special cases with respect to the joint service time distribution of\ntasks.\n  In this paper we provide a non-trivial, closed form lower bound on the\nstability region of R(d) for a general joint service time distribution of tasks\nwith finite first and second moments. We consider a discrete time system with\nBernoulli arrivals and assume that jobs are processed by their order of\narrival. We use the workload processes and a quadratic Lyapunov function to\ncharacterize the set of arrival rates for which the system is stable. While\nsimulation results indicate our bound is not tight, it provides an\neasy-to-check performance guarantee.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.14793v2"
    },
    {
        "title": "Flexibility in an asymmetric system with prolonged service time at\n  non-dedicated servers",
        "authors": [
            "Yanting Chen",
            "Jingui Xie",
            "Taozeng Zhu"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  The prolonged service time at non-dedicated servers has been observed in [1].\nMotivated by such real problems, we propose a stylized model which\ncharacterizes the feature of the prolonged service time at non-dedicated\nservers in an asymmetric system. We study the independent system, the full\nflexibility system and the partial flexibility system when the occupation rate\nof the system, the degree of the prolonged service time and the degree of the\nasymmetry are allowed to change. We show that under certain circumstances, the\npartial flexibility scheme outperforms the full flexibility system and the\nindependent system in such a model. Our results also provide instructions on\nhow to introduce flexibility when the service time at non-dedicated servers is\nprolonged in an asymmetric system.\n",
        "pdf_link": "http://arxiv.org/pdf/2007.01463v2"
    },
    {
        "title": "LETI: Latency Estimation Tool and Investigation of Neural Networks\n  inference on Mobile GPU",
        "authors": [
            "Evgeny Ponomarev",
            "Sergey Matveev",
            "Ivan Oseledets"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  A lot of deep learning applications are desired to be run on mobile devices.\nBoth accuracy and inference time are meaningful for a lot of them. While the\nnumber of FLOPs is usually used as a proxy for neural network latency, it may\nbe not the best choice. In order to obtain a better approximation of latency,\nresearch community uses look-up tables of all possible layers for latency\ncalculation for the final prediction of the inference on mobile CPU. It\nrequires only a small number of experiments. Unfortunately, on mobile GPU this\nmethod is not applicable in a straight-forward way and shows low precision. In\nthis work, we consider latency approximation on mobile GPU as a data and\nhardware-specific problem. Our main goal is to construct a convenient latency\nestimation tool for investigation(LETI) of neural network inference and\nbuilding robust and accurate latency prediction models for each specific task.\nTo achieve this goal, we build open-source tools which provide a convenient way\nto conduct massive experiments on different target devices focusing on mobile\nGPU. After evaluation of the dataset, we learn the regression model on\nexperimental data and use it for future latency prediction and analysis. We\nexperimentally demonstrate the applicability of such an approach on a subset of\npopular NAS-Benchmark 101 dataset and also evaluate the most popular neural\nnetwork architectures for two mobile GPUs. As a result, we construct latency\nprediction model with good precision on the target evaluation subset. We\nconsider LETI as a useful tool for neural architecture search or massive\nlatency evaluation. The project is available at https://github.com/leti-ai\n",
        "pdf_link": "http://arxiv.org/pdf/2010.02871v2"
    },
    {
        "title": "Autotuning PolyBench Benchmarks with LLVM Clang/Polly Loop Optimization\n  Pragmas Using Bayesian Optimization",
        "authors": [
            "Xingfu Wu",
            "Michael Kruse",
            "Prasanna Balaprakash",
            "Hal Finkel",
            "Paul Hovland",
            "Valerie Taylor",
            "Mary Hall"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  An autotuning is an approach that explores a search space of possible\nimplementations/configurations of a kernel or an application by selecting and\nevaluating a subset of implementations/configurations on a target platform\nand/or use models to identify a high performance implementation/configuration.\nIn this paper, we develop an autotuning framework that leverages Bayesian\noptimization to explore the parameter space search. We select six of the most\ncomplex benchmarks from the application domains of the PolyBench benchmarks\n(syr2k, 3mm, heat-3d, lu, covariance, and Floyd-Warshall) and apply the newly\ndeveloped LLVM Clang/Polly loop optimization pragmas to the benchmarks to\noptimize them. We then use the autotuning framework to optimize the pragma\nparameters to improve their performance. The experimental results show that our\nautotuning approach outperforms the other compiling methods to provide the\nsmallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and\ncovariance with two large datasets in 200 code evaluations for effectively\nsearching the parameter spaces with up to 170,368 different configurations. We\ncompare four different supervised learning methods within Bayesian optimization\nand evaluate their effectiveness. We find that the Floyd-Warshall benchmark did\nnot benefit from autotuning because Polly uses heuristics to optimize the\nbenchmark to make it run much slower. To cope with this issue, we provide some\ncompiler option solutions to improve the performance.\n",
        "pdf_link": "http://arxiv.org/pdf/2010.08040v1"
    },
    {
        "title": "A Bounded Multi-Vacation Queue Model for Multi-stage Sleep Control 5G\n  Base station",
        "authors": [
            "Jie Chen"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  Modelling and control of energy consumption is an important problem in\ntelecommunication systems.To model such systems, this paper publishes a bounded\nmulti-vacation queue model. The energy consumption predicted by the model shows\nan average error rate of 0.0177 and the delay predicted by the model shows an\naverage error rate of 0.0655 over 99 test instances.Subsequently, an\noptimization algorithm is proposed to minimize the energy consumption while not\nviolate the delay bound. Furthermore, given current state of art 5G base\nstation system configuration, numerical results shows that with the increase of\ntraffic load, energy saving rate becomes less.\n",
        "pdf_link": "http://arxiv.org/pdf/2011.09951v1"
    },
    {
        "title": "A Preliminary Proposal for an Analytical Model for Evaluating the Impact\n  on Performance of Data Access Patterns in Transaction Execution",
        "authors": [
            "Pierangelo Di Sanzo"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  We present a preliminary proposal for an analytical model for evaluating the\nimpact on performance of data access patterns in concurrent transaction\nexecution. We consider the case of concurrency control protocols that use\nlocking to ensure isolation in the execution of transactions. We analyse\nscenarios where transactions access one or more sets of data items in the same\norder or in different order.\n",
        "pdf_link": "http://arxiv.org/pdf/2104.03187v4"
    },
    {
        "title": "Aggregate Cyber-Risk Management in the IoT Age: Cautionary Statistics\n  for (Re)Insurers and Likes",
        "authors": [
            "Ranjan Pal",
            "Ziyuan Huang",
            "Xinlong Yin",
            "Sergey Lototsky",
            "Swades De",
            "Sasu Tarkoma",
            "Mingyan Liu",
            "Jon Crowcroft",
            "Nishanth Sastry"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  In this paper, we provide (i) a rigorous general theory to elicit conditions\non (tail-dependent) heavy-tailed cyber-risk distributions under which a risk\nmanagement firm might find it (non)sustainable to provide aggregate cyber-risk\ncoverage services for smart societies, and (ii)a real-data driven numerical\nstudy to validate claims made in theory assuming boundedly rational cyber-risk\nmanagers, alongside providing ideas to boost markets that aggregate dependent\ncyber-risks with heavy-tails.To the best of our knowledge, this is the only\ncomplete general theory till date on the feasibility of aggregate cyber-risk\nmanagement.\n",
        "pdf_link": "http://arxiv.org/pdf/2105.01792v1"
    },
    {
        "title": "Scaling Power Management in Cloud Data Centers: A Multi-Level\n  Continuous-Time MDP Approach",
        "authors": [
            "Behzad Chitsaz",
            "Ahmad Khonsari",
            "Masoumeh Moradian",
            "Aresh Dadlani",
            "Mohammad Sadegh Talebi"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  Power management in multi-server data centers~especially at scale is a vital\nissue of increasing importance in cloud computing paradigm. Existing studies\nmostly consider thresholds on the number of idle servers to switch the servers\non or off and suffer from scalability issues. As a natural approach in\nview~of~the Markovian assumption, we present a multi-level continuous-time\nMarkov decision process (CTMDP) model based on state aggregation of\nmulti-server data centers with setup times that interestingly overcomes the\ninherent intractability of traditional MDP approaches due to their colossal\nstate-action space. The beauty of the presented model is that, while it keeps\nloyalty to the Markovian behavior, it approximates the calculation of the\ntransition probabilities in a way that keeps the accuracy of the results at a\ndesirable level. Moreover, near-optimal performance is attained at the expense\nof the increased state-space dimensionality by tuning the number of levels in\nthe multi-level approach. The simulation results were promising and confirm\nthat in many scenarios of interest, the proposed approach attains noticeable\nimprovements, namely a near 50% reduction in the size of CTMDP while yielding\nbetter rewards as compared to existing fixed threshold-based policies and\naggregation methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.01292v2"
    },
    {
        "title": "Labor-right Protecting Dispatch of Meal Delivery Platforms",
        "authors": [
            "Wentao Weng",
            "Yang Yu"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  The boom in the meal delivery industry brings growing concern about the labor\nrights of riders. Current dispatch policies of meal-delivery platforms focus\nmainly on satisfying consumers or minimizing the number of riders for cost\nsavings. There are few discussions on improving the working conditions of\nriders by algorithm design. The lack of concerns on labor rights in mechanism\nand dispatch design has resulted in a very large time waste for riders and\ntheir risky driving. In this research, we propose a queuing-model-based\nframework to discuss optimal dispatch policy with the goal of labor rights\nprotection. We apply our framework to develop an algorithm minimizing the\nwaiting time of food delivery riders with guaranteed user experience. Our\nframework also allows us to manifest the value of restaurants' data about their\noffline-order numbers on improving the benefits of riders.\n",
        "pdf_link": "http://arxiv.org/pdf/2109.14156v1"
    },
    {
        "title": "Dependability Analysis of Data Storage Systems in Presence of Soft\n  Errors",
        "authors": [
            "Mostafa Kishani",
            "Mehdi Tahoori",
            "Hossein Asadi"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  In recent years, high availability and reliability of Data Storage Systems\n(DSS) have been significantly threatened by soft errors occurring in storage\ncontrollers. Due to their specific functionality and hardware-software stack,\nerror propagation and manifestation in DSS is quite different from\ngeneral-purpose computing architectures. To our knowledge, no previous study\nhas examined the system-level effects of soft errors on the availability and\nreliability of data storage systems. In this paper, we first analyze the\neffects of soft errors occurring in the server processors of storage\ncontrollers on the entire storage system dependability. To this end, we\nimplemented the major functions of a typical data storage system controller,\nrunning on a full stack of storage system operating system, and developed a\nframework to perform fault injection experiments using a full system simulator.\nWe then propose a new metric, Storage System Vulnerability Factor (SSVF), to\naccurately capture the impact of soft errors in storage systems. By conducting\nextensive experiments, it is revealed that depending on the controller\nconfiguration, up to 40% of cache memory contains end-user data where any\nunrecoverable soft errors in this part will result in Data Loss (DL) in an\nirreversible manner. However, soft errors in the rest of cache memory filled by\nOperating System (OS) and storage applications will result in Data\nUnavailability (DU) at the storage system level. Our analysis also shows that\nDetectable Unrecoverable Errors (DUEs) on the cache data field are the major\ncause of DU in storage systems, while Silent Data Corruptions (SDCs) in the\ncache tag and data field are mainly the cause of DL in storage systems.\n",
        "pdf_link": "http://arxiv.org/pdf/2112.12520v1"
    },
    {
        "title": "Tackling Heterogeneous Traffic in Multi-access Systems via Erasure Coded\n  Servers",
        "authors": [
            "Tuhinangshu Choudhury",
            "Weina Wang",
            "Gauri Joshi"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Most data generated by modern applications is stored in the cloud, and there\nis an exponential growth in the volume of jobs to access these data and perform\ncomputations using them. The volume of data access or computing jobs can be\nheterogeneous across different job types and can unpredictably change over\ntime. Cloud service providers cope with this demand heterogeneity and\nunpredictability by over-provisioning the number of servers hosting each job\ntype. In this paper, we propose the addition of erasure-coded servers that can\nflexibly serve multiple job types without additional storage cost. We analyze\nthe service capacity region and the response time of such erasure-coded systems\nand compare them with standard uncoded replication-based systems currently used\nin the cloud. We show that coding expands the service capacity region, thus\nenabling the system to handle variability in demand for different data types.\nMoreover, we characterize the response time of the coded system in various\narrival rate regimes. This analysis reveals that adding even a small number of\ncoded servers can significantly reduce the mean response time, with a drastic\nreduction in regimes where the demand is skewed across different job types.\n",
        "pdf_link": "http://arxiv.org/pdf/2207.03983v2"
    },
    {
        "title": "A Markov Process Theory for Network Growth Processes of DAG-based\n  Blockchain Systems",
        "authors": [
            "Xing-Shuo Song",
            "Quan-Lin Li",
            "Yan-Xia Chang",
            "Chi Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Note that the serial structure of blockchain has many essential pitfalls,\nthus a data network structure and its DAG-based blockchain are introduced to\nresolve the blockchain pitfalls. From such a network perspective, analysis of\nthe DAG-based blockchain systems becomes interesting and challenging. So, the\nsimulation models are adopted widely. In this paper, we first describe a simple\nMarkov model for the DAG-based blockchain with IOTA Tangle by means of two\nlayers of tips and internal tips' impatient connection behavior. Then we set up\na continuous-time Markov process to analyze the DAG-based blockchain system and\nshow that this Markov process is a level-dependent quasi-birth-and-death (QBD)\nprocess. Based on this, we prove that the QBD process must be irreducible and\npositive recurrent. Furthermore, once the stationary probability vector of the\nQBD process is given, we provide performance analysis of the DAG-based\nblockchain system. Next, we propose a new effective method for computing the\naverage confirmation time of any arriving internal tip at this system by means\nof the first passage times and the PH distributions. Finally, we use numerical\nexamples to check the validity of our theoretical results and indicate how some\nkey system parameters influence the performance measures of this system.\nTherefore, we hope that the methodology and results developed in this paper can\nbe applicable to deal with more general DAG-based blockchain systems such that\na series of promising research can be developed potentially.\n",
        "pdf_link": "http://arxiv.org/pdf/2209.01458v2"
    },
    {
        "title": "Application Performance Modeling via Tensor Completion",
        "authors": [
            "Edward Hutter",
            "Edgar Solomonik"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Performance tuning, software/hardware co-design, and job scheduling are among\nthe many tasks that rely on models to predict application performance. We\npropose and evaluate low-rank tensor decomposition for modeling application\nperformance. We discretize the input and configuration domains of an\napplication using regular grids. Application execution times mapped within\ngrid-cells are averaged and represented by tensor elements. We show that\nlow-rank canonical-polyadic (CP) tensor decomposition is effective in\napproximating these tensors. We further show that this decomposition enables\naccurate extrapolation of unobserved regions of an application's parameter\nspace. We then employ tensor completion to optimize a CP decomposition given a\nsparse set of observed execution times. We consider alternative\npiecewise/grid-based models and supervised learning models for six applications\nand demonstrate that CP decomposition optimized using tensor completion offers\nhigher prediction accuracy and memory-efficiency for high-dimensional\nperformance modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.10184v3"
    },
    {
        "title": "FullPack: Full Vector Utilization for Sub-Byte Quantized Inference on\n  General Purpose CPUs",
        "authors": [
            "Hossein Katebi",
            "Navidreza Asadi",
            "Maziar Goudarzi"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  Although prior art has demonstrated negligible accuracy drop in sub-byte\nquantization -- where weights and/or activations are represented by less than 8\nbits -- popular SIMD instructions of CPUs do not natively support these\ndatatypes. While recent methods, such as ULPPACK, are already using sub-byte\nquantization on general-purpose CPUs with vector units, they leave out several\nempty bits between the sub-byte values in memory and in vector registers to\navoid overflow to the neighbours during the operations. This results in memory\nfootprint and bandwidth-usage inefficiencies and suboptimal performance. In\nthis paper, we present memory layouts for storing, and mechanisms for\nprocessing sub-byte (4-, 2-, or 1-bit) models that utilize all the bits in the\nmemory as well as in the vector registers for the actual data. We provide\ncompute kernels for the proposed layout for the GEMV (GEneral Matrix-Vector\nmultiplication) operations between weights and activations of different\ndatatypes (e.g., 8-bit activations and 4-bit weights). For evaluation, we\nextended the TFLite package and added our methods to it, then ran the models on\nthe cycle-accurate gem5 simulator to compare detailed memory and CPU cycles of\neach method. We compare against nine other methods that are actively used in\nproduction including GEMLOWP, Ruy, XNNPack, and ULPPACK. Furthermore, we\nexplore the effect of different input and output sizes of deep learning layers\non the performance of our proposed method. Experimental results show 0.96-2.1x\nspeedup for small sizes and 1.2-6.7x speedup for mid to large sizes. Applying\nour proposal to a real-world speech recognition model, Mozilla DeepSpeech, we\nproved that our method achieves 1.56-2.11x end-to-end speedup compared to the\nstate-of-the-art, depending on the bit-width employed.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.06982v2"
    },
    {
        "title": "Asymptotic behaviour of a conservative reaction-diffusion system\n  associated with a Markov process algebra model",
        "authors": [
            "Jie Ding",
            "Ruiming Ma",
            "Zhigui Lin",
            "Zhi Ling"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  This paper demonstrates a lower and upper solution method to investigate the\nasymptotic behaviour of the conservative reaction-diffusion systems associated\nwith Markovian process algebra models. In particular, we have proved the\nuniform convergence of the solution to its constant equilibrium for a case\nstudy as time tends to infinity, together with experimental results\nillustrations.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.09298v1"
    },
    {
        "title": "COMET: Neural Cost Model Explanation Framework",
        "authors": [
            "Isha Chaudhary",
            "Alex Renda",
            "Charith Mendis",
            "Gagandeep Singh"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Cost models predict the cost of executing given assembly code basic blocks on\na specific microarchitecture. Recently, neural cost models have been shown to\nbe fairly accurate and easy to construct. They can replace heavily engineered\nanalytical cost models used in mainstream compiler workflows. However, their\nblack-box nature discourages their adoption. In this work, we develop the first\nframework, COMET, for generating faithful, generalizable, and intuitive\nexplanations for neural cost models. We generate and compare COMET's\nexplanations for the popular neural cost model, Ithemal against those for an\naccurate CPU simulation-based cost model, uiCA. Our empirical findings show an\ninverse correlation between the prediction errors of Ithemal and uiCA and the\ngranularity of basic block features in COMET's explanations for them, thus\nindicating potential reasons for the higher error of Ithemal with respect to\nuiCA.\n",
        "pdf_link": "http://arxiv.org/pdf/2302.06836v3"
    },
    {
        "title": "eBPF-based Working Set Size Estimation in Memory Management",
        "authors": [
            "Zhilu Lian",
            "Yangzi Li",
            "Zhixiang Chen",
            "Shiwen Shan",
            "Baoxin Han",
            "Yuxin Su"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Working set size estimation (WSS) is of great significance to improve the\nefficiency of program executing and memory arrangement in modern operating\nsystems. Previous work proposed several methods to estimate WSS, including\nself-balloning, Zballoning and so on. However, these methods which are based on\nvirtual machine usually cause a large overhead. Thus, using those methods to\nestimate WSS is impractical. In this paper, we propose a novel framework to\nefficiently estimate WSS with eBPF (extended Berkeley Packet Filter), a\ncutting-edge technology which monitors and filters data by being attached to\nthe kernel. With an eBPF program pinned into the kernel, we get the times of\npage fault and other information of memory allocation. Moreover, we collect WSS\nvia vanilla tool to train a predictive model to complete estimation work with\nLightGBM, a useful tool which performs well on generating decision trees over\ncontinuous value. The experimental results illustrate that our framework can\nestimate WSS precisely with 98.5\\% reduction in overhead compared to\ntraditional methods.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.05919v1"
    },
    {
        "title": "How does SSD Cluster Perform for Distributed File Systems: An Empirical\n  Study",
        "authors": [
            "Jiashu Wu",
            "Yang Wang",
            "Jinpeng Wang",
            "Hekang Wang",
            "Taorui Lin"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  As the capacity of Solid-State Drives (SSDs) is constantly being optimised\nand boosted with gradually reduced cost, the SSD cluster is now widely deployed\nas part of the hybrid storage system in various scenarios such as cloud\ncomputing and big data processing. However, despite its rapid developments, the\nperformance of the SSD cluster remains largely under-investigated, leaving its\nsub-optimal applications in reality. To address this issue, in this paper we\nconduct extensive empirical studies for a comprehensive understanding of the\nSSD cluster in diverse settings. To this end, we configure a real SSD cluster\nand gather the generated trace data based on some often-used benchmarks, then\nadopt analytical methods to analyse the performance of the SSD cluster with\ndifferent configurations. In particular, regression models are built to provide\nbetter performance predictability under broader configurations, and the\ncorrelations between influential factors and performance metrics with respect\nto different numbers of nodes are investigated, which reveal the high\nscalability of the SSD cluster. Additionally, the cluster's network bandwidth\nis inspected to explain the performance bottleneck. Finally, the knowledge\ngained is summarised to benefit the SSD cluster deployment in practice.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.12258v2"
    },
    {
        "title": "Tensor Slicing and Optimization for Multicore NPUs",
        "authors": [
            "Rafael Sousa",
            "Marcio Pereira",
            "Yongin Kwon",
            "Taeho Kim",
            "Namsoon Jung",
            "Chang Soo Kim",
            "Michael Frank",
            "Guido Araujo"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Although code generation for Convolution Neural Network (CNN) models has been\nextensively studied, performing efficient data slicing and parallelization for\nhighly-constrai\\-ned Multicore Neural Processor Units (NPUs) is still a\nchallenging problem. Given the size of convolutions' input/output tensors and\nthe small footprint of NPU on-chip memories, minimizing memory transactions\nwhile maximizing parallelism and MAC utilization are central to any effective\nsolution. This paper proposes a TensorFlow XLA/LLVM compiler optimization pass\nfor Multicore NPUs, called Tensor Slicing Optimization (TSO), which: (a)\nmaximizes convolution parallelism and memory usage across NPU cores; and (b)\nreduces data transfers between host and NPU on-chip memories by using DRAM\nmemory burst time estimates to guide tensor slicing. To evaluate the proposed\napproach, a set of experiments was performed using the NeuroMorphic Processor\n(NMP), a multicore NPU containing 32 RISC-V cores extended with novel CNN\ninstructions. Experimental results show that TSO is capable of identifying the\nbest tensor slicing that minimizes execution time for a set of CNN models.\nSpeed-ups of up to 21.7\\% result when comparing the TSO burst-based technique\nto a no-burst data slicing approach. To validate the generality of the TSO\napproach, the algorithm was also ported to the Glow Machine Learning framework.\nThe performance of the models were measured on both Glow and TensorFlow\nXLA/LLVM compilers, revealing similar results.\n",
        "pdf_link": "http://arxiv.org/pdf/2304.03013v1"
    },
    {
        "title": "On the Fair Comparison of Optimization Algorithms in Different Machines",
        "authors": [
            "Etor Arza",
            "Josu Ceberio",
            "Ekhiñe Irurozki",
            "Aritz Pérez"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  An experimental comparison of two or more optimization algorithms requires\nthe same computational resources to be assigned to each algorithm. When a\nmaximum runtime is set as the stopping criterion, all algorithms need to be\nexecuted in the same machine if they are to use the same resources.\nUnfortunately, the implementation code of the algorithms is not always\navailable, which means that running the algorithms to be compared in the same\nmachine is not always possible. And even if they are available, some\noptimization algorithms might be costly to run, such as training large\nneural-networks in the cloud.\n  In this paper, we consider the following problem: how do we compare the\nperformance of a new optimization algorithm B with a known algorithm A in the\nliterature if we only have the results (the objective values) and the runtime\nin each instance of algorithm A? Particularly, we present a methodology that\nenables a statistical analysis of the performance of algorithms executed in\ndifferent machines. The proposed methodology has two parts. First, we propose a\nmodel that, given the runtime of an algorithm in a machine, estimates the\nruntime of the same algorithm in another machine. This model can be adjusted so\nthat the probability of estimating a runtime longer than what it should be is\narbitrarily low. Second, we introduce an adaptation of the one-sided sign test\nthat uses a modified p-value and takes into account that probability. Such\nadaptation avoids increasing the probability of type I error associated with\nexecuting algorithms A and B in different machines.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.07345v2"
    },
    {
        "title": "Optimizing Memory Mapping Using Deep Reinforcement Learning",
        "authors": [
            "Pengming Wang",
            "Mikita Sazanovich",
            "Berkin Ilbeyi",
            "Phitchaya Mangpo Phothilimthana",
            "Manish Purohit",
            "Han Yang Tay",
            "Ngân Vũ",
            "Miaosen Wang",
            "Cosmin Paduraru",
            "Edouard Leurent",
            "Anton Zhernov",
            "Po-Sen Huang",
            "Julian Schrittwieser",
            "Thomas Hubert",
            "Robert Tung",
            "Paula Kurylowicz",
            "Kieran Milan",
            "Oriol Vinyals",
            "Daniel J. Mankowitz"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Resource scheduling and allocation is a critical component of many high\nimpact systems ranging from congestion control to cloud computing. Finding more\noptimal solutions to these problems often has significant impact on resource\nand time savings, reducing device wear-and-tear, and even potentially improving\ncarbon emissions. In this paper, we focus on a specific instance of a\nscheduling problem, namely the memory mapping problem that occurs during\ncompilation of machine learning programs: That is, mapping tensors to different\nmemory layers to optimize execution time.\n  We introduce an approach for solving the memory mapping problem using\nReinforcement Learning. RL is a solution paradigm well-suited for sequential\ndecision making problems that are amenable to planning, and combinatorial\nsearch spaces with high-dimensional data inputs. We formulate the problem as a\nsingle-player game, which we call the mallocGame, such that high-reward\ntrajectories of the game correspond to efficient memory mappings on the target\nhardware. We also introduce a Reinforcement Learning agent, mallocMuZero, and\nshow that it is capable of playing this game to discover new and improved\nmemory mapping solutions that lead to faster execution times on real ML\nworkloads on ML accelerators. We compare the performance of mallocMuZero to the\ndefault solver used by the Accelerated Linear Algebra (XLA) compiler on a\nbenchmark of realistic ML workloads. In addition, we show that mallocMuZero is\ncapable of improving the execution time of the recently published AlphaTensor\nmatrix multiplication model.\n",
        "pdf_link": "http://arxiv.org/pdf/2305.07440v2"
    },
    {
        "title": "CAMEO: A Causal Transfer Learning Approach for Performance Optimization\n  of Configurable Computer Systems",
        "authors": [
            "Md Shahriar Iqbal",
            "Ziyuan Zhong",
            "Iftakhar Ahmad",
            "Baishakhi Ray",
            "Pooyan Jamshidi"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Modern computer systems are highly configurable, with hundreds of\nconfiguration options that interact, resulting in an enormous configuration\nspace. As a result, optimizing performance goals (e.g., latency) in such\nsystems is challenging due to frequent uncertainties in their environments\n(e.g., workload fluctuations). Recently, transfer learning has been applied to\naddress this problem by reusing knowledge from configuration measurements from\nthe source environments, where it is cheaper to intervene than the target\nenvironment, where any intervention is costly or impossible. Recent empirical\nresearch showed that statistical models can perform poorly when the deployment\nenvironment changes because the behavior of certain variables in the models can\nchange dramatically from source to target. To address this issue, we propose\nCAMEO, a method that identifies invariant causal predictors under environmental\nchanges, allowing the optimization process to operate in a reduced search\nspace, leading to faster optimization of system performance. We demonstrate\nsignificant performance improvements over state-of-the-art optimization methods\nin MLperf deep learning systems, a video analytics pipeline, and a database\nsystem.\n",
        "pdf_link": "http://arxiv.org/pdf/2306.07888v2"
    },
    {
        "title": "Energy Efficiency of Quantum Statevector Simulation at Scale",
        "authors": [
            "Jakub Adamski",
            "James Peter Richings",
            "Oliver Thomson Brown"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Classical simulations are essential for the development of quantum computing,\nand their exponential scaling can easily fill any modern supercomputer. In this\npaper we consider the performance and energy consumption of large Quantum\nFourier Transform (QFT) simulations run on ARCHER2, the UK's National\nSupercomputing Service, with QuEST toolkit. We take into account CPU clock\nfrequency and node memory size, and use cache-blocking to rearrange the\ncircuit, which minimises communications. We find that using 2.00GHz instead of\n2.25GHz can save as much as 25% of energy at 5% increase in runtime. Higher\nnode memory also has the potential to be more efficient, and cost the user\nfewer CUs, but at higher runtime penalty. Finally, we present a cache-blocking\nQFT circuit, which halves the required communication. All our optimisations\ncombined result in 40% faster simulations and 35% energy savings in 44 qubit\nsimulations on 4,096 ARCHER2 nodes.\n",
        "pdf_link": "http://arxiv.org/pdf/2308.07402v2"
    },
    {
        "title": "A Comprehensive Performance Study of Large Language Models on Novel AI\n  Accelerators",
        "authors": [
            "Murali Emani",
            "Sam Foreman",
            "Varuni Sastry",
            "Zhen Xie",
            "Siddhisanket Raskar",
            "William Arnold",
            "Rajeev Thakur",
            "Venkatram Vishwanath",
            "Michael E. Papka"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Artificial intelligence (AI) methods have become critical in scientific\napplications to help accelerate scientific discovery. Large language models\n(LLMs) are being considered as a promising approach to address some of the\nchallenging problems because of their superior generalization capabilities\nacross domains. The effectiveness of the models and the accuracy of the\napplications is contingent upon their efficient execution on the underlying\nhardware infrastructure. Specialized AI accelerator hardware systems have\nrecently become available for accelerating AI applications. However, the\ncomparative performance of these AI accelerators on large language models has\nnot been previously studied. In this paper, we systematically study LLMs on\nmultiple AI accelerators and GPUs and evaluate their performance\ncharacteristics for these models. We evaluate these systems with (i) a\nmicro-benchmark using a core transformer block, (ii) a GPT- 2 model, and (iii)\nan LLM-driven science use case, GenSLM. We present our findings and analyses of\nthe models' performance to better understand the intrinsic capabilities of AI\naccelerators. Furthermore, our analysis takes into account key factors such as\nsequence lengths, scaling behavior, sparsity, and sensitivity to gradient\naccumulation steps.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.04607v1"
    },
    {
        "title": "Look-Up mAI GeMM: Increasing AI GeMMs Performance by Nearly 2.5x via\n  msGeMM",
        "authors": [
            "Saeed Maleki"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  AI models are increasing in size and recent advancement in the community has\nshown that unlike HPC applications where double precision datatype are\nrequired, lower-precision datatypes such as fp8 or int4 are sufficient to bring\nthe same model quality both for training and inference. Following these trends,\nGPU vendors such as NVIDIA and AMD have added hardware support for fp16, fp8\nand int8 GeMM operations with an exceptional performance via Tensor Cores.\nHowever, this paper proposes a new algorithm called msGeMM which shows that AI\nmodels with low-precision datatypes can run with ~2.5x fewer multiplication and\nadd instructions. Efficient implementation of this algorithm requires special\nCUDA cores with the ability to add elements from a small look-up table at the\nrate of Tensor Cores.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.06178v1"
    },
    {
        "title": "A Performance-Portable SYCL Implementation of CRK-HACC for Exascale",
        "authors": [
            "Esteban M. Rangel",
            "S. John Pennycook",
            "Adrian Pope",
            "Nicholas Frontiere",
            "Zhiqiang Ma",
            "Varsha Madananth"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  The first generation of exascale systems will include a variety of machine\narchitectures, featuring GPUs from multiple vendors. As a result, many\ndevelopers are interested in adopting portable programming models to avoid\nmaintaining multiple versions of their code. It is necessary to document\nexperiences with such programming models to assist developers in understanding\nthe advantages and disadvantages of different approaches.\n  To this end, this paper evaluates the performance portability of a SYCL\nimplementation of a large-scale cosmology application (CRK-HACC) running on\nGPUs from three different vendors: AMD, Intel, and NVIDIA. We detail the\nprocess of migrating the original code from CUDA to SYCL and show that\nspecializing kernels for specific targets can greatly improve performance\nportability without significantly impacting programmer productivity. The SYCL\nversion of CRK-HACC achieves a performance portability of 0.96 with a code\ndivergence of almost 0, demonstrating that SYCL is a viable programming model\nfor performance-portable applications.\n",
        "pdf_link": "http://arxiv.org/pdf/2310.16122v1"
    },
    {
        "title": "Dissecting the Runtime Performance of the Training, Fine-tuning, and\n  Inference of Large Language Models",
        "authors": [
            "Longteng Zhang",
            "Xiang Liu",
            "Zeyu Li",
            "Xinglin Pan",
            "Peijie Dong",
            "Ruibo Fan",
            "Rui Guo",
            "Xin Wang",
            "Qiong Luo",
            "Shaohuai Shi",
            "Xiaowen Chu"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Large Language Models (LLMs) have seen great advance in both academia and\nindustry, and their popularity results in numerous open-source frameworks and\ntechniques in accelerating LLM pre-training, fine-tuning, and inference.\nTraining and deploying LLMs are expensive as it requires considerable computing\nresources and memory, hence many efficient approaches have been developed for\nimproving system pipelines as well as operators. However, the runtime\nperformance can vary significantly across hardware and software stacks, which\nmakes it difficult to choose the best configuration. In this work, we aim to\nbenchmark the performance from both macro and micro perspectives. First, we\nbenchmark the end-to-end performance of pre-training, fine-tuning, and serving\nLLMs in different sizes , i.e., 7, 13, and 70 billion parameters (7B, 13B, and\n70B) on three 8-GPU platforms with and without individual optimization\ntechniques, including ZeRO, quantization, recomputation, FlashAttention. Then,\nwe dive deeper to provide a detailed runtime analysis of the sub-modules,\nincluding computing and communication operators in LLMs. For end users, our\nbenchmark and findings help better understand different optimization\ntechniques, training and inference frameworks, together with hardware platforms\nin choosing configurations for deploying LLMs. For researchers, our in-depth\nmodule-wise analyses discover potential opportunities for future work to\nfurther optimize the runtime performance of LLMs.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.03687v2"
    },
    {
        "title": "An Efficient Algorithm for Unbalanced 1D Transportation",
        "authors": [
            "Gabriel Gouvine"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Optimal transport (OT) and unbalanced optimal transport (UOT) are central in\nmany machine learning, statistics and engineering applications. 1D OT is easily\nsolved, with complexity O(n log n), but no efficient algorithm was known for 1D\nUOT. We present a new approach that leverages the successive shortest path\nalgorithm for the corresponding network flow problem. By employing a suitable\nrepresentation, we bundle together multiple steps that do not change the cost\nof the shortest path. We prove that our algorithm solves 1D UOT in O(n log n),\nclosing the gap.\n",
        "pdf_link": "http://arxiv.org/pdf/2311.17704v2"
    },
    {
        "title": "How Does It Function? Characterizing Long-term Trends in Production\n  Serverless Workloads",
        "authors": [
            "Artjom Joosen",
            "Ahmed Hassan",
            "Martin Asenov",
            "Rajkarn Singh",
            "Luke Darlow",
            "Jianfeng Wang",
            "Adam Barker"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  This paper releases and analyzes two new Huawei cloud serverless traces. The\ntraces span a period of over 7 months with over 1.4 trillion function\ninvocations combined. The first trace is derived from Huawei's internal\nworkloads and contains detailed per-second statistics for 200 functions running\nacross multiple Huawei cloud data centers. The second trace is a representative\nworkload from Huawei's public FaaS platform. This trace contains per-minute\narrival rates for over 5000 functions running in a single Huawei data center.\nWe present the internals of a production FaaS platform by characterizing\nresource consumption, cold-start times, programming languages used,\nperiodicity, per-second versus per-minute burstiness, correlations, and\npopularity. Our findings show that there is considerable diversity in how\nserverless functions behave: requests vary by up to 9 orders of magnitude\nacross functions, with some functions executed over 1 billion times per day;\nscheduling time, execution time and cold-start distributions vary across 2 to 4\norders of magnitude and have very long tails; and function invocation counts\ndemonstrate strong periodicity for many individual functions and on an\naggregate level. Our analysis also highlights the need for further research in\nestimating resource reservations and time-series prediction to account for the\nhuge diversity in how serverless functions behave.\n  Datasets and code available at https://github.com/sir-lab/data-release\n",
        "pdf_link": "http://arxiv.org/pdf/2312.10127v1"
    },
    {
        "title": "Execution time budget assignment for mixed criticality systems",
        "authors": [
            "Mohamed Amine Khelassi",
            "Yasmina Abdeddaïm"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  In this paper we propose to quantify execution time variability of programs\nusing statistical dispersion parameters. We show how the execution time\nvariability can be exploited in mixed criticality real-time systems. We propose\na heuristic to compute the execution time budget to be allocated to each low\ncriticality real-time task according to its execution time variability. We show\nusing experiments and simulations that the proposed heuristic reduces the\nprobability of exceeding the allocated budget compared to algorithms which do\nnot take into account the execution time variability parameter.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.02431v2"
    },
    {
        "title": "AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence\n  Inference",
        "authors": [
            "Xuanlei Zhao",
            "Shenggan Cheng",
            "Guangyang Lu",
            "Jiarui Fang",
            "Haotian Zhou",
            "Bin Jia",
            "Ziming Liu",
            "Yang You"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Large deep learning models have achieved impressive performance across a\nrange of applications. However, their large memory requirements, including\nparameter memory and activation memory, have become a significant challenge for\ntheir practical serving. While existing methods mainly address parameter\nmemory, the importance of activation memory has been overlooked. Especially for\nlong input sequences, activation memory is expected to experience a significant\nexponential growth as the length of sequences increases. In this approach, we\npropose AutoChunk, an automatic and adaptive compiler system that efficiently\nreduces activation memory for long sequence inference by chunk strategies. The\nproposed system generates chunk plans by optimizing through multiple stages. In\neach stage, the chunk search pass explores all possible chunk candidates and\nthe chunk selection pass identifies the optimal one. At runtime, AutoChunk\nemploys code generation to automatically apply chunk strategies. The\nexperiments demonstrate that AutoChunk can reduce over 80\\% of activation\nmemory while maintaining speed loss within 10%, extend max sequence length by\n3.2x to 11.7x, and outperform state-of-the-art methods by a large margin.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.10652v3"
    },
    {
        "title": "GPU Cluster Scheduling for Network-Sensitive Deep Learning",
        "authors": [
            "Aakash Sharma",
            "Vivek M. Bhasi",
            "Sonali Singh",
            "George Kesidis",
            "Mahmut T. Kandemir",
            "Chita R. Das"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  We propose a novel GPU-cluster scheduler for distributed DL (DDL) workloads\nthat enables proximity based consolidation of GPU resources based on the DDL\njobs' sensitivities to the anticipated communication-network delays. Our\nscheduler consists of three major components: (i) a classical delay scheduling\nalgorithm to facilitate job placement and consolidation; (ii) a\nnetwork-sensitive job preemption strategy; and (iii) an \"auto-tuner\" mechanism\nto optimize delay timers for effective delay scheduling. Additionally, to\nenable a cost-effective methodology for large-scale experiments, we develop a\ndata-driven DDL cluster simulation platform. Employing the simulation platform\nwe compare against several state-of-the-art alternatives on real-world workload\ntraces to demonstrate the benefits of our design. Our scheduler can provide\nimprovement of up to 69% in end-to-end Makespan for training all jobs compared\nto the prevailing consolidation-based scheduling methods, while reducing the\naverage job completion time by up to 83% and minimizing the communication\noverheads by up to 98% under congested networking conditions.\n",
        "pdf_link": "http://arxiv.org/pdf/2401.16492v1"
    },
    {
        "title": "Fusing Depthwise and Pointwise Convolutions for Efficient Inference on\n  GPUs",
        "authors": [
            "Fareed Qararyah",
            "Muhammad Waqar Azhar",
            "Mohammad Ali Maleki",
            "Pedro Trancoso"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Depthwise and pointwise convolutions have fewer parameters and perform fewer\noperations than standard convolutions. As a result, they have become\nincreasingly used in various compact DNNs, including convolutional neural\nnetworks (CNNs) and vision transformers (ViTs). However, they have a lower\ncompute-to-memory-access ratio than standard convolutions, making their memory\naccesses often the performance bottleneck. This paper explores fusing depthwise\nand pointwise convolutions to overcome the memory access bottleneck. The focus\nis on fusing these operators on GPUs. The prior art on GPU-based fusion suffers\nfrom one or more of the following: (1) fusing either a convolution with an\nelement-wise or multiple non-convolutional operators, (2) not explicitly\noptimizing for memory accesses, (3) not supporting depthwise convolutions. This\npaper proposes Fused Convolutional Modules (FCMs), a set of novel fused\ndepthwise and pointwise GPU kernels. FCMs significantly reduce pointwise and\ndepthwise convolutions memory accesses, improving execution time and energy\nefficiency. To evaluate the trade-offs associated with fusion and determine\nwhich convolutions are beneficial to fuse and the optimal FCM parameters, we\npropose FusePlanner. FusePlanner consists of cost models to estimate the memory\naccesses of depthwise, pointwise, and FCM kernels given GPU characteristics.\nOur experiments on three GPUs using representative CNNs and ViTs demonstrate\nthat FCMs save up to 83\\% of the memory accesses and achieve speedups of up to\n3.7x compared to cuDNN. Complete model implementations of various CNNs using\nour modules outperform TVMs' achieving speedups of up to 1.8x and saving up to\ntwo-thirds of the energy. FCM and FusePlanner implementations are open source:\nhttps://github.com/fqararyah/Fusing_DW_and_PW_on_GPUs.\n",
        "pdf_link": "http://arxiv.org/pdf/2404.19331v2"
    },
    {
        "title": "It's all about PR -- Smart Benchmarking AI Accelerators using\n  Performance Representatives",
        "authors": [
            "Alexander Louis-Ferdinand Jung",
            "Jannik Steinmetz",
            "Jonathan Gietz",
            "Konstantin Lübeck",
            "Oliver Bringmann"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Statistical models are widely used to estimate the performance of commercial\noff-the-shelf (COTS) AI hardware accelerators. However, training of statistical\nperformance models often requires vast amounts of data, leading to a\nsignificant time investment and can be difficult in case of limited hardware\navailability. To alleviate this problem, we propose a novel performance\nmodeling methodology that significantly reduces the number of training samples\nwhile maintaining good accuracy. Our approach leverages knowledge of the target\nhardware architecture and initial parameter sweeps to identify a set of\nPerformance Representatives (PR) for deep neural network (DNN) layers. These\nPRs are then used for benchmarking, building a statistical performance model,\nand making estimations. This targeted approach drastically reduces the number\nof training samples needed, opposed to random sampling, to achieve a better\nestimation accuracy. We achieve a Mean Absolute Percentage Error (MAPE) of as\nlow as 0.02% for single-layer estimations and 0.68% for whole DNN estimations\nwith less than 10000 training samples. The results demonstrate the superiority\nof our method for single-layer estimations compared to models trained with\nrandomly sampled datasets of the same size.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.08330v1"
    },
    {
        "title": "Comment on paper: Position: Rethinking Post-Hoc Search-Based Neural\n  Approaches for Solving Large-Scale Traveling Salesman Problems",
        "authors": [
            "Yimeng Min"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  We identify two major issues in the SoftDist paper (Xia et al.): (1) the\nfailure to run all steps of different baselines on the same hardware\nenvironment, and (2) the use of inconsistent time measurements when comparing\nto other baselines. These issues lead to flawed conclusions. When all steps are\nexecuted in the same hardware environment, the primary claim made in SoftDist\nis no longer supported.\n",
        "pdf_link": "http://arxiv.org/pdf/2406.09441v1"
    },
    {
        "title": "Toward Smart Scheduling in Tapis",
        "authors": [
            "Joe Stubbs",
            "Smruti Padhy",
            "Richard Cardone"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The Tapis framework provides APIs for automating job execution on remote\nresources, including HPC clusters and servers running in the cloud. Tapis can\nsimplify the interaction with remote cyberinfrastructure (CI), but the current\nservices require users to specify the exact configuration of a job to run,\nincluding the system, queue, node count, and maximum run time, among other\nattributes. Moreover, the remote resources must be defined and configured in\nTapis before a job can be submitted. In this paper, we present our efforts to\ndevelop an intelligent job scheduling capability in Tapis, where various\nattributes about a job configuration can be automatically determined for the\nuser, and computational resources can be dynamically provisioned by Tapis for\nspecific jobs. We develop an overall architecture for such a feature, which\nsuggests a set of core challenges to be solved. Then, we focus on one such\nspecific challenge: predicting queue times for a job on different HPC systems\nand queues, and we present two sets of results based on machine learning\nmethods. Our first set of results cast the problem as a regression, which can\nbe used to select the best system from a list of existing options. Our second\nset of results frames the problem as a classification, allowing us to compare\nthe use of an existing system with a dynamically provisioned resource.\n",
        "pdf_link": "http://arxiv.org/pdf/2408.03349v1"
    },
    {
        "title": "Automatic Generation of Fast and Accurate Performance Models for Deep\n  Neural Network Accelerators",
        "authors": [
            "Konstantin Lübeck",
            "Alexander Louis-Ferdinand Jung",
            "Felix Wedlich",
            "Mika Markus Müller",
            "Federico Nicolás Peccia",
            "Felix Thömmes",
            "Jannik Steinmetz",
            "Valentin Biermaier",
            "Adrian Frischknecht",
            "Paul Palomero Bernardo",
            "Oliver Bringmann"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Implementing Deep Neural Networks (DNNs) on resource-constrained edge devices\nis a challenging task that requires tailored hardware accelerator architectures\nand a clear understanding of their performance characteristics when executing\nthe intended AI workload. To facilitate this, we present an automated\ngeneration approach for fast performance models to accurately estimate the\nlatency of a DNN mapped onto systematically modeled and concisely described\naccelerator architectures. Using our accelerator architecture description\nmethod, we modeled representative DNN accelerators such as Gemmini, UltraTrail,\nPlasticine-derived, and a parameterizable systolic array. Together with DNN\nmappings for those modeled architectures, we perform a combined DNN/hardware\ndependency graph analysis, which enables us, in the best case, to evaluate only\n154 loop kernel iterations to estimate the performance for 4.19 billion\ninstructions achieving a significant speedup. We outperform regression and\nanalytical models in terms of mean absolute percentage error (MAPE) compared to\nsimulation results, while being several magnitudes faster than an RTL\nsimulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.08595v1"
    },
    {
        "title": "Deploying Open-Source Large Language Models: A performance Analysis",
        "authors": [
            "Yannis Bendi-Ouis",
            "Dan Dutartre",
            "Xavier Hinaut"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Since the release of ChatGPT in November 2022, large language models (LLMs)\nhave seen considerable success, including in the open-source community, with\nmany open-weight models available. However, the requirements to deploy such a\nservice are often unknown and difficult to evaluate in advance. To facilitate\nthis process, we conducted numerous tests at the Centre Inria de l'Universit\\'e\nde Bordeaux. In this article, we propose a comparison of the performance of\nseveral models of different sizes (mainly Mistral and LLaMa) depending on the\navailable GPUs, using vLLM, a Python library designed to optimize the inference\nof these models. Our results provide valuable information for private and\npublic groups wishing to deploy LLMs, allowing them to evaluate the performance\nof different models based on their available hardware. This study thus\ncontributes to facilitating the adoption and use of these large language models\nin various application domains.\n",
        "pdf_link": "http://arxiv.org/pdf/2409.14887v3"
    },
    {
        "title": "Less is More: Optimizing Function Calling for LLM Execution on Edge\n  Devices",
        "authors": [
            "Varatheepan Paramanayakam",
            "Andreas Karatzas",
            "Iraklis Anagnostopoulos",
            "Dimitrios Stamoulis"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  The advanced function-calling capabilities of foundation models open up new\npossibilities for deploying agents to perform complex API tasks. However,\nmanaging large amounts of data and interacting with numerous APIs makes\nfunction calling hardware-intensive and costly, especially on edge devices.\nCurrent Large Language Models (LLMs) struggle with function calling at the edge\nbecause they cannot handle complex inputs or manage multiple tools effectively.\nThis results in low task-completion accuracy, increased delays, and higher\npower consumption. In this work, we introduce Less-is-More, a novel\nfine-tuning-free function-calling scheme for dynamic tool selection. Our\napproach is based on the key insight that selectively reducing the number of\ntools available to LLMs significantly improves their function-calling\nperformance, execution time, and power efficiency on edge devices. Experimental\nresults with state-of-the-art LLMs on edge hardware show agentic success rate\nimprovements, with execution time reduced by up to 70% and power consumption by\nup to 40%.\n",
        "pdf_link": "http://arxiv.org/pdf/2411.15399v1"
    },
    {
        "title": "Rethinking Performance Analysis for Configurable Software Systems: A\n  Case Study from a Fitness Landscape Perspective",
        "authors": [
            "Mingyu Huang",
            "Peili Mao",
            "Ke Li"
        ],
        "category": "cs.PF",
        "published_year": "2024",
        "summary": "  Modern software systems are often highly configurable to tailor varied\nrequirements from diverse stakeholders. Understanding the mapping between\nconfigurations and the desired performance attributes plays a fundamental role\nin advancing the controllability and tuning of the underlying system, yet has\nlong been a dark hole of knowledge due to its black-box nature. While there\nhave been previous efforts in performance analysis for these systems, they\nanalyze the configurations as isolated data points without considering their\ninherent spatial relationships. This renders them incapable of interrogating\nmany important aspects of the configuration space like local optima. In this\nwork, we advocate a novel perspective to rethink performance analysis --\nmodeling the configuration space as a structured ``landscape''. To support this\nproposition, we designed \\our, an open-source, graph data mining empowered\nfitness landscape analysis (FLA) framework. By applying this framework to $86$M\nbenchmarked configurations from $32$ running workloads of $3$ real-world\nsystems, we arrived at $6$ main findings, which together constitute a holistic\npicture of the landscape topography, with thorough discussions about their\nimplications on both configuration tuning and performance modeling.\n",
        "pdf_link": "http://arxiv.org/pdf/2412.16888v2"
    },
    {
        "title": "HPC Application Parameter Autotuning on Edge Devices: A Bandit Learning\n  Approach",
        "authors": [
            "Abrar Hossain",
            "Abdel-Hameed A. Badawy",
            "Mohammad A. Islam",
            "Tapasya Patki",
            "Kishwar Ahmed"
        ],
        "category": "cs.PF",
        "published_year": "2025",
        "summary": "  The growing necessity for enhanced processing capabilities in edge devices\nwith limited resources has led us to develop effective methods for improving\nhigh-performance computing (HPC) applications. In this paper, we introduce LASP\n(Lightweight Autotuning of Scientific Application Parameters), a novel strategy\ndesigned to address the parameter search space challenge in edge devices. Our\nstrategy employs a multi-armed bandit (MAB) technique focused on online\nexploration and exploitation. Notably, LASP takes a dynamic approach, adapting\nseamlessly to changing environments. We tested LASP with four HPC applications:\nLulesh, Kripke, Clomp, and Hypre. Its lightweight nature makes it particularly\nwell-suited for resource-constrained edge devices. By employing the MAB\nframework to efficiently navigate the search space, we achieved significant\nperformance improvements while adhering to the stringent computational limits\nof edge devices. Our experimental results demonstrate the effectiveness of LASP\nin optimizing parameter search on edge devices.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.01057v1"
    },
    {
        "title": "sTiles: An Accelerated Computational Framework for Sparse Factorizations\n  of Structured Matrices",
        "authors": [
            "Esmail Abdul Fattah",
            "Hatem Ltaief",
            "Havard Rue",
            "David Keyes"
        ],
        "category": "cs.PF",
        "published_year": "2025",
        "summary": "  This paper introduces sTiles, a GPU-accelerated framework for factorizing\nsparse structured symmetric matrices. By leveraging tile algorithms for\nfine-grained computations, sTiles uses a structure-aware task execution flow to\nhandle challenging arrowhead sparse matrices with variable bandwidths, common\nin scientific and engineering fields. It minimizes fill-in during Cholesky\nfactorization using permutation techniques and employs a static scheduler to\nmanage tasks on shared-memory systems with GPU accelerators. sTiles balances\ntile size and parallelism, where larger tiles enhance algorithmic intensity but\nincrease floating-point operations and memory usage, while parallelism is\nconstrained by the arrowhead structure. To expose more parallelism, a\nleft-looking Cholesky variant breaks sequential dependencies in trailing\nsubmatrix updates via tree reductions. Evaluations show sTiles achieves\nspeedups of up to 8.41X, 9.34X, 5.07X, and 11.08X compared to CHOLMOD, SymPACK,\nMUMPS, and PARDISO, respectively, and a 5X speedup compared to a 32-core AMD\nEPYC CPU on an NVIDIA A100 GPU. Our generic software framework imports\nwell-established concepts from dense matrix computations but they all require\ncustomizations in their deployments on hybrid architectures to best handle\nfactorizations of sparse matrices with arrowhead structures.\n",
        "pdf_link": "http://arxiv.org/pdf/2501.02483v1"
    },
    {
        "title": "Performance Analysis of Connection Admission Control Scheme in IEEE\n  802.16 OFDMA Networks",
        "authors": [
            "Abdelali El Bouchti",
            "Said El Kafhali",
            "Abdelkrim Haqiq"
        ],
        "category": "cs.PF",
        "published_year": "2013",
        "summary": "  IEEE 802.16 OFDMA (Orthogonal Frequency Division Multiple Access) technology\nhas emerged as a promising technology for broadband access in a Wireless\nMetropolitan Area Network (WMAN) environment. In this paper, we address the\nproblem of queueing theoretic performance modeling and analysis of OFDMA under\nbroad-band wireless networks. We consider a single-cell IEEE 802.16 environment\nin which the base station allocates subchannels to the subscriber stations in\nits coverage area. The subchannels allocated to a subscriber station are shared\nby multiple connections at that subscriber station. To ensure the Quality of\nService (QoS) performances, a Connection Admission Control (CAC) scheme is\nconsidered at a subscriber station. A queueing analytical framework for these\nadmission control schemes is presented considering OFDMA-based transmission at\nthe physical layer. Then, based on the queueing model, both the\nconnection-level and the packet-level performances are studied and compared\nwith their analogues in the case without CAC. The connection arrival is modeled\nby a Poisson process and the packet arrival for a connection by a two-state\nMarkov Modulated Poisson Process (MMPP). We determine analytically and\nnumerically different performance parameters, such as connection blocking\nprobability, average number of ongoing connections, average queue length,\npacket dropping probability, queue throughput and average packet delay.\n",
        "pdf_link": "http://arxiv.org/pdf/1308.3127v1"
    },
    {
        "title": "Competitive Online Optimization under Inventory Constraints",
        "authors": [
            "Qiulin Lin",
            "Hanling Yi",
            "John Pang",
            "Minghua Chen",
            "Adam Wierman",
            "Michael Honig",
            "Yuanzhang Xiao"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This paper studies online optimization under inventory (budget) constraints.\nWhile online optimization is a well-studied topic, versions with inventory\nconstraints have proven difficult. We consider a formulation of\ninventory-constrained optimization that is a generalization of the classic\none-way trading problem and has a wide range of applications. We present a new\nalgorithmic framework, \\textsf{CR-Pursuit}, and prove that it achieves the\nminimal competitive ratio among all deterministic algorithms (up to a\nproblem-dependent constant factor) for inventory-constrained online\noptimization. Our algorithm and its analysis not only simplify and unify the\nstate-of-the-art results for the standard one-way trading problem, but they\nalso establish novel bounds for generalizations including concave revenue\nfunctions. For example, for one-way trading with price elasticity, the\n\\textsf{CR-Pursuit} algorithm achieves a competitive ratio that is within a\nsmall additive constant (i.e., 1/3) to the lower bound of $\\ln \\theta+1$, where\n$\\theta$ is the ratio between the maximum and minimum base prices.\n",
        "pdf_link": "http://arxiv.org/pdf/1901.09161v1"
    },
    {
        "title": "A Closed Queueing Maintenance Network with Two Batch Policies",
        "authors": [
            "Rui-Na Fan",
            "Quan-Lin Li",
            "Xiaole Wu",
            "Zhe George Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2019",
        "summary": "  This paper discusses a maintenance network with failed items that can be\nremoved, repaired, redistributed, and reused under two batch policies: one for\nremoving the failed items from each base to a maintenance shop and the other\nfor redistributing the repaired items from the maintenance shop to bases. This\nmaintenance network can be considered a virtual closed queueing network, and\nthe Markov system of each node is described as an elegant block-structured\nMarkov process whose stationary probabilities can be computed by the\nRG-factorizations. The structure of this maintenance network is novel and\ninteresting. To compute the closed queueing network, we set up a new nonlinear\nmatrix equation to determine the relative arrival rates, in which the\nnonlinearity comes from two different groups of processes: the failure and\nremoval processes and the repair and redistribution processes. This paper also\nextends a simple queueing system of a node to a more general block-structured\nMarkov process which can be computed by the RG-factorizations. Based on this,\nthe paper establishes a more general product-form solution for the closed\nqueueing network and provides performance analysis of the maintenance network.\nOur method will open a new avenue for quantitative evaluation of more general\nmaintenance networks.\n",
        "pdf_link": "http://arxiv.org/pdf/1910.02276v3"
    },
    {
        "title": "Age of Information for Single Buffer Systems with Vacation Server",
        "authors": [
            "Jin Xu",
            "I-Hong Hou",
            "Natarajan Gautam"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  In this research, we study the information freshness in M/G/1 queueing system\nwith a single buffer and the server taking multiple vacations. This system has\nwide applications in communication systems. We aim to evaluate the information\nfreshness in this system with both i.i.d. and non-i.i.d. vacations under three\ndifferent scheduling policies, namely Conventional Buffer System (CBS), Buffer\nRelaxation System (BRS), and Conventional Buffer System with Preemption in\nService (CBS-P). For the systems with i.i.d. vacations, we derive the\nclosed-form expressions of information freshness metrics such as the expected\nAge of Information (AoI), the expected Peak Age of Information (PAoI), and the\nvariance of peak age under each policy. For systems with non-i.i.d. vacations,\nwe use the polling system as an example and provide the closed-form expression\nof its PAoI under each policy. We explore the conditions under which one of\nthese policies has advantages over the others for each information freshness\nmetric. We further perform numerical studies to validate our results and\ndevelop insights.\n",
        "pdf_link": "http://arxiv.org/pdf/2004.11847v2"
    },
    {
        "title": "Two Basic Queueing Models of Service Platforms in Digital Sharing\n  Economy",
        "authors": [
            "Heng-Li Liu",
            "Quan-Lin Li",
            "Xiaole Wu",
            "Chi Zhang"
        ],
        "category": "cs.PF",
        "published_year": "2021",
        "summary": "  This paper describes two basic queueing models of service platforms in\ndigital sharing economy by means of two different policies of platform matching\ninformation. We show that the two queueing models of service platforms can be\nexpressed as the level-independent quasi birth-and-death (QBD) processes. Using\nthe proposed QBD processes, we provide a detailed analysis for the two queueing\nmodels of service platforms, including the system stability, the average\nstationary numbers of seekers and of idle owners, the expected sojourn time of\nan arriving seeker, and the expected profits for both the service platform and\neach owner. Finally, numerical examples are employed to verify our theoretical\nresults, and demonstrate how the performance measures of service platforms are\ninfluenced by some key system parameters. We believe that the methodology and\nresults developed in this paper not only can be applied to develop a broad\nclass of queuing models of service platforms, but also will open a series of\npromising innovative research on performance evaluation, optimal control and\nqueueing-game of service platforms and digital sharing economy.\n",
        "pdf_link": "http://arxiv.org/pdf/2108.02852v1"
    },
    {
        "title": "Dynamic Practical Byzantine Fault Tolerance and Its Blockchain System: A\n  Large-Scale Markov Modeling",
        "authors": [
            "Yan-Xia Chang",
            "Quan-Lin Li",
            "Qing Wang",
            "Xing-Shuo Song"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  In a practical Byzantine fault tolerance (PBFT) blockchain network, the\nvoting nodes may always leave the network while some new nodes can also enter\nthe network, thus the number of voting nodes is constantly changing. Such a new\nPBFT with dynamic nodes is called a dynamic PBFT. Clearly, the dynamic PBFT can\nmore strongly support the decentralization and distributed structure of\nblockchain. However, analyzing dynamic PBFT blockchain systems will become more\ninteresting and challenging.\n  In this paper, we propose a large-scale Markov modeling technique to analyze\nthe dynamic PBFT voting processes and its dynamic PBFT blockchain system. To\nthis end, we set up a large-scale Markov process (and further a\nmulti-dimensional Quasi-Birth-and-Death (QBD) process) and provide performance\nanalysis for both the dynamic PBFT voting processes and the dynamic PBFT\nblockchain system. In particular, we obtain an effective computational method\nfor the throughput of the complicated dynamic PBFT blockchain system. Finally,\nwe use numerical examples to check the validity of our theoretical results and\nindicate how some key system parameters influence the performance measures of\nthe dynamic PBFT voting processes and of the dynamic PBFT blockchain system.\nTherefore, by using the theory of multi-dimensional QBD processes and the\nRG-factorization technique, we hope that the methodology and results developed\nin this paper shed light on the study of dynamic PBFT blockchain systems such\nthat a series of promising research can be developed potentially.\n",
        "pdf_link": "http://arxiv.org/pdf/2210.14003v1"
    },
    {
        "title": "Performance Evaluation, Optimization and Dynamic Decision in Blockchain\n  Systems: A Recent Overview",
        "authors": [
            "Quan-Lin Li",
            "Yan-Xia Chang",
            "Qing Wang"
        ],
        "category": "cs.PF",
        "published_year": "2022",
        "summary": "  With rapid development of blockchain technology as well as integration of\nvarious application areas, performance evaluation, performance optimization,\nand dynamic decision in blockchain systems are playing an increasingly\nimportant role in developing new blockchain technology. This paper provides a\nrecent systematic overview of this class of research, and especially,\ndeveloping mathematical modeling and basic theory of blockchain systems.\nImportant examples include (a) performance evaluation: Markov processes,\nqueuing theory, Markov reward processes, random walks, fluid and diffusion\napproximations, and martingale theory; (b) performance optimization: Linear\nprogramming, nonlinear programming, integer programming, and multi-objective\nprogramming; (c) optimal control and dynamic decision: Markov decision\nprocesses, and stochastic optimal control; and (d) artificial intelligence:\nMachine learning, deep reinforcement learning, and federated learning. So far,\na little research has focused on these research lines. We believe that the\nbasic theory with mathematical methods, algorithms and simulations of\nblockchain systems discussed in this paper will strongly support future\ndevelopment and continuous innovation of blockchain technology.\n",
        "pdf_link": "http://arxiv.org/pdf/2211.15907v1"
    },
    {
        "title": "DIPPM: a Deep Learning Inference Performance Predictive Model using\n  Graph Neural Networks",
        "authors": [
            "Karthick Panner Selvam",
            "Mats Brorsson"
        ],
        "category": "cs.PF",
        "published_year": "2023",
        "summary": "  Deep Learning (DL) has developed to become a corner-stone in many everyday\napplications that we are now relying on. However, making sure that the DL model\nuses the underlying hardware efficiently takes a lot of effort. Knowledge about\ninference characteristics can help to find the right match so that enough\nresources are given to the model, but not too much. We have developed a DL\nInference Performance Predictive Model (DIPPM) that predicts the inference\nlatency, energy, and memory usage of a given input DL model on the NVIDIA A100\nGPU. We also devised an algorithm to suggest the appropriate A100\nMulti-Instance GPU profile from the output of DIPPM. We developed a methodology\nto convert DL models expressed in multiple frameworks to a generalized graph\nstructure that is used in DIPPM. It means DIPPM can parse input DL models from\nvarious frameworks. Our DIPPM can be used not only helps to find suitable\nhardware configurations but also helps to perform rapid design-space\nexploration for the inference performance of a model. We constructed a graph\nmulti-regression dataset consisting of 10,508 different DL models to train and\nevaluate the performance of DIPPM, and reached a resulting Mean Absolute\nPercentage Error (MAPE) as low as 1.9%.\n",
        "pdf_link": "http://arxiv.org/pdf/2303.11733v1"
    },
    {
        "title": "Learning Algorithms for Minimizing Queue Length Regret",
        "authors": [
            "Thomas Stahlbuhk",
            "Brooke Shrader",
            "Eytan Modiano"
        ],
        "category": "cs.PF",
        "published_year": "2020",
        "summary": "  We consider a system consisting of a single transmitter/receiver pair and $N$\nchannels over which they may communicate. Packets randomly arrive to the\ntransmitter's queue and wait to be successfully sent to the receiver. The\ntransmitter may attempt a frame transmission on one channel at a time, where\neach frame includes a packet if one is in the queue. For each channel, an\nattempted transmission is successful with an unknown probability. The\ntransmitter's objective is to quickly identify the best channel to minimize the\nnumber of packets in the queue over $T$ time slots. To analyze system\nperformance, we introduce queue length regret, which is the expected difference\nbetween the total queue length of a learning policy and a controller that knows\nthe rates, a priori. One approach to designing a transmission policy would be\nto apply algorithms from the literature that solve the closely-related\nstochastic multi-armed bandit problem. These policies would focus on maximizing\nthe number of successful frame transmissions over time. However, we show that\nthese methods have $\\Omega(\\log{T})$ queue length regret. On the other hand, we\nshow that there exists a set of queue-length based policies that can obtain\norder optimal $O(1)$ queue length regret. We use our theoretical analysis to\ndevise heuristic methods that are shown to perform well in simulation.\n",
        "pdf_link": "http://arxiv.org/pdf/2005.05206v2"
    },
    {
        "title": "On Delay-Optimal Scheduling in Queueing Systems with Replications",
        "authors": [
            "Yin Sun",
            "C. Emre Koksal",
            "Ness B. Shroff"
        ],
        "category": "cs.PF",
        "published_year": "2016",
        "summary": "  In modern computer systems, jobs are divided into short tasks and executed in\nparallel. Empirical observations in practical systems suggest that the task\nservice times are highly random and the job service time is bottlenecked by the\nslowest straggling task. One common solution for straggler mitigation is to\nreplicate a task on multiple servers and wait for one replica of the task to\nfinish early. The delay performance of replications depends heavily on the\nscheduling decisions of when to replicate, which servers to replicate on, and\nwhich job to serve first. So far, little is understood on how to optimize these\nscheduling decisions for minimizing the delay to complete the jobs. In this\npaper, we present a comprehensive study on delay-optimal scheduling of\nreplications in both centralized and distributed multi-server systems.\nLow-complexity scheduling policies are designed and are proven to be\ndelay-optimal or near delay-optimal in stochastic ordering among all causal and\nnon-preemptive policies. These theoretical results are established for general\nsystem settings and delay metrics that allow for arbitrary arrival processes,\narbitrary job sizes, arbitrary due times, and heterogeneous servers with data\nlocality constraints. Novel sample-path tools are developed to prove these\nresults.\n",
        "pdf_link": "http://arxiv.org/pdf/1603.07322v8"
    }
]